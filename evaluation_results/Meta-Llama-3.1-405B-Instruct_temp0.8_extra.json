{"raw_generations": {"sample_0": ["def test_conversion_to_variance_without_unit(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5)\n    start_uncert = UncertClass(uncert)\n    var_uncert = start_uncert.represent_as(VarianceUncertainty)\n    assert var_uncert.unit is None\n", "def test_conversion_to_from_variance_with_units(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    var_uncert = start_uncert.represent_as(VarianceUncertainty)\n    final_uncert = var_uncert.represent_as(UncertClass)\n    assert var_uncert.unit == (start_uncert.unit ** 2).unit\n    assert final_uncert.unit == start_uncert.unit\n", "def test_conversion_to_variance_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    var_uncert = start_uncert.represent_as(VarianceUncertainty)\n    expected_var = start_uncert._convert_to_variance()\n    assert_array_equal(var_uncert.array, expected_var.array)\n    assert var_uncert.unit == expected_var.unit\n", "def test_parent_nddata_setter(UncertClass):\n    # Test setting parent_nddata directly\n    uncert = UncertClass([1, 2, 3])\n    ndd = NDData([1, 2, 3])\n    uncert.parent_nddata = ndd\n    assert uncert.parent_nddata is ndd\n\n    # Test setting parent_nddata to None\n    uncert.parent_nddata = None\n    with pytest.raises(MissingDataAssociationException):\n        uncert.parent_nddata\n", "def test_conversion_to_from_variance_with_units_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    var_uncert = start_uncert.represent_as(VarianceUncertainty)\n    final_uncert = var_uncert.represent_as(UncertClass)\n    assert_allclose(start_uncert.quantity.value, final_uncert.quantity.value)\n    assert start_uncert.quantity.unit == final_uncert.quantity.unit\n"], "sample_3": ["def test_ecsv_round_trip_quantity_column(tmpdir):\n    \"\"\"Ensure that we can round-trip a Quantity column.\"\"\"\n    filename = str(tmpdir.join('test.ecsv'))\n    t = Table()\n    t['a'] = [1, 2] * u.meter\n    t.write(filename)\n    t2 = Table.read(filename)\n    assert isinstance(t2['a'], Column)\n    assert np.all(t2['a'].value == t['a'].value)\n    assert str(t2['a'].unit) == str(t['a'].unit)\n", "def test_ecsv_round_trip_with_object_dtype():\n    \"\"\"Test round-trip of a table with an object dtype column\"\"\"\n    t = Table()\n    t['a'] = np.array([1, 2], dtype=np.int64)\n    t['b'] = np.array(['hello', 'world'], dtype=object)\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2['a'].dtype == t['a'].dtype\n    assert t2['b'].dtype == t['b'].dtype\n    assert np.all(t2['a'] == t['a'])\n    assert np.all(t2['b'] == t['b'])\n", "def test_ecsv_version():\n    \"\"\"Test that an ECSV file with a version other than 1.0 raises an exception\"\"\"\n    txt = \"\"\"\n    # %ECSV 2.0\n    # ---\n    # datatype:\n    # - {name: col, datatype: string}\n    # schema: astropy-2.0\n    col\n    1\n    2\n    \"\"\"\n    with pytest.raises(ascii.InconsistentTableError):\n        Table.read(txt, format='ascii.ecsv')\n", "def test_read_ecsv_with_missing_metadata():\n    \"\"\"Test reading an ECSV file with missing metadata\"\"\"\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    # datatype:\n    # - {name: a, datatype: int64}\n    a\n    1\n    2\n    \"\"\"\n    t = Table.read(txt, format='ascii.ecsv')\n    assert t['a'].dtype.name == 'int64'\n    assert len(t) == 2\n", "def test_ecsv_round_trip_masked_column():\n    \"\"\"Test round-trip of MaskedColumn through ECSV\"\"\"\n    t = Table()\n    t['a'] = MaskedColumn([1, 2, 3], mask=[True, False, True])\n    t['a'].info.description = 'description'\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert np.all(t2['a'].mask == t['a'].mask)\n    assert np.all(t2['a'] == t['a'])\n    assert t2['a'].info.description == t['a'].info.description\n"], "sample_1": ["def test_separable_single_input_multi_output():\n    model = models.Polynomial1D(1, n_outputs=2)\n    result = is_separable(model)\n    assert_allclose(result, np.array([False, False]))\n    result = separability_matrix(model)\n    assert_allclose(result, np.array([[True], [True]]))\n", "def test_separable_single_input_multi_output():\n    model = models.Polynomial1D(1, n_outputs=2)\n    assert np.all(is_separable(model) == [False, False])\n    assert np.all(separability_matrix(model) == [[True], [True]])\n", "def test_separable_single_input_multi_output_model():\n    model = models.Polynomial2D(1)\n    assert_allclose(is_separable(model), np.array([False, False]))\n    assert_allclose(separability_matrix(model), np.array([[True, True], [True, True]]))\n", "def test_separable_single_input_multiple_outputs():\n    model = models.Shift(1) & models.Shift(2)\n    assert_allclose(is_separable(model), np.array([False, False]))\n    assert_allclose(separability_matrix(model), np.array([[True], [True]]))\n", "def test_separable_single_input_multi_output():\n    model = models.Shift(1) & models.Shift(2)\n    model.n_inputs = 1\n    model.n_outputs = 3\n    result = is_separable(model)\n    assert_allclose(result, np.array([False, False, False]))\n"], "sample_2": ["def test_ccddata_mask_setter():\n    ccd_data = create_ccd_data()\n    mask = np.zeros_like(ccd_data.data, dtype='bool')\n    ccd_data.mask = mask\n    np.testing.assert_array_equal(ccd_data.mask, mask)\n\n    with pytest.raises(ValueError):\n        ccd_data.mask = np.array([1, 2, 3])\n\n    with pytest.raises(TypeError):\n        ccd_data.mask = 'invalid mask'\n", "def test_to_hdu_flags_not_supported():\n    ccd_data = create_ccd_data()\n    with pytest.raises(NotImplementedError):\n        ccd_data.to_hdu(hdu_flags='FLAGS')\n", "def test_ccddata_to_hdu_with_flags(tmpdir):\n    ccd_data = create_ccd_data()\n    flags = np.ones_like(ccd_data.data, dtype='int16')\n    ccd_data.flags = flags\n    with pytest.raises(NotImplementedError):\n        ccd_data.to_hdu(hdu_flags='FLAGS')\n", "def test_mask_uncertainty_setter():\n    ccd_data = create_ccd_data()\n    mask = np.ones_like(ccd_data.data, dtype=np.bool_)\n    uncertainty = StdDevUncertainty(np.ones_like(ccd_data.data))\n\n    # Test setting mask and uncertainty attributes directly\n    ccd_data.mask = mask\n    ccd_data.uncertainty = uncertainty\n\n    np.testing.assert_array_equal(ccd_data.mask, mask)\n    assert isinstance(ccd_data.uncertainty, StdDevUncertainty)\n    np.testing.assert_array_equal(ccd_data.uncertainty.array, uncertainty.array)\n\n    # Test setting mask and uncertainty attributes through constructor\n    ccd_data_2 = CCDData(ccd_data.data, mask=mask, uncertainty=uncertainty, unit='adu')\n    np.testing.assert_array_equal(ccd_data_2.mask, mask)\n    assert isinstance(ccd_data_2.uncertainty, StdDevUncertainty)\n    np.testing.assert_array_equal(ccd_data_2.uncertainty.array, uncertainty.array)\n", "def test_ccddata_invalid_unit_equivalency():\n    ccd_data = create_ccd_data()\n    ccd_data.uncertainty = StdDevUncertainty(np.ones_like(ccd_data))\n    ccd_data.uncertainty.unit = u.second  # invalid unit for uncertainty\n    with pytest.raises(ValueError):\n        ccd_data.to_hdu()\n"], "sample_4": ["def test_read_html_table_format_argument(self, read, write, tmp_path):\n    \"\"\"Test if the format argument is correctly checked\"\"\"\n    fp = tmp_path / \"test_read_html_table_format_argument.html\"\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not\"):\n        read(fp, format=\"csv\")\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not\"):\n        write(fp, format=\"csv\")\n", "    def test_readwrite_html_format_argument(self, cosmo, read, write, tmp_path):\n        \"\"\"Test format argument in read and write functions\"\"\"\n        fp = tmp_path / \"test_readwrite_html_format_argument.html\"\n\n        # Test that format argument is set to 'ascii.html' by default\n        write(fp)\n        assert QTable.read(fp, format=\"ascii.html\")\n\n        # Test that format argument can be set to 'ascii.html' explicitly\n        write(fp, format=\"ascii.html\")\n        assert QTable.read(fp, format=\"ascii.html\")\n\n        # Test that invalid format argument raises an error\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html', not\"):\n            write(fp, format=\"invalid_format\")\n\n        # Test that invalid format argument raises an error when reading\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html', not\"):\n            read(fp, format=\"invalid_format\")\n", "def test_readwrite_html_format_error(self, write, tmp_path):\n    \"\"\"Test if an error is raised when the format is not 'ascii.html'\"\"\"\n    fp = tmp_path / \"test_readwrite_html_format_error.html\"\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not\"):\n        write(fp, format='csv')\n", "def test_readwrite_html_format_argument(self, cosmo, read, write, tmp_path):\n    \"\"\"Test that the format argument is checked in read/write.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_format_argument.html\"\n\n    # ------------\n    # Test write\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n        write(fp, format=\"ascii.csv\")\n\n    # ------------\n    # Test read\n\n    write(fp, format=\"ascii.html\")\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n        read(fp, format=\"ascii.csv\")\n", "def test_readwrite_html_table_format_argument(self, cosmo, read, write, tmp_path):\n    \"\"\"Test passing 'format' argument to read/write functions.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_table_format_argument.html\"\n\n    # Test that format is set correctly when not provided\n    write(fp)\n    assert QTable.read(fp, format=\"ascii.html\")\n\n    # Test that format is set correctly when provided\n    write(fp, format=\"ascii.html\")\n    assert QTable.read(fp, format=\"ascii.html\")\n\n    # Test that incorrect format raises an error\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not 'csv'\"):\n        write(fp, format=\"csv\")\n"], "sample_5": ["def test_parameter_representation(model):\n    m = model['class'](**model['parameters'])\n    for param_name in m.param_names:\n        param = getattr(m, param_name)\n        assert isinstance(repr(param), str)\n        if param.unit is not None:\n            assert param.unit.to_string() in repr(param)\n", "def test_models_parameter_fixed(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n\n    for param_name in m.param_names:\n        orig_value = getattr(m, param_name).value\n        setattr(getattr(m, param_name), 'fixed', True)\n\n        new_value = orig_value + 1\n        with pytest.raises(ValueError):\n            setattr(m, param_name, new_value)\n\n        setattr(getattr(m, param_name), 'fixed', False)\n        setattr(m, param_name, new_value)\n\n        assert getattr(m, param_name).value == new_value\n", "def test_Parameter_validator():\n    # Test that setting a validator on a parameter works correctly\n\n        if value < 0:\n            raise ValueError(\"Value must be non-negative\")\n\n    param = Parameter('test', validator=validate)\n\n    with pytest.raises(ValueError):\n        param.value = -1\n\n    param.value = 1\n    assert param.value == 1\n\n    # Test that setting a validator after creating the parameter works\n    param2 = Parameter('test')\n    param2.validator = validate\n\n    with pytest.raises(ValueError):\n        param2.value = -1\n\n    param2.value = 1\n    assert param2.value == 1\n", "def test_parameter_repr():\n    param = Parameter(name='test', value=1.0, unit=u.m)\n    assert repr(param) == \"Parameter('test', value=1.0, unit=m)\"\n\n    param = Parameter(name='test', value=[1.0, 2.0], unit=u.m)\n    assert repr(param) == \"Parameter('test', value=[1. 2.], unit=m)\"\n", "def test_Parameter_repr():\n    p = Parameter(name='test', value=42, unit=u.deg)\n    expected = \"Parameter('test', value=42, unit=deg)\"\n    assert repr(p) == expected\n\n    p = Parameter(name='test', value=[1, 2, 3], unit=u.deg)\n    expected = \"Parameter('test', value=[1 2 3], unit=deg)\"\n    assert repr(p) == expected\n\n    p = Parameter(name='test', value=np.array([1, 2, 3]), unit=u.deg)\n    expected = \"Parameter('test', value=[1 2 3], unit=deg)\"\n    assert repr(p) == expected\n"], "sample_6": ["def test_angle_to_string():\n    \"\"\"\n    Test Angle to_string method.\n    \"\"\"\n    a = Angle(12.3456, unit=u.deg)\n    assert a.to_string() == '12d20m44.16s'\n    assert a.to_string(decimal=True) == '12.3456'\n    assert a.to_string(unit=u.hour) == '00h49m22.224s'\n    assert a.to_string(sep=':', decimal=False) == '12:20:44.16'\n    assert a.to_string(sep=':', precision=1, decimal=False) == '12:20:44.2'\n    assert a.to_string(format='latex', decimal=False) == r'$12^{\\circ}20{}^{\\prime}44.16{}^{\\prime\\prime}$'\n    assert a.to_string(alwayssign=True, decimal=True) == '+12.3456'\n", "def test_angle_string_formats():\n    \"\"\"\n    Test string formatting for Angle objects.\n    \"\"\"\n    a = Angle(1.2345, unit=u.degree)\n\n    # Test decimal representation\n    assert a.to_string(decimal=True) == '1.2345'\n\n    # Test sexagesimal representation\n    assert a.to_string(decimal=False) == '1d14m4.2s'\n\n    # Test LaTeX representation\n    assert a.to_string(format='latex') == r'$1^{\\circ}14{}^{\\prime}4.2{}^{\\prime\\prime}$'\n\n    # Test unicode representation\n    assert a.to_string(format='unicode') == '1\u00b014\u20324.2\u2033'\n\n    # Test precision\n    assert a.to_string(decimal=True, precision=3) == '1.235'\n\n    # Test padding\n    assert a.to_string(decimal=False, pad=True) == '01d14m04.2s'\n\n    # Test fields\n    assert a.to_string(decimal=False, fields=2) == '1d14m'\n", "def test_angle_wrap_at():\n    a = Angle([10, 350]*u.deg)\n    wrapped_a = a.wrap_at(180*u.deg)\n\n    assert_allclose(wrapped_a.degree, [-170, -10])\n\n    a = Angle([-180, -150, -100, 0, 100, 150, 180, 200, 250, 300, 350]*u.deg)\n    wrapped_a = a.wrap_at(360*u.deg)\n\n    assert_allclose(wrapped_a.degree, [180, 210, 260, 0, 100, 150, 180, 200, 250, 300, 350])\n\n    a = Angle([-180, -150, -100, 0, 100, 150, 180, 200, 250, 300, 350]*u.deg)\n    wrapped_a = a.wrap_at(180*u.deg)\n\n    assert_allclose(wrapped_a.degree, [-180, -150, -100, 0, 100, 150, -180, -160, -110, -60, -10])\n", "def test_angle_string_parsing():\n    \"\"\"\n    Test parsing of string input for Angle objects.\n    \"\"\"\n    # Test parsing with default unit\n    a = Angle('1:2:3')\n    assert a.unit == u.degree\n    assert_allclose(a.value, 1 + 2/60. + 3/3600.)\n\n    # Test parsing with specified unit\n    a = Angle('1:2:3', unit=u.hour)\n    assert a.unit == u.hour\n    assert_allclose(a.value, 1 + 2/60. + 3/3600.)\n\n    # Test parsing with degree symbol\n    a = Angle('1\u00b02\u20323\u2033')\n    assert a.unit == u.degree\n    assert_allclose(a.value, 1 + 2/60. + 3/3600.)\n\n    # Test parsing with hour symbol\n    a = Angle('1\u02b02\u1d503\u02e2')\n    assert a.unit == u.hour\n    assert_allclose(a.value, 1 + 2/60. + 3/3600.)\n\n    # Test parsing with sign\n    a = Angle('-1:2:3')\n    assert a.unit == u.degree\n    assert_allclose(a.value, -1 - 2/60. - 3/3600.)\n\n    # Test parsing with decimal degrees\n    a = Angle('1.2345d')\n    assert a.unit == u.degree\n    assert_allclose(a.value, 1.2345)\n\n    # Test parsing with decimal hours\n    a = Angle('1.2345h')\n    assert a.unit == u.hour\n    assert_allclose(a.value, 1.2345)\n", "def test_angle_to_string():\n    # Test the to_string method for angles in different units and formats.\n    a = Angle(10.2345, unit=u.deg)\n\n    assert a.to_string(unit=u.deg) == \"10.2345\"\n    assert a.to_string(unit=u.deg, decimal=False) == \"10 14 04.2\"\n    assert a.to_string(unit=u.hour) == \"0.6809677419354839\"\n    assert a.to_string(unit=u.hour, decimal=False) == \"00 40 51.48\"\n\n    # Test the format argument.\n    assert a.to_string(format=\"latex\") == r\"10.2345^{\\circ}\"\n    assert a.to_string(format=\"latex\", decimal=False) == r\"10^{\\circ}14{}'04.2{}''\"\n    assert a.to_string(format=\"unicode\") == \"10.2345\u00b0\"\n    assert a.to_string(format=\"unicode\", decimal=False) == \"10\u00b014\u203204.2\u2033\"\n"], "sample_7": ["def test_col_copy_with_metadata():\n    \"\"\"Test that copying a Column also copies its metadata.\"\"\"\n    c = table.Column([1, 2, 3], name='a', unit='m', format='%i',\n                     description='test column', meta={'c': 8, 'd': 12})\n    c2 = c.copy()\n    assert c2.name == 'a'\n    assert c2.unit == u.m\n    assert c2.format == '%i'\n    assert c2.description == 'test column'\n    assert c2.meta == {'c': 8, 'd': 12}\n\n    # Make sure changes to the original do not affect the copy\n    c.name = 'b'\n    c.unit = u.km\n    c.format = '%f'\n    c.description = 'new test column'\n    c.meta['e'] = 42\n    assert c2.name == 'a'\n    assert c2.unit == u.m\n    assert c2.format == '%i'\n    assert c2.description == 'test column'\n    assert c2.meta == {'c': 8, 'd': 12}\n", "def test_column_to_pandas_series():\n    \"\"\"Test conversion of Column to pandas.Series\"\"\"\n    import pandas as pd\n\n    # Test simple column with no mask\n    c = table.Column([1, 2, 3], name='a')\n    s = c.to_pandas()\n    assert isinstance(s, pd.Series)\n    assert np.all(s.values == c)\n    assert s.name == 'a'\n\n    # Test column with mask\n    c = table.MaskedColumn([1, 2, 3], name='a', mask=[False, True, False])\n    s = c.to_pandas()\n    assert isinstance(s, pd.Series)\n    assert np.all(s.values == c.filled(np.nan))\n    assert s.name == 'a'\n", "def test_col_copy_indices():\n    \"\"\"Test copying of indices when creating a new Column or MaskedColumn\"\"\"\n    from astropy.table import Column, MaskedColumn\n    from astropy.table.index import Index\n\n    # Create an index for testing\n    idx = Index(name='my_idx')\n\n    # Test Column\n    c = Column([1, 2, 3], name='a', indices=[idx])\n    c2 = c.copy()\n    assert c.indices is not c2.indices\n    assert len(c2.indices) == 1\n    assert c2.indices[0].name == 'my_idx'\n\n    # Test MaskedColumn\n    mc = MaskedColumn([1, 2, 3], name='a', mask=[True, False, True], indices=[idx])\n    mc2 = mc.copy()\n    assert mc.indices is not mc2.indices\n    assert len(mc2.indices) == 1\n    assert mc2.indices[0].name == 'my_idx'\n", "def test_column_info_groups():\n    \"\"\"Test that ColumnInfo groups are correctly handled\"\"\"\n    col = table.Column([1, 2, 3], name='a')\n    assert col.info.groups == []\n    col.info.groups = ['group1', 'group2']\n    assert col.info.groups == ['group1', 'group2']\n    new_col = col.copy()\n    assert new_col.info.groups == ['group1', 'group2']\n    new_col.info.groups = ['group3']\n    assert new_col.info.groups == ['group3']\n    assert col.info.groups == ['group1', 'group2']\n", "def test_to_list():\n    \"\"\"Test converting a Column to a list.\"\"\"\n    c = table.Column([1, 2, 3], dtype='i4')\n    assert c.tolist() == [1, 2, 3]\n\n    c = table.Column(['a', 'b', 'c'], dtype='S1')\n    assert c.tolist() == ['a', 'b', 'c']\n\n    c = table.MaskedColumn([1, 2, np.ma.masked], dtype='i4')\n    assert c.tolist() == [1, 2, None]\n"], "sample_8": ["def test_masked_array_from_ma_masked(self):\n    \"\"\"Check that we can initialize a MaskedArray from np.ma.masked.\"\"\"\n    np_ma = np.ma.masked\n    ma = Masked(np_ma)\n    assert type(ma) is MaskedNDArray\n    assert ma.mask\n", "    def test_ufunc_with_masked_array(self):\n        \"\"\"Check that ufuncs work properly with masked arrays.\"\"\"\n        result = np.add(self.ma, self.mb)\n        assert type(result) is type(self.ma)\n        assert_array_equal(result.unmasked, np.add(self.a, self.b))\n        assert_array_equal(result.mask, np.logical_or(self.mask_a, self.mask_b))\n", "    def test_masked_array_with_numpy_function(self):\n        \"\"\"Check that numpy functions can handle MaskedArrays.\"\"\"\n        np_func = np.sum\n        result = np_func(self.ma)\n        expected = Masked(np_func(self.a), self.mask_a.any())\n        assert_masked_equal(result, expected)\n", "def test_masked_array_interaction_with_numpy_functions(MaskedArraySetup):\n    \"\"\"Check that we can use numpy functions on MaskedArray.\"\"\"\n    np_all = np.all(self.ma)\n    assert isinstance(np_all, bool)\n    assert np_all == np.all(self.a)\n\n    np_any = np.any(self.ma)\n    assert isinstance(np_any, bool)\n    assert np_any == np.any(self.a)\n\n    np_sum = np.sum(self.ma)\n    assert isinstance(np_sum, (int, float))\n    assert np_sum == np.sum(self.a)\n\n    np_mean = np.mean(self.ma)\n    assert isinstance(np_mean, float)\n    assert np_mean == np.mean(self.a)\n\n    np_std = np.std(self.ma)\n    assert isinstance(np_std, float)\n    assert np_std == np.std(self.a)\n\n    np_var = np.var(self.ma)\n    assert isinstance(np_var, float)\n    assert np_var == np.var(self.a)\n\n    np_min = np.min(self.ma)\n    assert isinstance(np_min, (int, float))\n    assert np_min == np.min(self.a)\n\n    np_max = np.max(self.ma)\n    assert isinstance(np_max, (int, float))\n    assert np_max == np.max(self.a)\n", "    def test_ufunc_propagation(self):\n        \"\"\"Test that ufuncs propagate masks.\"\"\"\n        result = np.sin(self.ma)\n        assert isinstance(result, Masked)\n        assert_array_equal(result.unmasked, np.sin(self.a))\n        assert_array_equal(result.mask, self.mask_a)\n"], "sample_9": ["def test_write_table_html_js_files():\n    \"\"\"\n    Test that passing jsfiles should include script tags for the files in the output.\n    \"\"\"\n\n    col1 = [1, 2]\n    col2 = [1.234567e-11, -9.876543e11]\n    table = Table([col1, col2], names=('C1', 'C2'))\n\n    expected = \"\"\"\\", "def test_read_html_css_files():\n    \"\"\"\n    Test reading an HTML table with CSS files\n    \"\"\"\n    table_in = ['<table>',\n                '<tr><th>A</th></tr>',\n                '<tr><td>1</td></tr>',\n                '</table>']\n    htmldict = {'cssfiles': ['data/style.css']}\n    dat = Table.read(table_in, format='ascii.html', htmldict=htmldict)\n    assert len(dat) == 1\n    assert np.all(dat['A'] == [1])\n", "def test_write_table_html_js_css():\n    \"\"\"\n    Test that passing js and css files to the HTML writer includes them in the output.\n    \"\"\"\n\n    col1 = [1, 2]\n    col2 = [1.234567e-11, -9.876543e11]\n    table = Table([col1, col2], names=('C1', 'C2'))\n\n    expected = \"\"\"\\", "def test_read_html_css_files():\n    \"\"\"\n    Test reading an HTML table with CSS files specified in the htmldict\n    \"\"\"\n    table_in = ['<table>',\n                '<tr><th>A</th> <th>B</th></tr>',\n                '<tr><td>1</td><td>2</td></tr>',\n                '</table>']\n    css_file = 'data/html.css'\n    htmldict = {'cssfiles': [css_file]}\n    dat = Table.read(table_in, format='ascii.html', htmldict=htmldict)\n    assert np.all(dat['A'] == [1])\n    assert np.all(dat['B'] == [2])\n", "def test_write_table_html_js_files():\n    \"\"\"\n    Test that passing jsfiles should include script tags in the HTML output.\n    \"\"\"\n\n    col1 = [1, 2]\n    col2 = [3, 4]\n    table = Table([col1, col2], names=('C1', 'C2'))\n    expected = \"\"\"\\"], "sample_11": ["def test_combine_slices_with_start():\n    slice1 = slice(10, 20)\n    slice2 = slice(5, 15)\n    expected = slice(15, 25)\n    assert combine_slices(slice1, slice2) == expected\n\n    slice1 = slice(10, None)\n    slice2 = slice(5, 15)\n    expected = slice(15, None)\n    assert combine_slices(slice1, slice2) == expected\n\n    slice1 = slice(None, 20)\n    slice2 = slice(5, 15)\n    expected = slice(5, 20)\n    assert combine_slices(slice1, slice2) == expected\n", "def test_dropped_dimensions_serialization():\n    wcs = WCS_SPECTRAL_CUBE\n\n    sub = SlicedLowLevelWCS(wcs, np.s_[:, 0])\n\n    dwd = sub.dropped_world_dimensions\n    assert not dwd[\"serialized_classes\"]\n\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    assert len(wao_classes) == 1 and \"spectral\" in wao_classes and wao_classes[\"spectral\"][0] == u.Quantity\n    validate_info_dict(dwd, {\n        \"value\": [20],\n        \"world_axis_physical_types\": [\"em.freq\"],\n        \"world_axis_names\": [\"Frequency\"],\n        \"world_axis_units\": [\"Hz\"],\n        \"world_axis_object_components\": [('spectral', 0, 'value')],\n        })\n", "def test_sliced_low_level_wcs_repr_with_no_pixel_bounds():\n    wcs = WCS_SPECTRAL_CUBE\n    wcs.pixel_bounds = None\n    sliced_wcs = SlicedLowLevelWCS(wcs, Ellipsis)\n    assert \"Bounds\" not in str(sliced_wcs)\n", "def test_sliced_low_level_wcs_repr_with_dropped_dimensions():\n    wcs = WCS_SPECTRAL_CUBE\n    sub = SlicedLowLevelWCS(wcs, np.s_[:, 0])\n    assert 'Dropped dimensions' in repr(sub)\n    assert 'value' in repr(sub)\n    assert 'world_axis_physical_types' in repr(sub)\n    assert 'world_axis_names' in repr(sub)\n    assert 'world_axis_units' in repr(sub)\n", "def test_sliced_low_level_wcs_pixel_bounds():\n    wcs = WCS_SPECTRAL_CUBE\n    sub = SlicedLowLevelWCS(wcs, np.s_[5:10, 2:8, :])\n    assert_equal(sub.pixel_bounds, [(-6, 6), (-8, 4), (5, 15)])\n"], "sample_12": ["def test_longitude_wrap_at_quantity():\n    \"\"\"\n    Test that Longitude correctly handles wrap_at when a Quantity is passed.\n    \"\"\"\n    lon = Longitude(450 * u.deg)\n    assert lon.wrap_at(360 * u.deg).degree == 90\n    assert lon.wrap_at(u.Quantity(360, u.deg)).degree == 90\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the to_string method of Angle correctly handles precision.\n    \"\"\"\n    a = Angle('1.23456789d')\n    assert a.to_string(precision=2) == '1d14m04.26s'\n    assert a.to_string(precision=4) == '1d14m04.2572s'\n    assert a.to_string(precision=8) == '1d14m04.257896s'\n    assert a.to_string(unit=u.hour, precision=2) == '0h05m38.18s'\n    assert a.to_string(unit=u.hour, precision=4) == '0h05m38.1836s'\n    assert a.to_string(unit=u.hour, precision=8) == '0h05m38.1835944s'\n", "def test_angle_input_list_of_strings_with_invalid_unit():\n    \"\"\"\n    Test that an Angle initialized with a list of strings and an invalid unit raises an exception.\n    \"\"\"\n    with pytest.raises(u.UnitsError):\n        Angle(['1d', '2d'], unit='invalid_unit')\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the to_string method of Angle correctly handles precision\n    when converting to string.\n    \"\"\"\n\n    angle = Angle(1.23456789, unit=u.degree)\n\n    # Test that default precision is used when not specified\n    assert angle.to_string() == '1d14m04.2s'\n\n    # Test that specified precision is used\n    assert angle.to_string(precision=0) == '1d14m04s'\n    assert angle.to_string(precision=1) == '1d14m04.2s'\n    assert angle.to_string(precision=5) == '1d14m04.20002s'\n\n    # Test that precision is correctly handled for negative angles\n    angle = Angle(-1.23456789, unit=u.degree)\n    assert angle.to_string() == '-1d14m04.2s'\n    assert angle.to_string(precision=0) == '-1d14m04s'\n    assert angle.to_string(precision=1) == '-1d14m04.2s'\n    assert angle.to_string(precision=5) == '-1d14m04.20002s'\n", "def test_angle_to_string_precision():\n    # Test that the precision argument in to_string works as expected\n    a = Angle('10:20:30.12345678d')\n    assert a.to_string(precision=2) == '10d20m30.12s'\n    assert a.to_string(precision=4) == '10d20m30.1235s'\n    assert a.to_string(precision=8) == '10d20m30.12345678s'\n    assert a.to_string(precision=None) == '10d20m30.12345678s'\n"], "sample_13": ["def test_angle_with_various_unicode_minus():\n    \"\"\"\n    Test parsing of angle strings with various unicode minus characters.\n    \"\"\"\n    a = Angle('\u221200:00:10', u.deg)  # U+2212 MINUS SIGN\n    assert_allclose(a.degree, -10. / 3600.)\n\n    a = Angle('\u201000:00:10', u.deg)  # U+2010 HYPHEN\n    assert_allclose(a.degree, -10. / 3600.)\n\n    a = Angle('\u201300:00:10', u.deg)  # U+2013 EN DASH\n    assert_allclose(a.degree, -10. / 3600.)\n\n    a = Angle('\u201400:00:10', u.deg)  # U+2014 EM DASH\n    assert_allclose(a.degree, -10. / 3600.)\n\n    a = Angle('\u201500:00:10', u.deg)  # U+2015 HORIZONTAL BAR\n    assert_allclose(a.degree, -10. / 3600.)\n", "def test_angle_to_string_precision():\n    # Test that the precision argument in to_string works correctly\n    angle = Angle(1.23456789, unit=u.deg)\n    assert angle.to_string(precision=2) == '1d14m04.27s'\n    assert angle.to_string(precision=4) == '1d14m04.2719s'\n    assert angle.to_string(decimal=True, precision=2) == '1.23deg'\n    assert angle.to_string(decimal=True, precision=4) == '1.2346deg'\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the to_string method handles precision correctly.\n    \"\"\"\n    a = Angle(1.23456789, unit=u.deg)\n    assert a.to_string(precision=2) == '1d14m04.27s'\n    assert a.to_string(precision=4) == '1d14m04.2745s'\n    assert a.to_string(precision=6) == '1d14m04.274567s'\n    assert a.to_string(precision=8) == '1d14m04.27456789s'\n\n    # Test with different units\n    a = Angle(1.23456789, unit=u.hour)\n    assert a.to_string(precision=2) == '1h14m04.27s'\n    assert a.to_string(precision=4) == '1h14m04.2745s'\n    assert a.to_string(precision=6) == '1h14m04.274567s'\n    assert a.to_string(precision=8) == '1h14m04.27456789s'\n\n    # Test with arrays\n    a = Angle([1.23456789, 2.3456789], unit=u.deg)\n    assert np.all(a.to_string(precision=2) == ['1d14m04.27s', '2d20m44.45s'])\n    assert np.all(a.to_string(precision=4) == ['1d14m04.2745s', '2d20m44.4523s'])\n    assert np.all(a.to_string(precision=6) == ['1d14m04.274567s', '2d20m44.452346s'])\n    assert np.all(a.to_string(precision=8) == ['1d14m04.27456789s', '2d20m44.45234568s'])\n", "def test_angle_roundtrip_with_format():\n    \"\"\"\n    Test that creating an Angle object from a string and then converting it back\n    to a string with the same format gives the same result.\n    \"\"\"\n    angle = Angle(\"12:34:56.789\", unit=u.degree)\n    assert str(angle) == \"12d34m56.789s\"\n    assert angle.to_string(sep=\":\", precision=3) == \"12:34:56.789\"\n\n    angle = Angle(\"12h34m56.789s\")\n    assert str(angle) == \"12h34m56.789s\"\n    assert angle.to_string(sep=\":\", precision=3) == \"12:34:56.789\"\n\n    angle = Angle(\"12.3456\", unit=u.degree)\n    assert str(angle) == \"12d20m44.16s\"\n    assert angle.to_string(decimal=True, precision=4) == \"12.3456\"\n", "def test_longitude_setitem():\n    lon = Longitude(['10d', '20d'])\n    lon[0] = '30d'\n    assert np.all(lon == Angle(['30d', '20d']))\n\n    # Test setting a mutable value and having it wrap\n    lon[1] = -10 * u.deg\n    assert np.all(lon == Angle(['30d', '350d']))\n\n    # Test setting an array value\n    lon[:] = ['40d', '50d']\n    assert np.all(lon == Angle(['40d', '50d']))\n\n    # Test setting a single value for all elements in the array\n    lon[:] = '60d'\n    assert np.all(lon == Angle(['60d', '60d']))\n"], "sample_14": ["def test_angle_to_string_with_format():\n    \"\"\"\n    Test to_string method with format argument.\n\n    Regression test for gh-14158.\n    \"\"\"\n    angle = Angle(0.25 * u.hourangle)\n    assert angle.to_string(format='latex') == r'$0^{\\mathrm{h}}15^{\\mathrm{m}}00^{\\mathrm{s}}$'\n    assert angle.to_string(format='unicode') == '0\u02b015\u1d5000\u02e2'\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test the precision of the to_string method.\n\n    Regression test for issue #14173.\n    \"\"\"\n    angle = Angle(1.23456789 * u.deg)\n    assert angle.to_string(precision=2) == '1d14m04.26s'\n    assert angle.to_string(precision=4) == '1d14m04.2556s'\n    assert angle.to_string(precision=6) == '1d14m04.255512s'\n    assert angle.to_string(precision=8) == '1d14m04.25551156s'\n    assert angle.to_string(precision=10) == '1d14m04.2555115648s'\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the to_string method of Angle objects correctly handles precision.\n    \"\"\"\n    angle = Angle(1.23456789, unit=u.degree)\n    assert angle.to_string(precision=2) == '1d14m04.27s'\n    assert angle.to_string(precision=4) == '1d14m04.2743s'\n    assert angle.to_string(precision=6) == '1d14m04.274322s'\n    assert angle.to_string(precision=8) == '1d14m04.27432222s'\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the to_string method handles precision correctly.\n\n    This is a regression test for gh-14257.\n    \"\"\"\n    angle = Angle(1.23456789 * u.deg)\n    assert angle.to_string(precision=3) == \"1d14m04.2s\"\n    assert angle.to_string(decimal=True, precision=3) == \"1.235\"\n    assert angle.to_string(unit=u.hour, precision=3) == \"0h05m38.5s\"\n    assert angle.to_string(unit=u.hour, decimal=True, precision=3) == \"0.093\"\n", "def test_angle_format_with_fields():\n    \"\"\"\n    Test the 'fields' argument of Angle.to_string()\n    \"\"\"\n\n    a = Angle('1h2m3s')\n\n    # default is to show all fields\n    assert a.to_string(fields=None) == '1h02m03s'\n\n    # show only hours and minutes\n    assert a.to_string(fields=2) == '1h02m'\n\n    # show only hours\n    assert a.to_string(fields=1) == '1h'\n\n    # show more fields than we have\n    assert a.to_string(fields=4) == '1h02m03s'\n"], "sample_15": ["    def test_create(self):\n        data = np.array([(1, 2), (3, 4)], dtype=[('x', float), ('y', int)])\n        q = u.Quantity(data, u.m)\n        assert isinstance(q, u.Quantity)\n        assert q.unit is u.m\n        assert np.all(q.value == data)\n", "    def test_item(self):\n        q = np.array([1.0, 2.0, 3.0]) * u.m\n        assert isinstance(q.item(), u.Quantity)\n        assert q.item() == 1.0 * u.m\n", "    def test_masked_array_init(self):\n        masked_array = np.ma.array([1, 2, 3], mask=[True, False, True])\n        quantity = u.Quantity(masked_array, unit=\"m\")\n        assert isinstance(quantity.value, np.ma.MaskedArray)\n", "    def test_init_with_subok(self):\n        q = u.Quantity(1.0, u.m, subok=True)\n        assert isinstance(q, u.Quantity)\n\n        q2 = u.Quantity(1.0, u.m, subok=False)\n        assert isinstance(q2, u.Quantity)\n\n        class QuantitySubclass(u.Quantity):\n            pass\n\n        q3 = QuantitySubclass(1.0, u.m)\n        q4 = u.Quantity(q3, subok=True)\n        assert isinstance(q4, QuantitySubclass)\n\n        q5 = u.Quantity(q3, subok=False)\n        assert not isinstance(q5, QuantitySubclass)\n        assert isinstance(q5, u.Quantity)\n", "    def test_indexing_keeps_unit(self):\n        q = np.arange(10.0) * u.m\n        assert q[3].unit == u.m\n        assert q[:3].unit == u.m\n"], "sample_16": ["    def setup_method(self):\n        self.data = np.array([(1, 2), (3, 4)], dtype=[(\"x\", float), (\"y\", float)])\n        self.mask = np.array([True, False])\n        self.unit = u.StructuredUnit((u.m, u.s), (\"x\", \"y\"))\n        self.quantity = u.Quantity(self.data, self.unit)\n        self.masked_quantity = u.Quantity(self.data, self.unit, mask=self.mask)\n", "    def test_structured_unit_creation(self):\n        unit = u.StructuredUnit((u.m, u.s), (\"length\", \"time\"))\n        assert isinstance(unit, u.StructuredUnit)\n", "def test_einsum_subclass_safe(self):\n    # Test einsum with a subclass of Quantity that has the same __array_function__\n    class QuantitySubclass(u.Quantity):\n        pass\n\n    q1 = np.arange(9.0).reshape(3, 3) * u.m\n    q2 = np.eye(3) / u.s\n    q1_subclass = QuantitySubclass(q1)\n    q2_subclass = QuantitySubclass(q2)\n\n    o = np.einsum(\"ij,jk\", q1_subclass, q2_subclass)\n    expected = np.einsum(\"ij,jk\", q1.value, q2.value) * (q1.unit / q2.unit)\n    assert np.all(o == expected)\n", "    def test_structured_to_unstructured(self):\n        # Create a structured quantity with units of m and s\n        structured_quantity = np.array([(1.0, 2.0), (3.0, 4.0)], dtype=[('x', float), ('y', float)]) * u.Unit(\"(m, s)\")\n        \n        # Convert it to an unstructured quantity\n        unstructured_quantity = rfn.structured_to_unstructured(structured_quantity)\n        \n        # Check that the resulting quantity has the correct shape and units\n        assert unstructured_quantity.shape == (2, 2)\n        assert unstructured_quantity.unit == u.m\n\n        # Check that the values are correct\n        expected_values = np.array([[1.0, 2.0], [3.0, 4.0]]) * u.m\n        assert_array_equal(unstructured_quantity, expected_values)\n", "    def setup_class(self):\n        self.pv_dtype = np.dtype([(\"p\", \"f8\"), (\"v\", \"f8\")])\n        self.pv_t_dtype = np.dtype(\n            [(\"pv\", np.dtype([(\"pp\", \"f8\"), (\"vv\", \"f8\")])), (\"t\", \"f8\")]\n        )\n\n        self.pv = np.array([(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)], self.pv_dtype)\n        self.pv_t = np.array(\n            [((4.0, 2.5), 0.0), ((5.0, 5.0), 1.0), ((6.0, 7.5), 2.0)], self.pv_t_dtype\n        )\n\n        self.pv_unit = u.StructuredUnit((u.km, u.km / u.s), (\"p\", \"v\"))\n        self.pv_t_unit = u.StructuredUnit((self.pv_unit, u.s), (\"pv\", \"t\"))\n\n        self.q_pv = self.pv << self.pv_unit\n        self.q_pv_t = self.pv_t << self.pv_t_unit\n"], "sample_17": ["    def test_structured_to_unstructured(self):\n        # Create a structured Quantity with units of km and km/s\n        pv_dtype = np.dtype([(\"p\", \"f8\"), (\"v\", \"f8\")])\n        pv_unit = u.StructuredUnit((u.km, u.km / u.s), (\"p\", \"v\"))\n        q_pv = np.array([(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)], pv_dtype) << pv_unit\n\n        # Convert it to an unstructured Quantity\n        unstruct = rfn.structured_to_unstructured(q_pv)\n\n        # Check the result\n        assert_array_equal(unstruct, [[1.0, 0.25], [2.0, 0.5], [3.0, 0.75]] * u.km)\n", "def test_array_function_enabled():\n    assert ARRAY_FUNCTION_ENABLED == getattr(np.core.overrides, \"ENABLE_ARRAY_FUNCTION\", True)\n", "    def test_structured_to_unstructured(self):\n        q = u.Quantity(np.array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)]), u.Unit(\"(m, m, s)\"))\n        unstruct = rfn.structured_to_unstructured(q)\n        assert_array_equal(unstruct, [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]] * u.m)\n", "    def test_structured_to_unstructured_error_mismatched_units(self):\n        mismatched_unit = u.Unit(\"(m, s)\")\n        q = np.array([(1.0, 2.0), (3.0, 4.0)], dtype=[(\"x\", float), (\"y\", float)]) * mismatched_unit\n        with pytest.raises(u.UnitConversionError):\n            rfn.structured_to_unstructured(q)\n", "    def test_unsupported_units(self):\n        # Create a structured quantity with different units for each field\n        dtype = np.dtype([(\"length\", float), (\"time\", float)])\n        unit = u.StructuredUnit((u.meter, u.second), (\"length\", \"time\"))\n        q = np.array([(1.0, 2.0)], dtype=dtype) * unit\n\n        # Attempt to convert the quantity to an incompatible unit\n        with pytest.raises(u.UnitConversionError):\n            q.to(u.kilogram)\n"], "sample_18": ["    def test_quantity_iterator(self):\n        q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        q_iter = q.flat\n\n        # Test iteration\n        for i, val in enumerate(q_iter):\n            assert val == q[i]\n\n        # Test indexing\n        assert q_iter[1] == q[1]\n\n        # Test slicing\n        assert np.array_equal(q_iter[1:3], q[1:3])\n\n        # Test len\n        assert len(q_iter) == len(q)\n", "    def setup_class(self):\n        self.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n", "    def setup_class(self):\n        self.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n", "    def setup_class(self):\n        self.q = u.Quantity(\n            np.array([(1.0, 2.0), (3.0, 4.0)], dtype=[(\"p\", \"f8\"), (\"v\", \"f8\")]),\n            \"m, m/s\",\n        )\n        self.q.info.name = \"pv\"\n        self.q.info.description = \"Location and speed\"\n", "    def setup_class(self):\n        self.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        self.q.info.name = \"v\"\n        self.q.info.description = \"air speed of a african swallow\"\n"], "sample_20": ["def test_read_fits_with_no_data(tmp_path):\n    filename = tmp_path / \"test_empty.fits\"\n    hdu = BinTableHDU(np.array([]), nrows=0)\n    hdu.writeto(filename, overwrite=True)\n\n    with pytest.raises(ValueError) as exc:\n        Table.read(filename)\n    assert exc.value.args[0] == \"No data in HDU\"\n\n    t = Table.read(filename, hdu=1)\n    assert len(t) == 0\n", "def test_is_fits_fileobj(tmp_path):\n    \"\"\"Test is_fits with file object\"\"\"\n    filename = tmp_path / \"test.fits\"\n    with open(filename, \"wb\") as f:\n        f.write(b\"SIMP\")\n\n    with open(filename, \"rb\") as f:\n        assert connect.is_fits(None, None, f)\n", "def test_read_with_unit_parse_strict(tmp_path):\n    filename = tmp_path / \"test_read_with_unit_parse_strict.fits\"\n    hdu = BinTableHDU(np.array([1, 2, 3]), name=\"data\")\n    hdu.columns[0].unit = \" invalid unit\"\n    hdu.writeto(filename)\n\n    with pytest.warns(u.UnitsWarning, match=\"did not parse as fits unit\"):\n        Table.read(filename, unit_parse_strict=\"warn\")\n\n    with pytest.raises(ValueError, match=\"did not parse as fits unit\"):\n        Table.read(filename, unit_parse_strict=\"raise\")\n\n    Table.read(filename, unit_parse_strict=\"silent\")\n", "def test_is_fits_with_HDUList():\n    hdul = HDUList([PrimaryHDU()])\n    assert connect.is_fits(hdul, None, None)\n    assert not connect.is_fits(None, \"foo.bar\", None)\n\n    class Dummy:\n        pass\n\n    dummy = Dummy()\n    assert not connect.is_fits(dummy, None, None)\n", "def test_read_table_fits_with_scale(tmp_path):\n    \"\"\"\n    Test reading a FITS table with scaled columns.\n    \"\"\"\n    filename = tmp_path / \"test_scaled.fits\"\n    data = np.array([1, 2, 3, 4], dtype=np.int32)\n    col = fits.Column(name=\"a\", array=data, format=\"I\", bscale=2.0, bzero=10.0)\n    hdu = fits.TableHDU.from_columns([col])\n    hdu.writeto(filename)\n\n    t = Table.read(filename)\n    assert_array_equal(t[\"a\"].data, data * 2.0 + 10.0)\n"], "sample_21": ["def test_line_type():\n    assert _line_type(\"READ SERR 3\") == \"command\"\n    assert _line_type(\" \\\\n    !some gibberish\") == \"comment\"\n    assert _line_type(\"   \") == \"comment\"\n    assert _line_type(\" 21345.45\") == \"data,1\"\n    assert _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\") == \"data,6\"\n    assert _line_type(\" 21345.45 ! a comment to disturb\") == \"data,1\"\n    assert _line_type(\"NO NO NO NO NO\") == \"new\"\n    with pytest.raises(ValueError):\n        _line_type(\" some non-comment gibberish\")\n    with pytest.raises(ValueError):\n        _line_type(\"N O N NOON OON O\")\n", "def test_line_type():\n    assert _line_type(\"READ SERR 3\") == 'command'\n    assert _line_type(\" \\\\n    !some gibberish\") == 'comment'\n    assert _line_type(\"   \") == 'comment'\n    assert _line_type(\" 21345.45\") == 'data,1'\n    assert _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\") == 'data,6'\n    assert _line_type(\" 21345.45 ! a comment to disturb\") == 'data,1'\n    assert _line_type(\"NO NO NO NO NO\") == 'new'\n    \n    with pytest.raises(ValueError):\n        _line_type(\"N O N NOON OON O\")\n        \n    with pytest.raises(ValueError):\n        _line_type(\" some non-comment gibberish\")\n", "def test_write_table_qdp_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"b\", data=[4, 5, 6]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [2]})\n\n    with open(test_file) as fobj:\n        lines = fobj.readlines()\n\n    assert \"READ TERR 1\\n\" in lines\n    assert \"READ SERR 2\\n\" in lines\n    assert len(lines) == 7\n    assert lines[-1].startswith(\"1 4\")\n", "def test_line_type():\n    assert _line_type(\"READ SERR 3\") == \"command\"\n    assert _line_type(\" \\\\n    !some gibberish\") == \"comment\"\n    assert _line_type(\"   \") == \"comment\"\n    assert _line_type(\" 21345.45\") == \"data,1\"\n    assert _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\") == \"data,6\"\n    assert _line_type(\" 21345.45,1.53e-3,1e-3,.04,NO,nan\", delimiter=',') == \"data,6\"\n    assert _line_type(\" 21345.45 ! a comment to disturb\") == \"data,1\"\n    assert _line_type(\"NO NO NO NO NO\") == \"new\"\n    assert _line_type(\"NO,NO,NO,NO,NO\", delimiter=',') == \"new\"\n\n    with pytest.raises(ValueError):\n        _line_type(\" some non-comment gibberish\")\n\n    with pytest.raises(ValueError):\n        _line_type(\"N O N NOON OON O\")\n", "def test_line_type():\n    assert _line_type(\"READ SERR 3\") == \"command\"\n    assert _line_type(\" \\\\n    !some gibberish\") == \"comment\"\n    assert _line_type(\"   \") == \"comment\"\n    assert _line_type(\" 21345.45\") == \"data,1\"\n    assert _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\") == \"data,6\"\n    assert _line_type(\" 21345.45,1.53e-3,1e-3,.04,NO,nan\", delimiter=',') == \"data,6\"\n    assert _line_type(\" 21345.45 ! a comment to disturb\") == \"data,1\"\n    assert _line_type(\"NO NO NO NO NO\") == \"new\"\n    assert _line_type(\"NO,NO,NO,NO,NO\", delimiter=',') == \"new\"\n    with pytest.raises(ValueError):\n        _line_type(\"N O N NOON OON O\")\n    with pytest.raises(ValueError):\n        _line_type(\" some non-comment gibberish\")\n"], "sample_22": ["def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    m1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert_array_equal(matrix_transpose(m1), m1.T)\n\n    # and (M, 3, 3)\n    n1 = np.tile(m1, (2, 1, 1))\n    assert_array_equal(matrix_transpose(n1), n1.swapaxes(-2, -1))\n\n    # Check that it works with non-square matrices\n    m2 = np.array([[1, 2, 3], [4, 5, 6]])\n    assert_array_equal(matrix_transpose(m2), m2.T)\n", "def test_rotation_matrix_with_array_angles():\n    angles = np.array([30, 45, 60]) * u.deg\n    axis = \"x\"\n    matrices = rotation_matrix(angles, axis)\n    assert matrices.shape == (3, 3, 3)\n\n    for i, angle in enumerate(angles):\n        matrix = rotation_matrix(angle, axis)\n        assert_allclose(matrices[i], matrix)\n", "def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    # Create a 3x3 rotation matrix\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n\n    # Transpose it using the function\n    m1_transposed = matrix_transpose(m1)\n\n    # Compare with numpy's transpose function\n    assert_allclose(m1_transposed, m1.T)\n\n    # Test with a stack of matrices\n    n1 = np.tile(m1, (2, 1, 1))\n    n1_transposed = matrix_transpose(n1)\n    assert_allclose(n1_transposed, n1.swapaxes(-2, -1))\n", "def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    # Create a sample 3x3 matrix\n    m = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    # Transpose using the function and numpy's transpose function\n    m_transposed_func = matrix_transpose(m)\n    m_transposed_np = m.T\n\n    # Check if results are equal\n    assert_array_equal(m_transposed_func, m_transposed_np)\n\n    # Test for a stack of matrices\n    m_stack = np.stack((m, m))\n    m_transposed_func_stack = matrix_transpose(m_stack)\n    m_transposed_np_stack = m_stack.swapaxes(-2, -1)\n\n    # Check if results are equal\n    assert_array_equal(m_transposed_func_stack, m_transposed_np_stack)\n", "def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    # Create a 3x3 rotation matrix\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    \n    # Transpose the matrix using the function\n    m1_transposed = matrix_transpose(m1)\n    \n    # Check if the transposed matrix is equal to the original matrix transposed\n    assert_array_equal(m1_transposed, m1.T)\n    \n    # Create a stack of 2 matrices\n    n1 = np.stack((m1, m1))\n    \n    # Transpose the stack of matrices\n    n1_transposed = matrix_transpose(n1)\n    \n    # Check if the transposed stack of matrices is equal to the original stack transposed\n    assert_array_equal(n1_transposed, n1.swapaxes(-2, -1))\n"], "sample_23": ["def test_angle_formatting_unicode():\n    \"\"\"\n    Tests string formatting for Angle objects with unicode characters\n    \"\"\"\n\n    angle = Angle(\"54.12412\", unit=u.degree)\n\n    res = \"Angle as HMS: 3\u02b036\u1d5029.7888\u02e2\"\n    assert f\"Angle as HMS: {angle.to_string(unit=u.hour, sep=('\u02b0', '\u1d50', '\u02e2'), precision=4)}\" == res\n\n    res = \"Angle as DMS: 54\u00b007\u203226.832\u2033\"\n    assert f\"Angle as DMS: {angle.to_string(unit=u.deg, sep=('\u00b0', '\u2032', '\u2033'), precision=4)}\" == res\n\n    res = \"Angle as HMS: 3\u02e136\u1d5629.7888\u02e2\"\n    assert f\"Angle as HMS: {angle.to_string(unit=u.hour, sep=('\u02e1', '\u1d56', '\u02e2'), precision=4)}\" == res\n\n    res = \"Angle as DMS: 54\u02da07\u02dc26.832\u02dd\"\n    assert f\"Angle as DMS: {angle.to_string(unit=u.deg, sep=('\u02da', '\u02dc', '\u02dd'), precision=4)}\" == res\n", "def test_angle_to_string_format():\n    \"\"\"\n    Test the different format options for Angle.to_string.\n    \"\"\"\n    angle = Angle(\"12:34:56.789\", unit=u.hour)\n\n    # default format\n    assert angle.to_string() == \"12h34m56.789s\"\n\n    # sexagesimal with specified precision\n    assert angle.to_string(precision=3) == \"12h34m56.789s\"\n    assert angle.to_string(precision=2) == \"12h34m56.79s\"\n    assert angle.to_string(precision=1) == \"12h34m56.8s\"\n    assert angle.to_string(precision=0) == \"12h34m57s\"\n\n    # decimal format\n    assert angle.to_string(decimal=True) == \"12.582743056\"\n    assert angle.to_string(decimal=True, precision=5) == \"12.58274\"\n\n    # latex format\n    assert angle.to_string(format=\"latex\") == r\"12^{\\mathrm{h}}34^{\\mathrm{m}}56.789^{\\mathrm{s}}\"\n    assert angle.to_string(format=\"latex\", precision=2) == r\"12^{\\mathrm{h}}34^{\\mathrm{m}}56.79^{\\mathrm{s}}\"\n\n    # unicode format\n    assert angle.to_string(format=\"unicode\") == \"12\u02b034\u1d5056.789\u02e2\"\n    assert angle.to_string(format=\"unicode\", precision=2) == \"12\u02b034\u1d5056.79\u02e2\"\n\n    # with pad option\n    assert angle.to_string(pad=True) == \"12h34m56.789s\"\n    assert Angle(\"1:2:3\", unit=u.hour).to_string(pad=True) == \"01h02m03s\"\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the precision argument in Angle.to_string works correctly.\n    \"\"\"\n    angle = Angle(\"1d2m3.456789s\")\n    assert angle.to_string(precision=0) == \"1d02m03s\"\n    assert angle.to_string(precision=1) == \"1d02m03.5s\"\n    assert angle.to_string(precision=2) == \"1d02m03.46s\"\n    assert angle.to_string(precision=3) == \"1d02m03.457s\"\n    assert angle.to_string(precision=4) == \"1d02m03.4568s\"\n    assert angle.to_string(precision=5) == \"1d02m03.45679s\"\n", "def test_angle_to_string_with_format():\n    \"\"\"\n    Test that to_string works with different formats.\n\n    Regression test for gh-14034.\n    \"\"\"\n    angle = Angle(1.2345 * u.deg)\n    assert angle.to_string(format=\"latex\") == r\"$1^\\circ14{}^\\prime04.2{}^{\\prime\\prime}$\"\n    assert (\n        angle.to_string(format=\"latex\", sep=(\"d\", \"m\", \"s\"))\n        == r\"$1^\\circ14{}^\\primed{}04.2{}^\\second$\"\n    )\n    assert angle.to_string(format=\"unicode\") == \"1\u00b014\u203204.2\u2033\"\n    assert angle.to_string(format=\"unicode\", sep=(\"d\", \"m\", \"s\")) == \"1d14m04.2s\"\n    assert angle.to_string(format=None) == \"1d14m04.2s\"\n", "def test_angle_to_string_unicode():\n    \"\"\"\n    Test that the to_string method works correctly with Unicode characters.\n    \"\"\"\n    angle = Angle(12.5, u.deg)\n    assert angle.to_string(unit=u.degree, sep=\"\u00b0\", precision=1) == \"12\u00b030\u20320.0\u2033\"\n    assert angle.to_string(unit=u.hourangle, sep=\"\u02b0\u1d50\u02e2\", precision=1) == \"0\u02b050\u1d500.0\u02e2\"\n"], "sample_24": ["    def setup_class(self):\n        self.a = np.array([(1, 2), (3, 4)], dtype=[(\"x\", \"i4\"), (\"y\", \"i4\")])\n        self.mask_a = np.array([True, False])\n        self.ma = Masked(self.a, mask=self.mask_a)\n", "    def test_array_equal(self):\n        ma1 = Masked(np.array([(1, 2.0), (3, 4.0)], dtype=[(\"x\", int), (\"y\", float)]))\n        ma2 = Masked(\n            np.array([(1, 2.0), (3, 4.000001)], dtype=[(\"x\", int), (\"y\", float)])\n        )\n        assert not np.array_equal(ma1, ma2)\n", "    def setup_class(self):\n        self.a = np.array([(1, 2), (3, 4)], dtype=[(\"x\", int), (\"y\", int)])\n        self.mask_a = np.array([True, False])\n        self.ma = Masked(self.a, mask=self.mask_a)\n        self.b = np.array([(5, 6), (7, 8)], dtype=[(\"x\", int), (\"y\", int)])\n        self.mask_b = np.array([False, True])\n        self.mb = Masked(self.b, mask=self.mask_b)\n", "    def test_histogram(self):\n        o, e = np.histogram(self.ma)\n        assert type(o) is MaskedNDArray\n        assert_array_equal(o.unmasked, np.histogram(self.a)[0])\n        assert_array_equal(o.mask, np.zeros_like(o.unmasked))\n", "    def test_structured_to_unstructured(self):\n        dtype = np.array([(1.0, 2), (3.0, 4)], dtype=[(\"x\", float), (\"y\", int)])\n        ma = Masked(dtype, mask=[True, False])\n        out = np.lib.recfunctions.structured_to_unstructured(ma)\n        assert out.shape == (2, 2)\n        assert_array_equal(out.unmasked, [[1.0, 2], [3.0, 4]])\n        assert_array_equal(out.mask, [[True, True], [False, False]])\n"], "sample_25": ["def test_header_tostring_endcard(self):\n    header = fits.Header()\n    header[\"TEST\"] = \"value\"\n    header_str = header.tostring(endcard=True)\n    assert header_str.endswith(\"END\" + \" \" * 77 + \"\\n\")\n    header_str = header.tostring(endcard=False)\n    assert not header_str.endswith(\"END\" + \" \" * 77 + \"\\n\")\n", "def test_header_update_with_mapping(self):\n    \"\"\"\n    Test updating a header with a mapping.\n    \"\"\"\n\n    header = fits.Header([(\"A\", 1), (\"B\", 2)])\n    header.update({\"C\": 3, \"D\": 4})\n    assert list(header.keys()) == [\"A\", \"B\", \"C\", \"D\"]\n    assert header[\"C\"] == 3\n    assert header[\"D\"] == 4\n\n    # Test updating with an ordered dict to ensure order is preserved\n    from collections import OrderedDict\n\n    header = fits.Header([(\"A\", 1), (\"B\", 2)])\n    header.update(OrderedDict([(\"C\", 3), (\"D\", 4)]))\n    assert list(header.keys()) == [\"A\", \"B\", \"C\", \"D\"]\n    assert header[\"C\"] == 3\n    assert header[\"D\"] == 4\n", "def test_header_tostring(self):\n    \"\"\"\n    Test that Header.tostring() behaves correctly, including the case of long\n    string values that continue over multiple cards.\n    \"\"\"\n\n    header = fits.Header()\n    header[\"A\"] = \"B\"\n    header[\"C\"] = \"D\" * 70\n\n    # If we don't include the END card the header should still be parseable but\n    # should raise a warning if it's read back in\n    s = header.tostring(endcard=False)\n    assert len(s) == 160\n    assert s.count(\"CONTINUE\") == 2\n    with pytest.warns(UserWarning):\n        new_header = fits.Header.fromstring(s)\n\n    # The new header should still be equivalent to the original\n    assert new_header == header\n\n    # Now test including the END card\n    s = header.tostring(endcard=True)\n    assert len(s) == 160\n    assert s.endswith(\"END\" + \" \" * 77)\n    new_header = fits.Header.fromstring(s)\n    assert new_header == header\n\n    # Test without padding (which should also exclude the END card)\n    s = header.tostring(endcard=True, padding=False)\n    assert len(s) < 160\n    assert not s.endswith(\"END\" + \" \" * 77)\n    with pytest.warns(UserWarning):\n        new_header = fits.Header.fromstring(s)\n    assert new_header == header\n", "def test_card_with_colon_in_value(self):\n    \"\"\"Test that a card with a colon in its value is correctly parsed.\"\"\"\n    c = fits.Card.fromstring(\"KEYWORD= 'value:with:colon'\")\n    assert c.keyword == \"KEYWORD\"\n    assert c.value == \"value:with:colon\"\n    assert str(c) == _pad(\"KEYWORD= 'value:with:colon'\")\n", "def test_header_set_with_positional_args(self):\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n    header.set(\"E\", 1, \"Comment\", before=\"C\")\n    assert header[\"E\"] == 1\n    assert header.comments[\"E\"] == \"Comment\"\n    assert list(header) == [\"A\", \"E\", \"C\"]\n"], "sample_26": ["def test_image_hdu_data_assignment(tmp_path):\n    # Test assigning data to an existing ImageHDU\n\n    hdu = fits.ImageHDU()\n    assert hdu.data is None\n\n    # Assign a numpy array\n    arr = np.arange(10)\n    hdu.data = arr\n    assert_equal(hdu.data, arr)\n\n    # Assign a list that can be converted to a numpy array\n    lst = [1, 2, 3, 4, 5]\n    hdu.data = lst\n    assert_equal(hdu.data, np.array(lst))\n\n    # Assign something that can't be converted to a numpy array\n    with pytest.raises(TypeError):\n        hdu.data = \"hello\"\n\n    # Test assigning data with a different dtype\n    hdu = fits.ImageHDU(np.arange(10, dtype=np.int32))\n    hdu.data = np.arange(10, dtype=np.float64)\n    assert hdu.data.dtype == np.float64\n\n    # Test assigning data with a different shape\n    hdu = fits.ImageHDU(np.arange(10))\n    hdu.data = np.arange(20).reshape(2, 10)\n    assert hdu.data.shape == (2, 10)\n\n    # Test assigning data to a compressed HDU\n    chdu = fits.CompImageHDU()\n    arr = np.arange(10)\n    chdu.data = arr\n    assert_equal(chdu.data, arr)\n\n    # Test assigning data to a compressed HDU with a different dtype\n    chdu = fits.CompImageHDU(np.arange(10, dtype=np.int32))\n    chdu.data = np.arange(10, dtype=np.float64)\n    assert chdu.data.dtype == np.float64\n\n    # Test assigning data to a compressed HDU with a different shape\n    chdu = fits.CompImageHDU(np.arange(10))\n    chdu.data = np.arange(20).reshape(2, 10)\n    assert chdu.data.shape == (2, 10)\n", "def test_section_data_with_ellipsis(tmp_path):\n    \"\"\"\n    Test that using an ellipsis in the section data indexing works correctly.\n    \"\"\"\n    data = np.arange(100).reshape((10, 10))\n    hdu = fits.PrimaryHDU(data)\n    hdu.writeto(tmp_path / \"test.fits\")\n\n    with fits.open(tmp_path / \"test.fits\") as hdul:\n        assert np.array_equal(hdul[0].section[:, ...], data)\n        assert np.array_equal(hdul[0].section[..., :], data)\n        assert np.array_equal(hdul[0].section[1:3, ...], data[1:3, :])\n        assert np.array_equal(hdul[0].section[..., 1:3], data[:, 1:3])\n", "def test_image_read_with_memmap(tmp_path):\n    # Test reading image data with memmap=True\n    filename = tmp_path / \"test.fits\"\n    arr = np.arange(100).reshape(10, 10)\n    fits.PrimaryHDU(arr).writeto(filename)\n\n    with fits.open(filename, memmap=True) as hdul:\n        assert hdul[0].data.flags.writeable == False\n        assert np.all(hdul[0].data == arr)\n\n    # Test reading image data with memmap=False\n    with fits.open(filename, memmap=False) as hdul:\n        assert hdul[0].data.flags.writeable == True\n        assert np.all(hdul[0].data == arr)\n\n    # Test writing image data with memmap=True\n    with fits.open(filename, mode='update', memmap=True) as hdul:\n        hdul[0].data[:] = 1\n\n    with fits.open(filename) as hdul:\n        assert np.all(hdul[0].data == 1)\n", "def test_uint32(tmp_path):\n    \"\"\"Test for uint32 support\"\"\"\n    img = np.arange(0, 100, dtype=np.uint32).reshape(10, 10)\n    hdu = fits.PrimaryHDU(img)\n    hdu.writeto(tmp_path / \"uint32.fits\")\n\n    with fits.open(tmp_path / \"uint32.fits\") as hdul:\n        assert hdul[0].header[\"BITPIX\"] == 32\n        assert hdul[0].header[\"BZERO\"] == 2147483648\n        assert hdul[0].header[\"BSCALE\"] == 1.0\n        assert_equal(hdul[0].data, img)\n        assert hdul[0].data.dtype == img.dtype\n", "def test_image_hdu_data_setter_uint16(tmp_path):\n    # Test that setting data to uint16 on an ImageHDU works correctly\n    data = np.arange(100, dtype=np.uint16)\n    hdu = fits.ImageHDU()\n    hdu.data = data\n\n    filename = tmp_path / \"test.fits\"\n    hdu.writeto(filename)\n\n    with fits.open(filename) as hdul:\n        assert hdul[0].data.dtype == np.uint16\n        assert_equal(hdul[0].data, data)\n"], "sample_27": ["def test_fitsdiff_ignore_hdus(tmp_path):\n    \"\"\"Test FITSDiff with ignore_hdus option\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\")])\n    hdulist.writeto(path1)\n    hdulist[1].data[0] = 1\n    hdulist.writeto(path2)\n\n    # Ignore the SCI extension\n    diff = FITSDiff(path1, path2, ignore_hdus=[\"SCI\"])\n    assert diff.identical\n\n    # Ignore all extensions starting with S\n    diff = FITSDiff(path1, path2, ignore_hdus=[\"S*\"])\n    assert diff.identical\n", "def test_fitsdiff_extension_types(tmp_path):\n    \"\"\"Make sure diff report reports extension type differences\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist1 = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5))])\n    hdulist1.writeto(path1)\n\n    hdulist2 = HDUList([PrimaryHDU(), BinTableHDU.from_columns([Column(\"A\", format=\"I\", array=[1])])])\n    hdulist2.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension types differ:\" in diff.report()\n    assert \"a: IMAGE\" in diff.report()\n    assert \"b: BINTABLE\" in diff.report()\n", "def test_rawdatadiff_diff_with_atol(tmp_path):\n    \"\"\"Regression test for https://github.com/astropy/astropy/issues/13330\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n    a = np.zeros((10, 2), dtype=\"float32\")\n    a[:, 0] = np.arange(10, dtype=\"float32\") + 10\n    a[:, 1] = np.arange(10, dtype=\"float32\") + 20\n    b = a.copy()\n    changes = [(3, 13.1, 23.1), (8, 18.00001, 28.00001)]\n    for i, v, w in changes:\n        b[i, 0] = v\n        b[i, 1] = w\n\n    ca = Column(\"A\", format=\"20E\", array=[a])\n    cb = Column(\"A\", format=\"20E\", array=[b])\n    hdu_a = BinTableHDU.from_columns([ca])\n    hdu_a.writeto(path1, overwrite=True)\n    hdu_b = BinTableHDU.from_columns([cb])\n    hdu_b.writeto(path2, overwrite=True)\n    with fits.open(path1) as fits1:\n        with fits.open(path2) as fits2:\n            diff = FITSDiff(fits1, fits2, atol=0.001, rtol=0)\n            str1 = diff.report(fileobj=None, indent=0)\n\n            diff = FITSDiff(fits1, fits2, atol=0.01, rtol=0)\n            str2 = diff.report(fileobj=None, indent=0)\n\n    assert \"...and at 1 more indices.\" in str1\n    assert \"...and at 1 more indices.\" not in str2\n", "def test_diff_with_duplicate_columns(tmp_path):\n    \"\"\"Test diff with duplicate columns\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    c1 = Column(\"A\", format=\"L\", array=[True, False])\n    c2 = Column(\"B\", format=\"X\", array=[[0], [1]])\n    c3 = Column(\"C\", format=\"4I\", dim=\"(2, 2)\", array=[[0, 1, 2, 3], [4, 5, 6, 7]])\n    c4 = Column(\"A\", format=\"L\", array=[True, True])\n\n    ta = BinTableHDU.from_columns([c1, c2, c3])\n    tb = BinTableHDU.from_columns([c4, c2, c3])\n\n    hdula = HDUList([PrimaryHDU(), ta])\n    hdulb = HDUList([PrimaryHDU(), tb])\n\n    hdula.writeto(path1)\n    hdulb.writeto(path2)\n\n    with fits.open(path1) as fits1:\n        with fits.open(path2) as fits2:\n            diff = FITSDiff(fits1, fits2)\n            report = diff.report()\n\n    assert \"Inconsistent duplicates of keyword 'TFORM1'\" in report\n    assert \"Occurs 1 time(s) in a, 2 times in (b)\" in report\n", "def test_fitsdiff_nonstandard_extension(tmp_path):\n    \"\"\"Make sure diff report works for non-standard extension HDUs\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), DummyNonstandardExtHDU(data=np.zeros(5))])\n    hdulist.writeto(path1)\n    hdulist[1]._buffer = np.ones(5).tobytes()\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension HDU 1:\" in diff.report()\n    assert \"Data contains differences:\" in diff.report()\n"], "sample_28": ["def test_header_tostring_endcard(self):\n    \"\"\"Test that tostring(endcard=True) includes the END card.\"\"\"\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n    s = header.tostring(endcard=True)\n    assert s.endswith(_pad(\"END\"))\n", "def test_header_with_non_ascii_characters_in_value():\n    \"\"\"\n    Test creating a header with non-ASCII characters in the value.\n    \"\"\"\n\n    header = fits.Header()\n    pytest.raises(ValueError, header.set, \"TEST\", \"\\u30a8\\u30ea\\u30c3\\u30af\")\n", "def test_header_tostring(self):\n    \"\"\"Test that Header.tostring() returns the correct string representation.\"\"\"\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n    expected = _pad(\"A       =                    B\") + _pad(\"C       =                    D\")\n    assert header.tostring() == expected\n", "def test_header_eq_ne(self):\n    header1 = fits.Header([(\"A\", 1), (\"B\", 2)])\n    header2 = fits.Header([(\"A\", 1), (\"B\", 2)])\n    header3 = fits.Header([(\"A\", 1), (\"B\", 3)])\n\n    assert header1 == header2\n    assert header1 != header3\n\n    # Test that headers with different keyword order are considered equal\n    header4 = fits.Header([(\"B\", 2), (\"A\", 1)])\n    assert header1 == header4\n", "def test_header_fromfile_with_trailing_bytes(self):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/10037\n\n    This test checks that a FITS file with extra bytes at the end of the header\n    block is correctly parsed and reported.\n    \"\"\"\n    filename = self.temp(\"header.fits\")\n    hdu = fits.PrimaryHDU()\n    hdu.writeto(filename)\n\n    # Append some extra bytes to the end of the header block\n    with open(filename, \"ab\") as f:\n        f.write(b\"EXTRA BYTES\")\n\n    with pytest.warns(AstropyUserWarning) as w:\n        header = fits.Header.fromfile(filename)\n    assert len(w) == 1\n    assert \"Unexpected bytes trailing END keyword\" in str(w[0].message)\n"], "sample_29": ["def test_write_latex_kwargs_passed_through(self, write, tmp_path):\n    \"\"\"Test that kwargs are passed through to QTable.write\"\"\"\n    fp = tmp_path / \"test_write_latex_kwargs_passed_through.tex\"\n    with pytest.raises(TypeError, match=\"unexpected keyword argument\"):\n        write(fp, format=\"latex\", invalid_kwarg=True)\n", "def test_write_latex_format_mismatch(self, write, tmp_path):\n    \"\"\"Test passing a format that does not match 'latex'\"\"\"\n    fp = tmp_path / \"test_write_latex_format_mismatch.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex'\"):\n        write(fp, format=\"csv\")\n", "def test_write_latex_invalid_format(self, write, tmp_path):\n    \"\"\"Test passing an invalid format\"\"\"\n    fp = tmp_path / \"test_write_latex_invalid_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex', not\"):\n        write(fp, format=\"invalid\")\n", "def test_write_latex_kwargs_passed_through(self, write, tmp_path):\n    \"\"\"Test that kwargs are passed through to QTable.write\"\"\"\n    fp = tmp_path / \"test_write_latex_kwargs.tex\"\n    with pytest.warns(UserWarning, match=\"No meta supplied\"):\n        write(fp, format=\"latex\", include_names=[\"H0\", \"Om0\"])\n", "def test_write_latex_invalid_format(self, write, tmp_path):\n    \"\"\"Test passing an invalid format\"\"\"\n    fp = tmp_path / \"test_write_latex_invalid_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex'\"):\n        write(fp, format=\"invalid\")\n"], "sample_30": ["def test_votable_from_table():\n    from astropy.table import Table\n\n    table = Table(\n        [[1, 2, 3], [4.0, 5.0, 6.0], [\"a\", \"b\", \"c\"]],\n        names=[\"col1\", \"col2\", \"col3\"],\n        dtype=[\"i4\", \"f8\", \"U1\"],\n    )\n    votable = tree.VOTableFile.from_table(table)\n    assert len(votable.resources) == 1\n    assert len(votable.resources[0].tables) == 1\n    fields = votable.resources[0].tables[0].fields\n    assert len(fields) == 3\n    assert fields[0].name == \"col1\"\n    assert fields[0].datatype == \"int\"\n    assert fields[1].name == \"col2\"\n    assert fields[1].datatype == \"double\"\n    assert fields[2].name == \"col3\"\n    assert fields[2].datatype == \"char\"\n", "def test_get_field_by_id():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n          <RESOURCE type=\"results\">\n            <TABLE>\n              <FIELD ID=\"col1\" datatype=\"int\"/>\n              <FIELD ID=\"col2\" datatype=\"float\"/>\n            </TABLE>\n          </RESOURCE>\n        </VOTABLE>\"\"\"\n        )\n    )\n    field = vot.get_field_by_id(\"col1\")\n    assert field.datatype == \"int\"\n\n    with pytest.raises(KeyError):\n        vot.get_field_by_id(\"nonexistent\")\n", "def test_get_links_by_id():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n          <RESOURCE type=\"results\">\n            <TABLE ID=\"table-1\">\n              <FIELD ID=\"field-1\" datatype=\"char\" name=\"name\"/>\n              <LINK ID=\"link-1\" content-role=\"doc\" href=\"https://example.com/link-1\"/>\n              <LINK ID=\"link-2\" content-role=\"doc\" href=\"https://example.com/link-2\"/>\n            </TABLE>\n          </RESOURCE>\n        </VOTABLE>\"\"\"\n        )\n    )\n    table = vot.get_table_by_id(\"table-1\")\n    links = [l.href for l in table.links]\n    assert links == [\"https://example.com/link-1\", \"https://example.com/link-2\"]\n", "def test_get_table_by_index():\n    votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    table = votable.get_table_by_index(0)\n    assert isinstance(table, tree.Table)\n    assert len(table.array) == 5\n\n    with pytest.raises(IndexError):\n        votable.get_table_by_index(10)\n", "def test_get_table_by_index():\n    votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    assert votable.get_table_by_index(0) == votable.get_first_table()\n    with pytest.raises(IndexError):\n        votable.get_table_by_index(3)\n"], "sample_31": ["def test_write_latex_parameter_units(self, write, tmp_path, format):\n    \"\"\"Test that parameter units are correctly applied.\"\"\"\n    fp = tmp_path / \"test_write_latex_parameter_units.tex\"\n    write(fp, format=format)\n    tbl = QTable.read(fp)\n    # Get a Parameter from the Cosmology class\n    param = next(\n        param\n        for param in type(self.cosmo).__parameters__\n        if getattr(type(self.cosmo), param).unit is not None\n    )\n    assert tbl[param].unit == getattr(type(self.cosmo), param).unit\n", "def test_write_latex_cosmology_in_meta(self, write, tmp_path, format):\n    \"\"\"Test that cosmology metadata is not stored in the latex table\"\"\"\n    fp = tmp_path / \"test_write_latex_cosmology_in_meta.tex\"\n    write(fp, format=format)\n    tbl = QTable.read(fp)\n    assert \"cosmology\" not in tbl.meta\n", "def test_write_latex_parameters_units(self, write, tmp_path, format):\n    \"\"\"Test writing cosmology parameters with units\"\"\"\n    fp = tmp_path / \"test_write_latex_parameters_units.tex\"\n    write(fp, format=format)\n    tbl = QTable.read(fp)\n    # Check that the units are correctly written\n    for column_name in tbl.colnames[2:]:\n        param_name = [k for k, v in _FORMAT_TABLE.items() if v == column_name]\n        if param_name:\n            param_name = param_name[0]\n            param = getattr(type(self.cosmo), param_name, None)\n            if isinstance(param, Parameter) and param.unit not in (None, u.one):\n                assert tbl[column_name].unit == param.unit\n", "    def test_write_latex_cosmology_in_meta(self, write, tmp_path, format):\n        \"\"\"Test writing cosmology with metadata\"\"\"\n        fp = tmp_path / \"test_write_latex_cosmology_in_meta.tex\"\n        write(fp, format=format, cosmology_in_meta=True)\n        tbl = QTable.read(fp)\n        # asserts metadata contains cosmology class and name\n        assert \"cosmology\" in tbl.meta\n", "    def test_write_latex_cosmology_in_meta(self, write, tmp_path, format):\n        \"\"\"Test that cosmology metadata is not written\"\"\"\n        fp = tmp_path / \"test_write_latex_cosmology_in_meta.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        assert \"cosmology\" not in tbl.meta\n"], "sample_32": ["def test_w0wzCDM_H0_is_scalar():\n    \"\"\"Test that H0 is a scalar Quantity.\"\"\"\n    cosmo = w0wzCDM(H0=70 * u.km / u.s / u.Mpc, Om0=0.3, Ode0=0.7, w0=-1, wz=0.5)\n    assert isinstance(cosmo.H0, u.Quantity)\n    assert cosmo.H0.isscalar\n", "def test_w0wzcdm_distance_modulus():\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.distance_modulus`.\"\"\"\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1, wz=0.5)\n    z = np.array([0.1, 0.2, 0.5, 1.5, 2.5])\n\n    dm = cosmo.distance_modulus(z)\n    dm_expected = np.array([38.73568159, 40.17587055, 42.98769879, 46.61731544, 49.20478066])\n    assert u.allclose(dm.value, dm_expected, rtol=1e-4)\n", "def test_w0wzcdm_de_density_scale_flat_vs_nonflat():\n    \"\"\"Test that flat and non-flat w0wzCDM models with same parameters give the same de_density_scale.\"\"\"\n    # Create a flat and non-flat w0wzCDM model with same parameters\n    flat_cosmo = Flatw0wzCDM(H0=70, Om0=0.3, w0=-1, wz=0.5)\n    nonflat_cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1, wz=0.5)\n\n    # Test that de_density_scale is the same for both models at various redshifts\n    z = np.array([0.1, 0.2, 0.5, 1.5, 2.5])\n    assert u.allclose(\n        flat_cosmo.de_density_scale(z), nonflat_cosmo.de_density_scale(z), rtol=1e-4\n    )\n", "def test_w0wzcdm_de_density_scale(z):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.50, w0=-1, wz=0.5)\n\n    assert u.allclose(\n        cosmo.de_density_scale(z),\n        (1 + z) ** (3 * (1 + cosmo.w0 - cosmo.wz)) * np.exp(3 * cosmo.wz * z),\n        rtol=1e-4,\n    )\n", "def test_w0wzcdm_inv_efuncs():\n    \"\"\"Test inverse efunc scalar functions for w0wzCDM.\"\"\"\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1, wz=0.5)\n    flatcosmo = Flatw0wzCDM(H0=70, Om0=0.3, w0=-1, wz=0.5)\n\n    z = np.array([0.1, 0.2, 0.5, 1.5, 2.5])\n\n    # Test the three different functions (massive/massless neutrinos and no relativistic species)\n    for cosmo_obj, func in [\n        (cosmo, scalar_inv_efuncs.w0wzcdm_inv_efunc),\n        (flatcosmo, scalar_inv_efuncs.fw0wzcdm_inv_efunc),\n        (cosmo, scalar_inv_efuncs.w0wzcdm_inv_efunc_nomnu),\n        (flatcosmo, scalar_inv_efuncs.fw0wzcdm_inv_efunc_nomnu),\n        (cosmo, scalar_inv_efuncs.w0wzcdm_inv_efunc_norel),\n        (flatcosmo, scalar_inv_efuncs.fw0wzcdm_inv_efunc_norel),\n    ]:\n        # Get the correct arguments\n        args = cosmo_obj._inv_efunc_scalar_args\n\n        # Compute with the scalar function and the object method\n        x = func(z, *args)\n        y = cosmo_obj._inv_efunc(z)\n\n        # Compare\n        assert u.allclose(x, y, atol=1e-4)\n"], "sample_33": ["def test_indent():\n    text = \"Hello\\nWorld\"\n    assert misc.indent(text, shift=1, width=4) == \"    Hello\\n    World\\n\"\n    assert misc.indent(text, shift=2, width=4) == \"        Hello\\n        World\\n\"\n    assert misc.indent(text, shift=1, width=8) == \"        Hello\\n        World\\n\"\n", "def test_indent():\n    s = \"Hello\\nWorld\"\n    assert misc.indent(s, shift=1, width=4) == \"    Hello\\n    World\\n\"\n    assert misc.indent(s, shift=2, width=4) == \"        Hello\\n        World\\n\"\n    assert misc.indent(s, shift=1, width=2) == \"  Hello\\n  World\\n\"\n", "def test_indent():\n    text = \"This is a line of text.\\nAnd this is another.\"\n    indented_text = misc.indent(text, shift=1, width=4)\n    expected_text = \"    This is a line of text.\\n    And this is another.\"\n    assert indented_text == expected_text\n\n    indented_text = misc.indent(text, shift=2, width=4)\n    expected_text = \"        This is a line of text.\\n        And this is another.\"\n    assert indented_text == expected_text\n", "def test_indent():\n    s = \"This is a\\nmultiline string\"\n    indented_s = misc.indent(s, shift=1, width=4)\n    expected_s = \"    This is a\\n    multiline string\"\n    assert indented_s == expected_s\n\n    indented_s = misc.indent(s, shift=2, width=4)\n    expected_s = \"        This is a\\n        multiline string\"\n    assert indented_s == expected_s\n\n    s = \"This is a string\"\n    indented_s = misc.indent(s, shift=1, width=4)\n    expected_s = \"    This is a string\"\n    assert indented_s == expected_s\n", "def test_indent():\n    text = \"This is a block of text.\\n\\nWith multiple lines.\"\n    indented_text = misc.indent(text)\n    expected_text = \"    This is a block of text.\\n\\n    With multiple lines.\"\n    assert indented_text == expected_text\n\n    indented_text = misc.indent(text, shift=2, width=4)\n    expected_text = \"        This is a block of text.\\n\\n        With multiple lines.\"\n    assert indented_text == expected_text\n"], "sample_34": ["def test_find_equivalent_units_with_prefix():\n    assert u.km in u.m.find_equivalent_units(include_prefix_units=True)\n    assert u.km not in u.m.find_equivalent_units()\n", "def test_add_enabled_units():\n    with u.add_enabled_units([u.lyr]):\n        assert u.lyr in u.get_current_unit_registry().all_units\n    assert u.lyr not in u.get_current_unit_registry().all_units\n\n    u.add_enabled_units([u.lyr])\n    assert u.lyr in u.get_current_unit_registry().all_units\n", "def test_unit_scale_roundtrip():\n    # Test that creating a unit with a scale and then converting back to the original unit works correctly.\n    scaled_unit = u.Unit(10 * u.m)\n    assert scaled_unit.to(u.m, 1) == 10\n", "def test_hashing():\n    # Ensure that identical units have the same hash value\n    assert hash(u.m) == hash(u.m)\n\n    # Ensure that different units have different hash values\n    assert hash(u.m) != hash(u.s)\n", "def test_set_enabled_units_context():\n    with u.set_enabled_units([u.pc]):\n        assert len(u.m.find_equivalent_units()) == 1\n\n    assert len(u.m.find_equivalent_units()) > 1\n"], "sample_35": ["def test_isinstancemethod():\n    class MyClass:\n            pass\n\n        @classmethod\n            pass\n\n        @staticmethod\n            pass\n\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n\n    # Test with metaclass\n    class Meta(type):\n            return type.__new__(meta, name, bases, namespace)\n\n    class MyClassWithMeta(metaclass=Meta):\n            pass\n\n    assert isinstancemethod(MyClassWithMeta, MyClassWithMeta.an_instancemethod)\n", "def test_isinstancemethod():\n    class TestClass:\n            pass\n\n        @classmethod\n            pass\n\n        @staticmethod\n            pass\n\n    assert isinstancemethod(TestClass, TestClass.instance_method)\n    assert not isinstancemethod(TestClass, TestClass.class_method)\n    assert not isinstancemethod(TestClass, TestClass.static_method)\n\n    # Test with a subclass to ensure it works with inherited methods\n    class Subclass(TestClass):\n        pass\n\n    assert isinstancemethod(Subclass, Subclass.instance_method)\n    assert not isinstancemethod(Subclass, Subclass.class_method)\n    assert not isinstancemethod(Subclass, Subclass.static_method)\n", "def test_isinstancemethod():\n    class MyClass:\n            pass\n\n        @classmethod\n            pass\n\n        @staticmethod\n            pass\n\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n\n    # Test that _isinstancemethod works correctly even if the method is not in\n    # the immediate class's __dict__\n    class MySubclass(MyClass):\n        pass\n\n    assert isinstancemethod(MySubclass, MySubclass.an_instancemethod)\n    assert not isinstancemethod(MySubclass, MySubclass.a_classmethod)\n    assert not isinstancemethod(MySubclass, MySubclass.a_staticmethod)\n", "def test_isinstancemethod():\n    class MyClass:\n\n        @classmethod\n\n        @staticmethod\n\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n\n    # Test with inheritance\n    class MySubclass(MyClass):\n        pass\n\n    assert isinstancemethod(MySubclass, MySubclass.an_instancemethod)\n    assert not isinstancemethod(MySubclass, MySubclass.a_classmethod)\n    assert not isinstancemethod(MySubclass, MySubclass.a_staticmethod)\n", "def test_isinstancemethod():\n    class MyClass:\n            pass\n\n        @classmethod\n            pass\n\n        @staticmethod\n            pass\n\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n\n    with pytest.raises(AttributeError):\n        isinstancemethod(MyClass, 'nonexistent_method')\n"], "sample_36": ["def test_biweight_location_M_axis():\n    \"\"\"Test that the M argument works correctly with axis keyword.\"\"\"\n    with NumpyRNGContext(12345):\n        ny = 100\n        nx = 200\n        data = normal(5, 2, (ny, nx))\n\n        # Use the median as the location estimate along each axis\n        M = np.median(data, axis=0)\n        bw = biweight_location(data, M=M, axis=0)\n        bwi = []\n        for i in range(nx):\n            bwi.append(biweight_location(data[:, i], M=M[i]))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n\n        M = np.median(data, axis=1)\n        bw = biweight_location(data, M=M, axis=1)\n        bwi = []\n        for i in range(ny):\n            bwi.append(biweight_location(data[i, :], M=M[i]))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n", "def test_biweight_midcorrelation_M():\n    x = np.array([0, 1, 2])\n    y = np.array([2, 1, 0])\n    M = 1.0\n    assert_allclose(biweight_midcorrelation(x, y, M=M),\n                    biweight_midcorrelation(x - M, y - M))\n", "def test_biweight_midcorrelation_constant():\n    x = np.ones(10)\n    y = np.ones(10)\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n\n    x = np.ones(10)\n    y = np.full(10, 5)\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n", "def test_biweight_location_M_not_none():\n    \"\"\"Test biweight_location with M not None.\"\"\"\n    with NumpyRNGContext(12345):\n        randvar = randn(10000)\n        cbl = biweight_location(randvar, M=0.5)\n        assert abs(cbl - 0.5) < 1e-2\n\n    # Test with an array for M\n    data = np.arange(50).reshape(10, 5)\n    M = np.median(data, axis=1)\n    cbl = biweight_location(data, axis=1, M=M)\n    assert_allclose(cbl, M)\n", "def test_biweight_midcorrelation_edge_cases():\n    x = [1, 1, 1]\n    y = [2, 2, 2]\n    assert_allclose(biweight_midcorrelation(x, y), np.nan)\n\n    x = [1, 2, 3]\n    y = [4, 5, 6]\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n\n    x = [1, 2, 3]\n    y = [3, 2, 1]\n    assert_allclose(biweight_midcorrelation(x, y), -1.0)\n"], "sample_37": ["def test_axis_type_names():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cname = [\"Longitude\", \"Latitude\"]\n\n    assert w.axis_type_names == [\"Longitude\", \"Latitude\"]\n\n    w.wcs.cname = [\"\", \"\"]\n\n    assert w.axis_type_names == [\"RA\", \"DEC\"]\n", "def test_wcs_get_axis_types():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.cunit = ['deg', 'deg', 'Hz']\n    w.wcs.crpix = [1, 1, 1]\n    result = w.get_axis_types()\n    assert len(result) == 3\n    assert result[0]['coordinate_type'] == 'celestial'\n    assert result[0]['scale'] == 'non-linear celestial'\n    assert result[0]['number'] == 0\n    assert result[1]['coordinate_type'] == 'celestial'\n    assert result[1]['scale'] == 'non-linear celestial'\n    assert result[1]['number'] == 1\n    assert result[2]['coordinate_type'] == 'spectral'\n    assert result[2]['scale'] == 'linear'\n    assert result[2]['number'] == 0\n", "def test_wcs_reorient_celestial_first():\n    # Create a WCS object with spectral and celestial axes\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'WAVE']\n    w.wcs.crval = [1, 2, 3]\n    w.wcs.crpix = [4, 5, 6]\n    w.wcs.cdelt = [7, 8, 9]\n\n    # Reorient the WCS to have celestial axes first\n    w_reoriented = w.reorient_celestial_first()\n\n    # Check that the reoriented WCS has the correct axis order\n    assert w_reoriented.wcs.ctype == ['RA---TAN', 'DEC--TAN', 'WAVE']\n\n    # Check that the other WCS attributes are preserved\n    assert_array_equal(w_reoriented.wcs.crval, w.wcs.crval)\n    assert_array_equal(w_reoriented.wcs.crpix, w.wcs.crpix)\n    assert_array_equal(w_reoriented.wcs.cdelt, w.wcs.cdelt)\n", "def test_to_fits_2():\n    \"\"\"\n    Test to_fits() with SIP distortion.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    wfits = w.to_fits()\n    assert isinstance(wfits, fits.HDUList)\n    assert isinstance(wfits[0], fits.PrimaryHDU)\n    assert 'CTYPE1' in wfits[0].header\n    assert 'CTYPE2' in wfits[0].header\n    assert 'A_0_2' in wfits[0].header\n    assert 'B_0_2' in wfits[0].header\n", "def test_get_axis_types():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n    result = w.get_axis_types()\n    assert len(result) == 2\n    assert result[0]['coordinate_type'] == 'celestial'\n    assert result[0]['scale'] == 'linear'\n    assert result[0]['group'] == 0\n    assert result[0]['number'] == 0\n    assert result[1]['coordinate_type'] == 'celestial'\n    assert result[1]['scale'] == 'linear'\n    assert result[1]['group'] == 0\n    assert result[1]['number'] == 1\n"], "sample_38": ["def test_reorient_celestial_first():\n    \"\"\"\n    Test the reorient_celestial_first method.\n    \"\"\"\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'SPECTRAL']\n    w.reorient_celestial_first()\n    assert_array_equal(w.wcs.ctype, ['RA---TAN', 'DEC--TAN', 'SPECTRAL'])\n\n    w.wcs.ctype = ['SPECTRAL', 'DEC--TAN', 'RA---TAN']\n    w.reorient_celestial_first()\n    assert_array_equal(w.wcs.ctype, ['RA---TAN', 'DEC--TAN', 'SPECTRAL'])\n\n    w.wcs.ctype = ['RA---TAN', 'SPECTRAL', 'DEC--TAN']\n    w.reorient_celestial_first()\n    assert_array_equal(w.wcs.ctype, ['RA---TAN', 'DEC--TAN', 'SPECTRAL'])\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    assert w.axis_type_names == ['RA', 'DEC']\n\n    w.wcs.ctype = [\"RA---TAN-SIP\", \"DEC--TAN-SIP\"]\n    assert w.axis_type_names == ['RA', 'DEC']\n\n    w.wcs.cname = [\"Longitude\", \"Latitude\"]\n    assert w.axis_type_names == ['Longitude', 'Latitude']\n", "def test_axis_types():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'FREQ']\n\n    types = w.get_axis_types()\n\n    assert len(types) == 3\n\n    assert types[0]['coordinate_type'] == 'celestial'\n    assert types[1]['coordinate_type'] == 'celestial'\n    assert types[2]['coordinate_type'] == 'spectral'\n\n    assert types[0]['scale'] == 'linear'\n    assert types[1]['scale'] == 'linear'\n    assert types[2]['scale'] == 'linear'\n\n    assert types[0]['group'] == 0\n    assert types[1]['group'] == 0\n    assert types[2]['group'] == 0\n\n    assert types[0]['number'] == 0\n    assert types[1]['number'] == 1\n    assert types[2]['number'] == 3\n", "def test_sip_with_no_crval():\n    \"\"\"\n    Test that when creating a WCS object using a header with SIP distortion,\n    but no CRVAL, an error is raised.\n    \"\"\"\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---TAN-SIP'\n    header['CTYPE2'] = 'DEC--TAN-SIP'\n    header['A_0_2'] = 1.0\n\n    with pytest.raises(ValueError):\n        wcs.WCS(header)\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cname = [\"RA\", \"\"]\n    assert w.axis_type_names == [\"RA\", \"DEC\"]\n"], "sample_39": ["def test_axis_type_names():\n    \"\"\"\n    Test WCS.axis_type_names property\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cname = [\"Longitude\", \"\"]\n    assert w.axis_type_names == [\"Longitude\", \"DEC--TAN\"]\n", "def test_axis_type_names():\n    \"\"\"\n    Test that the axis type names are correctly returned.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n    w.wcs.cname = ['', '']\n    assert w.axis_type_names == ['RA', 'DEC']\n\n    w.wcs.cname = ['Longitude', 'Latitude']\n    assert w.axis_type_names == ['Longitude', 'Latitude']\n", "def test_axis_type_names():\n    \"\"\"\n    Test axis type names for WCS object.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n    w.wcs.cname = ['', '']\n\n    assert w.axis_type_names == ['RA', 'DEC']\n\n    w.wcs.cname = ['Longitude', 'Latitude']\n\n    assert w.axis_type_names == ['Longitude', 'Latitude']\n", "def test_naxis_deprecation():\n    \"\"\"\n    Test the deprecation warning for WCS.naxis1 and WCS.naxis2\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w._naxis = [1000, 500]\n\n    with pytest.warns(AstropyDeprecationWarning) as record:\n        naxis1 = w.naxis1\n    assert naxis1 == 1000\n\n    with pytest.warns(AstropyDeprecationWarning) as record:\n        naxis2 = w.naxis2\n    assert naxis2 == 500\n\n    with pytest.warns(AstropyDeprecationWarning) as record:\n        w.naxis1 = 2000\n    assert w._naxis[0] == 2000\n\n    with pytest.warns(AstropyDeprecationWarning) as record:\n        w.naxis2 = 600\n    assert w._naxis[1] == 600\n", "def test_has_celestial():\n    \"\"\"\n    Test WCS.has_celestial property\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    assert w.has_celestial\n\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"SPATIAL\"]\n    assert not w.has_celestial\n"], "sample_40": ["def test_with_H0_invalid():\n    H0_bad = 70*u.Jy\n    h100dist = 100 * u.Mpc/u.littleh\n\n    with pytest.raises(u.UnitsError):\n        h100dist.to(u.Mpc, u.with_H0(H0_bad))\n", "def test_with_H0():\n    H0_70 = 70*u.km/u.s/u.Mpc\n    h100dist = 100 * u.Mpc/u.littleh\n\n    assert_quantity_allclose(h100dist.to(u.Mpc, u.with_H0(H0_70)), 70*u.Mpc)\n\n    # make sure using the default cosmology works\n    H0_default_cosmo = cosmology.default_cosmology.get().H0\n    assert_quantity_allclose(h100dist.to(u.Mpc, u.with_H0()),\n                             H0_default_cosmo.value*u.Mpc/100)\n\n    # Now try a luminosity scaling\n    h1lum = 1 * u.Lsun * u.littleh**-2\n    assert_quantity_allclose(h1lum.to(u.Lsun, u.with_H0(H0_70)), (70/100)**2*u.Lsun)\n\n    # And the trickiest one: magnitudes.  Using H0=10 here for the round numbers\n    H0_10 = 10*u.km/u.s/u.Mpc\n    # assume the \"true\" magnitude M = 12.\n    # Then M - 5*log_10(h)  = M + 5 = 17\n    withlittlehmag = 17 * (u.mag + u.MagUnit(u.littleh**2))\n    assert_quantity_allclose(withlittlehmag.to(u.mag, u.with_H0(H0_10)), 12*u.mag)\n", "def test_equivalency_context_manager_multiple():\n    base_registry = u.get_current_unit_registry()\n\n        return [(equiv[0], equiv[1]) for equiv in equivalencies]\n\n    tf_dimensionless_angles = just_to_from_units(u.dimensionless_angles())\n    tf_spectral = just_to_from_units(u.spectral())\n\n    with u.set_enabled_equivalencies([u.dimensionless_angles(), u.spectral()]):\n        new_registry = u.get_current_unit_registry()\n        assert (set(just_to_from_units(new_registry.equivalencies)) ==\n                set(tf_dimensionless_angles) | set(tf_spectral))\n        assert set(new_registry.all_units) == set(base_registry.all_units)\n\n    assert base_registry is u.get_current_unit_registry()\n", "def test_equivalency_registry():\n    # Test that we can register new equivalencies and that they are used\n        return x * 2\n\n    u.add_enabled_equivalencies([(u.m, u.cm, equivalency, lambda x: x / 2)])\n\n    assert u.m.to(u.cm) == 200\n\n    # Test that we can remove equivalencies\n    u.remove_enabled_equivalencies([(u.m, u.cm, equivalency, lambda x: x / 2)])\n\n    assert u.m.to(u.cm) == 100\n\n    # Test that adding an invalid equivalency raises an error\n    with pytest.raises(ValueError):\n        u.add_enabled_equivalencies([(u.m, u.cm, equivalency)])\n", "def test_with_H0_unit():\n    # Test that the with_H0 equivalency can handle different units for H0\n    H0_70 = 70*u.km/u.s/u.Mpc\n    H0_70_in_Mpc_per_Gyr = H0_70.to(u.Mpc/u.Gyr)\n    h100dist = 100 * u.Mpc/u.littleh\n\n    assert_quantity_allclose(h100dist.to(u.Mpc, u.with_H0(H0_70)),\n                             h100dist.to(u.Mpc, u.with_H0(H0_70_in_Mpc_per_Gyr)))\n"], "sample_41": ["def test_unit_decomposition_with_scale():\n    \"\"\"\n    Test that decomposition of a unit with a scale gives the correct result.\n\n    Regression test for https://github.com/astropy/astropy/issues/4413\n    \"\"\"\n    assert (u.Jy.decompose().scale * u.Jy.decompose().bases[0].scale *\n            u.Jy.decompose().bases[1].scale) == 1e-26\n", "def test_unit_get_units_with_same_physical_type():\n    # Test that get_units_with_same_physical_type returns a list of units with the same physical type\n    unit = u.m\n    units_with_same_type = unit._get_units_with_same_physical_type()\n    for u in units_with_same_type:\n        assert u.physical_type == unit.physical_type\n\n    # Test that the function also works with equivalencies\n    unit = u.Jy\n    units_with_same_type = unit._get_units_with_same_physical_type(equivalencies=u.spectral())\n    for u in units_with_same_type:\n        assert u.is_equivalent(unit, equivalencies=u.spectral())\n", "def test_unrecognized_unit_equality_with_none():\n    unit = u.Unit(\"FOO\", parse_strict='silent')\n    assert not (unit == None)  # noqa\n    assert unit != None  # noqa\n    assert unit not in (None, u.m)\n", "def test_decompose_with_invalid_bases():\n    with pytest.raises(u.UnitsError):\n        u.m.decompose(bases=[u.s])\n", "def test_get_current_unit_registry():\n    # Test that get_current_unit_registry() returns the current unit registry.\n    registry = u.get_current_unit_registry()\n    assert isinstance(registry, u._UnitRegistry)\n    assert registry is u.get_current_unit_registry()\n"], "sample_42": ["def test_with_H0_roundtrip():\n    H0_70 = 70*u.km/u.s/u.Mpc\n    h70dist = 100 * u.Mpc\n\n    roundtrip_dist = h70dist.to(u.Mpc/u.littleh, u.with_H0(H0_70)).to(u.Mpc, u.with_H0(H0_70))\n    assert_quantity_allclose(roundtrip_dist, h70dist)\n\n    h1lum = 1*u.Lsun\n    roundtrip_lum = h1lum.to(u.Lsun*u.littleh**-2, u.with_H0(H0_70)).to(u.Lsun, u.with_H0(H0_70))\n    assert_quantity_allclose(roundtrip_lum, h1lum)\n", "def test_equivalency_registry():\n    base_registry = u.get_current_unit_registry()\n    assert not base_registry.equivalencies\n\n    with u.set_enabled_equivalencies(u.dimensionless_angles()):\n        new_registry = u.get_current_unit_registry()\n        assert len(new_registry.equivalencies) == 1\n        assert (new_registry.equivalencies[0][0] == u.radian and\n                new_registry.equivalencies[0][1] is None)\n\n    with u.add_enabled_equivalencies(u.spectral()):\n        newer_registry = u.get_current_unit_registry()\n        assert len(newer_registry.equivalencies) == 2\n        assert (newer_registry.equivalencies[0][0] == u.radian and\n                newer_registry.equivalencies[0][1] is None)\n        assert (newer_registry.equivalencies[1][0] == u.m and\n                newer_registry.equivalencies[1][1] == u.Hz)\n\n    assert base_registry is u.get_current_unit_registry()\n", "def test_with_H0_roundtrip():\n    H0_70 = 70*u.km/u.s/u.Mpc\n    h70dist = 100 * u.Mpc\n\n    roundtripped_dist = h70dist.to(u.Mpc/u.littleh, u.with_H0(H0_70)).to(u.Mpc, u.with_H0(H0_70))\n    assert_quantity_allclose(roundtripped_dist, h70dist)\n", "def test_with_H0_context():\n    H0_70 = 70*u.km/u.s/u.Mpc\n    h70dist = 70 * u.Mpc/u.littleh\n\n    with u.set_enabled_equivalencies(u.with_H0(H0_70)):\n        assert_quantity_allclose(h70dist.to(u.Mpc), 100*u.Mpc)\n\n    # make sure using the default cosmology works\n    cosmodist = cosmology.default_cosmology.get().H0.value * u.Mpc/u.littleh\n    with u.set_enabled_equivalencies(u.with_H0()):\n        assert_quantity_allclose(cosmodist.to(u.Mpc), 100*u.Mpc)\n", "def test_logarithmic_quantity():\n    # check conversion of mag, dB, and dex to dimensionless and vice versa\n    # for quantities with non-trivial values\n\n    q_dex = np.array([0., -1., 1., 2.]) * u.dex\n    q_expected = 10.**q_dex.value * u.dimensionless_unscaled\n    q_log_unit = q_dex.to(u.mag)\n    assert np.all(q_log_unit.to(1, equivalencies=u.logarithmic()) ==\n                  q_expected)\n    assert np.all(q_expected.to(u.mag, equivalencies=u.logarithmic()) ==\n                  q_log_unit)\n\n    q_dex = np.array([0., -1., 1., 2.]) * u.dex\n    q_expected = 10.**q_dex.value * u.dimensionless_unscaled\n    q_log_unit = q_dex.to(u.dB)\n    assert np.all(q_log_unit.to(1, equivalencies=u.logarithmic()) ==\n                  q_expected)\n    assert np.all(q_expected.to(u.dB, equivalencies=u.logarithmic()) ==\n                  q_log_unit)\n"], "sample_43": ["def test_events_fitness_with_gamma(rseed=0):\n    rng = np.random.RandomState(rseed)\n    t = np.concatenate([rng.rand(100),\n                        1 + rng.rand(200)])\n\n    gamma_sel = 0.05\n    bins_gamma = bayesian_blocks(t, fitness='events', gamma=gamma_sel)\n\n    p0_sel = np.exp(-4 / gamma_sel) * (len(t) ** -0.478) / 73.53\n    bins_p0 = bayesian_blocks(t, fitness='events', p0=p0_sel)\n\n    ncp_prior_sel = -np.log(gamma_sel)\n    bins_ncp = bayesian_blocks(t, fitness='events', ncp_prior=ncp_prior_sel)\n\n    assert_allclose(bins_gamma, bins_p0)\n    assert_allclose(bins_gamma, bins_ncp)\n", "def test_bayesian_blocks_repeated_values():\n    # Test bayesian_blocks with repeated values in t when x is None\n    t = np.array([1, 2, 2, 3, 4, 4, 5])\n    edges = bayesian_blocks(t)\n    assert_allclose(edges, [1, 2, 3, 4, 5])\n\n    # Test bayesian_blocks with repeated values in t when x is specified\n    t = np.array([1, 2, 2, 3, 4, 4, 5])\n    x = np.array([1, 2, 2, 3, 4, 4, 5])\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, x)\n", "def test_events_fitness_gamma():\n    \"\"\"Test events fitness with specified gamma\"\"\"\n    rng = np.random.RandomState(42)\n    t = rng.randn(100)\n    edges_default = bayesian_blocks(t, fitness='events')\n\n    # specify gamma directly\n    gamma_sel = 0.05\n    edges_gamma = bayesian_blocks(t, fitness='events', gamma=gamma_sel)\n\n    # specify ncp_prior equivalent to the above gamma\n    ncp_prior_sel = -np.log(gamma_sel)\n    edges_ncp_prior = bayesian_blocks(t, fitness='events',\n                                      ncp_prior=ncp_prior_sel)\n\n    # all should be equal for this data set\n    assert_allclose(edges_default, edges_gamma)\n    assert_allclose(edges_default, edges_ncp_prior)\n", "def test_bayesian_blocks_invalid_fitness():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n\n    # invalid fitness string\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='invalid')\n\n    # invalid fitness class\n    class InvalidFitness:\n        pass\n\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness=InvalidFitness)\n\n    # fitness is not a class or string\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness=123)\n", "def test_events_fitness_function_edge_cases():\n    \"\"\"Test edge cases for Events fitness function\"\"\"\n    rng = np.random.RandomState(42)\n\n    # Empty input array\n    t = np.array([])\n    edges = bayesian_blocks(t, fitness='events')\n    assert_allclose(edges, [])\n\n    # Single-element input array\n    t = np.array([1])\n    edges = bayesian_blocks(t, fitness='events')\n    assert_allclose(edges, [1])\n\n    # Two-element input array\n    t = np.array([1, 2])\n    edges = bayesian_blocks(t, fitness='events')\n    assert_allclose(edges, [1, 2])\n\n    # Input array with negative values\n    t = rng.randn(100)\n    t[t < 0] *= -1\n    edges_pos = bayesian_blocks(t, fitness='events')\n    t[t > 0] *= -1\n    edges_neg = bayesian_blocks(t, fitness='events')\n    assert_allclose(edges_pos, edges_neg)\n"], "sample_44": ["    def setup(self):\n        self.mJy = np.arange(1., 5.).reshape(2, 2) * u.mag(u.Jy)\n        self.m1 = np.arange(1., 5.5, 0.5).reshape(3, 3) * u.mag()\n", "    def test_log_quantity_pickle(self):\n        lq = u.Magnitude(np.arange(1., 10.)*u.Jy)\n        s = pickle.dumps(lq)\n        lq2 = pickle.loads(s)\n        assert type(lq) is type(lq2)\n        assert lq.unit == lq2.unit\n        assert np.all(lq.value == lq2.value)\n", "    def test_pickle(self):\n        lq1 = u.Magnitude(np.arange(1., 11.)*u.Jy)\n        s = pickle.dumps(lq1)\n        lq2 = pickle.loads(s)\n        assert type(lq2) is type(lq1)\n        assert np.all(lq1 == lq2)\n", "def test_pickling_function_quantity():\n    \"\"\"Test that function quantities can be pickled and unpickled.\"\"\"\n    lq1 = u.Magnitude(np.arange(1., 10.)*u.Jy)\n    s = pickle.dumps(lq1)\n    lq2 = pickle.loads(s)\n    assert type(lq2) is type(lq1)\n    assert np.all(lq2.value == lq1.value)\n    assert lq2.unit == lq1.unit\n", "    def test_ufuncs(self, function):\n        \"\"\"Test that supported ufuncs work correctly.\"\"\"\n        lq = u.Magnitude(np.array([1., np.nan, np.inf]) * u.Jy)\n        res = function(lq)\n        assert np.all(res == function(lq._function_view))\n"], "sample_45": ["def test_trunc_func_with_timezone_transition(self):\n    \"\"\"\n    If the truncated datetime transitions to a different offset (daylight\n    saving) then the returned value will have that new timezone/offset.\n    \"\"\"\n    start_datetime = datetime(2022, 3, 13, 1, 30, 0)\n    end_datetime = datetime(2022, 11, 6, 1, 30, 0)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    nyc = pytz.timezone('America/New_York')\n\n    model = DTModel.objects.annotate(\n        spring_truncated=TruncHour('start_datetime', tzinfo=nyc),\n        fall_truncated=TruncHour('end_datetime', tzinfo=nyc),\n    ).get()\n\n    self.assertEqual(model.spring_truncated.dst(), timedelta(0, 3600))\n    self.assertEqual(model.fall_truncated.dst(), timedelta(0))\n", "def test_trunc_func_explicit_timezone_priority(self):\n    start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n    end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    melb = pytz.timezone('Australia/Melbourne')\n\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            day_melb=TruncDay('start_datetime'),\n            day_utc=TruncDay('start_datetime', tzinfo=timezone.utc),\n        ).order_by('start_datetime').get()\n        self.assertEqual(model.day_melb, truncate_to(start_datetime.astimezone(melb), 'day'))\n        self.assertEqual(model.day_utc, truncate_to(start_datetime, 'day'))\n", "def test_trunc_func_with_timezone_transition(self):\n    \"\"\"\n    If the truncated datetime transitions to a different offset (daylight\n    saving) then the returned value will have that new timezone/offset.\n    \"\"\"\n    start_datetime = datetime(2022, 3, 13, 1, 30)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    self.create_model(start_datetime, None)\n\n    nyc = pytz.timezone('America/New_York')\n\n    qs = DTModel.objects.annotate(\n        truncated=Trunc('start_datetime', 'hour', output_field=DateTimeField(), tzinfo=nyc)\n    )\n    model = qs.get()\n\n    # The original datetime is in standard time, but the truncated datetime\n    # should be in daylight saving time.\n    self.assertEqual(model.start_datetime.tzinfo.utcoffset(model.start_datetime), timedelta(hours=-5))\n    self.assertEqual(model.truncated.tzinfo.utcoffset(model.truncated), timedelta(hours=-4))\n", "def test_trunc_timezone_explicit_priority(self):\n    start_datetime = datetime(2016, 1, 1, 1, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            melb_year=TruncYear('start_datetime'),\n            utc_year=TruncYear('start_datetime', tzinfo=pytz.UTC),\n        ).order_by('start_datetime').get()\n        self.assertEqual(model.melb_year, truncate_to(start_datetime, 'year', melb))\n        self.assertEqual(model.utc_year, truncate_to(start_datetime, 'year', pytz.UTC))\n", "def test_trunc_func_with_timezone_transition_crossing(self):\n    \"\"\"\n    If the truncated datetime transitions to a different offset (daylight\n    saving) then the returned value will have that new timezone/offset.\n    \"\"\"\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = sao.localize(datetime(2016, 10, 15, 23), is_dst=False)\n    self.create_model(start_datetime, None)\n\n    qs = DTModel.objects.annotate(\n        truncated=TruncHour('start_datetime', tzinfo=sao)\n    )\n    with self.assertRaisesMessage(pytz.NonExistentTimeError, '2016-10-16 00:00:00'):\n        model = qs.get()\n\n    qs = DTModel.objects.annotate(\n        truncated=TruncHour('start_datetime', tzinfo=sao, is_dst=True)\n    )\n    model = qs.get()\n    self.assertEqual(model.truncated.hour, 1)\n    self.assertEqual(model.truncated.dst(), timedelta(0, 3600))\n\n    qs = DTModel.objects.annotate(\n        truncated=TruncHour('start_datetime', tzinfo=sao, is_dst=False)\n    )\n    model = qs.get()\n    self.assertEqual(model.truncated.hour, 23)\n    self.assertEqual(model.truncated.dst(), timedelta(0))\n"], "sample_46": ["    def setUp(self):\n        self.obj1 = UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440000')\n        self.obj2 = UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440001')\n", "    def test_exact(self):\n        instance = UUIDModel.objects.create(field=uuid.uuid4())\n        self.assertSequenceEqual(\n            UUIDModel.objects.filter(field__exact=instance.field),\n            [instance]\n        )\n        self.assertSequenceEqual(\n            UUIDModel.objects.filter(field__exact=str(instance.field)),\n            [instance]\n        )\n", "    def setUpTestData(cls):\n        cls.objs = [\n            UUIDModel.objects.create(field=uuid.uuid4()),\n            UUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n            UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440000'),\n        ]\n", "    def setUpTestData(cls):\n        cls.obj = UUIDModel.objects.create(field=uuid.UUID('550e8400e29b41d4a716446655440000'))\n", "    def test_uuid_field_with_database_functions(self):\n        instance = UUIDModel.objects.create(field=uuid.UUID('550e8400-e29b-41d4-a716-446655440000'))\n        qs = UUIDModel.objects.annotate(\n            field_str=Func(F('field'), function='LOWER', output_field=CharField())\n        ).filter(field_str='550e8400-e29b-41d4-a716-446655440000')\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs.get().pk, instance.pk)\n"], "sample_47": ["    def test_get_safe_settings(self):\n        # Test that get_safe_settings returns a dictionary of settings with sensitive values cleansed\n        settings_dict = get_safe_settings()\n        self.assertIsInstance(settings_dict, dict)\n        for key, value in settings_dict.items():\n            if HIDDEN_SETTINGS.search(key):\n                self.assertEqual(value, CLEANSED_SUBSTITUTE)\n", "    def test_is_active(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertFalse(filter.is_active(request=None))\n        request = RequestFactory().get('/test_view/')\n        with override_settings(DEBUG=True):\n            self.assertFalse(filter.is_active(request))\n        with override_settings(DEBUG=False):\n            self.assertTrue(filter.is_active(request))\n", "    def test_get_cleansed_multivaluedict(self):\n        \"\"\"\n        Test that the get_cleansed_multivaluedict method correctly cleanses\n        sensitive POST parameters from a MultiValueDict.\n        \"\"\"\n        request = RequestFactory().post('/some_url/', {'password': 'super_secret'})\n        filter = SafeExceptionReporterFilter()\n        cleansed_dict = filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleansed_dict['password'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_dictionary_with_non_string_key(self):\n        initial = {42: 'foo', 'password': 'secret'}\n        expected = {42: 'foo', 'password': CLEANSED_SUBSTITUTE}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_callable(self):\n            return \"super_secret\"\n        self.assertEqual(cleanse_setting('SETTING_NAME', callable_setting), CallableSettingWrapper(callable_setting))\n"], "sample_48": ["def test_aggregate_with_filter(self):\n    max_price = Book.objects.aggregate(\n        max_price=Max('price', filter=Q(rating__gt=4.0))\n    )\n    self.assertEqual(max_price['max_price'], Decimal('82.80'))\n", "def test_aggregate_with_distinct_and_expression(self):\n    # Test aggregate with distinct and expression\n    books = Book.objects.annotate(\n        num_authors=Count('authors', distinct=True)\n    ).aggregate(Max('num_authors'))\n    self.assertEqual(books, {'num_authors__max': 3})\n\n    # Test aggregate with distinct and expression with filter\n    books = Book.objects.annotate(\n        num_authors=Count('authors', distinct=True)\n    ).filter(num_authors__gt=1).aggregate(Max('num_authors'))\n    self.assertEqual(books, {'num_authors__max': 3})\n", "def test_filter_on_annotation_with_values(self):\n    books = Book.objects.annotate(num_authors=Count(\"authors\")).filter(num_authors__gt=1).values('name')\n    self.assertEqual(list(books), [{'name': 'The Definitive Guide to Django: Web Development Done Right'}, \n                                   {'name': 'Python Web Development with Django'}, \n                                   {'name': 'Artificial Intelligence: A Modern Approach'}])\n", "def test_stddev_variance(self):\n    # Create some sample data.\n    values = [1, 2, 3, 4, 5]\n    Book.objects.all().delete()\n    for value in values:\n        Book.objects.create(\n            isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n            pages=value, rating=4.5, price=Decimal('30.00'), contact=self.a1, publisher=self.p1,\n            pubdate=datetime.date(2007, 12, 6)\n        )\n\n    # Calculate the population standard deviation and variance.\n    population_stddev = Decimal(sqrt(sum((x - mean(values)) ** 2 for x in values) / len(values)))\n    population_variance = sum((x - mean(values)) ** 2 for x in values) / len(values)\n\n    # Calculate the sample standard deviation and variance.\n    sample_stddev = Decimal(sqrt(sum((x - mean(values)) ** 2 for x in values) / (len(values) - 1)))\n    sample_variance = sum((x - mean(values)) ** 2 for x in values) / (len(values) - 1)\n\n    # Test that stddev and variance return the correct results.\n    queryset = Book.objects.aggregate(\n        stddev_pop=StdDev('pages', sample=False),\n        variance_pop=Variance('pages', sample=False),\n        stddev_sample=StdDev('pages'),\n        variance_sample=Variance('pages')\n    )\n    self.assertEqual(queryset['stddev_pop'], Approximate(population_stddev, places=6))\n    self.assertEqual(queryset['variance_pop'], Approximate(population_variance, places=6))\n    self.assertEqual(queryset['stddev_sample'], Approximate(sample_stddev, places=6))\n    self.assertEqual(queryset['variance_sample'], Approximate(sample_variance, places=6))\n\n    # Test that StdDev and Variance accept a numeric field.\n    queryset = Book.objects.aggregate(stddev=StdDev('pages', output_field=FloatField()))\n    self.assertIsInstance(queryset['stddev'], float)\n\n    queryset = Book.objects.aggregate(variance=Variance('pages', output_field=FloatField()))\n    self.assertIsInstance(queryset['variance'], float)\n", "def test_aggregate_with_distinct(self):\n    books = Book.objects.annotate(num_authors=Count(\"authors\", distinct=True)).order_by(\"name\")\n    self.assertQuerysetEqual(\n        books, [\n            ('Artificial Intelligence: A Modern Approach', 2),\n            ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 1),\n            ('Practical Django Projects', 1),\n            ('Python Web Development with Django', 3),\n            ('Sams Teach Yourself Django in 24 Hours', 1),\n            ('The Definitive Guide to Django: Web Development Done Right', 2)\n        ],\n        lambda b: (b.name, b.num_authors)\n    )\n\n    books = Book.objects.annotate(num_authors=Count(\"authors\")).order_by(\"name\")\n    self.assertQuerysetEqual(\n        books, [\n            ('Artificial Intelligence: A Modern Approach', 2),\n            ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 1),\n            ('Practical Django Projects', 1),\n            ('Python Web Development with Django', 3),\n            ('Sams Teach Yourself Django in 24 Hours', 1),\n            ('The Definitive Guide to Django: Web Development Done Right', 2)\n        ],\n        lambda b: (b.name, b.num_authors)\n    )\n"], "sample_49": ["def test_media_order_conflict_warning(self):\n    msg = 'Detected duplicate Media files in an opposite order:\\n/path/to/css1\\n/path/to/css2'\n    with self.assertWarnsMessage(RuntimeWarning, msg):\n        Media(css={'all': ('/path/to/css1', '/path/to/css2')}) + Media(css={'all': ('/path/to/css2', '/path/to/css1')})\n", "def test_media_order_conflict_warning(self):\n    class MyWidget1(TextInput):\n        class Media:\n            js = ('/path/to/js1', '/path/to/js2')\n\n    class MyWidget2(TextInput):\n        class Media:\n            js = ('/path/to/js2', '/path/to/js1')\n\n    msg = 'Detected duplicate Media files in an opposite order:\\n/path/to/js2\\n/path/to/js1'\n    with self.assertWarnsMessage(MediaOrderConflictWarning, msg):\n        MyWidget1().media + MyWidget2().media\n", "def test_media_order_conflict_warning(self):\n    widget1 = Media(js=['script1.js', 'script2.js'])\n    widget2 = Media(js=['script2.js', 'script1.js'])\n    with self.assertWarnsMessage(MediaOrderConflictWarning, 'Detected duplicate Media files in an opposite order'):\n        widget1 + widget2\n", "def test_media_order_conflict_warning(self):\n    class MyWidget1(TextInput):\n        class Media:\n            js = ('/path/to/js1', '/path/to/js2')\n\n    class MyWidget2(TextInput):\n        class Media:\n            js = ('/path/to/js2', '/path/to/js1')\n\n    with self.assertWarnsMessage(RuntimeWarning, 'Detected duplicate Media files in an opposite order:\\n/path/to/js1\\n/path/to/js2'):\n        MyWidget1().media + MyWidget2().media\n", "def test_media_addition_order(self):\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1',)\n\n    class MyWidget2(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css3', '/path/to/css4')\n            }\n            js = ('/path/to/js2',)\n\n    media1 = MyWidget1().media\n    media2 = MyWidget2().media\n    combined_media = media1 + media2\n\n    self.assertEqual(\n        str(combined_media),\n        \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_50": ["def test_empty_values(self):\n    self.assertEqual(\n        self._run_it({\n            'database': '',\n            'user': '',\n            'password': '',\n            'host': '',\n            'port': '',\n        }), (\n            ['psql', ''],\n            None,\n        )\n    )\n", "def test_empty_host_and_port(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n        }), (\n            ['psql', '-U', 'someuser', 'dbname'],\n            'somepassword',\n        )\n    )\n\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': '',\n            'port': '',\n        }), (\n            ['psql', '-U', 'someuser', 'dbname'],\n            'somepassword',\n        )\n    )\n", "def test_empty_host_and_port(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': '',\n            'port': '',\n        }), (\n            ['psql', '-U', 'someuser', 'dbname'],\n            'somepassword',\n        )\n    )\n", "def test_runshell(self):\n    with mock.patch.object(DatabaseClient, 'runshell_db') as mock_runshell_db:\n        db_client = DatabaseClient(None)\n        db_client.connection = mock.Mock()\n        db_client.connection.get_connection_params.return_value = {\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        }\n        db_client.runshell()\n        mock_runshell_db.assert_called_once_with({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        })\n", "def test_empty_host_and_port(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n        }), (\n            ['psql', '-U', 'someuser', 'dbname'],\n            'somepassword',\n        )\n    )\n"], "sample_51": ["def test_parse_duration_edge_cases(self):\n    # Edge cases\n    self.assertIsNone(parse_duration(''))\n    self.assertIsNone(parse_duration(' '))\n    self.assertIsNone(parse_duration(':'))\n    self.assertIsNone(parse_duration('::'))\n    self.assertIsNone(parse_duration(':::'))\n    self.assertIsNone(parse_duration('-'))\n    self.assertIsNone(parse_duration('+'))\n    self.assertIsNone(parse_duration('P'))  # ISO 8601 prefix without duration\n    self.assertIsNone(parse_duration('PT'))  # ISO 8601 prefix without duration\n    self.assertIsNone(parse_duration('T'))  # Time part of ISO 8601 without prefix\n    with self.assertRaises(ValueError):\n        parse_duration('4 days invalid')\n    with self.assertRaises(ValueError):\n        parse_duration('4 invalid')\n", "def test_parse_duration_edge_cases(self):\n    # Test parsing of edge case durations\n    test_values = (\n        ('0', timedelta(0)),\n        ('-0', timedelta(0)),\n        ('0:00', timedelta(0)),\n        ('-0:00', timedelta(0)),\n        ('0:00:00', timedelta(0)),\n        ('-0:00:00', timedelta(0)),\n        ('0.000001', timedelta(microseconds=1)),\n        ('-0.000001', timedelta(microseconds=-1)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_duration_invalid_inputs(self):\n    # Invalid inputs\n    invalid_inputs = (\n        'abc',\n        '4 days 15:30:aa',\n        '4 days 15:aa:30',\n        '4 days aa:15:30',\n        '4 aa 15:30:30',\n        'P',\n        'PT',\n        'P4Y2M',\n        'P4YT5H',\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "def test_parse_duration_edge_cases(self):\n    # Test edge cases for parse_duration function\n    test_values = (\n        ('', None),\n        (' ', None),\n        ('P', None),\n        ('PT', None),\n        ('P1', None),\n        ('PT1', None),\n        ('-P4D', timedelta(days=-4)),\n        ('-PT5H', timedelta(hours=-5)),\n        ('-PT5M', timedelta(minutes=-5)),\n        ('-PT5S', timedelta(seconds=-5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso_8601_combined(self):\n    test_values = (\n        ('P4DT5H', timedelta(days=4, hours=5)),\n        ('P4DT5M', timedelta(days=4, minutes=5)),\n        ('P4DT5S', timedelta(days=4, seconds=5)),\n        ('PT5H10M', timedelta(hours=5, minutes=10)),\n        ('PT5H10S', timedelta(hours=5, seconds=10)),\n        ('PT5M10S', timedelta(minutes=5, seconds=10)),\n        ('P4DT5H10M', timedelta(days=4, hours=5, minutes=10)),\n        ('P4DT5H10S', timedelta(days=4, hours=5, seconds=10)),\n        ('P4DT5M10S', timedelta(days=4, minutes=5, seconds=10)),\n        ('PT5H10M10S', timedelta(hours=5, minutes=10, seconds=10)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n"], "sample_52": ["def test_queryset_from_manager(self):\n    class CustomManager(models.Manager):\n            return super().get_queryset().filter(slug='test')\n\n    Category.objects.create(name='Filtered', slug='filtered')\n    Category.objects.create(name='Another', slug='another')\n\n    f = forms.ModelChoiceField(Category.objects)\n    self.assertEqual(len(f.choices), 6)\n\n    Category.custom_manager = CustomManager()\n    f = forms.ModelChoiceField(Category.custom_manager)\n    self.assertEqual(len(f.choices), 2)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n", "def test_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name__startswith': 'A'})\n    self.assertEqual(len(f.choices), 2)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=lambda: {'name__startswith': 'A'})\n    self.assertEqual(len(f.choices), 2)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n", "def test_modelform_defines_fields(self):\n    class CategoryForm(forms.ModelForm):\n        pass\n\n    self.assertIs(modelform_defines_fields(CategoryForm), False)\n\n    class CategoryForm(forms.ModelForm):\n        class Meta:\n            model = Category\n            fields = ('name', 'slug')\n\n    self.assertIs(modelform_defines_fields(CategoryForm), True)\n\n    class CategoryForm(forms.ModelForm):\n        class Meta:\n            model = Category\n            exclude = ('url',)\n\n    self.assertIs(modelform_defines_fields(CategoryForm), True)\n", "def test_queryset_values(self):\n    f = forms.ModelChoiceField(Category.objects.values_list('id', 'name'))\n    with self.assertRaises(ValueError):\n        f.choices\n", "def test_label_from_instance(self):\n    class CustomModelChoiceField(forms.ModelChoiceField):\n            return 'Custom ' + obj.name\n\n    f = CustomModelChoiceField(Category.objects.all())\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Custom Entertainment'),\n        (self.c2.pk, 'Custom A test'),\n        (self.c3.pk, 'Custom Third'),\n    ])\n"], "sample_54": ["def test_file_response_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_response_close(self):\n    file = open(__file__, 'rb')\n    response = FileResponse(file)\n    response.close()\n    self.assertTrue(file.closed)\n", "def test_filename_with_path(self):\n    file_path = '/path/to/file.txt'\n    response = FileResponse(open(__file__, 'rb'), filename=file_path)\n    self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{os.path.basename(file_path)}\"')\n", "def test_file_response_close_called(self):\n    filelike = open(__file__, 'rb')\n    response = FileResponse(filelike)\n    self.assertFalse(filelike.closed)\n    response.close()\n    self.assertTrue(filelike.closed)\n", "def test_file_response_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt', as_attachment=True)\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"example.txt\"')\n"], "sample_53": ["def test_use_required_attribute(self):\n    widget = Select()\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertTrue(widget.use_required_attribute(''))\n    self.assertFalse(widget.use_required_attribute('some_value'))\n\n    widget = SelectMultiple()\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertTrue(widget.use_required_attribute(''))\n    self.assertFalse(widget.use_required_attribute('some_value'))\n\n    widget = CheckboxSelectMultiple()\n    self.assertFalse(widget.use_required_attribute(None))\n    self.assertFalse(widget.use_required_attribute(''))\n    self.assertFalse(widget.use_required_attribute('some_value'))\n", "def test_autocomplete_select_with_initial_value(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    form = AlbumForm(initial={'band': beatles.pk})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    self.assertIn(selected_option, output)\n", "def test_split_datetime_widget_render(self):\n    widget = SplitDateTimeWidget()\n    output = widget.render('date', datetime.datetime(2022, 12, 25, 10, 30))\n    self.assertHTMLEqual(output, \"\"\"\n        <p class=\"datetime\">Date: <input type=\"text\" name=\"date_0\" value=\"2022-12-25\" />\n        <br />Time: <input type=\"text\" name=\"date_1\" value=\"10:30\" /></p>\n    \"\"\")\n", "def test_split_datetime_widget(self):\n    widget = SplitDateTimeWidget()\n    self.assertEqual(widget.widgets[0].input_type, 'date')\n    self.assertEqual(widget.widgets[1].input_type, 'time')\n    self.assertEqual(len(widget.widgets), 2)\n\n    # Test decompress\n    value = datetime.datetime(2022, 1, 1, 12, 0)\n    decompressed_value = widget.decompress(value)\n    self.assertEqual(decompressed_value, [value.date(), value.time()])\n\n    # Test render\n    output = widget.render('my_field', value)\n    self.assertIn('type=\"date\"', output)\n    self.assertIn('type=\"time\"', output)\n", "def test_decompress(self):\n    widget = SplitDateTimeWidget()\n    value = datetime.datetime(2022, 1, 1, 12, 0)\n    decompressed_value = widget.decompress(value)\n    self.assertEqual(decompressed_value, [datetime.date(2022, 1, 1), datetime.time(12, 0)])\n    value = None\n    decompressed_value = widget.decompress(value)\n    self.assertEqual(decompressed_value, [None, None])\n"], "sample_58": ["def test_add_prefix_with_empty_prefix(self):\n    class Person(Form):\n        first_name = CharField()\n        last_name = CharField()\n\n            if self.prefix:\n                return '%s-%s' % (self.prefix, field_name)\n            return field_name\n\n    p = Person(prefix='')\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n", "def test_renderer_is_cached(self):\n    class CustomForm(Form):\n        default_renderer = DjangoTemplates()\n\n    form = CustomForm()\n    renderer1 = form.renderer\n    renderer2 = form.renderer\n    self.assertIs(renderer1, renderer2)\n", "def test_renderer_used_in_template(self):\n    class CustomForm(Form):\n        name = CharField()\n\n            return self._html_output(\n                normal_row='<tr%(html_class_attr)s><th>%(label)s</th><td>%(errors)s%(field)s%(help_text)s</td></tr>',\n                error_row='<tr><td colspan=\"2\">%s</td></tr>',\n                row_ender='</td></tr>',\n                help_text_html='<br><span class=\"helptext\">%s</span>',\n                errors_on_separate_row=False,\n            )\n\n    form = CustomForm(renderer=CustomRenderer())\n    self.assertHTMLEqual(\n        form.as_table(),\n        '<tr><th>Name:</th><td><input type=\"text\" name=\"name\" required></td></tr>',\n    )\n", "def test_media_property(self):\n    class MyForm(Form):\n        class Media:\n            css = {\n                'all': ('/path/to/css1.css', '/path/to/css2.css'),\n            }\n            js = ['/path/to/js1.js', '/path/to/js2.js']\n\n    form = MyForm()\n    media = form.media\n\n    self.assertEqual(media._css, {'all': ['/path/to/css1.css', '/path/to/css2.css']})\n    self.assertEqual(media._js, ['/path/to/js1.js', '/path/to/js2.js'])\n", "def test_custom_renderer_passed_to_widgets(self):\n    class CustomRenderer(DjangoTemplates):\n        pass\n\n    class CustomForm(Form):\n        field = CharField()\n\n    form = CustomForm(renderer=CustomRenderer())\n    self.assertEqual(form['field'].field.widget.renderer, form.renderer)\n"], "sample_56": ["def test_autocomplete_fields_must_be_foreign_keys(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ['title']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n            obj=SongAdmin,\n            id='admin.E038'\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_with_invalid_value(self):\n    class BookAdmin(admin.ModelAdmin):\n        list_filter = ['invalid_field']\n\n    errors = BookAdmin(Book, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'invalid_field', which does not refer to a Field.\",\n            obj=BookAdmin,\n            id='admin.E116',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_item_callable(self):\n    class MyListFilter:\n        pass\n\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [MyListFilter]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' must inherit from 'ListFilter'.\",\n            obj=SongAdmin,\n            id='admin.E113',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_item_is_callable(self):\n    class MyListFilter:\n        pass\n\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [MyListFilter]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' must inherit from 'ListFilter'.\",\n            obj=SongAdmin,\n            id='admin.E113',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_item_with_field_and_list_filter_class(self):\n    from django.contrib.admin import ListFilter, FieldListFilter\n\n    class MyListFilter(FieldListFilter):\n        pass\n\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [('title', MyListFilter)]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_57": ["    def test_normalize_username(self):\n        field = UsernameField()\n        self.assertEqual(field.to_python('test\u2126'), 'test\u03a9')  # U+03A9 GREEK CAPITAL LETTER OMEGA\n", "    def test_bound_data(self):\n        field = ReadOnlyPasswordHashField()\n        initial = 'initial'\n        data = 'data'\n        self.assertEqual(field.bound_data(data, initial), initial)\n", "    def test_render_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'unusable_password'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_autocapitalize_none(self):\n        field = UsernameField()\n        self.assertEqual(field.widget.attrs.get('autocapitalize'), 'none')\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'unusable'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n"], "sample_59": ["    def test_deferred_fields(self):\n        obj = Model1.objects.create(pkey=1000)\n        self.assertEqual(obj.get_deferred_fields(), set())\n        obj = Model1.objects.defer('pkey').get(pkey=1000)\n        self.assertEqual(obj.get_deferred_fields(), {'pkey'})\n", "    def test_constructor(self):\n        ms = ModelState()\n        self.assertIsNone(ms.db)\n        self.assertTrue(ms.adding)\n        self.assertEqual(ms.fields_cache, {})\n", "def test_full_clean_with_exclude(self):\n    # Test that full_clean(exclude=...) works as expected.\n    a = Article(headline=\"Look at me!\", pub_date=datetime.datetime.now())\n    a.full_clean(exclude=('headline',))\n    with self.assertRaises(ValidationError):\n        a.full_clean()\n", "    def test_fields_cache(self):\n        obj = Model1(pkey=1000)\n        self.assertEqual(obj._state.fields_cache, {})\n", "    def test_fields_cache(self):\n        # Ensure Model.fields_cache is initialized properly.\n        m = Model1()\n        self.assertEqual(m.fields_cache, {})\n"], "sample_60": ["    def setUp(self):\n        self.site = AdminSite()\n", "    def test_get_exclude(self):\n        class TestModelAdmin(BaseModelAdmin):\n            exclude = ('field1', 'field2')\n\n        ma = TestModelAdmin(User, admin_site)\n        self.assertEqual(ma.get_exclude(request=None), ('field1', 'field2'))\n", "    def test_custom_admin_site_name(self):\n        admin_site = AdminSite(name='custom-admin')\n        ma = admin.ModelAdmin(Episode, admin_site)\n        request = MockRequest()\n        request.user = MockSuperUser()\n        response = ma.changelist_view(request)\n        self.assertEqual(response.context_data['site_title'], 'Custom AdminSite')\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.site = AdminSite()\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_61": ["    def test_custom_validator(self):\n        class CustomValidator:\n                if 'custom' not in password:\n                    raise ValidationError('Password must contain \"custom\".')\n\n                return 'Password must contain \"custom\".'\n\n        validator = CustomValidator()\n        self.assertIsNone(validator.validate('password-with-custom'))\n        with self.assertRaises(ValidationError) as cm:\n            validator.validate('password-without-custom')\n        self.assertEqual(cm.exception.messages, ['Password must contain \"custom\".'])\n        self.assertEqual(validator.get_help_text(), 'Password must contain \"custom\".')\n", "    def test_ascii_and_unicode_validator_regex(self):\n        self.assertEqual(UnicodeUsernameValidator.regex, r'^[\\w.@+-]+\\Z')\n        self.assertEqual(ASCIIUsernameValidator.regex, r'^[\\w.@+-]+\\Z')\n", "    def test_validator_flags(self):\n        v = validators.ASCIIUsernameValidator()\n        self.assertEqual(v.flags, re.ASCII)\n        # Ensure the validator only matches ASCII characters.\n        self.assertRaises(ValidationError, v, 'Ren\u00e9')\n        # Ensure the validator allows ASCII characters.\n        v('joe')\n", "    def test_validator_flags(self):\n        v = ASCIIUsernameValidator()\n        self.assertEqual(v.flags, re.ASCII)\n        # Test that the validator correctly raises an error for non-ASCII characters\n        with self.assertRaises(ValidationError):\n            v('\u00c9ric')\n        # Test that the validator correctly passes for ASCII characters\n        v('eric')\n", "    def test_custom_validator(self):\n        class CustomValidator(validators.RegexValidator):\n            regex = r'^[a-zA-Z0-9]+$'\n            message = 'Enter a valid username. This value may contain only letters and numbers.'\n\n        v = CustomValidator()\n        valid_usernames = ['hello', 'HELLO', 'hello123']\n        invalid_usernames = ['hello!', '@hello', 'hello world']\n\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_62": ["    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def test_get_urls_empty_registry(self):\n        admin_site = admin.AdminSite()\n        urls = admin_site.get_urls()\n        self.assertEqual(len(urls), 8)  # Check for the default admin URLs\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def test_each_context(self):\n        request = self.request_factory.get('/admin')\n        admin_site = admin.AdminSite()\n        context = admin_site.each_context(request)\n        self.assertEqual(context['site_title'], admin_site.site_title)\n        self.assertEqual(context['site_header'], admin_site.site_header)\n        self.assertEqual(context['site_url'], admin_site.site_url)\n"], "sample_63": ["    def setUp(self):\n        self.engine = Engine(dirs=[TEMPLATE_DIR])\n", "    def test_app_dirs_and_loaders_mutually_exclusive(self):\n        msg = 'app_dirs must not be set when loaders is defined.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            Engine(app_dirs=True, loaders=['django.template.loaders.filesystem.Loader'])\n", "    def test_default_builtins(self):\n        engine = Engine()\n        self.assertEqual(len(engine.template_builtins), len(engine.default_builtins))\n", "    def test_builtins(self):\n        engine = Engine()\n        self.assertEqual(engine.builtins, Engine.default_builtins)\n", "    def setUp(self):\n        self.engine = Engine(dirs=[TEMPLATE_DIR])\n"], "sample_64": ["    def test_file_response(self):\n        with open(__file__, 'rb') as file:\n            response = FileResponse(file)\n            self.assertEqual(response.status_code, 200)\n            self.assertTrue(response.streaming)\n            self.assertEqual(response['Content-Type'], 'text/x-python')\n            self.assertGreater(int(response['Content-Length']), 0)\n", "    def test_file_response(self):\n        file = open(__file__, 'rb')\n        response = FileResponse(file)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/x-python; charset=utf-8')\n", "    def test_set_cookie(self):\n        response = HttpResponse()\n        response.set_cookie('foo', 'bar')\n        self.assertIn('foo=bar', response.cookies.output(header=''))\n", "    def test_file_response(self):\n        response = FileResponse(open(__file__, 'rb'))\n        self.assertEqual(response.status_code, 200)\n        self.assertIsInstance(response, HttpResponse)\n        self.assertTrue(response.streaming)\n", "    def test_file_response(self):\n        file = open(__file__, 'rb')\n        response = FileResponse(file)\n        self.assertEqual(response.status_code, 200)\n        self.assertIsInstance(response, StreamingHttpResponse)\n"], "sample_65": ["def test_i18n_language_non_english_default_with_en_translation(self):\n    \"\"\"\n    Check if the Javascript i18n view returns a non empty language catalog\n    if the default language is non-English, the selected language is English\n    and there is an 'en' translation available.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='fr'), override('en-us'):\n        response = self.client.get('/jsi18n_app0/')\n        self.assertContains(response, 'this app0 string is to be translated')\n", "def test_i18n_language_non_english_default_with_safe_next(self):\n    \"\"\"\n    Check if the set_language view correctly redirects to a safe URL when\n    settings.LANGUAGE_CODE is not 'en-us' and the next parameter is provided.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='fr'), override('en-us'):\n        lang_code = 'en-us'\n        post_data = {'language': lang_code, 'next': '/safe/next/'}\n        response = self.client.post('/i18n/setlang/', data=post_data)\n        self.assertRedirects(response, '/safe/next/', fetch_redirect_response=False)\n        self.assertEqual(self.client.cookies[settings.LANGUAGE_COOKIE_NAME].value, lang_code)\n        with ignore_warnings(category=RemovedInDjango40Warning):\n            self.assertEqual(self.client.session[LANGUAGE_SESSION_KEY], lang_code)\n", "    def test_javascript_catalog(self):\n        response = self.client.get('/jsi18n/')\n        self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n        catalog = response.context['catalog']\n        self.assertIn('djangojs', catalog)\n        self.assertIn('this is to be translated', catalog)\n", "def test_i18n_language_non_english_default_with_en_fallback(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns an English language catalog\n    if the default language is non-English, the selected language has no\n    translation available and there is an 'en' translation available.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='fr'), override('es-ar'):\n        response = self.client.get('/jsi18n_app6/')\n        self.assertContains(response, 'this app6 string is to be translated')\n", "def test_jsi18n_format_indented(self):\n    \"\"\"The jsi18n view returns indented formats.\"\"\"\n    with override('de'):\n        response = self.client.get('/jsi18n/')\n        self.assertContains(response, '  \"DATE_INPUT_FORMATS\": [')\n        self.assertContains(response, '  \"DECIMAL_SEPARATOR\": \".\",')\n        self.assertContains(response, '  \"FIRST_DAY_OF_WEEK\": 0,')\n"], "sample_67": ["    def test_model_formfield_for_foreign_key_with_use_required_attribute(self):\n        f = forms.ModelChoiceField(queryset=Writer.objects.all(), use_required_attribute=False)\n        self.assertEqual(f.widget.use_required_attribute, False)\n", "    def test_fields_for_model_all(self):\n        fields = fields_for_model(Person, fields='__all__')\n        self.assertEqual(len(fields), 1)\n        self.assertIn('name', fields)\n", "    def test_fields_for_model_with_localized_fields(self):\n        class LocalizedTripleForm(forms.ModelForm):\n            class Meta:\n                model = Triple\n                localized_fields = ('left', 'right',)\n                fields = '__all__'\n\n        fields = fields_for_model(Triple, localized_fields=('left', 'right'))\n        for name in fields:\n            if name in ('left', 'right'):\n                self.assertTrue(fields[name].localize)\n            else:\n                self.assertFalse(fields[name].localize)\n", "    def test_modelform_invalid_data(self):\n        # Create a model form for the Triple model\n        class TripleForm(forms.ModelForm):\n            class Meta:\n                model = Triple\n                fields = '__all__'\n\n        # Create a form instance with invalid data (non-integer values for integer fields)\n        form = TripleForm({'left': 'foo', 'middle': 'bar', 'right': 'baz'})\n\n        # The form should not be valid\n        self.assertFalse(form.is_valid())\n\n        # Check the error messages\n        self.assertEqual(form.errors, {\n            'left': ['Enter a whole number.'],\n            'middle': ['Enter a whole number.'],\n            'right': ['Enter a whole number.']\n        })\n", "    def test_fields_for_model_includes_many_to_many(self):\n        fields = fields_for_model(Book, fields=['categories'])\n        self.assertIn('categories', fields)\n        self.assertIsInstance(fields['categories'], forms.ModelMultipleChoiceField)\n"], "sample_68": ["    def test_technical_404(self):\n        request = RequestFactory().get('/test_view/')\n        exception = Http404(\"Page not found\")\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertInHTML('<h1>Page not found <span>(404)</span></h1>', response.content.decode())\n        self.assertIn('Page not found', response.content.decode())\n", "    def test_cleanse_setting_dictionary_with_non_string_keys(self):\n        initial = {'login': 'cooper', 123: 'secret'}\n        expected = {'login': 'cooper', 123: CLEANSED_SUBSTITUTE}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_safe_exception_reporter_filter(self):\n        \"\"\"\n        SafeExceptionReporterFilter is used when DEBUG=False.\n        \"\"\"\n        with self.settings(DEBUG=False, DEFAULT_EXCEPTION_REPORTER_FILTER='django.views.debug.SafeExceptionReporterFilter'):\n            self.verify_safe_response(sensitive_view)\n            self.verify_safe_email(sensitive_view)\n", "    def test_cleanse_setting_callable(self):\n            return 'callable_value'\n        self.assertEqual(cleanse_setting('SETTING_NAME', my_callable), CallableSettingWrapper(my_callable))\n", "    def test_cleanse_setting_callable(self):\n            return \"This should not be displayed\"\n        cleansed = cleanse_setting('SETTING_NAME', callable_setting)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n"], "sample_69": ["def test_tick_calls_receive(self):\n    with mock.patch.object(self.reloader.client, 'receive') as mocked_receive:\n        next(self.reloader.tick())\n    self.assertEqual(mocked_receive.call_count, 1)\n", "    def test_stop_sets_should_stop(self):\n        reloader = autoreload.BaseReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n", "def test_tick_with_request_finished(self):\n    with mock.patch.object(self.reloader, 'update_watches') as mocked_update_watches:\n        with mock.patch.object(self.reloader, 'processed_request') as mocked_processed_request:\n            mocked_processed_request.is_set.return_value = True\n            next(self.reloader.tick())\n            self.assertEqual(mocked_update_watches.call_count, 1)\n            self.assertEqual(mocked_processed_request.clear.call_count, 1)\n", "    def test_file_changed_signal_sent(self):\n        signal_received = False\n\n            nonlocal signal_received\n            signal_received = True\n            self.assertEqual(file_path, '/path/to/file')\n            self.assertIsNone(kind)\n\n        autoreload.file_changed.connect(receiver)\n        autoreload.notify_file_changed('/path/to/file')\n        self.assertTrue(signal_received)\n", "    def test_ensure_echo_on_not_tty(self, mocked_termios):\n        with mock.patch('sys.stdin.isatty', return_value=False):\n            autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcgetattr.called)\n"], "sample_70": ["def test_delete_queryset_with_inheritance(self):\n    \"\"\"\n    Test that deleting a queryset of models with inheritance works correctly.\n    \"\"\"\n    child = RChild.objects.create()\n    parent = child.r_ptr\n    self.assertNumQueries(2, R.objects.filter(pk=parent.pk).delete())\n    self.assertFalse(RChild.objects.exists())\n    self.assertFalse(R.objects.exists())\n", "def test_collector_sort(self):\n    a = A.objects.create()\n    r = R.objects.create()\n    a.cascade = r\n    a.save()\n\n    collector = Collector(using='default')\n    collector.add([a])\n    collector.collect([r], source=a.__class__)\n    self.assertEqual(list(collector.data), [R, A])\n\n    # Reverse dependency.\n    collector = Collector(using='default')\n    collector.add([r])\n    collector.collect([a], source=r.__class__, reverse_dependency=True)\n    self.assertEqual(list(collector.data), [A, R])\n", "def test_collector_sort(self):\n    \"\"\"\n    Test that the Collector.sort() method properly sorts the collected instances.\n    This is necessary for databases that don't support deferred constraint checking.\n    \"\"\"\n    r1 = R.objects.create()\n    r2 = R.objects.create()\n    a1 = A.objects.create(cascade=r1)\n    a2 = A.objects.create(cascade=r2)\n\n    collector = Collector(using='default')\n    collector.collect([a1, a2], source=None, nullable=False, collect_related=True)\n    collector.sort()\n\n    self.assertEqual(collector.data, {A: [a1, a2], R: [r1, r2]})\n", "def test_fast_delete_with_existing_pre_delete_signal(self):\n    \"\"\"\n    Fast deletion should be disabled when a pre_delete signal is connected.\n    \"\"\"\n    pre_delete_calls = []\n\n        pre_delete_calls.append(kwargs['instance'])\n\n    models.signals.pre_delete.connect(pre_delete_handler, sender=User)\n\n    u = User.objects.create()\n    collector = Collector(using='default')\n    self.assertFalse(collector.can_fast_delete(u))\n\n    u.delete()\n    self.assertEqual(len(pre_delete_calls), 1)\n    self.assertEqual(pre_delete_calls[0], u)\n\n    models.signals.pre_delete.disconnect(pre_delete_handler, sender=User)\n", "def test_delete_with_queryset_chain(self):\n    r = R.objects.create()\n    S.objects.create(r=r)\n    T.objects.create(s=S.objects.get(r=r))\n    # 1 for select related T, 1 for delete T, 1 for select related S, 1 for delete S, 1 for delete R\n    self.assertNumQueries(5, R.objects.filter(pk=r.pk).delete())\n    self.assertFalse(R.objects.exists())\n    self.assertFalse(S.objects.exists())\n    self.assertFalse(T.objects.exists())\n"], "sample_71": ["def test_non_uniform_digit_grouping(self):\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '123,45,6789')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2), thousand_sep=','), '123,45,6789')\n    self.assertEqual(nformat(123456789, '.', grouping=(3,), thousand_sep=','), '123,456,789')\n    self.assertEqual(nformat(123456789, '.', grouping=(2,), thousand_sep=','), '12,34,56,78,9')\n", "def test_format_with_non_uniform_digit_grouping(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(-1234567, '.', grouping=(3, 2), thousand_sep=','), '-1,23,45,67')\n    self.assertEqual(nformat(Decimal('1234567'), '.', grouping=(3, 2), thousand_sep=','), '1,23,45,67')\n", "def test_format_non_uniform_grouping(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2), thousand_sep=','), '1,234,56,7')\n    self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=','), '1,234,56,7890')\n    self.assertEqual(nformat(123456789012, '.', grouping=(3, 2, 2), thousand_sep=','), '1,234,56,78,9012')\n    self.assertEqual(nformat(12345678901234, '.', grouping=(3, 2, 2, 0), thousand_sep=','), '1,234,56,78,901234')\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,78,9')\n    self.assertEqual(nformat(-1234567, '.', grouping=(3, 2), thousand_sep=','), '-1,23,45,67')\n    self.assertEqual(nformat(Decimal('1234567'), '.', grouping=(3, 2), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(1234567.89, '.', grouping=(3, 2), thousand_sep=','), '1,23,45,67.89')\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2), thousand_sep=','), '123,45,6789')\n    self.assertEqual(nformat(123456789, '.', grouping=(2, 3, 2), thousand_sep=','), '12,345,67,89')\n    self.assertEqual(nformat(123456789, '.', grouping=(1, 2, 3), thousand_sep=','), '1,23,456,789')\n"], "sample_72": ["def test_serialize_type_with_module(self):\n    class TestType:\n        pass\n\n    string, imports = MigrationWriter.serialize(type(TestType))\n    self.assertEqual(string, \"type(migrations.test_writer.TestType)\")\n    self.assertEqual(imports, {'import migrations.test_writer'})\n", "def test_serialize_type(self):\n    self.assertSerializedEqual(int)\n    self.assertSerializedResultEqual(\n        int,\n        (\"int\", set())\n    )\n    self.assertSerializedEqual(type)\n    self.assertSerializedResultEqual(\n        type,\n        (\"type\", set())\n    )\n    self.assertSerializedEqual(models.Model)\n    self.assertSerializedResultEqual(\n        models.Model,\n        (\"models.Model\", {\"from django.db import models\"})\n    )\n", "def test_serialize_type_with_module(self):\n    class TestType:\n        pass\n\n    self.assertSerializedResultEqual(\n        type(TestType),\n        (\"type(migrations.test_writer.TestType)\", {'import migrations.test_writer'})\n    )\n", "def test_register_serializer_with_module(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    MigrationWriter.register_serializer(complex, 'migrations.test_writer.ComplexSerializer')\n    self.assertSerializedEqual(complex(1, 2))\n    MigrationWriter.unregister_serializer(complex)\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize: (1+2j)'):\n        self.assertSerializedEqual(complex(1, 2))\n", "def test_serialize_enum_with_module_alias(self):\n    class TextEnum(enum.Enum):\n        A = 'a-value'\n        B = 'value-b'\n\n    import migrations.test_writer as tw\n    self.assertSerializedResultEqual(\n        TextEnum.A,\n        (\"tw.TextEnum('a-value')\", {'import migrations.test_writer as tw'})\n    )\n"], "sample_73": ["    def setUp(self):\n        super().setUp()\n        storage.staticfiles_storage.keep_intermediate_files = True\n", "    def test_manifest_creation(self):\n        # Make sure there are no files in the static root.\n        shutil.rmtree(settings.STATIC_ROOT, ignore_errors=True)\n        os.makedirs(settings.STATIC_ROOT)\n\n        # Run collectstatic.\n        call_command('collectstatic', interactive=False, verbosity=0)\n\n        # Check that the manifest file was created.\n        manifest_path = os.path.join(settings.STATIC_ROOT, 'staticfiles.json')\n        self.assertTrue(os.path.exists(manifest_path))\n\n        # Check that the manifest file is empty.\n        with open(manifest_path, 'r') as f:\n            manifest = json.load(f)\n        self.assertEqual(manifest, {'version': '1.0', 'paths': {}})\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def test_non_existent_file_referenced_from_css(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"non_existent.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # Run collectstatic.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n                # The non-existent file should not be rewritten.\n                self.assertIn(b'non_existent.png', content)\n", "    def setUp(self):\n        super().setUp()\n        self.patched_settings = self.settings(\n            CACHES={\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n                },\n                'staticfiles': {\n                    'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n                },\n            },\n            STATICFILES_STORAGE='django.contrib.staticfiles.storage.CachedStaticFilesStorage',\n        )\n        self.patched_settings.enable()\n"], "sample_75": ["    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Les confessions Volume I')\n        cls.book2 = Book.objects.create(title='Candide')\n        cls.author1 = Author.objects.create(name='Rousseau', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Voltaire', first_book=cls.book2)\n        cls.book1.authors.add(cls.author1)\n        cls.book2.authors.add(cls.author2)\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.author2 = Author.objects.create(name='Author 2')\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n        cls.book3 = Book.objects.create(title='Book 3')\n        cls.book1.authors.add(cls.author1, cls.author2)\n        cls.book2.authors.add(cls.author1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.author1 = Author.objects.create(name='Author 1', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Author 2', first_book=cls.book1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n        cls.book1.authors.add(cls.author1, cls.author2)\n"], "sample_74": ["def test_runshell_without_database(self):\n    with self.assertRaises(KeyError):\n        DatabaseClient.runshell_db({\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        })\n", "def test_empty_params(self):\n    self.assertEqual(\n        self._run_it({}),\n        (['psql', ''], {})\n    )\n", "def test_empty_params(self):\n    self.assertEqual(\n        self._run_it({}),\n        (['psql'], {})\n    )\n", "def test_empty_params(self):\n    self.assertEqual(\n        self._run_it({}),\n        (['psql', ''], {})\n    )\n", "def test_empty_params(self):\n    self.assertEqual(\n        self._run_it({}),\n        (['psql', ''], {})\n    )\n"], "sample_76": ["def test_consistent_language_settings(self):\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(\n                'You have provided a value for the LANGUAGE_CODE setting that is '\n                'not in the LANGUAGES setting.',\n                id='translation.E004',\n            ),\n        ])\n", "def test_consistent_language_settings(self):\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('fr', 'French'), ('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('fr', 'French'), ('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('fr', 'French'), ('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en-us', 'English'), ('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_settings_consistent_with_default_language(self):\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_77": ["def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('Check out www.google.com!', 'Check out <a href=\"http://www.google.com\">www.google.com</a>!'),\n        ('Check out www.google.com?', 'Check out <a href=\"http://www.google.com\">www.google.com</a>?'),\n        ('Check out www.google.com...', 'Check out <a href=\"http://www.google.com\">www.google.com</a>...'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Check out google.com.', 'Check out <a href=\"http://google.com/\">google.com</a>.'),\n        ('See google.com?', 'See <a href=\"http://google.com/\">google.com</a>?'),\n        ('I love www.example.com!', 'I love <a href=\"http://www.example.com/\">www.example.com</a>!'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('What is http://example.com?', 'What is <a href=\"http://example.com\">http://example.com</a>?'),\n        ('Visit https://www.djangoproject.com!', 'Visit <a href=\"https://www.djangoproject.com\">https://www.djangoproject.com</a>!'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_nofollow(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n        ),\n        ('foo@example.com', '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),  # nofollow doesn't apply to email\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('Check out www.google.com!', 'Check out <a href=\"http://www.google.com\">www.google.com</a>!'),\n        ('Check out www.google.com?', 'Check out <a href=\"http://www.google.com\">www.google.com</a>?'),\n        ('Check out www.google.com:', 'Check out <a href=\"http://www.google.com\">www.google.com</a>:'),\n        ('Check out www.google.com;', 'Check out <a href=\"http://www.google.com\">www.google.com</a>;'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_78": ["def test_management_command_color_output(self):\n    out = StringIO()\n    management.call_command('dance', stdout=out, force_color=True)\n    self.assertIn(\"\\033[\", out.getvalue())\n\n    out = StringIO()\n    management.call_command('dance', stdout=out, no_color=True)\n    self.assertNotIn(\"\\033[\", out.getvalue())\n", "def test_handle_default_options(self):\n    options = mock.Mock()\n    options.settings = 'test_settings'\n    options.pythonpath = '/path/to/pythonpath'\n\n    handle_default_options(options)\n\n    self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], 'test_settings')\n    self.assertEqual(sys.path[0], '/path/to/pythonpath')\n", "def test_command_error_output(self):\n    msg = \"Error message\"\n    with captured_stderr() as stderr:\n        try:\n            raise CommandError(msg)\n        except SystemExit:\n            pass\n    self.assertIn(msg, stderr.getvalue())\n    self.assertNotIn(\"Traceback\", stderr.getvalue())\n    self.assertNotIn(\"raise CommandError\", stderr.getvalue())\n", "def test_output_wrapper_write_with_ending(self):\n    out = StringIO()\n    output_wrapper = OutputWrapper(out, ending='\\r\\n')\n    output_wrapper.write('Hello, world!')\n    self.assertEqual(out.getvalue(), 'Hello, world!\\r\\n')\n", "def test_output_wrapper_write_with_ending(self):\n    out = StringIO()\n    wrapper = OutputWrapper(out)\n    wrapper.write('Hello', ending='')\n    self.assertEqual(out.getvalue(), 'Hello')\n    wrapper.write(' World', ending='!')\n    self.assertEqual(out.getvalue(), 'Hello World!')\n    wrapper.write(' Test', ending=None)\n    self.assertEqual(out.getvalue(), 'Hello World!\\n Test\\n')\n"], "sample_79": ["    def test_string_format(self):\n        self.assertEqual(self.engine.render_to_string('t', {'value': 42}), 'The answer is 42')\n", "    def test_string(self):\n        self.check_values(\n            ('', 'nothing'), ('Something', 'Something'),\n            (None, 'nothing'), (False, 'nothing'), (True, 'True'),\n        )\n", "    def test_default(self):\n        self.assertEqual(self.engine.render_to_string('t', {'value': 'value'}), 'value')\n        self.assertEqual(self.engine.render_to_string('t', {'value': ''}), 'default_value')\n        self.assertEqual(self.engine.render_to_string('t', {'value': None}), 'default_value')\n", "    def test_add(self):\n        self.check_values(('0', '5'), ('1', '6'), ('2', '7'))\n", "    def test_add(self):\n        self.check_values(('0', '5'), ('1', '6'), ('2', '7'), ('-1', '4'))\n"], "sample_80": ["def test_query_add_immediate_loading(self):\n    query = Query(Author)\n    query.add_immediate_loading(['name', 'num'])\n    self.assertEqual(query.deferred_loading, (frozenset({'name', 'num'}), False))\n\n    query = Query(Author)\n    query.add_deferred_loading(['bio'])\n    query.add_immediate_loading(['name', 'num'])\n    self.assertEqual(query.deferred_loading, (frozenset({'name', 'num'}), False))\n\n    query = Query(Author)\n    query.add_immediate_loading(['name', 'num'])\n    query.add_immediate_loading(['bio'])\n    self.assertEqual(query.deferred_loading, (frozenset({'name', 'num', 'bio'}), False))\n", "def test_resolve_ref(self):\n    query = Query(Author)\n    field = Author._meta.get_field('num')\n    resolved = query.resolve_ref('num', allow_joins=False, reuse=None, summarize=False)\n    self.assertEqual(resolved.target, field)\n    self.assertEqual(resolved.output_field, field)\n\n    with self.assertRaises(FieldError):\n        query.resolve_ref('foo__bar', allow_joins=False, reuse=None, summarize=False)\n\n    query = Query(Item)\n    field = Item._meta.get_field('created')\n    resolved = query.resolve_ref('created', allow_joins=False, reuse=None, summarize=False)\n    self.assertEqual(resolved.target, field)\n    self.assertEqual(resolved.output_field, field)\n", "def test_resolve_ref(self):\n    query = Query(Author)\n    lookup = query.resolve_ref('num', allow_joins=True, reuse=None, summarize=False, simple_col=False)\n    self.assertIsInstance(lookup, SimpleCol)\n    self.assertEqual(lookup.target, Author._meta.get_field('num'))\n", "def test_is_nullable(self):\n    query = Query(Author)\n    self.assertTrue(query.is_nullable(Author._meta.get_field('name')))\n    self.assertFalse(query.is_nullable(Author._meta.get_field('num')))\n", "def test_resolve_lookup_value(self):\n    query = Query(Author)\n    lookup_value = 'test'\n    resolved_value = query.resolve_lookup_value(lookup_value, None, True, False)\n    self.assertEqual(resolved_value, lookup_value)\n\n    f_expr = F('num')\n    resolved_f_expr = query.resolve_lookup_value(f_expr, None, True, False)\n    self.assertIsInstance(resolved_f_expr, SimpleCol)\n    self.assertEqual(resolved_f_expr.target, Author._meta.get_field('num'))\n\n    list_value = [1, 2, 3]\n    resolved_list_value = query.resolve_lookup_value(list_value, None, True, False)\n    self.assertEqual(resolved_list_value, list_value)\n\n    f_expr_in_list = [F('num')]\n    resolved_f_expr_in_list = query.resolve_lookup_value(f_expr_in_list, None, True, False)\n    self.assertIsInstance(resolved_f_expr_in_list[0], SimpleCol)\n    self.assertEqual(resolved_f_expr_in_list[0].target, Author._meta.get_field('num'))\n"], "sample_82": ["def test_use_required_attribute(self):\n    widget = SelectDateWidget(required=True)\n    self.assertTrue(widget.use_required_attribute(None))\n    widget = SelectDateWidget(required=False)\n    self.assertFalse(widget.use_required_attribute(None))\n\n    # Make sure first option is not empty\n    widget = SelectDateWidget(required=True, years=[2014])\n    self.assertFalse(widget.use_required_attribute(None))\n", "def test_value_from_datadict_invalid(self):\n    widget = SelectDateWidget(years=('2014',))\n    data = {'date_year': '2014', 'date_month': '2', 'date_day': '30'}\n    self.assertEqual(widget.value_from_datadict(data, {}, 'date'), '2014-02-30')\n    self.assertEqual(widget.value_from_datadict({}, {}, 'date'), None)\n    self.assertEqual(widget.value_from_datadict({'date_year': '2014'}, {}, 'date'), None)\n    self.assertEqual(widget.value_from_datadict({'date_month': '2'}, {}, 'date'), None)\n    self.assertEqual(widget.value_from_datadict({'date_day': '30'}, {}, 'date'), None)\n", "def test_format_value_invalid_date(self):\n    # An invalid date should still return a dictionary with the failed date.\n    self.assertEqual(\n        self.widget.format_value('2010-02-31'),\n        {'year': '2010', 'month': '2', 'day': '31'},\n    )\n", "def test_value_from_datadict_with_invalid_input(self):\n    widget = SelectDateWidget(years=('2007', '2008', '2009'))\n    data = {'date_year': 'abc', 'date_month': '12', 'date_day': '1'}\n    self.assertIsNone(widget.value_from_datadict(data, {}, 'date'))\n    data = {'date_year': '2007', 'date_month': 'abc', 'date_day': '1'}\n    self.assertIsNone(widget.value_from_datadict(data, {}, 'date'))\n    data = {'date_year': '2007', 'date_month': '12', 'date_day': 'abc'}\n    self.assertIsNone(widget.value_from_datadict(data, {}, 'date'))\n", "def test_value_from_datadict_with_invalid_date(self):\n    widget = SelectDateWidget(years=('2014',))\n    data = {'date_year': '2014', 'date_month': '13', 'date_day': '32'}\n    self.assertEqual(widget.value_from_datadict(data, {}, 'date'), '2014-13-32')\n"], "sample_81": ["    def test_str(self):\n        func = lambda: None\n        match = get_resolver().resolve('/func/')\n        match.func = func\n        self.assertEqual(str(match), \"ResolverMatch(func=tests.test_urlresolvers.<lambda>, args=(), kwargs={}, url_name=None, app_names=[], namespaces=[], route=None)\")\n", "    def test_str(self):\n        self.assertEqual(str(LocalePrefixPattern()), '')\n", "    def test_str(self):\n        pattern = RegexPattern(r'^test/$')\n        url_pattern = URLPattern(pattern, lambda x: x)\n        self.assertEqual(str(url_pattern), '<URLPattern ^test/$>')\n", "    def test_lookup_str_function_based_view(self):\n            pass\n\n        pattern = URLPattern(RegexPattern(r'^test/$'), view_function)\n        self.assertEqual(pattern.lookup_str, 'django.urls.resolvers.view_function')\n", "    def test_get(self):\n        pattern = RegexPattern(r'^example/$')\n        self.assertIsInstance(pattern.regex, re.Pattern)\n"], "sample_83": ["    def test_library_instantiation(self):\n        library = Library()\n        self.assertEqual(library.filters, {})\n        self.assertEqual(library.tags, {})\n", "    def test_get_resolved_arguments(self):\n        class TestNode(TagHelperNode):\n                super().__init__(func, takes_context, args, kwargs)\n\n            return args, kwargs\n\n        node = TestNode(test_func, True, ['arg1', 'arg2'], {'kwarg1': 'value1'})\n        context = {'arg1': 'resolved_arg1', 'arg2': 'resolved_arg2', 'kwarg1': 'resolved_kwarg1'}\n        resolved_args, resolved_kwargs = node.get_resolved_arguments(context)\n        self.assertEqual(resolved_args, [context, 'resolved_arg1', 'resolved_arg2'])\n        self.assertEqual(resolved_kwargs, {'kwarg1': 'resolved_kwarg1'})\n", "    def test_get_resolved_arguments(self):\n        node = TagHelperNode(lambda x: x, False, ['arg1', 'arg2'], {'kwarg1': 'value1'})\n        context = {'arg1': 'resolved_arg1', 'arg2': 'resolved_arg2'}\n        resolved_args, resolved_kwargs = node.get_resolved_arguments(context)\n        self.assertEqual(resolved_args, ['resolved_arg1', 'resolved_arg2'])\n        self.assertEqual(resolved_kwargs, {'kwarg1': 'resolved_value1'})\n", "    def test_library_init(self):\n        library = Library()\n        self.assertEqual(library.filters, {})\n        self.assertEqual(library.tags, {})\n", "    def test_get_resolved_arguments(self):\n        class TestNode(TagHelperNode):\n                super().__init__(func, takes_context, args, kwargs)\n\n            return ''\n\n        node = TestNode(test_func, True, ['arg1', 'arg2'], {'kwarg1': 'kwarg1_value'})\n\n        context = {'arg1': 'arg1_value', 'arg2': 'arg2_value'}\n        resolved_args, resolved_kwargs = node.get_resolved_arguments(context)\n\n        self.assertEqual(resolved_args, [context, 'arg1_value', 'arg2_value'])\n        self.assertEqual(resolved_kwargs, {'kwarg1': 'kwarg1_value'})\n"], "sample_85": ["    def test_reverse_one_to_one_related_object_descriptor(self):\n        # Create an instance of R, which has a reverse one-to-one\n        # relationship with A through the o2o_setnull field.\n        r = R.objects.create()\n\n        # Create an instance of A that is related to the R instance.\n        a = A.objects.create(o2o_setnull=r)\n\n        # Check that we can access the related A instance through the\n        # reverse relationship on the R instance.\n        self.assertEqual(r.o2o_setnull_related, a)\n\n        # Check that we get DoesNotExist if there is no related object.\n        r2 = R.objects.create()\n        with self.assertRaises(R.o2o_setnull_related.RelatedObjectDoesNotExist):\n            r2.o2o_setnull_related\n\n        # Check that we can assign a new related object.\n        a2 = A.objects.create()\n        r2.o2o_setnull_related = a2\n        self.assertEqual(r2.o2o_setnull_related, a2)\n", "    def test_fk_to_field(self):\n        r = R.objects.create(name='r')\n        a = A.objects.create(fk_to_field=r)\n        self.assertEqual(a.fk_to_field_id, r.name)\n", "    def test_related_query_name(self):\n        # Test that related_query_name is correctly set on the related field.\n        class Model(models.Model):\n            rel = models.ForeignKey('self', on_delete=models.CASCADE, related_name='related')\n\n        self.assertEqual(Model.rel.remote_field.related_query_name, 'model')\n", "    def test_create_foreign_object_with_non_integer_primary_key(self):\n        # Test creating a ForeignObject with a non-integer primary key.\n        class Model(models.Model):\n            id = models.CharField(max_length=10, primary_key=True)\n            other = models.ForeignKey('self', on_delete=models.CASCADE)\n\n        field = Model._meta.get_field('other')\n        self.assertIsInstance(field, models.ForeignKey)\n", "    def test_m2m_field_deletion(self):\n        m = M.objects.create()\n        r = R.objects.create()\n        MR.objects.create(m=m, r=r)\n        # 1 query to delete the MR relation\n        # 1 query to delete the MR object\n        self.assertNumQueries(2, m.delete)\n        self.assertFalse(M.objects.exists())\n        self.assertTrue(R.objects.exists())\n"], "sample_84": ["    def test_limited_parse_qsl(self):\n        qs = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(qs), [('a', '1'), ('b', '2'), ('c', '3')])\n", "    def test_valid_date(self):\n        date = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date), parse_http_date(date))\n", "    def test_valid_date(self):\n        date_string = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date_string), parse_http_date(date_string))\n", "    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date_str), parse_http_date(date_str))\n", "    def test_limited_parse_qsl(self):\n        tests = (\n            ('a=1&b=2', [('a', '1'), ('b', '2')]),\n            ('a=1&a=2', [('a', '1'), ('a', '2')]),\n            ('a=1;a=2', [('a', '1'), ('a', '2')]),\n        )\n        for query_string, expected in tests:\n            with self.subTest(query_string=query_string):\n                self.assertEqual(limited_parse_qsl(query_string), expected)\n"], "sample_86": ["def test_lazy_hash(self):\n    \"\"\"\n    hash() works correctly for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n    self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n", "def test_keep_lazy(self):\n    counter = 0\n\n    @keep_lazy(int)\n        nonlocal counter\n        counter += 1\n        return x + 1\n\n    lazy_obj = lazy(lambda: 5, int)\n    result = add_one(lazy_obj())\n    self.assertEqual(result, 6)\n    self.assertEqual(counter, 1)\n\n    # The function is evaluated immediately if the argument isn't lazy.\n    result = add_one(5)\n    self.assertEqual(result, 6)\n    self.assertEqual(counter, 2)\n", "def test_partition(self):\n    values = [1, 2, 3, 4, 5]\n    predicate = lambda x: x > 3\n    left, right = partition(predicate, values)\n    self.assertEqual(left, [1, 2, 3])\n    self.assertEqual(right, [4, 5])\n", "def test_lazy_object_hash(self):\n    \"\"\"\n    Test that the hash of a lazy object is correctly calculated based on its\n    wrapped object.\n    \"\"\"\n    class HashableObject:\n            self.value = value\n\n            return hash(self.value)\n\n    lazy_obj1 = lazy(lambda: HashableObject(1), HashableObject)\n    lazy_obj2 = lazy(lambda: HashableObject(1), HashableObject)\n    lazy_obj3 = lazy(lambda: HashableObject(2), HashableObject)\n\n    self.assertEqual(hash(lazy_obj1()), hash(lazy_obj2()))\n    self.assertNotEqual(hash(lazy_obj1()), hash(lazy_obj3()))\n", "def test_lazy_hash(self):\n    \"\"\"\n    hash() works correctly for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n    self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n"], "sample_88": ["def test_sanitize_address_with_idna_encoding(self):\n    \"\"\"\n    Email addresses with non-ASCII domain names are encoded correctly.\n    \"\"\"\n    email_address = \"user@\u00f6\u00e4\u00fc.com\"\n    sanitized_address = sanitize_address(email_address, encoding=\"utf-8\")\n    self.assertEqual(sanitized_address, \"user@xn--4ca9at.com\")\n", "def test_sanitize_address_with_nameless_email(self):\n    \"\"\"Sanitizing a nameless email address returns the address itself.\"\"\"\n    self.assertEqual(sanitize_address('test@example.com', 'utf-8'), 'test@example.com')\n", "def test_safe_mime_multipart_long_lines(self):\n    \"\"\"\n    Make sure headers can be set with a different encoding than utf-8 in\n    SafeMIMEMultipart and that long lines are properly wrapped.\n    \"\"\"\n    headers = {\"Date\": \"Fri, 09 Nov 2001 01:08:47 -0000\", \"Message-ID\": \"foo\"}\n    from_email, to = 'from@example.com', '\"S\u00fcrname, Firstname\" <to@example.com>'\n    text_content = 'This is an important message.'\n    html_content = '<p>This is an <strong>important</strong> message.</p>' * 10\n    msg = EmailMultiAlternatives('Message from Firstname S\u00fcrname', text_content, from_email, [to], headers=headers)\n    msg.attach_alternative(html_content, \"text/html\")\n    msg.encoding = 'iso-8859-1'\n    self.assertEqual(msg.message()['To'], '=?iso-8859-1?q?S=FCrname=2C_Firstname?= <to@example.com>')\n    self.assertEqual(msg.message()['Subject'], '=?iso-8859-1?q?Message_from_Firstname_S=FCrname?=')\n    payload = msg.message().get_payload(1)\n    self.assertLessEqual(len(payload.as_string().splitlines()[0]), 998)\n", "def test_sanitize_address_with_invalid_input(self):\n    \"\"\"\n    Test that sanitize_address raises a ValueError for invalid input.\n    \"\"\"\n    with self.assertRaises(ValueError):\n        sanitize_address(None, 'utf-8')\n    with self.assertRaises(ValueError):\n        sanitize_address('', 'utf-8')\n    with self.assertRaises(ValueError):\n        sanitize_address('invalid_email', 'utf-8')\n", "def test_header_sanitize(self):\n    \"\"\"Header values are sanitized to prevent header injection.\"\"\"\n    msg = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    msg.extra_headers['Subject'] = 'New Subject\\nInjected-Header: value'\n    with self.assertRaises(BadHeaderError):\n        msg.message()\n    msg.extra_headers['To'] = 'to@example.com\\nInjected-Header: value'\n    with self.assertRaises(BadHeaderError):\n        msg.message()\n    msg.extra_headers['From'] = 'from@example.com\\nInjected-Header: value'\n    with self.assertRaises(BadHeaderError):\n        msg.message()\n"], "sample_87": ["    def test_trigger_reload(self):\n        with mock.patch('sys.exit') as mocked_exit:\n            autoreload.trigger_reload('test.py')\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_get_child_arguments_with_manage_py(self):\n        sys.argv = ['./manage.py', 'runserver']\n        self.assertEqual(autoreload.get_child_arguments(), [sys.executable] + sys.argv)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "def test_tick_sleep_time(self):\n    with mock.patch.object(time, 'sleep') as mocked_sleep:\n        next(self.reloader.tick())\n        self.assertEqual(mocked_sleep.call_args[0][0], self.reloader.SLEEP_TIME)\n"], "sample_89": ["def test_run_loop_tick_without_stop(self):\n        while True:\n            yield\n\n    with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n        thread = threading.Thread(target=self.reloader.run_loop)\n        thread.daemon = True\n        thread.start()\n        time.sleep(0.1)  # Allow the thread to run for a bit\n        self.reloader.stop()\n        thread.join()\n    self.assertGreater(tick.call_count, 1)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_raises_custom_exception(self):\n        class MyException(Exception):\n                super().__init__(msg)\n                self.extra_context = extra_context\n\n        try:\n            raise MyException('Test Message', 'extra context')\n        except MyException:\n            exc_info = sys.exc_info()\n\n        with mock.patch('django.utils.autoreload._exception', exc_info):\n            with self.assertRaises(MyException) as cm:\n                autoreload.raise_last_exception()\n            self.assertEqual(cm.exception.args[0], 'Test Message')\n            self.assertEqual(cm.exception.extra_context, 'extra context')\n"], "sample_90": ["    def test_construct_instance_excludes_foreign_keys_with_to_field(self):\n        \"\"\"\n        Foreign keys with to_field should not be set by construct_instance() if they\n        are not in fields or are excluded.\n        \"\"\"\n        instance = ColourfulItem()\n        form = modelform_factory(ColourfulItem, fields='__all__')\n        data = {'name': 'item', 'colours': Colour.objects.create(name='red').pk}\n        form = form(data)\n        self.assertTrue(form.is_valid())\n        construct_instance(form, instance)\n        self.assertEqual(instance.name, 'item')\n        self.assertEqual(list(instance.colours.all()), [])\n\n        # Foreign key with to_field is included if it's explicitly listed in fields.\n        instance = ColourfulItem()\n        form = modelform_factory(ColourfulItem, fields=('name', 'colours'))\n        form = form(data)\n        self.assertTrue(form.is_valid())\n        construct_instance(form, instance)\n        self.assertEqual(instance.name, 'item')\n        self.assertEqual(list(instance.colours.all()), [Colour.objects.get(name='red')])\n\n        # Foreign key with to_field is excluded if it's explicitly listed in exclude.\n        instance = ColourfulItem()\n        form = modelform_factory(ColourfulItem, exclude=('colours',))\n        form = form({'name': 'item'})\n        self.assertTrue(form.is_valid())\n        construct_instance(form, instance)\n        self.assertEqual(instance.name, 'item')\n        self.assertEqual(list(instance.colours.all()), [])\n", "    def test_disabled_fields(self):\n        class DisabledFieldsForm(forms.ModelForm):\n            class Meta:\n                model = Category\n                fields = '__all__'\n                widgets = {\n                    'name': forms.TextInput(attrs={'disabled': True}),\n                }\n\n        form = DisabledFieldsForm()\n        self.assertIn('disabled', str(form['name']))\n\n        # The field is not disabled when the widget doesn't have disabled=True.\n        self.assertNotIn('disabled', str(form['slug']))\n", "    def test_save_new_with_commit_false(self):\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = Triple\n                fields = '__all__'\n\n        formset = modelformset_factory(Triple, form=TestModelForm, extra=1)\n        data = {\n            'form-TOTAL_FORMS': '1',\n            'form-INITIAL_FORMS': '0',\n            'form-MIN_NUM_FORMS': '0',\n            'form-MAX_NUM_FORMS': '1000',\n            'form-0-left': '1',\n            'form-0-middle': '2',\n            'form-0-right': '3',\n        }\n        fs = formset(data, queryset=Triple.objects.none())\n        self.assertTrue(fs.is_valid())\n        instances = fs.save(commit=False)\n        self.assertEqual(len(instances), 1)\n        instance = instances[0]\n        self.assertIsInstance(instance, Triple)\n        self.assertEqual(instance.left, 1)\n        self.assertEqual(instance.middle, 2)\n        self.assertEqual(instance.right, 3)\n        # Check that the instance has not been saved to the database.\n        self.assertIsNone(instance.pk)\n        instance.save()\n        self.assertIsNotNone(instance.pk)\n", "    def test_add_fields(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = Category\n                fields = '__all__'\n\n        formset = modelformset_factory(Category, form=MyModelForm)\n        form = formset.forms[0]\n        self.assertIsInstance(form.fields['id'], forms.ModelChoiceField)\n", "    def test_choices(self):\n        category = Category.objects.create(name='Entertainment', slug='entertainment', url='entertainment')\n        field = forms.ModelChoiceField(Category.objects.all())\n        self.assertEqual(list(field.choices), [(category.pk, 'Entertainment')])\n"], "sample_91": ["def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n    self.assertIn(b'<!doctype html>', response.content)\n    self.assertIn(b'<html lang=\"en\">', response.content)\n    self.assertIn(b'<head>', response.content)\n    self.assertIn(b'<title>403 Forbidden</title>', response.content)\n    self.assertIn(b'<body>', response.content)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n    exception_message = 'Testing permission denied exception'\n    response = permission_denied(request, Exception(exception_message))\n    self.assertContains(response, exception_message, status_code=403)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n    self.assertIn(b'<!doctype html>', response.content)\n    self.assertIn(b'<html lang=\"en\">', response.content)\n    self.assertIn(b'<head>', response.content)\n    self.assertIn(b'<title>403 Forbidden</title>', response.content)\n    self.assertIn(b'<body>', response.content)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    exception = Exception('Test exception')\n    response = permission_denied(request, exception)\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n    self.assertIn(b'<!doctype html>', response.content)\n    self.assertIn(b'<html lang=\"en\">', response.content)\n    self.assertIn(b'<head>', response.content)\n    self.assertIn(b'<title>403 Forbidden</title>', response.content)\n    self.assertIn(b'<body>', response.content)\n"], "sample_92": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com')\n", "    def test_remote_user(self):\n        username = 'testuser'\n        user = User.objects.create_user(username, 'test@example.com')\n        request = HttpRequest()\n        request.META['REMOTE_USER'] = username\n        user = authenticate(request=request)\n        self.assertEqual(user.username, username)\n", "    def setUp(self):\n        self.request = HttpRequest()\n        self.request.META['REMOTE_USER'] = 'testuser'\n", "    def test_remote_user_backend_create_unknown_user(self):\n        backend = RemoteUserBackend()\n        user = backend.authenticate(request=HttpRequest(), remote_user='test')\n        self.assertIsNotNone(user)\n", "    def setUp(self):\n        self.request = HttpRequest()\n        self.request.META['REMOTE_USER'] = 'testuser'\n"], "sample_93": ["def test_aggregate_with_expression(self):\n    # Test that aggregate can handle an expression as input.\n    # In this case, we're using the 'rating' field, but we're applying a custom expression to it.\n    class DoubleRating(Func):\n        function = 'POWER'\n        template = \"%(function)s(%(expressions)s, %(power)s)\"\n        power = 2\n\n            super().__init__(expression, output_field=FloatField(), **extra)\n\n    double_avg_rating = Book.objects.aggregate(Avg(DoubleRating('rating')))['rating__double']\n    self.assertAlmostEqual(double_avg_rating, 17.583333, places=6)\n", "def test_annotate_with_mixed_value_and_expression(self):\n    msg = \"Expression contains mixed types. You must set output_field\"\n    with self.assertRaisesMessage(FieldError, msg):\n        Book.objects.annotate(val=Sum(F('price') + 2))\n\n    with self.assertRaisesMessage(FieldError, msg):\n        Book.objects.annotate(val=Sum(2 + F('price')))\n\n    with self.assertRaisesMessage(FieldError, msg):\n        Book.objects.annotate(val=Sum(2 + F('price') + 3))\n", "def test_aggregation_with_subqueries(self):\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).order_by('-pubdate').values('pubdate')[:1]\n    publishers = Publisher.objects.annotate(latest_pubdate=Subquery(subquery)).aggregate(Max('latest_pubdate'))\n    self.assertEqual(publishers['latest_pubdate__max'], datetime.date(2008, 12, 6))\n", "def test_expression_wrapper(self):\n    wrapper = ExpressionWrapper(F('rating'), output_field=IntegerField())\n    qs = Book.objects.annotate(rating_alias=wrapper).values('rating_alias')\n    self.assertEqual(list(qs), [{'rating_alias': 3}, {'rating_alias': 4}, {'rating_alias': 4}, {'rating_alias': 5}])\n", "def test_annotate_with_case(self):\n    authors = Author.objects.annotate(\n        num_friends=Case(\n            When(friends__isnull=True, then=0),\n            default=Count('friends'),\n        )\n    ).order_by('name')\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty', 2),\n            ('Brad Dayley', 0),\n            ('Jacob Kaplan-Moss', 2),\n            ('James Bennett', 1),\n            ('Jeffrey Forcier', 2),\n            ('Paul Bissex', 2),\n            ('Peter Norvig', 1),\n            ('Stuart Russell', 1),\n            ('Wesley J. Chun', 3)\n        ],\n        lambda a: (a.name, a.num_friends)\n    )\n"], "sample_94": ["    def test_non_unique_username(self):\n        User.objects.create(username='joe')\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'Error: That username is already taken.'):\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='joe',\n                email='joe@somewhere.org',\n                stdout=new_io,\n            )\n", "    def test_environment_variable_username(self):\n        \"\"\"The DJANGO_SUPERUSER_USERNAME environment variable is used if --username isn't provided.\"\"\"\n        new_io = StringIO()\n        os.environ['DJANGO_SUPERUSER_USERNAME'] = 'env_username'\n        call_command(\n            \"createsuperuser\",\n            interactive=False,\n            email=\"env@somewhere.org\",\n            stdout=new_io\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = User.objects.get(username=\"env_username\")\n        self.assertEqual(u.email, 'env@somewhere.org')\n        del os.environ['DJANGO_SUPERUSER_USERNAME']\n", "def test_create_superuser_command_output(self):\n    new_io = StringIO()\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username='joe',\n        email='joe@somewhere.org',\n        stdout=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertEqual(command_output, 'Superuser created successfully.')\n    self.assertNotIn('Password:', new_io.getvalue())\n    self.assertNotIn('Password (again):', new_io.getvalue())\n", "def test_get_input_message_with_foreign_key(self):\n    field = self.UserModel._meta.get_field('username')\n    field.remote_field = mock.Mock()\n    field.remote_field.model._meta.object_name = 'ForeignKeyModel'\n    field.remote_field.field_name = 'foreign_key_field'\n\n    message = createsuperuser.Command()._get_input_message(field)\n    expected_message = '%s (%s.%s): ' % (\n        capfirst(field.verbose_name),\n        field.remote_field.model._meta.object_name,\n        field.remote_field.field_name,\n    )\n    self.assertEqual(message, expected_message)\n", "    def setUp(self):\n        self.old_get_system_username = management.get_system_username\n"], "sample_98": ["    def test_wsgi_application_loading(self):\n        \"\"\"\n        The WSGI application is loaded correctly from the module specified by\n        settings.WSGI_APPLICATION.\n        \"\"\"\n        with self.settings(WSGI_APPLICATION='servers.wsgi.application'):\n            response = self.urlopen('/example_view/')\n            self.assertEqual(response.read(), b'example view')\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.live_server_url_ipv6 = f'http://[{cls.server_thread.host}]:{cls.server_thread.port}/'\n", "    def test_get_internal_wsgi_application(self):\n        \"\"\"\n        get_internal_wsgi_application returns the WSGI application as configured\n        by the user in settings.WSGI_APPLICATION.\n        \"\"\"\n        from django.conf import settings\n        app_path = getattr(settings, 'WSGI_APPLICATION')\n        wsgi_app = get_internal_wsgi_application()\n        self.assertEqual(app_path, 'servers.wsgi.application')\n        self.assertIsInstance(wsgi_app, callable)\n        # Test that get_internal_wsgi_application returns the same application\n        # when called multiple times.\n        self.assertIs(wsgi_app, get_internal_wsgi_application())\n", "    def test_request_handler_log_message(self):\n        # Test that the log_message method is called with the correct arguments\n        class TestWSGIRequestHandler(WSGIRequestHandler):\n                self.log_message_called = True\n                self.log_message_args = args\n\n        server_thread = LiveServerThread(\n            host='localhost',\n            port=0,\n            request_handler_class=TestWSGIRequestHandler,\n        )\n        server_thread.start()\n        try:\n            with self.urlopen('/example_view/') as f:\n                self.assertEqual(f.read(), b'example view')\n            self.assertTrue(server_thread.server.RequestHandlerClass.log_message_called)\n            self.assertEqual(len(server_thread.server.RequestHandlerClass.log_message_args), 2)\n        finally:\n            server_thread.stop()\n", "    def test_get_internal_wsgi_application(self):\n        \"\"\"\n        Ensure get_internal_wsgi_application returns the WSGI application\n        as configured by the user in settings.WSGI_APPLICATION.\n        \"\"\"\n        from django.conf import settings\n        wsgi_app_path = getattr(settings, 'WSGI_APPLICATION')\n        self.assertEqual(get_internal_wsgi_application().__module__, wsgi_app_path.split('.')[-2])\n        self.assertEqual(get_internal_wsgi_application().__name__, wsgi_app_path.split('.')[-1])\n"], "sample_96": ["    def test_readonly_fields_is_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 'field1'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            id='admin.E034',\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_readonly_fields_is_list_or_tuple(self):\n        class Admin(ModelAdmin):\n            readonly_fields = 'name'\n\n        self.assertIsInvalid(\n            Admin, Band,\n            msg=\"The value of 'readonly_fields' must be a list or tuple.\",\n            id='admin.E034',\n            invalid_obj=Admin,\n        )\n", "    def test_readonly_fields_is_not_iterable(self):\n        class Admin(ModelAdmin):\n            readonly_fields = 'name'\n\n        self.assertIsInvalid(\n            Admin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            id='admin.E034',\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 'hello'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            'admin.E025'\n        )\n"], "sample_99": ["def test_trunc_func_with_timezone_and_dst(self):\n    start_datetime = datetime(2015, 3, 29, 1, 30, 50, 321)\n    end_datetime = datetime(2016, 3, 29, 1, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('hour')\n    test_datetime_kind('minute')\n    test_datetime_kind('second')\n", "def test_trunc_func_explicit_timezone_priority(self):\n    start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n    end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    melb = pytz.timezone('Australia/Melbourne')\n\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            day_melb=Trunc('start_datetime', 'day'),\n            day_utc=Trunc('start_datetime', 'day', tzinfo=timezone.utc),\n        ).order_by('start_datetime').get()\n        self.assertEqual(model.day_melb, truncate_to(start_datetime, 'day', melb))\n        self.assertEqual(model.day_utc, truncate_to(start_datetime, 'day', timezone.utc))\n", "def test_trunc_func_explicit_timezone_priority(self):\n    start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n    end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    melb = pytz.timezone('Australia/Melbourne')\n\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            day_melb=TruncDay('start_datetime'),\n            day_utc=TruncDay('start_datetime', tzinfo=timezone.utc),\n        ).order_by('start_datetime').get()\n        self.assertEqual(model.day_melb, truncate_to(start_datetime, 'day', melb))\n        self.assertEqual(model.day_utc, truncate_to(start_datetime, 'day', timezone.utc))\n", "def test_trunc_func_with_timezone_and_subquery(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n    qs = DTModel.objects.annotate(\n        truncated=Trunc(\n            'start_datetime',\n            'day',\n            output_field=DateTimeField(),\n            tzinfo=melb,\n        ),\n    ).filter(truncated__in=Subquery(DTModel.objects.values('start_datetime')))\n\n    self.assertEqual(qs.count(), 2)\n", "def test_trunc_func_explicit_timezone_priority(self):\n    start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n    end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    melb = pytz.timezone('Australia/Melbourne')\n\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            day_melb=Trunc('start_datetime', 'day'),\n            day_utc=Trunc('start_datetime', 'day', tzinfo=timezone.utc),\n        ).order_by('start_datetime').get()\n        self.assertEqual(model.day_melb, truncate_to(start_datetime, 'day', melb))\n        self.assertEqual(model.day_utc, truncate_to(start_datetime, 'day', timezone.utc))\n"], "sample_97": ["    def test_ensure_echo_on_when_termios_available(self, mocked_termios):\n        with mock.patch.object(sys.stdin, 'isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_does_nothing_if_termios_is_none(self, mocked_termios):\n        mocked_termios.__bool__.return_value = False\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcgetattr.called)\n", "    def test_ensure_echo_on_if_termios_available(self, mocked_termios):\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n            self.assertTrue(mocked_termios.tcgetattr.called)\n            self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_trigger_reload_calls_sys_exit(self):\n        with mock.patch('sys.exit') as mocked_exit:\n            autoreload.trigger_reload('test.py')\n        self.assertEqual(mocked_exit.call_count, 1)\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n"], "sample_100": ["    def test_get_child_arguments(self):\n        with mock.patch('sys.argv', ['manage.py', 'runserver']):\n            args = autoreload.get_child_arguments()\n        self.assertEqual(args, [sys.executable] + ['-W%s' % o for o in sys.warnoptions] + ['manage.py', 'runserver'])\n", "    def test_not_a_tty(self, mocked_isatty):\n        autoreload.ensure_echo_on()\n        self.assertFalse(termios.called)\n", "    def test_signal_sent(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.utils.autoreload.autoreload_started.send') as mocked_send:\n            reloader.run(mock.MagicMock())\n        self.assertEqual(mocked_send.call_count, 1)\n        self.assertSequenceEqual(mocked_send.call_args[1], {'sender': reloader})\n", "def test_run_loop_stop_and_return_with_keyboard_interrupt(self):\n        yield\n        raise KeyboardInterrupt\n\n    with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n        self.reloader.run_loop()\n\n    self.assertEqual(tick.call_count, 1)\n", "    def test_non_tty_input(self, mocked_isatty):\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_isatty.called)\n"], "sample_102": ["def test_union_with_distinct_and_order(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    self.assertNumbersEqual(qs1.union(qs2).distinct().order_by('num'), range(10))\n    self.assertNumbersEqual(qs1.union(qs2).order_by('num').distinct(), range(10))\n", "def test_database_error_on_unsupported_union(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.none()\n    msg = 'union is not supported on this database backend'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        list(qs1.union(qs2))\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.filter(pk__in=[])\n    self.assertQuerysetEqual(qs1.union(qs2), qs1, ordered=False)\n    self.assertQuerysetEqual(qs2.union(qs1), qs1, ordered=False)\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.filter(pk__in=[])\n    self.assertEqual(len(qs1.union(qs2)), 10)\n    self.assertEqual(len(qs2.union(qs1)), 10)\n", "def test_values_list_union_with_different_annotation_names(self):\n    qs1 = Number.objects.annotate(annotation=Value(1, IntegerField())).values_list('num', 'annotation')\n    qs2 = Number.objects.annotate(other_annotation=Value(1, IntegerField())).values_list('num', 'other_annotation')\n    msg = \"Cannot combine queries once value() or values_list() has been called with annotations having different names\"\n    with self.assertRaisesMessage(ValueError, msg):\n        list(qs1.union(qs2))\n"], "sample_101": ["def test_limited_stream(self):\n    \"\"\"\n    LimitedStream does not read more than the specified limit.\n    \"\"\"\n    stream = BytesIO(b'Hello World!')\n    limited_stream = LimitedStream(stream, 5)\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(), b'')\n", "def test_limited_stream(self):\n    \"\"\"\n    LimitedStream limits the amount of data read from the underlying stream.\n    \"\"\"\n    class FakeStream:\n            self.data = data\n\n            if size is None:\n                return self.data\n            else:\n                return self.data[:size]\n\n    stream = FakeStream(b'Hello World!')\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(10), b'')\n", "def test_limited_stream(self):\n    \"\"\"\n    LimitedStream only reads up to its specified limit.\n    \"\"\"\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(), b'')\n", "def test_limited_stream(self):\n    \"\"\"\n    Test LimitedStream's read and readline methods.\n    \"\"\"\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(3), b'Hel')\n    self.assertEqual(limited_stream.read(3), b'lo,')\n    self.assertEqual(limited_stream.read(), b'')\n\n    stream = BytesIO(b'Hello\\nWorld!')\n    limited_stream = LimitedStream(stream, 10)\n\n    self.assertEqual(limited_stream.readline(), b'Hello\\n')\n    self.assertEqual(limited_stream.readline(), b'World!')\n    self.assertEqual(limited_stream.readline(), b'')\n", "def test_limited_stream(self):\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(), b'')\n    self.assertEqual(limited_stream.readline(), b'')\n"], "sample_103": ["def test_aggregate_over_subquery_annotation(self):\n    \"\"\"Aggregate over subquery annotation.\"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).aggregate(max_latest_book_pubdate=Max('latest_book_pubdate'))\n    self.assertIsNotNone(publisher_qs['max_latest_book_pubdate'])\n", "def test_aggregation_with_filter(self):\n    \"\"\"\n    Aggregations can be filtered.\n    \"\"\"\n    expensive_books = Count('book', filter=Q(book__price__gt=50))\n    publisher_qs = Publisher.objects.annotate(expensive_books=expensive_books)\n    a_press = publisher_qs.get(name='Apress')\n    self.assertEqual(a_press.expensive_books, 0)\n\n    p_hall = publisher_qs.get(name='Prentice Hall')\n    self.assertEqual(p_hall.expensive_books, 1)\n\n    # The filter can reference the annotation's field\n    affordable_books = Count('book', filter=Q(book__price__lte=F('num_awards') * 10))\n    publisher_qs = Publisher.objects.annotate(affordable_books=affordable_books)\n    a_press = publisher_qs.get(name='Apress')\n    self.assertEqual(a_press.affordable_books, 2)\n", "def test_aggregate_filter(self):\n    \"\"\"\n    Test that aggregate filters are correctly applied.\n    \"\"\"\n    books = Book.objects.aggregate(\n        count_all=Count('pk'),\n        count_expensive=Count('pk', filter=Q(price__gt=Decimal(\"40.0\"))),\n        count_not_expensive=Count('pk', filter=~Q(price__gt=Decimal(\"40.0\"))),\n    )\n    self.assertEqual(books['count_all'], 6)\n    self.assertEqual(books['count_expensive'], 2)\n    self.assertEqual(books['count_not_expensive'], 4)\n\n    publishers = Publisher.objects.annotate(\n        count_all=Count('book__pk'),\n        count_expensive=Count('book__pk', filter=Q(book__price__gt=Decimal(\"40.0\"))),\n    ).order_by('name')\n    expected_publishers = [\n        {'name': 'Apress', 'count_all': 2, 'count_expensive': 1},\n        {'name': \"Jonno's House of Books\", 'count_all': 0, 'count_expensive': 0},\n        {'name': 'Morgan Kaufmann', 'count_all': 1, 'count_expensive': 1},\n        {'name': 'Prentice Hall', 'count_all': 2, 'count_expensive': 1},\n        {'name': 'Sams', 'count_all': 1, 'count_expensive': 0},\n    ]\n    self.assertEqual(list(publishers.values('name', 'count_all', 'count_expensive')), expected_publishers)\n", "def test_aggregation_with_case(self):\n    \"\"\"\n    Aggregations can be used with Case expressions.\n    \"\"\"\n    from django.db.models import Case, When\n\n    case = Case(\n        When(pages__gt=300, then=1),\n        When(pages__gt=200, then=2),\n        default=3,\n        output_field=IntegerField(),\n    )\n\n    vals = Book.objects.aggregate(Sum(case))\n    self.assertEqual(vals[case.name + '__sum'], 13)\n", "def test_aggregate_over_subquery_annotation(self):\n    \"\"\"Aggregates can be used over subquery annotations.\"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).aggregate(Max('latest_book_pubdate'))\n    self.assertIsInstance(publisher_qs['latest_book_pubdate__max'], datetime.date)\n"], "sample_104": ["    def setUp(self):\n        super().setUp()\n        self._max_post_process_passes = storage.staticfiles_storage.max_post_process_passes\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def test_max_post_process_passes(self):\n        configured_storage = storage.staticfiles_storage\n        # Clear cache to force rehashing of the files\n        configured_storage.hashed_files.clear()\n        # Simulate a corrupt chain of intermediate files by ensuring they don't\n        # resolve before the max post-process count, which would normally be\n        # high enough.\n        original_max_post_process_passes = configured_storage.max_post_process_passes\n        configured_storage.max_post_process_passes = 1\n        # File without intermediates that can be rehashed without a problem.\n        self.hashed_file_path('cached/css/img/window.png')\n        # File with too many intermediates to rehash with the low max\n        # post-process passes.\n        err_msg = \"The name 'cached/styles.css' could not be hashed with %r.\" % (configured_storage._wrapped,)\n        with self.assertRaisesMessage(ValueError, err_msg):\n            self.hashed_file_path('cached/styles.css')\n        # Restore max_post_process_passes to its original value\n        configured_storage.max_post_process_passes = original_max_post_process_passes\n", "    def test_manifest_with_non_ascii_characters(self):\n        filename = \"test/nonascii.css\"\n        hashed_name = self.hashed_file_path(filename)\n        # check if the manifest is filled correctly as expected\n        cache_key = storage.staticfiles_storage.hash_key(filename)\n        cached_name = storage.staticfiles_storage.hashed_files.get(cache_key)\n        self.assertEqual(hashed_name, cached_name)\n\n        # Clearing the cache to make sure we re-set it correctly in the url method\n        storage.staticfiles_storage.hashed_files.clear()\n        cached_name = storage.staticfiles_storage.hashed_files.get(cache_key)\n        self.assertIsNone(cached_name)\n\n        # Check that the manifest still contains the correct hashed name\n        manifest = storage.staticfiles_storage.load_manifest()\n        self.assertEqual(manifest[cache_key], hashed_name)\n", "    def test_manifest_load_save(self):\n        # Create multiple threads that load and save the manifest\n        threads = []\n        for _ in range(10):\n            thread = threading.Thread(target=self._load_save_manifest)\n            threads.append(thread)\n            thread.start()\n\n        for thread in threads:\n            thread.join()\n\n        # Check that the manifest was loaded and saved correctly\n        manifest = storage.staticfiles_storage.load_manifest()\n        self.assertIsNotNone(manifest)\n"], "sample_107": ["    def test_cleanse_setting_callable(self):\n            return \"This should not be displayed\"\n        cleansed = cleanse_setting('SETTING_NAME', callable_setting)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n", "    def test_cleanse_setting_callable(self):\n            return \"Hello, World!\"\n        self.assertEqual(cleanse_setting('SETTING_NAME', my_callable), CallableSettingWrapper(my_callable))\n", "    def test_cleanse_setting_for_callable(self):\n            return \"Hello, World!\"\n        cleansed = cleanse_setting('SETTING_NAME', my_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(my_callable))\n", "    def test_empty_path(self):\n        request = self.rf.get('/')\n        exception = Http404()\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n", "    def test_get_default_exception_reporter_filter(self):\n        filter = get_default_exception_reporter_filter()\n        self.assertIsInstance(filter, SafeExceptionReporterFilter)\n"], "sample_106": ["def test_patch_cache_control_max_age(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=3600)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600')\n\n    patch_cache_control(response, max_age=None)\n    self.assertEqual(response['Cache-Control'], '')\n\n    patch_cache_control(response, max_age='invalid')\n    self.assertEqual(response['Cache-Control'], '')\n", "def test_patch_cache_control_private_and_public(self):\n    response = HttpResponse()\n    patch_cache_control(response, private=True)\n    self.assertEqual(response['Cache-Control'], 'private')\n\n    patch_cache_control(response, public=True)\n    self.assertEqual(response['Cache-Control'], 'public')\n", "    def test_get_max_age(self):\n        response = HttpResponse()\n        self.assertIsNone(get_max_age(response))\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(get_max_age(response), 3600)\n", "    def test_patch_response_headers(self):\n        response = HttpResponse()\n        patch_response_headers(response)\n        self.assertIn('Expires', response)\n        self.assertIn('Cache-Control', response)\n", "def test_patch_cache_control_max_age_zero(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=0)\n    self.assertEqual(response['Cache-Control'], 'max-age=0')\n"], "sample_105": ["    def test_get_context_data(self):\n        class TestView(ContextMixin):\n                self.extra_context = {'extra': 'context'}\n\n        view = TestView()\n        context = view.get_context_data(key='value')\n        self.assertEqual(context, {'key': 'value', 'view': view, 'extra': 'context'})\n", "def test_head_method_not_allowed(self):\n    \"\"\"\n    Test a view which only allows POST and GET doesn't allow HEAD.\n    \"\"\"\n    class OnlyPostAndGetView(View):\n            return HttpResponse('This view only accepts POST and GET')\n\n            return HttpResponse('This view only accepts POST and GET')\n\n    response = OnlyPostAndGetView.as_view()(self.rf.head('/'))\n    self.assertEqual(response.status_code, 405)\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data(test_name='test_value')\n        self.assertEqual(context['test_name'], 'test_value')\n        self.assertIn('view', context)\n", "    def test_get_template_names(self):\n        view = TemplateResponseMixin()\n        with self.assertRaises(ImproperlyConfigured):\n            view.get_template_names()\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data()\n        self.assertIn('view', context)\n        self.assertIs(context['view'], view)\n"], "sample_108": ["def test_resolve_with_route_pattern(self):\n    match = resolve('/route-pattern/')\n    self.assertEqual(match.url_name, 'route-pattern')\n    self.assertEqual(match.route, 'route-pattern/')\n", "    def test_locale_prefix_pattern(self):\n        # Ensure the locale prefix pattern is correctly included in the URL.\n        match = resolve('/en/about/')\n        self.assertEqual(match.url_name, 'about')\n        self.assertEqual(match.kwargs, {'lang': 'en'})\n", "    def test_locale_prefix_pattern(self):\n        # Create a LocalePrefixPattern instance with prefix_default_language=True\n        locale_prefix = LocalePrefixPattern(prefix_default_language=True)\n        \n        # Test that the language prefix is correctly generated for the default language\n        self.assertEqual(locale_prefix.language_prefix, '')\n\n        # Test that the language prefix is correctly generated for a non-default language\n        with override_settings(LANGUAGE_CODE='fr'):\n            self.assertEqual(locale_prefix.language_prefix, 'fr/')\n\n        # Test that the locale prefix pattern matches the expected language prefix\n        match = locale_prefix.match('/fr/test/')\n        self.assertEqual(match, ('test/', (), {}))\n", "    def test_resolvermatch_repr(self):\n        match = resolve('/articles/2003/')\n        expected = (\n            \"ResolverMatch(func=django.urls.resolvers.tests.views.empty_view, \"\n            \"args=(), kwargs={}, url_name='articles-2003', app_names=[], \"\n            \"namespaces=[], route='articles/2003/')\"\n        )\n        self.assertEqual(repr(match), expected)\n", "    def test_resolvermatch_str(self):\n        match = resolve('/articles/2003/')\n        self.assertEqual(str(match), (\n            \"ResolverMatch(func=django.urls.resolvers.tests.views.empty_view, \"\n            \"args=(), kwargs={}, url_name='articles-2003', app_names=[], namespaces=[], route='articles/2003/')\"\n        ))\n"], "sample_109": ["def test_autocomplete_select_multiple(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option1 = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    selected_option2 = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    self.assertIn(selected_option1, output)\n    self.assertIn(selected_option2, output)\n", "def test_render_options_with_duplicates(self):\n    \"\"\"Duplicates in initial data are ignored.\"\"\"\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'featuring': [beatles.pk, beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    option = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    self.assertEqual(output.count(selected_option), 1)\n    self.assertIn(option, output)\n", "def test_render_options_with_special_characters(self):\n    band = Band.objects.create(name='AC/DC', style='rock')\n    form = AlbumForm(initial={'band': band.pk})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>AC/DC</option>' % band.pk\n    self.assertIn(selected_option, output)\n", "def test_label_and_url_for_value(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    rel = Album._meta.get_field('band').remote_field\n    w = ForeignKeyRawIdWidget(rel, admin.site)\n    label, url = w.label_and_url_for_value(beatles.pk)\n    self.assertEqual(label, 'The Beatles')\n    self.assertIn('/admin_widgets/band/%s/change/' % beatles.pk, url)\n", "def test_autocomplete_select_multiple(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option1 = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    selected_option2 = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    self.assertIn(selected_option1, output)\n    self.assertIn(selected_option2, output)\n    # Check that AutocompleteSelectMultiple widget allows multiple selections\n    widget = form['featuring'].field.widget\n    self.assertIsInstance(widget, AutocompleteSelectMultiple)\n    self.assertTrue(widget.allow_multiple_selected)\n"], "sample_111": ["def test_get_filters_params(self):\n    m = BandAdmin(Band, custom_site)\n    request = self.factory.get('/band/', data={'genres': 'Rock', 'name': 'The Beatles'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    filters_params = cl.get_filters_params()\n    self.assertEqual(filters_params, {'genres': 'Rock', 'name': 'The Beatles'})\n\n    # Test with ignored parameters\n    request = self.factory.get('/band/', data={'genres': 'Rock', 'name': 'The Beatles', ALL_VAR: ''})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    filters_params = cl.get_filters_params()\n    self.assertEqual(filters_params, {'genres': 'Rock', 'name': 'The Beatles'})\n\n    # Test with None as params\n    filters_params = cl.get_filters_params(params=None)\n    self.assertEqual(filters_params, {})\n", "def test_get_filters_params(self):\n    m = BandAdmin(Band, custom_site)\n    request = self.factory.get('/band/', data={'genres': '1', 'name': 'The Beatles'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    lookup_params = cl.get_filters_params()\n    self.assertEqual(lookup_params, {'genres': '1', 'name': 'The Beatles'})\n\n    # Ensure ignored parameters are excluded\n    request = self.factory.get('/band/', data={'genres': '1', ALL_VAR: 'True'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    lookup_params = cl.get_filters_params()\n    self.assertEqual(lookup_params, {'genres': '1'})\n", "def test_get_filters_params(self):\n    \"\"\"\n    Test that get_filters_params returns all params except IGNORED_PARAMS.\n    \"\"\"\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get('/child/', data={'o': '1', 'ot': 'desc', 'q': 'test', 'other': 'param'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    filters_params = cl.get_filters_params()\n    self.assertEqual(filters_params, {'other': 'param'})\n", "def test_changelist_sorting_by_expression(self):\n    \"\"\"\n    The changelist supports sorting by an expression.\n    \"\"\"\n    m = BandAdmin(Band, custom_site)\n    request = self.factory.get('/band/', data={'o': '-2'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {2: 'desc'})\n    self.assertEqual(cl.queryset.count(), Band.objects.all().count())\n", "def test_changelist_search_form_preserves_params(self):\n    \"\"\"\n    The changelist search form preserves all query parameters.\n    \"\"\"\n    m = BandAdmin(Band, custom_site)\n    request = self.factory.get('/band/', data={'genres': '1', SEARCH_VAR: 'test'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertContains(cl.search_form.as_p(), 'value=\"test\"')\n    self.assertContains(cl.search_form.as_p(), 'name=\"genres\" value=\"1\" type=\"hidden\"')\n"], "sample_110": ["    def setUpTestData(cls):\n        cls.event = Event.objects.create(title='Event 1')\n", "    def setUpTestData(cls):\n        Group.objects.create(name='Group 1')\n        Group.objects.create(name='Group 2')\n", "    def setUpTestData(cls):\n        cls.e1 = Event.objects.create(title='Event 1')\n        cls.e2 = Event.objects.create(title='Event 2')\n", "    def setUpTestData(cls):\n        cls.event1 = Event.objects.create(title='Event 1')\n        cls.event2 = Event.objects.create(title='Event 2')\n", "    def test_combine_expression(self):\n        expr1 = F('field1')\n        expr2 = F('field2')\n        combined_expr = expr1 + expr2\n        self.assertIsInstance(combined_expr, CombinedExpression)\n        self.assertEqual(combined_expr.connector, '+')\n"], "sample_112": ["def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should create a JSON object with the\n    prepopulated fields' information.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields_json', template_context)\n    prepopulated_fields = json.loads(template_context['prepopulated_fields_json'])\n    self.assertIsInstance(prepopulated_fields, list)\n    for field in prepopulated_fields:\n        self.assertIn('id', field)\n        self.assertIn('name', field)\n        self.assertIn('dependency_ids', field)\n        self.assertIn('dependency_list', field)\n        self.assertIn('maxLength', field)\n        self.assertIn('allowUnicode', field)\n", "def test_cell_count(self):\n    \"\"\"\n    The cell_count filter returns the correct number of cells for a tabular inline.\n    \"\"\"\n    modeladmin = ModelAdmin(Article, site)\n    inline_admin_formset = modeladmin.get_inline_formsets(request=self.request_factory.get('/'), obj=Article())[0]\n    inline_admin_form = inline_admin_formset.forms[0]\n    self.assertEqual(cell_count(inline_admin_form), 4)  # 1 hidden cell + 3 visible fields\n\n    # Test with a formset that has can_delete=True\n    inline_admin_formset.can_delete = True\n    self.assertEqual(cell_count(inline_admin_form), 5)  # 1 hidden cell + 3 visible fields + 1 delete checkbox\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should return a JSON object with\n    prepopulated fields information.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields_json', template_context)\n    self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n    try:\n        json.loads(template_context['prepopulated_fields_json'])\n    except ValueError:\n        self.fail(\"prepopulated_fields_json is not valid JSON\")\n", "def test_cell_count(self):\n    \"\"\"\n    The cell_count filter returns the correct number of cells for a tabular inline.\n    \"\"\"\n    modeladmin = ModelAdmin(Article, site)\n    inline_admin_formset = modeladmin.get_inline_formsets(None)[0]\n    inline_admin_form = inline_admin_formset.forms[0]\n    self.assertEqual(cell_count(inline_admin_form), 4)  # Hidden 'id' field + 3 fields\n\n    # Add a delete checkbox\n    inline_admin_formset.can_delete = True\n    self.assertEqual(cell_count(inline_admin_form), 5)\n\n    # Test with a different number of fields\n    modeladmin = ModelAdmin(Question, site)\n    inline_admin_formset = modeladmin.get_inline_formsets(None)[0]\n    inline_admin_form = inline_admin_formset.forms[0]\n    self.assertEqual(cell_count(inline_admin_form), 3)  # Hidden 'id' field + 2 fields\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should return a list of prepopulated fields.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields', template_context)\n    self.assertIn('prepopulated_fields_json', template_context)\n    self.assertIsInstance(template_context['prepopulated_fields'], list)\n    self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n"], "sample_113": ["    def test_trim_docstring(self):\n        tests = (\n            (\"   Hello world!  \", \"Hello world!\"),\n            (\"\\t\\tHello world!\\n\\n\", \"Hello world!\"),\n            (\"   Hello world!\\n   This is a test.\", \"Hello world!\\nThis is a test.\"),\n            (\"\\t\\tHello world!\\n\\t\\tThis is a test.\\n\\n\", \"Hello world!\\nThis is a test.\"),\n        )\n        for docstring, output in tests:\n            with self.subTest(docstring=docstring):\n                self.assertEqual(utils.trim_docstring(docstring), output)\n", "    def test_function_view(self):\n            pass\n        self.assertEqual(get_view_name(view_func), 'tests.test_get_view_name.view_func')\n", "    def test_parse_docstring(self):\n        docstring = \"\"\"This is the title.\n\n        This is the body.\n\n        :param foo: This is a parameter.\n        :return: This is the return value.\"\"\"\n        title, body, metadata = utils.parse_docstring(docstring)\n        self.assertEqual(title, \"This is the title.\")\n        self.assertEqual(body, \"This is the body.\")\n        self.assertEqual(metadata, {\"param foo\": \"This is a parameter.\", \"return\": \"This is the return value.\"})\n", "    def test_get_view_name(self):\n        view_func = lambda x: x\n        view_func.__module__ = 'mymodule'\n        view_func.__qualname__ = 'myview'\n        self.assertEqual(utils.get_view_name(view_func), 'mymodule.myview')\n", "    def test_parse_docstring(self):\n        docstring = \"\"\"This is the title\n\n        This is the body.\n        \n        :param foo: This is a parameter\n        :return: This is the return value\"\"\"\n        title, body, metadata = utils.parse_docstring(docstring)\n        self.assertEqual(title, \"This is the title\")\n        self.assertEqual(body, \"This is the body.\")\n        self.assertEqual(metadata, {\"param foo\": \"This is a parameter\", \"return\": \"This is the return value\"})\n"], "sample_114": ["def test_mti_inheritance_field_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=255)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"breed\", models.CharField(max_length=255)),\n    ], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ]), ModelState('app', 'Dog', [], bases=('app.Animal',))])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='animal', name='name')\n    self.assertOperationAttributes(changes, 'app', 0, 1, model_name='dog', name='breed')\n", "def test_add_check_constraint(self):\n    \"\"\"Test change detection of new CheckConstraints.\"\"\"\n    changes = self.get_changes([self.author_name], [self.author_name_check_constraint])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    added_constraint = models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', constraint=added_constraint)\n", "def test_add_mti_field(self):\n    \"\"\"\n    #24814 - Adding a field to a multi-table inheritance parent model\n    should not prompt for a default.\n    \"\"\"\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [\n        ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=255)),\n        ]),\n        Dog,\n    ])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name=\"name\")\n", "def test_mti_inheritance_field_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=255)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ]), Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='animal', name='name')\n", "def test_add_unique_constraint(self):\n    \"\"\"\n    Test change detection of new unique constraints.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_options])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelOptions\"])\n    options = {\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n    }\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options=options)\n    altered_author = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ], options=options)\n    changes = self.get_changes([altered_author], [altered_author.clone(\n        fields=[(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=100))],\n        options={\n            \"permissions\": [('can_hire', 'Can hire')],\n            \"verbose_name\": \"Authi\",\n            \"unique_together\": {('name',)},\n        }\n    )])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"author\", unique_together={('name',)})\n"], "sample_115": ["    def test_get_traceback_frame_variables(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().get('/')\n        frame = sys._getframe()\n        vars = reporter_filter.get_traceback_frame_variables(request, frame)\n        self.assertIsInstance(vars, list)\n        for name, value in vars:\n            self.assertIsInstance(name, str)\n            self.assertIsInstance(value, str)\n", "    def test_template_with_non_ascii_chars(self):\n        \"\"\"\n        The technical 404 template is correctly rendered even if it contains\n        non-ASCII characters.\n        \"\"\"\n        msg = \"Non-ASCII characters: \u00e9\u00e0\u00fc\"\n        with mock.patch.object(DebugPath, 'open', return_value=StringIO(msg)):\n            response = technical_404_response(mock.MagicMock(), mock.Mock())\n            self.assertContains(response, msg, status_code=404)\n", "    def test_get_safe_settings(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        settings_dict = reporter_filter.get_safe_settings()\n        self.assertIn('DEBUG', settings_dict)\n        self.assertEqual(settings_dict['SECRET_KEY'], reporter_filter.cleansed_substitute)\n", "    def test_get_traceback_frame_variables(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        tb_frame = inspect.currentframe()\n        tb_frame.f_locals['foo'] = 'bar'\n        request = RequestFactory().get('/test_view/')\n        result = reporter_filter.get_traceback_frame_variables(request, tb_frame)\n        self.assertEqual(list(result), [('foo', \"'bar'\")])\n", "    def testtechnical_404_response(self):\n        request = self.rf.get('/test_view/')\n        exception = Http404('Not found')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n"], "sample_116": ["    def test_non_string_fragment_name(self):\n        with self.assertRaises(TypeError):\n            make_template_fragment_key(123, ['abc'])\n", "    def test_without_vary_on(self):\n        key = make_template_fragment_key('a.fragment')\n        self.assertEqual(key, 'template.cache.a.fragment.d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_cache_versioning_get_set(self):\n        # set, using default version = 1\n        cache.set('answer1', 42)\n        self.assertEqual(cache.get('answer1'), 42)\n        self.assertEqual(cache.get('answer1', version=1), 42)\n        self.assertIsNone(cache.get('answer1', version=2))\n\n        # set, default version = 1, but manually override version = 2\n        cache.set('answer2', 42, version=2)\n        self.assertIsNone(cache.get('answer2'))\n        self.assertIsNone(cache.get('answer2', version=1))\n        self.assertEqual(cache.get('answer2', version=2), 42)\n", "def test_cache_fragment_name_collision(self):\n    key1 = make_template_fragment_key('foo.bar', ['abc'])\n    key2 = make_template_fragment_key('foo', ['bar.abc'])\n    self.assertNotEqual(key1, key2)\n", "    def test_without_vary_on(self):\n        key = make_template_fragment_key('a.fragment')\n        self.assertEqual(key, 'template.cache.cacheprefix.a.fragment.d41d8cd98f00b204e9800998ecf8427e')\n"], "sample_117": ["    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'unusable'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        self.assertHTMLEqual(\n            widget.render('name', value, {'id': 'id_password'}),\n            \"\"\"\n            <div id=\"id_password\">\n                <strong>{}</strong>\n            </div>\n            \"\"\".format(_(\"No password set.\"))\n        )\n", "    def test_bound_data(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertEqual(field.bound_data('new value', 'initial value'), 'initial value')\n"], "sample_118": ["def test_year_lookup_bounds(self):\n    # Create an article with a specific date\n    article = Article.objects.create(\n        headline='Article 8',\n        pub_date=datetime(2005, 7, 26),\n        author=self.au1,\n        slug='a8',\n    )\n\n    # Test year lookup bounds for datetime field\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year=2005),\n        [\n            '<Article: Article 5>',\n            '<Article: Article 6>',\n            '<Article: Article 4>',\n            '<Article: Article 2>',\n            '<Article: Article 3>',\n            '<Article: Article 7>',\n            '<Article: Article 1>',\n            '<Article: Article 8>',\n        ],\n        ordered=False\n    )\n\n    # Test year lookup bounds for date field\n    Season.objects.create(year=2005, gt=None)\n    self.assertQuerysetEqual(\n        Season.objects.filter(year=2005),\n        ['<Season: 2005>']\n    )\n", "def test_in_bulk_with_large_input(self):\n    # Create a large number of articles\n    for i in range(1001):\n        Article.objects.create(headline=f'Article {i}', pub_date=datetime(2005, 7, 26))\n    \n    # Test that in_bulk() works with a large input\n    ids = list(Article.objects.values_list('id', flat=True))\n    articles = Article.objects.in_bulk(ids)\n    self.assertEqual(len(articles), len(ids))\n", "def test_in_bulk_with_subquery(self):\n    inner_qs = Article.objects.filter(headline__startswith='Article')\n    articles = Article.objects.in_bulk(inner_qs.values_list('id', flat=True))\n    self.assertEqual(len(articles), 7)\n", "def test_lookup_collision_with_expression(self):\n    \"\"\"\n    Genuine field names don't collide with built-in lookup types when using\n    expressions.\n    \"\"\"\n    season_2009 = Season.objects.create(year=2009, gt=111)\n    season_2010 = Season.objects.create(year=2010, gt=222)\n    season_2011 = Season.objects.create(year=2011, gt=333)\n\n    self.assertEqual(\n        list(Season.objects.filter(gt__gt=222).values('year')),\n        [{'year': 2011}],\n    )\n    self.assertEqual(\n        list(Season.objects.filter(gt__exact=222).values('year')),\n        [{'year': 2010}],\n    )\n", "def test_in_bulk_with_unsupported_field(self):\n    msg = (\n        \"in_bulk()'s field_name must be a unique field but 'headline' isn't \"\n        \"unique. You can use 'headline__exact' or specify a custom lookup type.\"\n    )\n    with self.assertRaisesMessage(ValueError, msg):\n        Article.objects.in_bulk(['Article 1'], field_name='headline')\n"], "sample_119": ["def test_is_nullable(self):\n    query = Query(Author)\n    field = Author._meta.get_field('num')\n    self.assertFalse(query.is_nullable(field))\n\n    field = Author._meta.get_field('name')\n    self.assertTrue(query.is_nullable(field))\n", "def test_resolve_ref(self):\n    query = Query(Author)\n    ref = query.resolve_ref('num', allow_joins=True, reuse=None)\n    self.assertIsInstance(ref, SimpleCol)\n    self.assertEqual(ref.target, Author._meta.get_field('num'))\n    self.assertEqual(ref.alias, 'auth_author')\n", "def test_is_nullable(self):\n    query = Query(Author)\n    field = Author._meta.get_field('name')\n    self.assertFalse(query.is_nullable(field))\n\n    field = Item._meta.get_field('modified')\n    self.assertTrue(query.is_nullable(field))\n", "def test_clone_annotations(self):\n    query = Query(Item)\n    query.add_annotation(Lower('name'), alias='lower_name')\n    clone = query.clone()\n    clone.add_annotation(Lower('modified'), alias='lower_modified')\n    self.assertEqual(query.annotations, {'lower_name': Lower('name')})\n", "def test_clone_annotations(self):\n    query = Query(Item)\n    query.add_annotation(Lower('name'), alias='lower_name', is_summary=False)\n    clone = query.clone()\n    clone.add_annotation(Lower('description'), alias='lower_description', is_summary=False)\n    self.assertEqual(query.annotations, {'lower_name': Lower('name')})\n"], "sample_120": ["def test_serialize_decimal_context(self):\n    decimal.getcontext().prec = 28\n    decimal_value = decimal.Decimal('1.2345678901234567890123456789')\n    serialized_value, imports = MigrationWriter.serialize(decimal_value)\n    self.assertEqual(serialized_value, \"Decimal('1.2345678901234567890123456789')\")\n    self.assertEqual(imports, {'from decimal import Decimal'})\n", "def test_serialize_nested_functions(self):\n            return x + y\n        return inner_function\n\n    with self.assertRaisesMessage(ValueError, 'Could not find function inner_function in migrations.test_writer'):\n        self.serialize_round_trip(outer_function(1))\n", "def test_serialize_datetime_timezone(self):\n    tz = get_fixed_timezone(180)\n    dt = datetime.datetime(2014, 1, 1, 1, 1, tzinfo=tz)\n    string, imports = MigrationWriter.serialize(dt)\n    self.assertEqual(string, \"datetime.datetime(2014, 1, 1, 1, 1, tzinfo=utc)\")\n    self.assertEqual(imports, {'import datetime', 'from django.utils.timezone import utc'})\n", "def test_register_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    self.assertIn(complex, MigrationWriter._registry)\n    MigrationWriter.unregister_serializer(complex)\n    self.assertNotIn(complex, MigrationWriter._registry)\n\n    # Test unregistering a non-existent serializer\n    with self.assertRaises(KeyError):\n        MigrationWriter.unregister_serializer(complex)\n", "def test_serialize_set_with_unhashable_elements(self):\n    with self.assertRaises(TypeError):\n        self.serialize_round_trip({[1, 2], [3, 4]})\n"], "sample_121": ["    def test_swappable_model(self):\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPED_MODEL'\n\n        class Model(models.Model):\n            swapped = models.ForeignKey(SwappedModel, models.CASCADE)\n\n        self.assertEqual(Model.check(), [])\n", "    def test_swapped_model_checks(self):\n        class Model(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPED_MODEL'\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'TEST_SWAPPED_MODEL' references 'invalid_models_tests.Model', \"\n                \"which has not been installed or is abstract.\",\n                id='models.E002',\n            ),\n        ])\n", "    def test_swappable_model(self):\n        class Model(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPED_MODEL'\n\n        self.assertEqual(Model.check(), [])\n", "    def test_swappable_model(self):\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPED_MODEL'\n\n        class Model(models.Model):\n            swapped = models.ForeignKey(SwappedModel, models.CASCADE)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'TEST_SWAPPED_MODEL' references 'invalid_models_tests.SwappedModel', \"\n                \"which has not been installed, or is abstract.\",\n                id='models.E002',\n            ),\n        ])\n", "    def test_swapped_model(self):\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPED_MODEL'\n\n        class Model(models.Model):\n            swapped = models.ForeignKey(SwappedModel, models.CASCADE)\n\n        self.assertEqual(Model.check(), [])\n"], "sample_122": ["    def test_adds_headers(self):\n        response = HttpResponse()\n        patch_response_headers(response, cache_timeout=10)\n        self.assertIn('Expires', response)\n        self.assertIn('Cache-Control', response)\n", "def test_conditional_response_etag(self):\n    request = self.factory.get('/view/')\n    response = HttpResponse('Hello World!')\n    etag = '\"12345\"'\n    response['ETag'] = etag\n\n    # Simulate a request with an If-None-Match header.\n    request.META['HTTP_IF_NONE_MATCH'] = etag\n\n    # The response should be a 304 Not Modified.\n    conditional_response = get_conditional_response(request, etag=etag, response=response)\n    self.assertIsInstance(conditional_response, HttpResponseNotModified)\n\n    # Now simulate a request without the If-None-Match header.\n    request.META.pop('HTTP_IF_NONE_MATCH', None)\n\n    # The response should be the original response.\n    conditional_response = get_conditional_response(request, etag=etag, response=response)\n    self.assertIs(conditional_response, response)\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True, max_age=123)\n        self.assertEqual(response['Cache-Control'], 'public, max-age=123')\n", "def test_get_max_age(self):\n    response = HttpResponse()\n    self.assertIsNone(get_max_age(response))\n\n    response['Cache-Control'] = 'max-age=100'\n    self.assertEqual(get_max_age(response), 100)\n\n    response['Cache-Control'] = 'max-age=abc'\n    self.assertIsNone(get_max_age(response))\n\n    response['Cache-Control'] = 'no-cache, max-age=100'\n    self.assertEqual(get_max_age(response), 100)\n\n    response['Cache-Control'] = 'no-cache, max-age=abc'\n    self.assertIsNone(get_max_age(response))\n", "    def test_get_max_age(self):\n        response = HttpResponse()\n        self.assertIsNone(get_max_age(response))\n\n        patch_cache_control(response, max_age=100)\n        self.assertEqual(get_max_age(response), 100)\n\n        patch_cache_control(response, max_age='100')\n        self.assertEqual(get_max_age(response), 100)\n\n        patch_cache_control(response, max_age=None)\n        self.assertIsNone(get_max_age(response))\n"], "sample_123": ["    def test_fields_limit(self):\n        query_string = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(query_string, fields_limit=2), [('a', '1'), ('b', '2')])\n        with self.assertRaises(TooManyFieldsSent):\n            limited_parse_qsl(query_string, fields_limit=1)\n", "    def test_fields_limit(self):\n        query_string = 'a=1&b=2&c=3'\n        result = limited_parse_qsl(query_string, fields_limit=2)\n        self.assertEqual(result, [('a', '1'), ('b', '2')])\n", "    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date_str), parse_http_date(date_str))\n", "    def test_too_many_fields(self):\n        with self.assertRaisesMessage(TooManyFieldsSent, 'The number of GET/POST parameters exceeded settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'):\n            limited_parse_qsl('&'.join(f'key{i}=value{i}' for i in range(2001)), fields_limit=2000)\n", "    def test_fields_limit(self):\n        qs = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(qs), [('a', '1'), ('b', '2'), ('c', '3')])\n        with self.assertRaises(TooManyFieldsSent):\n            limited_parse_qsl(qs, fields_limit=2)\n"], "sample_125": ["    def test_content_type(self):\n        response = HttpResponse('content')\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n", "    def test_set_signed_cookie(self):\n        response = HttpResponse()\n        value = 'value'\n        key = 'key'\n        salt = 'salt'\n        response.set_signed_cookie(key, value, salt=salt)\n        self.assertIn(key, response.cookies)\n        signed_value = response.cookies[key].value\n        self.assertNotEqual(value, signed_value)\n", "    def test_default_reason_phrase(self):\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n", "    def test_serialize_headers(self):\n        response = HttpResponse()\n        response['Content-Type'] = 'text/plain'\n        response['X-Custom-Header'] = 'value'\n        self.assertEqual(\n            b'Content-Type: text/plain\\r\\nX-Custom-Header: value',\n            response.serialize_headers().rstrip(b'\\r\\n')\n        )\n", "    def test_content_type(self):\n        response = HttpResponse()\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n"], "sample_126": ["def test_mti_inheritance_field_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=100)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ]), Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='animal', name='name')\n", "def test_alter_model_table_on_unmanaged_model(self):\n    \"\"\"Tests detection for changing db_table in unmanaged model's options.\"\"\"\n    changes = self.get_changes([self.author_unmanaged_custom_pk], [self.author_unmanaged_default_pk])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"authorunmanaged\", table=None)\n", "def test_mti_inheritance_field_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=100)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ]), Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='name')\n", "def test_mti_inheritance_model_addition(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal], [Animal, Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "def test_deep_deconstruct_with_lambda(self):\n    model_state = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"field\",\n                models.CharField(max_length=255, validators=[lambda x: x.startswith(\"test\")]),\n            ),\n        ],\n    )\n    changes = self.get_changes([], [model_state])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"CreateModel\"])\n    # Make sure the lambda function is correctly deconstructed\n    operations = changes['testapp'][0].operations\n    self.assertEqual(len(operations), 1)\n    self.assertIsInstance(operations[0], operations.CreateModel)\n    field = next((f for f in operations[0].fields if f[0] == \"field\"), None)\n    self.assertIsNotNone(field)\n    self.assertEqual(field[1].validators, [lambda x: x.startswith(\"test\")])\n"], "sample_127": ["def test_bulk_update(self):\n    countries = [\n        Country(name='France', iso_two_letter='FR'),\n        Country(name='Germany', iso_two_letter='DE'),\n        Country(name='Italy', iso_two_letter='IT'),\n    ]\n    Country.objects.bulk_create(countries)\n    Country.objects.bulk_update(countries, ['name'])\n    for country in countries:\n        self.assertEqual(Country.objects.get(pk=country.pk).name, country.name)\n    # Test update of a single field.\n    Country.objects.bulk_update(countries, ['iso_two_letter'])\n    for country in countries:\n        self.assertEqual(Country.objects.get(pk=country.pk).iso_two_letter, country.iso_two_letter)\n    # Test update of multiple fields.\n    Country.objects.bulk_update(countries, ['name', 'iso_two_letter'])\n    for country in countries:\n        self.assertEqual(Country.objects.get(pk=country.pk).name, country.name)\n        self.assertEqual(Country.objects.get(pk=country.pk).iso_two_letter, country.iso_two_letter)\n    # Test with batch_size parameter.\n    countries[0].name = 'Spain'\n    countries[1].iso_two_letter = 'PT'\n    Country.objects.bulk_update(countries, ['name', 'iso_two_letter'], batch_size=2)\n    for country in countries:\n        self.assertEqual(Country.objects.get(pk=country.pk).name, country.name)\n        self.assertEqual(Country.objects.get(pk=country.pk).iso_two_letter, country.iso_two_letter)\n", "def test_bulk_create_with_field_defaults(self):\n    countries = Country.objects.bulk_create([Country(name='France')])\n    self.assertEqual(len(countries), 1)\n    country = countries[0]\n    self.assertEqual(country.iso_two_letter, '')\n    self.assertEqual(country.description, '')\n    self.assertEqual(Country.objects.get(pk=country.pk), country)\n", "def test_bulk_create_nullable_fields_with_default(self):\n    # If a model has nullable fields with default values, then bulk_create\n    # should respect those defaults when NULL is passed.\n    obj = NullableFields.objects.create()\n    self.assertIsNotNone(obj.f1)\n    self.assertEqual(obj.f2, '')\n    bulk_objs = [NullableFields(f1=None), NullableFields(f2=None)]\n    NullableFields.objects.bulk_create(bulk_objs)\n    for obj in NullableFields.objects.filter(id__gt=1):\n        self.assertIsNotNone(obj.f1)\n        self.assertEqual(obj.f2, '')\n", "def test_bulk_create_with_nullable_foreign_key(self):\n    restaurant = Restaurant.objects.create(name='Test')\n    restaurants = [\n        Restaurant(name='Foo', parent=restaurant),\n        Restaurant(name='Bar', parent=None),\n        Restaurant(name='Baz'),\n    ]\n    Restaurant.objects.bulk_create(restaurants)\n    self.assertEqual(Restaurant.objects.count(), 4)\n    self.assertEqual(Restaurant.objects.filter(parent=restaurant).count(), 1)\n    self.assertEqual(Restaurant.objects.filter(parent__isnull=True).count(), 2)\n", "def test_bulk_create_with_inheritance_and_no_model_fields(self):\n    # This test ensures that bulk_create works correctly for a model with a\n    # parent that has no fields.\n    NoFields.objects.bulk_create([NoFields()])\n    self.assertEqual(NoFields.objects.count(), 1)\n"], "sample_128": ["    def test_covering_index_with_opclasses(self):\n        index = Index(\n            name='covering_headline_idx',\n            fields=['headline'],\n            opclasses=['varchar_pattern_ops'],\n            include=['pub_date', 'published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s varchar_pattern_ops) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "def test_clone(self):\n    index = Index(\n        name='test_clone',\n        fields=['headline'],\n        include=['pub_date'],\n        condition=Q(pub_date__isnull=False),\n    )\n    cloned_index = index.clone()\n    self.assertEqual(index.name, cloned_index.name)\n    self.assertEqual(index.fields, cloned_index.fields)\n    self.assertEqual(index.include, cloned_index.include)\n    self.assertEqual(index.condition, cloned_index.condition)\n", "    def test_covering_index_with_opclasses(self):\n        index = Index(\n            name='covering_headline_idx',\n            fields=['headline'],\n            opclasses=['varchar_pattern_ops'],\n            include=['pub_date', 'published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s varchar_pattern_ops) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "def test_index_deconstruction(self):\n    index = Index(fields=['headline', '-pub_date'], name='test_idx')\n    path, args, kwargs = index.deconstruct()\n    self.assertEqual(path, 'django.db.models.Index')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'fields': ['headline', '-pub_date'], 'name': 'test_idx'})\n    reconstructed_index = Index(**kwargs)\n    self.assertEqual(index.fields, reconstructed_index.fields)\n    self.assertEqual(index.name, reconstructed_index.name)\n", "def test_index_clone(self):\n    index = Index(\n        name='test_idx',\n        fields=['headline', '-pub_date'],\n        opclasses=['varchar_pattern_ops', 'date_trunc'],\n        condition=Q(pub_date__isnull=False),\n        include=['published'],\n    )\n    cloned_index = index.clone()\n    self.assertEqual(index.name, cloned_index.name)\n    self.assertEqual(index.fields, cloned_index.fields)\n    self.assertEqual(index.opclasses, cloned_index.opclasses)\n    self.assertEqual(index.condition, cloned_index.condition)\n    self.assertEqual(index.include, cloned_index.include)\n    self.assertIsNot(index, cloned_index)\n"], "sample_129": ["    def test_add01(self):\n        output = self.engine.render_to_string('add01', {\"a\": 5})\n        self.assertEqual(output, '10')\n", "    def test_addslashes01(self):\n        output = self.engine.render_to_string('addslashes01', {\"a\": \"foo'bar\"})\n        self.assertEqual(output, \"foo\\\\'bar\")\n", "    def test_addslashes01(self):\n        output = self.engine.render_to_string('addslashes01', {\"a\": \"Hello 'world'!\", \"b\": mark_safe(\"Hello 'world'!\")})\n        self.assertEqual(output, \"Hello \\\\'world\\\\'! Hello \\\\'world\\\\'!\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string('stringformat01', {\"a\": 1.42, \"b\": \"1.42\"})\n        self.assertEqual(output, \"1.420000e+00 1.42\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string('add01', {\"a\": 10})\n        self.assertEqual(output, '15')\n"], "sample_130": ["def test_add_filter(self):\n    query = Query(Author)\n    query.add_filter(('num__gt', 2))\n    where = query.where\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_get_loaded_field_names(self):\n    query = Query(Author)\n    self.assertEqual(query.get_loaded_field_names(), {})\n    query.add_deferred_loading(['name'])\n    loaded_fields = query.get_loaded_field_names()\n    self.assertIn(Author, loaded_fields)\n    self.assertEqual(loaded_fields[Author], {'id', 'num'})\n    query.clear_deferred_loading()\n    self.assertEqual(query.get_loaded_field_names(), {})\n", "def test_check_related_objects(self):\n    query = Query(Item)\n    with self.assertRaises(ValueError):\n        query.check_related_objects(Author._meta.get_field('name'), 'string', Author._meta)\n\n    query.check_related_objects(Author._meta.get_field('name'), Author.objects.all(), Author._meta)\n\n    query.check_related_objects(Item._meta.get_field('creator'), Author.objects.all(), Item._meta)\n", "def test_add_annotation(self):\n    query = Query(Item)\n    annotation = F('modified')\n    query.add_annotation(annotation, 'modified_alias', is_summary=True)\n    self.assertIn('modified_alias', query.annotations)\n    self.assertEqual(query.annotations['modified_alias'], annotation)\n    self.assertEqual(query.annotation_select_mask, {'modified_alias'})\n", "def test_add_annotation(self):\n    query = Query(Author)\n    annotation = F('num') + 1\n    query.add_annotation(annotation, 'new_num', is_summary=True)\n    self.assertIn('new_num', query.annotations)\n    self.assertEqual(query.annotations['new_num'], annotation)\n    self.assertIn('new_num', query.annotation_select_mask)\n"], "sample_131": ["def test_create_test_db_clone_settings(self):\n    # Test that get_test_db_clone_settings correctly modifies the connection settings dict.\n    test_connection = self.get_connection_copy()\n    test_database_name = 'test_hodor'\n    test_connection.settings_dict['NAME'] = test_database_name\n    suffix = '_clone1'\n    clone_settings = BaseDatabaseCreation(test_connection).get_test_db_clone_settings(suffix)\n    self.assertEqual(clone_settings['NAME'], test_database_name + suffix)\n    self.assertEqual(clone_settings['ENGINE'], test_connection.settings_dict['ENGINE'])\n    self.assertEqual(clone_settings['HOST'], test_connection.settings_dict['HOST'])\n    self.assertEqual(clone_settings['PORT'], test_connection.settings_dict['PORT'])\n", "    def test_clone_settings(self, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        suffix = 'test_suffix'\n        clone_settings = creation.get_test_db_clone_settings(suffix)\n        self.assertEqual(clone_settings['NAME'], '{}_{}'.format(connection.settings_dict['NAME'], suffix))\n        for key, value in connection.settings_dict.items():\n            if key != 'NAME':\n                self.assertEqual(clone_settings[key], value)\n", "    def test_serialize_db_to_string(self, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(serializers, 'serialize') as mocked_serialize:\n            creation.serialize_db_to_string()\n            mocked_serialize.assert_called_once_with(\n                \"json\",\n                mock.ANY,\n                indent=None,\n                stream=mock.ANY\n            )\n", "    def test_sql_table_creation_suffix(self, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        self.assertEqual(creation.sql_table_creation_suffix(), '')\n", "    def test_get_test_db_clone_settings(self, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        suffix = 'clone1'\n        clone_settings = creation.get_test_db_clone_settings(suffix)\n        self.assertEqual(clone_settings['NAME'], '{}_{}'.format(connection.settings_dict['NAME'], suffix))\n"], "sample_132": ["    def test_custom_exception_reporter_filter(self):\n        request = RequestFactory().get('/test_view/')\n        exception = Http404('Page not found')\n        filter = CustomExceptionReporterFilter()\n        with mock.patch('django.views.debug.get_default_exception_reporter_filter', return_value=filter):\n            response = technical_404_response(request, exception)\n            self.assertContains(response, 'Page not found', status_code=404)\n", "    def test_templated_response(self):\n        exception = Http404('Template response')\n        request = RequestFactory().get('/raises404/')\n        response = technical_404_response(request, exception)\n        self.assertContains(response, '<h1>Page not found <span>(404)</span></h1>', status_code=404)\n", "    def test_url_pattern_with_regex(self):\n        response = self.client.get('/regex-post/12345/')\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, 'Page not found <span>(404)</span>', status_code=404)\n", "    def test_technical_404_response(self):\n        exception = Http404('Page not found')\n        request = self.rf.get('/test_view/')\n        response = technical_404_response(request, exception)\n        self.assertContains(response, 'Page not found <span>(404)</span>', status_code=404)\n", "    def test_technical_404_response(self):\n        request = self.rf.get('/test_view/')\n        exception = Http404('Not found')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, 'Page not found', status_code=404)\n"], "sample_133": ["def test_jsoncatalog_with_app_label(self):\n    \"\"\"\n    The json_catalog view returns the catalog for a specific app.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='fr'), override('fr'):\n        response = self.client.get('/jsoni18n/app1/')\n        data = json.loads(response.content.decode())\n        self.assertIn('catalog', data)\n        self.assertIn('formats', data)\n        self.assertIn('plural', data)\n        self.assertEqual(data['catalog']['app1 string'], 'il faut traduire cette cha\u00eene de caract\u00e8res de app1')\n", "def test_get_formats_returns_all_format_strings(self):\n    \"\"\"get_formats() returns all format strings required for i18n to work.\"\"\"\n    formats = get_formats()\n    expected_format_settings = (\n        'DATE_FORMAT', 'DATETIME_FORMAT', 'TIME_FORMAT',\n        'YEAR_MONTH_FORMAT', 'MONTH_DAY_FORMAT', 'SHORT_DATE_FORMAT',\n        'SHORT_DATETIME_FORMAT', 'FIRST_DAY_OF_WEEK', 'DECIMAL_SEPARATOR',\n        'THOUSAND_SEPARATOR', 'NUMBER_GROUPING',\n        'DATE_INPUT_FORMATS', 'TIME_INPUT_FORMATS', 'DATETIME_INPUT_FORMATS'\n    )\n    self.assertEqual(set(formats.keys()), set(expected_format_settings))\n", "    def test_get_paths(self):\n        view = JavaScriptCatalog()\n        packages = ['django.contrib.admin', 'django.contrib.auth']\n        paths = view.get_paths(packages)\n        self.assertEqual(len(paths), 2)\n        for package in packages:\n            app_config = apps.get_app_config(package)\n            self.assertIn(os.path.join(app_config.path, 'locale'), paths)\n", "    def test_get_plural(self):\n        view = JavaScriptCatalog()\n        view._plural_string = 'nplurals=2; plural=(n != 1);'\n        self.assertEqual(view.get_plural(), '(n != 1)')\n", "def test_get_plural(self):\n    view = JavaScriptCatalog()\n    view._plural_string = 'nplurals=3; plural=(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2);'\n    self.assertEqual(view.get_plural(), '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)')\n    view._plural_string = None\n    self.assertIsNone(view.get_plural())\n"], "sample_135": ["def test_f_format(self):\n    dt = datetime(2022, 7, 8, 22, 0)\n    self.assertEqual(dateformat.format(dt, 'f'), '10 p.m.')\n\n    dt = datetime(2022, 7, 8, 22, 30)\n    self.assertEqual(dateformat.format(dt, 'f'), '10:30 p.m.')\n", "def test_format_with_unsupported_timezone(self):\n    # Test that an empty string is returned when timezone information is not available.\n    dt = datetime(2009, 5, 16, 5, 30, 30)\n    tf = TimeFormat(dt)\n    tf.timezone = None\n    self.assertEqual(tf.e(), \"\")\n    self.assertEqual(tf.O(), \"\")\n    self.assertEqual(tf.T(), \"\")\n    self.assertEqual(tf.Z(), \"\")\n", "def test_E_format(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, 'E'), 'July')\n\n    # Test alternative month names in different locales.\n    with translation.override('fr'):\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'juillet')\n", "def test_E_format(self):\n    # Test alternative month names as required by some locales.\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, 'E'), 'July')\n\n    # Ensure the translation is activated for this test.\n    with translation.override('fr'):\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'juillet')\n", "def test_escaped_format_characters(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, r'\\a \\A'), 'a A')\n    self.assertEqual(dateformat.format(my_birthday, r'\\b \\B'), 'b B')\n    self.assertEqual(dateformat.format(my_birthday, r'\\d \\D'), 'd D')\n"], "sample_134": ["def test_serialize_operation(self):\n    operation = migrations.RunPython(lambda apps, schema_editor: None)\n    writer = OperationWriter(operation, indentation=0)\n    output, imports = writer.serialize()\n    self.assertEqual(output, \"migrations.RunPython(lambda apps, schema_editor: None),\")\n    self.assertEqual(imports, set())\n", "def test_serialize_type(self):\n    self.assertSerializedEqual(type)\n    self.assertSerializedResultEqual(\n        type,\n        ('type', set())\n    )\n    self.assertSerializedEqual(models.Model)\n    self.assertSerializedResultEqual(\n        models.Model,\n        (\"models.Model\", {\"from django.db import models\"})\n    )\n", "def test_serialize_operation_with_nested_model(self):\n    class NestedModel(models.Model):\n        pass\n\n    operation = migrations.CreateModel(\n        name='MyModel',\n        fields=[\n            ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n            ('nested', models.ForeignKey(on_delete=models.CASCADE, to='NestedModel')),\n        ],\n    )\n\n    writer = MigrationWriter(operation)\n    output = writer.as_string()\n    result = self.safe_exec(output)\n\n    self.assertIn(\"NestedModel\", result)\n", "def test_serialize_lazy_translation(self):\n    lazy_translation = _('Hello, world!')\n    self.assertSerializedEqual(lazy_translation)\n    string, imports = MigrationWriter.serialize(lazy_translation)\n    self.assertEqual(string, \"'Hello, world!'\")\n    self.assertEqual(imports, set())\n", "def test_serialize_lazy_translated_objects(self):\n    # Test that lazy translated objects are correctly serialized.\n    lazy_translated_object = _('Hello, world!')\n    string, imports = MigrationWriter.serialize(lazy_translated_object)\n    self.assertEqual(string, \"'Hello, world!'\")\n    self.assertEqual(imports, set())\n"], "sample_139": ["def test_changelist_view_search_box(self):\n    \"\"\"\n    The search box is displayed on the changelist view.\n    \"\"\"\n    superuser = self._create_superuser('superuser')\n    m = EventAdmin(Event, custom_site)\n    request = self._mocked_authenticated_request('/event/', superuser)\n    response = m.changelist_view(request)\n    self.assertContains(response, 'Search:')\n    self.assertContains(response, 'name=\"q\"')\n    self.assertContains(response, 'value=\"\"')\n", "def test_change_list_queryset_defer(self):\n    \"\"\"\n    The changelist's queryset can defer fields to optimize database queries.\n    \"\"\"\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get('/child/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    qs = cl.get_queryset(request)\n    self.assertIs(qs.query.deferred_loading[0], ['name'])\n", "def test_changelist_view_uses_correct_template(self):\n    \"\"\"\n    Ensure that the correct template is used for rendering the changelist view.\n    \"\"\"\n    superuser = self._create_superuser('superuser')\n    m = EventAdmin(Event, custom_site)\n    request = self._mocked_authenticated_request('/event/', superuser)\n    response = m.changelist_view(request)\n    self.assertTemplateUsed(response, 'admin/admin_changelist/event/change_list.html')\n", "def test_changelist_view_sorting(self):\n    \"\"\"\n    The changelist view should sort the objects according to the 'o' GET parameter.\n    \"\"\"\n    self.client.force_login(self.superuser)\n    event1 = Event.objects.create(date=datetime.date.today())\n    event2 = Event.objects.create(date=datetime.date.today() + datetime.timedelta(days=1))\n    event3 = Event.objects.create(date=datetime.date.today() - datetime.timedelta(days=1))\n\n    # Test default sorting (descending by date)\n    response = self.client.get(reverse('admin:admin_changelist_event_changelist'))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context_data['cl'].result_list, [event2, event1, event3])\n\n    # Test sorting by date in ascending order\n    response = self.client.get(reverse('admin:admin_changelist_event_changelist') + '?o=1')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context_data['cl'].result_list, [event3, event1, event2])\n\n    # Test sorting by date in descending order\n    response = self.client.get(reverse('admin:admin_changelist_event_changelist') + '?o=-1')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context_data['cl'].result_list, [event2, event1, event3])\n", "def test_changelist_view_list_editable_redirects_after_save(self):\n    \"\"\"\n    After saving changes to list_editable fields on the changelist page,\n    redirect back to the changelist page.\n    \"\"\"\n    a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n    superuser = self._create_superuser('superuser')\n    self.client.force_login(superuser)\n    changelist_url = reverse('admin:admin_changelist_swallow_changelist')\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '1',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '1000',\n        'form-0-uuid': str(a.pk),\n        'form-0-load': '10',\n        '_save': 'Save',\n    }\n    response = self.client.post(changelist_url, data, follow=True)\n    self.assertRedirects(response, changelist_url)\n    self.assertEqual(response.status_code, 200)\n"], "sample_137": ["def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = r'^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected_output = r'^<a>/b/(\\w+)$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    expected_output = r'^<a>/b/<c>'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected_output = '^<a>/b/(\\\\w+)$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = '^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)'\n    expected_output = '^<a>/b/(\\\\w+)'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    expected_output = '^<a>/b/<c>'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected_output = r'^<a>/b/(\\w+)$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = r'^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = r'^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected_output = r'^<a>/b/(\\w+)$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    expected_output = r'^<a>/b/<c>'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = '^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected_output = '^<a>/b/(\\w+)$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n"], "sample_138": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def test_path_with_fragment_and_querystring(self):\n        relpath = self.hashed_file_path(\"cached/css/fragments.css?query#fragment\")\n        self.assertEqual(relpath, \"cached/css/fragments.a60c0e74834f.css?query#fragment\")\n        with storage.staticfiles_storage.open(\"cached/css/fragments.a60c0e74834f.css\") as relfile:\n            content = relfile.read()\n            self.assertIn(b'fonts/font.b9b105392eb8.eot?#iefix', content)\n            self.assertIn(b'fonts/font.b8d603e42714.svg#webfontIyfZbseF', content)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        storage.staticfiles_storage.hashed_files.clear()\n", "    def test_hash_key(self):\n        relpath = self.hashed_file_path(\"cached/styles.css\")\n        cache_name = storage.staticfiles_storage.stored_name(relpath)\n        hash_key = storage.staticfiles_storage.hash_key(relpath)\n        self.assertEqual(hash_key, storage.staticfiles_storage.clean_name(relpath))\n        self.assertIn(hash_key, storage.staticfiles_storage.hashed_files)\n        self.assertEqual(storage.staticfiles_storage.hashed_files[hash_key], cache_name)\n"], "sample_140": ["    def test_sensitive_variables_all(self):\n        @sensitive_variables()\n            raise Exception('Test')\n        try:\n            test_func('secret', 'user123')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('password', html)\n        self.assertNotIn('secret', html)\n        self.assertIn('username', html)\n        self.assertNotIn('user123', html)\n", "    def test_sensitive_variables(self):\n        @sensitive_variables('password')\n            raise Exception\n\n        try:\n            test_func('secret')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('<td>password</td>', html)\n        self.assertIn('<td class=\"code\"><pre>***</pre></td>', html)\n", "    def test_sensitive_variables_all(self):\n        @sensitive_variables()\n            raise Exception('Test')\n        try:\n            test_func('secret', 'user123')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertNotIn('password', html)\n        self.assertNotIn('username', html)\n", "    def test_cleanse_setting_with_bytes(self):\n        self.assertEqual(cleanse_setting('TEST', b'TEST'), 'TEST')\n        self.assertEqual(cleanse_setting('PASSWORD', b'super_secret'), CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_with_bytes(self):\n        self.assertEqual(cleanse_setting('PASSWORD', b'super_secret'), CLEANSED_SUBSTITUTE)\n"], "sample_141": ["    def test_repr(self):\n        obj = DeserializedObject(object(), m2m_data={'field': [1, 2]})\n        self.assertEqual(repr(obj), '<DeserializedObject: object(pk=None)>')\n", "    def test_deferred_fields(self):\n        test_string = \"\"\"[{\n            \"pk\": 1,\n            \"model\": \"serializers.article\",\n            \"fields\": {\n                \"headline\": \"Deferred fields\",\n                \"pub_date\": \"2006-06-16T15:00:00\",\n                \"categories\": [1],\n                \"author\": 1\n            }\n        },\n        {\n            \"pk\": 1,\n            \"model\": \"serializers.category\",\n            \"fields\": {\n                \"name\": \"Reference\"\n            }\n        },\n        {\n            \"pk\": 1,\n            \"model\": \"serializers.author\",\n            \"fields\": {\n                \"name\": \"Agnes\"\n            }\n        }]\"\"\"\n\n        objs = list(serializers.deserialize('json', test_string))\n        article = [obj for obj in objs if obj.object.__class__.__name__ == 'Article'][0]\n\n        # Simulate deferred fields by deleting the field value from the object\n        del article.object.author\n\n        # Save the deserialized objects, but don't save the m2m data yet\n        for obj in objs:\n            obj.save(save_m2m=False)\n\n        # Now, let's try to save the m2m data (which should work even though\n        # the author field is deferred)\n        article.save_deferred_fields()\n", "    def test_progress_bar(self):\n        output = StringIO()\n        progress_bar = serializers.base.ProgressBar(output, 5)\n        for i in range(5):\n            progress_bar.update(i + 1)\n        self.assertEqual(output.getvalue().count('\\n'), 1)\n", "def test_build_instance_with_natural_key(self):\n    model = Score\n    data = {'name': 'Test Score'}\n    db = None\n\n    instance = build_instance(model, data, db)\n\n    self.assertIsInstance(instance, Score)\n    self.assertEqual(instance.name, 'Test Score')\n\n    # Test with natural key\n    model = Score\n    data = {'name': 'Test Score'}\n    db = None\n\n    instance = build_instance(model, data, db)\n\n    self.assertIsInstance(instance, Score)\n    self.assertEqual(instance.name, 'Test Score')\n", "    def test_save_deferred_fields(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=10)\n            related = models.ForeignKey('self', on_delete=models.CASCADE, null=True)\n\n        obj = MyModel(name='Test')\n        deserialized_obj = DeserializedObject(obj, deferred_fields={'related': 1})\n\n        # Simulate a database where MyModel with id 1 exists\n        existing_obj = MyModel.objects.create(id=1, name='Existing')\n\n        deserialized_obj.save_deferred_fields()\n\n        # Check that the related field is correctly set to the existing object\n        self.assertEqual(deserialized_obj.object.related, existing_obj)\n"], "sample_142": ["def test_model_form_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        pass\n\n    self.assertFalse(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            exclude = []\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n", "def test_modelform_defines_fields_with_both_fields_and_exclude(self):\n    class SongForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = ['title']\n            exclude = ['original_release']\n\n    self.assertTrue(modelform_defines_fields(SongForm))\n", "def test_check_sublists_for_duplicates_in_fieldsets(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        fieldsets = [\n            (None, {\n                'fields': ['title', ('album', 'title')]\n            }),\n        ]\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"There are duplicate field(s) in 'fieldsets[0][1]'.\",\n            obj=MyModelAdmin,\n            id='admin.E012'\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_modelform_defines_fields(self):\n    class SongForm(forms.ModelForm):\n        pass\n\n    self.assertFalse(modelform_defines_fields(SongForm))\n\n    class SongForm(forms.ModelForm):\n        class Meta:\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(SongForm))\n\n    class SongForm(forms.ModelForm):\n        class Meta:\n            exclude = []\n\n    self.assertTrue(modelform_defines_fields(SongForm))\n", "def test_modelform_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        pass\n\n    self.assertFalse(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            exclude = []\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n"], "sample_143": ["def test_camel_case_to_spaces(self):\n    items = [\n        ('helloWorld', 'hello world'),\n        ('HelloWorld', 'hello world'),\n        ('helloHTMLWorld', 'hello html world'),\n        ('hello  World', 'hello  world'),\n        ('_helloWorld', '_hello world'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n    self.assertEqual(text.camel_case_to_spaces(lazystr('helloWorld')), 'hello world')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces(\"helloWorld\"), \"hello world\")\n    self.assertEqual(text.camel_case_to_spaces(\"helloHTMLWorld\"), \"hello html world\")\n    self.assertEqual(text.camel_case_to_spaces(\"HelloWorld\"), \"hello world\")\n    self.assertEqual(text.camel_case_to_spaces(\"helloWORLD\"), \"hello world\")\n    self.assertEqual(text.camel_case_to_spaces(\"\"), \"\")\n    self.assertEqual(text.camel_case_to_spaces(lazystr(\"helloWorld\")), \"hello world\")\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        ('CamelCase', 'camel case'),\n        ('CAMELCase', 'camel case'),\n        ('_camelCase', '_camel case'),\n        ('__camelCase', '__camel case'),\n        ('Camel_Case', 'camel _case'),\n        ('Camel-Case', 'camel -case'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('HelloWorld', 'hello world'),\n        ('helloWorld', 'hello world'),\n        ('helloHTMLWorld', 'hello html world'),\n        ('hello123World', 'hello123 world'),\n        ('hello_World', 'hello_ world'),\n        ('__HelloWorld__', '__ hello world __'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        ('CamelCase', 'camel case'),\n        ('CAMELCase', 'camel case'),\n        ('_camelCase', '_camel case'),\n        ('__camelCase', '__camel case'),\n        ('Camel_Case', 'camel case'),\n        ('Camel-Case', 'camel case'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n"], "sample_144": ["def test_deferred_field_access_on_parent(self):\n    restaurant = Restaurant.objects.create(\n        name=\"Guido's House of Pasta\",\n        address='944 W. Fullerton',\n        serves_hot_dogs=True,\n        serves_pizza=False,\n    )\n    deferred_restaurant = Restaurant.objects.defer('serves_hot_dogs').get(pk=restaurant.pk)\n    with self.assertNumQueries(1):\n        self.assertTrue(deferred_restaurant.serves_hot_dogs)\n    # Accessing the parent model should not re-evaluate the deferred fields.\n    place = deferred_restaurant.place_ptr\n    with self.assertNumQueries(0):\n        self.assertEqual(place.name, \"Guido's House of Pasta\")\n    # The deferred attribute should be passed to the parent model.\n    with self.assertNumQueries(1):\n        self.assertEqual(place.serves_hot_dogs, True)\n", "def test_check_parent_link(self):\n    # Test for #21829: If a model has multi-table inheritance from two parents,\n    # and the parent models have the same field, the child model should use the\n    # parent link to access the field.\n    parent1 = Person.objects.create(name='Parent 1')\n    parent2 = Politician.objects.create(person_ptr=parent1, title='Parent 2')\n    child = Congressman.objects.create(politician_ptr=parent2, state='PA')\n\n    # Accessing the field through the child model should return the value from\n    # the parent model.\n    self.assertEqual(child.name, 'Parent 1')\n\n    # Accessing the field through the parent link should return the value from\n    # the parent model.\n    self.assertEqual(child.politician_ptr.name, 'Parent 1')\n\n    # Changing the value of the field on the child model should not affect the\n    # parent model.\n    child.name = 'Child'\n    child.save()\n    self.assertEqual(Person.objects.get(pk=parent1.pk).name, 'Parent 1')\n\n    # Changing the value of the field on the parent model should affect the\n    # child model.\n    parent1.name = 'New Parent 1'\n    parent1.save()\n    self.assertEqual(Congressman.objects.get(pk=child.pk).name, 'New Parent 1')\n", "def test_deferred_loading_of_inherited_fields(self):\n    italian_restaurant = ItalianRestaurant.objects.create(\n        name=\"Guido's House of Pasta\",\n        address='944 W. Fullerton',\n        serves_hot_dogs=True,\n        serves_pizza=False,\n        serves_gnocchi=True,\n    )\n\n    # Loading a child model with deferred fields should not result in additional\n    # queries when accessing fields on the parent model.\n    italian_restaurant_deferred = ItalianRestaurant.objects.defer('serves_gnocchi').get(pk=italian_restaurant.pk)\n    with self.assertNumQueries(0):\n        self.assertEqual(italian_restaurant_deferred.name, italian_restaurant.name)\n        self.assertEqual(italian_restaurant_deferred.address, italian_restaurant.address)\n\n    # Accessing a deferred field on the child model results in one query to load\n    # the deferred fields.\n    with self.assertNumQueries(1):\n        self.assertEqual(italian_restaurant_deferred.serves_gnocchi, italian_restaurant.serves_gnocchi)\n\n    # Loading a child model with only() specified should only load the fields\n    # specified and should not result in additional queries when accessing those\n    # fields.\n    italian_restaurant_only = ItalianRestaurant.objects.only('name', 'serves_gnocchi').get(pk=italian_restaurant.pk)\n    with self.assertNumQueries(0):\n        self.assertEqual(italian_restaurant_only.name, italian_restaurant.name)\n        self.assertEqual(italian_restaurant_only.serves_gnocchi, italian_restaurant.serves_gnocchi)\n\n    # Accessing a non-loaded field results in one query to load the remaining\n    # fields.\n    with self.assertNumQueries(1):\n        self.assertEqual(italian_restaurant_only.address, italian_restaurant.address)\n", "def test_parent_link_on_child_model(self):\n    # Test for #20962: Check that a ParentLink on a child model is correctly\n    # created when the parent model has a custom primary key.\n    child = Child.objects.create(name='child', created=datetime.datetime(2008, 6, 26, 17, 0, 0))\n    self.assertEqual(child.parent.id, child.id)\n", "def test_validate_unique_on_parent(self):\n    # Regression test for #17541: validate_unique() should work on parent model\n    # when multi-table inheritance is in use.\n    restaurant1 = Restaurant.objects.create(\n        name=\"Guido's House of Pasta\",\n        address='944 W. Fullerton',\n        serves_hot_dogs=True,\n        serves_pizza=False,\n    )\n    restaurant2 = Restaurant(\n        name=\"Guido's House of Pasta\",\n        address='944 W. Fullerton',\n        serves_hot_dogs=True,\n        serves_pizza=False,\n    )\n\n    with self.assertRaises(ValidationError):\n        restaurant2.validate_unique()\n"], "sample_145": ["    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 'hello'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            'admin.E025'\n        )\n", "    def test_not_boolean_or_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 'hello'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            'admin.E025'\n        )\n", "    def test_readonly_fields_is_not_list_or_tuple(self):\n        class Admin(ModelAdmin):\n            readonly_fields = 'name'\n\n        self.assertIsInvalid(\n            Admin, ValidationTestModel,\n            msg=\"The value of 'readonly_fields' must be a list or tuple.\",\n            id='admin.E034',\n            invalid_obj=Admin,\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 'non_existent_field'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            'admin.E025'\n        )\n", "    def test_readonly_fields_is_list(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 'field'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n"], "sample_146": ["def test_invalid_variant_consistent_language_settings(self):\n    tests = [\n        # language + region that is not a valid variant.\n        'fr-US',\n        'es-UK',\n        # language + script (not currently supported).\n        'zh-Hans',\n        # invalid language subtag.\n        'en-abc',\n    ]\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "def test_invalid_variant_consistent_language_settings(self):\n    tests = [\n        # language + region.\n        'fr-US',\n        'es-ES',\n        'de-UK',\n        # language + script.\n        'zh-Hant',\n        # language + region + variant.\n        'ca-ES-barcelona',\n    ]\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "def test_invalid_variant_consistent_language_settings(self):\n    tests = [\n        # language + region.\n        'it-CA',\n        'pt-419',\n        # language + region + variant.\n        'ca-ES-barcelona',\n    ]\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "def test_invalid_variant_consistent_language_settings(self):\n    tests = [\n        # invalid language.\n        'zh-Hans',\n        # invalid region.\n        'fr-ABC',\n        # invalid variant.\n        'ca-ES-invalid',\n    ]\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "def test_language_settings_consistent_with_supported_language_variant(self):\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    tests = [\n        # language + region.\n        'fr-CA',\n        'fr-ca',\n    ]\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_147": ["def test_union_with_default_ordering(self):\n    ReservedName.objects.create(name='a', order=2)\n    ReservedName.objects.create(name='b', order=1)\n    qs1 = ReservedName.objects.all()\n    self.assertEqual(list(qs1.union(qs1).values_list('order', flat=True)), [1, 2])\n", "def test_union_with_subqueries(self):\n    subquery = Number.objects.filter(num__lte=5).values_list('num', flat=True)\n    qs1 = Number.objects.filter(num__in=subquery)\n    qs2 = Number.objects.filter(num__gt=5)\n    self.assertNumbersEqual(qs1.union(qs2), list(range(10)))\n", "def test_union_with_none_queryset(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.none()\n    self.assertNumbersEqual(qs1.union(qs2), list(range(10)))\n    self.assertNumbersEqual(qs2.union(qs1), list(range(10)))\n    self.assertNumbersEqual(qs1.union(Number.objects.filter(pk__in=[])), list(range(10)))\n", "def test_union_with_values_and_limit(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.all()\n    self.assertEqual(len(qs1.union(qs1).values('name', 'order')[:1]), 1)\n", "def test_union_with_subqueries(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__gte=2, num__lte=3).values_list('num', flat=True)\n    subquery = Number.objects.filter(num__in=qs2)\n    self.assertNumbersEqual(qs1.union(subquery), [0, 1, 2, 3], ordered=False)\n"], "sample_148": ["def test_reverse_field_path(self):\n    \"\"\"\n    Test reversing a field path.\n    \"\"\"\n    # Test simple field path\n    model, path = reverse_field_path(Article, \"site\")\n    self.assertEqual(model, Site)\n    self.assertEqual(path, \"article\")\n\n    # Test multi-part field path\n    model, path = reverse_field_path(Event, \"location__address\")\n    self.assertEqual(model, Location)\n    self.assertEqual(path, \"event__location\")\n\n    # Test field path with related fields\n    model, path = reverse_field_path(EventGuide, \"event__location__address\")\n    self.assertEqual(model, Location)\n    self.assertEqual(path, \"eventguide__event__location\")\n\n    # Test invalid field path\n    with self.assertRaises(FieldDoesNotExist):\n        reverse_field_path(Article, \"invalid_field\")\n\n    # Test invalid model\n    with self.assertRaises(AttributeError):\n        reverse_field_path(\"InvalidModel\", \"field\")\n", "def test_unquote(self):\n    self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n", "def test_unquote(self):\n    self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n    self.assertEqual(unquote('nothing'), 'nothing')\n    self.assertEqual(unquote('something_3Delse'), 'something=else')\n", "def test_reverse_field_path(self):\n    \"\"\"\n    Test reverse_field_path utility function.\n    \"\"\"\n    model = Event\n    path = \"location__address\"\n    reversed_model, reversed_path = reverse_field_path(model, path)\n    self.assertEqual(reversed_model, Location)\n    self.assertEqual(reversed_path, \"event__location\")\n\n    model = Article\n    path = \"site\"\n    reversed_model, reversed_path = reverse_field_path(model, path)\n    self.assertEqual(reversed_model, Site)\n    self.assertEqual(reversed_path, \"article\")\n", "def test_unquote(self):\n    self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n"], "sample_151": ["def test_alter_order_with_respect_to_added_field(self):\n    \"\"\"\n    Alter order_with_respect_to to point to a field that is added in the same\n    migration.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty], \n        [ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ], options={\"order_with_respect_to\": \"book\"})]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name=\"author\", name=\"book\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"author\", order_with_respect_to=\"book\")\n", "def test_mti_inheritance_field_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=255)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"breed\", models.CharField(max_length=255)),\n    ], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [\n        ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Dog', [], bases=('app.Animal',)),\n    ])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='animal', name='name')\n    self.assertOperationAttributes(changes, 'app', 0, 1, model_name='dog', name='breed')\n", "def test_mti_inheritance_model_removal_with_related_fields(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"owner\", models.ForeignKey(\"app.Person\", models.CASCADE)),\n    ], bases=('app.Animal',))\n    Person = ModelState('app', 'Person', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"pet\", models.ForeignKey(\"app.Dog\", models.CASCADE)),\n    ])\n    changes = self.get_changes([Animal, Dog, Person], [Animal, Person])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='pet', model_name='person')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n", "def test_mti_inheritance_model_removal_with_foreign_key(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"owner\", models.ForeignKey(\"app.Owner\", models.CASCADE)),\n    ], bases=('app.Animal',))\n    Owner = ModelState('app', 'Owner', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([Animal, Dog, Owner], [Animal, Owner])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='owner', model_name='dog')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n", "def test_mti_inheritance_field_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=100)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"breed\", models.CharField(max_length=100)),\n    ], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [Animal, ModelState('app', 'Dog', [], bases=('app.Animal',))])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='dog', name='breed')\n"], "sample_149": ["def test_multiple_models_permissions(self):\n    class Model1(models.Model):\n        class Meta:\n            permissions = [\n                ('my_custom_permission', 'Some permission'),\n            ]\n\n    class Model2(models.Model):\n        class Meta:\n            permissions = [\n                ('other_custom_permission', 'Some other permission'),\n            ]\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_permission_name_length_with_custom_permissions(self):\n    class Checked(models.Model):\n        class Meta:\n            permissions = [\n                ('my_custom_permission', 'Some permission'),\n                ('other_one', 'Some other permission' * 50),\n            ]\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The permission named '%s' of model 'auth_tests.Checked' is longer \"\n            \"than 255 characters.\" % ('Some other permission' * 50),\n            obj=Checked,\n            id='auth.E008',\n        ),\n    ])\n", "def test_empty_permissions(self):\n    class Checked(models.Model):\n        class Meta:\n            permissions = []\n\n    self.assertEqual(checks.run_checks(self.apps.get_app_configs()), [])\n", "def test_default_permissions_with_empty_model_name(self):\n    class Checked(models.Model):\n        class Meta:\n            verbose_name = ''\n            default_permissions = ('add',)\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n            \"characters for its builtin permission names to be at most 255 characters.\",\n            obj=Checked,\n            id='auth.E007',\n        ),\n    ])\n", "def test_permission_name_max_length_with_non_ascii_characters(self):\n    class Checked(models.Model):\n        class Meta:\n            verbose_name = 'some permission name with non-ascii characters \u00e9\u00e7\u00e0'\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n"], "sample_152": ["def test_fast_delete_collector(self):\n    collector = Collector(using='default')\n    u = User.objects.create()\n    a = Avatar.objects.create(desc='a')\n    u.avatar = a\n    collector.collect([u], keep_parents=False)\n    self.assertEqual(len(collector.fast_deletes), 2)\n    with self.assertNumQueries(2):\n        collector.delete()\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n", "def test_collector_clear_restricted_objects_from_set(self):\n    collector = Collector(using='default')\n    model = R\n    restricted_field = 'restricted_field'\n    objs = [R.objects.create() for _ in range(3)]\n    collector.restricted_objects[model][restricted_field].update(objs)\n    collector.clear_restricted_objects_from_set(model, objs[:2])\n    self.assertEqual(\n        len(collector.restricted_objects[model][restricted_field]),\n        1\n    )\n    self.assertIn(objs[2], collector.restricted_objects[model][restricted_field])\n", "def test_delete_with_m2m_through_model(self):\n    m1 = M.objects.create()\n    r1 = R.objects.create()\n    MR.objects.create(m=m1, r=r1)\n    r1.delete()\n    self.assertFalse(MR.objects.exists())\n\n    m2 = M.objects.create()\n    r2 = R.objects.create()\n    m2.m2m.add(r2)\n    r2.delete()\n    through = M._meta.get_field('m2m').remote_field.through\n    self.assertFalse(through.objects.exists())\n", "def test_can_fast_delete_with_deferred_fields(self):\n    # If an instance has deferred fields, it should not be fast-deleted.\n    u = User.objects.create()\n    u = User.objects.defer('avatar').get(pk=u.pk)\n    collector = Collector(using='default')\n    self.assertFalse(collector.can_fast_delete(u))\n", "def test_can_fast_delete_collector(self):\n    u = User.objects.create()\n    collector = Collector(using='default')\n    self.assertTrue(collector.can_fast_delete(u))\n    # Adding signal receiver should prevent fast delete.\n        pass\n    models.signals.pre_delete.connect(receiver, sender=User)\n    self.assertFalse(collector.can_fast_delete(u))\n    models.signals.pre_delete.disconnect(receiver, sender=User)\n    # Adding dependency should prevent fast delete.\n    a = Avatar.objects.create()\n    u.avatar = a\n    u.save()\n    self.assertFalse(collector.can_fast_delete(u))\n"], "sample_150": ["    def test_base_command_has_no_output(self):\n        command = BaseCommand()\n        output = command.handle()\n        self.assertIsNone(output)\n", "    def test_create_parser(self):\n        command = BaseCommand()\n        parser = command.create_parser('prog_name', 'subcommand')\n        self.assertIsInstance(parser, CommandParser)\n        self.assertEqual(parser.prog, 'prog_name subcommand')\n", "    def test_base_command_style(self):\n        command = BaseCommand()\n        self.assertEqual(command.style.ERROR('Error'), '\\x1b[31;1mError\\x1b[0m')\n        self.assertEqual(command.style.WARNING('Warning'), '\\x1b[33;1mWarning\\x1b[0m')\n        self.assertEqual(command.style.SQL_KEYWORD('SELECT'), '\\x1b[34;1mSELECT\\x1b[0m')\n        self.assertEqual(command.style.NOTICE('Notice'), '\\x1b[32;1mNotice\\x1b[0m')\n", "    def test_base_command_has_no_output(self):\n        command = BaseCommand()\n        self.assertIsNone(command.handle())\n", "    def test_create_parser(self):\n        command = BaseCommand()\n        parser = command.create_parser('manage.py', 'test')\n        self.assertIsInstance(parser, CommandParser)\n"], "sample_153": ["def test_sqlite_integrity_constraint(self):\n    # Test checks for pending integrity constraint errors on SQLite databases.\n    with mock.patch(\n        'django.db.backends.sqlite3.base.DatabaseWrapper.is_in_memory_db',\n        return_value=False,\n    ), mock.patch(\n        'django.db.backends.sqlite3.base.DatabaseWrapper._pragmas',\n        create=True,\n        return_value={'integrity_check': ['error message']},\n    ):\n        result = check_database_backends(databases=self.databases)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0].id, 'sqlite3.W003')\n\n    # No warnings if no integrity constraint errors are found.\n    with mock.patch(\n        'django.db.backends.sqlite3.base.DatabaseWrapper.is_in_memory_db',\n        return_value=False,\n    ), mock.patch(\n        'django.db.backends.sqlite3.base.DatabaseWrapper._pragmas',\n        create=True,\n        return_value={'integrity_check': []},\n    ):\n        self.assertEqual(check_database_backends(databases=self.databases), [])\n", "    def test_model_checks_called(self):\n        from django.db.models.base import ModelBase\n        with mock.patch.object(ModelBase, 'check') as mocked_check:\n            from django.core.checks.model_checks import check_models\n            check_models()\n            self.assertTrue(mocked_check.called)\n", "    def test_check_id_field(self):\n        # Create a model with an 'id' field that is not a primary key\n        model = type('Model', (models.Model,), {\n            'id': models.IntegerField(),\n        })\n        errors = model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'models.E004')\n", "    def test_deferred_fields(self):\n        model = Model()\n        self.assertEqual(model.get_deferred_fields(), set())\n        model._meta.concrete_fields = [mock.Mock(attname='field1'), mock.Mock(attname='field2')]\n        model.__dict__['field1'] = 'value'\n        self.assertEqual(model.get_deferred_fields(), {'field2'})\n", "    def test_model_check_database_backends(self):\n        # Ensure model validation doesn't trigger database backend checks.\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mocked_check:\n            model = Model()\n            model.full_clean()\n            self.assertFalse(mocked_check.called)\n"], "sample_154": ["def test_database_checks_called_for_all_aliases(self, mocked_connections):\n    mocked_connections.items.return_value = [\n        ('default', mock.Mock(validation=mock.Mock(check=mock.Mock()))),\n        ('other', mock.Mock(validation=mock.Mock(check=mock.Mock()))),\n    ]\n    check_database_backends(databases=self.databases)\n    for alias in self.databases:\n        mocked_connections[alias].validation.check.assert_called_once()\n", "def test_check_database_backends_empty_connections(self, mocked_connections):\n    mocked_connections.items.return_value = []\n    self.assertEqual(check_database_backends(databases=self.databases), [])\n", "def test_check_database_backends_connections(self, mocked_connections):\n    issues = ['issue1', 'issue2']\n    mocked_connections.__getitem__.return_value.validation.check.return_value = issues\n    result = check_database_backends(databases=self.databases)\n    self.assertEqual(result, issues * len(self.databases))\n    mocked_connections.__getitem__.assert_called_with(list(self.databases)[0])\n", "def test_check_database_backends_connections(self, mocked_connections):\n    databases = {'default', 'other'}\n    check_database_backends(databases=databases)\n    for alias in databases:\n        self.assertTrue(mocked_connections.__getitem__.called_with(alias))\n", "def test_database_checks_called_for_each_database(self, mocked_connections):\n    databases = {'default', 'other'}\n    mocked_connections.items.return_value = [\n        ('default', mock.Mock(validation=mock.Mock())),\n        ('other', mock.Mock(validation=mock.Mock())),\n    ]\n    check_database_backends(databases=databases)\n    for alias in databases:\n        mocked_connections[alias].validation.check.assert_called_once()\n"], "sample_155": ["def test_file_response_close(self):\n    filelike = open(__file__, 'rb')\n    response = FileResponse(filelike)\n    response.close()\n    self.assertTrue(response.closed)\n    self.assertTrue(filelike.closed)\n", "def test_file_response_close(self):\n    file = open(__file__, 'rb')\n    response = FileResponse(file)\n    self.assertFalse(file.closed)\n    response.close()\n    self.assertTrue(file.closed)\n", "def test_file_response_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), as_attachment=True, filename='example.txt')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"example.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_response_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_response_with_content_disposition_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), as_attachment=True, filename='example.txt')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(\n        response['Content-Disposition'],\n        \"attachment; filename*=utf-8''example.txt\"\n    )\n"], "sample_156": ["def test_field_order_with_inheritance(self):\n    class BaseForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n    class ChildForm(BaseForm):\n        field3 = CharField()\n        field4 = CharField()\n\n    form = ChildForm(field_order=['field3', 'field1', 'field2'])\n    self.assertEqual(list(form.fields), ['field3', 'field1', 'field2', 'field4'])\n\n    form = ChildForm(field_order=None)\n    self.assertEqual(list(form.fields), ['field1', 'field2', 'field3', 'field4'])\n", "def test_invalid_renderer(self):\n    msg = \"Renderer must be a class or instance that implements a render method.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        Form(renderer='invalid')\n\n    class InvalidRenderer:\n        pass\n\n    with self.assertRaisesMessage(ValueError, msg):\n        Form(renderer=InvalidRenderer())\n\n    with self.assertRaisesMessage(ValueError, msg):\n        Form(renderer=InvalidRenderer)\n", "def test_renderer_attribute_from_parent(self):\n    class ParentForm(Form):\n        default_renderer = CustomRenderer()\n\n    class ChildForm(ParentForm):\n        pass\n\n    form = ChildForm()\n    self.assertEqual(form.renderer, ParentForm.default_renderer)\n", "def test_add_prefix(self):\n    # A form's add_prefix method can be overridden to change the way prefixes are\n    # applied to field names.\n    class Person(Form):\n        first_name = CharField()\n        last_name = CharField()\n\n            return 'prefix-' + field_name\n\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'prefix-first_name')\n\n    class Person(Form):\n        first_name = CharField()\n        last_name = CharField()\n\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n", "def test_media_property(self):\n    class CustomForm(Form):\n        media = Media(css={'all': ['test.css']}, js=['test.js'])\n\n    form = CustomForm()\n    self.assertIsInstance(form.media, Media)\n    self.assertEqual(str(form.media), '<link href=\"test.css\" type=\"text/css\" media=\"all\"><script src=\"test.js\"></script>')\n"], "sample_157": ["    def test_get_test_db_name_with_test_name(self):\n        test_name = 'test_hodor'\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST'] = {'NAME': test_name}\n        creation = BaseDatabaseCreation(test_connection)\n        self.assertEqual(creation._get_test_db_name(), test_name)\n", "    def test_get_test_db_clone_settings(self, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        suffix = 'test_suffix'\n        clone_settings = creation.get_test_db_clone_settings(suffix)\n        self.assertEqual(clone_settings['NAME'], test_connection.settings_dict['NAME'] + '_' + suffix)\n", "    def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            self.assertEqual(test_connection.settings_dict['NAME'], creation._get_test_db_name())\n            mocked_migrate.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            mocked_create_test_db.assert_called_once_with(verbosity=0, autoclobber=True, keepdb=True)\n            self.assertEqual(test_connection.settings_dict['NAME'], creation._get_test_db_name())\n        finally:\n            test_connection.settings_dict['NAME'] = old_database_name\n", "    def test_clone_test_db(self, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_clone_test_db') as mocked_clone:\n            creation.clone_test_db(suffix='test_suffix', verbosity=0, autoclobber=True)\n            mocked_clone.assert_called_once_with('test_suffix', 0, False)\n"], "sample_158": ["    def test_fk_to_field_with_unique_constraint(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['a', 'b'],\n                        name='unique_a_b',\n                    ),\n                ]\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            parent = models.ForeignKey(\n                Parent,\n                on_delete=models.SET_NULL,\n                to_field='a',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"'Parent.a' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a '\n                    'UniqueConstraint (without condition) in the model '\n                    'Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n", "    def test_m2m_through_model_table_space(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n\n            class Meta:\n                db_tablespace = 'different_tablespace'\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"The model is used as an intermediate model by \"\n                \"'invalid_models_tests.Event.invitees', but it has a different \"\n                \"table space. Ensure that the table space of this model matches \"\n                \"the table space of the parent model.\",\n                obj=field,\n                id='fields.E341',\n            ),\n        ])\n", "    def test_foreign_key_to_field(self):\n        class Target(models.Model):\n            field = models.IntegerField(unique=True)\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, models.CASCADE, to_field='field')\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [])\n", "    def test_unique_foreign_key(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, models.CASCADE, unique=True)\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [\n            DjangoWarning(\n                'Setting unique=True on a ForeignKey has the same effect as using a OneToOneField.',\n                hint='ForeignKey(unique=True) is usually better served by a OneToOneField.',\n                obj=field,\n                id='fields.W342',\n            )\n        ])\n", "    def test_foreign_key_to_swapped_model(self):\n        class SwappedModel(models.Model):\n            pass\n\n        class Model(models.Model):\n            field = models.ForeignKey('SwappedModel', models.CASCADE)\n\n        with override_settings(TEST_SWAPPABLE_MODEL='invalid_models_tests.Replacement'):\n            self.assertEqual(Model.check(), [\n                Error(\n                    \"Field defines a relation with the model 'invalid_models_tests.SwappedModel', \"\n                    \"which has been swapped out.\",\n                    hint=\"Update the relation to point at 'settings.TEST_SWAPPABLE_MODEL'.\",\n                    obj=Model._meta.get_field('field'),\n                    id='fields.E301',\n                ),\n            ])\n"], "sample_159": ["def test_permission_name_max_length_with_empty_builtin_permissions(self):\n    class Checked(models.Model):\n        class Meta:\n            verbose_name = 'some ridiculously long verbose name that is out of control' * 5\n            default_permissions = ()\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_custom_permission_name_length_edge_case(self):\n    custom_permission_name = 'x' * 255\n\n    class Checked(models.Model):\n        class Meta:\n            permissions = [\n                ('my_custom_permission', custom_permission_name),\n            ]\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_required_fields_not_set(self):\n    \"\"\"REQUIRED_FIELDS should be set.\"\"\"\n    class CustomUserNoRequiredFields(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        date_of_birth = models.DateField()\n\n        USERNAME_FIELD = 'username'\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserNoRequiredFields,\n            id='auth.E001',\n        ),\n    ])\n", "def test_required_fields_is_tuple(self):\n    \"\"\"REQUIRED_FIELDS should be a list or tuple.\"\"\"\n    class CustomUserTupleRequiredFields(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        date_of_birth = models.DateField()\n\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = ('date_of_birth',)\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_check_models_permissions_with_app_configs(self):\n    class Checked(models.Model):\n        pass\n\n    app_config = self.apps.get_app_config('auth_tests')\n    errors = check_models_permissions(app_configs=[app_config])\n    self.assertEqual(errors, [])\n\n    class CheckedWithPermissions(models.Model):\n        class Meta:\n            permissions = [\n                ('my_custom_permission', 'Some permission'),\n            ]\n\n    errors = check_models_permissions(app_configs=[app_config])\n    self.assertEqual(errors, [])\n"], "sample_160": ["def test_format_with_non_uniform_digit_grouping(self):\n    # Test non-uniform digit grouping (e.g. (3, 2, 0)).\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,567')\n    self.assertEqual(nformat(12345, '.', grouping=(3, 2, 0), thousand_sep=','), '12,345')\n    self.assertEqual(nformat(123, '.', grouping=(3, 2, 0), thousand_sep=','), '123')\n", "def test_format_with_non_uniform_grouping(self):\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=',', force_grouping=True), '12,34,56,789')\n    self.assertEqual(nformat(-123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '-12,34,56,789')\n    self.assertEqual(nformat(-123456789, '.', grouping=(3, 2, 0), thousand_sep=',', force_grouping=True), '-12,34,56,789')\n    self.assertEqual(nformat(123456789, '.', decimal_pos=2, grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789.00')\n    self.assertEqual(nformat(123456789, '.', decimal_pos=2, grouping=(3, 2, 0), thousand_sep=',', force_grouping=True), '12,34,56,789.00')\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '1,234,56,7')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '123,45,67,89')\n    self.assertEqual(nformat(1234, '.', grouping=(3, 2, 0), thousand_sep=','), '1,234')\n    self.assertEqual(nformat(123456789012, '.', grouping=(3, 2, 0), thousand_sep=','), '12,3456,78,9012')\n", "def test_non_uniform_digit_grouping(self):\n    # Test non-uniform digit grouping.\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '123,45,67,89')\n    self.assertEqual(nformat(123456789012, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,78,9012')\n", "def test_non_uniform_digit_grouping(self):\n    # Test non-uniform digit grouping.\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(1234567, '.', grouping=(2, 3), thousand_sep=','), '12,345,67')\n    self.assertEqual(nformat(1234567, '.', grouping=(2, 3, 0), thousand_sep=','), '12,345,67')\n    # Make sure the function handles incorrect input correctly.\n    with self.assertRaises(TypeError):\n        nformat(1234567, '.', grouping='3,2', thousand_sep=',')\n    with self.assertRaises(TypeError):\n        nformat(1234567, '.', grouping=(3, '2'), thousand_sep=',')\n"], "sample_161": ["    def test_custom_primary_key(self):\n        class Target(models.Model):\n            custom_id = models.CharField(max_length=255, primary_key=True)\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, models.CASCADE)\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [])\n", "    def test_reverse_one_to_one_field(self):\n        class Model(models.Model):\n            field = models.OneToOneField('RelatedModel', models.CASCADE)\n\n        class RelatedModel(models.Model):\n            pass\n\n        field = Model._meta.get_field('field')\n        self.assertEqual(field.check(), [\n            Error(\n                \"Reverse accessor for 'Model.field' clashes with reverse accessor for 'RelatedModel.model'.\",\n                hint=(\n                    \"Add or change a related_name argument to the definition \"\n                    \"for 'Model.field' or 'RelatedModel.model'.\"\n                ),\n                obj=field,\n                id='fields.E304',\n            ),\n            Error(\n                \"Reverse query name for 'Model.field' clashes with reverse query name for 'RelatedModel.model'.\",\n                hint=(\n                    \"Add or change a related_name argument to the definition \"\n                    \"for 'Model.field' or 'RelatedModel.model'.\"\n                ),\n                obj=field,\n                id='fields.E305',\n            ),\n        ])\n", "    def test_foreign_key_target_field_does_not_exist(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey('Target', models.CASCADE, to_field='missing_field')\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [\n            Error(\n                \"The to_field 'missing_field' doesn't exist on the related model 'invalid_models_tests.Target'.\",\n                obj=field,\n                id='fields.E312',\n            ),\n        ])\n", "    def test_foreign_key_to_non_unique_field_with_meta_constraint(self):\n        class Target(models.Model):\n            source = models.IntegerField()\n            other = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['source', 'other'],\n                        name='tfktoufwm_c',\n                    ),\n                ]\n\n        class Model(models.Model):\n            field = models.ForeignKey(Target, models.CASCADE, to_field='source')\n\n        field = Model._meta.get_field('field')\n        self.assertEqual(field.check(), [\n            Error(\n                \"'Target.source' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a UniqueConstraint '\n                    '(without condition) in the model Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n", "    def test_swappable_model(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            explicit_fk = models.ForeignKey(\n                SwappableModel,\n                models.CASCADE,\n                related_name='explicit_fk',\n            )\n            implicit_fk = models.ForeignKey(\n                'invalid_models_tests.SwappableModel',\n                models.CASCADE,\n                related_name='implicit_fk',\n            )\n            explicit_m2m = models.ManyToManyField(SwappableModel, related_name='explicit_m2m')\n            implicit_m2m = models.ManyToManyField(\n                'invalid_models_tests.SwappableModel',\n                related_name='implicit_m2m',\n            )\n\n        fields = [\n            Model._meta.get_field('explicit_fk'),\n            Model._meta.get_field('implicit_fk'),\n            Model._meta.get_field('explicit_m2m'),\n            Model._meta.get_field('implicit_m2m'),\n        ]\n\n        expected_error = Error(\n            (\"Field defines a relation with the model \"\n             \"'invalid_models_tests.SwappableModel', which has been swapped out.\"),\n            hint=\"Update the relation to point at 'settings.TEST_SWAPPABLE_MODEL'.\",\n            id='fields.E301',\n        )\n\n        with override_settings(TEST_SWAPPABLE_MODEL='invalid_models_tests.Replacement'):\n            for field in fields:\n                expected_error.obj = field\n                self.assertEqual(field.check(from_model=Model), [expected_error])\n"], "sample_162": ["    def test_no_obsolete_disabled_by_default(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertIn('#~ msgid \"\"', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"Obsolete message\"', po_contents)\n", "    def test_remove_obsolete_disabled_by_default(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE, encoding='utf-8') as fp:\n            po_contents = fp.read()\n            self.assertIn('#~ msgid \"Obsolete message\"', po_contents)\n", "    def test_extension_parsing(self):\n        \"\"\"\n        makemessages command handles file extensions correctly (#24399).\n        \"\"\"\n        management.call_command('makemessages', locale=[LOCALE], extensions=['html,txt'], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId(\"Translatable literal #6a\", po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n"], "sample_163": ["    def test_session_auth_hash_is_updated(self):\n        user = User.objects.get(username=\"testclient\")\n        user.set_password(\"newpassword\")\n        user.save()\n\n        self.login()\n        session_key = self.client.session.session_key\n\n        # The session auth hash should be updated after logging in with a new password.\n        self.login(password=\"newpassword\")\n        self.assertEqual(self.client.session.session_key, session_key)\n        self.assertNotEqual(\n            self.client.session[SESSION_KEY], self.client.session[HASH_SESSION_KEY]\n        )\n", "    def test_get_success_url_allowed_hosts_same_host(self):\n        login_view = LoginView()\n        login_view.request = HttpRequest()\n        login_view.request.get_host = lambda: \"testserver\"\n        self.assertTrue(login_view.get_success_url_allowed_hosts())\n", "    def test_template(self):\n        response = self.client.get(\"/login/\")\n        self.assertTemplateUsed(response, \"registration/login.html\")\n", "    def test_logout_view_dispatch_post(self):\n        request = self.factory.post(\"/logout/\")\n        response = LogoutView.as_view()(request)\n        self.assertEqual(response.status_code, 200)\n", "    def test_next_page_parameter(self):\n        next_page = \"/somewhere/else/\"\n        response = self.client.post(\n            \"/login/\", {\"username\": \"testclient\", \"password\": \"password\", \"next\": next_page}\n        )\n        self.assertRedirects(response, next_page, fetch_redirect_response=False)\n"], "sample_164": ["    def test_custom_logging_config(self):\n        configure_logging(logging_config=settings.LOGGING_CONFIG, logging_settings=None)\n        self.assertTrue(logging.config.dictConfig.called)\n", "    def test_log_response(self):\n        request = self.request_factory.get('/')\n        response = views.http_response('Hello World!')\n        log_response('message', response=response, request=request)\n        self.assertEqual(self.logger_output.getvalue(), '')\n", "    def test_format(self):\n        formatter = ServerFormatter()\n        record = logging.makeLogRecord({'msg': 'log message', 'status_code': 200})\n        self.assertEqual(formatter.format(record), '[/] log message')\n", "    def test_log_response(self):\n        logger = logging.getLogger('django')\n        request = RequestFactory().get('/')\n        response = views.HttpResponse('Hello, World!', status=200)\n        log_response('message', response=response, request=request, logger=logger)\n        self.assertEqual(len(logger.handlers[0].stream.getvalue()), 0)\n", "    def test_log_response(self):\n        response = type('', (), {'status_code': 500, 'reason_phrase': 'Internal Server Error'})\n        logger = logging.getLogger('django.request')\n        log_response('message', response=response, request=None, logger=logger)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertIn('Report at None', mail.outbox[0].body)\n"], "sample_165": ["    def test_modelchoiceiterator(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all())\n        iterator = f.iterator(f)\n        self.assertEqual(len(iterator), 4)  # 3 choices + empty label\n\n        choices = list(iterator)\n        self.assertEqual(choices[0], ('', '---------'))\n        self.assertEqual(choices[1][0].instance.pk, 1)\n        self.assertEqual(choices[1][1], 'a')\n        self.assertEqual(choices[2][0].instance.pk, 2)\n        self.assertEqual(choices[2][1], 'b')\n        self.assertEqual(choices[3][0].instance.pk, 3)\n        self.assertEqual(choices[3][1], 'c')\n", "    def test_model_choice_iterator(self):\n        # Create some choices for the model choice iterator tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all())\n        iterator = f.iterator(f)\n\n        # Test the length of the iterator.\n        self.assertEqual(len(iterator), 3)\n\n        # Test the values returned by the iterator.\n        expected_choices = [\n            (1, 'a'),\n            (2, 'b'),\n            (3, 'c'),\n        ]\n        self.assertEqual(list(iterator), expected_choices)\n\n        # Test that the iterator can be iterated over multiple times.\n        self.assertEqual(list(iterator), expected_choices)\n        self.assertEqual(list(iterator), expected_choices)\n", "    def test_modelformset_factory(self):\n        # Create a model formset factory\n        ChoiceFormSet = modelformset_factory(ChoiceModel, fields=('name',))\n\n        # Create a queryset of choices\n        choices = [ChoiceModel.objects.create(name='a'), ChoiceModel.objects.create(name='b')]\n\n        # Create a formset instance\n        formset = ChoiceFormSet(queryset=choices)\n\n        # Check that the formset has the correct forms\n        self.assertEqual(len(formset.forms), 2)\n\n        # Check that each form has the correct fields\n        for form in formset.forms:\n            self.assertIn('id', form.fields)\n            self.assertIn('name', form.fields)\n\n        # Check that the formset can be validated and saved\n        data = {\n            'form-TOTAL_FORMS': 2,\n            'form-INITIAL_FORMS': 2,\n            'form-MIN_NUM_FORMS': 0,\n            'form-MAX_NUM_FORMS': 1000,\n            'form-0-id': choices[0].pk,\n            'form-0-name': 'a',\n            'form-1-id': choices[1].pk,\n            'form-1-name': 'b',\n        }\n        formset = ChoiceFormSet(data, queryset=choices)\n        self.assertTrue(formset.is_valid())\n        formset.save()\n", "    def test_model_choice_iterator(self):\n        # Create choices for the model choice iterator tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all())\n        iterator = f.iterator(f)\n\n        self.assertEqual(len(iterator), 4)  # including empty label\n        self.assertEqual(list(iterator)[0], (\"\", \"---------\"))\n        self.assertEqual(list(iterator)[1][0].value, 1)\n        self.assertEqual(list(iterator)[1][1], 'a')\n", "    def test_modelform_with_custom_formfield_callback(self):\n        class CustomFormfieldCallbackForm(ModelForm):\n            formfield_callback = lambda model_field, **kwargs: ModelChoiceField(queryset=ChoiceModel.objects.all())\n\n        form = modelform_factory(ChoiceModel, form=CustomFormfieldCallbackForm)\n        self.assertIsInstance(form.base_fields['name'], ModelChoiceField)\n"], "sample_166": ["def test_get_random_string_custom_length(self):\n    length = 22\n    self.assertEqual(len(get_random_string(length)), length)\n", "    def test_get_random_string_length(self):\n        length = 20\n        random_string = get_random_string(length)\n        self.assertEqual(len(random_string), length)\n", "def test_get_random_string_custom_length(self):\n    length = 22\n    random_string = get_random_string(length)\n    self.assertEqual(len(random_string), length)\n", "def test_get_random_string_length(self):\n    for length in range(1, 100):\n        self.assertEqual(len(get_random_string(length)), length)\n", "def test_get_random_string_provided_length(self):\n    length = 22\n    random_string = get_random_string(length)\n    self.assertEqual(len(random_string), length)\n"], "sample_167": ["def test_intword_non_integer_input(self):\n    test_list = ('123.45', '1234.56', '12345.67', '123456.78', '1234567.89')\n    result_list = ('123', '1.2 thousand', '12.3 thousand', '123.5 thousand', '1.2 million')\n\n    with translation.override('en'):\n        self.humanize_tester(test_list, result_list, 'intword')\n", "def test_naturaltime_for_microseconds(self):\n    \"\"\"\n    naturaltime should display \"now\" for time differences of less than one second.\n    \"\"\"\n    test_list = [\n        now,\n        now - datetime.timedelta(microseconds=1),\n        now - datetime.timedelta(microseconds=999999),\n        now + datetime.timedelta(microseconds=1),\n        now + datetime.timedelta(microseconds=999999),\n    ]\n    result_list = ['now'] * len(test_list)\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_naturaltime_for_microseconds(self):\n    \"\"\"\n    Test that naturaltime correctly handles microseconds.\n    \"\"\"\n    test_list = [\n        now,\n        now - datetime.timedelta(microseconds=1),\n        now - datetime.timedelta(microseconds=1000),  # 1 millisecond\n        now + datetime.timedelta(microseconds=1),\n        now + datetime.timedelta(microseconds=1000),  # 1 millisecond\n    ]\n    result_list = [\n        'now',\n        'now',\n        'now',\n        'now',\n        'now',\n    ]\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_naturaltime_overflow(self):\n    \"\"\"\n    Test naturaltime for dates far in the future/past.\n    \"\"\"\n    with translation.override('en'):\n        # Regression for #28792: dates before 1900 and after 2038 should not\n        # crash due to time_t limits.\n        self.humanize_tester(\n            [datetime.datetime(1899, 12, 31, tzinfo=utc)],\n            ['116\\xa0years ago'],\n            'naturaltime',\n        )\n        self.humanize_tester(\n            [datetime.datetime(2040, 1, 1, tzinfo=utc)],\n            ['18\\xa0years from now'],\n            'naturaltime',\n        )\n", "def test_naturaltime_with_small_differences(self):\n    \"\"\"\n    Test naturaltime with small differences, to check that it doesn't return 'now'\n    when the difference is less than a second.\n    \"\"\"\n    test_list = [\n        now,\n        now - datetime.timedelta(microseconds=1),\n        now - datetime.timedelta(milliseconds=1),\n        now - datetime.timedelta(seconds=0.5),\n        now + datetime.timedelta(microseconds=1),\n        now + datetime.timedelta(milliseconds=1),\n        now + datetime.timedelta(seconds=0.5),\n    ]\n    result_list = [\n        'now',\n        'a second ago',\n        'a second ago',\n        'a second ago',\n        'a second from now',\n        'a second from now',\n        'a second from now',\n    ]\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n"], "sample_168": ["def test_interactive_false_with_dependent_objects(self):\n    \"\"\"\n    Non-interactive mode deletes stale content types and their dependent objects.\n    \"\"\"\n    post = Post.objects.create(title='post', content_type=self.content_type)\n    ModelWithNullFKToSite.objects.create(post=post)\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n    self.assertEqual(Post.objects.count(), 0)\n    self.assertEqual(ModelWithNullFKToSite.objects.count(), 0)\n    output = stdout.getvalue()\n    self.assertIn(\"Deleting stale content type\", output)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_interactive_false_with_dependent_objects(self):\n    \"\"\"\n    Non-interactive mode deletes stale content types and their dependent objects.\n    \"\"\"\n    post = Post.objects.create(title='post', content_type=self.content_type)\n    ModelWithNullFKToSite.objects.create(post=post)\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n    self.assertEqual(Post.objects.count(), 0)\n    output = stdout.getvalue()\n    self.assertIn('Deleting stale content type', output)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_interactive_false_with_dependent_objects(self):\n    \"\"\"\n    non-interactive mode deletes stale content types and their dependent objects.\n    \"\"\"\n    post = Post.objects.create(title='post', content_type=self.content_type)\n    ModelWithNullFKToSite.objects.create(post=post)\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n    self.assertEqual(Post.objects.count(), 0)\n    output = stdout.getvalue()\n    self.assertIn('Deleting stale content type', output)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_interactive_false_with_dependent_objects(self):\n    \"\"\"\n    non-interactive mode deletes stale content types and their dependent objects.\n    \"\"\"\n    post = Post.objects.create(title='post', content_type=self.content_type)\n    ModelWithNullFKToSite.objects.create(post=post)\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n    self.assertEqual(Post.objects.count(), 0)\n    output = stdout.getvalue()\n    self.assertIn('Deleting stale content type', output)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_non_interactive_mode_with_dependent_objects(self):\n    \"\"\"\n    Non-interactive mode deletes stale content types and their dependent objects.\n    \"\"\"\n    post = Post.objects.create(title='post', content_type=self.content_type)\n    ModelWithNullFKToSite.objects.create(post=post)\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n    self.assertEqual(Post.objects.count(), 0)\n    output = stdout.getvalue()\n    self.assertIn('Deleting stale content type', output)\n    self.assertNotIn('- Content type for contenttypes_tests.Fake', output)\n    self.assertNotIn('- 1 contenttypes_tests.Post object(s)', output)\n    self.assertNotIn('- 1 contenttypes_tests.ModelWithNullFKToSite', output)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n"], "sample_169": ["    def test_key_transform_with_expression(self):\n        obj = JSONModel.objects.create(value={'a': 'b'})\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__a=F('value__a')),\n            [obj],\n        )\n", "    def test_xml_serializer(self):\n        obj = JSONModel(value={'a': 'b', 'c': 14})\n        data = serializers.serialize('xml', [obj], fields=['value'])\n        new_obj = list(serializers.deserialize('xml', data))[0].object\n        self.assertEqual(new_obj.value, {'a': 'b', 'c': 14})\n", "    def test_serializer_deserializer(self):\n        obj = NullableJSONModel.objects.create(value={'a': 'b'})\n        data = serializers.serialize('xml', [obj])\n        deserialized_obj = next(serializers.deserialize('xml', data)).object\n        self.assertEqual(deserialized_obj.value, {'a': 'b'})\n", "    def test_xml_serializer(self):\n        data = [\n            {'model': 'jsonmodel', 'fields': {'value': '{\"a\": \"b\"}'}},\n        ]\n        xml = serializers.serialize('xml', data, fields=['value'])\n        expected = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"jsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">{&quot;a&quot;: &quot;b&quot;}</field>'\n            '</object></django-objects>'\n        )\n        self.assertXMLEqual(xml, expected)\n", "def test_key_transform_with_empty_string(self):\n    obj = NullableJSONModel.objects.create(value={'': 'empty'})\n    self.assertEqual(NullableJSONModel.objects.filter(**{'value__': 'empty'}).get(), obj)\n"], "sample_171": ["def test_migrate_syncdb_unmigrated_app_with_dependencies(self):\n    \"\"\"\n    Running migrate --run-syncdb on an unmigrated app that depends on a\n    migrated app should succeed.\n    \"\"\"\n    # Create the tables for the migrated app\n    call_command(\"migrate\", \"migrations\", verbosity=0)\n\n    # Run --run-syncdb on the unmigrated app\n    stdout = io.StringIO()\n    with mock.patch.object(BaseDatabaseSchemaEditor, 'execute') as execute:\n        call_command('migrate', 'unmigrated_app_syncdb', run_syncdb=True, stdout=stdout)\n        create_table_count = len([call for call in execute.mock_calls if 'CREATE TABLE' in str(call)])\n        self.assertEqual(create_table_count, 2)\n        self.assertGreater(len(execute.mock_calls), 2)\n        self.assertIn('Synchronize unmigrated app: unmigrated_app_syncdb', stdout.getvalue())\n", "def test_migrate_fake_initial_with_inconsistent_history(self):\n    \"\"\"\n    --fake-initial should fail if the migration history is inconsistent.\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    msg = \"Migration migrations.0002_second is applied before its dependency migrations.0001_initial\"\n    with self.assertRaisesMessage(InconsistentMigrationHistory, msg):\n        call_command(\"migrate\", \"migrations\", fake_initial=True, verbosity=0)\n", "def test_migrate_syncdb_with_deferred_sql(self):\n    \"\"\"\n    For an app without migrations, editor.execute() is used for executing the\n    syncdb deferred SQL. This test checks that create table statements are\n    executed before the deferred SQL.\n    \"\"\"\n    stdout = io.StringIO()\n    with mock.patch.object(BaseDatabaseSchemaEditor, 'execute') as execute:\n        call_command('migrate', run_syncdb=True, verbosity=1, stdout=stdout, no_color=True)\n        create_table_calls = [call for call in execute.mock_calls if 'CREATE TABLE' in str(call)]\n        deferred_sql_calls = [call for call in execute.mock_calls if 'ALTER TABLE' in str(call)]\n        self.assertGreater(len(deferred_sql_calls), 0)\n        self.assertLess(create_table_calls[0], deferred_sql_calls[0])\n", "def test_migrate_fake_initial_with_non_default_database(self):\n    \"\"\"\n    --fake-initial works with non-default databases.\n    \"\"\"\n    # Make sure no tables are created\n    self.assertTableNotExists(\"migrations_author\", using=\"other\")\n    self.assertTableNotExists(\"migrations_tribble\", using=\"other\")\n\n    call_command(\"migrate\", \"migrations\", \"0001\", database=\"other\", verbosity=0)\n    call_command(\"migrate\", \"migrations\", \"zero\", fake=True, database=\"other\", verbosity=0)\n\n    out = io.StringIO()\n    with mock.patch('django.core.management.color.supports_color', lambda *args: False):\n        call_command(\"migrate\", \"migrations\", \"0001\", fake_initial=True, database=\"other\", stdout=out, verbosity=1)\n    self.assertIn(\n        \"migrations.0001_initial... faked\",\n        out.getvalue().lower()\n    )\n\n    # Fake an apply\n    call_command(\"migrate\", \"migrations\", fake=True, database=\"other\", verbosity=0)\n    # Unmigrate everything\n    call_command(\"migrate\", \"migrations\", \"zero\", database=\"other\", verbosity=0)\n", "def test_migrate_syncdb_app_label_conflict(self):\n    \"\"\"\n    Running migrate --run-syncdb with an app_label that conflicts with a \n    migration in another app should raise a CommandError.\n    \"\"\"\n    msg = \"App label 'migrations' is used by multiple apps. Choose the correct one to syncdb.\"\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command('migrate', 'migrations', run_syncdb=True, verbosity=0)\n"], "sample_170": ["    def test_no_args(self):\n        SafeExceptionReporterFilter()\n", "    def test_get_safe_settings(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        settings_dict = reporter_filter.get_safe_settings()\n        for k in dir(settings):\n            if k.isupper():\n                self.assertIn(k, settings_dict)\n", "    def test_get_traceback_data(self):\n        reporter = ExceptionReporter(None, None, None, None)\n        data = reporter.get_traceback_data()\n        self.assertIn('django_version_info', data)\n        self.assertIn('sys_executable', data)\n        self.assertIn('sys_path', data)\n        self.assertIn('sys_version_info', data)\n        self.assertIn('server_time', data)\n        self.assertIn('settings', data)\n        self.assertIn('user_str', data)\n        self.assertIn('request_method', data)\n        self.assertIn('request_url', data)\n        self.assertIn('request_meta', data)\n        self.assertIn('filtered_POST_items', data)\n", "    def test_default_urlconf_template_used(self):\n        response = default_urlconf(None)\n        self.assertContains(response, 'Page not found')\n        self.assertContains(response, 'main urlconf is empty')\n", "    def test_get_traceback_frame_variables_safe(self):\n        filter = SafeExceptionReporterFilter()\n        tb_frame = inspect.currentframe()\n        request = RequestFactory().get('/')\n        vars = filter.get_traceback_frame_variables(request, tb_frame)\n        for name, value in vars:\n            self.assertNotIn('request', name)\n            self.assertNotIn('self', name)\n            self.assertNotIn('tb_frame', name)\n"], "sample_172": ["    def test_render(self):\n        widget = widgets.AdminURLFieldWidget()\n        output = widget.render('test', 'http://example-\u00e4\u00fc\u00f6.com/<sometag>some-text</sometag>')\n        self.assertHTMLEqual(\n            output,\n            '<p class=\"url\">Currently: <a href=\"http://xn--example--7za4pnc.com/%3Csometag%3Esome-text%3C/sometag%3E\">'\n            'http://example-\u00e4\u00fc\u00f6.com/&lt;sometag&gt;some-text&lt;/sometag&gt;</a><br>'\n            'Change:<input class=\"vURLField\" name=\"test\" type=\"url\" '\n            'value=\"http://example-\u00e4\u00fc\u00f6.com/&lt;sometag&gt;some-text&lt;/sometag&gt;\"></p>'\n        )\n", "    def test_get_list_display(self):\n        ma = admin.ModelAdmin(MyModel, admin.site)\n        request = object()\n        self.assertEqual(ma.get_list_display(request), ['__str__'])\n", "    def test_inline_admin_formset_fieldsets(self):\n        class TestInline(admin.TabularInline):\n            model = Album\n            fieldsets = [\n                (None, {'fields': ['name', 'band']}),\n                ('Advanced options', {\n                    'classes': ['collapse'],\n                    'fields': ['release_date', 'cover_art']\n                })\n            ]\n\n        formset = TestInline(Band, admin.site).get_formset(request=None)\n        expected = [\n            ('name', 'band'),\n            ('release_date', 'cover_art')\n        ]\n        self.assertEqual(formset.form.base_fields.keys(), expected)\n", "    def test_widget_overrides(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {CharField: {'widget': forms.Textarea}}\n\n        ma = MyModelAdmin(Member, admin.site)\n        ff = ma.formfield_for_dbfield(Member._meta.get_field('name'), request=None)\n        self.assertIsInstance(ff.widget, forms.Textarea)\n", "    def test_get_fieldsets(self):\n        ma = admin.ModelAdmin(Event, admin.site)\n        fieldsets = ma.get_fieldsets(request=None)\n        self.assertEqual(fieldsets, [(None, {'fields': ['name']})])\n"], "sample_173": ["    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def test_autoinc_sql(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n        self.assertIsNone(self.ops.autoinc_sql('table', 'column'))\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def test_autoinc_sql(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        self.assertIsNone(ops.autoinc_sql(table='test_table', column='id'))\n"], "sample_174": ["    def test_explain_query_raise_not_supported_error(self):\n        msg = 'This backend does not support explaining query execution.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            connection.ops.explain_query_prefix()\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def test_get_db_converters(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n        expression = 'some_expression'\n        self.assertEqual(self.ops.get_db_converters(expression), [])\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n"], "sample_175": ["def test_collector_sort(self):\n    r1 = R.objects.create()\n    r2 = R.objects.create()\n    s1 = S.objects.create(r=r1)\n    s2 = S.objects.create(r=r2)\n    collector = Collector(using='default')\n    collector.collect([s1, s2])\n    self.assertEqual(len(collector.data), 3)  # S, R, and the parent model of R\n    collector.sort()\n    self.assertEqual(list(collector.data.keys())[0], RChild)  # Child models come first\n    self.assertEqual(list(collector.data.keys())[-1], R)  # Parent models come last\n", "def test_collector_sort(self):\n    a = create_a('cascade')\n    b = B.objects.create(cascade=a.cascade)\n    collector = Collector(using='default')\n    collector.add([a], source=None, nullable=False, reverse_dependency=False)\n    collector.add([b], source=None, nullable=False, reverse_dependency=False)\n\n    # Before sorting, the order is undefined.\n    self.assertEqual(len(collector.data), 2)\n    self.assertIn(A, collector.data.keys())\n    self.assertIn(B, collector.data.keys())\n\n    collector.sort()\n\n    # After sorting, A should come before B because B has a foreign key to A.\n    self.assertEqual(list(collector.data.keys()), [B, A])\n", "def test_collector_data_update(self):\n    collector = Collector(using='default')\n    a = A.objects.create(name='test')\n    collector.collect([a])\n    self.assertIn(a, collector.data[A])\n    # Update the collector data to remove the instance\n    collector.data[A] = set()\n    self.assertNotIn(a, collector.data[A])\n    # Add a new instance\n    a2 = A.objects.create(name='test2')\n    collector.collect([a2])\n    self.assertIn(a2, collector.data[A])\n", "def test_collector_sorts_models_in_order_of_dependencies(self):\n    collector = Collector(using='default')\n    a = A.objects.create()\n    r = R.objects.create()\n    a.cascade = r\n    a.save()\n    collector.collect([a], keep_parents=False)\n    collector.sort()\n    self.assertEqual(collector.data, {R: [r], A: [a]})\n", "def test_delete_with_restricted_objects(self):\n    a = create_a('restrict')\n    b = B3.objects.create(restrict=a.restrict)\n    collector = Collector(using='default')\n    collector.collect([a.restrict], collect_related=False)\n    self.assertEqual(len(collector.data), 1)\n    self.assertIn(a.restrict, collector.data[R])\n    self.assertEqual(len(collector.restricted_objects), 1)\n    self.assertIn(R, collector.restricted_objects)\n    self.assertEqual(len(collector.restricted_objects[R]), 1)\n    self.assertIn(B3._meta.get_field('restrict'), collector.restricted_objects[R])\n    self.assertEqual(len(collector.restricted_objects[R][B3._meta.get_field('restrict')]), 1)\n    self.assertIn(b, collector.restricted_objects[R][B3._meta.get_field('restrict')])\n"], "sample_176": ["def test_add_model_with_field_removed_from_base_model_different_app(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name, even if they are in different apps.\n    \"\"\"\n    before = [\n        ModelState('app1', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app1', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app2', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app1.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app1', 1)\n    self.assertOperationTypes(changes, 'app1', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app1', 0, 0, name='title', model_name='readable')\n    self.assertNumberMigrations(changes, 'app2', 1)\n    self.assertOperationTypes(changes, 'app2', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app2', 0, 0, name='book')\n", "def test_mti_inheritance_model_removal_with_fks(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    Kennel = ModelState('app', 'Kennel', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"dog\", models.ForeignKey(\"app.Dog\", models.CASCADE)),\n    ])\n    changes = self.get_changes([Animal, Dog, Kennel], [Animal, Kennel])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='dog', model_name='kennel')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n", "def test_add_model_with_unique_field_removed_from_base_model(self):\n    \"\"\"\n    Removing a unique field from a base model takes place before adding a new \n    inherited model that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200, unique=True)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200, unique=True)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_add_model_with_field_removed_from_grandparent_model(self):\n    \"\"\"\n    Removing a field from a grandparent model takes place before adding a new \n    inherited model that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book_base', [], bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book_base', [], bases=('app.readable',)),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.book_base',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_alter_model_table_with_inheritance(self):\n    \"\"\"\n    Test that AlterModelTable is correctly generated for a model and its \n    subclasses when the db_table is changed.\n    \"\"\"\n    before = [\n        ModelState('app', 'Animal', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Dog', [], bases=('app.Animal',)),\n    ]\n    after = [\n        ModelState('app', 'Animal', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={'db_table': 'new_animal'}),\n        ModelState('app', 'Dog', [], bases=('app.Animal',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='animal', table='new_animal')\n"], "sample_177": ["def test_get_related_models_recursive_with_custom_model(self):\n    class CustomModel(models.Model):\n        pass\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(CustomModel))\n    related_models = get_related_models_recursive(CustomModel)\n\n    self.assertEqual(related_models, {(CustomModel._meta.app_label, CustomModel._meta.model_name)})\n", "def test_clone_constraints(self):\n    \"\"\"\n    Cloning a ModelState should clone its constraints.\n    \"\"\"\n    field = models.IntegerField()\n    constraint = models.CheckConstraint(check=models.Q(field__gt=1), name='field_gt_1')\n    original_state = ModelState('app', 'Model', [('field', field)], options={'constraints': [constraint]})\n    cloned_state = original_state.clone()\n    self.assertIsNot(original_state.options['constraints'], cloned_state.options['constraints'])\n    self.assertEqual(original_state.options['constraints'], cloned_state.options['constraints'])\n", "def test_model_state_equality_with_different_manager_definitions(self):\n    manager1 = models.Manager()\n    manager2 = models.Manager()\n\n    state1 = ModelState(\n        app_label='migrations',\n        name='Tag',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n        ],\n        managers=[('manager', manager1)],\n    )\n\n    state2 = ModelState(\n        app_label='migrations',\n        name='Tag',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n        ],\n        managers=[('manager', manager2)],\n    )\n\n    self.assertEqual(state1, state2)\n", "def test_get_related_models_recursive_with_self_referential_fk(self):\n    \"\"\"\n    get_related_models_recursive() should not enter an infinite loop with a self-referential FK.\n    \"\"\"\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('A', models.CASCADE)])\n    self.assertRelated(A, [])\n", "def test_reload_model_with_self_referential_field(self):\n    \"\"\"\n    Reloading a model with a self-referential field should not cause infinite recursion.\n    \"\"\"\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"Tag\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"parent\", models.ForeignKey(\"migrations.Tag\", models.CASCADE, null=True)),\n        ],\n    ))\n    project_state.apps  # Render project state.\n    project_state.reload_model('migrations', 'tag', delay=True)\n    Tag = project_state.apps.get_model('migrations.Tag')\n    self.assertIs(Tag._meta.get_field('parent').related_model, Tag)\n"], "sample_178": ["def test_formset_media(self):\n    \"\"\"A formset's media is the union of its form's media.\"\"\"\n    class MediaForm(Form):\n        class Media:\n            js = ('some-file.js',)\n\n    FormSet = formset_factory(MediaForm, extra=2)\n    formset = FormSet()\n    self.assertIn('some-file.js', str(formset.media))\n", "def test_total_error_count_with_non_form_errors(self):\n    \"\"\"A formset's total error count includes non-form errors.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '1',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertEqual(formset.total_error_count(), 1)\n    data['choices-1-votes'] = ''\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertEqual(formset.total_error_count(), 2)\n", "def test_get_default_prefix(self):\n    \"\"\"The default prefix for a formset is 'form'.\"\"\"\n    self.assertEqual(BaseFormSet.get_default_prefix(), 'form')\n", "def test_formset_add_fields(self):\n    \"\"\"Formset.add_fields is called for each form.\"\"\"\n    class AddFieldsFormSet(BaseFormSet):\n            form.fields['extra_field'] = CharField()\n\n    AddFieldsFormSet = formset_factory(Choice, formset=AddFieldsFormSet)\n    formset = AddFieldsFormSet()\n    for form in formset.forms:\n        self.assertIn('extra_field', form.fields)\n", "def test_formset_is_bound_with_files(self):\n    \"\"\"A formset is bound if files are provided, even if data is empty.\"\"\"\n    formset = self.make_choiceformset(files={'choices-0-votes': '100'})\n    self.assertTrue(formset.is_bound)\n    self.assertTrue(formset.forms[0].is_bound)\n"], "sample_180": ["    def test_model_attribute(self):\n        class TestModel(models.Model):\n            pass\n\n        self.assertEqual(TestModel.check(), [])\n", "    def test_model_with_name_clash(self):\n        class Model(models.Model):\n            some_field = models.CharField(max_length=100)\n\n            class Meta:\n                db_table = 'model'\n\n        class Model(models.Model):\n            other_field = models.CharField(max_length=100)\n\n            class Meta:\n                db_table = 'model'\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"db_table 'model' is used by multiple models: \"\n                \"invalid_models_tests.Model, invalid_models_tests.Model.\",\n                obj=Model,\n                id='models.E028',\n            ),\n        ])\n", "    def test_unique_together_non_iterable(self):\n        class Model(models.Model):\n            class Meta:\n                unique_together = 'not-a-list'\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'unique_together' must be a list or tuple.\",\n                obj=Model,\n                id='models.E010',\n            ),\n        ])\n", "    def test_model_with_manager(self):\n        class Model(models.Model):\n            objects = models.Manager()\n\n        self.assertEqual(Model.check(), [])\n", "    def test_model_attribute_name_clashes_with_field_name(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n\n                self.name = 'Some Name'\n                super().__init__(*args, **kwargs)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The field 'name' clashes with the model attribute 'name'.\",\n                obj=Model._meta.get_field('name'),\n                id='models.E006',\n            )\n        ])\n"], "sample_179": ["    def test_check_lazy_reference(self):\n        class Model(models.Model):\n            fk = models.ForeignKey('MissingModel', models.CASCADE)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The field invalid_models_tests.Model.fk was declared with a \"\n                \"lazy reference to 'invalid_models_tests.missingmodel', but app \"\n                \"'invalid_models_tests' isn't installed.\",\n                hint=None,\n                obj=Model._meta.get_field('fk'),\n                id='fields.E307',\n            ),\n        ])\n", "    def test_model_validation(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=10)\n\n                if self.field != 'test':\n                    raise ValidationError('field must be \"test\"')\n\n        model = Model(field='wrong')\n        with self.assertRaises(ValidationError):\n            model.full_clean()\n", "    def test_ordering_with_expression(self):\n        class Model(models.Model):\n            some_field = models.IntegerField()\n\n            class Meta:\n                ordering = [Lower('some_field')]\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'ordering' refers to the nonexistent field, related field, \"\n                \"or lookup 'some_field'.\",\n                obj=Model,\n                id='models.E015',\n            )\n        ])\n", "    def test_full_clean(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=10)\n\n        instance = Model(field='Hello, World!')\n        with self.assertRaises(ValidationError) as cm:\n            instance.full_clean()\n        self.assertEqual(cm.exception.error_dict, {\n            'field': ['Ensure this value has at most 10 characters (it has 13).'],\n        })\n", "    def test_model_fields_validation(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=10)\n\n        instance = Model()\n        with self.assertRaises(ValidationError):\n            instance.full_clean()\n"], "sample_182": ["def test_union_with_subqueries(self):\n    subquery1 = Number.objects.filter(num__lte=5).values_list('num', flat=True)\n    subquery2 = Number.objects.filter(num__gte=5).values_list('num', flat=True)\n    qs1 = Number.objects.filter(num__in=subquery1)\n    qs2 = Number.objects.filter(num__in=subquery2)\n    self.assertNumbersEqual(qs1.union(qs2), list(range(10)))\n", "def test_union_with_ordered_subqueries(self):\n    qs1 = Number.objects.order_by('-num')[:3]\n    qs2 = Number.objects.order_by('num')[:3]\n    self.assertNumbersEqual(qs1.union(qs2), [9, 8, 7, 0, 1, 2], ordered=False)\n", "def test_union_with_subqueries(self):\n    subquery1 = Number.objects.filter(num__lte=1).values_list('num', flat=True)\n    subquery2 = Number.objects.filter(num__gte=8).values_list('num', flat=True)\n    qs1 = Number.objects.filter(num__in=subquery1)\n    qs2 = Number.objects.filter(num__in=subquery2)\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 8, 9], ordered=False)\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.none()\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1], ordered=False)\n    self.assertNumbersEqual(qs2.union(qs1), [0, 1], ordered=False)\n", "def test_order_by_different_type(self):\n    qs1 = Number.objects.all()\n    qs2 = ReservedName.objects.all()\n    union = qs1.union(qs2)\n    msg = 'ORDER BY term does not match any column in the result set'\n    with self.assertRaisesMessage(DatabaseError, msg):\n        list(union.order_by('num'))\n    with self.assertRaisesMessage(DatabaseError, msg):\n        list(union.order_by('name'))\n"], "sample_181": ["def test_filtered_aggregate_ref_subquery_annotation_with_filter(self):\n    aggs = Author.objects.annotate(\n        earliest_book_year=Subquery(\n            Book.objects.filter(\n                contact__pk=OuterRef('pk'),\n                rating__gt=3,\n            ).order_by('pubdate').values('pubdate__year')[:1]\n        ),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(earliest_book_year=2008)),\n    )\n    self.assertEqual(aggs['cnt'], 1)\n", "def test_filtered_aggregate_ref_annotation_in_subquery(self):\n    subquery = Author.objects.annotate(\n        double_age=F('age') * 2,\n    ).filter(double_age__gt=100).values('pk')\n    aggs = Author.objects.aggregate(\n        cnt=Count('pk', filter=Q(pk__in=subquery)),\n    )\n    self.assertEqual(aggs['cnt'], 2)\n", "def test_filtered_aggregate_ref_annotation_in_subquery(self):\n    subquery = Author.objects.annotate(\n        double_age=F('age') * 2,\n    ).filter(double_age__gt=100).values('pk')\n    aggs = Author.objects.aggregate(\n        cnt=Count('pk', filter=Q(pk__in=subquery)),\n    )\n    self.assertEqual(aggs['cnt'], 2)\n", "def test_filtered_aggregate_with_exists(self):\n    subquery = Book.objects.filter(\n        contact__pk=OuterRef('pk'),\n        rating__gt=4,\n    )\n    aggs = Author.objects.annotate(\n        has_highly_rated_book=Exists(subquery),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(has_highly_rated_book=True)),\n    )\n    self.assertEqual(aggs['cnt'], 2)\n", "def test_filtered_aggregate_ref_annotation_from_subquery(self):\n    subquery = Author.objects.annotate(\n        double_age=F('age') * 2,\n    ).filter(double_age__gt=100)\n    aggs = Author.objects.aggregate(\n        cnt=Count('pk', filter=Q(pk__in=subquery.values('pk'))),\n    )\n    self.assertEqual(aggs['cnt'], 2)\n"], "sample_183": ["    def test_expression_wrapper(self):\n        wrapper = ExpressionWrapper(expression=Value(1), output_field=IntegerField())\n        self.assertEqual(wrapper.output_field.get_internal_type(), 'IntegerField')\n        self.assertEqual(wrapper.as_sql(compiler=None, connection=None), ('%s', [1]))\n", "    def test_resolve_expression_with_join(self):\n        expression = F('fk_rel__integer')\n        resolved_expression = expression.resolve_expression(\n            query=None, allow_joins=True, reuse=None, summarize=False, for_save=False\n        )\n        self.assertEqual(resolved_expression.target, 'fk_rel__integer')\n", "    def test_expression_wrapper(self):\n        wrapper = ExpressionWrapper(expression=Value(1), output_field=IntegerField())\n        self.assertEqual(wrapper.output_field.get_internal_type(), 'IntegerField')\n        self.assertEqual(wrapper.as_sql(None, None), ('%s', [1]))\n", "    def test_when_equality(self):\n        when1 = When(Q(pk=1), then=Value('value'))\n        when2 = When(Q(pk=1), then=Value('value'))\n        self.assertEqual(when1, when2)\n", "    def test_equality(self):\n        self.assertEqual(\n            When(condition='foo', then='bar'),\n            When(condition='foo', then='bar')\n        )\n        self.assertNotEqual(\n            When(condition='foo', then='bar'),\n            When(condition='baz', then='bar')\n        )\n        self.assertNotEqual(\n            When(condition='foo', then='bar'),\n            When(condition='foo', then='baz')\n        )\n"], "sample_184": ["    def test_model_with_abstract_base_and_mixin(self):\n        class AbstractBase(models.Model):\n            class Meta:\n                abstract = True\n\n        class Mixin:\n            pass\n\n        class Model(Mixin, AbstractBase):\n            pass\n\n        self.assertEqual(Model.check(), [])\n", "    def test_abstract_model_with_deferred_field(self):\n        class AbstractModel(models.Model):\n            field = models.IntegerField()\n\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(AbstractModel):\n            @property\n                return 42\n\n        self.assertEqual(ConcreteModel.check(), [\n            Error(\n                \"The field 'field' clashes with the field 'field' from \"\n                \"model 'invalid_models_tests.abstractmodel'.\",\n                obj=ConcreteModel,\n                id='models.E006',\n            )\n        ])\n", "    def test_model_state_fields_cache_descriptor(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        instance = Model(field1='value1', field2='value2')\n        self.assertEqual(instance._state.fields_cache, {})\n        instance.refresh_from_db()\n        self.assertEqual(instance._state.fields_cache, {'field1': 'value1', 'field2': 'value2'})\n", "    def test_unique_for_date(self):\n        class Model(models.Model):\n            title = models.CharField(max_length=100, unique_for_date='pub_date')\n            pub_date = models.DateField()\n\n        self.assertEqual(Model.check(), [])\n\n        class Model(models.Model):\n            title = models.CharField(max_length=100, unique_for_date='nonexistent_field')\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'models.E029')\n", "    def test_refresh_from_db_with_deferred_fields(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        instance = Model.objects.create(field1='value1', field2='value2')\n        instance.refresh_from_db(fields=['field1'])\n        self.assertEqual(instance.get_deferred_fields(), {'field2'})\n"], "sample_185": ["    def test_reset_format_cache(self):\n        # Populate _format_cache with temporary values\n        _format_cache[('DATE_FORMAT', 'en')] = 'temporary date format'\n        _format_cache[('TIME_FORMAT', 'en')] = 'temporary time format'\n        self.assertEqual(len(_format_cache), 2)\n        \n        reset_format_cache()\n        self.assertEqual(len(_format_cache), 0)\n", "    def test_sanitize_separators(self):\n        tests = [\n            ('1.234,56', '1234.56', {'DECIMAL_SEPARATOR': ',', 'THOUSAND_SEPARATOR': '.'}),\n            ('1,234.56', '1234.56', {'DECIMAL_SEPARATOR': '.', 'THOUSAND_SEPARATOR': ','}),\n            ('1 234,56', '1234.56', {'DECIMAL_SEPARATOR': ',', 'THOUSAND_SEPARATOR': ' '}),\n            ('1\\xa0234,56', '1234.56', {'DECIMAL_SEPARATOR': ',', 'THOUSAND_SEPARATOR': '\\xa0'}),\n            ('12,34', '12.34', {'DECIMAL_SEPARATOR': ',', 'THOUSAND_SEPARATOR': '.'}),\n            ('12.34', '12.34', {'DECIMAL_SEPARATOR': '.', 'THOUSAND_SEPARATOR': ','}),\n        ]\n        for value, expected, format_settings in tests:\n            with self.subTest(value=value), patch_formats(get_language(), **format_settings):\n                self.assertEqual(sanitize_separators(value), expected)\n", "    def test_reset_format_cache(self):\n        # Populate _format_cache with temporary values\n        _format_cache[('DATE_FORMAT', 'en')] = 'custom'\n        self.assertEqual(get_format('DATE_FORMAT', lang='en'), 'custom')\n        reset_format_cache()\n        # After resetting, it should fall back to the project settings.\n        self.assertEqual(get_format('DATE_FORMAT', lang='en'), 'N j, Y')\n", "    def test_reset_cache(self):\n        # Populate _format_cache with temporary values\n        _format_cache[('DATE_FORMAT', 'en')] = '%Y-%m-%d'\n        _format_cache[('TIME_FORMAT', 'fr')] = '%H:%M:%S'\n        self.assertEqual(len(_format_cache), 2)\n        reset_format_cache()\n        self.assertEqual(len(_format_cache), 0)\n", "    def test_decimal_separator(self):\n        tests = [\n            ('12,34', '12.34'),\n            ('12.34', '12.34'),\n            ('12 34', '1234'),\n            ('12\\xa034', '1234'),\n        ]\n        for value, expected in tests:\n            with self.subTest(value=value):\n                self.assertEqual(sanitize_separators(value), expected)\n"], "sample_186": ["def test_autocomplete_fields_relation(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ['album']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ['title']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n            obj=SongAdmin,\n            id='admin.E038',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_actions_uniqueness(self):\n    class MyAdmin(admin.ModelAdmin):\n                pass\n\n                pass\n\n            return {\n                'action1': (action1, 'action1', 'Action 1'),\n                'action2': (action1, 'action2', 'Action 2'),  # duplicate action\n            }\n\n    errors = MyAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            '__name__ attributes of actions defined in %s must be unique. '\n            'Name \\'action1\\' is not unique.' % (\n                MyAdmin.__name__,\n            ),\n            obj=MyAdmin,\n            id='admin.E130',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_item(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        list_filter = ['nonexistent']\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent', which does not refer to a Field.\",\n            obj=MyModelAdmin,\n            id='admin.E116',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_works_on_through_field_with_invalid_field_name(self):\n    \"\"\"\n    Ensure list_filter raises an error when trying to access a non-existent field.\n    \"\"\"\n    class BookAdminWithListFilter(admin.ModelAdmin):\n        list_filter = ['authorsbooks__invalid_field']\n\n    errors = BookAdminWithListFilter(Book, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'authorsbooks__invalid_field', \"\n            \"which does not refer to a Field.\",\n            obj=BookAdminWithListFilter,\n            id='admin.E116',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_ordering_item(self):\n    class SongAdmin(admin.ModelAdmin):\n        ordering = ('nonexistent',)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering[0]' refers to 'nonexistent', which is not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E033',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_187": ["def test_camel_case_to_spaces(self):\n    items = [\n        ('CamelCase', 'camel case'),\n        ('CamelCamelCase', 'camel camel case'),\n        ('_CamelCase', '_camel case'),\n        ('__CamelCase', '__camel case'),\n        ('123CamelCase', '123camel case'),\n        (' CamelCase', ' camel case'),\n        ('Camel Case', 'camel  case'),  # Note the double space\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n    # lazy strings are handled correctly\n    self.assertEqual(text.camel_case_to_spaces(lazystr('CamelCase')), 'camel case')\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('helloWorld', 'hello world'),\n        ('HelloWorld', 'hello world'),\n        ('helloHTMLWorld', 'hello html world'),\n        ('hello_World', 'hello_world'),\n        ('hello__World', 'hello__world'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n    # lazy strings are handled correctly\n    self.assertEqual(text.camel_case_to_spaces(lazystr('helloWorld')), 'hello world')\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('HelloWorld', 'hello world'),\n        ('helloWorld', 'hello world'),\n        ('HelloWORLD', 'hello world'),\n        ('helloWORLD', 'hello world'),\n        ('helloworld', 'helloworld'),\n        ('Hello', 'hello'),\n        ('hello', 'hello'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        ('CamelCase', 'camel case'),\n        ('CAMELCase', 'camel case'),\n        ('_camelCase', '_camel case'),\n        (' camelCase', ' camel case'),\n        ('camel Case', 'camel  case'),\n        ('getHTTPResponseCode', 'get http response code'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('helloWorld', 'hello world'),\n        ('HelloWorld', 'hello world'),\n        ('helloHTMLWorld', 'hello html world'),\n        ('hello-world', 'hello-world'),\n        ('hello_world', 'hello_world'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n    # Test with lazy string\n    self.assertEqual(text.camel_case_to_spaces(lazystr('helloWorld')), 'hello world')\n"], "sample_188": ["    def test_empty_expression_list(self):\n        msg = 'ExpressionList requires at least one expression'\n        with self.assertRaisesMessage(ValueError, msg):\n            ExpressionList()\n", "    def test_repr(self):\n        expr = ExpressionList(F('col1'), F('col2'))\n        self.assertEqual(repr(expr), 'ExpressionList(F(col1), F(col2))')\n", "    def test_deconstruct(self):\n        outer_ref = OuterRef('name')\n        path, args, kwargs = outer_ref.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.OuterRef')\n        self.assertEqual(args, (outer_ref.name,))\n        self.assertEqual(kwargs, {})\n", "    def test_deconstruct(self):\n        frame = RowRange(start=1, end=10)\n        path, args, kwargs = frame.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.WindowFrame')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'start': 1, 'end': 10})\n", "    def test_repr(self):\n        expr = ExpressionList(F('a'), F('b'))\n        self.assertEqual(repr(expr), \"ExpressionList(F(a), F(b))\")\n"], "sample_189": ["def test_cache_key_warnings(self):\n    # Ensure that only a single warning is generated when a cache key\n    # contains a warning character. See #6744.\n    bad_key = 'key with spaces and \\n'\n    expected_warnings = [\n        KEY_ERRORS_WITH_MEMCACHED_MSG % bad_key,\n        \"Cache key contains characters that will cause errors if used \"\n        \"with memcached: %r (unprintable chars)\" % bad_key,\n    ]\n    with self.assertWarnsMessage(CacheKeyWarning, expected_warnings[0]):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            cache.set(bad_key, 'value')\n            self.assertEqual(len(w), 1)\n            self.assertTrue(issubclass(w[-1].category, CacheKeyWarning))\n            self.assertEqual(str(w[-1].message), expected_warnings[0])\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.path = '/cache/test/'\n        self.request = self.factory.get(self.path)\n", "    def test_without_vary_on(self):\n        key = make_template_fragment_key('a.fragment')\n        self.assertEqual(key, 'template.cache.cacheprefix.a.fragment.d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_incr_with_version(self):\n        cache.set('answer', 42, version=1)\n        self.assertEqual(cache.get('answer', version=1), 42)\n        self.assertEqual(cache.incr('answer', version=1), 43)\n        self.assertEqual(cache.get('answer', version=1), 43)\n", "    def test_get_cache_key_with_empty_vary(self):\n        request = self.factory.get('/view/')\n        response = HttpResponse()\n        response['Vary'] = ''\n        learn_cache_key(request, response)\n        key = get_cache_key(request)\n        self.assertIn('views.decorators.cache.cache_page', key)\n"], "sample_190": ["def test_range_lookup_type_mismatch(self):\n    msg = \"The 'range' lookup type requires two parameters\"\n    with self.assertRaisesMessage(ValueError, msg):\n        Article.objects.filter(pub_date__range='2005-07-27')\n", "def test_filtering_on_subqueries_with_exists(self):\n    subquery = Article.objects.filter(author=self.au1)\n    self.assertCountEqual(\n        Author.objects.filter(Exists(subquery.filter(author=OuterRef('pk')))),\n        [self.au1]\n    )\n    self.assertCountEqual(\n        Author.objects.exclude(Exists(subquery.filter(author=OuterRef('pk')))),\n        [self.au2]\n    )\n", "def test_range_lookup_optimization(self):\n    # The range lookup type can be optimized by the database. Make sure that\n    # works.\n    Season.objects.create(year=2012, gt=None)\n    Season.objects.create(year=2013, gt=None)\n    self.assertCountEqual(\n        Season.objects.filter(year__range=[2012, 2013]),\n        Season.objects.all(),\n    )\n", "def test_in_bulk_with_subquery(self):\n    subquery = Article.objects.filter(author=self.au1).values('id')\n    articles = Article.objects.in_bulk(subquery)\n    self.assertEqual(len(articles), 4)\n    self.assertIn(self.a1.id, articles)\n    self.assertIn(self.a2.id, articles)\n    self.assertIn(self.a3.id, articles)\n    self.assertIn(self.a4.id, articles)\n", "def test_in_bulk_with_string_primary_key(self):\n    article = Article.objects.create(headline='Test', slug='test-slug')\n    articles = Article.objects.in_bulk(['test-slug'], field_name='slug')\n    self.assertEqual(articles['test-slug'], article)\n"], "sample_191": ["    def test_ensure_echo_on_non_tty(self, mock_isatty):\n        # Should not raise an exception.\n        autoreload.ensure_echo_on()\n", "    def test_not_a_tty(self, mocked_isatty):\n        # If not a tty, it shouldn't attempt to do anything.\n        with mock.patch('termios.tcgetattr') as mocked_tcgetattr:\n            autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcgetattr.called)\n", "    def test_ensure_echo_on_when_termios_available(self, mocked_termios):\n        with mock.patch.object(sys.stdin, 'isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_non_tty(self, mocked_isatty):\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_isatty.call_count, 1)\n", "    def test_extra_files(self):\n        reloader = autoreload.BaseReloader()\n        file1 = Path('file1.txt')\n        file2 = Path('file2.txt')\n        reloader.extra_files = {file1, file2}\n        watched_files = list(reloader.watched_files())\n        self.assertIn(file1, watched_files)\n        self.assertIn(file2, watched_files)\n"], "sample_192": ["def test_formset_media(self):\n    \"\"\"A formset's media is the collection of all its forms' media.\"\"\"\n    class MediaForm(Form):\n        class Media:\n            js = ('some-file.js',)\n\n    self.assertIn('some-file.js', str(formset_factory(MediaForm)().media))\n    self.assertIn('some-file.js', str(formset_factory(MediaForm, extra=2)().media))\n", "def test_formset_is_multipart_with_filefield_in_empty_form(self):\n    \"\"\"is_multipart() works with a formset that contains a FileField.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    formset = formset_factory(FileForm, extra=1)()\n    self.assertTrue(formset.is_multipart())\n    self.assertTrue(formset.empty_form.is_multipart())\n", "def test_formset_add_fields_with_can_delete(self):\n    \"\"\"Formsets can dynamically add fields with can_delete.\"\"\"\n    class DynamicFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            if self.can_delete:\n                form.fields['custom_delete'] = BooleanField(label='Custom Delete', required=False)\n\n    ChoiceFormSet = formset_factory(Choice, formset=DynamicFormSet, can_delete=True)\n    initial = [{'choice': 'Calexico', 'votes': 100}]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    self.assertHTMLEqual(\n        '\\n'.join(form.as_ul() for form in formset.forms),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_formset_absolute_max(self):\n    \"\"\"A formset has an absolute maximum number of forms.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=1)\n    # someone fiddles with the mgmt form data...\n    data = {\n        'choices-TOTAL_FORMS': '2001',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '1000',\n    }\n    for i in range(2000):\n        data[f'choices-{i}-choice'] = f'Zero {i}'\n        data[f'choices-{i}-votes'] = 0\n    formset = ChoiceFormSet(data, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit 1000 or fewer forms.'])\n", "def test_formset_non_field_error_class(self):\n    \"\"\"Formset's non-form errors use the formset's error_class.\"\"\"\n    class CustomErrorList(ErrorList):\n        pass\n\n    formset = FavoriteDrinksFormSet(error_class=CustomErrorList)\n    self.assertEqual(formset.error_class, CustomErrorList)\n    self.assertEqual(formset.non_form_errors().__class__, CustomErrorList)\n"], "sample_193": ["    def test_related_field_check(self):\n        new_apps = Apps()\n\n        class Target(models.Model):\n            model = models.IntegerField()\n            model_set = models.IntegerField()\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Model(models.Model):\n            foreign = models.ForeignKey(Target, models.CASCADE)\n            m2m = models.ManyToManyField(Target)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        field_state = project_state.models['migrations', 'model'].fields['foreign']\n        errors = field_state.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(\n            errors[0].msg,\n            \"Reverse accessor for 'Model.foreign' clashes with field name 'Target.model'.\",\n        )\n        self.assertEqual(\n            errors[0].hint,\n            \"Rename field 'Target.model', or add/change a related_name argument to the definition for field 'Model.foreign'.\",\n        )\n        self.assertEqual(errors[0].obj, field_state)\n\n        field_state = project_state.models['migrations', 'model'].fields['m2m']\n        errors = field_state.check()\n        self.assertEqual(len(errors), 2)\n        self.assertEqual(\n            errors[0].msg,\n            \"Reverse accessor for 'Model.m2m' clashes with field name 'Target.model'.\",\n        )\n        self.assertEqual(\n            errors[0].hint,\n            \"Rename field 'Target.model', or add/change a related_name argument to the definition for field 'Model.m2m'.\",\n        )\n        self.assertEqual(errors[0].obj, field_state)\n        self.assertEqual(\n            errors[1].msg,\n            \"Reverse accessor for 'Model.m2m' clashes with reverse accessor for 'Model.foreign'.\",\n        )\n        self.assertEqual(\n            errors[1].hint,\n            \"Add or change a related_name argument to the definition for 'Model.m2m' or 'Model.foreign'.\",\n        )\n        self.assertEqual(errors[1].obj, field_state)\n", "def test_check_on_delete_target(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk = models.ForeignKey(Target, on_delete=models.SET_NULL)\n\n    model_field = Model._meta.get_field('fk')\n    errors = model_field.check()\n    self.assertEqual(errors, [\n        checks.Error(\n            'Field specifies on_delete=SET_NULL, but cannot be null.',\n            hint='Set null=True argument on the field, or change the on_delete rule.',\n            obj=model_field,\n            id='fields.E320',\n        )\n    ])\n\n    class Model(models.Model):\n        fk = models.ForeignKey(Target, on_delete=models.SET_DEFAULT)\n\n    model_field = Model._meta.get_field('fk')\n    errors = model_field.check()\n    self.assertEqual(errors, [\n        checks.Error(\n            'Field specifies on_delete=SET_DEFAULT, but has no default value.',\n            hint='Set a default value, or change the on_delete rule.',\n            obj=model_field,\n            id='fields.E321',\n        )\n    ])\n", "def test_recursive_m2m(self):\n    \"\"\"\n    Recursive M2M fields are correctly rendered and don't cause infinite loops.\n    \"\"\"\n    new_apps = Apps()\n\n    class A(models.Model):\n        a_set = models.ManyToManyField('A', symmetrical=False)\n\n        class Meta:\n            app_label = 'migrations'\n            apps = new_apps\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(A))\n    final_apps = project_state.apps\n    self.assertEqual(len(final_apps.get_models()), 1)\n    A = final_apps.get_model('migrations', 'A')\n    self.assertEqual(len(A._meta.many_to_many), 1)\n    m2m_field = A._meta.many_to_many[0]\n    self.assertIs(m2m_field.related_model, A)\n    self.assertEqual(m2m_field.remote_field.through.__name__, 'A_a_set')\n", "def test_get_path_to_parent(self):\n    class A(models.Model):\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(A))\n    project_state.add_model(ModelState.from_model(B))\n    project_state.add_model(ModelState.from_model(C))\n\n    model_c_opts = project_state.models['migrations', 'c'].options\n    path_to_parent = model_c_opts.get_path_to_parent(B)\n    self.assertEqual(len(path_to_parent), 1)\n    self.assertEqual(path_to_parent[0].target_opts.model_name, 'b')\n\n    model_b_opts = project_state.models['migrations', 'b'].options\n    path_to_parent = model_b_opts.get_path_to_parent(A)\n    self.assertEqual(len(path_to_parent), 1)\n    self.assertEqual(path_to_parent[0].target_opts.model_name, 'a')\n", "    def test_m2m_through_model(self):\n        new_apps = Apps()\n\n        class Author(models.Model):\n            name = models.TextField()\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Book(models.Model):\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Contract(models.Model):\n            authors = models.ManyToManyField(Author, through='migrations.Book')\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(Author))\n        project_state.add_model(ModelState.from_model(Book))\n        project_state.add_model(ModelState.from_model(Contract))\n\n        # Make sure the through model is in the app registry before looking for related models.\n        project_state.apps  # Fill the apps cache\n\n        related_models = get_related_models_recursive(project_state.apps.get_model(\"migrations\", \"Author\"))\n        self.assertEqual(len(related_models), 3)\n        self.assertIn((\"migrations\", \"book\"), related_models)\n        self.assertIn((\"migrations\", \"contract\"), related_models)\n"], "sample_194": ["def test_clone(self):\n    check = models.Q(price__gt=models.F('discounted_price'))\n    name = 'price_gt_discounted_price'\n    constraint = models.CheckConstraint(check=check, name=name)\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint.name, cloned_constraint.name)\n    self.assertEqual(constraint.check, cloned_constraint.check)\n", "def test_clone(self):\n    constraint = models.CheckConstraint(check=models.Q(price__gt=models.F('discounted_price')), name='price')\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint.name, cloned_constraint.name)\n    self.assertEqual(constraint.check, cloned_constraint.check)\n", "def test_clone(self):\n    constraint = models.UniqueConstraint(fields=['foo', 'bar'], name='unique')\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n\n    constraint_with_condition = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        condition=models.Q(foo=models.F('bar'))\n    )\n    cloned_constraint_with_condition = constraint_with_condition.clone()\n    self.assertEqual(constraint_with_condition, cloned_constraint_with_condition)\n    self.assertIsNot(constraint_with_condition, cloned_constraint_with_condition)\n\n    constraint_with_deferrable = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    cloned_constraint_with_deferrable = constraint_with_deferrable.clone()\n    self.assertEqual(constraint_with_deferrable, cloned_constraint_with_deferrable)\n    self.assertIsNot(constraint_with_deferrable, cloned_constraint_with_deferrable)\n\n    constraint_with_include = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        include=['baz_1', 'baz_2'],\n    )\n    cloned_constraint_with_include = constraint_with_include.clone()\n    self.assertEqual(constraint_with_include, cloned_constraint_with_include)\n    self.assertIsNot(constraint_with_include, cloned_constraint_with_include)\n\n    constraint_with_opclasses = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        opclasses=['text_pattern_ops', 'varchar_pattern_ops'],\n    )\n    cloned_constraint_with_opclasses = constraint_with_opclasses.clone()\n    self.assertEqual(constraint_with_opclasses, cloned_constraint_with_opclasses)\n    self.assertIsNot(constraint_with_opclasses, cloned_constraint_with_opclasses)\n", "def test_clone(self):\n    constraint = models.UniqueConstraint(fields=['foo', 'bar'], name='unique')\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n\n    constraint_with_condition = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        condition=models.Q(foo=models.F('bar')),\n    )\n    cloned_constraint_with_condition = constraint_with_condition.clone()\n    self.assertEqual(constraint_with_condition, cloned_constraint_with_condition)\n    self.assertIsNot(constraint_with_condition, cloned_constraint_with_condition)\n\n    constraint_with_deferrable = models.UniqueConstraint(\n        fields=['foo'],\n        name='unique',\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    cloned_constraint_with_deferrable = constraint_with_deferrable.clone()\n    self.assertEqual(constraint_with_deferrable, cloned_constraint_with_deferrable)\n    self.assertIsNot(constraint_with_deferrable, cloned_constraint_with_deferrable)\n\n    constraint_with_include = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='include',\n        include=['baz_1', 'baz_2'],\n    )\n    cloned_constraint_with_include = constraint_with_include.clone()\n    self.assertEqual(constraint_with_include, cloned_constraint_with_include)\n    self.assertIsNot(constraint_with_include, cloned_constraint_with_include)\n\n    constraint_with_opclasses = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='opclasses',\n        opclasses=['text_pattern_ops', 'varchar_pattern_ops'],\n    )\n    cloned_constraint_with_opclasses = constraint_with_opclasses.clone()\n    self.assertEqual(constraint_with_opclasses, cloned_constraint_with_opclasses)\n    self.assertIsNot(constraint_with_opclasses, cloned_constraint_with_opclasses)\n", "def test_clone(self):\n    constraint = models.UniqueConstraint(fields=['foo', 'bar'], name='unique')\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n\n    constraint_with_condition = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        condition=models.Q(foo=models.F('bar')),\n    )\n    cloned_constraint_with_condition = constraint_with_condition.clone()\n    self.assertEqual(constraint_with_condition, cloned_constraint_with_condition)\n    self.assertIsNot(constraint_with_condition, cloned_constraint_with_condition)\n"], "sample_195": ["    def setUp(self):\n        self.ops = connection.ops\n", "    def setUp(self):\n        self.ops = connection.ops\n", "    def test_datetime_cast_date_sql(self):\n        ops = connection.ops\n        tzname = timezone.get_default_timezone_name()\n        field_name = 'created_at'\n        sql = ops.datetime_cast_date_sql(field_name, tzname)\n        self.assertEqual(sql, 'django_datetime_cast_date(%s, %s, %s)' % (field_name, \"'%s'\" % tzname, \"'%s'\" % ops.connection.timezone_name))\n", "    def test_bulk_insert_sql(self):\n        ops = connection.ops\n        fields = ['field1', 'field2']\n        placeholder_rows = [\n            ['%s', '%s'],\n            ['%s', '%s'],\n        ]\n        sql = ops.bulk_insert_sql(fields, placeholder_rows)\n        self.assertEqual(sql, \" UNION ALL \".join(\"SELECT %s, %s\" for _ in placeholder_rows))\n", "    def setUp(self):\n        self.ops = connection.ops\n"], "sample_196": ["    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n"], "sample_198": ["    def test_params_with_database_functions(self):\n        raw_sql = RawSQL(\"SELECT LOWER(%s)\", [\"test\"])\n        self.assertEqual(raw_sql.as_sql(None, connection), (\"SELECT LOWER(%s)\", [\"test\"]))\n", "    def test_window_frame_start_end(self):\n        end = WindowFrame(start=5, end=10)\n        self.assertEqual(end.window_frame_start_end(connection, 5, 10), (5, 10))\n\n        start = WindowFrame(start=None, end=10)\n        self.assertEqual(\n            start.window_frame_start_end(connection, None, 10),\n            (connection.ops.UNBOUNDED_PRECEDING, 10)\n        )\n\n        unbounded = WindowFrame(start=5, end=None)\n        self.assertEqual(\n            unbounded.window_frame_start_end(connection, 5, None),\n            (5, connection.ops.UNBOUNDED_FOLLOWING)\n        )\n\n        both_unbounded = WindowFrame(start=None, end=None)\n        self.assertEqual(\n            both_unbounded.window_frame_start_end(connection, None, None),\n            (connection.ops.UNBOUNDED_PRECEDING, connection.ops.UNBOUNDED_FOLLOWING)\n        )\n", "    def setUpTestData(cls):\n        cls.manager = Manager.objects.create(name='John')\n        cls.employee1 = Employee.objects.create(\n            firstname='John',\n            lastname='Lennon',\n            salary=1500,\n            manager=cls.manager,\n        )\n        cls.employee2 = Employee.objects.create(\n            firstname='Paul',\n            lastname='McCartney',\n            salary=2000,\n            manager=cls.manager,\n        )\n", "    def setUpTestData(cls):\n        cls.company_query = Company.objects.values(\n            \"name\", \"num_employees\", \"num_chairs\"\n        ).order_by(\n            \"name\", \"num_employees\", \"num_chairs\"\n        )\n", "    def setUpTestData(cls):\n        cls.company = Company.objects.create(name='Example Inc.')\n        cls.employee = Employee.objects.create(company=cls.company)\n"], "sample_197": ["def test_custom_time_strings(self):\n    time_strings = {\n        'year': npgettext_lazy('custom-time-since', '%d year', '%d years'),\n        'month': npgettext_lazy('custom-time-since', '%d month', '%d months'),\n        'week': npgettext_lazy('custom-time-since', '%d week', '%d weeks'),\n        'day': npgettext_lazy('custom-time-since', '%d day', '%d days'),\n        'hour': npgettext_lazy('custom-time-since', '%d hour', '%d hours'),\n        'minute': npgettext_lazy('custom-time-since', '%d minute', '%d minutes'),\n    }\n    with translation.override('en'):\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n        expected = '1\\xa0custom year, 1\\xa0custom month'\n        self.assertEqual(timesince(self.t, t, time_strings=time_strings, depth=2), expected)\n", "def test_timesince_with_custom_time_strings(self):\n    \"\"\" Test timesince with custom time strings. \"\"\"\n    time_strings = {\n        'year': npgettext_lazy('custom-time-since', '%d custom year', '%d custom years'),\n        'month': npgettext_lazy('custom-time-since', '%d custom month', '%d custom months'),\n        'week': npgettext_lazy('custom-time-since', '%d custom week', '%d custom weeks'),\n        'day': npgettext_lazy('custom-time-since', '%d custom day', '%d custom days'),\n        'hour': npgettext_lazy('custom-time-since', '%d custom hour', '%d custom hours'),\n        'minute': npgettext_lazy('custom-time-since', '%d custom minute', '%d custom minutes'),\n    }\n    t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n    tests = [\n        (t, 1, '1\\xa0custom year'),\n        (t, 2, '1\\xa0custom year, 1\\xa0custom month'),\n        (t, 3, '1\\xa0custom year, 1\\xa0custom month, 1\\xa0custom week'),\n        (t, 4, '1\\xa0custom year, 1\\xa0custom month, 1\\xa0custom week, 1\\xa0custom day'),\n        (t, 5, '1\\xa0custom year, 1\\xa0custom month, 1\\xa0custom week, 1\\xa0custom day, 1\\xa0custom hour'),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(self.t, value, time_strings=time_strings, depth=depth), expected)\n            self.assertEqual(timeuntil(value, self.t, time_strings=time_strings, depth=depth), expected)\n", "def test_custom_time_strings(self):\n    \"\"\" Test custom time strings. \"\"\"\n    time_strings = {\n        'year': npgettext_lazy('custom', '%d year ago', '%d years ago'),\n        'month': npgettext_lazy('custom', '%d month ago', '%d months ago'),\n        'week': npgettext_lazy('custom', '%d week ago', '%d weeks ago'),\n        'day': npgettext_lazy('custom', '%d day ago', '%d days ago'),\n        'hour': npgettext_lazy('custom', '%d hour ago', '%d hours ago'),\n        'minute': npgettext_lazy('custom', '%d minute ago', '%d minutes ago'),\n    }\n    t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n    tests = [\n        (t, 1, '1\\xa0year ago'),\n        (t, 2, '1\\xa0year ago, 1\\xa0month ago'),\n        (t, 3, '1\\xa0year ago, 1\\xa0month ago, 1\\xa0week ago'),\n        (t, 4, '1\\xa0year ago, 1\\xa0month ago, 1\\xa0week ago, 1\\xa0day ago'),\n        (t, 5, '1\\xa0year ago, 1\\xa0month ago, 1\\xa0week ago, 1\\xa0day ago, 1\\xa0hour ago'),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(self.t, value, time_strings=time_strings, depth=depth), expected)\n", "def test_custom_time_strings(self):\n    \"\"\" Test using custom time strings. \"\"\"\n    time_strings = {\n        'year': npgettext_lazy('custom-years', '%d year', '%d years'),\n        'month': npgettext_lazy('custom-months', '%d month', '%d months'),\n        'week': npgettext_lazy('custom-weeks', '%d week', '%d weeks'),\n        'day': npgettext_lazy('custom-days', '%d day', '%d days'),\n        'hour': npgettext_lazy('custom-hours', '%d hour', '%d hours'),\n        'minute': npgettext_lazy('custom-minutes', '%d minute', '%d minutes'),\n    }\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=time_strings), '1\\xa0minute')\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=time_strings), '1\\xa0hour')\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=time_strings), '1\\xa0day')\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=time_strings), '1\\xa0week')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=time_strings), '1\\xa0month')\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=time_strings), '1\\xa0year')\n", "def test_custom_time_strings(self):\n    \"\"\"Test timesince with custom time strings.\"\"\"\n    time_strings = {\n        'year': npgettext_lazy('custom-years', '%d year', '%d years'),\n        'month': npgettext_lazy('custom-months', '%d month', '%d months'),\n        'week': npgettext_lazy('custom-weeks', '%d week', '%d weeks'),\n        'day': npgettext_lazy('custom-days', '%d day', '%d days'),\n        'hour': npgettext_lazy('custom-hours', '%d hour', '%d hours'),\n        'minute': npgettext_lazy('custom-minutes', '%d minute', '%d minutes'),\n    }\n    t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n    tests = [\n        (t, 1, '1\\xa0year'),\n        (t, 2, '1\\xa0year, 1\\xa0month'),\n        (t, 3, '1\\xa0year, 1\\xa0month, 1\\xa0week'),\n        (t, 4, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day'),\n        (t, 5, '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour'),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(self.t, value, time_strings=time_strings, depth=depth), expected)\n            self.assertEqual(timeuntil(value, self.t, time_strings=time_strings, depth=depth), expected)\n"], "sample_199": ["def test_subqueries_with_nested_annotations(self):\n    subquery = Book.objects.annotate(\n        total_authors=Count('authors'),\n    ).filter(\n        total_authors__gt=2,\n    ).values('publisher').annotate(count=Count('pk')).values('count')\n    publishers = Publisher.objects.annotate(\n        count_books_with_many_authors=Subquery(subquery, output_field=IntegerField()),\n    )\n    self.assertEqual(publishers.get(name='Prentice Hall').count_books_with_many_authors, 1)\n    self.assertEqual(publishers.get(name='Apress').count_books_with_many_authors, 0)\n", "def test_annotation_with_expression_wrapper(self):\n    wrapper = ExpressionWrapper(F('rating') + F('pages'), output_field=IntegerField())\n    books = Book.objects.annotate(combined=wrapper).filter(combined__gt=500)\n    self.assertGreater(len(books), 0)\n    for book in books:\n        self.assertEqual(book.combined, book.rating + book.pages)\n", "def test_expression_wrapper(self):\n    wrapper = ExpressionWrapper(F('rating') + 1, output_field=IntegerField())\n    self.assertEqual(wrapper.output_field.__class__, IntegerField)\n    self.assertEqual(wrapper.expression.__class__, CombinedExpression)\n\n    # Make sure output_field is optional if we're wrapping an expression that\n    # already has an output field.\n    wrapper = ExpressionWrapper(F('rating'), output_field=None)\n    self.assertEqual(wrapper.output_field.__class__, FloatField)\n", "def test_outerref_in_annotation(self):\n    subquery = Book.objects.filter(\n        authors=OuterRef('pk'),\n    ).values_list('pk', flat=True)\n    qs = Author.objects.annotate(\n        book_ids=Subquery(subquery),\n    )\n    self.assertQuerysetEqual(\n        qs, [\n            (self.a1.pk, [self.b1.pk]),\n            (self.a2.pk, [self.b1.pk]),\n            (self.a3.pk, [self.b2.pk]),\n            (self.a4.pk, [self.b3.pk]),\n            (self.a5.pk, [self.b4.pk]),\n            (self.a6.pk, [self.b4.pk]),\n            (self.a7.pk, [self.b4.pk]),\n            (self.a8.pk, [self.b5.pk, self.b6.pk]),\n            (self.a9.pk, [self.b5.pk]),\n        ],\n        lambda a: (a.pk, a.book_ids),\n        ordered=False,\n    )\n", "def test_annotation_with_subquery_and_exists(self):\n    long_books_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n    ).values('publisher').annotate(count=Count('pk')).values('count')\n    publisher_books_qs = Publisher.objects.annotate(\n        total_books=Count('book'),\n        has_long_books=Exists(long_books_qs.filter(count__gt=0)),\n    ).filter(has_long_books=True).values('name')\n    self.assertCountEqual(publisher_books_qs, [{'name': 'Sams'}, {'name': 'Morgan Kaufmann'}])\n"], "sample_200": ["def test_sanitize_address_with_non_ascii_localpart(self):\n    email_address = ('Firstname S\u00fcrname', 't\u00f3@example.com')\n    encoding = 'utf-8'\n    sanitized_address = sanitize_address(email_address, encoding)\n    self.assertEqual(sanitized_address, '=?utf-8?q?Firstname_S=C3=BCrname?= <t%C3%B3@example.com>')\n", "def test_sanitize_address_with_name_and_angle_brackets(self):\n    \"\"\"\n    Test that sanitize_address handles names with angle brackets correctly.\n    \"\"\"\n    email_address = 'Name <test@example.com>'\n    expected_result = 'Name <test@example.com>'\n    self.assertEqual(sanitize_address(email_address, encoding='utf-8'), expected_result)\n", "def test_safe_mime_multipart_with_empty_body(self):\n    \"\"\"\n    EmailMultiAlternatives includes alternatives if the body is empty and\n    it has attachments.\n    \"\"\"\n    msg = EmailMultiAlternatives(body='')\n    html_content = '<p>This is <strong>html</strong></p>'\n    msg.attach_alternative(html_content, 'text/html')\n    msg.attach('example.txt', 'Text file content', 'text/plain')\n    self.assertIn(html_content, msg.message().as_string())\n    # Make sure message has no Content-Transfer-Encoding header\n    self.assertNotIn('Content-Transfer-Encoding', msg.message())\n    payload = msg.message().get_payload()\n    # First part is a multipart/alternative\n    self.assertEqual(payload[0].get_content_type(), 'multipart/alternative')\n    # First part of the multipart/alternative is the HTML message\n    self.assertEqual(payload[0].get_payload()[0].get_content_type(), 'text/html')\n    # Second part is the attachment\n    self.assertEqual(payload[1].get_content_type(), 'text/plain')\n", "def test_attachment_filename_header_omitted_for_no_attachment(self):\n    message = EmailMessage('Subject', 'Content', 'bounce@example.com', ['to@example.com'], attachments=[])\n    self.assertNotIn('Content-Disposition', message.message().as_string())\n", "def test_address_header_max_line_length(self):\n    \"\"\"\n    Address headers are line-wrapped to a reasonable length, even when\n    individual addresses are very long (#22561).\n    \"\"\"\n    recipient = 'recipient' + '@' * 9000 + 'example.com'\n    email = EmailMessage('Subject', 'Content', 'from@example.com', [recipient])\n    message = email.message()\n    lines = message.as_bytes().splitlines()\n    max_line_length = max(len(line) for line in lines)\n    self.assertLess(max_line_length, 998)\n"], "sample_201": ["def test_remove_oldest(self):\n    \"\"\"\n    If the data exceeds what is allowed in a cookie and remove_oldest=False,\n    newer messages are removed before saving (and returned by the ``update`` method).\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 4 messages into the cookie, but not 5.\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        storage.add(constants.INFO, str(i) * msg_size)\n    unstored_messages = storage.update(response, remove_oldest=False)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, '4' * msg_size)\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    The not_finished sentinel value is correctly added and removed from the \n    messages list when the data exceeds what is allowed in a cookie.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        storage.add(constants.INFO, str(i) * msg_size)\n    storage.update(response)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    # Check that the not_finished sentinel value is present\n    data = storage._decode(response.cookies['messages'].value)\n    self.assertEqual(data[-1], CookieStorage.not_finished)\n\n    # Check that the not_finished sentinel value is removed when retrieving messages\n    storage = self.storage_class(self.get_request())\n    set_cookie_data(storage, data)\n    messages, all_retrieved = storage._get()\n    self.assertFalse(all_retrieved)\n    self.assertEqual(len(messages), 4)\n    self.assertNotEqual(messages[-1], CookieStorage.not_finished)\n", "def test_update_cookie_empty_value(self):\n    \"\"\"\n    If the encoded data is empty, _update_cookie sets a cookie with an empty value\n    to delete it.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage._update_cookie('', response)\n    self.assertEqual(response.cookies['messages'].value, '')\n    self.assertEqual(response.cookies['messages']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "def test_update_cookie(self):\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.add(constants.INFO, 'test')\n    storage.update(response)\n    self.assertIn('messages', response.cookies)\n\n    # Test update with empty value\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertNotIn('messages', response.cookies)\n\n    # Test update with not_finished sentinel value\n    storage = self.get_storage()\n    response = self.get_response()\n    messages = ['message1', 'message2']\n    set_cookie_data(storage, messages + [CookieStorage.not_finished])\n    storage.update(response)\n    self.assertIn('messages', response.cookies)\n    self.assertEqual(response.cookies['messages'].value, storage._encode(messages + [CookieStorage.not_finished]))\n", "def test_update_cookie(self):\n    \"\"\"\n    The _update_cookie method correctly sets or deletes the cookie.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test setting the cookie with encoded data\n    encoded_data = storage._encode(['message'])\n    storage._update_cookie(encoded_data, response)\n    self.assertIn('messages', response.cookies)\n    self.assertEqual(response.cookies['messages'].value, encoded_data)\n\n    # Test deleting the cookie with no encoded data\n    storage._update_cookie(None, response)\n    self.assertIn('messages', response.cookies)\n    self.assertEqual(response.cookies['messages']['max-age'], 0)\n"], "sample_202": ["def test_remove_oldest_parameter(self):\n    \"\"\"\n    The remove_oldest parameter of the _store method is used to determine which\n    messages should be removed when the data exceeds what is allowed in a cookie.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Create 5 messages to store\n    for i in range(5):\n        storage.add(constants.INFO, str(i) * 100)\n\n    # Store the messages with remove_oldest=True (default)\n    unstored_messages_true = storage._store(storage, response, remove_oldest=True)\n\n    # Store the messages with remove_oldest=False\n    storage = self.get_storage()\n    for i in range(5):\n        storage.add(constants.INFO, str(i) * 100)\n    response = self.get_response()\n    unstored_messages_false = storage._store(storage, response, remove_oldest=False)\n\n    # Check that the messages are different\n    self.assertNotEqual(unstored_messages_true[0].message, unstored_messages_false[0].message)\n", "def test_update_cookie_empty_value(self):\n    \"\"\"\n    If the data is empty, update_cookie should delete the cookie.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage._update_cookie('', response)\n    self.assertEqual(response.cookies['messages']['max-age'], 0)\n", "def test_message_encoder_extra_tags(self):\n    \"\"\"\n    Test that MessageEncoder correctly encodes messages with extra tags.\n    \"\"\"\n    message = Message(constants.INFO, 'Test message', extra_tags='tag1 tag2')\n    encoder = MessageEncoder(separators=(',', ':'))\n    encoded_message = encoder.encode([message])\n    decoded_message = json.loads(encoded_message, cls=MessageDecoder)[0]\n    self.assertEqual(decoded_message.level, constants.INFO)\n    self.assertEqual(decoded_message.message, 'Test message')\n    self.assertEqual(decoded_message.extra_tags, 'tag1 tag2')\n", "def test_process_messages(self):\n    \"\"\"\n    Test that MessageDecoder's process_messages method correctly handles nested\n    data structures containing Message instances.\n    \"\"\"\n    messages = [\n        {\n            'message': Message(constants.INFO, 'Test message'),\n            'message_list': [\n                Message(constants.INFO, 'message %s') for x in range(5)\n            ] + [{'another-message': Message(constants.ERROR, 'error')}],\n        },\n        Message(constants.INFO, 'message %s'),\n    ]\n    decoder = MessageDecoder()\n    processed_messages = decoder.process_messages(messages)\n    self.assertEqual(processed_messages, messages)\n\n    # Test that the method also handles non-list and non-dict values\n    self.assertIsNone(decoder.process_messages(None))\n    self.assertEqual(decoder.process_messages('string'), 'string')\n    self.assertEqual(decoder.process_messages(123), 123)\n", "def test_extra_tags(self):\n    \"\"\"\n    A message containing extra tags is properly encoded and decoded.\n    \"\"\"\n    storage = self.get_storage()\n    message = Message(constants.INFO, 'Test message', extra_tags='tag1 tag2')\n    encoded = storage._encode([message])\n    decoded_messages = storage._decode(encoded)\n    self.assertEqual(decoded_messages[0].extra_tags, 'tag1 tag2')\n"], "sample_203": ["    def test_regex_validator_with_flags(self):\n        validator = validators.RegexValidator(regex='^[a-z]*$', flags=re.IGNORECASE)\n        with self.assertRaises(ValidationError) as e:\n            validator('123')\n        self.assertEqual(e.exception.messages, ['Enter a valid value.'])\n", "    def test_null_character_in_string(self):\n        validator = validators.ProhibitNullCharactersValidator()\n        with self.assertRaises(ValidationError) as e:\n            validator('Hello\\x00World')\n        self.assertEqual(e.exception.code, 'null_characters_not_allowed')\n", "    def test_value_placeholder_with_url_field(self):\n        class MyForm(forms.Form):\n            field = forms.URLField(\n                error_messages={'invalid': '%(value)s'},\n            )\n\n        form = MyForm({'field': 'no_scheme'})\n        self.assertIs(form.is_valid(), False)\n        self.assertEqual(form.errors, {'field': ['no_scheme']})\n\n        # Also check with a scheme but invalid URL.\n        form = MyForm({'field': 'http://[::1:2::3]/'})\n        self.assertIs(form.is_valid(), False)\n        self.assertEqual(form.errors, {'field': ['http://[::1:2::3]/']})\n", "    def test_regex_validator_flags(self):\n        # Ensure flags are used correctly in the RegexValidator.\n        validator = validators.RegexValidator(regex='^[a-z]*$', flags=re.IGNORECASE)\n        self.assertIsNone(validator('Hello'))\n\n        validator = validators.RegexValidator(regex='^[a-z]*$')\n        with self.assertRaises(ValidationError):\n            validator('Hello')\n", "    def test_regex_validator_equality(self):\n        v1 = validators.RegexValidator(regex='^[a-zA-Z]*$')\n        v2 = validators.RegexValidator(regex='^[a-zA-Z]*$')\n        self.assertEqual(v1, v2)\n\n        v3 = validators.RegexValidator(regex='^[0-9]*$')\n        self.assertNotEqual(v1, v3)\n"], "sample_204": ["def test_collect_sql(self):\n    loader = MigrationLoader(connection)\n    plan = [\n        ('migrations', '0001_initial'),\n        ('migrations', '0002_second'),\n    ]\n    sql_statements = loader.collect_sql(plan)\n    self.assertGreater(len(sql_statements), 0)\n    for statement in sql_statements:\n        self.assertRegex(statement, r'^-- Create model|INSERT INTO')\n", "def test_detect_conflicts(self):\n    \"\"\"\n    MigrationLoader.detect_conflicts() returns a dict of apps with conflicts.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0001_initial', '0002_second']})\n", "def test_detect_conflicts(self):\n    \"\"\"\n    Tests detecting conflicts between migrations.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    recorder.record_applied('migrations', '0001_initial')\n    recorder.record_applied('migrations', '0002_second')\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0002_second', '0003_third']})\n", "def test_circular_dependencies(self):\n    \"\"\"\n    MigrationLoader should raise an exception when encountering circular dependencies.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    with self.assertRaises(NodeNotFoundError):\n        loader.build_graph()\n", "def test_loading_replaces(self):\n    \"Tests loading a migration that replaces another\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Load with nothing applied: both migrations should be loaded\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '0003_third')))\n    expected_plan = {\n        ('migrations', '0001_initial'),\n        ('migrations', '0002_second'),\n        ('migrations', '0003_third'),\n    }\n    self.assertEqual(plan, expected_plan)\n\n    # Fake-apply the replaced migration: now only the replacing one should be loaded\n    recorder.record_applied('migrations', '0001_initial')\n    recorder.record_applied('migrations', '0002_second')\n    loader.build_graph()\n    plan = set(loader.graph.forwards_plan(('migrations', '0003_third')))\n    expected_plan = {\n        ('migrations', '0003_third'),\n    }\n    self.assertEqual(plan, expected_plan)\n"], "sample_205": ["def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError('message')\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'__all__': ['message']})\n\n    error_dict = {}\n    exception = ValidationError({'field1': 'message'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['message']})\n\n    error_dict = {'field1': ['existing message']}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['existing message', 'message'], 'field2': ['other']})\n", "def test_repr(self):\n    error1 = ValidationError('message')\n    self.assertEqual(repr(error1), 'ValidationError([\\'message\\'])')\n\n    error2 = ValidationError('message', code='my_code')\n    self.assertEqual(repr(error2), 'ValidationError([\\'message\\'])')\n\n    error3 = ValidationError({\n        'field1': 'message1',\n        'field2': 'message2',\n    })\n    self.assertEqual(repr(error3), 'ValidationError({\\'field1\\': [\\'message1\\'], \\'field2\\': [\\'message2\\']})')\n\n    error4 = ValidationError([\n        ValidationError('message1'),\n        ValidationError('message2'),\n    ])\n    self.assertEqual(repr(error4), 'ValidationError([\\'message1\\', \\'message2\\'])')\n", "def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError('message')\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'__all__': ['message']})\n\n    error_dict = {}\n    exception = ValidationError({'field1': 'message'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['message']})\n\n    error_dict = {'field1': ['existing message']}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['existing message', 'message'], 'field2': ['other']})\n\n    error_dict = {}\n    exception = ValidationError(['message1', 'message2'])\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'__all__': ['message1', 'message2']})\n", "def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError('message')\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {NON_FIELD_ERRORS: ['message']})\n\n    error_dict = {'field1': []}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['message'], 'field2': ['other']})\n\n    error_dict = {'field1': ['existing']}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['existing', 'message'], 'field2': ['other']})\n", "def test_update_error_dict(self):\n    error_dict = {}\n    exception1 = ValidationError('message')\n    exception2 = ValidationError({'field1': 'error1', 'field2': 'error2'})\n    exception3 = ValidationError([\n        ValidationError({'field1': 'error3', 'field3': 'error4'}),\n        'error5',\n    ])\n\n    exception1.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {NON_FIELD_ERRORS: ['message']})\n\n    error_dict = {}\n    exception2.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n    error_dict = {}\n    exception3.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {\n        'field1': ['error3'],\n        'field3': ['error4'],\n        NON_FIELD_ERRORS: ['error5'],\n    })\n\n    error_dict = {'field1': ['existing error']}\n    exception2.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {\n        'field1': ['existing error', 'error1'],\n        'field2': ['error2'],\n    })\n"], "sample_206": ["def test_save_with_pathlib(self):\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        with override_settings(MEDIA_ROOT=Path(tmp_dir)):\n            document = Document(myfile='test_file.py')\n            file1 = File(__file__, name='test_file.py')\n            document.myfile.save(Path('unused') / 'test_file.py', file1)\n            self.assertTrue(os.path.exists(os.path.join(tmp_dir, 'unused', 'test_file.py')))\n", "def test_generate_filename_callable(self):\n    \"\"\"\n    Test that the generate_filename method is called when it's a callable.\n    \"\"\"\n    class MyDocument(Document):\n        myfile = models.FileField(upload_to=lambda instance, filename: 'custom/' + filename)\n\n    document = MyDocument(myfile='test_file.py')\n    self.assertEqual(document.myfile.generate_filename(document, 'test_file.py'), 'custom/test_file.py')\n", "def test_save_form_data_with_empty_string(self):\n    \"\"\"\n    FileField.save_form_data() will clear its instance attribute value if\n    passed an empty string.\n    \"\"\"\n    d = Document(myfile='something.txt')\n    self.assertEqual(d.myfile, 'something.txt')\n    field = d._meta.get_field('myfile')\n    field.save_form_data(d, '')\n    self.assertEqual(d.myfile, '')\n", "def test_hash(self):\n    d = Document(myfile='something.txt')\n    self.assertEqual(hash(d.myfile), hash('something.txt'))\n\n    d2 = Document(myfile='something_else.txt')\n    self.assertNotEqual(hash(d.myfile), hash(d2.myfile))\n", "def test_close_method(self):\n    \"\"\"\n    Test that the close method of a FieldFile correctly closes the underlying file.\n    \"\"\"\n    d = Document.objects.create(myfile='something.txt')\n    file = open(d.myfile.path, 'rb')\n    d.myfile.file = file\n    self.assertFalse(file.closed)\n    d.myfile.close()\n    self.assertTrue(file.closed)\n"], "sample_207": ["    def test_model_validation(self):\n        model = JSONModel(value={'foo': 'bar'})\n        model.clean_fields()\n        model.full_clean()\n", "def test_key_transform_invalid_input(self):\n    with self.assertRaisesMessage(\n        TypeError,\n        'Transform should be an instance of KeyTransform in order to use this lookup.',\n    ):\n        KeyTransformTextLookupMixin(Transform('test'))\n", "def test_key_transform_raw_expression_with_params(self):\n    expr = RawSQL(self.raw_sql, ['{\"x\": %s}'], params=['bar'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr)),\n        [self.objs[7]],\n    )\n", "    def test_adding_json_field(self):\n        # Create a model with a JSONField\n        project_state = self.apply_migrations(\n            ('jsonfield', '0001_initial'),\n            state_operations=[\n                migrations.CreateModel(\n                    name='JSONModel',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('value', models.JSONField()),\n                    ],\n                ),\n            ],\n        )\n        # Add a new JSONField\n        new_state = project_state.clone()\n        operations = [\n            migrations.AddField(\n                model_name='jsonmodel',\n                name='new_value',\n                field=models.JSONField(),\n            ),\n        ]\n        new_state = new_state.__class__(new_state.apps, new_state.models, operations)\n        with connection.schema_editor() as editor:\n            new_state.apply(editor)\n        # Check that the new field was created correctly\n        self.assertColumnExists('jsonfield_jsonmodel', 'new_value')\n", "    def test_key_transform_deconstruction(self):\n        transform = KeyTransform('test', 'field')\n        path, args, kwargs = transform.deconstruct()\n        self.assertEqual(path, 'django.db.models.fields.json.KeyTransform')\n        self.assertEqual(args, ('test', 'field'))\n        self.assertEqual(kwargs, {})\n"], "sample_208": ["def test_add_model_with_field_removed_from_grandparent_base_model(self):\n    \"\"\"\n    Removing a base field from a grandparent model takes place before adding a new \n    inherited model that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [], bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [], bases=('app.readable',)),\n        ModelState('app', 'novel', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.book',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='novel')\n", "def test_rename_field_with_choices(self):\n    \"\"\"Tests autodetection of renamed fields with choices.\"\"\"\n    before = [\n        ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.IntegerField(choices=[(1, 'One'), (2, 'Two')])),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('renamed_field', models.IntegerField(choices=[(1, 'One'), (2, 'Two')])),\n        ]),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, old_name='field', new_name='renamed_field')\n", "def test_rename_model_with_inheritance(self):\n    \"\"\"\n    Renaming a model that is inherited by another model should result in the\n    correct RenameModel operation.\n    \"\"\"\n    before = [\n        ModelState('app', 'Parent', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Child', [], bases=('app.Parent',)),\n    ]\n    after = [\n        ModelState('app', 'NewParent', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Child', [], bases=('app.NewParent',)),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename_model': True}))\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, old_name='Parent', new_name='NewParent')\n", "def test_add_model_with_base_model_field_removed(self):\n    \"\"\"\n    Removing a field from a base model takes place before adding a new inherited \n    model.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [], bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [], bases=('app.readable',)),\n        ModelState('app', 'article', [], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='article')\n", "def test_add_model_with_field_removed_from_base_model_and_proxy(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name and when a proxy model is involved.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'readableproxy', [], options={'proxy': True}, bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'readableproxy', [], options={'proxy': True}, bases=('app.readable',)),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n"], "sample_209": ["    def test_model_state_fields_cache_descriptor(self):\n        # Test that the fields cache descriptor returns a new dictionary for each instance.\n        worker1 = Worker()\n        worker2 = Worker()\n\n        self.assertIsNot(worker1._state.fields_cache, worker2._state.fields_cache)\n", "    def test_deferred_fields(self):\n        article = Article.objects.create(\n            headline=\"Look at me!\", pub_date=datetime.datetime.now()\n        )\n        article = Article.objects.defer('article_text').get(pk=article.pk)\n        self.assertIn('article_text', article.get_deferred_fields())\n        self.assertNotIn('headline', article.get_deferred_fields())\n", "    def test_full_clean(self):\n        # Create a model instance with invalid data.\n        article = Article(headline='Test', pub_date='not a date')\n        \n        # full_clean() should raise an exception.\n        with self.assertRaises(ValidationError):\n            article.full_clean()\n", "    def test_fields_cache_reset_on_copy(self):\n        m1 = Model1.objects.create(pkey=1000)\n        m2 = Model2.objects.create(model1=m1)\n        m3 = Model3.objects.create(model2=m2)\n\n        # Accessing deferred fields of an instance should refresh the fields cache.\n        m3.refresh_from_db(fields=['model2'])\n        self.assertIn('model2', m3.__dict__)\n\n        # Now, check that copying the model instance doesn't cause an exception\n        # due to shared state in ModelStateFieldsCacheDescriptor.\n        copy.copy(m3)\n", "    def test_fields_cache(self):\n        # Create a model instance and check that the fields cache is empty\n        worker = Worker()\n        self.assertEqual(worker._state.fields_cache, {})\n\n        # Access a field and check that it's cached\n        worker.name\n        self.assertIn('name', worker._state.fields_cache)\n\n        # Check that the cache is cleared when the instance is refreshed from the database\n        worker.refresh_from_db()\n        self.assertEqual(worker._state.fields_cache, {})\n"], "sample_210": ["    def test_template_response_render(self):\n        view = TemplateView.as_view(template_name='generic_views/about.html')\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.template_name, ['generic_views/about.html'])\n        self.assertEqual(response.context_data, {'view': response.context_data['view']})\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data()\n        self.assertIn('view', context)\n        self.assertEqual(context['view'], view)\n", "    def test_get_context_data(self):\n        class TestView(ContextMixin, View):\n            extra_context = {'test': 'value'}\n\n        view = TestView()\n        context = view.get_context_data()\n        self.assertIn('view', context)\n        self.assertEqual(context['view'], view)\n        self.assertIn('test', context)\n        self.assertEqual(context['test'], 'value')\n", "    def test_valid_kwargs(self):\n        class TestView(View):\n                super().__init__()\n                self.valid_kwarg = valid_kwarg\n\n        view = TestView.as_view(valid_kwarg='test_value')\n        request = RequestFactory().get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 405)  # Method Not Allowed\n", "def test_http_method_not_allowed(self):\n    \"\"\"\n    Test that a request with an HTTP method not allowed returns HTTP 405.\n    \"\"\"\n    view = SimpleView.as_view()\n    request = self.rf.put('/')\n    response = view(request)\n    self.assertEqual(response.status_code, 405)\n    self.assertEqual(response['Allow'], 'GET, HEAD')\n"], "sample_211": ["    def test_class_attribute_overrides(self):\n        class TestView(View):\n            request = 'test_request'\n\n        view_instance = TestView()\n        view_instance.setup(RequestFactory().get('/'))\n        self.assertEqual(view_instance.request, 'test_request')\n", "    def test_extra_context(self):\n        view = ContextMixin()\n        view.extra_context = {'foo': 'bar'}\n        context = view.get_context_data()\n        self.assertEqual(context['foo'], 'bar')\n        self.assertIsInstance(context['view'], ContextMixin)\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data(test_key='test_value')\n        self.assertEqual(context, {'test_key': 'test_value', 'view': view})\n", "    def test_get_context_data(self):\n        class TestView(ContextMixin, View):\n            extra_context = {'key': 'value'}\n\n        view = TestView()\n        context = view.get_context_data()\n        self.assertEqual(context['view'], view)\n        self.assertEqual(context['key'], 'value')\n", "    def test_extra_context(self):\n        view = ContextMixin()\n        view.extra_context = {'extra_key': 'extra_value'}\n        context = view.get_context_data()\n        self.assertIn('extra_key', context)\n        self.assertEqual(context['extra_key'], 'extra_value')\n"], "sample_213": ["    def test_init(self):\n        instance = Storage()\n        field = instance._meta.get_field('normal')\n        name = 'test_file.txt'\n        ff = FieldFile(instance, field, name)\n        self.assertEqual(ff.instance, instance)\n        self.assertEqual(ff.field, field)\n        self.assertEqual(ff.name, name)\n        self.assertEqual(ff.storage, field.storage)\n", "    def test_getattr(self):\n        # Set up a field and instance\n        field = FileField(upload_to='test', max_length=255)\n        instance = Storage()\n\n        # Create a file\n        file = ContentFile('test content')\n        file_name = field.generate_filename(instance, 'test.txt')\n        field.storage.save(file_name, file)\n\n        # Assign the file to the instance\n        field_file = field.attr_class(instance, field, file_name)\n        setattr(instance, field.attname, field_file)\n\n        # Test getting the attribute from the descriptor\n        descriptor = field.descriptor_class(field)\n        retrieved_field_file = descriptor.__get__(instance)\n        self.assertEqual(retrieved_field_file.name, file_name)\n", "    def test_get_file_descriptor(self):\n        instance = Storage()\n        fd = FileDescriptor('file')\n        fd.storage = default_storage\n        setattr(instance.__class__, 'file', fd)\n        self.assertIsInstance(instance.file, FieldFile)\n", "    def test_field_file_can_be_assigned_pathlib_path(self):\n        obj = Storage()\n        obj.normal.save('test.txt', ContentFile('test'))\n        obj.normal = Path(obj.normal.name)\n        self.assertIsInstance(obj.normal, FieldFile)\n        self.assertEqual(obj.normal.name, 'tests/test.txt')\n", "    def test_get_file_descriptor(self):\n        instance = Storage()\n        field_file = instance.normal\n        descriptor = FileDescriptor(field_file.field)\n\n        # Test getting a file descriptor from the descriptor.\n        self.assertEqual(descriptor.__get__(instance, instance.__class__), field_file)\n"], "sample_212": ["def test_session_middleware_process_response(self):\n    class DummySession:\n            self.session_key = session_key\n            self.accessed = True\n            self.modified = True\n\n            return False\n\n            return 60\n\n            return False\n\n            pass\n\n    class DummyRequest:\n            self.COOKIES = {settings.SESSION_COOKIE_NAME: 'session_key'}\n            self.session = DummySession('session_key')\n\n    class DummyResponse(HttpResponse):\n            super().__init__()\n            self.status_code = 200\n\n    middleware = SessionMiddleware(get_response=lambda request: DummyResponse())\n    response = middleware.process_response(DummyRequest(), DummyResponse())\n\n    self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME]['max-age'], 60)\n    self.assertIsNotNone(response.cookies[settings.SESSION_COOKIE_NAME]['expires'])\n    self.assertTrue(response.has_header('Vary'))\n    self.assertEqual(response.get_header('Vary'), 'Cookie')\n", "def test_session_middleware_process_response(self):\n    class DummySessionStore:\n            self.session_key = session_key\n            self.accessed = False\n            self.modified = False\n\n            return False\n\n            return False\n\n            return 3600\n\n            pass\n\n    class DummyRequest:\n        COOKIES = {}\n        session = DummySessionStore('dummy_session_key')\n\n    class DummyResponse:\n        status_code = 200\n        headers = {}\n\n            self.cookie_set = True\n\n            self.cookie_deleted = True\n\n    request = DummyRequest()\n    response = DummyResponse()\n\n    # Test case: Session is not empty and has been modified\n    request.session.accessed = True\n    request.session.modified = True\n    middleware = SessionMiddleware(get_response=lambda req: response)\n    new_response = middleware.process_response(request, response)\n    self.assertTrue(new_response.cookie_set)\n\n    # Test case: Session is empty and has not been modified\n    request.session.accessed = False\n    request.session.modified = False\n    request.session.is_empty = lambda: True\n    middleware = SessionMiddleware(get_response=lambda req: response)\n    new_response = middleware.process_response(request, response)\n    self.assertTrue(new_response.cookie_deleted)\n\n    # Test case: Session is not empty, but has not been modified\n    request.session.accessed = True\n    request.session.modified = False\n    request.session.is_empty = lambda: False\n    middleware = SessionMiddleware(get_response=lambda req: response)\n    new_response = middleware.process_response(request, response)\n    self.assertFalse(hasattr(new_response, 'cookie_set'))\n    self.assertFalse(hasattr(new_response, 'cookie_deleted'))\n", "def test_session_middleware_process_response(self):\n    request = HttpRequest()\n    session_key = 'session_key'\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = session_key\n\n    # Simulate an empty session\n    session = self.client.session\n    session.clear()\n    session.save()\n\n    response = HttpResponse()\n    SessionMiddleware().process_response(request, response)\n\n    # Check that the session cookie is deleted when the session is empty\n    self.assertEqual(response.cookies.get(settings.SESSION_COOKIE_NAME)['max-age'], 0)\n\n    # Simulate a non-empty session\n    session['key'] = 'value'\n    session.save()\n\n    response = HttpResponse()\n    SessionMiddleware().process_response(request, response)\n\n    # Check that the session cookie is not deleted when the session is not empty\n    self.assertNotEqual(response.cookies.get(settings.SESSION_COOKIE_NAME)['max-age'], 0)\n", "def test_session_middleware_process_response(self):\n    class DummySessionStore:\n            self.session_key = session_key\n            self.accessed = False\n            self.modified = False\n\n            return False\n\n            return False\n\n            return 3600\n\n            pass\n\n    class DummyRequest:\n            self.COOKIES = {}\n            self.session = None\n\n    class DummyResponse:\n            self.status_code = 200\n            self.cookies = {}\n\n            self.cookies[key] = {\n                'value': value,\n                'max_age': max_age,\n                'expires': expires,\n                'domain': domain,\n                'path': path,\n                'secure': secure,\n                'httponly': httponly,\n                'samesite': samesite,\n            }\n\n            if key in self.cookies:\n                del self.cookies[key]\n\n    request = DummyRequest()\n    response = DummyResponse()\n\n    session_middleware = SessionMiddleware(get_response=lambda req: response)\n    session_middleware.SessionStore = DummySessionStore\n\n    # Test that the session cookie is set when the session is modified.\n    request.session = DummySessionStore('session_key')\n    request.session.modified = True\n    session_middleware.process_response(request, response)\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test that the session cookie is not set when the session is not modified.\n    request.session = DummySessionStore('session_key')\n    request.session.modified = False\n    session_middleware.process_response(request, response)\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test that the session cookie is deleted when the session is empty.\n    request.session = DummySessionStore('session_key')\n    request.session.accessed = True\n    request.session.modified = True\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'session_key'\n    session_middleware.process_response(request, response)\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n", "def test_session_middleware_process_response(self):\n    \"\"\"\n    Test that the SessionMiddleware's process_response hook correctly saves\n    the session and sets the session cookie.\n    \"\"\"\n    request = HttpRequest()\n    response = HttpResponse()\n\n    # Initialize a session with some data\n    session = self.client.session\n    session['test_key'] = 'test_value'\n    session.save()\n\n    # Set up the middleware instance\n    middleware = SessionMiddleware()\n\n    # Call the process_request hook to set up the session on the request\n    middleware.process_request(request)\n\n    # Modify the session\n    request.session['test_key'] = 'new_test_value'\n\n    # Call the process_response hook to save the session and set the cookie\n    response = middleware.process_response(request, response)\n\n    # Check that the session was saved and the cookie was set\n    self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME].value, request.session.session_key)\n    self.assertEqual(request.session['test_key'], 'new_test_value')\n\n    # Check that the Vary: Cookie header was added\n    self.assertIn('Vary', response)\n    self.assertEqual(response['Vary'], 'Cookie')\n"], "sample_214": ["    def test_key_transform(self):\n        obj = NullableJSONModel.objects.create(value={'a': 'b'})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a='b'),\n            [obj],\n        )\n", "def test_key_transform_sql_injection_with_key_transform(self):\n    with CaptureQueriesContext(connection) as queries:\n        self.assertIs(\n            NullableJSONModel.objects.filter(**{\n                \"\"\"value__test__a' = '\"a\"') OR 1 = 1 OR ('d\"\"\": 'x',\n            }).exists(),\n            False,\n        )\n    self.assertIn(\n        \"\"\".\"value\" -> 'test' -> 'a'' = ''\"a\"'') OR 1 = 1 OR (''d') = '\"x\"' \"\"\",\n        queries[0]['sql'],\n    )\n", "def test_key_transform_on_expression_with_params(self):\n    expr = RawSQL(\"JSON_BUILD_OBJECT('foo', %s)\", ['bar'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('foo', expr)),\n        [self.objs[7]],\n    )\n", "def test_key_transform_with_complex_values(self):\n    obj = NullableJSONModel.objects.create(value={'a': 1, 'b': [2, {'c': 3}]})\n    tests = [\n        ('value__a', 1),\n        ('value__b__0', 2),\n        ('value__b__1__c', 3),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup):\n            self.assertEqual(NullableJSONModel.objects.filter(**{lookup: value}).get(), obj)\n", "def test_select_format(self):\n    query = JSONModel.objects.filter(value={'a': 'b'})\n    compiler = query.query.get_compiler(connection=connection)\n    sql, params = query.query.select_format(compiler, sql=['%s'], params=[{}])\n    if connection.vendor == 'postgresql' and connection.features.has_native_json_field:\n        self.assertEqual(sql, [\"%s::jsonb\"])\n    else:\n        self.assertEqual(sql, ['%s'])\n    self.assertEqual(params, [{}])\n"], "sample_215": ["    def test_technical_404_template_used(self):\n        response = technical_404_response(mock.MagicMock(), Http404())\n        self.assertTemplateUsed(response, 'technical_404.html')\n", "    def test_request_with_broken_repr(self):\n        request = RequestFactory().get('/test_view/')\n        request.user = User()\n        request_repr = repr(request)\n        request.repr_called = True\n\n            if getattr(self, 'repr_called', False):\n                raise Exception('Request repr is broken')\n            self.repr_called = True\n            return request_repr\n\n        with mock.patch.object(Request, '__repr__', mock_repr):\n            try:\n                raise ValueError(\"Can't find my keys\")\n            except ValueError:\n                exc_type, exc_value, tb = sys.exc_info()\n            response = technical_500_response(request, exc_type, exc_value, tb)\n            self.assertContains(response, \"Request repr() unavailable\", status_code=500)\n", "    def test_get_traceback_data(self):\n        try:\n            request = self.rf.get('/test_view/')\n            request.user = User()\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIn('request', data)\n        self.assertIn('request_meta', data)\n        self.assertIn('filtered_POST_items', data)\n        self.assertIn('settings', data)\n        self.assertIn('sys_executable', data)\n        self.assertIn('sys_version_info', data)\n        self.assertIn('server_time', data)\n        self.assertIn('django_version_info', data)\n        self.assertIn('sys_path', data)\n        self.assertIn('template_info', data)\n        self.assertIn('template_does_not_exist', data)\n        self.assertIn('postmortem', data)\n", "    def test_cleanse_setting_for_callable(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        setting_value = lambda: 'super_secret'\n        cleansed_value = reporter_filter.cleanse_setting('SETTING_NAME', setting_value)\n        self.assertIsInstance(cleansed_value, CallableSettingWrapper)\n        self.assertEqual(str(cleansed_value), '********************')\n", "    def test_explicit_exception_chain(self):\n        try:\n            raise TypeError(\"outer\") from ValueError(\"inner\")\n        except TypeError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertEqual(len(frames), 2)\n"], "sample_216": ["def test_resolve_relation_recursive(self):\n    model_tuple = (\"testapp\", \"Author\")\n    self.assertEqual(resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, *model_tuple), model_tuple)\n    with self.assertRaises(TypeError):\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT)\n    with self.assertRaises(TypeError):\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, \"testapp\")\n", "def test_resolve_relation_with_app_label_and_model_name(self):\n    self.assertEqual(resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, 'testapp', 'ModelName'), ('testapp', 'ModelName'))\n    self.assertEqual(resolve_relation('testapp.ModelName', 'otherapp', 'OtherModelName'), ('testapp', 'modelname'))\n    self.assertEqual(resolve_relation('ModelName', 'testapp', 'OtherModelName'), ('testapp', 'modelname'))\n", "def test_resolve_relation_with_recursive_relationship_constant(self):\n    app_label = 'testapp'\n    model_name = 'Author'\n    self.assertEqual(\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, app_label, model_name),\n        (app_label, model_name)\n    )\n    with self.assertRaises(TypeError):\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, None, model_name)\n    with self.assertRaises(TypeError):\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, app_label, None)\n", "def test_field_references_recursive(self):\n    model_state = ModelState('app', 'Model', [\n        ('id', models.AutoField(primary_key=True)),\n        ('parent', models.ForeignKey('app.Model', models.CASCADE)),\n    ])\n    reference = field_references(('app', 'Model'), model_state.fields['parent'], ('app', 'Model'))\n    self.assertIsInstance(reference, FieldReference)\n    self.assertEqual(reference.to, (model_state.fields['parent'].remote_field, None))\n    self.assertIsNone(reference.through)\n", "def test_field_references(self):\n    model_tuple = ('testapp', 'Author')\n    field_tuple = ('name',)\n    reference_model_tuple = ('testapp', 'Book')\n    state = ProjectState()\n    state.add_model(ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ]))\n    state.add_model(ModelState('testapp', 'Book', [\n        ('id', models.AutoField(primary_key=True)),\n        ('author', models.ForeignKey('testapp.Author', models.CASCADE, to_field='name')),\n    ]))\n    self.assertTrue(field_is_referenced(state, model_tuple, field_tuple))\n    self.assertFalse(field_is_referenced(state, reference_model_tuple, field_tuple))\n"], "sample_217": ["def test_render_css(self):\n    media = Media(css={'screen': ['a.css'], 'all': ['b.css']})\n    rendered_media = str(media)\n    self.assertIn('<link href=\"http://media.example.com/static/a.css\" type=\"text/css\" media=\"screen\" rel=\"stylesheet\">', rendered_media)\n    self.assertIn('<link href=\"http://media.example.com/static/b.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">', rendered_media)\n\n    # Test absolute URL\n    media = Media(css={'screen': ['/path/to/a.css'], 'all': ['/path/to/b.css']})\n    rendered_media = str(media)\n    self.assertIn('<link href=\"/path/to/a.css\" type=\"text/css\" media=\"screen\" rel=\"stylesheet\">', rendered_media)\n    self.assertIn('<link href=\"/path/to/b.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">', rendered_media)\n\n    # Test HTTPS URL\n    media = Media(css={'screen': ['https://example.com/path/to/a.css'], 'all': ['https://example.com/path/to/b.css']})\n    rendered_media = str(media)\n    self.assertIn('<link href=\"https://example.com/path/to/a.css\" type=\"text/css\" media=\"screen\" rel=\"stylesheet\">', rendered_media)\n    self.assertIn('<link href=\"https://example.com/path/to/b.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">', rendered_media)\n", "def test_absolute_path(self):\n    media = Media(css={'screen': ['a.css', '/b.css', 'http://example.com/c.css']}, js=['a.js', '/b.js', 'http://example.com/c.js'])\n    self.assertEqual(media.absolute_path('a.css'), 'http://media.example.com/static/a.css')\n    self.assertEqual(media.absolute_path('/b.css'), '/b.css')\n    self.assertEqual(media.absolute_path('http://example.com/c.css'), 'http://example.com/c.css')\n    self.assertEqual(media.absolute_path('a.js'), 'http://media.example.com/static/a.js')\n    self.assertEqual(media.absolute_path('/b.js'), '/b.js')\n    self.assertEqual(media.absolute_path('http://example.com/c.js'), 'http://example.com/c.js')\n", "def test_widget_render(self):\n    class MyWidget(TextInput):\n            return 'Hello, world!'\n\n    widget = MyWidget()\n    self.assertEqual(widget.render('name', 'value'), 'Hello, world!')\n", "def test_media_absolute_path(self):\n    media = Media(css={'all': ['path/to/css']}, js=['path/to/js'])\n    self.assertEqual(media.absolute_path('path/to/css'), 'http://media.example.com/static/path/to/css')\n    self.assertEqual(media.absolute_path('/path/to/css'), '/path/to/css')\n    self.assertEqual(media.absolute_path('http://example.com/path/to/css'), 'http://example.com/path/to/css')\n", "def test_add_media_with_extend_false(self):\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n    class MyWidget2(MyWidget1):\n        class Media:\n            extend = False\n            css = {\n                'all': ('/path/to/css3', 'path/to/css1')\n            }\n            js = ('/path/to/js1', '/path/to/js4')\n\n    w1 = MyWidget1()\n    w2 = MyWidget2()\n    merged = w1.media + w2.media\n    self.assertEqual(\n        str(merged),\n        \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_218": ["def test_trunc_func_explicit_timezone_priority(self):\n    start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n    end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    melb = pytz.timezone('Australia/Melbourne')\n\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            day_melb=TruncDay('start_datetime'),\n            day_utc=TruncDay('start_datetime', tzinfo=timezone.utc),\n        ).order_by('start_datetime').get()\n        self.assertEqual(model.day_melb, truncate_to(start_datetime.astimezone(melb), 'day'))\n        self.assertEqual(model.day_utc, truncate_to(start_datetime.astimezone(timezone.utc), 'day'))\n", "def test_trunc_func_with_non_naive_datetime(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    if settings.USE_TZ:\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    qs = DTModel.objects.annotate(\n        truncated_start=Trunc('start_datetime', 'day', output_field=DateTimeField(), tzinfo=melb),\n        truncated_end=Trunc('end_datetime', 'day', output_field=DateTimeField(), tzinfo=melb),\n    ).order_by('start_datetime')\n\n    for obj in qs:\n        self.assertEqual(obj.truncated_start.tzinfo, melb)\n        self.assertEqual(obj.truncated_end.tzinfo, melb)\n", "def test_trunc_func_with_invalid_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    invalid_tzinfo = 'Invalid/TZ'\n\n    msg = 'Unknown time zone: Invalid/TZ'\n    with self.assertRaisesMessage(pytz.UnknownTimeZoneError, msg):\n        list(DTModel.objects.annotate(\n            truncated=Trunc('start_datetime', 'day', output_field=DateTimeField(), tzinfo=invalid_tzinfo)\n        ))\n", "def test_trunc_timezone_awareness(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    model = DTModel.objects.annotate(\n        truncated_start=TruncHour('start_datetime', tzinfo=melb),\n        truncated_end=TruncHour('end_datetime', tzinfo=melb),\n    ).get()\n\n    self.assertEqual(model.truncated_start.tzinfo, melb)\n    self.assertEqual(model.truncated_end.tzinfo, melb)\n\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            truncated_start=TruncHour('start_datetime'),\n            truncated_end=TruncHour('end_datetime'),\n        ).get()\n\n        self.assertEqual(model.truncated_start.tzinfo, melb)\n        self.assertEqual(model.truncated_end.tzinfo, melb)\n", "def test_trunc_func_with_timezone_transition(self):\n    \"\"\"\n    Truncating a datetime that has a timezone with a daylight saving transition.\n    \"\"\"\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = datetime(2016, 10, 16, 1)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    self.create_model(start_datetime, None)\n\n    qs = DTModel.objects.annotate(\n        truncated_start=TruncHour('start_datetime', tzinfo=sao),\n    )\n    with self.assertRaisesMessage(pytz.NonExistentTimeError, '2016-10-16 01:00:00'):\n        qs.get()\n\n    qs = DTModel.objects.annotate(\n        truncated_start=TruncHour('start_datetime', tzinfo=sao, is_dst=True),\n    )\n    model = qs.get()\n    self.assertEqual(model.truncated_start.dst(), timedelta(0, -3600))\n"], "sample_219": ["    def test_repr(self):\n        expr = ExpressionList(F('col'), F('anothercol'))\n        self.assertEqual(repr(expr), 'ExpressionList(F(col), F(anothercol))')\n", "    def test_rawsql_requires_params(self):\n        msg = 'params must be a list or tuple.'\n        with self.assertRaisesMessage(ValueError, msg):\n            RawSQL('table.col', 'param')\n", "    def test_window_expression(self):\n        with self.assertNumQueries(1) as ctx:\n            list(Employee.objects.annotate(\n                row_number=Window(expression=RowNumber(), order_by=F('salary')),\n            ))\n        sql = ctx.captured_queries[0]['sql']\n        self.assertIn('ROW_NUMBER() OVER (ORDER BY', sql)\n", "    def test_window_with_order_by(self):\n        qs = Employee.objects.annotate(\n            row_number=Window(\n                expression=RowNumber(),\n                order_by=[F('salary')],\n            )\n        ).order_by('salary')\n        self.assertEqual(qs[0].row_number, 1)\n        self.assertEqual(qs[1].row_number, 2)\n", "    def test_params(self):\n        raw_sql = RawSQL(\"select col from table where foo = %s\", [\"bar\"])\n        self.assertEqual(raw_sql.sql, \"select col from table where foo = %s\")\n        self.assertEqual(raw_sql.params, [\"bar\"])\n"], "sample_220": ["    def test_reason_phrase(self):\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n", "    def test_file_response(self):\n        response = FileResponse(open(__file__, 'rb'))\n        self.assertEqual(response.status_code, 200)\n        self.assertIsInstance(response.file_to_stream, file)\n        self.assertIn('Content-Length', response)\n        self.assertIn('Content-Type', response)\n        self.assertEqual(response['Content-Type'], 'text/x-python; charset=utf-8')\n", "    def test_default_charset(self):\n        response = HttpResponse('Hello, world!')\n        self.assertEqual(response.charset, 'utf-8')\n", "    def test_set_signed_cookie(self):\n        response = HttpResponse()\n        response.set_signed_cookie('test', 'value')\n        self.assertIn('test', response.cookies)\n        value = response.cookies['test'].value\n        self.assertNotEqual(value, 'value')\n        parts = value.split(':')\n        self.assertEqual(len(parts), 2)\n        self.assertEqual(parts[0], 'value')\n", "    def test_reason_phrase(self):\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n"], "sample_221": ["    def setUpTestData(cls):\n        cls.group = Group.objects.create(name=\"Ponies Who Own Maybachs\")\n        cls.event = Event.objects.create(title=\"Event 1\", group=cls.group)\n", "def test_prefetch_related_objects_cache_name_collision(self):\n    # Issue #28832 -- prefetch_related should not cache results under an\n    # attribute name that may collide with a field name on the model.\n    event = Event.objects.create(title='Event 1', group=Group.objects.create(name='Group 1'))\n    events = Event.objects.prefetch_related('group')\n    self.assertEqual(events.get().group.name, 'Group 1')\n\n    # If there's a field called \"group\" on the Event model, it would be\n    # overwritten by the prefetched Group object if we didn't take care to\n    # avoid collisions in the cache names used by prefetch_related.\n    class EventWithGroupField(Event):\n        group = models.CharField(max_length=10)\n\n    event_with_group_field = EventWithGroupField.objects.get()\n    self.assertIsNone(event_with_group_field.group)\n", "def test_pickle_with_annotate_and_extra(self):\n    # Test that pickling works with annotate() and extra()\n    qs = Event.objects.annotate(extra_field=models.Value('extra')).extra(select={'other_field': 'title'})\n    self.assert_pickles(qs)\n", "def test_update_or_create_with_pickling(self):\n    obj, created = Group.objects.get_or_create(name='foo')\n    self.assertTrue(created)\n    qs = Group.objects.filter(name='foo')\n    self.assert_pickles(qs)\n    # Update or create after pickling should still work.\n    obj, created = pickle.loads(pickle.dumps(qs)).update_or_create(name='foo', defaults={'name': 'bar'})\n    self.assertFalse(created)\n    self.assertEqual(obj.name, 'bar')\n", "def test_union_queryset_pickle(self):\n    # Create some events and groups to test union queryset pickling\n    event1 = Event.objects.create(title='Event 1', group=Group.objects.create(name='Group 1'))\n    event2 = Event.objects.create(title='Event 2', group=Group.objects.create(name='Group 2'))\n\n    qs1 = Event.objects.filter(title='Event 1')\n    qs2 = Event.objects.filter(title='Event 2')\n\n    union_qs = qs1.union(qs2)\n    self.assert_pickles(union_qs)\n"], "sample_222": ["    def test_exclusive_lock_fcntl(self):\n        if locks.LOCK_EX == 0:\n            self.skipTest(\"File locking is not supported\")\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            f1 = open(temp_file.name, 'wb')\n            self.assertTrue(locks.lock(f1, locks.LOCK_EX))\n            f2 = open(temp_file.name, 'wb')\n            self.assertFalse(locks.lock(f2, locks.LOCK_EX | locks.LOCK_NB))\n            locks.unlock(f1)\n            self.assertTrue(locks.lock(f2, locks.LOCK_EX))\n", "    def test_lock_unlock_windows(self):\n        if os.name != 'nt':\n            self.skipTest('Windows-only test')\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path, 'wb') as f:\n            self.assertIs(locks.lock(f, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(f), True)\n", "    def test_lock_unlock(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path) as f:\n            self.assertIs(locks.lock(f, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(f), True)\n", "    def test_lock_exclusive_and_shared(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path) as f1, open(file_path) as f2:\n            self.assertIs(locks.lock(f1, locks.LOCK_EX), True)\n            self.assertIs(locks.lock(f2, locks.LOCK_SH | locks.LOCK_NB), False)\n            locks.unlock(f1)\n            self.assertIs(locks.lock(f2, locks.LOCK_SH), True)\n            self.assertIs(locks.lock(f1, locks.LOCK_EX | locks.LOCK_NB), False)\n            locks.unlock(f2)\n", "    def test_exclusive_lock_with_contention(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path) as f1, open(file_path) as f2:\n            self.assertIs(locks.lock(f1, locks.LOCK_EX), True)\n            self.assertIs(locks.lock(f2, locks.LOCK_EX | locks.LOCK_NB), False)\n            self.assertIs(locks.unlock(f1), True)\n            self.assertIs(locks.lock(f2, locks.LOCK_EX), True)\n"], "sample_223": ["    def test_ticket_24956(self):\n        school = School.objects.create()\n        classroom1 = Classroom.objects.create(school=school)\n        student1 = Student.objects.create(school=school)\n        classroom1.students.add(student1)\n        self.assertEqual(\n            Student.objects.filter(classroom__school=F('school')).count(),\n            1\n        )\n        self.assertEqual(\n            Student.objects.exclude(classroom__school=F('school')).count(),\n            0\n        )\n", "    def test_exists_subquery(self):\n        note = Note.objects.create(note='note')\n        annotation1 = Annotation.objects.create(tag=Tag.objects.create(name='tag1'))\n        annotation2 = Annotation.objects.create(tag=Tag.objects.create(name='tag2'))\n        annotation1.notes.add(note)\n        self.assertSequenceEqual(\n            Note.objects.filter(Exists(Annotation.objects.filter(notes=OuterRef('pk')))),\n            [note],\n        )\n        self.assertSequenceEqual(\n            Note.objects.filter(~Exists(Annotation.objects.filter(notes=OuterRef('pk')))),\n            [],\n        )\n", "    def test_ticket_24858(self):\n        # Ensure that an extra() call with an empty select dictionary\n        # does not raise a KeyError.\n        Tag.objects.extra(select={})\n", "    def test_ticket_24742(self):\n        \"\"\"\n        Test that the ORM can handle queries with multiple levels of subqueries.\n        \"\"\"\n        author1 = Author.objects.create(name='author1')\n        author2 = Author.objects.create(name='author2')\n        book1 = Book.objects.create(title='book1', author=author1)\n        book2 = Book.objects.create(title='book2', author=author2)\n        chapter1 = Chapter.objects.create(title='chapter1', book=book1)\n        chapter2 = Chapter.objects.create(title='chapter2', book=book2)\n\n        subquery1 = Chapter.objects.filter(book__author__name='author1').values('book_id')\n        subquery2 = Book.objects.filter(id__in=subquery1).values('author_id')\n        queryset = Author.objects.filter(id__in=subquery2)\n\n        self.assertSequenceEqual(queryset, [author1])\n", "    def test_ticket_24661(self):\n        \"\"\"\n        Using __contains lookup in a filter subquery should work correctly.\n        \"\"\"\n        n1 = Note.objects.create(note='n1', misc='foo')\n        e1 = ExtraInfo.objects.create(info='e1', note=n1)\n        a1 = Author.objects.create(name='a1', num=1001, extra=e1)\n        Item.objects.create(name='one', created=datetime.datetime.now(), creator=a1, note=n1)\n\n        qs = Item.objects.filter(\n            Q(creator__extra__info__contains='e1') |\n            Q(creator__extra__note__note='n1')\n        )\n        self.assertSequenceEqual(qs, [Item.objects.get(name='one')])\n"], "sample_224": ["def test_aggregation_subquery_annotation_nested(self):\n    \"\"\"\n    Subquery annotations work correctly when nested.\n    \"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    latest_book_price_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-price').values('price')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n        latest_book_price=Subquery(latest_book_price_qs),\n    ).annotate(count=Count('book'))\n    self.assertTrue(publisher_qs.exists())\n", "def test_aggregation_over_subquery_annotation(self):\n    \"\"\"Aggregations over subquery annotations are allowed.\"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).aggregate(Max('latest_book_pubdate'))\n    self.assertEqual(publisher_qs['latest_book_pubdate__max'], datetime.date(2008, 12, 6))\n", "def test_aggregation_subquery_annotation_nested(self):\n    subquery_qs = Author.objects.filter(\n        pk=OuterRef('contact_id'),\n        book__name=OuterRef('name'),\n    ).values('pk')\n    books_qs = Book.objects.annotate(\n        contact_id=Subquery(subquery_qs),\n    ).annotate(count=Count('authors'))\n    self.assertEqual(books_qs.count(), Book.objects.count())\n", "def test_aggregate_over_subquery(self):\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).values('rating')\n    publishers = Publisher.objects.annotate(max_rating=Max(Subquery(subquery))).order_by('-max_rating')\n    self.assertEqual(\n        list(publishers.values_list('name', 'max_rating')),\n        [\n            ('Morgan Kaufmann', 5.0),\n            ('Apress', 4.5),\n            ('Prentice Hall', 4.5),\n            ('Sams', 3.0),\n            (\"Jonno's House of Books\", None),\n        ]\n    )\n", "def test_aggregation_on_boolean_fields(self):\n    # Test aggregation on boolean fields (Refs #25354)\n    Book.objects.create(is_published=True)\n    Book.objects.create(is_published=True)\n    Book.objects.create(is_published=False)\n\n    self.assertEqual(Book.objects.aggregate(Sum('is_published'))['is_published__sum'], 2)\n    self.assertEqual(Book.objects.aggregate(Avg('is_published'))['is_published__avg'], 2/3)\n    self.assertEqual(Book.objects.aggregate(Max('is_published'))['is_published__max'], 1)\n    self.assertEqual(Book.objects.aggregate(Min('is_published'))['is_published__min'], 0)\n    self.assertEqual(Book.objects.aggregate(Count('is_published'))['is_published__count'], 3)\n"], "sample_225": ["    def setUp(self):\n        self.site = admin.AdminSite(name='test_adminsite')\n        self.request_factory = RequestFactory()\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def test_final_catch_all_view(self):\n        response = self.client.get('/test_admin/admin/non-existent-url/')\n        self.assertEqual(response.status_code, 404)\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n"], "sample_226": ["    def test_serialize_db_to_string_with_excluded_models(self):\n        # serialize_db_to_string() doesn't include models from apps in TEST_NON_SERIALIZED_APPS.\n        obj = Object.objects.create()\n        with override_settings(TEST_NON_SERIALIZED_APPS=['backends']):\n            data = connection.creation.serialize_db_to_string()\n        self.assertEqual(data, '[]')\n        obj.delete()\n", "    def test_clone_test_db(self):\n        # Create a test database.\n        creation = connection.creation\n        old_database_name = connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Clone the test database.\n            suffix = 'clone'\n            creation.clone_test_db(suffix, verbosity=0)\n            clone_settings = creation.get_test_db_clone_settings(suffix)\n            # Check that the cloned database exists and has the correct name.\n            clone_connection = connections[DEFAULT_DB_ALIAS]\n            clone_connection.settings_dict = clone_settings\n            with clone_connection.cursor() as cursor:\n                cursor.execute(\"SELECT 1\")\n                self.assertEqual(cursor.fetchone(), (1,))\n        finally:\n            # Destroy the test database and its clone.\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n            clone_connection = connections[DEFAULT_DB_ALIAS]\n            clone_connection.settings_dict = clone_settings\n            with mock.patch.object(clone_connection.creation, '_destroy_test_db'):\n                clone_connection.creation.destroy_test_db(clone_settings['NAME'], verbosity=0)\n", "    def test_clone_test_db(self):\n        # Create a test database.\n        creation = connection.creation\n        test_database_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        \n        # Clone the test database.\n        clone_suffix = '_clone'\n        clone_settings = creation.get_test_db_clone_settings(clone_suffix)\n        creation.clone_test_db(clone_suffix, verbosity=0, autoclobber=True, keepdb=False)\n\n        # Check that the clone database exists and has the same settings as the original test database.\n        self.assertEqual(clone_settings['NAME'], test_database_name + clone_suffix)\n        self.assertEqual(clone_settings['ENGINE'], creation.connection.settings_dict['ENGINE'])\n        self.assertEqual(clone_settings['HOST'], creation.connection.settings_dict['HOST'])\n        self.assertEqual(clone_settings['PORT'], creation.connection.settings_dict['PORT'])\n\n        # Destroy the clone database.\n        creation.destroy_test_db(test_database_name + clone_suffix, verbosity=0, keepdb=False)\n\n        # Destroy the original test database.\n        creation.destroy_test_db(test_database_name, verbosity=0, keepdb=False)\n", "    def test_create_test_db_with_keepdb(self):\n        # create_test_db() doesn't recreate the database if keepdb is True.\n        connection.creation.create_test_db(keepdb=True)\n        with mock.patch.object(connection.creation, '_create_test_db') as mocked_create:\n            connection.creation.create_test_db(keepdb=True)\n            mocked_create.assert_not_called()\n        connection.creation.destroy_test_db()\n", "    def test_clone_test_db(self):\n        # Create a test database.\n        creation = connection.creation\n        old_database_name = connection.settings_dict['NAME']\n        try:\n            test_database_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Clone the test database.\n            clone_database_name = creation.get_test_db_clone_settings('clone')['NAME']\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix='clone', verbosity=0, autoclobber=True)\n            # The cloned database name is correctly generated.\n            self.assertEqual(clone_database_name, test_database_name + '_clone')\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n"], "sample_227": ["def test_listfilter_genericrelation_with_limit_choices_to(self):\n    django_bookmark = Bookmark.objects.create(url='https://www.djangoproject.com/')\n    python_bookmark = Bookmark.objects.create(url='https://www.python.org/')\n    kernel_bookmark = Bookmark.objects.create(url='https://www.kernel.org/')\n\n    TaggedItem.objects.create(content_object=django_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=python_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=kernel_bookmark, tag='linux')\n\n    class BookmarkAdminGenericRelation(ModelAdmin):\n        list_filter = ('tags__tag',)\n\n    modeladmin = BookmarkAdminGenericRelation(Bookmark, site)\n\n    request = self.request_factory.get('/', {'tags__tag': 'python'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n\n    expected = [python_bookmark, django_bookmark]\n    self.assertEqual(list(queryset), expected)\n\n    # Test limit_choices_to\n    class TaggedItemLimitChoicesTo(TaggedItem):\n        limit_choices_to = {'tag': 'python'}\n\n    class BookmarkAdminGenericRelation(ModelAdmin):\n        list_filter = ('tags__tag',)\n\n    modeladmin = BookmarkAdminGenericRelation(Bookmark, site)\n\n    request = self.request_factory.get('/', {'tags__tag': 'python'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n\n    expected = [python_bookmark, django_bookmark]\n    self.assertEqual(list(queryset), expected)\n\n    request = self.request_factory.get('/', {'tags__tag': 'linux'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n\n    self.assertEqual(list(queryset), [])\n", "def test_genericrelation_reverse(self):\n    class TaggedItemAdmin(ModelAdmin):\n        list_filter = ['content_type']\n\n    modeladmin = TaggedItemAdmin(TaggedItem, site)\n\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'content type')\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(len(choices), 2)\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], True)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    # Create a tagged item with content type of Book\n    tagged_item = TaggedItem.objects.create(content_object=self.django_book, tag='python')\n\n    # Check that the filter options are updated\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'content type')\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(len(choices), 3)\n    self.assertEqual(choices[1]['display'], 'book')\n    self.assertIs(choices[1]['selected'], False)\n    self.assertEqual(choices[1]['query_string'], '?content_type__id__exact=%s' % ContentType.objects.get_for_model(Book).pk)\n\n    # Filter by content type of Book\n    request = self.request_factory.get('/', {'content_type__id__exact': ContentType.objects.get_for_model(Book).pk})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'content type')\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(len(choices), 3)\n    self.assertEqual(choices[1]['display'], 'book')\n    self.assertIs(choices[1]['selected'], True)\n    self.assertEqual(choices[1]['query_string'], '?content_type__id__exact=%s' % ContentType.objects.get_for_model(Book).pk)\n\n    # Check that the queryset is filtered correctly\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [tagged_item])\n", "def test_genericrelation_reverse(self):\n    django_bookmark = Bookmark.objects.create(url='https://www.djangoproject.com/')\n    python_bookmark = Bookmark.objects.create(url='https://www.python.org/')\n\n    TaggedItem.objects.create(content_object=django_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=python_bookmark, tag='python')\n\n    class TaggedItemAdmin(ModelAdmin):\n        list_filter = ('content_type',)\n\n    modeladmin = TaggedItemAdmin(TaggedItem, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'content type')\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(choices[1]['display'], 'Bookmark')\n    self.assertIs(choices[1]['selected'], False)\n    self.assertEqual(choices[1]['query_string'], '?content_type__exact=2')\n", "def test_genericrelation_list_filter(self):\n    django_bookmark = Bookmark.objects.create(url='https://www.djangoproject.com/')\n    python_bookmark = Bookmark.objects.create(url='https://www.python.org/')\n    kernel_bookmark = Bookmark.objects.create(url='https://www.kernel.org/')\n\n    TaggedItem.objects.create(content_object=django_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=python_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=kernel_bookmark, tag='linux')\n\n    class BookmarkGenericRelation(ModelAdmin):\n        list_filter = (\n            ('tags__tag', AllValuesFieldListFilter),\n        )\n\n    modeladmin = BookmarkGenericRelation(Bookmark, site)\n\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'tag')\n    choices = list(filterspec.choices(changelist))\n\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], True)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    self.assertEqual(choices[1]['display'], 'linux')\n    self.assertIs(choices[1]['selected'], False)\n    self.assertEqual(choices[1]['query_string'], '?tags__tag=linux')\n\n    self.assertEqual(choices[2]['display'], 'python')\n    self.assertIs(choices[2]['selected'], False)\n    self.assertEqual(choices[2]['query_string'], '?tags__tag=python')\n\n    request = self.request_factory.get('/', {'tags__tag': 'python'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [python_bookmark, django_bookmark])\n", "def test_genericrelation_prefetch(self):\n    django_bookmark = Bookmark.objects.create(url='https://www.djangoproject.com/')\n    python_bookmark = Bookmark.objects.create(url='https://www.python.org/')\n    TaggedItem.objects.create(content_object=django_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=python_bookmark, tag='python')\n\n    modeladmin = BookmarkAdminGenericRelation(Bookmark, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(len(queryset._prefetched_objects_cache['tags']), 2)\n    self.assertEqual(len(queryset._prefetched_objects_cache['tags'][0]), 1)\n    self.assertEqual(len(queryset._prefetched_objects_cache['tags'][1]), 1)\n"], "sample_228": ["def test_formset_with_non_integer_min_num(self):\n    \"\"\"min_num must be an integer.\"\"\"\n    with self.assertRaisesMessage(TypeError, \"'min_num' must be an integer\"):\n        formset_factory(Choice, min_num='not an integer')\n", "def test_formset_absolute_max_cannot_be_lower_than_default_max_num(self):\n    \"\"\"absolute_max cannot be lower than DEFAULT_MAX_NUM.\"\"\"\n    with self.assertRaises(ValueError):\n        formset_factory(FavoriteDrinkForm, absolute_max=formsets.DEFAULT_MAX_NUM - 1)\n", "def test_formset_with_disabled_fields(self):\n    \"\"\"\n    FormSets with disabled fields should not cause a ValidationError when the \n    field is missing from the submitted data.\n    \"\"\"\n    class DisabledForm(Form):\n        field = IntegerField(disabled=True)\n\n    DisabledFormSet = formset_factory(DisabledForm, extra=1)\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n    }\n    formset = DisabledFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([{}], formset.cleaned_data)\n", "def test_formset_absolute_max_overridden(self):\n    # absolute_max can be overridden by a subclass.\n    class MyFormSet(BaseFormSet):\n        absolute_max = 2000\n\n    MyChoiceFormSet = formset_factory(Choice, formset=MyFormSet)\n    self.assertEqual(MyChoiceFormSet.absolute_max, 2000)\n\n    data = {\n        'choices-TOTAL_FORMS': '2001',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n        'choices-2-choice': 'Two',\n        'choices-2-votes': '2',\n        'choices-3-choice': 'Three',\n        'choices-3-votes': '3',\n    }\n    formset = MyChoiceFormSet(data, prefix='choices')\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at most 1000 forms.'],\n    )\n", "def test_baseformset_absolute_max(self):\n    \"\"\"The absolute_max parameter of BaseFormSet limits the number of forms.\"\"\"\n    formset = BaseFormSet({\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n    }, absolute_max=1)\n    self.assertEqual(formset.total_form_count(), 1)\n    formset = BaseFormSet({\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n    }, absolute_max=3)\n    self.assertEqual(formset.total_form_count(), 2)\n"], "sample_229": ["def test_union_with_annotation(self):\n    qs1 = Number.objects.annotate(is_even=F('num') % 2).filter(is_even=0)\n    qs2 = Number.objects.annotate(is_even=F('num') % 2).filter(is_even=1)\n    self.assertNumbersEqual(qs1.union(qs2), list(range(10)), ordered=False)\n", "def test_union_with_empty_values_list(self):\n    qs1 = Number.objects.values_list('num', flat=True)\n    qs2 = Number.objects.none().values_list('num', flat=True)\n    self.assertEqual(list(qs1.union(qs2)), list(range(10)))\n", "def test_union_with_annotate(self):\n    qs1 = Number.objects.annotate(other_num_null=ExpressionWrapper(Q(other_num__isnull=True), output_field=BooleanField()))\n    qs2 = Number.objects.annotate(other_num_null=ExpressionWrapper(Q(other_num__isnull=True), output_field=BooleanField()))\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ordered=False)\n", "def test_combining_multiple_values_list(self):\n    ReservedName.objects.create(name='99 little bugs', order=99)\n    qs1 = Number.objects.filter(num=1).values_list('num', flat=True)\n    qs2 = ReservedName.objects.values_list('order')\n    qs3 = Number.objects.filter(num=2).values_list('num', 'other_num')\n    self.assertEqual(list(qs1.union(qs2).union(qs3)), [1, 99, (2, 8)])\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num=1)\n    qs2 = Number.objects.none()\n    self.assertNumbersEqual(qs1.union(qs2), [1])\n"], "sample_230": ["def test_invalid_json_input(self):\n    class JSONForm(Form):\n        json_field = JSONField()\n\n    # Invalid JSON input should be redisplayed as is.\n    form = JSONForm({'json_field': '{foo}'})\n    self.assertEqual(form.errors['json_field'], ['Enter a valid JSON.'])\n    self.assertIn('{foo}</textarea>', form.as_p())\n", "def test_bound_data(self):\n    field = JSONField()\n    tests = [\n        '{\"a\": 1}',\n        '[\"a\", \"b\"]',\n        'true',\n        'false',\n        'null',\n        '1',\n        '1.5',\n        '\"foo\"',\n    ]\n    for json_string in tests:\n        with self.subTest(json_string=json_string):\n            val = json.loads(json_string)\n            self.assertEqual(field.bound_data(json_string, None), val)\n\n    invalid_json_strings = ['{', '[', '{\"a\": \"b\"} extra']\n    for json_string in invalid_json_strings:\n        with self.subTest(json_string=json_string):\n            bound_data = field.bound_data(json_string, None)\n            self.assertIsInstance(bound_data, InvalidJSONInput)\n            self.assertEqual(str(bound_data), json_string)\n\n    disabled_field = JSONField(disabled=True)\n    self.assertIsNone(disabled_field.bound_data('{\"a\": 1}', None))\n", "def test_bound_data(self):\n    field = JSONField()\n    self.assertIsNone(field.bound_data(None, None))\n    self.assertEqual(field.bound_data('{\"a\": \"b\"}', None), {\"a\": \"b\"})\n    self.assertIsInstance(field.bound_data('invalid json', None), field.InvalidJSONInput)\n", "def test_bound_data(self):\n    field = JSONField()\n    tests = [\n        (None, None),\n        ('null', None),\n        ('\"foo\"', 'foo'),\n        ('\"\u4f60\u597d\uff0c\u4e16\u754c\"', '\u4f60\u597d\uff0c\u4e16\u754c'),\n        ('{\"a\": 1}', {'a': 1}),\n        ('[1, 2]', [1, 2]),\n    ]\n    for input_value, expected in tests:\n        with self.subTest(input_value=input_value):\n            bound_field = field.bound_data(input_value, None)\n            self.assertEqual(bound_field, expected)\n", "def test_bound_data(self):\n    field = JSONField()\n    self.assertIsNone(field.bound_data(None, None))\n    self.assertEqual(field.bound_data('{\"a\": 1}', None), {'a': 1})\n    invalid_json_input = field.bound_data('{invalid: json}', None)\n    self.assertIsInstance(invalid_json_input, InvalidJSONInput)\n    self.assertEqual(str(invalid_json_input), '{invalid: json}')\n"], "sample_231": ["    def test_technical_404_response(self):\n        request = self.rf.get('/test_view/')\n        exception = Http404('Test message')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, 'technical_404.html')\n", "    def test_default_urlconf(self):\n        request = RequestFactory().get('/path/')\n        response = default_urlconf(request)\n        self.assertContains(response, \"Page not found\", status_code=404)\n        self.assertContains(response, \"The current URL, <code>path/</code>, didn't match any of these.\", status_code=404)\n", "    def test_template_debug_repr(self):\n        template_debug = ExceptionReporter(\n            None, None, None, None\n        ).template_info\n        self.assertIsNone(template_debug)\n        template_debug = ExceptionReporter(\n            None, None, None, None\n        ).get_traceback_data()['template_info']\n        self.assertIsNone(template_debug)\n", "    def test_technical_500_response(self):\n        exc_type = RuntimeError\n        exc_value = exc_type('Test exception')\n        tb = sys.exc_info()[2]\n        request = RequestFactory().get('/')\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertIn(b'Test exception', response.content)\n        self.assertIn(b'Traceback', response.content)\n", "    def test_override_filter(self):\n        filter = get_exception_reporter_filter(None)\n        self.assertIsInstance(filter, CustomExceptionReporter)\n"], "sample_232": ["    def test_adding_json_field(self):\n        # Create the initial migration with a model that doesn't have the JSON field.\n        with open('migrations/0001_initial.py', 'w') as initial_file:\n            initial_file.write(\"\"\"", "    def test_invalid_json_value(self):\n        # Invalid JSON value should raise an error.\n        model = NullableJSONModel(value='invalid json')\n        with self.assertRaises((IntegrityError, DataError, OperationalError)):\n            model.save()\n", "def test_key_transform_with_subqueries(self):\n    subquery = NullableJSONModel.objects.filter(value__c=14).values('value')\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__in=subquery),\n        self.objs[3:5],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__in=subquery),\n        [],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__baz__in=subquery),\n        [],\n    )\n", "def test_key_transform_with_custom_decoder(self):\n    value = {'uuid': uuid.UUID('{d85e2076-b67c-4ee7-8c3a-2bf5a2cc2475}')}\n    obj = NullableJSONModel.objects.create(value_custom=value)\n    self.assertEqual(\n        NullableJSONModel.objects.filter(\n            value_custom__uuid=KeyTransform('uuid', 'value_custom'),\n        ).get(),\n        obj,\n    )\n", "def test_key_transform_with_non_string_keys(self):\n    tests = (\n        ('value__0', [self.objs[5]]),\n        ('value__1_0', [self.objs[5]]),\n        ('value__d_0', [self.objs[4]]),\n        ('value__d_1_f', [self.objs[4]]),\n    )\n    for lookup, expected in tests:\n        with self.subTest(lookup=lookup):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(**{lookup: 'e'}),\n                expected,\n            )\n"], "sample_233": ["def test_token_with_invalid_input(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n\n    # Test with invalid token format\n    self.assertIs(p0.check_token(user, tk1.replace('-', '')), False)\n    self.assertIs(p0.check_token(user, tk1.split('-')[0]), False)\n\n    # Test with non-integer timestamp\n    ts_b36, _ = tk1.split(\"-\")\n    self.assertIs(p0.check_token(user, f'{ts_b36}abc-{tk1.split(\"-\")[1]}'), False)\n\n    # Test with negative timestamp\n    ts = base36_to_int(ts_b36)\n    self.assertIs(p0.check_token(user, f'{int_to_base36(-ts)}-{tk1.split(\"-\")[1]}'), False)\n", "def test_make_hash_value(self):\n    user = User.objects.create_user('hashvaluetestuser', 'test@example.com', 'testpw')\n    timestamp = self._num_seconds(datetime.now())\n    p0 = PasswordResetTokenGenerator()\n    hash_value = p0._make_hash_value(user, timestamp)\n    expected_hash_value = f'{user.pk}{user.password}{user.last_login.replace(microsecond=0, tzinfo=None)}{timestamp}{user.email}'\n    self.assertEqual(hash_value, expected_hash_value)\n\n    # Test with None last_login\n    user.last_login = None\n    user.save()\n    hash_value = p0._make_hash_value(user, timestamp)\n    expected_hash_value = f'{user.pk}{user.password}\"\"{timestamp}{user.email}'\n    self.assertEqual(hash_value, expected_hash_value)\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = self._num_seconds(datetime.now())\n    token = p0._make_token_with_timestamp(user, timestamp)\n    self.assertRegex(token, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n    # Test with legacy=True\n    token_legacy = p0._make_token_with_timestamp(user, timestamp, legacy=True)\n    self.assertRegex(token_legacy, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n    self.assertNotEqual(token, token_legacy)\n", "def test_token_with_last_login_updated(self):\n    \"\"\"Updating the user's last login date invalidates the token.\"\"\"\n    user = User.objects.create_user('lastloginuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now()\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_last_login_change(self):\n    \"\"\"Updating the user last_login field invalidates the token.\"\"\"\n    user = User.objects.create_user('lastloginuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now() + timedelta(seconds=10)\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_234": ["def test_union_with_annotate(self):\n    qs1 = Number.objects.annotate(\n        has_reserved_name=Exists(ReservedName.objects.filter(order=OuterRef('num')))\n    ).filter(has_reserved_name=True)\n    qs2 = Number.objects.filter(num=9)\n    self.assertCountEqual(qs1.union(qs2).values_list('num', flat=True), [1, 9])\n", "def test_union_with_limit_and_offset(self):\n    qs1 = Number.objects.all()[:5]\n    qs2 = Number.objects.all()[5:]\n    self.assertNumbersEqual(qs1.union(qs2)[:10], range(10))\n    self.assertNumbersEqual(qs1.union(qs2)[5:], range(5, 10))\n", "def test_union_with_distinct(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=4)\n    self.assertEqual(len(list(qs1.union(qs2).distinct())), 6)\n", "def test_database_error_on_invalid_ordering(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__gte=2, num__lte=3)\n    msg = 'ORDER BY term does not match any column in the result set'\n    with self.assertRaisesMessage(DatabaseError, msg):\n        list(qs1.union(qs2).order_by('invalid_column'))\n", "def test_order_by_on_union_with_values_list_and_filter(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=5),\n        ReservedName(name='rn0', order=6),\n        ReservedName(name='rn9', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=6)\n    qs2 = ReservedName.objects.filter(order__lte=5)\n    union_qs = qs1.union(qs2).values_list('order', flat=True)\n    self.assertEqual(list(union_qs.order_by('-order')[:2]), [7, 6])\n"], "sample_235": ["def test_on_commit_run_with_set_autocommit_on(self):\n        self.notify(1)\n\n    connection.set_autocommit(False)\n    transaction.on_commit(on_commit_func)\n    connection.set_autocommit(True)\n\n    self.assertDone([1])\n", "def test_commit_hooks_on_set_autocommit_on(self):\n        self.notify(1)\n\n    with transaction.atomic():\n        transaction.on_commit(on_commit)\n\n    connection.set_autocommit(False)\n    connection.set_autocommit(True)\n\n    self.assertDone([1])\n", "def test_on_commit_in_atomic_block_with_autocommit_disabled(self):\n    try:\n        connection.set_autocommit(False)\n        with transaction.atomic():\n            with self.assertRaises(transaction.TransactionManagementError):\n                transaction.on_commit(lambda: self.notify(1))\n    finally:\n        connection.set_autocommit(True)\n", "def test_run_and_clear_commit_hooks(self):\n        self.notify(1)\n\n    transaction.on_commit(on_commit)\n    connection.run_and_clear_commit_hooks()\n    self.assertDone([1])\n\n    # Hooks are cleared after running\n    connection.run_and_clear_commit_hooks()\n    self.assertDone([1])\n", "def test_on_commit_with_atomic_block(self):\n    with transaction.atomic():\n        self.do(1)\n        try:\n            with transaction.atomic():\n                self.do(2)\n                raise ForcedError()\n        except ForcedError:\n            pass\n\n    self.assertDone([1])\n"], "sample_236": ["    def test_add_field_update(self):\n        collector = Collector(using='default')\n        model = R\n        field = model._meta.get_field('p')\n        value = P.objects.create()\n        objs = [R.objects.create()]\n        collector.add_field_update(field, value, objs)\n        self.assertEqual(collector.field_updates[model][(field, value)], set(objs))\n", "def test_collector_sort(self):\n    collector = Collector(using='default')\n    a = A.objects.create(name='a')\n    b = B.objects.create(a=a)\n    c = Child.objects.create(parent=b)\n\n    # Add objects to the collector in reverse deletion order.\n    collector.add([c], source=None, nullable=False)\n    collector.add([b], source=None, nullable=False)\n    collector.add([a], source=None, nullable=False)\n\n    # Sort the collector.\n    collector.sort()\n\n    # Check that the collector has sorted the objects correctly.\n    self.assertEqual(list(collector.data.keys()), [A, B, Child])\n", "    def test_add_field_update(self):\n        collector = Collector(using='default')\n        field = R._meta.get_field('p')\n        value = P.objects.create()\n        obj1, obj2 = R.objects.create(), R.objects.create()\n        collector.add_field_update(field, value, [obj1, obj2])\n        self.assertEqual(collector.field_updates[R][field, value], {obj1, obj2})\n", "def test_fast_delete_non_nullable_fk(self):\n    a = Avatar.objects.create(desc='a')\n    u = User.objects.create(avatar=a)\n    # 1 query to delete avatar, 1 query to update user's avatar to NULL\n    self.assertNumQueries(2, a.delete)\n    self.assertIsNone(User.objects.get(pk=u.pk).avatar)\n", "def test_collector_instances_with_model(self):\n    collector = Collector(using='default')\n    a = create_a('auto')\n    collector.collect([a])\n    self.assertEqual(list(collector.instances_with_model()), [(A, a)])\n"], "sample_237": ["def test_permission_name_max_length_with_non_ascii_characters(self):\n    class Checked(models.Model):\n        class Meta:\n            verbose_name = 'some ridiculously long verbose name avec des caract\u00e8res sp\u00e9ciaux \u00e9\u00e9\u00e9'\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n            \"characters for its builtin permission names to be at most 255 characters.\",\n            obj=Checked,\n            id='auth.E007',\n        ),\n    ])\n", "def test_username_field_max_length(self):\n    class CustomUserWithTooLongUsernameField(AbstractBaseUser):\n        username = models.CharField(max_length=101, unique=True)\n        USERNAME_FIELD = 'username'\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"'CustomUserWithTooLongUsernameField.username' must be at most 100 characters.\",\n            obj=CustomUserWithTooLongUsernameField,\n            id='auth.E013',\n        ),\n    ])\n", "def test_is_anonymous_property(self):\n    \"\"\"\n    <User Model>.is_anonymous must be a property or attribute.\n    \"\"\"\n    class CustomUserBadIsAnonymousProperty(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        @property\n            return False\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n\n    class CustomUserBadIsAnonymousProperty2(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n        is_anonymous = True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_builtins_permissions_length(self):\n    class Checked(models.Model):\n        class Meta:\n            verbose_name = 'some ridiculously long verbose name that is out of control' * 3\n\n    Permission = self.apps.get_model('auth', 'Permission')\n    permission_codename_max_length = Permission._meta.get_field('codename').max_length\n    model_name_max_length = permission_codename_max_length - (\n        len('add_') - len(Checked._meta.model_name)\n    )\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The name of model 'auth_tests.Checked' must be at most %d \"\n            \"characters for its builtin permission codenames to be at most %d \"\n            \"characters.\" % (\n                model_name_max_length,\n                permission_codename_max_length,\n            ),\n            obj=Checked,\n            id='auth.E011',\n        ),\n    ])\n", "def test_multiple_custom_permissions_max_length(self):\n    custom_permission_name1 = 'some ridiculously long verbose name that is out of control' * 2\n    custom_permission_name2 = 'some other ridiculously long verbose name that is out of control' * 2\n\n    class Checked(models.Model):\n        class Meta:\n            permissions = [\n                ('my_custom_permission', custom_permission_name1),\n                ('other_custom_permission', custom_permission_name2),\n            ]\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(len(errors), 2)\n    self.assertEqual(errors[0], checks.Error(\n        \"The permission named '%s' of model 'auth_tests.Checked' is longer \"\n        \"than 255 characters.\" % custom_permission_name1,\n        obj=Checked,\n        id='auth.E008',\n    ))\n    self.assertEqual(errors[1], checks.Error(\n        \"The permission named '%s' of model 'auth_tests.Checked' is longer \"\n        \"than 255 characters.\" % custom_permission_name2,\n        obj=Checked,\n        id='auth.E008',\n    ))\n"], "sample_239": ["def test_all_valid_short_circuits(self):\n    \"\"\"all_valid() short-circuits on the first invalid formset.\"\"\"\n    class InvalidFormSet(BaseFormSet):\n            raise AssertionError('This should not be called')\n\n    ChoiceFormSet = formset_factory(Choice)\n    data = {\n        'choices-TOTAL_FORMS': '1',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': '',\n        'choices-0-votes': '',\n    }\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = InvalidFormSet()\n    self.assertIs(all_valid((formset1, formset2)), False)\n    self.assertFalse(formset1.is_valid())\n", "def test_formset_initial_data_with_disabled_fields(self):\n    class DisabledFieldForm(Form):\n        field = CharField(disabled=True)\n\n    DisabledFieldFormSet = formset_factory(DisabledFieldForm, extra=1)\n    initial = [{'field': 'Initial value'}]\n    formset = DisabledFieldFormSet(initial=initial)\n    self.assertHTMLEqual(\n        '\\n'.join(form.as_ul() for form in formset.forms),\n        \"\"\"<li>Field: <input type=\"text\" name=\"form-0-field\" value=\"Initial value\" disabled></li>", "def test_add_fields_with_custom_formset(self):\n    \"\"\"A formset's add_fields method is called with the correct arguments.\"\"\"\n    class BaseCustomFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            self.added_fields = True\n\n    CustomChoiceFormSet = formset_factory(Choice, formset=BaseCustomFormSet)\n    formset = CustomChoiceFormSet()\n    self.assertTrue(hasattr(formset, 'added_fields'))\n    self.assertTrue(formset.added_fields)\n", "def test_total_error_count_with_empty_forms(self):\n    \"\"\"A formset with empty forms should not count them as errors.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-1-choice': '',\n        'choices-1-votes': '',\n    }\n    formset = self.make_choiceformset(data)\n    self.assertEqual(formset.total_error_count(), 0)\n", "def test_formset_empty_permitted(self):\n    \"\"\"If empty_permitted is True, then forms in the formset are allowed to be empty.\"\"\"\n    class EmptyForm(Form):\n        name = CharField()\n\n    EmptyFormSet = formset_factory(EmptyForm, extra=1)\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-0-name': '',\n    }\n    formset = EmptyFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([{}], formset.cleaned_data)\n\n    # But if validate_min is True, then we must have at least min_num non-empty forms.\n    MinFormSet = formset_factory(EmptyForm, extra=2, min_num=1, validate_min=True)\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '1',\n        'form-0-name': '',\n        'form-1-name': '',\n    }\n    formset = MinFormSet(data, prefix='form')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual([], formset.cleaned_data)\n    self.assertEqual(['Please submit at least 1 form.'], list(formset.non_form_errors()))\n"], "sample_240": ["def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_changed_last_login(self):\n    \"\"\"Updating the user's last login invalidates the token.\"\"\"\n    user = User.objects.create_user('changelluser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now() + timedelta(days=1)\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_check_token_with_different_password(self):\n    \"\"\"\n    Updating the user password invalidates the token.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_make_token_with_timestamp_truncates_microseconds(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    dt = datetime.now()\n    ts = p0._num_seconds(dt)\n    token1 = p0._make_token_with_timestamp(user, ts)\n    token2 = p0._make_token_with_timestamp(user, ts, legacy=True)\n    self.assertNotEqual(token1, token2)\n"], "sample_241": ["    def test_select_related(self):\n        Company.objects.create(name='Example Inc.', point_of_contact=self.max)\n        self.assertQuerysetEqual(\n            Employee.objects.filter(pk=self.max.pk).select_related('company_point_of_contact_set'),\n            ['<Employee: Max Mustermann>'],\n        )\n        self.assertQuerysetEqual(\n            Employee.objects.select_related('company_ceo_set').annotate(\n                company_name=F('company_ceo_set__name')\n            ).filter(company_name=F('company_point_of_contact_set__name')),\n            ['<Employee: Max Mustermann>'],\n        )\n", "    def test_repr(self):\n        expr = ExpressionList(F('col'), F('anothercol'))\n        self.assertEqual(repr(expr), 'ExpressionList(F(col), F(anothercol))')\n", "    def test_extra_values_list(self):\n        qs = Company.objects.extra(select={'foo': 'num_employees + num_chairs'})\n        self.assertEqual(\n            list(qs.values_list('foo')),\n            [(2305,), (7,), (33,)],\n        )\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n", "    def test_repr(self):\n        self.assertEqual(repr(OuterRef('name')), \"OuterRef(name)\")\n"], "sample_242": ["    def test_relabel_clone(self):\n        lookup = Lookup(Value(1), Value(2))\n        relabeled_lookup = lookup.relabeled_clone({'value': 'new_value'})\n        self.assertEqual(relabeled_lookup.lhs, lookup.lhs.relabeled_clone({'value': 'new_value'}))\n        self.assertEqual(relabeled_lookup.rhs, lookup.rhs)\n", "    def test_get_bound_params_exact(self):\n        lookup = YearExact(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n        start, finish = datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59)\n        self.assertEqual(lookup.get_bound_params(start, finish), (start, finish))\n", "    def test_get_bound_params(self):\n        start = datetime(2010, 1, 1, 0, 0, 0)\n        finish = datetime(2010, 12, 31, 23, 59, 59)\n        look_ups = [\n            YearExact(Value(start), Value(finish)),\n            YearGt(Value(start), Value(finish)),\n            YearGte(Value(start), Value(finish)),\n            YearLt(Value(start), Value(finish)),\n            YearLte(Value(start), Value(finish)),\n        ]\n        expected_bound_params = [\n            (start, finish),\n            (finish,),\n            (start,),\n            (start,),\n            (finish,),\n        ]\n        for look_up, expected in zip(look_ups, expected_bound_params):\n            with self.subTest(look_up=look_up):\n                self.assertEqual(look_up.get_bound_params(start, finish), expected)\n", "    def test_year_lookup_bounds_for_date_field(self):\n        lookup = YearLookup(lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()), rhs=Value(2010))\n        start, finish = lookup.year_lookup_bounds(None, 2010)\n        self.assertEqual(start, datetime(2010, 1, 1, 0, 0, 0))\n        self.assertEqual(finish, datetime(2010, 12, 31, 23, 59, 59))\n", "    def test_get_bound_params(self):\n        look_up = YearExact(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n        start, finish = look_up.year_lookup_bounds(mock.Mock(), 2010)\n        self.assertEqual(look_up.get_bound_params(start, finish), (start, finish))\n\n        look_up = YearGt(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n        start, finish = look_up.year_lookup_bounds(mock.Mock(), 2010)\n        self.assertEqual(look_up.get_bound_params(start, finish), (finish,))\n\n        look_up = YearGte(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n        start, finish = look_up.year_lookup_bounds(mock.Mock(), 2010)\n        self.assertEqual(look_up.get_bound_params(start, finish), (start,))\n\n        look_up = YearLt(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n        start, finish = look_up.year_lookup_bounds(mock.Mock(), 2010)\n        self.assertEqual(look_up.get_bound_params(start, finish), (start,))\n\n        look_up = YearLte(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n        start, finish = look_up.year_lookup_bounds(mock.Mock(), 2010)\n        self.assertEqual(look_up.get_bound_params(start, finish), (finish,))\n"], "sample_243": ["def test_build_filtered_relation_q(self):\n    query = Query(Author)\n    q_object = ~Q(num__gt=2) | Q(name='John')\n    clause = query.build_filtered_relation_q(q_object, reuse=None)\n    self.assertEqual(clause.connector, OR)\n    self.assertEqual(len(clause.children), 2)\n    self.assertIsInstance(clause.children[0], query.where_class)\n    self.assertIsInstance(clause.children[1], query.where_class)\n    self.assertTrue(clause.children[0].negated)\n    self.assertFalse(clause.children[1].negated)\n", "def test_add_annotation(self):\n    query = Query(Author)\n    annotation = F('num')\n    query.add_annotation(annotation, 'annotation', is_summary=False)\n    self.assertIn('annotation', query.annotations)\n    self.assertEqual(query.annotations['annotation'], annotation)\n", "def test_query_get_loaded_field_names(self):\n    query = Query(Author)\n    query.deferred_loading = (frozenset(), True)\n    self.assertEqual(query.get_loaded_field_names(), {})\n\n    query.deferred_loading = (frozenset(['name']), True)\n    loaded_field_names = query.get_loaded_field_names()\n    self.assertIn(Author, loaded_field_names)\n    self.assertEqual(loaded_field_names[Author], {'id', 'num'})\n\n    query.deferred_loading = (frozenset(), False)\n    loaded_field_names = query.get_loaded_field_names()\n    self.assertIn(Author, loaded_field_names)\n    self.assertEqual(loaded_field_names[Author], set(field.attname for field in Author._meta.concrete_fields))\n", "def test_get_external_cols(self):\n    query = Query(Item)\n    query.add_annotation(F('name'), 'name', select=False)\n    query.add_annotation(Lower('note__note'), 'note', select=False)\n    external_cols = query.get_external_cols()\n    self.assertEqual(len(external_cols), 2)\n    self.assertEqual(external_cols[0].alias, 'item')\n    self.assertEqual(external_cols[1].alias, 'note')\n", "def test_build_filtered_relation_q(self):\n    query = Query(Author)\n    filtered_relation = Author.objects.filter(num__gt=2)\n    where = query.build_filtered_relation_q(filtered_relation)\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n"], "sample_244": ["def test_formset_absolute_max_is_not_overridden_by_validate_max(self):\n    \"\"\"The absolute_max limit isn't overridden by the validate_max check.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=1, max_num=2, validate_max=True, absolute_max=1)\n    data = {\n        'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.forms), 1)\n    self.assertEqual(formset.non_form_errors(), ['Please submit at most 1 form.'])\n", "def test_formset_non_field_errors(self):\n    \"\"\"A FormSet's non-form errors are available via the non_form_errors() method.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n    }\n    ChoiceFormSet = formset_factory(Choice, max_num=1, validate_max=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at most 1 form.'])\n", "def test_formset_is_multipart(self):\n    \"\"\"is_multipart() returns True if any form in the formset has a FileField.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    class NoFileForm(Form):\n        text = CharField()\n\n    FileFormSet = formset_factory(FileForm)\n    NoFileFormSet = formset_factory(NoFileForm)\n\n    file_formset = FileFormSet()\n    no_file_formset = NoFileFormSet()\n\n    self.assertTrue(file_formset.is_multipart())\n    self.assertFalse(no_file_formset.is_multipart())\n\n    # If a form in the formset has a FileField, but the formset has no data,\n    # is_multipart() should still return True.\n    empty_file_formset = FileFormSet(data={})\n    self.assertTrue(empty_file_formset.is_multipart())\n", "def test_formset_with_empty_management_form(self):\n    \"\"\"A formset with an empty management form is invalid.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '',\n        'choices-INITIAL_FORMS': '',\n        'choices-MIN_NUM_FORMS': '',\n        'choices-MAX_NUM_FORMS': '',\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            'ManagementForm data is missing or has been tampered with. '\n            'Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS. '\n            'You may need to file a bug report if the issue persists.',\n        ],\n    )\n", "def test_all_valid_empty_formset(self):\n    \"\"\"all_valid() works with an empty formset.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet()\n    formset2 = ChoiceFormSet()\n    self.assertIs(all_valid((formset1, formset2)), False)\n    self.assertEqual(formset1._errors, [])\n    self.assertEqual(formset2._errors, [])\n"], "sample_245": ["    def test_build_file_path(self):\n        class MockCommand:\n                self.domain = 'django'\n                self.gettext_version = (0, 18, 3)\n\n        translatable = TranslatableFile('path/to/dir', 'file.txt', 'locale')\n        build_file = BuildFile(MockCommand(), 'django', translatable)\n        self.assertEqual(build_file.path, os.path.join('path/to/dir', 'file.txt'))\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn(\"#~ msgid\", po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"\\n#~ msgstr \"\"', po_contents)\n", "    def test_no_obsolete_disabled_by_default(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE, encoding='utf-8') as fp:\n            po_contents = fp.read()\n            self.assertIn('#~ msgid \"Obsolete message\"', po_contents)\n"], "sample_246": ["    def test_plural_forms_re(self):\n        \"\"\"\n        Test that the plural forms regular expression correctly matches the\n        \"Plural-Forms\" entry in a PO file header.\n        \"\"\"\n        match = plural_forms_re.search('\"Plural-Forms: nplurals=2; plural=(n != 1)\\\\n\"')\n        self.assertIsNotNone(match)\n        self.assertEqual(match['value'], '\"Plural-Forms: nplurals=2; plural=(n != 1)\\\\n\"')\n\n        no_match = plural_forms_re.search('\"Not-a-Plural-Forms: nplurals=2; plural=(n != 1)\\\\n\"')\n        self.assertIsNone(no_match)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"\\n#~ msgstr \"\"', po_contents)\n", "    def test_write_pot_file(self):\n        potfile = 'locale/django.pot'\n        msgs = '# SOME DESCRIPTIVE TITLE.\\n\\nmsgid \"\"\\nmsgstr \"\"\\n'\n        write_pot_file(potfile, msgs)\n        with open(potfile) as fp:\n            self.assertEqual(fp.read(), msgs)\n", "    def test_no_obsolete(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE, 'a') as fp:\n            fp.write('#~ msgid \"Obsolete message\"\\n#~ msgstr \"\"\\n')\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"Obsolete message\"', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"Obsolete message\"', po_contents)\n"], "sample_247": ["def test_alias_subquery_and_aggregate_values_chaining(self):\n    subquery = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n    ).values('publisher').annotate(count=Count('pk')).values('count')\n    qs = Publisher.objects.alias(\n        total_books=Value(1),\n        max_pubdate=Subquery(subquery, output_field=IntegerField()),\n    ).values('name', 'total_books', 'max_pubdate').order_by('max_pubdate')\n    self.assertCountEqual(qs, [\n        {'max_pubdate': 1, 'name': 'Sams', 'total_books': 1},\n        {'max_pubdate': 1, 'name': 'Morgan Kaufmann', 'total_books': 1},\n        {'max_pubdate': None, 'name': 'Apress', 'total_books': 1},\n        {'max_pubdate': None, 'name': \"Jonno's House of Books\", 'total_books': 1},\n        {'max_pubdate': None, 'name': 'Prentice Hall', 'total_books': 1},\n    ])\n", "def test_alias_with_m2m(self):\n    books = Book.objects.alias(author_age=F('authors__age')).filter(pk=self.b1.pk).order_by('author_age')\n    self.assertIs(hasattr(books[0], 'author_age'), False)\n    self.assertEqual(books[0].authors.all()[0].age, 34)\n    self.assertEqual(books[1].authors.all()[0].age, 35)\n", "def test_alias_with_subquery(self):\n    subquery = Book.objects.filter(pk=OuterRef('pk')).values('rating')\n    qs = Book.objects.alias(\n        rating_alias=Subquery(subquery),\n    ).annotate(rating=F('rating_alias'))\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertEqual(book.rating, book.rating_alias)\n", "def test_alias_with_m2m(self):\n    qs = Author.objects.alias(\n        friend_names=F('friends__name'),\n    ).filter(friend_names='Adrian Holovaty')\n    self.assertIs(hasattr(qs.first(), 'friend_names'), False)\n    self.assertSequenceEqual(qs, [self.a2, self.a4])\n", "def test_values_with_alias(self):\n    qs = Book.objects.alias(\n        rating_alias=F('rating') - 1,\n    ).annotate(rating=F('rating_alias')).values('rating')\n    self.assertIs(hasattr(qs.first(), 'rating_alias'), False)\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertEqual(book['rating'], book['rating'])\n"], "sample_248": ["def test_shell_with_python_interface(self, select):\n    select.return_value = ([], [], [])\n    with captured_stdout() as stdout:\n        call_command('shell', interface='python')\n    self.assertIn('>>> ', stdout.getvalue())\n", "def test_shell_with_no_enhanced_shells_installed(self, select):\n    select.return_value = ([], [], [])\n    with captured_stdout() as stdout:\n        call_command('shell')\n    self.assertIn(\"Python\", stdout.getvalue())\n", "def test_shell_with_python_interface(self, select):\n    select.return_value = ([], [], [])\n    with captured_stdout() as stdout:\n        call_command('shell', interface='python', command='print(\"Hello, World!\")')\n    self.assertEqual(stdout.getvalue().strip(), 'Hello, World!')\n", "def test_shell_with_no_interfaces_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n", "def test_shell_with_no_interfaces_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n"], "sample_249": ["    def test_clone_test_db(self):\n        # Create a test database.\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Clone the test database.\n            clone_suffix = 'clone'\n            creation.clone_test_db(clone_suffix, verbosity=0, autoclobber=True, keepdb=False)\n            # Check that the cloned database is different from the original one.\n            self.assertNotEqual(\n                creation.get_test_db_clone_settings(clone_suffix)['NAME'],\n                creation._get_test_db_name(),\n            )\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_test_database_prefix(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'mydb'\n        creation = BaseDatabaseCreation(test_connection)\n        self.assertEqual(creation._get_test_db_name(), TEST_DATABASE_PREFIX + 'mydb')\n", "    def test_create_test_db_keepdb(self):\n        # Ensure the database doesn't exist.\n        connection.creation.destroy_test_db(verbosity=0)\n        # Create the database with keepdb=True.\n        connection.creation.create_test_db(keepdb=True, verbosity=0)\n        # Ensure the database still exists after the test.\n        self.assertTrue(connection.creation._database_exists())\n", "    def test_clone_settings(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'test_database'\n        creation = BaseDatabaseCreation(test_connection)\n        clone_settings = creation.get_test_db_clone_settings(suffix='clone_1')\n        self.assertEqual(clone_settings['NAME'], 'test_database_clone_1')\n        self.assertEqual(clone_settings['ENGINE'], test_connection.settings_dict['ENGINE'])\n        self.assertEqual(clone_settings['HOST'], test_connection.settings_dict['HOST'])\n        self.assertEqual(clone_settings['PORT'], test_connection.settings_dict['PORT'])\n", "    def test_clone_test_db(self):\n        # Create a test database.\n        connection.creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        \n        # Clone the test database.\n        connection.creation.clone_test_db(suffix='clone', verbosity=0, autoclobber=True, keepdb=False)\n        \n        # Check that the cloned database has the same tables as the original test database.\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n            tables = [row[0] for row in cursor.fetchall()]\n            self.assertIn('backends_object', tables)\n\n        # Destroy the cloned database.\n        connection.creation.destroy_test_db(suffix='clone', verbosity=0, keepdb=False)\n\n        # Destroy the original test database.\n        connection.creation.destroy_test_db(verbosity=0, keepdb=False)\n"], "sample_250": ["def test_timeformat_with_non_datetime_object(self):\n    # Test that time_format raises an error when passed a non-datetime object\n    with self.assertRaises(TypeError):\n        time_format(\"not a datetime object\", 'H:i')\n", "def test_dateformat_with_backslashes(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    self.assertEqual(dateformat.format(my_birthday, r'\\a\\b\\c'), 'abc')\n    self.assertEqual(dateformat.format(my_birthday, r'\\\\a\\\\b\\\\c'), '\\\\a\\\\b\\\\c')\n", "def test_month_names(self):\n    # Test that the month names are correctly formatted\n    dt = datetime(2022, 9, 1)\n    self.assertEqual(dateformat.format(dt, 'F'), 'September')\n    self.assertEqual(dateformat.format(dt, 'M'), 'Sep')\n    self.assertEqual(dateformat.format(dt, 'b'), 'sep')\n    self.assertEqual(dateformat.format(dt, 'N'), 'September')\n    self.assertEqual(dateformat.format(dt, 'E'), 'Sept')\n", "def test_E_format(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    self.assertEqual(dateformat.format(my_birthday, 'E'), 'July')\n\n    with translation.override('fr'):\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'juillet')\n", "def test_E_format(self):\n    # Test alternative month names as required by some locales.\n    dt = datetime(2022, 5, 16)\n    self.assertEqual(dateformat.format(dt, 'E'), 'May')\n"], "sample_251": ["def test_alias_with_m2m(self):\n    books = Book.objects.alias(\n        author_age=F('authors__age')\n    ).annotate(author_age_output=F('author_age'))\n    self.assertIs(hasattr(books.first(), 'author_age'), False)\n    for book in books:\n        with self.subTest(book=book):\n            self.assertEqual(book.author_age_output, book.authors.first().age)\n", "def test_filter_alias_with_m2m(self):\n    qs = Author.objects.alias(\n        book_isbn=F('book__isbn'),\n    ).filter(book_isbn='159059725')\n    self.assertIs(hasattr(qs.first(), 'book_isbn'), False)\n    self.assertEqual(qs.count(), 2)\n\n    qs = Author.objects.alias(\n        book_isbn=F('book_contact_set__isbn'),\n    ).filter(book_isbn='159059725')\n    self.assertIs(hasattr(qs.first(), 'book_isbn'), False)\n    self.assertEqual(qs.count(), 1)\n", "def test_alias_in_f_grouped_by_annotation(self):\n    qs = (\n        Publisher.objects.alias(multiplier=Value(3))\n        # group by option => sum of value * multiplier\n        .values('name')\n        .annotate(multiplied_value_sum=Sum(F('multiplier') * F('num_awards')))\n        .order_by()\n    )\n    self.assertCountEqual(\n        qs, [\n            {'multiplied_value_sum': 9, 'name': 'Apress'},\n            {'multiplied_value_sum': 0, 'name': \"Jonno's House of Books\"},\n            {'multiplied_value_sum': 27, 'name': 'Morgan Kaufmann'},\n            {'multiplied_value_sum': 21, 'name': 'Prentice Hall'},\n            {'multiplied_value_sum': 3, 'name': 'Sams'},\n        ]\n    )\n", "def test_alias_with_raw_sql(self):\n    qs = Book.objects.alias(\n        rating_alias=RawSQL(\"SELECT AVG(rating) FROM annotations_book\", ())\n    ).annotate(rating=F('rating_alias'))\n    self.assertIs(hasattr(qs.first(), 'rating_alias'), False)\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertIsNotNone(book.rating)\n", "def test_alias_with_raw_sql(self):\n    qs = Book.objects.alias(\n        raw_sql_alias=RawSQL('SELECT COUNT(*) FROM annotations_author', ()),\n    ).annotate(raw_sql=F('raw_sql_alias'))\n    self.assertIs(hasattr(qs.first(), 'raw_sql_alias'), False)\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertEqual(book.raw_sql, Author.objects.count())\n"], "sample_252": ["    def test_check_constraints(self):\n        model = NullableJSONModel(value='invalid json')\n        with self.assertRaises((IntegrityError, DataError, OperationalError)):\n            model.save()\n", "    def test_check_constraint(self):\n        model = NullableJSONModel(value='{\"a\": 1}')\n        with CaptureQueriesContext(connection) as queries:\n            model.save()\n        self.assertGreater(len([q for q in queries if 'CHECK' in q['sql']]), 0)\n", "    def test_key_transform_json_exact(self):\n        obj = NullableJSONModel.objects.create(value={'a': 'b', 'c': 14})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a__exact='b'),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__c__exact=14),\n            [obj],\n        )\n", "def test_key_transform_with_empty_string(self):\n    obj = NullableJSONModel.objects.create(value={'': 'empty'})\n    self.assertEqual(NullableJSONModel.objects.filter(**{'value__': 'empty'}).get(), obj)\n    self.assertIs(NullableJSONModel.objects.filter(**{'value__': 'non-empty'}).exists(), False)\n", "def test_key_transform_with_annotation(self):\n    qs = NullableJSONModel.objects.annotate(\n        foo=KeyTransform('foo', 'value'),\n        baz=KeyTransform('baz', 'value'),\n    ).filter(foo='bar')\n    self.assertSequenceEqual(qs, [self.objs[7]])\n"], "sample_253": ["    def test_file_changed_signal(self):\n        # Create a mock signal receiver\n        receiver = mock.Mock()\n\n        # Connect the receiver to the file_changed signal\n        autoreload.file_changed.connect(receiver)\n\n        # Trigger the file_changed signal\n        autoreload.trigger_reload('test_file.py')\n\n        # Assert that the receiver was called\n        self.assertTrue(receiver.called)\n        self.assertEqual(receiver.call_count, 1)\n        self.assertEqual(receiver.call_args[1]['file_path'], 'test_file.py')\n", "    def test_ensure_echo_on_calls_tcsetattr(self, mocked_termios):\n        fd = 123\n        when = termios.TCSANOW\n        attrs = [1, 2, 3, 4]\n        mocked_termios.tcgetattr.return_value = attrs\n\n        with mock.patch('sys.stdin.fileno', return_value=fd):\n            autoreload.ensure_echo_on()\n\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n        self.assertSequenceEqual(mocked_termios.tcgetattr.call_args[0], [fd])\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n        self.assertSequenceEqual(\n            mocked_termios.tcsetattr.call_args[0],\n            [fd, when, [1, 2, 3, 5]]  # 5 is the value for ECHO.\n        )\n", "    def test_trigger_reload(self, mocked_exit):\n        autoreload.trigger_reload('/path/to/file')\n        self.assertEqual(mocked_exit.call_count, 1)\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_reloader_stop(self):\n        reloader = autoreload.StatReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n", "    def test_no_tty(self, mocked_isatty):\n        autoreload.ensure_echo_on()\n        self.assertFalse(termios.tcgetattr.called)\n"], "sample_254": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', email='super@example.com', password='secret')\n", "    def test_inline_admin_form_validation(self):\n        class InlineFormValidationModelAdmin(ModelAdmin):\n            inlines = [InnerInline]\n\n        model_admin = InlineFormValidationModelAdmin(Holder, admin_site)\n        request = self.factory.get(reverse('admin:admin_inlines_holder_add'))\n        request.user = self.superuser\n        inline_formset = model_admin.get_inline_formsets(request, obj=None)[0]\n        form = inline_formset.form\n        form.instance = Inner()\n        form.cleaned_data = {'dummy': 'test'}\n        self.assertTrue(model_admin.validate_form(request, form))\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_get_formsets_with_inlines(self):\n        model_admin = ModelAdmin(Holder, admin_site)\n        request = RequestFactory().get(reverse('admin:admin_inlines_holder_add'))\n        request.user = self.superuser\n        obj = Holder.objects.create(dummy=13)\n        inlines = model_admin.get_inline_instances(request, obj)\n        formsets, inline_instances = model_admin._create_formsets(request, obj, change=True)\n        expected_inline_formsets = model_admin.get_formsets_with_inlines(request, obj)\n        expected_inline_instances = [inline[1] for inline in expected_inline_formsets]\n        self.assertEqual(inlines, inline_instances)\n        self.assertEqual(inlines, expected_inline_instances)\n        self.assertEqual(len(formsets), len(inlines))\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', email='super@example.com', password='secret')\n"], "sample_256": ["    def test_render_with_non_string_value(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 12345\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'unusable'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'unusable password'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'foo'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_widget_attrs(self):\n        field = UsernameField()\n        self.assertEqual(field.widget.attrs['autocapitalize'], 'none')\n        self.assertEqual(field.widget.attrs['autocomplete'], 'username')\n"], "sample_255": ["    def test_close(self):\n        \"\"\"ServerHandler's close() doesn't raise an exception.\"\"\"\n        request = Stub(makefile=lambda mode, *a, **kw: BytesIO())\n        server = Stub(base_environ={}, get_app=lambda: None)\n\n        handler = ServerHandler(request.makefile('rb'), request.makefile('wb'), request.makefile('wb'), {})\n        handler.request_handler = request\n        handler.close()\n", "    def test_close(self):\n        \"\"\"ServerHandler's close() method calls _read_limited()\"\"\"\n        stdin = BytesIO(b'test data')\n        handler = ServerHandler(stdin, BytesIO(), BytesIO(), {})\n        with self.assertLogs('django.server', 'DEBUG') as cm:\n            handler.close()\n        self.assertEqual(stdin.tell(), len(b'test data'))\n        self.assertEqual(cm.records[0].getMessage(), 'request body read after response sent.')\n", "    def test_close(self):\n        \"\"\"ServerHandler's close() calls _read_limited() on the LimitedStream.\"\"\"\n        stream = LimitedStream(BytesIO(b'Hello'), 5)\n        environ = {'CONTENT_LENGTH': 5}\n        handler = ServerHandler(stream, BytesIO(), BytesIO(), environ)\n        handler.close()\n        self.assertEqual(stream.tell(), 5)\n", "    def test_cleanup_headers(self):\n        class StubWSGIRequestHandler:\n                self.close_connection = False\n\n        environ = {'SERVER_PROTOCOL': 'HTTP/1.1'}\n        request_handler = StubWSGIRequestHandler()\n        handler = ServerHandler(BytesIO(), BytesIO(), BytesIO(), environ)\n        handler.request_handler = request_handler\n\n        # Test that 'Connection: close' is added if 'Content-Length' is missing.\n        handler.headers = {}\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers['Connection'], 'close')\n\n        # Test that 'Connection: close' is not added if 'Content-Length' is present.\n        handler.headers = {'Content-Length': '10'}\n        handler.cleanup_headers()\n        self.assertNotIn('Connection', handler.headers)\n\n        # Test that 'Connection: close' is added if the server doesn't support persistent connections.\n        class StubServer:\n                self.__class__ = type('StubServer', (), {})\n\n        handler.request_handler.server = StubServer()\n        handler.headers = {'Content-Length': '10'}\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers['Connection'], 'close')\n", "    def test_close(self):\n        \"\"\"ServerHandler's close method closes the stdin stream\"\"\"\n        class MockWSGIRequestHandler:\n                return BytesIO()\n\n        request_handler = MockWSGIRequestHandler()\n        stdin = BytesIO(b\"Hello, World!\")\n        stdout = BytesIO()\n        stderr = BytesIO()\n        environ = {}\n        handler = ServerHandler(stdin, stdout, stderr, environ)\n        handler.request_handler = request_handler\n\n        handler.close()\n\n        self.assertEqual(stdin.tell(), len(b\"Hello, World!\"))\n"], "sample_257": ["    def test_subquery_key_transform(self):\n        obj = NullableJSONModel.objects.create(value={'a': 'b'})\n        qs = NullableJSONModel.objects.filter(\n            value__a=Subquery(\n                NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value__a')[:1]\n            )\n        )\n        self.assertSequenceEqual(qs, [obj])\n", "    def test_subquery_json_field(self):\n        obj1 = JSONModel.objects.create(value={'a': 'b'})\n        obj2 = JSONModel.objects.create(value={'c': 'd'})\n        qs = JSONModel.objects.filter(\n            value__in=JSONModel.objects.filter(value={'a': 'b'}).values('value'),\n        )\n        self.assertSequenceEqual(qs, [obj1])\n", "    def test_expression_in_key_transform(self):\n        obj = JSONModel.objects.create(value={'d': ['e', 'f']})\n        self.assertSequenceEqual(\n            JSONModel.objects.annotate(\n                key=KeyTransform(F('value__d__0'), 'value'),\n            ).filter(key='e'),\n            [obj],\n        )\n", "def test_key_transform_on_text_lookup(self):\n    tests = [\n        ('value__foo__iexact', 'BaR'),\n        ('value__foo__icontains', 'Ar'),\n        ('value__foo__istartswith', 'B'),\n        ('value__foo__iendswith', 'R'),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup):\n            self.assertIs(NullableJSONModel.objects.filter(**{lookup: value}).exists(), True)\n", "    def test_migrate_to_jsonfield(self):\n        # Create a model with a TextField\n        project_state = self.apply_operations(\n            ProjectState(),\n            operations=[\n                CreateModel(\n                    name='JSONModel',\n                    fields=[\n                        ('id', models.AutoField(primary_key=True)),\n                        ('value', models.TextField()),\n                    ],\n                ),\n            ],\n        )\n        # Add some data\n        with override_settings(DATABASES={'default': db_settings(project_state)}):\n            with connection.schema_editor() as schema_editor:\n                schema_editor.create_model(project_state.apps.get_model('app', 'JSONModel'))\n            JSONModel = project_state.apps.get_model('app', 'JSONModel')\n            JSONModel.objects.create(value='{}')\n\n        # Change the TextField to JSONField\n        new_state = self.apply_operations(\n            project_state,\n            operations=[\n                AlterField(\n                    model_name='jsonmodel',\n                    name='value',\n                    field=models.JSONField(),\n                ),\n            ],\n        )\n        # Make sure the data is still there\n        with override_settings(DATABASES={'default': db_settings(new_state)}):\n            with connection.schema_editor() as schema_editor:\n                schema_editor.deferred_sql = []\n                schema_editor.alter_db_table(\n                    project_state.apps.get_model('app', 'JSONModel'),\n                    new_state.apps.get_model('app', 'JSONModel'),\n                )\n            JSONModel = new_state.apps.get_model('app', 'JSONModel')\n            self.assertEqual(JSONModel.objects.count(), 1)\n"], "sample_258": ["def test_receiver_sender(self):\n    @receiver(a_signal, sender=self.__class__)\n        self.state = val\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n    a_signal.send(sender=object(), val=False)\n    self.assertTrue(self.state)\n", "def test_sender_receivers_cache(self):\n    receiver_1 = Callable()\n    receiver_2 = Callable()\n\n    d_signal.connect(receiver_1)\n    d_signal.connect(receiver_2)\n\n    # Populate the cache for a specific sender.\n    d_signal.send(sender=self, val='test')\n\n    # Ensure that the cache contains the correct receivers.\n    self.assertIn(self, d_signal.sender_receivers_cache)\n    self.assertEqual(len(d_signal.sender_receivers_cache[self]), 2)\n\n    # Disconnect a receiver and check that the cache is cleared.\n    d_signal.disconnect(receiver_1)\n    self.assertNotIn(self, d_signal.sender_receivers_cache)\n\n    # Re-populate the cache and disconnect the remaining receiver.\n    d_signal.send(sender=self, val='test')\n    d_signal.disconnect(receiver_2)\n    self.assertNotIn(self, d_signal.sender_receivers_cache)\n", "def test_weakref_finalization(self):\n    receiver = Callable()\n    weak_receiver = weakref.ref(receiver)\n    a_signal.connect(receiver)\n    del receiver\n    garbage_collect()\n    self.assertIsNone(weak_receiver())\n    self.assertTestIsClean(a_signal)\n", "def test_sender_receivers_cache_clear(self):\n    receiver_1 = Callable()\n    receiver_2 = Callable()\n    d_signal.connect(receiver_1)\n    d_signal.connect(receiver_2)\n\n    # Populate sender_receivers_cache\n    d_signal.send(sender=self, val='test')\n\n    # Disconnect one of the receivers and check that the cache is cleared\n    d_signal.disconnect(receiver_1)\n    self.assertNotIn(self, d_signal.sender_receivers_cache)\n    self.assertEqual(d_signal.sender_receivers_cache.get(self), None)\n\n    # Re-populate sender_receivers_cache and check that it's re-created correctly\n    d_signal.send(sender=self, val='test')\n    self.assertIn(self, d_signal.sender_receivers_cache)\n    self.assertEqual(len(d_signal.sender_receivers_cache[self]), 1)\n", "def test_sender_receivers_cache(self):\n    receiver_1 = Callable()\n    receiver_2 = Callable()\n    d_signal.connect(receiver_1)\n    d_signal.connect(receiver_2)\n\n    # Populate the cache for a specific sender.\n    d_signal.send(sender=self, val='test')\n\n    # Ensure the cache is used for subsequent sends with the same sender.\n    with self.assertNumQueries(0):\n        d_signal.send(sender=self, val='test')\n\n    # Clearing the receivers should also clear the cache.\n    d_signal.receivers = []\n    self.assertEqual(d_signal.sender_receivers_cache, {})\n\n    # Disconnection should clear the cache.\n    d_signal.connect(receiver_1)\n    d_signal.send(sender=self, val='test')\n    d_signal.disconnect(receiver_1)\n    self.assertEqual(d_signal.sender_receivers_cache, {})\n"], "sample_259": ["def test_prefetch_queryset_to_attr(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', queryset=Author.objects.filter(id__in=[self.author1.id, self.author2.id]), to_attr='specific_authors')\n        )\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.specific_authors, [self.author1, self.author2])\n", "def test_prefetch_queryset_with_filter(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', queryset=Author.objects.filter(name__startswith='A'))\n        )\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.authors.all(), [self.author1, self.author2])\n", "def test_prefetch_queryset_with_filter(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', queryset=Author.objects.filter(name__startswith='C'))\n        )\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.authors.all(), [self.author1])\n", "def test_prefetch_queryset_with_to_attr(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', queryset=Author.objects.filter(id__in=[self.author1.id, self.author2.id]), to_attr='the_authors')\n        )\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.the_authors, [self.author1, self.author2])\n", "def test_prefetch_queryset_to_attr(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', queryset=Author.objects.filter(id__in=[self.author1.id, self.author2.id]), to_attr='filtered_authors')\n        )\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.filtered_authors, [self.author1, self.author2])\n"], "sample_260": ["def test_add_remove_index(self):\n    \"\"\"\n    RemoveIndex should cancel AddIndex.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.AddIndex(\"Foo\", models.Index(fields=['name'], name='foo_name_idx')),\n            migrations.RemoveIndex(\"Foo\", 'foo_name_idx'),\n        ],\n        [],\n    )\n\n    # But not the other way around\n    self.assertDoesNotOptimize(\n        [\n            migrations.RemoveIndex(\"Foo\", 'foo_name_idx'),\n            migrations.AddIndex(\"Foo\", models.Index(fields=['name'], name='foo_name_idx')),\n        ],\n    )\n", "def test_create_alter_model_managers(self):\n    \"\"\"\n    AlterModelManagers should optimize into CreateModel.\n    \"\"\"\n    managers = [('objects', EmptyManager())]\n    new_managers = [('new_objects', EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={'verbose_name': 'Foo'},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterModelManagers(name='Foo', managers=new_managers),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n                options={'verbose_name': 'Foo'},\n                bases=(UnicodeModel,),\n                managers=new_managers,\n            ),\n        ],\n    )\n", "def test_add_remove_index(self):\n    \"\"\"\n    RemoveIndex should cancel AddIndex.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"])),\n            migrations.RemoveIndex(\"Foo\", \"name\"),\n        ],\n        [],\n    )\n", "def test_optimize_remove_add_field(self):\n    \"\"\"\n    RemoveField followed by AddField should optimize into AlterField if the\n    removed field has the same name as the added field.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.RemoveField(\"Foo\", \"age\"),\n            migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n        ],\n        [\n            migrations.AlterField(\"Foo\", \"age\", models.IntegerField(null=True)),\n        ],\n    )\n", "def test_optimize_remove_index(self):\n    \"\"\"\n    RemoveIndex should optimize with itself.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"], name=\"foo_name_idx\")),\n            migrations.RemoveIndex(\"Foo\", name=\"foo_name_idx\"),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n        ],\n    )\n"], "sample_261": ["def test_parse_duration_invalid_inputs(self):\n    invalid_inputs = (\n        'abc',\n        'P',\n        '-P',\n        'PT',\n        '-PT',\n        'P1',\n        '-P1',\n        'PT1',\n        '-PT1',\n        'P1Y2M3W4D5H6M7S',\n        '-P1Y2M3W4D5H6M7S',\n        'P1.2.3D',\n        '-P1.2.3D',\n        'P1,2,3D',\n        '-P1,2,3D',\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "def test_parse_datetime_edge_cases(self):\n    # Test datetime parsing with edge cases\n    valid_inputs = (\n        ('2012-04-23T23:59:59', datetime(2012, 4, 23, 23, 59, 59)),\n        ('2012-04-23T00:00:00', datetime(2012, 4, 23, 0, 0, 0)),\n        ('2012-04-23T12:00:00.000001', datetime(2012, 4, 23, 12, 0, 0, 1)),\n        ('2012-04-23T12:00:00,000001', datetime(2012, 4, 23, 12, 0, 0, 1)),\n        ('2012-04-23T12:00:00+23:59', datetime(2012, 4, 23, 12, 0, 0, tzinfo=get_fixed_timezone(86340))),\n        ('2012-04-23T12:00:00-23:59', datetime(2012, 4, 23, 12, 0, 0, tzinfo=get_fixed_timezone(-86340))),\n    )\n    for source, expected in valid_inputs:\n        with self.subTest(source=source):\n            self.assertEqual(parse_datetime(source), expected)\n\n    invalid_inputs = (\n        '2012-04-23T24:00:00',\n        '2012-04-23T12:60:00',\n        '2012-04-23T12:00:60',\n        '2012-04-23T12:00:00.0000001',\n        '2012-04-23T12:00:00+24:00',\n        '2012-04-23T12:00:00-24:00',\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_datetime(source))\n", "def test_parse_datetime_with_microseconds(self):\n    # Valid inputs\n    self.assertEqual(parse_datetime('2012-04-23T09:15:00.123456'), datetime(2012, 4, 23, 9, 15, 0, 123456))\n    self.assertEqual(parse_datetime('2012-04-23T09:15:00,123456'), datetime(2012, 4, 23, 9, 15, 0, 123456))\n\n    # Invalid inputs\n    self.assertIsNone(parse_datetime('20120423091500.123456'))\n    with self.assertRaises(ValueError):\n        parse_datetime('2012-04-56T09:15:90.123456')\n", "def test_parse_datetime_with_timezone_offset(self):\n    # Test that parse_datetime correctly handles timezone offsets\n    test_values = (\n        ('2012-04-23T09:15:00+02', datetime(2012, 4, 23, 9, 15, tzinfo=get_fixed_timezone(120))),\n        ('2012-04-23T09:15:00-02', datetime(2012, 4, 23, 9, 15, tzinfo=get_fixed_timezone(-120))),\n        ('2012-04-23T09:15:00+02:30', datetime(2012, 4, 23, 9, 15, tzinfo=get_fixed_timezone(150))),\n        ('2012-04-23T09:15:00-02:30', datetime(2012, 4, 23, 9, 15, tzinfo=get_fixed_timezone(-150))),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_datetime(source), expected)\n", "def test_parse_duration_invalid(self):\n    # Test invalid inputs for parse_duration function\n    invalid_inputs = [\n        'abc',\n        'P',\n        'PT',\n        'P1',\n        'PT1',\n        '-P',\n        '-PT',\n        '-P1',\n        '-PT1',\n        'P1Y2M3W4D5H6M7S',  # mixed duration with year, month and week\n        'P1Y2M3D4H5M6S',   # mixed duration with year and month\n        'P1Y2W3D4H5M6S',   # mixed duration with year and week\n        'P1M2W3D4H5M6S',   # mixed duration with month and week\n    ]\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n"], "sample_262": ["def test_lazy_deepcopy(self):\n    original_object = [[1, 2], [3, 4]]\n    lazy_obj = lazy(lambda: original_object, list)\n    copied_lazy_obj = copy.deepcopy(lazy_obj())\n    self.assertEqual(original_object, copied_lazy_obj)\n    copied_lazy_obj[0][0] = 'X'\n    self.assertNotEqual(original_object, copied_lazy_obj)\n", "def test_lazy_object_repr(self):\n    class Klazz:\n            return \"\u00ce am \u0101 \u01e8l\u00e2zz.\"\n\n    lazy_obj = lazy(lambda: Klazz(), Klazz)\n    self.assertEqual(repr(lazy_obj()), \"\u00ce am \u0101 \u01e8l\u00e2zz.\")\n", "def test_lazy_object_hash(self):\n    original_object = 'Lazy translation text'\n    lazy_obj = lazy(lambda: original_object, str)\n    self.assertEqual(hash(original_object), hash(lazy_obj()))\n", "def test_keep_lazy(self):\n    \"\"\"Test that keep_lazy decorator works as expected.\"\"\"\n        return x\n\n    lazy_func = keep_lazy(str)(func)\n    self.assertIsInstance(lazy_func(\"hello\"), str)\n\n    lazy_value = lazy(lambda: \"hello\", str)()\n    self.assertIsInstance(lazy_func(lazy_value), Promise)\n", "def test_partition(self):\n    values = list(range(5))\n    left, right = partition(lambda x: x > 3, values)\n    self.assertEqual(left, [0, 1, 2, 3])\n    self.assertEqual(right, [4])\n\n    values = ['hello', 'world', 'a', 'b', 'c']\n    left, right = partition(lambda x: len(x) > 3, values)\n    self.assertEqual(left, ['a', 'b', 'c'])\n    self.assertEqual(right, ['hello', 'world'])\n"], "sample_263": ["    def test_dumpdata_to_stdout(self):\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        new_io = StringIO()\n        management.call_command(\n            'dumpdata',\n            'fixtures',\n            stdout=new_io,\n            stderr=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertJSONEqual(command_output, (\n            '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": '\n            '\"News Stories\"}}, {\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place '\n            'on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, {\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": '\n            '{\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]'\n        ))\n", "def test_dumpdata_progressbar_with_file_output(self):\n    \"\"\"\n    Dumpdata shows a progress bar on the command line when --output is set,\n    stdout is a tty, and verbosity > 0.\n    \"\"\"\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    new_io = StringIO()\n    new_io.isatty = lambda: True\n    with NamedTemporaryFile() as file:\n        options = {\n            'format': 'json',\n            'stdout': new_io,\n            'stderr': new_io,\n            'output': file.name,\n            'verbosity': 2,\n        }\n        management.call_command('dumpdata', 'fixtures', **options)\n        self.assertTrue(new_io.getvalue().endswith('[' + '.' * ProgressBar.progress_width + ']\\n'))\n\n        # Test no progress bar when verbosity = 0\n        options['verbosity'] = 0\n        new_io = StringIO()\n        new_io.isatty = lambda: True\n        options.update({'stdout': new_io, 'stderr': new_io})\n        management.call_command('dumpdata', 'fixtures', **options)\n        self.assertEqual(new_io.getvalue(), '')\n\n        # Test no progress bar when stdout is not a tty\n        options['verbosity'] = 2\n        new_io = StringIO()\n        new_io.isatty = lambda: False\n        options.update({'stdout': new_io, 'stderr': new_io})\n        management.call_command('dumpdata', 'fixtures', **options)\n        self.assertEqual(new_io.getvalue(), '')\n", "def test_dumpdata_with_indent(self):\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    self._dumpdata_assert(\n        ['fixtures'],\n        '[\\n'\n        '    {\\n'\n        '        \"pk\": 1,\\n'\n        '        \"model\": \"fixtures.category\",\\n'\n        '        \"fields\": {\\n'\n        '            \"description\": \"Latest news stories\",\\n'\n        '            \"title\": \"News Stories\"\\n'\n        '        }\\n'\n        '    },\\n'\n        '    {\\n'\n        '        \"pk\": 2,\\n'\n        '        \"model\": \"fixtures.article\",\\n'\n        '        \"fields\": {\\n'\n        '            \"headline\": \"Poker has no place on ESPN\",\\n'\n        '            \"pub_date\": \"2006-06-16T12:00:00\"\\n'\n        '        }\\n'\n        '    },\\n'\n        '    {\\n'\n        '        \"pk\": 3,\\n'\n        '        \"model\": \"fixtures.article\",\\n'\n        '        \"fields\": {\\n'\n        '            \"headline\": \"Time to reform copyright\",\\n'\n        '            \"pub_date\": \"2006-06-16T13:00:00\"\\n'\n        '        }\\n'\n        '    }\\n'\n        ']',\n        indent=4,\n    )\n", "    def test_output_options(self):\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n\n        # Test output to stdout.\n        output = StringIO()\n        management.call_command('dumpdata', 'fixtures', stdout=output)\n        self.assertIn('{\"pk\": 1, \"model\": \"fixtures.category\"', output.getvalue())\n\n        # Test output to file.\n        with NamedTemporaryFile(mode='w+', delete=False) as tmp_file:\n            management.call_command('dumpdata', 'fixtures', output=tmp_file.name)\n            tmp_file.seek(0)\n            self.assertIn('{\"pk\": 1, \"model\": \"fixtures.category\"', tmp_file.read())\n            os.remove(tmp_file.name)\n\n        # Test output to stdout with indent option.\n        output = StringIO()\n        management.call_command('dumpdata', 'fixtures', stdout=output, indent=4)\n        self.assertIn('{\\n    \"pk\": 1,\\n    \"model\": \"fixtures.category\"', output.getvalue())\n\n        # Test output to file with indent option.\n        with NamedTemporaryFile(mode='w+', delete=False) as tmp_file:\n            management.call_command('dumpdata', 'fixtures', output=tmp_file.name, indent=4)\n            tmp_file.seek(0)\n            self.assertIn('{\\n    \"pk\": 1,\\n    \"model\": \"fixtures.category\"', tmp_file.read())\n            os.remove(tmp_file.name)\n", "    def test_dumpdata_progress_indicator(self):\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        new_io = StringIO()\n        new_io.isatty = lambda: True\n        options = {\n            'format': 'json',\n            'stdout': new_io,\n            'stderr': new_io,\n        }\n        management.call_command('dumpdata', 'fixtures', **options)\n        self.assertTrue(new_io.getvalue().endswith('\\n'))\n        lines = new_io.getvalue().splitlines()\n        self.assertEqual(lines[-2], '.' * ProgressBar.progress_width)\n        self.assertEqual(lines[-1], '')\n"], "sample_264": ["def test_update_cookie_with_empty_value(self):\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "def test_store_empty_messages(self):\n    storage = self.get_storage()\n    response = self.get_response()\n    messages = []\n    encoded_data = storage._encode(messages)\n    storage._update_cookie(encoded_data, response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n", "def test_not_finished_sentinel(self):\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 4 messages into the cookie, but not 5.\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        msg = get_random_string(msg_size)\n        storage.add(constants.INFO, msg)\n\n    unstored_messages = storage.update(response)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n\n    # The not_finished sentinel value should be present when there are\n    # unstored messages.\n    encoded_data = response.cookies['messages'].value\n    decoded_data = storage._decode(encoded_data)\n    self.assertEqual(decoded_data[-1], CookieStorage.not_finished)\n", "def test_update_cookie_no_data(self):\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(response.cookies.get(storage.cookie_name), None)\n    self.assertTrue(storage.used)\n\n    # Test the edge case where storage.update() is called after messages have been consumed.\n    storage = self.get_storage()\n    response = self.get_response()\n    for m in storage:\n        pass  # Iterate through the storage to simulate consumption of messages.\n    storage.update(response)\n    self.assertIn(storage.cookie_name, response.cookies)\n    self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n", "def test_update_cookie_empty_data(self):\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertFalse(response.cookies.get(storage.cookie_name))\n    storage._update_cookie('', response)\n    self.assertTrue(response.cookies.get(storage.cookie_name).value == '')\n    self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n"], "sample_265": ["def test_copy_exception(self):\n    exc = TemplateDoesNotExist('template.html', tried=['path1', 'path2'])\n    new_exc = copy_exception(exc)\n    self.assertIsInstance(new_exc, TemplateDoesNotExist)\n    self.assertEqual(new_exc.args, exc.args)\n    self.assertEqual(new_exc.tried, exc.tried)\n    self.assertIsNone(new_exc.__traceback__)\n    self.assertIsNone(new_exc.__context__)\n    self.assertIsNone(new_exc.__cause__)\n\n    # Test that the backend is preserved if provided.\n    backend = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n    new_exc = copy_exception(exc, backend)\n    self.assertEqual(new_exc.backend, backend)\n", "def test_copy_exception(self):\n    original_exc = TemplateDoesNotExist('template.html', tried=None, backend='django')\n    original_exc.template_debug = 'debug info'\n    copied_exc = copy_exception(original_exc)\n    self.assertEqual(copied_exc.args, original_exc.args)\n    self.assertEqual(copied_exc.tried, original_exc.tried)\n    self.assertEqual(copied_exc.backend, original_exc.backend)\n    self.assertEqual(copied_exc.template_debug, original_exc.template_debug)\n    self.assertNotEqual(copied_exc.__traceback__, original_exc.__traceback__)\n    self.assertNotEqual(copied_exc.__context__, original_exc.__context__)\n    self.assertNotEqual(copied_exc.__cause__, original_exc.__cause__)\n", "def test_copy_exception(self):\n    original_exc = TemplateDoesNotExist('template.html', tried=None, backend=self.engine_class)\n    copied_exc = copy_exception(original_exc)\n\n    # Check that the exception attributes are preserved.\n    self.assertEqual(copied_exc.args, original_exc.args)\n    self.assertEqual(copied_exc.tried, original_exc.tried)\n    self.assertEqual(copied_exc.backend, original_exc.backend)\n\n    # Check that the exception's __traceback__, __context__, and __cause__ are not copied.\n    self.assertIsNone(copied_exc.__traceback__)\n    self.assertIsNone(copied_exc.__context__)\n    self.assertIsNone(copied_exc.__cause__)\n\n    # Check that the template debug data is preserved.\n    original_exc.template_debug = {'foo': 'bar'}\n    copied_exc = copy_exception(original_exc)\n    self.assertEqual(copied_exc.template_debug, original_exc.template_debug)\n", "def test_copy_exception(self):\n    original_exc = TemplateDoesNotExist('template.html')\n    new_exc = copy_exception(original_exc)\n    self.assertEqual(new_exc.args, original_exc.args)\n    self.assertEqual(new_exc.tried, original_exc.tried)\n    self.assertEqual(new_exc.backend, original_exc.backend)\n    self.assertEqual(new_exc.chain, original_exc.chain)\n    if hasattr(original_exc, 'template_debug'):\n        self.assertEqual(new_exc.template_debug, original_exc.template_debug)\n", "def test_copy_exception(self):\n    exc = TemplateDoesNotExist('template.html')\n    new_exc = copy_exception(exc)\n    self.assertEqual(new_exc.args, exc.args)\n    self.assertEqual(new_exc.tried, exc.tried)\n    self.assertEqual(new_exc.backend, exc.backend)\n    self.assertIsNone(new_exc.__traceback__)\n    self.assertIsNone(new_exc.__context__)\n    self.assertIsNone(new_exc.__cause__)\n\n    # Test that template debug info is preserved\n    exc.template_debug = {'name': 'template.html'}\n    new_exc = copy_exception(exc)\n    self.assertEqual(new_exc.template_debug, exc.template_debug)\n"], "sample_266": ["def test_conflict_detection(self):\n    \"\"\"\n    MigrationLoader.detect_conflicts() correctly detects conflicts in the graph.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0001_initial', '0002_second']})\n", "def test_detect_conflicts(self):\n    \"\"\"\n    Tests the detect_conflicts method to ensure it can find conflicts between\n    migrations.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0002_second', '0002_conflicting']})\n", "def test_circular_dependency(self):\n    \"\"\"\n    MigrationLoader raises a ValueError when encountering a circular dependency.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    msg = \"Circular dependency detected\"\n    with self.assertRaisesMessage(ValueError, msg):\n        loader.build_graph()\n", "def test_detect_conflicts(self):\n    \"\"\"Tests MigrationLoader.detect_conflicts() returns conflicting migrations.\"\"\"\n    loader = MigrationLoader(connection)\n    loader.disk_migrations = {\n        ('app1', '0001_initial'): None,\n        ('app1', '0002_second'): None,\n        ('app2', '0001_initial'): None,\n    }\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'app1': ['0001_initial', '0002_second']})\n", "def test_conflict_detection(self):\n    \"\"\"\n    Tests detecting conflicting migrations.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {\"migrations\": [\"0001_initial\", \"0002_conflicting\"]})\n"], "sample_267": ["    def test_check_constraints(self):\n        # Create an object with a foreign key that doesn't exist.\n        Object.objects.create(fk=1)\n        # Check constraints should raise an IntegrityError.\n        with self.assertRaises(IntegrityError):\n            connection.check_constraints()\n", "    def test_is_in_memory_db(self):\n        db = connection.creation\n        self.assertTrue(db.is_in_memory_db(':memory:'))\n        self.assertTrue(db.is_in_memory_db('file::memory:?cache=shared'))\n        self.assertFalse(db.is_in_memory_db('test.db'))\n", "    def test_get_connection_params(self):\n        settings_dict = {\n            'NAME': 'test.db',\n            'OPTIONS': {\n                'timeout': 10,\n            }\n        }\n        wrapper = DatabaseWrapper(settings_dict)\n        params = wrapper.get_connection_params()\n        self.assertEqual(params['database'], 'test.db')\n        self.assertEqual(params['timeout'], 10)\n", "    def test_database_wrapper_close(self):\n        wrapper = connection.__class__(connection.settings_dict)\n        wrapper.connect()\n        self.assertTrue(wrapper.connection is not None)\n        wrapper.close()\n        self.assertTrue(wrapper.connection is not None)\n", "    def test_get_connection_params(self):\n        settings_dict = {\n            'NAME': 'test.db',\n            'OPTIONS': {'timeout': 10},\n        }\n        wrapper = DatabaseWrapper(settings_dict)\n        params = wrapper.get_connection_params()\n        self.assertEqual(params['database'], 'test.db')\n        self.assertEqual(params['detect_types'], Database.PARSE_DECLTYPES | Database.PARSE_COLNAMES)\n        self.assertEqual(params['timeout'], 10)\n        self.assertFalse(params['check_same_thread'])\n"], "sample_268": ["    def test_autoreload_started_signal_sent(self):\n        reloader = autoreload.BaseReloader()\n        django_main_thread = threading.Thread(target=lambda: None, name='django-main-thread')\n        with mock.patch('django.utils.autoreload.autoreload_started') as mocked_signal:\n            reloader.run(django_main_thread)\n            self.assertEqual(mocked_signal.send.call_count, 1)\n            self.assertEqual(mocked_signal.send.call_args[1]['sender'], reloader)\n", "    def test_not_a_tty(self, mocked_isatty):\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_isatty.call_count, 1)\n", "    def test_trigger_reload(self):\n        with mock.patch('sys.exit') as mocked_exit:\n            autoreload.trigger_reload('test_file')\n            self.assertEqual(mocked_exit.call_count, 1)\n            self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_autoreload_started_signal_sent(self, mocked_send):\n        reloader = autoreload.BaseReloader()\n        reloader.run(mock.MagicMock())\n        self.assertEqual(mocked_send.call_count, 1)\n        self.assertEqual(mocked_send.call_args[1]['sender'], reloader)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n"], "sample_269": ["    def test_content_type(self):\n        response = self.client.get('/jsi18n/')\n        self.assertEqual(response.headers['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n", "def test_i18n_language_non_english_default_language_code_override(self):\n    \"\"\"\n    Check if the Javascript i18n view returns a language catalog for the\n    selected language when it is different from the default language.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='fr'), override('es'):\n        response = self.client.get('/jsi18n/app1/')\n        self.assertContains(response, 'este texto de app1 debe ser traducido')\n", "    def test_javascript_catalog(self):\n        response = self.client.get('/jsi18n/')\n        self.assertEqual(response.headers['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n        # response content must include a line like:\n        # \"this is to be translated\": <value of trans_txt Python variable>\n        # json.dumps() is used to be able to check Unicode strings.\n        self.assertContains(response, 'Choisir une heure')\n        self.assertContains(response, '\"DATE_INPUT_FORMATS\": [\"%d.%m.%Y\", \"%d.%m.%y\", \"%Y-%m-%d\"]')\n", "    def test_get_plural(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = DjangoTranslation('de', domain='djangojs')\n        self.assertEqual(catalog.get_plural(), '(n != 1)')\n", "def test_i18n_language_with_non_ascii_characters(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    if the default language is en-us, the selected language has a\n    translation available and the app name contains non-ASCII characters.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, 'il faut traduire cette cha\u00eene de caract\u00e8res de app6')\n"], "sample_270": ["    def test_default_pk(self):\n        class Model(models.Model):\n            pass\n\n        self.assertEqual(Model.check(), [\n            Warning(\n                \"Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\",\n                hint=(\n                    \"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                    \"default_auto_field attribute on a per-app basis to point \"\n                    \"to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                ),\n                obj=Model,\n                id='models.W042',\n            )\n        ])\n", "    def test_default_pk(self):\n        class Model(models.Model):\n            field = models.IntegerField()\n\n        errors = Model.check()\n        expected = [] if settings.DEFAULT_AUTO_FIELD else [\n            Warning(\n                f\"Auto-created primary key used when not defining a primary \"\n                f\"key type, by default '{models.AutoField.__name__}'.\",\n                hint=(\n                    f\"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                    f\"{Model._meta.app_config.__class__.__qualname__}.\"\n                    f\"default_auto_field attribute to point to a subclass \"\n                    f\"of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                ),\n                obj=Model,\n                id='models.W042',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_property_and_field_name_clash(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=100)\n\n            @property\n                pass\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The property 'field' clashes with a model field.\",\n                obj=Model,\n                id='models.E029',\n            )\n        ])\n", "    def test_default_pk(self):\n        class Model(models.Model):\n            field = models.IntegerField()\n\n        self.assertEqual(Model.check(), [\n            Warning(\n                f\"Auto-created primary key used when not defining a primary key type, by default '{settings.DEFAULT_AUTO_FIELD}'.\",\n                hint=(\n                    \"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                    f\"{Model._meta.app_config.__class__.__qualname__}.\"\n                    \"default_auto_field attribute to point to a subclass \"\n                    \"of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                ),\n                obj=Model,\n                id='models.W042',\n            )\n        ])\n", "    def test_abstract_model_cannot_be_instantiated(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        with self.assertRaises(TypeError) as cm:\n            AbstractModel()\n        self.assertEqual(\n            str(cm.exception),\n            \"Abstract models cannot be instantiated.\"\n        )\n"], "sample_271": ["    def test_signal_sent(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch.object(autoreload.autoreload_started, 'send') as send_mock:\n            reloader.run(mock.MagicMock())\n        self.assertEqual(send_mock.call_count, 1)\n        self.assertEqual(send_mock.call_args[1]['sender'], reloader)\n", "    def test_does_nothing_if_termios_is_none(self, mocked_termios):\n        mocked_termios.__bool__.return_value = False\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 0)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 0)\n", "    def test_ensure_echo_on_calls_tcsetattr(self, mocked_termios):\n        # Ensure that echo mode is enabled.\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_trigger_reload_calls_sys_exit(self):\n        with mock.patch('sys.exit') as mocked_exit:\n            autoreload.trigger_reload('/path/to/file')\n            self.assertEqual(mocked_exit.call_count, 1)\n            self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_reloader_stops(self):\n        reloader = autoreload.BaseReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n"], "sample_272": ["def test_check_replacements(self):\n    \"\"\"\n    Replacements are checked and recorded as applied after migration.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Apply initial migrations\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    # Mark replaced migration as applied\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    # Check replacements\n    executor.check_replacements()\n    # Replaced migration should be marked as applied\n    self.assertIn((\"migrations\", \"0001_squashed_0002\"), recorder.applied_migrations())\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test that replaced migrations are not included in the migration plan.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    squash_impl = FakeMigration('squash')\n    squash = ('a', 'squash')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(squash, squash_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_replacement(squash, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n\n    plan = executor.migration_plan([squash])\n\n    self.assertEqual(plan, [(squash_impl, False)])\n", "def test_migration_plan_clean_start(self):\n    \"\"\"\n    When clean_start is True, applied migrations should not be considered.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_dependency(None, a2, a1)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n\n    plan = executor.migration_plan({a2}, clean_start=True)\n\n    self.assertEqual(plan, [(a1_impl, False), (a2_impl, False)])\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Replaced migrations should be excluded from the migration plan.\n    \"\"\"\n    graph = MigrationGraph()\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    squash_impl = FakeMigration('squash')\n    squash = ('a', '3_squashed_0002')\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(squash, squash_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_replacement(squash, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n    })\n\n    plan = executor.migration_plan([squash])\n\n    self.assertEqual(plan, [(squash_impl, False)])\n", "def test_migration_plan_for_replaced_migrations(self):\n    \"\"\"\n    Test that the migration plan includes the replacement migrations for a\n    given target.\n    \"\"\"\n    graph = MigrationGraph()\n    r1_impl = FakeMigration('r1')\n    r1 = ('r', '1')\n    r2_impl = FakeMigration('r2')\n    r2 = ('r', '2')\n    s_impl = FakeMigration('s')\n    s = ('s', '1')\n    graph.add_node(r1, r1_impl)\n    graph.add_node(r2, r2_impl)\n    graph.add_node(s, s_impl)\n    graph.add_dependency(None, r2, r1)\n    graph.add_replacement(s, [r1, r2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        r1: r1_impl,\n        r2: r2_impl,\n    })\n\n    plan = executor.migration_plan({s})\n\n    self.assertEqual(plan, [(s_impl, False)])\n"], "sample_273": ["    def test_unique_together(self):\n        class Model(models.Model):\n            a = models.IntegerField()\n            b = models.IntegerField()\n\n            class Meta:\n                unique_together = [['a', 'b']]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_unique_together_with_unsupported_field(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.ManyToManyField('self')\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"'unique_together' refers to a ManyToManyField 'field2', but \"\n                \"ManyToManyFields are not permitted in 'unique_together'.\",\n                obj=Model,\n                id='models.E013',\n            ),\n        ])\n", "    def test_unique_constraint_fields(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=20)\n            field2 = models.CharField(max_length=20)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['field1', 'field2'], name='unique_constraint'),\n                ]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_field_name_clashes(self):\n        class Model1(models.Model):\n            field = models.CharField(max_length=10)\n\n        class Model2(Model1):\n            field = models.CharField(max_length=10)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The field 'field' clashes with the field 'field' from model \"\n                \"'check_framework.Model1'.\",\n                obj=Model2._meta.get_field('field'),\n                id='models.E005',\n            )\n        ])\n", "    def test_default_auto_field(self):\n        class Model(models.Model):\n            pass\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(\n                \"Auto-created primary key used when not defining a primary key type, \"\n                \"by default 'django.db.models.AutoField'.\",\n                hint=(\n                    \"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                    \"CheckDefaultPKConfig.default_auto_field attribute to point to a \"\n                    \"subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                ),\n                obj=Model,\n                id='models.W042',\n            ),\n        ])\n"], "sample_274": ["    def test_modelform_unique_error_messages(self):\n        # Create a model form for the ChoiceModel.\n        class ChoiceModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n\n        # Create two choice models to test unique error messages.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n\n        # Test that the default unique error message is used when a duplicate\n        # name is provided.\n        form = ChoiceModelForm({'name': 'a'})\n        self.assertFormErrors(\n            ['Choice model with this Name already exists.'],\n            form.is_valid,\n        )\n\n        # Test that a custom unique error message is used when a duplicate name\n        # is provided.\n        class CustomChoiceModelForm(ChoiceModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n                error_messages = {\n                    'unique': 'Custom unique error message.',\n                }\n\n        form = CustomChoiceModelForm({'name': 'a'})\n        self.assertFormErrors(\n            ['Custom unique error message.'],\n            form.is_valid,\n        )\n", "    def test_modelform_unique_error_messages(self):\n        class UniqueModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n\n        # Create an existing choice.\n        ChoiceModel.objects.create(pk=1, name='a')\n\n        form = UniqueModelForm({'name': 'a'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors['name'],\n            ['Choice model with this Name already exists.'],\n        )\n\n        class UniqueModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n                error_messages = {\n                    'name': {'unique': 'This name is already in use.'},\n                }\n\n        form = UniqueModelForm({'name': 'a'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors['name'],\n            ['This name is already in use.'],\n        )\n", "    def test_modelformset(self):\n        # Create a model form set\n        from .forms import ChoiceModelFormSet\n\n        # ModelFormSet validation errors should use the form's error messages\n        data = {\n            'form-0-name': '',\n            'form-0-number': '1',\n            'form-1-name': 'b',\n            'form-1-number': '2',\n            'form-2-name': 'c',\n            'form-2-number': '',\n        }\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        formset = ChoiceModelFormSet(data, error_messages=e)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.errors), 3)\n\n        # Check that form 0 has one error on name field\n        self.assertFormErrors(['REQUIRED'], formset.forms[0].clean, '')\n        # Check that form 1 has no errors\n        self.assertTrue(formset.forms[1].is_valid())\n        # Check that form 2 has one error on number field\n        self.assertFormErrors(['REQUIRED'], formset.forms[2].clean, '')\n", "    def test_modelformfield_callback(self):\n        # Define a custom form field.\n        class CustomField(forms.CharField):\n            pass\n\n        # Define a model form with a callback that returns the custom field.\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n                formfield_callback = lambda f, **kwargs: CustomField()\n\n        form = TestModelForm()\n        self.assertIsInstance(form.fields['name'], CustomField)\n", "    def test_modelformfield_callback(self):\n        # Define a model form with a field that uses a formfield callback.\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n                formfield_callback = lambda f, **kwargs: forms.CharField(max_length=10)\n\n        # Create a form instance and verify the callback was applied.\n        form = MyModelForm()\n        self.assertIsInstance(form.fields['name'], forms.CharField)\n        self.assertEqual(form.fields['name'].max_length, 10)\n"], "sample_275": ["    def test_delete_prefetched_objects(self):\n        # Create some objects to test with\n        person = Person.objects.create(name='John')\n        award1 = Award.objects.create(name='Award 1', content_object=person)\n        award2 = Award.objects.create(name='Award 2', content_object=person)\n\n        # Prefetch the awards for the person\n        person = Person.objects.prefetch_related('award_set').get(pk=person.pk)\n\n        # Delete one of the awards\n        award1.delete()\n\n        # Check that the prefetch cache is updated correctly\n        self.assertEqual(len(person.award_set.all()), 1)\n        self.assertEqual(person.award_set.all()[0].pk, award2.pk)\n", "    def test_delete_exists(self):\n        # Create some books.\n        Book.objects.create(pagecount=100)\n        Book.objects.create(pagecount=200)\n        Book.objects.create(pagecount=300)\n\n        # Check that delete() returns the correct number of deleted objects.\n        deleted, _rows_count = Book.objects.filter(pagecount__lt=250).delete()\n        self.assertEqual(deleted, 2)\n        self.assertEqual(Book.objects.count(), 1)\n\n        # Check that the remaining book is the one with pagecount=300.\n        self.assertEqual(Book.objects.get().pagecount, 300)\n", "    def setUpTestData(cls):\n        cls.books = [Book.objects.create(pagecount=x) for x in range(10)]\n", "    def test_exists_after_delete(self):\n        book = Book.objects.create(pagecount=100)\n        self.assertTrue(Book.objects.filter(pk=book.pk).exists())\n        book.delete()\n        self.assertFalse(Book.objects.filter(pk=book.pk).exists())\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(pagecount=100)\n        cls.book2 = Book.objects.create(pagecount=200)\n"], "sample_276": ["    def test_get_return_data_type(self):\n        tests = (\n            ('get_list', 'List'),\n            ('get_count', 'Integer'),\n            ('other_method', ''),\n        )\n        for method_name, output in tests:\n            with self.subTest(method_name=method_name):\n                self.assertEqual(get_return_data_type(method_name), output)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_model_detail_with_non_trivial_templates(self):\n        self.client.force_login(self.superuser)\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '<h1>admin_docs.Person</h1>')\n", "    def test_function_view(self):\n            pass\n        self.assertEqual(get_view_name(view), 'view')\n", "    def test_extract_views_from_urlpatterns(self):\n        class View:\n            pass\n\n        patterns = [\n            path('test/', View.as_view(), name='test-view'),\n            path('test/<int:pk>/', View.as_view(), name='test-detail-view'),\n            re_path(r'^test/(?P<pk>\\d+)/$', View.as_view(), name='test-detail-view-regex'),\n        ]\n\n        views = extract_views_from_urlpatterns(patterns)\n\n        self.assertEqual(len(views), 3)\n        self.assertEqual(views[0][0], View.as_view())\n        self.assertEqual(views[1][0], View.as_view())\n        self.assertEqual(views[2][0], View.as_view())\n"], "sample_277": ["def test_register_lookup(self):\n    class TestLookup:\n        lookup_name = 'test'\n\n    class TestField(RegisterLookupMixin):\n        pass\n\n    TestField.register_lookup(TestLookup)\n    self.assertEqual(TestField._get_lookup('test'), TestLookup)\n\n    # Test registering the same lookup twice\n    with self.assertRaises(AttributeError):\n        TestField.register_lookup(TestLookup)\n\n    # Test registering a lookup with a different name\n    TestField.register_lookup(TestLookup, 'another_test')\n    self.assertEqual(TestField._get_lookup('another_test'), TestLookup)\n\n    # Test unregistering a lookup\n    TestField._unregister_lookup(TestLookup)\n    self.assertIsNone(TestField._get_lookup('test'))\n\n    # Test clearing cached lookups\n    TestField.get_lookups.cache_clear()\n    self.assertEqual(TestField.get_lookups(), {})\n", "def test_register_lookup(self):\n    class TestLookup:\n        lookup_name = 'test'\n\n    Q.register_lookup(TestLookup)\n    self.assertIn('test', Q.get_lookups())\n    Q._unregister_lookup(TestLookup)\n    self.assertNotIn('test', Q.get_lookups())\n", "def test_register_lookup(self):\n    class TestLookup:\n        lookup_name = 'test'\n\n    class TestClass(RegisterLookupMixin):\n        pass\n\n    TestClass.register_lookup(TestLookup)\n\n    self.assertIn('test', TestClass.get_lookups())\n    self.assertIs(TestClass.get_lookups()['test'], TestLookup)\n\n    # Test unregistering a lookup.\n    TestClass._unregister_lookup(TestLookup)\n    self.assertNotIn('test', TestClass.get_lookups())\n", "def test_combine_nested(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q3 = Q(category='electronics')\n    q = (q1 | q2) & q3\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price__gt', F('discounted_price')),\n        ('price', F('discounted_price')),\n        ('category', 'electronics'),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'AND'})\n    self.assertEqual(Q(*args, **kwargs), q)\n", "def test_deconstruct_negated_nested(self):\n    q = ~(Q(price__gt=F('discounted_price')) & Q(price=F('discounted_price')))\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price__gt', F('discounted_price')),\n        ('price', F('discounted_price')),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'AND', '_negated': True})\n"], "sample_278": ["    def test_negation(self):\n        q = ~Q(a=1)\n        self.assertEqual(q.children, [Q(a=1)])\n        self.assertTrue(q.negated)\n", "    def test_combine(self):\n        q1 = Q(a=1)\n        q2 = Q(b=2)\n        combined = q1 | q2\n        self.assertEqual(combined.connector, Q.OR)\n        self.assertEqual(len(combined.children), 2)\n\n        combined = q1 & q2\n        self.assertEqual(combined.connector, Q.AND)\n        self.assertEqual(len(combined.children), 2)\n", "    def test_deferred_attribute(self):\n        c = Company.objects.create(name='Example Inc.', num_employees=2300, num_chairs=5)\n        self.assertIsInstance(c._meta.get_field('name').get_cache_name(), str)\n", "    def test_combine(self):\n        q1 = Q(x=1)\n        q2 = Q(y=2)\n        self.assertEqual((q1 & q2).children, [q1, q2])\n        self.assertEqual((q1 | q2).children, [q1, q2])\n", "    def setUpTestData(cls):\n        cls.company = Company.objects.create(name='Example Inc.', num_employees=2300, num_chairs=5)\n        cls.employee1 = Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        cls.employee2 = Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n"], "sample_279": ["def test_clone(self):\n    constraint = models.CheckConstraint(check=models.Q(price__gt=models.F('discounted_price')), name='price')\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint.name, cloned_constraint.name)\n    self.assertEqual(constraint.check, cloned_constraint.check)\n", "def test_clone(self):\n    constraint = models.UniqueConstraint(fields=['foo', 'bar'], name='unique')\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n\n    constraint_with_condition = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        condition=models.Q(foo=models.F('bar'))\n    )\n    cloned_constraint_with_condition = constraint_with_condition.clone()\n    self.assertEqual(constraint_with_condition, cloned_constraint_with_condition)\n    self.assertIsNot(constraint_with_condition, cloned_constraint_with_condition)\n\n    constraint_with_deferrable = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    cloned_constraint_with_deferrable = constraint_with_deferrable.clone()\n    self.assertEqual(constraint_with_deferrable, cloned_constraint_with_deferrable)\n    self.assertIsNot(constraint_with_deferrable, cloned_constraint_with_deferrable)\n\n    constraint_with_include = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        include=['baz_1', 'baz_2'],\n    )\n    cloned_constraint_with_include = constraint_with_include.clone()\n    self.assertEqual(constraint_with_include, cloned_constraint_with_include)\n    self.assertIsNot(constraint_with_include, cloned_constraint_with_include)\n\n    constraint_with_opclasses = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        opclasses=['text_pattern_ops', 'varchar_pattern_ops'],\n    )\n    cloned_constraint_with_opclasses = constraint_with_opclasses.clone()\n    self.assertEqual(constraint_with_opclasses, cloned_constraint_with_opclasses)\n    self.assertIsNot(constraint_with_opclasses, cloned_constraint_with_opclasses)\n", "    def test_repr(self):\n        self.assertEqual(repr(models.Deferrable.DEFERRED), '<Deferrable.DEFERRED: \"deferred\">')\n        self.assertEqual(repr(models.Deferrable.IMMEDIATE), '<Deferrable.IMMEDIATE: \"immediate\">')\n", "def test_clone(self):\n    check = models.Q(price__gt=models.F('discounted_price'))\n    name = 'price_gt_discounted_price'\n    constraint = models.CheckConstraint(check=check, name=name)\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint.name, cloned_constraint.name)\n    self.assertEqual(constraint.check, cloned_constraint.check)\n", "def test_clone(self):\n    constraint = models.UniqueConstraint(fields=['foo', 'bar'], name='unique')\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n\n    constraint_with_condition = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        condition=models.Q(foo=models.F('bar')),\n    )\n    cloned_constraint_with_condition = constraint_with_condition.clone()\n    self.assertEqual(constraint_with_condition, cloned_constraint_with_condition)\n    self.assertIsNot(constraint_with_condition, cloned_constraint_with_condition)\n\n    constraint_with_deferrable = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    cloned_constraint_with_deferrable = constraint_with_deferrable.clone()\n    self.assertEqual(constraint_with_deferrable, cloned_constraint_with_deferrable)\n    self.assertIsNot(constraint_with_deferrable, cloned_constraint_with_deferrable)\n\n    constraint_with_include = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        include=['baz_1', 'baz_2'],\n    )\n    cloned_constraint_with_include = constraint_with_include.clone()\n    self.assertEqual(constraint_with_include, cloned_constraint_with_include)\n    self.assertIsNot(constraint_with_include, cloned_constraint_with_include)\n\n    constraint_with_opclasses = models.UniqueConstraint(\n        fields=['foo', 'bar'],\n        name='unique',\n        opclasses=['text_pattern_ops', 'varchar_pattern_ops'],\n    )\n    cloned_constraint_with_opclasses = constraint_with_opclasses.clone()\n    self.assertEqual(constraint_with_opclasses, cloned_constraint_with_opclasses)\n    self.assertIsNot(constraint_with_opclasses, cloned_constraint_with_opclasses)\n"], "sample_280": ["def test_aggregation_filter_on_subquery_annotation(self):\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).values('rating')\n    publishers = Publisher.objects.annotate(\n        highest_rated_book=Subquery(subquery.order_by('-rating')[:1]),\n    ).filter(highest_rated_book__gt=4.0).order_by('name')\n    self.assertQuerysetEqual(\n        publishers, [\n            'Morgan Kaufmann',\n        ],\n        lambda p: p.name,\n    )\n", "def test_aggregation_filter(self):\n    books = Book.objects.aggregate(\n        highly_rated=Sum('price', filter=Q(rating__gt=4.0)),\n        lowly_rated=Sum('price', filter=Q(rating__lte=4.0)),\n    )\n    self.assertEqual(books['highly_rated'], Decimal('82.80') + Decimal('75.00'))\n    self.assertEqual(books['lowly_rated'], Decimal('30.00') + Decimal('23.09') + Decimal('29.69') + Decimal('29.69'))\n", "def test_aggregation_default_passed_expression_with_params(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__lt=3.0), default=Value(10) * F('pages')),\n    )\n    self.assertAlmostEqual(result['value'], Decimal('5280'), places=2)\n", "def test_aggregation_default_passed_another_aggregate_with_filter(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__lt=3.0), default=Avg('pages', filter=Q(rating__gt=4.0)) / 10.0),\n    )\n    self.assertAlmostEqual(result['value'], Decimal('91.2'), places=1)\n", "def test_aggregation_default_passed_another_aggregate_with_filter(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__lt=3.0), default=Avg('pages', filter=Q(rating__gt=4.0)) / 10.0),\n    )\n    self.assertAlmostEqual(result['value'], Decimal('92.8'), places=2)\n"], "sample_281": ["def test_get_context_data(self):\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    context = response.context_data\n    self.assertIn('object_list', context)\n    self.assertIn('page_obj', context)\n    self.assertEqual(context['object_list'].count(), Question.objects.count())\n", "def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': ' invalid', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_get_context_data(self):\n    q = Question.objects.create(question='Is this a question?')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    context = response.context_data\n    self.assertIn('object_list', context)\n    self.assertEqual(len(context['object_list']), 1)\n    self.assertEqual(context['object_list'][0], q)\n    self.assertIn('page_obj', context)\n    self.assertEqual(context['page_obj'].number, 1)\n", "def test_to_field_allowed(self):\n    \"\"\"\n    Test that AutocompleteJsonView respects to_field_allowed() defined in the\n    target model admin.\n    \"\"\"\n    class RestrictedQuestionAdmin(QuestionAdmin):\n            return to_field != 'question_with_to_field'\n\n    with model_admin(Question, RestrictedQuestionAdmin):\n        q = Question.objects.create(question='Is this a question?')\n        opts = {\n            'app_label': Answer._meta.app_label,\n            'model_name': Answer._meta.model_name,\n            'field_name': 'question_with_to_field',\n        }\n        request = self.factory.get(self.url, {'term': 'is', **opts})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_pagination_params(self):\n    \"\"\"Test pagination parameters.\"\"\"\n    Question.objects.bulk_create(Question(question=str(i)) for i in range(PAGINATOR_SIZE + 10))\n    request = self.factory.get(self.url, {'term': '', 'page': '2', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data['results'], [{'id': str(q.pk), 'text': q.question} for q in Question.objects.all()[PAGINATOR_SIZE:]])\n    self.assertEqual(data['pagination'], {'more': False})\n\n    request = self.factory.get(self.url, {'term': '', 'page': 'invalid', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data['results'], [{'id': str(q.pk), 'text': q.question} for q in Question.objects.all()[:PAGINATOR_SIZE]])\n    self.assertEqual(data['pagination'], {'more': True})\n"], "sample_282": ["def test_boundfield_initial(self):\n    form = ComplexFieldForm(initial={'field1': 'some text,JP,2007-04-25 06:24:00'})\n    bound_field = form['field1']\n    self.assertEqual(bound_field.initial, 'some text,JP,2007-04-25 06:24:00')\n    self.assertEqual(bound_field.value(), 'some text,JP,2007-04-25 06:24:00')\n", "def test_boundfield_label_tag(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    self.assertHTMLEqual(\n        bound_field.label_tag(),\n        '<label for=\"id_field1_0\">Field1:</label>',\n    )\n    self.assertHTMLEqual(\n        bound_field.label_tag(contents='Custom label'),\n        '<label for=\"id_field1_0\">Custom label</label>',\n    )\n    self.assertHTMLEqual(\n        bound_field.label_tag(attrs={'class': 'custom-label'}),\n        '<label for=\"id_field1_0\" class=\"custom-label\">Field1:</label>',\n    )\n", "def test_boundfield_as_text(self):\n    form = ComplexFieldForm({\n        'field1_0': 'some text',\n        'field1_1': ['J', 'P'],\n        'field1_2_0': '2007-04-25',\n        'field1_2_1': '06:24:00',\n    })\n    bound_field = form['field1']\n    self.assertHTMLEqual(\n        bound_field.as_text(),\n        '<input type=\"text\" name=\"field1_0\" value=\"some text\" required id=\"id_field1_0\">',\n    )\n", "def test_label_tag(self):\n    form = ComplexFieldForm()\n    field = form.fields['field1']\n    bound_field = form['field1']\n    label_tag = bound_field.label_tag()\n    self.assertHTMLEqual(\n        label_tag,\n        '<label for=\"id_field1_0\">Field1:</label>',\n    )\n    # Test with custom attrs.\n    label_tag = bound_field.label_tag(attrs={'class': 'custom'})\n    self.assertHTMLEqual(\n        label_tag,\n        '<label class=\"custom\" for=\"id_field1_0\">Field1:</label>',\n    )\n    # Test without 'for' attribute if id is not specified.\n    widget = field.widget\n    widget.attrs['id'] = None\n    bound_field = form['field1']\n    label_tag = bound_field.label_tag()\n    self.assertEqual(label_tag, 'Field1:')\n", "def test_boundfield_widget_type(self):\n    form = ComplexFieldForm()\n    self.assertEqual(form.fields['field1'].widget_type, 'complexmultiwidget')\n    self.assertEqual(form.fields['field1'].fields[0].widget_type, 'textinput')\n    self.assertEqual(form.fields['field1'].fields[1].widget_type, 'selectmultiple')\n    self.assertEqual(form.fields['field1'].fields[2].widget_type, 'splitdatetimewidget')\n"], "sample_283": ["def test_no_dbname_or_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        (['psql', 'postgres'], {}),\n    )\n", "def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        (['psql', 'postgres'], {}),\n    )\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        (['psql', 'postgres'], {}),\n    )\n", "def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        (['psql', 'postgres'], {}),\n    )\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'PASSWORD': 'somepassword',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n"], "sample_284": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def test_max_post_process_passes(self):\n        # Reduce max_post_process_passes to simulate the error.\n        storage.staticfiles_storage.max_post_process_passes = 1\n\n        finders.get_finder.cache_clear()\n        err = StringIO()\n        with self.assertRaisesMessage(RuntimeError, 'Max post-process passes exceeded'):\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n        self.assertEqual(\"Post-processing 'All' failed!\\n\\n\", err.getvalue())\n"], "sample_285": ["def test_get_finder(self):\n    finder_path = 'django.contrib.staticfiles.finders.FileSystemFinder'\n    finder = get_finder(finder_path)\n    self.assertIsInstance(finder, BaseFinder)\n\n    # Test that the finder is cached\n    finder2 = get_finder(finder_path)\n    self.assertIs(finder, finder2)\n\n    # Test that an invalid finder path raises an error\n    with self.assertRaises(ImportError):\n        get_finder('nonexistent.Finder')\n\n    # Test that a finder that is not a subclass of BaseFinder raises an error\n    with self.assertRaisesMessage(ImproperlyConfigured, 'Finder \"int\" is not a subclass of \"BaseFinder\"'):\n        get_finder('builtins.int')\n", "def test_base_finder_find_not_implemented(self):\n    finder = BaseFinder()\n    msg = 'subclasses of BaseFinder must provide a find() method'\n    with self.assertRaisesMessage(NotImplementedError, msg):\n        finder.find('path')\n", "def test_get_finder_caching(self):\n    finder_path = 'django.contrib.staticfiles.finders.FileSystemFinder'\n    finder1 = get_finder(finder_path)\n    finder2 = get_finder(finder_path)\n    self.assertIs(finder1, finder2)\n", "def test_get_finder(self):\n    finder_path = 'django.contrib.staticfiles.finders.FileSystemFinder'\n    finder = get_finder(finder_path)\n    self.assertIsInstance(finder, BaseFinder)\n\n    # Test that get_finder raises an exception if the finder is not a subclass of BaseFinder\n    with mock.patch('django.utils.module_loading.import_string', return_value=object):\n        with self.assertRaisesMessage(ImproperlyConfigured, f'Finder \"{object}\" is not a subclass of \"{BaseFinder}\"'):\n            get_finder(finder_path)\n", "def test_get_finder(self):\n    finder_path = 'django.contrib.staticfiles.finders.FileSystemFinder'\n    finder = get_finder(finder_path)\n    self.assertIsInstance(finder, BaseFinder)\n\n    # Test that the finder is cached\n    finder2 = get_finder(finder_path)\n    self.assertIs(finder, finder2)\n\n    # Test that an invalid finder path raises an error\n    with self.assertRaises(ImportError):\n        get_finder('invalid_finder_path')\n\n    # Test that a finder that is not a subclass of BaseFinder raises an error\n    class InvalidFinder:\n        pass\n\n    with mock.patch('django.contrib.staticfiles.finders.import_string', return_value=InvalidFinder):\n        with self.assertRaisesMessage(ImproperlyConfigured, 'Finder \"InvalidFinder\" is not a subclass of \"BaseFinder\"'):\n            get_finder('InvalidFinder')\n"], "sample_286": ["    def test_get_pk_value(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        self.assertEqual(a._get_pk_val(), a.id)\n        self.assertEqual(a._get_pk_val(), a.pk)\n", "    def setUp(self):\n        self.pub_date = datetime.now()\n        Article.objects.create(headline='Article 1', pub_date=self.pub_date)\n        Article.objects.create(headline='Article 2', pub_date=self.pub_date + timedelta(days=10))\n        Article.objects.create(headline='Article 3', pub_date=self.pub_date + timedelta(days=20))\n", "    def test_save_with_given_database(self):\n        # Create a model instance without saving it to the database.\n        article = Article(headline='Article 1', pub_date=datetime(2005, 7, 28))\n\n        # Save the instance to the 'default' database.\n        article.save(using='default')\n        self.assertEqual(article._state.db, 'default')\n\n        # Try to save the instance to the 'other' database.\n        msg = \"Cannot force both insert and updating in model saving.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            article.save(using='other', force_insert=True, force_update=True)\n\n        # Try to update the instance in the 'other' database.\n        article.save(using='other', force_update=True)\n        self.assertEqual(article._state.db, 'other')\n", "    def test_model_from_db_with_deferred_fields(self):\n        # Create an instance with deferred fields and verify that accessing the\n        # deferred field loads it from the database.\n        a = Article.objects.create(headline='Article 1', pub_date=datetime(2005, 7, 28))\n        a_deferred = Article.objects.defer('headline').get(pk=a.pk)\n        self.assertEqual(a_deferred.pub_date, a.pub_date)\n        with self.assertNumQueries(1):\n            self.assertEqual(a_deferred.headline, a.headline)\n", "    def test_repr(self):\n        a = Article(\n            id=1,\n            headline='Swallow programs in Python',\n            pub_date=datetime(2005, 7, 28),\n        )\n        self.assertEqual(repr(a), '<Article: Swallow programs in Python>')\n"], "sample_287": ["def test_list_filter_with_non_field_value(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [lambda x: x]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' must not be a callable with a __name__ attribute.\",\n            obj=SongAdmin,\n            id='admin.E113',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_autocomplete_fields_item_not_a_foreign_key_or_many_to_many_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ['title']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n            obj=SongAdmin,\n            id='admin.E038',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_works_on_through_field_with_valid_field_path(self):\n    class BookAdminWithListFilter(admin.ModelAdmin):\n        list_filter = ['authors__name']\n\n    errors = BookAdminWithListFilter(Book, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_list_filter_item_not_a_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [\"nonexistent\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent', which does not refer to a Field.\",\n            obj=SongAdmin,\n            id='admin.E116',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_with_invalid_value(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = ['nonexistent']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent', which does not refer to a Field.\",\n            obj=SongAdmin,\n            id='admin.E116',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_288": ["    def test_key_transform_expression(self):\n        obj = NullableJSONModel.objects.create(value={'a': {'b': 1}})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                key=KeyTransform('a', 'value'),\n                expr=Cast('key', models.JSONField()),\n            ).filter(expr__b=1),\n            [obj],\n        )\n", "def test_key_transform_on_expression_with_params(self):\n    expr = RawSQL(\"JSON_BUILD_OBJECT('a', %s)\", ['foo'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('a', expr)),\n        [self.objs[7]],\n    )\n", "    def test_makemigrations(self):\n        with self.settings(APP_LABEL='model_fields'):\n            # Make sure no migrations are created for JSONField changes.\n            makemigrations_call_command('model_fields', dry_run=True, verbosity=0)\n            # Introduce a change to the JSONField.\n            with open(os.path.join(TEST_APP_DIR, 'models.py'), 'a') as models_file:\n                models_file.write('\\n\\n# Changing max_length should not generate a migration.\\n')\n                models_file.write('class JSONModel(models.Model):\\n')\n                models_file.write('    value = models.JSONField(max_length=255)\\n')\n            try:\n                makemigrations_call_command('model_fields', dry_run=True, verbosity=0)\n                self.fail('Changing max_length on JSONField should not generate a migration.')\n            except SystemExit as e:\n                self.assertEqual(e.code, 0)\n", "def test_key_transform_with_complex_value(self):\n    obj = NullableJSONModel.objects.create(value={'a': [1, {'b': 'c'}]})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__1__b='c'),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__0=1),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__1__b__isnull=False),\n        [obj],\n    )\n", "def test_key_transform_raw_expression_with_params(self):\n    expr = RawSQL(self.raw_sql, ['{\"x\": \"%s\"}'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr, params=['bar'])),\n        [self.objs[7]],\n    )\n"], "sample_289": ["    def test_update_not_supported(self):\n        d = CaseInsensitiveMapping({'a': 'b'})\n        msg = \"'CaseInsensitiveMapping' object does not support item assignment\"\n        with self.assertRaisesMessage(TypeError, msg):\n            d.update({'c': 'd'})\n", "    def test_update_with_dict(self):\n        original = CaseInsensitiveMapping({\n            'Accept': 'application/json',\n            'content-type': 'text/html',\n        })\n        update = {'accept': 'application/xml', 'Content-Type': 'text/plain'}\n        original.update(update)\n        self.assertEqual(original, {\n            'accept': 'application/xml',\n            'content-type': 'text/plain',\n        })\n", "    def test_case_insensitive_update(self):\n        dict1 = CaseInsensitiveMapping({'Accept': 'application/json'})\n        dict2 = {'accept': 'application/jso'}\n        msg = \"'CaseInsensitiveMapping' object does not support item assignment\"\n        with self.assertRaisesMessage(TypeError, msg):\n            dict1.update(dict2)\n", "    def setUp(self):\n        self.dict1 = CaseInsensitiveMapping({\n            'Accept': 'application/json',\n            'content-type': 'text/html',\n        })\n", "    def test_create_with_empty_dict(self):\n        dict1 = CaseInsensitiveMapping({})\n        self.assertEqual(len(dict1), 0)\n"], "sample_290": ["def test_suggest_name_for_long_operation_names(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel('MyVeryLongModelNameThatExceeds52Chars', fields=[]),\n            migrations.CreateModel('AnotherLongModelNameThatAlsoExceeds52Chars', fields=[]),\n        ]\n\n    migration = Migration('some_migration', 'test_app')\n    suggest_name = migration.suggest_name()\n    self.assertEqual(len(suggest_name), 52)\n    self.assertTrue(suggest_name.startswith('myverylongmodelnamethatexceeds5_and_more'))\n", "def test_suggest_name_with_custom_migration_name_fragment(self):\n    class CustomMigration(migrations.Migration):\n        operations = [migrations.CreateModel('Person', fields=[])]\n\n            return 'custom_person'\n\n    migration = CustomMigration('some_migration', 'test_app')\n    self.assertEqual(migration.suggest_name(), 'custom_person')\n", "def test_suggest_name_with_multiple_create_and_delete(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel('Person1', fields=[]),\n            migrations.DeleteModel('Person2'),\n            migrations.CreateModel('Person3', fields=[]),\n            migrations.DeleteModel('Person4'),\n        ]\n\n    migration = Migration('some_migration', 'test_app')\n    self.assertEqual(\n        migration.suggest_name(),\n        'person1_delete_person2_person3_delete_person4',\n    )\n", "def test_swappable_dependency_nonexistent_setting(self):\n    \"\"\"\n    A swappable dependency should be created when the referenced setting does not exist.\n    \"\"\"\n    changes = self.get_changes([], [self.author_with_user])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"CreateModel\"])\n    self.assertMigrationDependencies(changes, 'testapp', 0, [(\"auth\", \"__first__\"), (\"__setting__\", \"AUTH_USER_MODEL\")])\n", "def test_suggest_name_unique_suffix(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel('Person', fields=[]),\n        ]\n\n    migration = Migration('0001_initial', 'test_app')\n    suggest_name = migration.suggest_name()\n    self.assertEqual(suggest_name, 'person')\n\n    # Ensure the suggest name is unique by appending a suffix\n    graph = MigrationGraph()\n    graph.add_node((\"test_app\", suggest_name), None)\n    new_suggest_name = migration.suggest_name(graph=graph)\n    self.assertEqual(new_suggest_name, 'person_2')\n"], "sample_291": ["    def test_extra_context(self):\n        view = ContextMixin()\n        view.extra_context = {'title': 'Title'}\n        context = view.get_context_data()\n        self.assertEqual(context['title'], 'Title')\n", "    def test_extra_context(self):\n        view = ContextMixin()\n        view.extra_context = {'extra_key': 'extra_value'}\n        context = view.get_context_data(test_name='test_value')\n        self.assertEqual(context['extra_key'], 'extra_value')\n        self.assertEqual(context['test_name'], 'test_value')\n", "    def test_extra_context(self):\n        class TestView(ContextMixin, View):\n            extra_context = {'extra': 'context'}\n\n                return self.render_to_response({})\n\n        view = TestView()\n        context = view.get_context_data()\n        self.assertIn('extra', context)\n        self.assertEqual(context['extra'], 'context')\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        view.extra_context = {'key': 'value'}\n        context = view.get_context_data()\n        self.assertIn('view', context)\n        self.assertEqual(context['key'], 'value')\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data()\n        self.assertIn('view', context)\n        self.assertEqual(context['view'], view)\n"], "sample_292": ["def test_allowed_origin_subdomains(self):\n    \"\"\"\n    CsrfViewMiddleware correctly handles multiple allowed origins with subdomains.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'https://foo.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n\n    req.META['HTTP_ORIGIN'] = 'http://bar.example2.com'\n    self.assertIs(mw._origin_verified(req), True)\n\n    req.META['HTTP_ORIGIN'] = 'https://baz.example3.com'\n    self.assertIs(mw._origin_verified(req), False)\n", "def test_csrf_rotate_token(self):\n    \"\"\"\n    The rotate_token function changes the CSRF token in the session and sets a\n    flag to renew the CSRF cookie.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    get_token(req)\n    initial_token = req.session.get(CSRF_SESSION_KEY)\n    rotate_token(req)\n    self.assertNotEqual(initial_token, req.session.get(CSRF_SESSION_KEY))\n    self.assertTrue(getattr(req, 'csrf_cookie_needs_reset', False))\n", "def test_https_csrf_trusted_origin_disallowed(self):\n    \"\"\"\n    A POST HTTPS request with a referer that does not match any of the \n    CSRF_TRUSTED_ORIGINS setting is rejected.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_REFERER'] = 'https://dashboard.example3.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), False)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        response = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(response.status_code, 403)\n    msg = REASON_BAD_ORIGIN % req.META['HTTP_REFERER']\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % msg)\n", "def test_origin_verification_with_http_scheme(self):\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'http://example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_wildcard_csrf_trusted_origin_domain_only(self):\n    \"\"\"\n    A wildcard in CSRF_TRUSTED_ORIGINS only matches the domain part and not the scheme.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'http://subdomain.example.net'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), False)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        response = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(response.status_code, 403)\n    msg = REASON_BAD_ORIGIN % req.META['HTTP_ORIGIN']\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % msg)\n"], "sample_293": ["    def test_locale_regex_descriptor(self):\n        descriptor = LocaleRegexDescriptor('_regex')\n        pattern = RegexPattern(r'^test/')\n        self.assertEqual(descriptor.__get__(pattern).pattern, r'^test/')\n", "    def test_resolver_match_equality(self):\n        match1 = resolve('/no_kwargs/42/37/')\n        match2 = resolve('/no_kwargs/42/37/')\n        self.assertEqual(match1, match2)\n", "    def test_warning_for_missing_pattern_name(self):\n        msg = (\n            \"Your URL pattern <URLPattern '^no-name/$' [name=None]> has a \"\n            \"route that contains '(?P<', begins with a '^', or ends with a '$'. \"\n            \"This was likely an oversight when migrating to django.urls.path().\"\n        )\n        with self.assertWarnsMessage(Warning, msg):\n            check_resolver(get_resolver('urlpatterns_reverse.urls'))\n", "    def test_locale_regex_descriptor(self):\n        resolver = get_resolver('urlpatterns_reverse.urls')\n        locale_regex_descriptor = LocaleRegexDescriptor('_regex')\n        regex_pattern = RegexPattern(r'^test/')\n        locale_regex_descriptor.__set__(regex_pattern, '_regex')\n        self.assertEqual(locale_regex_descriptor.__get__(regex_pattern), regex_pattern._regex)\n", "    def test_warning_on_invalid_pattern(self):\n        msg = (\n            \"Your URL pattern '^$' [name='invalid-pattern'] has a route \"\n            \"that contains '(?P<', begins with a '^', or ends with a '$'. \"\n            \"This was likely an oversight when migrating to \"\n            \"django.urls.path().\"\n        )\n        warnings = get_resolver(None).url_patterns[0].check()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(str(warnings[0]), msg)\n"], "sample_294": ["def test_wildcard_origin(self):\n    \"\"\"\n    A wildcard origin in CSRF_TRUSTED_ORIGINS matches all subdomains.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://subdomain.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_origin_verified_for_trusted_origins(self):\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'example.com'\n    req.META['HTTP_ORIGIN'] = 'https://example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n\n    req.META['HTTP_ORIGIN'] = 'http://subdomain.example.com'\n    self.assertIs(mw._origin_verified(req), True)\n\n    req.META['HTTP_ORIGIN'] = 'http://example.com'\n    self.assertIs(mw._origin_verified(req), False)\n", "def test_csrf_trusted_origin_subdomains(self):\n    \"\"\"\n    A POST request with an origin that matches a CSRF_TRUSTED_ORIGINS\n    subdomain is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        self.assertIs(mw._origin_verified(req), True)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "def test_csrf_token_rotate_on_login(self):\n    \"\"\"\n    The CSRF token is rotated after a successful login.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(ensure_csrf_cookie_view)\n    mw.process_view(req, ensure_csrf_cookie_view, (), {})\n    mw(req)\n    initial_token = req.session.get(CSRF_SESSION_KEY)\n\n    # Simulate a successful login\n    req.user = True\n    rotate_token(req)\n    new_token = req.session.get(CSRF_SESSION_KEY)\n\n    self.assertNotEqual(initial_token, new_token)\n", "def test_allowed_origin_subdomains_multiple_schemes(self):\n    \"\"\"\n    CsrfViewMiddleware correctly constructs allowed_origin_subdomains when\n    multiple schemes are present in CSRF_TRUSTED_ORIGINS.\n    \"\"\"\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertEqual(mw.allowed_origin_subdomains, {\n        'https': ['.example.com'],\n        'http': ['.example2.com'],\n    })\n"], "sample_295": ["    def test_empty_expression_list(self):\n        with self.assertRaisesMessage(ValueError, 'ExpressionList requires at least one expression'):\n            ExpressionList()\n", "    def test_range_frame_start_end(self):\n        frame = ValueRange(start=1, end=10)\n        start, end = frame.window_frame_range_start_end(connection, 1, 10)\n        self.assertEqual(start, 'CURRENT ROW')\n        self.assertEqual(end, 'CURRENT ROW')\n", "    def setUpTestData(cls):\n        cls.company = Company.objects.create(name='Example Inc.', num_employees=2300, num_chairs=5)\n        cls.ceo = Employee.objects.create(firstname='Joe', lastname='Smith', salary=10)\n        cls.manager1 = Employee.objects.create(firstname='John', lastname='Doe', salary=20)\n        cls.manager2 = Employee.objects.create(firstname='Jane', lastname='Doe', salary=30)\n", "    def test_repr(self):\n        self.assertEqual(repr(OuterRef('name')), \"OuterRef(name)\")\n        outer_ref = OuterRef(OuterRef('name'))\n        self.assertEqual(repr(outer_ref), \"OuterRef(OuterRef(name))\")\n", "    def test_empty_expression_list(self):\n        with self.assertRaisesMessage(ValueError, 'ExpressionList requires at least one expression'):\n            ExpressionList()\n"], "sample_296": ["def test_remove_oldest_false(self):\n    \"\"\"\n    If the data exceeds what is allowed in a cookie and remove_oldest is False,\n    newer messages are removed before saving (and returned by the ``update`` method).\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    last_msg = None\n    random.seed(42)\n    for i in range(5):\n        msg = get_random_string(msg_size)\n        storage.add(constants.INFO, msg)\n        if i == 4:\n            last_msg = msg\n    unstored_messages = storage.update(response, remove_oldest=False)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, last_msg)\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    The not_finished sentinel value is properly removed and restored when\n    retrieving and storing messages.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Add some messages to the storage\n    for i in range(5):\n        storage.add(constants.INFO, f'message {i}')\n\n    # Simulate that not all messages were retrieved\n    storage._loaded_messages.append(CookieStorage.not_finished)\n    unstored_messages = storage.update(response)\n\n    # Check that the not_finished sentinel was removed\n    self.assertNotIn(CookieStorage.not_finished, storage._loaded_messages)\n\n    # Check that the not_finished sentinel was added to the stored messages\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 5)\n    self.assertEqual(response.cookies['messages'].value[-1], CookieStorage.not_finished)\n\n    # Simulate another request\n    storage = self.storage_class(self.get_request())\n    set_cookie_data(storage, storage._decode(response.cookies['messages'].value))\n\n    # Check that the not_finished sentinel is removed when retrieving messages\n    messages, all_retrieved = storage._get()\n    self.assertFalse(all_retrieved)\n    self.assertNotIn(CookieStorage.not_finished, messages)\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    The not_finished sentinel value is properly removed and restored when\n    messages are retrieved from the cookie.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Add some messages, then update the cookie to include the not_finished\n    # sentinel value.\n    for i in range(5):\n        storage.add(constants.INFO, 'Message %s' % i)\n    unstored_messages = storage.update(response)\n    messages = storage._decode(response.cookies['messages'].value)\n    self.assertEqual(len(messages), 4)\n    self.assertEqual(messages[-1], CookieStorage.not_finished)\n\n    # Simulate a subsequent request with the same messages.\n    storage = self.get_storage()\n    set_cookie_data(storage, messages)\n\n    # When retrieving messages, the not_finished sentinel value should be\n    # removed.\n    retrieved_messages = list(storage)\n    self.assertEqual(len(retrieved_messages), 4)\n    self.assertNotEqual(retrieved_messages[-1], CookieStorage.not_finished)\n\n    # When updating the cookie again, the not_finished sentinel value should\n    # be restored if there are still messages that were not stored.\n    response = self.get_response()\n    unstored_messages = storage.update(response)\n    messages = storage._decode(response.cookies['messages'].value)\n    self.assertEqual(len(messages), 4)\n    self.assertEqual(messages[-1], CookieStorage.not_finished)\n", "def test_update_cookie(self):\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test setting the cookie with encoded data\n    messages = ['test', 'me']\n    encoded_data = storage._encode(messages)\n    storage._update_cookie(encoded_data, response)\n    self.assertEqual(response.cookies[storage.cookie_name].value, encoded_data)\n\n    # Test deleting the cookie when there's no data to store\n    storage._update_cookie(None, response)\n    self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n", "def test_update_cookie(self):\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test setting the cookie with encoded data\n    messages = [Message(constants.INFO, 'Test message')]\n    encoded_data = storage._encode(messages)\n    storage._update_cookie(encoded_data, response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name].value, encoded_data)\n\n    # Test deleting the cookie when there is no encoded data\n    storage._update_cookie(None, response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n"], "sample_298": ["def test_check_token_with_invalid_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    # Parse the token\n    ts_b36, _ = tk1.split(\"-\")\n    # Invalidate the timestamp\n    ts_b36 = 'a' * len(ts_b36)\n    invalid_tk = f\"{ts_b36}-{tk1.split('-')[1]}\"\n    self.assertIs(p0.check_token(user, invalid_tk), False)\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = self._num_seconds(datetime.now())\n    token = p0._make_token_with_timestamp(user, timestamp)\n    self.assertIsNotNone(token)\n    ts_b36, _ = token.split(\"-\")\n    self.assertEqual(base36_to_int(ts_b36), timestamp)\n", "def test_token_with_invalid_input(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n\n    # Test with invalid token format (not in base36)\n    self.assertIs(p0.check_token(user, 'invalid-token'), False)\n\n    # Test with token that's too short (less than 6 characters)\n    self.assertIs(p0.check_token(user, 'abcde'), False)\n\n    # Test with token that's too long (more than 20 characters)\n    self.assertIs(p0.check_token(user, 'a' * 21), False)\n\n    # Test with token that's not a string\n    self.assertIs(p0.check_token(user, 12345), False)\n\n    # Test with None user and valid token\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(None, tk1), False)\n\n    # Test with valid user and None token\n    self.assertIs(p0.check_token(user, None), False)\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = self._num_seconds(datetime.now())\n    token = p0._make_token_with_timestamp(user, timestamp)\n    self.assertIsNotNone(token)\n    # Check the token format: timestamp (base36) + '-' + hash value\n    self.assertRegex(token, r'^[0-9a-z]+-[0-9a-f]+$')\n    # Check that the same timestamp and user generate the same token\n    token_again = p0._make_token_with_timestamp(user, timestamp)\n    self.assertEqual(token, token_again)\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = self._num_seconds(datetime.now())\n    tk1 = p0._make_token_with_timestamp(user, timestamp)\n    self.assertRegex(tk1, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n    # Check that the token is correctly parsed and validated.\n    ts_b36, _ = tk1.split(\"-\")\n    self.assertEqual(base36_to_int(ts_b36), timestamp)\n    self.assertIs(p0.check_token(user, tk1), True)\n"], "sample_299": ["    def test_multiple_caches(self):\n        caches_configuration = {\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n            'other': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n        }\n        with self.settings(CACHES=caches_configuration):\n            self.assertEqual(check_default_cache_is_configured(None), [])\n", "    def test_empty_media_root(self):\n        with self.settings(MEDIA_ROOT='', CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache',\n            },\n        }):\n            self.assertEqual(check_cache_location_not_exposed(None), [])\n", "    def test_check_default_cache_is_configured_with_multiple_caches(self):\n        \"\"\"\n        Don't error if 'default' is present in CACHES setting with multiple caches.\n        \"\"\"\n        with self.settings(CACHES=self.VALID_CACHES_CONFIGURATION):\n            self.assertEqual(check_default_cache_is_configured(None), [])\n", "    def test_absolute_path_multiple_caches(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache1',\n            },\n            'secondary': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache2',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n", "    def test_check_default_cache_is_configured_with_multiple_caches(self):\n        \"\"\"\n        Don't error if 'default' is present in CACHES setting with multiple caches.\n        \"\"\"\n        with self.settings(CACHES=self.VALID_CACHES_CONFIGURATION):\n            self.assertEqual(check_default_cache_is_configured(None), [])\n"], "sample_300": ["def test_filter_conditional_with_expression(self):\n    query = Query(Item)\n    filter_expr = Func('name', output_field=BooleanField())\n    where = query.build_where(filter_expr)\n    exact = where.children[0]\n    self.assertIsInstance(exact, Exact)\n    self.assertIsInstance(exact.lhs, Func)\n    self.assertIs(exact.rhs, True)\n    self.assertIsInstance(exact.lhs.args[0], Col)\n    self.assertEqual(exact.lhs.args[0].target, Item._meta.get_field('name'))\n", "def test_check_related_objects(self):\n    query = Query(Item)\n    with self.assertRaises(ValueError):\n        query.build_where(Q(creator=Author.objects.all()))\n    with self.assertRaises(ValueError):\n        query.build_where(Q(creator__in=[Author.objects.all()]))\n    with self.assertRaises(ValueError):\n        query.build_where(Q(creator__in=[Author()]))\n    query.build_where(Q(creator=Author()))\n", "def test_filterable(self):\n    query = Query(Item)\n    msg = 'Cannot filter against a non-conditional expression.'\n    with self.assertRaisesMessage(TypeError, msg):\n        query.build_where(Lower('name'))\n", "def test_add_fields(self):\n    query = Query(Author)\n    query.add_fields(['name', 'num'])\n    self.assertEqual(len(query.select), 2)\n    for col in query.select:\n        self.assertIsInstance(col, Col)\n    self.assertEqual(query.select[0].target, Author._meta.get_field('name'))\n    self.assertEqual(query.select[1].target, Author._meta.get_field('num'))\n", "def test_check_related_objects(self):\n    query = Query(Item)\n    with self.assertRaises(FieldError):\n        query.check_related_objects(Author._meta.get_field('num'), 'foo', Author._meta)\n"], "sample_301": ["    def test_does_nothing_if_termios_is_not_available(self, mocked_termios):\n        mocked_termios.tcgetattr.side_effect = AttributeError()\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_non_tty(self, mocked_isatty):\n        # No exception.\n        autoreload.ensure_echo_on()\n", "    def test_ensure_echo_on_no_termios(self, mocked_termios):\n        mocked_termios.tcgetattr.side_effect = Exception()\n        # Should not raise an exception\n        autoreload.ensure_echo_on()\n", "    def test_ensure_echo_on_with_termios(self, mocked_termios):\n        # Mocking the termios module to simulate Unix-like system\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n"], "sample_302": ["def test_settings_to_cmd_args_env_postgres(self):\n    settings_dict = {\n        'NAME': 'mydatabase',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'OPTIONS': {\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/root/cert',\n            'sslcert': '/path/to/cert',\n            'sslkey': '/path/to/key',\n        }\n    }\n    parameters = ['--extra-param']\n    client = DatabaseClient(connection=connection)\n    args, env = client.settings_to_cmd_args_env(settings_dict, parameters)\n\n    self.assertEqual(args, [\n        'psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydatabase',\n        '--extra-param'\n    ])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/cert',\n        'PGSSLKEY': '/path/to/key',\n    })\n", "    def setUp(self):\n        self.client = DatabaseClient(connection=connection)\n", "def test_database_client_settings_to_cmd_args_env(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        },\n    }\n    parameters = ['--extra-param']\n    args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n\n    expected_args = [\n        'psql',\n        '-U', 'myuser',\n        '-h', 'localhost',\n        '-p', '5432',\n        'mydb',\n        '--extra-param',\n    ]\n    expected_env = {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'myservice',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/sslrootcert',\n        'PGSSLCERT': '/path/to/sslcert',\n        'PGSSLKEY': '/path/to/sslkey',\n        'PGPASSFILE': '/path/to/passfile',\n    }\n\n    self.assertEqual(args, expected_args)\n    self.assertEqual(env, expected_env)\n", "def test_database_client_settings_to_cmd_args_env(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        },\n    }\n    parameters = ['--extra-param']\n    args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n\n    expected_args = [\n        DatabaseClient.executable_name,\n        '-U', settings_dict['USER'],\n        '-h', settings_dict['HOST'],\n        '-p', str(settings_dict['PORT']),\n        settings_dict['NAME'],\n        '--extra-param',\n    ]\n    self.assertEqual(args, expected_args)\n\n    expected_env = {\n        'PGPASSWORD': settings_dict['PASSWORD'],\n        'PGSERVICE': settings_dict['OPTIONS']['service'],\n        'PGSSLMODE': settings_dict['OPTIONS']['sslmode'],\n        'PGSSLROOTCERT': settings_dict['OPTIONS']['sslrootcert'],\n        'PGSSLCERT': settings_dict['OPTIONS']['sslcert'],\n        'PGSSLKEY': settings_dict['OPTIONS']['sslkey'],\n        'PGPASSFILE': settings_dict['OPTIONS']['passfile'],\n    }\n    self.assertEqual(env, expected_env)\n", "def test_settings_to_cmd_args_env_postgres(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        },\n    }\n    parameters = ['--extra-param']\n    client = DatabaseClient(connection=connection)\n\n    args, env = client.settings_to_cmd_args_env(settings_dict, parameters)\n\n    expected_args = [\n        'psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb', '--extra-param'\n    ]\n    expected_env = {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'myservice',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/sslrootcert',\n        'PGSSLCERT': '/path/to/sslcert',\n        'PGSSLKEY': '/path/to/sslkey',\n        'PGPASSFILE': '/path/to/passfile',\n    }\n\n    self.assertEqual(args, expected_args)\n    self.assertEqual(env, expected_env)\n"], "sample_303": ["def test_runshell_with_env(self):\n    env = {'TEST_VAR': 'test_value'}\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=([], env),\n        ):\n            self.client.runshell(None)\n        run.assert_called_once_with([], env={**os.environ, **env}, check=True)\n", "def test_runshell_with_env(self):\n    env = {'TEST_VAR': 'test_value'}\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=([], env),\n        ):\n            self.client.runshell(None)\n        run.assert_called_once_with([], env={'TEST_VAR': 'test_value'}, check=True)\n", "def test_runshell_with_env(self):\n    env = {'TEST_VAR': 'test_value'}\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=([], env),\n        ):\n            self.client.runshell(None)\n        run.assert_called_once_with([], env={**os.environ, **env}, check=True)\n", "def test_runshell_with_env(self):\n    env = {'TEST_VAR': 'test_value'}\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=([], env),\n        ):\n            self.client.runshell(None)\n        run.assert_called_once_with([], env={'TEST_VAR': 'test_value', **os.environ}, check=True)\n", "def test_runshell_with_env(self):\n    env = {'TEST_VAR': 'test_value'}\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=([], env),\n        ):\n            self.client.runshell(None)\n        run.assert_called_once_with([], env={**os.environ, **env}, check=True)\n"], "sample_304": ["    def test_domain_allowlist(self):\n        validator = EmailValidator(allowlist=['example.com'])\n        self.assertIsNone(validator('email@example.com'))\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid email address.'):\n            validator('email@otherdomain.com')\n", "    def test_repr(self):\n        validator = MaxValueValidator(10)\n        self.assertEqual(repr(validator), 'MaxValueValidator(10)')\n", "    def test_repr(self):\n        self.assertEqual(\n            repr(RegexValidator(r'^(?:[a-z0-9\\.\\-]*)://')),\n            \"RegexValidator(regex='^(?:[a-z0-9\\\\.\\\\-]*)://', message='Enter a valid value.', code='invalid', inverse_match=False, flags=0)\"\n        )\n        self.assertEqual(\n            repr(MaxValueValidator(10)),\n            \"MaxValueValidator(limit_value=10, message='Ensure this value is less than or equal to %(limit_value)s.')\"\n        )\n        self.assertEqual(\n            repr(MinValueValidator(5)),\n            \"MinValueValidator(limit_value=5, message='Ensure this value is greater than or equal to %(limit_value)s.')\"\n        )\n        self.assertEqual(\n            repr(MaxLengthValidator(100)),\n            \"MaxLengthValidator(limit_value=100, message='Ensure this value has at most %(limit_value)d character (it has %(show_value)d).')\"\n        )\n        self.assertEqual(\n            repr(MinLengthValidator(3)),\n            \"MinLengthValidator(limit_value=3, message='Ensure this value has at least %(limit_value)d character (it has %(show_value)d).')\"\n        )\n        self.assertEqual(\n            repr(EmailValidator()),\n            \"EmailValidator(message='Enter a valid email address.', code='invalid', allowlist=['localhost'])\"\n        )\n        self.assertEqual(\n            repr(FileExtensionValidator(['txt'])),\n            \"FileExtensionValidator(allowed_extensions=['txt'], message='File extension \u201c%(extension)s\u201d is not allowed. Allowed extensions are: %(allowed_extensions)s.')\"\n        )\n", "    def test_custom_schemes(self):\n        validator = URLValidator(schemes=['http', 'https', 'custom'])\n        self.assertIsNone(validator('custom://example.com'))\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid URL.'):\n            validator('ftp://example.com')\n", "    def test_url_validator_with_custom_schemes(self):\n        validator = URLValidator(schemes=['http', 'https', 'custom'])\n        self.assertIsNone(validator('http://example.com'))\n        self.assertIsNone(validator('https://example.com'))\n        self.assertIsNone(validator('custom://example.com'))\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid URL.'):\n            validator('ftp://example.com')\n"], "sample_305": ["    def test_lookup_can_use_none_as_rhs(self):\n        # Test that lookups can use None as the rhs value.\n        self.assertEqual(Book.objects.filter(contact=None).count(), 0)\n        self.assertEqual(Book.objects.filter(contact__isnull=True).count(), 0)\n\n        # Test that lookups can't use None as the lhs value.\n        msg = \"Cannot use None as a lookup value\"\n        with self.assertRaisesMessage(ValueError, msg):\n            Book.objects.filter(None='Adrian Holovaty')\n", "    def test_exact_lookup_type(self):\n        # Ensure that exact lookup type is preserved. Refs #17430.\n        self.assertIsInstance(Book.objects.filter(isbn__exact='159059725').query.where.children[0], Exact)\n", "    def test_stddev(self):\n        self.assertEqual(\n            Book.objects.aggregate(StdDev('pages', sample=False)),\n            {'pages__stddev': Approximate(311.46, 1)}\n        )\n\n        self.assertEqual(\n            Book.objects.aggregate(StdDev('rating', sample=False)),\n            {'rating__stddev': Approximate(0.60, 1)}\n        )\n\n        self.assertEqual(\n            Book.objects.aggregate(StdDev('price', sample=False)),\n            {'price__stddev': Approximate(Decimal('24.16'), 2)}\n        )\n", "    def setUpTestData(cls):\n        cls.b1 = Book.objects.create(\n            isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n            pages=447, rating=4.5, price=Decimal('30.00'), pubdate=datetime.date(2007, 12, 6)\n        )\n", "def test_aggregate_subquery_in_filter(self):\n    subquery = Book.objects.filter(rating__gt=3).values('publisher').annotate(c=Count('id')).values('c')\n    publishers = Publisher.objects.annotate(num_books=Count('book')).filter(num_books__in=subquery)\n    self.assertEqual(publishers.count(), 2)\n"], "sample_306": ["def test_parse_duration_invalid_inputs(self):\n    invalid_inputs = (\n        'abc',\n        '123abc',\n        '1 day 2',\n        '1:2:3:4',\n        'P',\n        'PT',\n        'P1',\n        'PT1',\n        'P1D2',\n        'PT1H2',\n        'P1DT',\n        'PT1H2M3',\n        '-P',\n        '-PT',\n        '-P1',\n        '-PT1',\n        '-P1D2',\n        '-PT1H2',\n        '-P1DT',\n        '-PT1H2M3',\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "def test_parse_duration_invalid(self):\n    invalid_inputs = (\n        'abc',\n        '123abc',\n        '1 day 2',\n        '1:2:3:',\n        '1:2:3.4.5',\n        '-P',\n        'PT',\n        'P1',\n        'PT1',\n        'P1D2',\n        'PT1H2',\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "def test_parse_duration_edge_cases(self):\n    # Test parsing of edge cases, such as very large or very small durations\n    test_values = (\n        ('36525 23:59:59.999999', timedelta(days=36525, hours=23, minutes=59, seconds=59, microseconds=999999)),\n        ('-36525 23:59:59.999999', timedelta(days=-36525, hours=-23, minutes=-59, seconds=-59, microseconds=-999999)),\n        ('0.000001', timedelta(microseconds=1)),\n        ('-0.000001', timedelta(microseconds=-1)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_datetime_invalid_tzinfo(self):\n    # Invalid tzinfo inputs\n    self.assertIsNone(parse_datetime('2012-04-23T09:15:00X'))\n    self.assertIsNone(parse_datetime('2012-04-23T09:15:00+1234'))\n    self.assertIsNone(parse_datetime('2012-04-23T09:15:00-1234'))\n    self.assertIsNone(parse_datetime('2012-04-23T09:15:00+99'))\n    self.assertIsNone(parse_datetime('2012-04-23T09:15:00-99'))\n", "def test_parse_duration_invalid_input(self):\n    # Test that invalid inputs return None or raise a ValueError\n    test_values = (\n        ('', None),\n        ('abc', None),\n        ('123abc', None),\n        ('-P', None),\n        ('PT', None),\n        ('P1D2H', None),  # invalid format\n        ('PT1M2S', None),  # invalid format\n        ('P1DT', None),  # invalid format\n        ('PT1H2M3', None),  # invalid format\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            try:\n                result = parse_duration(source)\n                self.assertEqual(result, expected)\n            except ValueError as e:\n                if expected is not None:\n                    self.fail(f'Unexpected ValueError: {e}')\n"], "sample_307": ["def test_week_number(self):\n    # Test week number at the start of the year.\n    self.assertEqual(dateformat.format(datetime(2019, 1, 1), 'W'), '1')\n\n    # Test week number in the middle of the year.\n    self.assertEqual(dateformat.format(datetime(2019, 6, 15), 'W'), '24')\n\n    # Test week number at the end of the year.\n    self.assertEqual(dateformat.format(datetime(2019, 12, 31), 'W'), '52')\n", "def test_E_format(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, 'E'), 'July')\n", "def test_N_format(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    self.assertEqual(dateformat.format(my_birthday, 'N'), 'July')\n", "def test_S_format(self):\n    tests = [\n        (1, 'st'),\n        (2, 'nd'),\n        (3, 'rd'),\n        (4, 'th'),\n        (11, 'th'),\n        (12, 'th'),\n        (13, 'th'),\n        (21, 'st'),\n        (22, 'nd'),\n        (23, 'rd'),\n        (24, 'th'),\n    ]\n    for day, expected in tests:\n        with self.subTest(day=day):\n            self.assertEqual(\n                dateformat.format(datetime(2000, 1, day), 'S'),\n                expected,\n            )\n", "def test_M_format(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, 'M'), 'Jul')\n"], "sample_308": ["def test_f_format(self):\n    tests = [\n        (datetime(2000, 1, 1, 0, 0), '12'),\n        (datetime(2000, 1, 1, 0, 30), '12:30'),\n        (datetime(2000, 1, 1, 12, 0), '12'),\n        (datetime(2000, 1, 1, 12, 30), '12:30'),\n        (datetime(2000, 1, 1, 23, 59), '11:59'),\n    ]\n    for dt, expected in tests:\n        with self.subTest(dt=dt):\n            self.assertEqual(dateformat.format(dt, 'f'), expected)\n", "def test_f_format(self):\n    tests = [\n        (datetime(2000, 1, 1, 0, 0), '12'),\n        (datetime(2000, 1, 1, 0, 30), '12:30'),\n        (datetime(2000, 1, 1, 12, 0), '12'),\n        (datetime(2000, 1, 1, 12, 30), '12:30'),\n        (datetime(2000, 1, 1, 13, 0), '1'),\n        (datetime(2000, 1, 1, 23, 59), '11:59'),\n    ]\n    for dt, expected in tests:\n        with self.subTest(dt=dt):\n            self.assertEqual(dateformat.format(dt, 'f'), expected)\n", "def test_z_format(self):\n    # Test 'z' format specifier which returns the day of the year.\n    tests = [\n        (date(2022, 1, 1), '001'),\n        (date(2022, 12, 31), '365'),\n        (date(2020, 12, 31), '366'),  # Leap year\n    ]\n    for date_obj, expected in tests:\n        with self.subTest(date_obj=date_obj):\n            self.assertEqual(dateformat.format(date_obj, 'z'), expected)\n", "def test_Z_format_timezone_offset(self):\n    # Test that the 'Z' format specifier returns the correct timezone offset\n    dt = datetime(2022, 1, 1, tzinfo=get_fixed_timezone(-300))\n    self.assertEqual(dateformat.format(dt, 'Z'), '-18000')\n", "def test_f_format(self):\n    tests = [\n        (0, 0, '12'),\n        (0, 30, '12:30'),\n        (1, 0, '1'),\n        (1, 30, '1:30'),\n        (11, 0, '11'),\n        (11, 30, '11:30'),\n        (12, 0, '12'),\n        (12, 30, '12:30'),\n        (13, 0, '1'),\n        (13, 30, '1:30'),\n        (23, 0, '11'),\n        (23, 30, '11:30'),\n    ]\n    for hour, minute, expected in tests:\n        with self.subTest(hour=hour, minute=minute):\n            self.assertEqual(\n                dateformat.format(datetime(2000, 1, 1, hour, minute), 'f'),\n                expected,\n            )\n"], "sample_309": ["    def test_parse_url(self):\n        url = 'https://example.com/path?query=value#fragment'\n        parsed_url = _urlparse(url)\n        self.assertEqual(parsed_url.scheme, 'https')\n        self.assertEqual(parsed_url.netloc, 'example.com')\n        self.assertEqual(parsed_url.path, '/path')\n        self.assertEqual(parsed_url.query, 'query=value')\n        self.assertEqual(parsed_url.fragment, 'fragment')\n", "    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date_str), parse_http_date(date_str))\n", "    def test_parse_http_date_safe(self):\n        self.assertIsNone(parse_http_date_safe('invalid date'))\n        self.assertEqual(\n            parse_http_date_safe('Sun, 06 Nov 1994 08:49:37 GMT'),\n            parse_http_date('Sun, 06 Nov 1994 08:49:37 GMT')\n        )\n", "    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date_str), parse_http_date(date_str))\n", "    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(parse_http_date_safe(date_str), parse_http_date(date_str))\n"], "sample_310": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_get_view_name(self):\n        view = 'django.contrib.admin.sites.AdminSite.index'\n        self.assertEqual(get_view_name(view), 'django.contrib.admin.sites.AdminSite.index')\n", "def test_get_return_data_type(self):\n    tests = (\n        ('get_status_count', 'Integer'),\n        ('get_groups_list', 'List'),\n        ('some_other_method', ''),\n    )\n    for method_name, expected_output in tests:\n        with self.subTest(method_name=method_name):\n            self.assertEqual(get_return_data_type(method_name), expected_output)\n", "    def test_function_view(self):\n            pass\n\n        self.assertEqual(get_view_name(view_function), 'view_function')\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_312": ["def test_add_squash(self):\n    node = Node([('a', 1), ('b', 2)], 'OR')\n    node_to_add = Node([('c', 3), ('d', 4)], 'OR')\n    node.add(node_to_add, 'OR')\n    self.assertEqual(len(node), 4)\n    self.assertEqual(str(node), \"(OR: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "def test_add_node_squash(self):\n    node1 = Node([('a', 1), ('b', 2)])\n    node2 = Node([('c', 3), ('d', 4)])\n    node1.add(node2, Node.default)\n    self.assertEqual(len(node1), 4)\n    self.assertEqual(str(node1), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "def test_add_squash_child(self):\n    node1 = Node([('a', 1), ('b', 2)])\n    node2 = Node([('c', 3), ('d', 4)], connector='DEFAULT')\n    node1.add(node2, 'DEFAULT')\n    self.assertEqual(len(node1), 4)\n    self.assertEqual(str(node1), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n    self.assertEqual(node1.children, [('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n", "def test_add_squash_child(self):\n    node = Node([('a', 1), ('b', 2)])\n    child_node = Node([('c', 3), ('d', 4)], connector='DEFAULT')\n    node.add(child_node, 'DEFAULT')\n    self.assertEqual(len(node), 4)\n    self.assertEqual(str(node), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "def test_add_squash(self):\n    # start with the same children of node1 then add an item\n    node3 = Node(self.node1_children)\n    node_to_add = Node([('c', 3), ('d', 4)], connector='DEFAULT')\n    node3.add(node_to_add, Node.default)\n    # we added exactly two items, len() should reflect that\n    self.assertEqual(len(self.node1) + 2, len(node3))\n    self.assertEqual(str(node3), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n"], "sample_313": ["    def test_reset_all_loaders_with_locmem_loader(self, mock_reset):\n        autoreload.reset_loaders()\n        self.assertEqual(mock_reset.call_count, 2)\n", "    def test_reset_all_loaders_with_cached_loader(self, mock_reset):\n        autoreload.reset_loaders()\n        self.assertEqual(mock_reset.call_count, 2)\n", "    def test_template_changed_with_custom_loader(self, mock_reset):\n        template_path = Path(__file__).parent / 'templates' / 'index.html'\n        self.assertTrue(autoreload.template_changed(None, template_path))\n        mock_reset.assert_called_once()\n", "def test_get_template_directories_with_django_path(self, mock_is_django_path):\n    mock_is_django_path.return_value = True\n    with override_settings(\n        TEMPLATES=[{\n            'DIRS': [str(ROOT) + '/absolute_str'],\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        }]\n    ):\n        self.assertSetEqual(autoreload.get_template_directories(), set())\n    mock_is_django_path.assert_called_once()\n", "def test_template_changed_subdirectory(self, mock_reset):\n    template_path = Path(__file__).parent / 'templates' / 'subdir' / 'index.html'\n    self.assertTrue(autoreload.template_changed(None, template_path))\n    mock_reset.assert_called_once()\n"], "sample_314": ["    def test_get_users(self):\n        \"\"\"PasswordResetForm's get_users returns an iterator.\"\"\"\n        user = User.objects.create_user('testuser', 'test@example.com', 'test')\n        form = PasswordResetForm({'email': 'test@example.com'})\n        self.assertTrue(form.is_valid())\n        users = form.get_users('test@example.com')\n        self.assertIsInstance(users, (list, tuple))\n        self.assertEqual(len(users), 1)\n        self.assertEqual(users[0].email, 'test@example.com')\n", "    def test_get_context(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5udm0='\n        context = widget.get_context(name='password', value=value, attrs={})\n        self.assertEqual(context['summary'], [\n            {'label': 'algorithm', 'value': 'pbkdf2_sha256'},\n            {'label': 'iterations', 'value': 100000},\n            {'label': 'salt', 'value': 'a6Pucb******'},\n            {'label': 'hash', 'value': 'WmCkn9**************************************'},\n        ])\n", "    def test_render_with_unknown_password_algorithm(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'unknown$algorithm'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"Invalid password format or unknown hashing algorithm.\"), html)\n", "    def test_email_with_custom_user_model(self):\n        user = User.objects.create_user('testuser', 'test@example.com', 'test')\n        form = PasswordResetForm({'email': 'test@example.com'})\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, ['test@example.com'])\n", "    def test_normalize_username(self):\n        field = UsernameField()\n        ohm_username = 'test\u2126'  # U+2126 OHM SIGN\n        omega_username = 'test\u03a9'  # U+03A9 GREEK CAPITAL LETTER OMEGA\n        self.assertNotEqual(ohm_username, omega_username)\n        self.assertEqual(field.to_python(ohm_username), omega_username)\n"], "sample_315": ["    def test_language_code_with_underscore(self):\n        # Make sure language codes with underscores are handled correctly.\n        response = self.client.get('/pt_br/conta/registre-se/')\n        self.assertEqual(response.status_code, 404)\n", "    def test_invalid_language_prefix(self):\n        response = self.client.get('/xx/account/register/')\n        self.assertEqual(response.status_code, 404)\n", "    def test_middleware_language_activation(self):\n        request = RequestFactory().get('/en/account/register/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(translation.get_language(), 'en')\n        self.assertEqual(request.LANGUAGE_CODE, 'en')\n", "    def test_middleware_with_language_prefix(self):\n        request = RequestFactory().get('/en/prefixed/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        response = middleware.process_request(request)\n        self.assertIsNone(response)  # No redirect expected\n", "    def test_404_response(self):\n        request = RequestFactory().get('/non-existent-url/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse(status=404))\n        response = middleware.process_response(request, HttpResponse(status=404))\n        self.assertEqual(response.status_code, 404)\n        self.assertEqual(response.headers['content-language'], settings.LANGUAGE_CODE)\n"], "sample_316": ["    def test_image_file_properties(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            self.assertEqual(image.width, Image.open(fh).size[0])\n            self.assertEqual(image.height, Image.open(fh).size[1])\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n", "    def test_image_file_properties(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n", "    def test_image_file_properties(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n"], "sample_317": ["def test_enclosure_length(self):\n    \"\"\"\n    Test that the enclosure length is correctly output as a string.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/single-enclosure/')\n    doc = minidom.parseString(response.content)\n    chan = doc.getElementsByTagName('rss')[0].getElementsByTagName('channel')[0]\n    items = chan.getElementsByTagName('item')\n    for item in items:\n        enclosures = item.getElementsByTagName('enclosure')\n        self.assertEqual(len(enclosures), 1)\n        enclosure = enclosures[0]\n        self.assertIsInstance(enclosure.getAttribute('length'), str)\n", "def test_rss2_feed_with_no_items(self):\n    \"\"\"\n    Test the structure and content of feeds generated by Rss201rev2Feed when there are no items.\n    \"\"\"\n    Entry.objects.all().delete()\n    response = self.client.get('/syndication/rss2/')\n    doc = minidom.parseString(response.content)\n\n    # Making sure there's only 1 `rss` element and that the correct\n    # RSS version was specified.\n    feed_elem = doc.getElementsByTagName('rss')\n    self.assertEqual(len(feed_elem), 1)\n    feed = feed_elem[0]\n    self.assertEqual(feed.getAttribute('version'), '2.0')\n\n    # Making sure there's only one `channel` element w/in the\n    # `rss` element.\n    chan_elem = feed.getElementsByTagName('channel')\n    self.assertEqual(len(chan_elem), 1)\n    chan = chan_elem[0]\n\n    # Ensure the content of the channel is correct\n    self.assertChildNodeContent(chan, {\n        'title': 'My blog',\n        'link': 'http://example.com/blog/',\n        'description': 'A more thorough description of my blog.',\n    })\n    self.assertCategories(chan, ['python', 'django'])\n\n    # Check that there are no items\n    items = chan.getElementsByTagName('item')\n    self.assertEqual(len(items), 0)\n", "def test_rfc2822_date(self):\n    \"\"\"\n    Test that rfc2822_date returns a correctly formatted date string.\n    \"\"\"\n    dt = datetime.datetime(2022, 1, 1, 12, 0, 0)\n    expected_output = \"Sat, 01 Jan 2022 12:00:00 +0000\"\n    self.assertEqual(rfc2822_date(dt), expected_output)\n", "def test_rss2_feed_with_empty_description(self):\n    \"\"\"\n    Test that RSS 2 feed with empty description does not include the description element.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/empty-description/')\n    doc = minidom.parseString(response.content)\n    chan = doc.getElementsByTagName('rss')[0].getElementsByTagName('channel')[0]\n    items = chan.getElementsByTagName('item')\n    for item in items:\n        self.assertFalse(item.getElementsByTagName('description'))\n", "def test_rss2_feed_with_no_items(self):\n    \"\"\"\n    Test the structure and content of feeds generated by Rss201rev2Feed with no items.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/no-items/')\n    doc = minidom.parseString(response.content)\n\n    # Making sure there's only 1 `rss` element and that the correct\n    # RSS version was specified.\n    feed_elem = doc.getElementsByTagName('rss')\n    self.assertEqual(len(feed_elem), 1)\n    feed = feed_elem[0]\n    self.assertEqual(feed.getAttribute('version'), '2.0')\n\n    # Making sure there's only one `channel` element w/in the\n    # `rss` element.\n    chan_elem = feed.getElementsByTagName('channel')\n    self.assertEqual(len(chan_elem), 1)\n    chan = chan_elem[0]\n\n    self.assertChildNodes(\n        chan, [\n            'title', 'link', 'description', 'language', 'lastBuildDate',\n            'atom:link', 'ttl', 'copyright', 'category',\n        ]\n    )\n    self.assertChildNodeContent(chan, {\n        'title': 'My blog',\n        'description': 'A more thorough description of my blog.',\n        'link': 'http://example.com/blog/',\n        'language': 'en',\n        'lastBuildDate': rfc2822_date(timezone.make_aware(datetime.datetime.now(), TZ)),\n        'ttl': '600',\n        'copyright': 'Copyright (c) 2007, Sally Smith',\n    })\n    self.assertCategories(chan, ['python', 'django'])\n\n    # Ensure there are no items in the feed\n    items = chan.getElementsByTagName('item')\n    self.assertEqual(len(items), 0)\n"], "sample_318": ["    def test_resolver404_repr(self):\n        exception = Resolver404({'tried': [['test']],'path': 'path'})\n        self.assertEqual(\n            repr(exception),\n            \"Resolver404({'tried': [['test']], 'path': 'path'})\"\n        )\n", "    def test_pattern_repr(self):\n        pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test-view')\n        self.assertEqual(\n            repr(pattern),\n            '<URLPattern test-view>',\n        )\n", "    def test_get_ns_resolver(self):\n        resolver = get_resolver('urlpatterns_reverse.namespace_urls')\n        ns_resolver = get_ns_resolver('test-ns1', resolver, {})\n        self.assertIsInstance(ns_resolver, URLResolver)\n        self.assertEqual(ns_resolver.namespace, 'test-ns1')\n", "    def test_url_pattern_check(self):\n        resolver = get_resolver()\n        checks = resolver.check()\n        self.assertEqual(len(checks), 0)\n", "    def test_non_empty_prefix_default_language(self):\n        pattern = LocalePrefixPattern(prefix_default_language=True)\n        self.assertEqual(pattern.language_prefix, 'en/')\n"], "sample_321": ["def test_csrf_token_rotation(self):\n    \"\"\"\n    The token is rotated after it's used in a POST request.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    mw.process_view(req, post_form_view, (), {})\n    resp = mw(req)\n    self.assertIn(settings.CSRF_COOKIE_NAME, resp.cookies)\n    new_cookie = resp.cookies[settings.CSRF_COOKIE_NAME]\n    self.assertNotEqual(new_cookie.value, self._csrf_id_cookie)\n    self.assertEqual(len(new_cookie.value), CSRF_TOKEN_LENGTH)\n", "def test_csrf_trusted_origins_wildcard_subdomains(self):\n    \"\"\"\n    A POST request with an origin that matches a wildcard in a subdomain of a \n    trusted origin is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://sub.foo.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.foo.example.com']):\n        self.assertIs(mw._origin_verified(req), True)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n", "def test_csrf_exempt_decorator_with_method_array(self):\n    \"\"\"\n    The csrf_exempt decorator should work with a view function that has the\n    'csrf_exempt' attribute set to an array of method names.\n    \"\"\"\n    @csrf_exempt(['POST'])\n        return HttpResponse()\n\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(view)\n    mw.process_request(req)\n    resp = mw.process_view(req, view, (), {})\n    self.assertIsNone(resp)\n", "def test_wildcard_origin_with_port(self):\n    \"\"\"\n    A POST request with an origin that matches a wildcard in\n    CSRF_TRUSTED_ORIGINS and has a port is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://foo.example.com:8080'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n    self.assertEqual(mw.allowed_origins_exact, set())\n    self.assertEqual(mw.allowed_origin_subdomains, {'https': ['.example.com']})\n", "def test_allowed_origin_subdomains_parsed(self):\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertEqual(mw.allowed_origin_subdomains, {\n        'https': ['.example.com'],\n        'http': ['example.net'],\n    })\n"], "sample_322": ["def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Tests that replaced migrations are correctly handled in the migration plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Load the initial migration\n    executor.loader.build_graph()\n    # Replace a migration\n    executor.loader.replace_migrations = True\n    # Rebuild the graph with the replaced migration\n    executor.loader.build_graph()\n    # Create a plan to migrate to the second migration\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Check that the replaced migration is not in the plan\n    self.assertNotIn((\"migrations\", \"0001_replaced\"), [n[0] for n in plan])\n", "def test_detect_soft_applied_with_through_model(self):\n    \"\"\"\n    Test detect_soft_applied() with a ManyToManyField using a through model.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    migration = executor.loader.get_migration(\"migrations\", \"0002_second\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n\n    # Create the tables for the migration\n    with connection.schema_editor() as editor:\n        editor.create_model(migration.operations[0].model)\n        editor.create_model(migration.operations[1].model)\n\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n", "def test_migration_plan_with_replaced(self):\n    \"\"\"\n    Tests the migration plan with replaced migrations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Replace the second migration\n    executor.loader.replace_migrations = True\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_replaced\"], False),\n        ],\n    )\n", "def test_create_project_state_with_applied_migrations(self):\n    \"\"\"\n    Test that _create_project_state includes all the applications without\n    migrations and applied migrations if with_applied_migrations=True.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Apply some migrations first\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Create the project state with applied migrations\n    state = executor._create_project_state(with_applied_migrations=True)\n    # Check that the applied migration is included in the project state\n    self.assertIn((\"migrations\", \"author\"), state.models)\n    self.assertIn((\"migrations\", \"tribble\"), state.models)\n    # Check that an unapplied migration is not included in the project state\n    self.assertNotIn((\"migrations\", \"book\"), state.models)\n    # Unapply the migrations\n    executor.migrate([(\"migrations\", None)])\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n", "def test_migration_plan_with_missing_target(self):\n    \"\"\"\n    Tests that migration_plan raises a LookupError when a target is not found.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    with self.assertRaises(LookupError) as cm:\n        executor.migration_plan([(\"migrations\", \"9999_missing\")])\n    self.assertEqual(str(cm.exception), \"No migration '9999_missing' in app 'migrations'\")\n"], "sample_323": ["def test_custom_user_with_migrations_disabled(self):\n    \"\"\"\n    Tests that a custom user model defined in an app with migrations disabled\n    doesn't cause errors.\n    \"\"\"\n    with self.settings(MIGRATION_MODULES={\"migrations\": None}):\n        executor = MigrationExecutor(connection)\n        # Make sure the migration is applied without any issues\n        executor.migrate([(\"django.contrib.auth\", \"0001_initial\")])\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Make sure we can migrate backwards without any issues\n        executor.migrate([(\"django.contrib.auth\", None)])\n", "def test_migration_plan_with_empty_targets(self):\n    \"\"\"\n    Tests running a migration plan with empty targets.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    plan = executor.migration_plan([])\n    self.assertEqual(plan, [])\n    # Ensure no migrations are applied or unapplied.\n    self.assertEqual(executor.loader.applied_migrations, {})\n    state = executor.migrate([])\n    self.assertIsInstance(state, ProjectState)\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Migration plan includes replaced migrations when necessary.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Let's look at the plan first and make sure it's up to scratch\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Fake-apply all migrations\n    executor.migrate([\n        (\"migrations\", \"0002_second\"),\n    ], fake=True)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Now replace the second migration with a new one\n    executor.loader.graph.add_node((\"migrations\", \"0003_third\"), FakeMigration('0003_third'))\n    executor.loader.graph.add_dependency(None, (\"migrations\", \"0003_third\"), (\"migrations\", \"0002_second\"))\n    executor.loader.graph.remove_replaced_nodes()\n    # Make sure the plan still includes the replaced migration\n    plan = executor.migration_plan([(\"migrations\", \"0003_third\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0003_third\"], False),\n        ],\n    )\n", "def test_detect_soft_applied_with_swapped_models(self):\n    \"\"\"\n    Tests detection of initial migrations already having been applied when\n    there are swapped models.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Create the tables for 0001 but make it look like the migration hasn't\n    # been applied.\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    executor.migrate([(\"migrations\", None)], fake=True)\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableExists(\"migrations_tribble\")\n    # Table detection sees 0001 is applied.\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    # And migrate back to clean up the database\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", None)])\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n", "def test_check_replacements(self):\n    \"\"\"\n    Mark replacement migrations applied if their replaced set all are.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Record all replaced migrations as applied\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    # Create a new squash migration\n    migration = executor.loader.get_migration(\"migrations\", \"0001_squashed_0002\")\n    # Check replacements before migrate\n    self.assertNotIn(\n        (\"migrations\", \"0001_squashed_0002\"),\n        recorder.applied_migrations(),\n    )\n    executor.check_replacements()\n    # After checking replacements, the squash migration should be marked as applied\n    self.assertIn(\n        (\"migrations\", \"0001_squashed_0002\"),\n        recorder.applied_migrations(),\n    )\n"], "sample_324": ["def test_origin_verification_lowercases_hosts(self):\n    \"\"\"\n    Origin verification lowercases hosts before comparing them.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'Www.Example.Com'\n    req.META['HTTP_ORIGIN'] = 'https://www.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n", "def test_csrf_origin_header_set(self):\n    \"\"\"\n    The 'Origin' header is checked when it is present in the request.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'https://example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n", "def test_csrf_trusted_origin_matches_subdomain(self):\n    \"\"\"\n    A POST request with a CSRF trusted origin that matches a subdomain is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://subdomain.dashboard.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n    self.assertEqual(mw.allowed_origins_exact, {'https://dashboard.example.com'})\n    self.assertEqual(mw.allowed_origin_subdomains, {})\n", "def test_csrf_trusted_origins_wildcard_domain(self):\n    \"\"\"\n    A POST request with an origin that matches a wildcard domain in \n    CSRF_TRUSTED_ORIGINS is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'https://subdomain.example.com'\n    with self.settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n", "def test_csrf_token_with_invalid_characters(self):\n    \"\"\"\n    If the CSRF token contains invalid characters, a new token is generated.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    req.COOKIES[settings.CSRF_COOKIE_NAME] = 'abc123!@#'\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n    self.assertIsNotNone(csrf_cookie)\n    self.assertNotEqual(csrf_cookie.value, 'abc123!@#')\n"], "sample_325": ["def test_boundfield_get_value_from_datadict(self):\n    class MyForm(Form):\n        name = CharField(max_length=10)\n\n    form = MyForm({'name': 'John'})\n    self.assertEqual(form['name'].get_value_from_datadict(form.data, {}, 'name'), 'John')\n    self.assertIsNone(form['name'].get_value_from_datadict({}, {}, 'name'))\n", "def test_boundfield_iterable(self):\n    class BeatleForm(Form):\n        name = ChoiceField(\n            choices=[('john', 'John'), ('paul', 'Paul'), ('george', 'George'), ('ringo', 'Ringo')],\n            widget=RadioSelect,\n        )\n\n    form = BeatleForm()\n    self.assertEqual(len(form['name']), 4)\n    for subwidget in form['name']:\n        self.assertIsInstance(subwidget, BoundWidget)\n", "def test_custom_renderer(self):\n    class CustomForm(Form):\n        default_renderer = CustomRenderer()\n\n    form = CustomForm()\n    self.assertIsInstance(form.renderer, CustomRenderer)\n    self.assertEqual(form.renderer, CustomForm.default_renderer)\n", "def test_add_prefix(self):\n    class MyForm(Form):\n        field1 = CharField()\n\n            return 'my_' + field_name\n\n    form = MyForm()\n    self.assertEqual(form.add_prefix('field1'), 'my_field1')\n    self.assertEqual(form['field1'].html_name, 'my_field1')\n\n    form = MyForm(prefix='my')\n    self.assertEqual(form.add_prefix('field1'), 'my_field1')\n    self.assertEqual(form['field1'].html_name, 'my_field1')\n", "def test_boundfield_str_when_disabled(self):\n    class MyForm(forms.Form):\n        field = forms.CharField(disabled=True)\n\n    form = MyForm()\n    self.assertEqual(str(form['field']), '<input type=\"text\" name=\"field\" disabled>')\n\n    form = MyForm({'field': 'value'})\n    self.assertEqual(str(form['field']), '<input type=\"text\" name=\"field\" value=\"value\" disabled>')\n"], "sample_326": ["def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('Check out www.google.com?', 'Check out <a href=\"http://www.google.com\">www.google.com</a>?'),\n        ('Check out www.google.com!', 'Check out <a href=\"http://www.google.com\">www.google.com</a>!'),\n        ('Check out www.google.com...', 'Check out <a href=\"http://www.google.com\">www.google.com</a>...'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_nofollow(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n        ),\n        (\n            lazystr('Search for google.com/?q=!'),\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!'\n        ),\n        ('foo@example.com', '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),  # nofollow not applied to emails\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Search for google.com/?q=1.', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>.'),\n        ('Search for google.com/?q=1,', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>,'),\n        ('Search for google.com/?q=1;', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>;'),\n        ('Search for google.com/?q=1:', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>:'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('Check out www.google.com!', 'Check out <a href=\"http://www.google.com\">www.google.com</a>!'),\n        ('Check out www.google.com?', 'Check out <a href=\"http://www.google.com\">www.google.com</a>?'),\n        ('Check out www.google.com...', 'Check out <a href=\"http://www.google.com\">www.google.com</a>...'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_trims_urls(self):\n    tests = (\n        (\n            'Search for google.com/?q=verylongurl&foo=bar and see.',\n            'Search for <a href=\"http://google.com/?q=verylongurl&amp;foo=bar\">google.com/?q=verylong\u2026</a> and see.'\n        ),\n        (\n            'Search for https://www.google.com/?q=verylongurl&foo=bar and see.',\n            'Search for <a href=\"https://www.google.com/?q=verylongurl&amp;foo=bar\">www.google.com/?q=verylon\u2026</a> and see.'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, trim_url_limit=20), output)\n"], "sample_327": ["def test_bound_data(self):\n    field = JSONField()\n    self.assertIsNone(field.bound_data(None, None))\n    self.assertEqual(field.bound_data('{\"a\": 1}', None), {'a': 1})\n    invalid_json_input = field.bound_data('{ invalid json }', None)\n    self.assertIsInstance(invalid_json_input, str)\n    self.assertEqual(invalid_json_input, '{ invalid json }')\n", "def test_bound_data(self):\n    field = JSONField()\n    tests = [\n        ('[\"a\", \"b\", \"c\"]', [\"a\", \"b\", \"c\"]),\n        ('{\"a\": 1, \"b\": 2}', {\"a\": 1, \"b\": 2}),\n        ('1', 1),\n        ('1.5', 1.5),\n        ('\"foo\"', \"foo\"),\n        ('true', True),\n        ('false', False),\n        ('null', None),\n    ]\n    for json_string, expected_value in tests:\n        with self.subTest(json_string=json_string):\n            self.assertEqual(field.bound_data(json_string, None), expected_value)\n\n    # Invalid JSON should return an instance of InvalidJSONInput.\n    invalid_json = '{\"foo}'\n    self.assertIsInstance(field.bound_data(invalid_json, None), field.InvalidJSONInput)\n", "def test_invalid_input(self):\n    field = JSONField()\n    tests = [\n        '{some badly formed: json}',\n        '[1, 2, 3,]',\n        '{\"key\": \"value\",}',\n        'nulll',\n        'truue',\n        'falsse',\n    ]\n    for value in tests:\n        with self.subTest(value=value):\n            with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n                field.clean(value)\n", "def test_bound_data(self):\n    field = JSONField()\n    self.assertIsNone(field.bound_data(None, None))\n    self.assertEqual(field.bound_data('{\"a\": \"b\"}', None), {\"a\": \"b\"})\n    invalid_input = '{\"foo}'\n    self.assertIsInstance(field.bound_data(invalid_input, None), InvalidJSONInput)\n    self.assertEqual(str(field.bound_data(invalid_input, None)), invalid_input)\n", "def test_bound_data(self):\n    field = JSONField()\n    tests = [\n        ('{\"a\": 1}', {'a': 1}),\n        ('\"foo\"', 'foo'),\n        ('null', None),\n        ('true', True),\n        ('false', False),\n        ('1', 1),\n        ('1.5', 1.5),\n    ]\n    for data, expected in tests:\n        with self.subTest(data=data):\n            self.assertEqual(field.bound_data(data, None), expected)\n\n    invalid_json_input = '{\"foo\"'\n    self.assertIsInstance(field.bound_data(invalid_json_input, None), InvalidJSONInput)\n    self.assertEqual(field.bound_data(invalid_json_input, None), invalid_json_input)\n"], "sample_328": ["def test_ignore_conflicts(self):\n    # Create a model with a unique constraint.\n    article = Article.objects.create(name='test', created=datetime.datetime.today())\n    # Try to bulk update a model that conflicts with the unique constraint.\n    article2 = Article(id=article.id, name='test2')\n    try:\n        with transaction.atomic():\n            Article.objects.bulk_update([article2], ['name'])\n    except IntegrityError:\n        self.fail('IntegrityError should not be raised when ignore_conflicts=True')\n    else:\n        article.refresh_from_db()\n        self.assertEqual(article.name, 'test')\n\n    Article.objects.bulk_update([article2], ['name'], ignore_conflicts=True)\n    article.refresh_from_db()\n    self.assertEqual(article.name, 'test')\n\n    # Test ignore_conflicts with multiple objects.\n    article3 = Article(id=article.id, name='test3')\n    article4 = Article(id=article.id + 1, name='test4')\n    Article.objects.bulk_update([article3, article4], ['name'], ignore_conflicts=True)\n    article.refresh_from_db()\n    self.assertEqual(article.name, 'test')\n    self.assertEqual(Article.objects.filter(id=article.id + 1).count(), 1)\n", "def test_update_with_expression(self):\n    notes = [\n        Note.objects.create(note='test-%s' % i, misc='misc-%s' % i)\n        for i in range(10)\n    ]\n    for note in notes:\n        note.note = F('note') + ' updated'\n    with self.assertNumQueries(1):\n        Note.objects.bulk_update(notes, ['note'])\n    self.assertCountEqual(\n        Note.objects.values_list('note', flat=True),\n        ['test-%s updated' % i for i in range(10)]\n    )\n", "def test_bulk_update_with_annotations(self):\n    articles = [\n        Article.objects.create(name=str(i), created=datetime.datetime.today())\n        for i in range(10)\n    ]\n    point_in_time = datetime.datetime(1991, 10, 31)\n    for article in articles:\n        article.created = point_in_time\n    Article.objects.bulk_update(articles, ['created'])\n    annotated_articles = list(Article.objects.annotate(created_lower=Lower('created')))\n    for article in annotated_articles:\n        article.created_lower = 'new value'\n    with self.assertRaisesMessage(ValueError, 'bulk_update() can only be used with concrete fields.'):\n        Article.objects.bulk_update(annotated_articles, ['created_lower'])\n", "def test_bulk_update_with_nullable_foreign_key(self):\n    tag = Tag.objects.create(name='test-tag')\n    notes = [\n        Note.objects.create(note=str(i), misc=str(i), tag=tag)\n        for i in range(5)\n    ] + [\n        Note.objects.create(note=str(i), misc=str(i))\n        for i in range(5, 10)\n    ]\n    for note in notes:\n        note.tag = None\n    with self.assertNumQueries(1):\n        Note.objects.bulk_update(notes, ['tag'])\n    self.assertCountEqual(Note.objects.filter(tag__isnull=True), notes)\n", "def test_bulk_update_with_select_for_update(self):\n    notes = [\n        Note.objects.create(note=str(i), misc=str(i))\n        for i in range(10)\n    ]\n    with transaction.atomic():\n        for note in notes:\n            note.note = 'test-%s' % note.id\n        rows_updated = Note.objects.select_for_update().bulk_update(notes, ['note'])\n        self.assertEqual(rows_updated, 10)\n    self.assertCountEqual(\n        Note.objects.values_list('note', flat=True),\n        [cat.note for cat in notes]\n    )\n"], "sample_329": ["def test_serialize_lazy_object(self):\n    lazy_object = SimpleLazyObject(lambda: \"Hello, World!\")\n    self.assertSerializedEqual(lazy_object)\n    string, imports = MigrationWriter.serialize(lazy_object)\n    self.assertEqual(string, \"'Hello, World!'\")\n    self.assertEqual(imports, set())\n", "def test_serialize_lazy_object_with_method(self):\n    class TestClass:\n            return \"Hello, World!\"\n\n    lazy_object = SimpleLazyObject(lambda: TestClass().method)\n    string, imports = MigrationWriter.serialize(lazy_object)\n    self.assertEqual(string, \"'Hello, World!'\", imports)\n", "def test_serialize_lazy_object(self):\n    lazy_obj = SimpleLazyObject(lambda: 'test_value')\n    self.assertSerializedEqual(lazy_obj)\n    string, imports = MigrationWriter.serialize(lazy_obj)\n    self.assertEqual(string, \"'test_value'\")\n    self.assertEqual(imports, set())\n", "def test_serialize_lazy_object(self):\n    lazy_obj = SimpleLazyObject(lambda: \"lazy_value\")\n    self.assertSerializedEqual(lazy_obj)\n    string, imports = MigrationWriter.serialize(lazy_obj)\n    self.assertEqual(string, \"'lazy_value'\")\n    self.assertEqual(imports, set())\n", "def test_serialize_type_with_module_name_conflict(self):\n    class models:\n        pass\n\n    self.assertSerializedEqual(models.Model)\n    self.assertSerializedResultEqual(\n        MigrationWriter.serialize(models.Model),\n        (\"('models.Model', {'from django.db import models'})\", set()),\n    )\n"], "sample_330": ["    def test_date(self):\n        s = '2022-01-01'\n        self.assertEqual(typecast_date(s), datetime.date(2022, 1, 1))\n", "    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(''))\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n", "    def test_date_typecast(self):\n        s = '2010-02-20'\n        self.assertEqual(typecast_date(s), datetime.date(2010, 2, 20))\n", "    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(None))\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n", "    def test_truncate_name_length_limits(self):\n        \"\"\"Test that truncate_name doesn't produce a string longer than the specified length.\"\"\"\n        very_long_name = 'a' * 1000\n        self.assertEqual(len(truncate_name(very_long_name, length=10)), 10)\n"], "sample_331": ["def test_invalid_input(self):\n    # Test that invalid input returns None\n    self.assertIsNone(parse_duration(''))\n    self.assertIsNone(parse_duration('abc'))\n    self.assertIsNone(parse_duration('123abc'))\n    self.assertIsNone(parse_duration('P'))\n    self.assertIsNone(parse_duration('PT'))\n    self.assertIsNone(parse_duration('P1'))\n    self.assertIsNone(parse_duration('PT1'))\n    self.assertIsNone(parse_duration('P1D2'))\n    self.assertIsNone(parse_duration('PT1H2'))\n    self.assertIsNone(parse_duration('P1DT1H2'))\n", "def test_parse_duration_invalid_inputs(self):\n    # Invalid inputs\n    self.assertIsNone(parse_duration(''))\n    self.assertIsNone(parse_duration('abc'))\n    self.assertIsNone(parse_duration('1 2 3 4'))\n    self.assertIsNone(parse_duration('1:2:3:4'))\n    self.assertIsNone(parse_duration('P'))\n    self.assertIsNone(parse_duration('PT'))\n    self.assertIsNone(parse_duration('P1'))\n    self.assertIsNone(parse_duration('PT1'))\n    self.assertIsNone(parse_duration('P1D1H'))\n    self.assertIsNone(parse_duration('PT1H1M1S1'))\n    self.assertIsNone(parse_duration('-P-1D'))\n    self.assertIsNone(parse_duration('-PT-1H'))\n    self.assertIsNone(parse_duration('-P-1D-1H'))\n", "def test_invalid_duration(self):\n    invalid_durations = [\n        'abc',  # invalid characters\n        '1 day 2',  # incomplete duration\n        '1:2:3:4',  # too many parts\n        '-P',  # incomplete ISO 8601 duration\n        'P1',  # incomplete ISO 8601 duration\n        'PT',  # incomplete ISO 8601 duration\n        'P1X',  # invalid unit in ISO 8601 duration\n        'PT1X',  # invalid unit in ISO 8601 duration\n    ]\n    for source in invalid_durations:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "def test_parse_datetime_invalid_tzinfo(self):\n    # Test invalid time zone offsets.\n    self.assertIsNone(parse_datetime('2012-04-23T10:20:30+25:00'))\n    self.assertIsNone(parse_datetime('2012-04-23T10:20:30-25:00'))\n    self.assertIsNone(parse_datetime('2012-04-23T10:20:30+99'))\n    self.assertIsNone(parse_datetime('2012-04-23T10:20:30-99'))\n    self.assertIsNone(parse_datetime('2012-04-23T10:20:30+00:60'))\n    self.assertIsNone(parse_datetime('2012-04-23T10:20:30-00:60'))\n", "def test_parse_duration_invalid_input(self):\n    # Test parse_duration function with invalid input formats\n    test_values = (\n        ('', None),\n        ('abc', None),\n        ('P', None),\n        ('PT', None),\n        ('P4', None),\n        ('P4Y5M', None),  # Mixed year and month duration not supported\n        ('-P-1D', None),  # Invalid negative sign position\n        ('P1D2H3M4S', None),  # Multiple units without 'T' separator\n        ('PT1H2M3S4D', None),  # Time units before date unit\n        ('P1DT1H2M3S4D', None),  # Multiple date units\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n"], "sample_332": ["def test_formset_with_file_field(self):\n    \"\"\"A formset with a file field can be validated.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    files = {\n        'form-0-file': SimpleUploadedFile('test.txt', b'test')\n    }\n    FileFormSet = formset_factory(FileForm)\n    formset = FileFormSet(data, files, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([form.cleaned_data for form in formset.forms], [{'file': 'test.txt'}])\n", "def test_add_fields(self):\n    \"\"\"Formsets can dynamically add fields to their forms.\"\"\"\n    class DynamicBaseFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields['extra_field'] = CharField()\n\n    DynamicFormSet = formset_factory(Choice, formset=DynamicBaseFormSet)\n    formset = DynamicFormSet()\n    for form in formset:\n        self.assertIn('extra_field', form.fields)\n", "def test_formset_add_fields(self):\n    \"\"\"A formset's add_fields method is called during form construction.\"\"\"\n    class AddFieldsFormSet(BaseFormSet):\n            form.fields['extra'] = CharField()\n\n    ChoiceFormSet = formset_factory(Choice, formset=AddFieldsFormSet)\n    formset = ChoiceFormSet()\n    self.assertIn('extra', formset.forms[0].fields)\n", "def test_formset_add_fields_with_callable(self):\n    \"\"\"Test formset's add_fields method with a callable.\"\"\"\n    class DynamicBaseFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields['custom'] = CharField()\n\n    DynamicFormSet = formset_factory(FavoriteDrinkForm, formset=DynamicBaseFormSet)\n    formset = DynamicFormSet()\n    self.assertIn('custom', formset.forms[0].fields)\n\n    # Ensure fields are added only once when formset is instantiated.\n    self.assertEqual(len(formset.forms[0].fields), 2)\n\n    # Ensure fields are not added when formset is iterated over.\n    for form in formset:\n        self.assertEqual(len(form.fields), 2)\n", "def test_formset_clean_is_called_after_forms(self):\n    \"\"\"\n    FormSet clean() is called after all forms have been cleaned.\n    \"\"\"\n    class CleanFormSet(BaseFormSet):\n            if any(form.cleaned_data.get('name') == 'Invalid' for form in self.forms):\n                raise ValidationError('Invalid name')\n\n    CleanForm = formset_factory(FavoriteDrinkForm, formset=CleanFormSet)\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n        'form-0-name': 'Gin and Tonic',\n        'form-1-name': 'Invalid',\n    }\n    formset = CleanForm(data, prefix='form')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Invalid name'])\n"], "sample_333": ["def test_media(self):\n    class CustomForm(Form):\n        class Media:\n            css = {\n                'all': ('/path/to/css.css',)\n            }\n            js = ('/path/to/js.js',)\n\n    form = CustomForm()\n    self.assertEqual(str(form.media), '<link href=\"/path/to/css.css\" type=\"text/css\" media=\"all\">'\n                                      '\\n<script src=\"/path/to/js.js\"></script>')\n", "def test_media(self):\n    class MediaForm(Form):\n        field = CharField(widget=Textarea)\n\n    f = MediaForm()\n    self.assertEqual(str(f.media), '')\n    self.assertIsInstance(f.media, Media)\n", "def test_invalid_renderer(self):\n    msg = \"The renderer must be an instance of or a class that inherits from 'BaseRenderer'.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        Form(renderer='string')\n\n    class InvalidRenderer:\n        pass\n\n    with self.assertRaisesMessage(ValueError, msg):\n        Form(renderer=InvalidRenderer())\n\n    with self.assertRaisesMessage(ValueError, msg):\n        Form(renderer=InvalidRenderer)\n", "def test_renderer_context(self):\n    class CustomForm(Form):\n            context = super().get_context()\n            context['custom'] = 'Hello, world!'\n            return context\n\n    form = CustomForm()\n    self.assertIn('custom', form.get_context())\n", "def test_fields_ordering_with_missing_fields(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n        field5 = CharField()\n\n    # Create a form with some missing fields\n    class TestFormMissing(TestForm):\n        field1 = None\n        field4 = None\n\n    # Define the ordering on the subclass\n    TestFormMissing.field_order = ['field3', 'field2', 'field5']\n\n    form = TestFormMissing()\n    self.assertEqual(list(form.fields), ['field3', 'field2', 'field5'])\n"], "sample_334": ["def test_boundfield_str(self):\n    class MyForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n    form = MyForm({'field1': 'value1', 'field2': 'value2'})\n\n    self.assertHTMLEqual(str(form['field1']), '<input id=\"id_field1\" name=\"field1\" type=\"text\" value=\"value1\" required>')\n    self.assertHTMLEqual(str(form['field2']), '<input id=\"id_field2\" name=\"field2\" type=\"text\" value=\"value2\" required>')\n", "def test_field_order_with_inheritance(self):\n    class BaseForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n    class InheritedForm(BaseForm):\n        field3 = CharField()\n        field4 = CharField()\n\n        field_order = ['field4', 'field2', 'field3']\n\n    form = InheritedForm()\n    self.assertEqual(list(form.fields), ['field4', 'field2', 'field3', 'field1'])\n", "def test_get_initial_for_field_callable(self):\n    class PersonForm(Form):\n        first_name = CharField(initial=lambda: 'John')\n        last_name = CharField()\n\n    form = PersonForm()\n    self.assertEqual(form.get_initial_for_field(form.fields['first_name'], 'first_name'), 'John')\n    self.assertIsNone(form.get_initial_for_field(form.fields['last_name'], 'last_name'))\n", "def test_media_property(self):\n    class MyForm(Form):\n        f1 = CharField(max_length=30, widget=Textarea)\n        f2 = CharField(max_length=30)\n\n    form = MyForm()\n    self.assertIsInstance(form.media, Media)\n    self.assertEqual(str(form.media), '')\n", "def test_field_required_with_empty_string(self):\n    class MyForm(Form):\n        f1 = CharField(max_length=30)\n        f2 = CharField(max_length=30, required=False)\n\n    form = MyForm({'f1': '', 'f2': ''})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {'f1': ['This field is required.']})\n"], "sample_335": ["def test_decimalfield_disabled(self):\n    f = DecimalField(max_digits=4, decimal_places=2, disabled=True)\n    self.assertWidgetRendersTo(f, '<input step=\"0.01\" name=\"f\" type=\"number\" id=\"id_f\" disabled required>')\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean('')\n    self.assertEqual(f.clean('1'), decimal.Decimal(\"1\"))\n    self.assertIsInstance(f.clean('1'), decimal.Decimal)\n    self.assertEqual(f.clean('23'), decimal.Decimal(\"23\"))\n    self.assertEqual(f.clean('3.14'), decimal.Decimal(\"3.14\"))\n    self.assertEqual(f.clean(3.14), decimal.Decimal(\"3.14\"))\n    self.assertEqual(f.clean(decimal.Decimal('3.14')), decimal.Decimal(\"3.14\"))\n    self.assertFalse(f.has_changed(decimal.Decimal(\"1\"), '1'))\n    self.assertFalse(f.has_changed(decimal.Decimal(\"1\"), decimal.Decimal(\"1\")))\n", "def test_decimalfield_validate(self):\n    f = DecimalField(max_digits=4, decimal_places=2)\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.validate('')\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.validate(None)\n    self.assertIsNone(f.validate('1.23'))\n    self.assertIsNone(f.validate(decimal.Decimal('1.23')))\n", "def test_decimalfield_disabled(self):\n    f = DecimalField(max_digits=4, decimal_places=2, disabled=True)\n    self.assertWidgetRendersTo(\n        f,\n        '<input step=\"0.01\" name=\"f\" type=\"number\" id=\"id_f\" disabled required>',\n    )\n    self.assertIsNone(f.clean(''))\n    self.assertEqual(f.clean('1.23'), decimal.Decimal('1.23'))\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(None)\n    self.assertFalse(f.has_changed(decimal.Decimal('1.23'), '1.23'))\n    self.assertTrue(f.has_changed(decimal.Decimal('1.23'), '4.56'))\n", "def test_decimalfield_min_max_value(self):\n    f = DecimalField(max_digits=5, decimal_places=2, min_value=10.01, max_value=99.99)\n    self.assertWidgetRendersTo(\n        f,\n        '<input step=\"0.01\" name=\"f\" min=\"10.01\" max=\"99.99\" type=\"number\" id=\"id_f\" required>',\n    )\n    with self.assertRaisesMessage(ValidationError, \"'Ensure this value is greater than or equal to 10.01.'\"):\n        f.clean('9.99')\n    with self.assertRaisesMessage(ValidationError, \"'Ensure this value is less than or equal to 99.99.'\"):\n        f.clean('100.00')\n    self.assertEqual(f.clean('12.34'), decimal.Decimal(\"12.34\"))\n", "def test_decimalfield_normalize(self):\n    f = DecimalField(max_digits=2, decimal_places=2, localize=True)\n    self.assertEqual(f.clean('001'), decimal.Decimal(\"1\"))\n    self.assertEqual(f.clean('000.01'), decimal.Decimal(\"0.01\"))\n    self.assertEqual(f.clean('.001'), decimal.Decimal(\"0.01\"))\n    self.assertEqual(f.clean('0.001'), decimal.Decimal(\"0.00\"))\n    self.assertEqual(f.clean('000.001'), decimal.Decimal(\"0.00\"))\n"], "sample_336": ["    def test_urlpattern_reverse_namespace(self):\n        resolver = get_resolver(None)\n        test_urls = [\n            # (name, namespace, expected)\n            ('urlobject-view', 'test-ns1', '/test1/inner/'),\n            ('urlobject-view', 'test-ns2', '/test2/inner/'),\n            ('inc-normal-view', 'inc-ns1', '/ns-included1/normal/'),\n            ('inc-normal-view', 'inc-ns2', '/ns-included2/normal/'),\n        ]\n        for name, namespace, expected in test_urls:\n            with self.subTest(name=name, namespace=namespace):\n                self.assertEqual(resolver.reverse(f'{namespace}:{name}'), expected)\n", "    def test_lazy_locale_regex(self):\n        resolver = URLResolver(RegexPattern(r'^/'), settings.ROOT_URLCONF)\n        locale_pattern = resolver.url_patterns[0].pattern.regex.pattern\n        self.assertEqual(locale_pattern, r'^en/')\n        with translation.override('fr'):\n            locale_pattern = resolver.url_patterns[0].pattern.regex.pattern\n            self.assertEqual(locale_pattern, r'^fr/')\n", "    def test_check_warning_for_regex_pattern_starting_with_slash(self):\n        msg = \"Your URL pattern ^/regex-starting-with-slash/ has a route beginning with a '/'. Remove this slash as it is unnecessary.\"\n        with self.assertWarnsMessage(Warning, msg):\n            check_resolver(get_resolver())\n", "    def test_route_pattern_repr(self):\n        route = RoutePattern('test/<int:pk>/')\n        self.assertEqual(repr(route), \"<RoutePattern 'test/<int:pk>/' from 'django.urls.resolvers'>\")\n", "    def test_pattern_startswith_slash(self):\n        resolver = get_resolver()\n        warnings = resolver.url_patterns[0].check()\n        self.assertEqual(len(warnings), 1)\n        self.assertIsInstance(warnings[0], Warning)\n        self.assertEqual(warnings[0].id, 'urls.W002')\n"], "sample_337": ["def test_csrf_token_in_template_context(self):\n    \"\"\"\n    The CSRF token is available in the template context.\n    \"\"\"\n    req = self._get_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_token = get_token(req)\n    self.assertIsNotNone(csrf_token)\n    self.assertEqual(resp.context['csrf_token'], csrf_token)\n", "def test_csrf_trusted_origins_with_port(self):\n    \"\"\"\n    A POST request with an origin that matches a CSRF_TRUSTED_ORIGINS\n    wildcard and includes a port is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://dashboard.example.com:8000'\n    mw = CsrfViewMiddleware(post_form_view)\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com:8000']):\n        self.assertIs(mw._origin_verified(req), True)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "def test_csrf_token_rotated_on_login(self):\n    \"\"\"\n    The CSRF token is rotated after a successful login.\n    \"\"\"\n    req = self._get_request()\n    get_token(req)\n    initial_token = req.META['CSRF_COOKIE']\n    rotate_token(req)\n    new_token = req.META['CSRF_COOKIE']\n    self.assertNotEqual(initial_token, new_token)\n", "def test_csrf_trusted_origins_wildcard(self):\n    \"\"\"\n    A wildcard in CSRF_TRUSTED_ORIGINS matches all subdomains.\n    \"\"\"\n    req = self._get_request(method='POST')\n    req.META['HTTP_ORIGIN'] = 'https://subdomain.example.com'\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n", "def test_csrf_rotate_token(self):\n    \"\"\"\n    The rotate_token() function changes the token and sets\n    request.csrf_cookie_needs_reset.\n    \"\"\"\n    req = self._get_request()\n    token1 = get_token(req)\n    rotate_token(req)\n    token2 = get_token(req)\n    self.assertNotEqual(token1, token2)\n    self.assertTrue(getattr(req, 'csrf_cookie_needs_reset', False))\n"], "sample_338": ["def test_alter_model_table_together(self):\n    \"\"\"Tests altering model table and index/unique_together simultaneously.\"\"\"\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_with_foo_together])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\n        \"AlterModelTable\", \"AlterUniqueTogether\", \"AlterIndexTogether\"\n    ])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", table=None)\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"author\", unique_together={(\"author\",)})\n    self.assertOperationAttributes(changes, 'testapp', 0, 2, name=\"author\", index_together={(\"author\",)})\n", "def test_deep_deconstruct(self):\n    \"\"\"Tests the deep_deconstruct method of MigrationAutodetector.\"\"\"\n    autodetector = MigrationAutodetector(None, None)\n    tests = [\n        (None, None),\n        (\"string\", \"string\"),\n        (1, 1),\n        ([1, 2, 3], [1, 2, 3]),\n        ((1, 2, 3), (1, 2, 3)),\n        ({'a': 1, 'b': 2}, {'a': 1, 'b': 2}),\n        (DeconstructibleObject(1, 2, a=3, b=4), ('tests.test_migrations.test_autodetector.DeconstructibleObject', (1, 2), {'a': 3, 'b': 4})),\n    ]\n    for value, expected in tests:\n        with self.subTest(value=value):\n            self.assertEqual(autodetector.deep_deconstruct(value), expected)\n", "def test_deep_deconstruct_preserves_builtin_types(self):\n    tests = [\n        (1, 1),\n        (None, None),\n        ('a', 'a'),\n        (b'a', b'a'),\n        ([], []),\n        ({}, {}),\n        ((1,), (1,)),\n        ({1, 2}, {1, 2}),\n        (frozenset({1, 2}), frozenset({1, 2})),\n    ]\n    for value, expected in tests:\n        with self.subTest(value=value):\n            self.assertEqual(\n                MigrationAutodetector.deep_deconstruct(value),\n                expected,\n            )\n", "def test_remove_model_options(self):\n    \"\"\"Changing a model's options to an empty dictionary should make a change.\"\"\"\n    changes = self.get_changes([self.author_with_options], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={})\n", "def test_remove_field_referenced_by_unique_together(self):\n    \"\"\"\n    Removing a field referenced by unique_together results in two separate\n    operations.\n    \"\"\"\n    before = ModelState('app', 'Model', [\n        ('id', models.AutoField(primary_key=True)),\n        ('field1', models.IntegerField()),\n        ('field2', models.IntegerField()),\n    ], options={'unique_together': {('field1', 'field2')}})\n    after = ModelState('app', 'Model', [\n        ('id', models.AutoField(primary_key=True)),\n        ('field2', models.IntegerField()),\n    ])\n    changes = self.get_changes([before], [after])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterUniqueTogether', 'RemoveField'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, name='model', unique_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes, 'app', 0, 1, model_name='model', name='field1',\n    )\n"], "sample_339": ["def test_modelformset_factory_with_custom_form(self):\n    class CustomAuthorForm(forms.ModelForm):\n        class Meta:\n            model = Author\n            fields = '__all__'\n\n    AuthorFormSet = modelformset_factory(Author, form=CustomAuthorForm)\n    self.assertEqual(AuthorFormSet.form.__name__, 'CustomAuthorForm')\n", "def test_modelformset_factory_edit_only_with_absolute_max(self):\n    AuthorFormSet = modelformset_factory(\n        Author,\n        fields='__all__',\n        edit_only=True,\n        absolute_max=1500,\n    )\n    data = {\n        'form-TOTAL_FORMS': '1501',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    formset = AuthorFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 1500)\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at most 1000 forms.'],\n    )\n", "def test_modelformset_factory_with_empty_fields(self):\n    # A ModelFormSet with empty fields should still validate correctly.\n    AuthorFormSet = modelformset_factory(Author, fields=[])\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '',\n    }\n    formset = AuthorFormSet(data)\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.errors, [])\n", "def test_modelformset_factory_with_disabled_fields(self):\n    AuthorFormSet = modelformset_factory(Author, fields='__all__', extra=1)\n    formset = AuthorFormSet()\n    for form in formset:\n        for field in form.fields.values():\n            field.disabled = True\n\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '',\n        'form-0-name': 'John Doe',\n        'form-1-name': 'Jane Doe',\n    }\n    formset = AuthorFormSet(data=data)\n    self.assertTrue(formset.is_valid())\n    formset.save()\n    self.assertEqual(Author.objects.count(), 2)\n", "def test_modelformset_factory_edit_only_with_delete(self):\n    charles = Author.objects.create(name='Charles Baudelaire')\n    walt = Author.objects.create(name='Walt Whitman')\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '2',\n        'form-MAX_NUM_FORMS': '',\n        'form-0-id': charles.pk,\n        'form-0-name': 'Arthur Rimbaud',\n        'form-1-id': walt.pk,\n        'form-1-name': 'Paul Verlaine',\n        'form-1-DELETE': 'on',\n    }\n    AuthorFormSet = modelformset_factory(Author, fields='__all__', edit_only=True, can_delete=True)\n    formset = AuthorFormSet(data, queryset=Author.objects.all())\n    self.assertIs(formset.is_valid(), True)\n    formset.save()\n    charles.refresh_from_db()\n    self.assertEqual(charles.name, 'Arthur Rimbaud')\n    with self.assertRaises(Author.DoesNotExist):\n        Author.objects.get(pk=walt.pk)\n"], "sample_340": ["def test_collect_sql(self):\n    \"\"\"\n    Test that collect_sql() returns the correct SQL for a migration plan.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    plan = [\n        ('migrations', '0001_initial'),\n        ('migrations', '0002_second'),\n    ]\n    sql = loader.collect_sql(plan)\n    self.assertGreater(len(sql), 0)\n    self.assertIsInstance(sql[0], str)\n\n    # Check that the SQL is correct\n    self.assertIn('CREATE TABLE \"migrations_author\"', sql[0])\n    self.assertIn('CREATE TABLE \"migrations_book\"', sql[1])\n", "def test_get_migration_by_prefix_exact_match(self):\n    loader = MigrationLoader(connection)\n    migration = loader.get_migration_by_prefix('migrations', '0001_initial')\n    self.assertEqual(migration.name, '0001_initial')\n", "def test_detect_conflicts(self):\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {})\n\n    # Simulate a conflict by adding a migration with the same name as an existing one\n    class ConflictingMigration(migrations.Migration):\n        dependencies = [(\"migrations\", \"0001_initial\")]\n        operations = [migrations.RunPython(lambda apps, schema_editor: None)]\n\n    loader.disk_migrations[(\"migrations\", \"0002_second\")] = ConflictingMigration(\n        \"0002_second\", \"migrations\"\n    )\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {\"migrations\": [\"0002_second\"]})\n", "def test_detect_conflicts(self):\n    \"\"\"\n    MigrationLoader.detect_conflicts() returns a dictionary of apps with\n    conflicting migrations.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0002_second', '0002_conflicting']})\n", "def test_detect_conflicts(self):\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {})\n\n    # Introduce a conflict by adding a new migration with the same app label and name.\n    with override_settings(MIGRATION_MODULES={'migrations': 'migrations.test_migrations_conflict'}):\n        loader = MigrationLoader(connection)\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {'migrations': ['0002_second', '0002_conflict']})\n"], "sample_341": ["def test_all_valid_short_circuits(self):\n    \"\"\"all_valid() short-circuits when it encounters an invalid formset.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '',\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n\n        self.assertIs(formset.is_valid(), False)\n        return False\n\n    with mock.patch('django.forms.formsets.BaseFormSet.is_valid', side_effect=is_valid):\n        self.assertIs(all_valid((formset1, formset2)), False)\n    self.assertEqual(formset1._errors, [{'votes': ['This field is required.']}, {'votes': ['This field is required.']}])\n    self.assertIsNone(formset2._errors)\n", "def test_add_fields_with_none_index(self):\n    class AddFieldsFormSet(BaseFormSet):\n            if index is not None:\n                form.fields['index'] = IntegerField(initial=index)\n\n    AddFieldsFormSetFormset = formset_factory(Choice, formset=AddFieldsFormSet)\n    formset = AddFieldsFormSetFormset()\n    self.assertIn('index', formset.forms[0].fields)\n    self.assertNotIn('index', formset.empty_form.fields)\n", "def test_formset_cleaned_data_after_validation_error(self):\n    \"\"\"A formset's cleaned_data is available after a validation error.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-1-choice': 'The Decemberists',\n        'choices-1-votes': '',  # missing value\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=3)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.forms],\n        [{'votes': 100, 'choice': 'Calexico'}, {}, {}]\n    )\n", "def test_formset_absolute_max(self):\n    ChoiceFormSet = formset_factory(Choice, max_num=1001, absolute_max=1001)\n    data = {\n        'choices-TOTAL_FORMS': '1002',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '1001',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertEqual(len(formset.forms), 1001)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at most 1001 forms.'],\n    )\n", "def test_absolute_max_with_can_delete(self):\n    data = {\n        'form-TOTAL_FORMS': '1001',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    for i in range(1001):\n        data[f'form-{i}-name'] = 'Drink'\n        if i > 999:\n            data[f'form-{i}-DELETE'] = 'on'\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        max_num=1000,\n        absolute_max=1000,\n        can_delete=True,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data)\n    self.assertIs(formset.is_valid(), True)\n    self.assertEqual(len(formset.forms), 1001)\n"], "sample_342": ["def test_serialize_result_custom_pk(self):\n    class AutocompleteJsonSerializeResultView(AutocompleteJsonView):\n            return {\n                'id': str(getattr(obj, to_field_name)),\n                'text': str(obj),\n                'custom': 'value',\n            }\n\n    q = Question.objects.create(question='Is this a question?')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonSerializeResultView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data, {\n        'results': [{'id': str(q.pk), 'text': q.question, 'custom': 'value'}],\n        'pagination': {'more': False},\n    })\n", "def test_serialize_result_override(self):\n    class AutocompleteJsonSerializeResultOverrideView(AutocompleteJsonView):\n            return {'custom_id': str(obj.pk), 'custom_text': str(obj)}\n\n    q = Question.objects.create(question='Is this a question?')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonSerializeResultOverrideView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data, {\n        'results': [{'custom_id': str(q.pk), 'custom_text': q.question}],\n        'pagination': {'more': False},\n    })\n", "def test_process_request_malformed_params(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'nonexistent'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    request = self.factory.get(self.url, {'term': 'is', 'model_name': 'nonexistent'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    request = self.factory.get(self.url, {'term': 'is', 'field_name': 'nonexistent'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_permission_denied(self):\n    request = self.factory.get(self.url, {'term': 'is', **self.opts, 'app_label': 'wrong_app'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    request = self.factory.get(self.url, {'term': 'is', **self.opts, 'model_name': 'wrong_model'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': Answer._meta.app_label})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': ' invalid', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_343": ["def test_get_content_type_with_for_concrete_model(self):\n    class Model(models.Model):\n        field = GenericForeignKey(for_concrete_model=False)\n\n    ct = Model.field.get_content_type(obj=Question())\n    self.assertEqual(ct.model_class(), Question)\n\n    class AbstractModel(models.Model):\n        class Meta:\n            abstract = True\n\n    class ConcreteModel(AbstractModel):\n        pass\n\n    ct = Model.field.get_content_type(obj=ConcreteModel())\n    self.assertEqual(ct.model_class(), ConcreteModel)\n", "def test_get_extra_restriction(self):\n    question = Question.objects.create(text='test')\n    restriction = Question.answer_set.field.get_extra_restriction('answer', 'question')\n    self.assertEqual(restriction.children[0].lookup_name, 'exact')\n    self.assertEqual(restriction.children[0].rhs, ContentType.objects.get_for_model(Question).pk)\n", "def test_generic_relation_related_manager(self):\n    question = Question.objects.create(text='test')\n    answer1 = Answer.objects.create(question=question)\n    answer2 = Answer.objects.create(question=question)\n\n    related_manager = question.answer_set\n    self.assertIsInstance(related_manager, ReverseGenericManyToOneDescriptor)\n    self.assertEqual(list(related_manager.all()), [answer1, answer2])\n\n    # Test adding an object to the relation\n    answer3 = Answer.objects.create()\n    related_manager.add(answer3)\n    self.assertEqual(list(related_manager.all()), [answer1, answer2, answer3])\n\n    # Test removing an object from the relation\n    related_manager.remove(answer2)\n    self.assertEqual(list(related_manager.all()), [answer1, answer3])\n", "def test_bulk_related_objects(self):\n    question = Question.objects.create(text='test')\n    answer1 = Answer.objects.create(question=question)\n    answer2 = Answer.objects.create(question=question)\n\n    related_objects = Question.answer_set.field.bulk_related_objects([question])\n    self.assertCountEqual(related_objects, [answer1, answer2])\n", "def test_generic_relation_get_prefetch_queryset(self):\n    questions = [Question.objects.create(text=f'Question {i}') for i in range(3)]\n    answers = [Answer.objects.create(question=question) for question in questions]\n\n    queryset, _, _, _, _, _ = Question.answer_set.field.get_prefetch_queryset(questions)\n    self.assertCountEqual(queryset, answers)\n"], "sample_344": ["def test_index_together_inheritance(self):\n    class Abstract(models.Model):\n        name = models.CharField(max_length=50)\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n            index_together = ['name']\n\n    class Child1(Abstract):\n        pass\n\n    class Child2(Abstract):\n        pass\n\n    child1_state = ModelState.from_model(Child1)\n    child2_state = ModelState.from_model(Child2)\n    self.assertEqual(child1_state.options['index_together'], [('name',)])\n    self.assertEqual(child2_state.options['index_together'], [('name',)])\n", "def test_get_related_models_recursive(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    B = self.create_model(\"B\", foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n    C = self.create_model(\"C\")\n    D = self.create_model(\"D\", bases=(A,))\n    E = self.create_model(\"E\", bases=(D,), foreign_keys=[models.ForeignKey('F', models.CASCADE)])\n    F = self.create_model(\"F\")\n\n    related_models = get_related_models_recursive(A)\n    self.assertEqual(\n        related_models,\n        {(m._meta.app_label, m._meta.model_name) for m in [A, B, C, D, E, F]}\n    )\n\n    related_models = get_related_models_recursive(B)\n    self.assertEqual(\n        related_models,\n        {(m._meta.app_label, m._meta.model_name) for m in [A, B, C, D, E, F]}\n    )\n\n    related_models = get_related_models_recursive(C)\n    self.assertEqual(\n        related_models,\n        {(m._meta.app_label, m._meta.model_name) for m in [A, B, C, D, E, F]}\n    )\n\n    related_models = get_related_models_recursive(D)\n    self.assertEqual(\n        related_models,\n        {(m._meta.app_label, m._meta.model_name) for m in [A, B, C, D, E, F]}\n    )\n\n    related_models = get_related_models_recursive(E)\n    self.assertEqual(\n        related_models,\n        {(m._meta.app_label, m._meta.model_name) for m in [A, B, C, D, E, F]}\n    )\n\n    related_models = get_related_models_recursive(F)\n    self.assertEqual(\n        related_models,\n        {(m._meta.app_label, m._meta.model_name) for m in [E, F]}\n    )\n", "def test_recursive_related_models(self):\n    \"\"\"\n    get_related_models_recursive() should not cause infinite recursion\n    when there are circular relations.\n    \"\"\"\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    B = self.create_model(\"B\", foreign_keys=[models.ForeignKey('A', models.CASCADE)])\n    self.assertRelated(A, [B])\n    self.assertRelated(B, [A])\n\n    C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('D', models.CASCADE)])\n    D = self.create_model(\"D\", foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n    E = self.create_model(\"E\", foreign_keys=[models.ForeignKey('C', models.CASCADE), models.ForeignKey('D', models.CASCADE)])\n    self.assertRelated(C, [D, E])\n    self.assertRelated(D, [C, E])\n    self.assertRelated(E, [C, D])\n", "def test_get_related_models_recursive_with_circular_dependencies(self):\n    \"\"\"\n    get_related_models_recursive should handle circular dependencies correctly.\n    \"\"\"\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    B = self.create_model(\"B\", foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n    C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('A', models.CASCADE)])\n\n    related_models = get_related_models_recursive(A)\n    self.assertEqual(related_models, {(A._meta.app_label, A._meta.model_name), (B._meta.app_label, B._meta.model_name), (C._meta.app_label, C._meta.model_name)})\n", "def test_get_related_models_tuples(self):\n    new_apps = Apps()\n\n    class Author(models.Model):\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class Book(models.Model):\n        author = models.ForeignKey(Author, models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(Author))\n    project_state.add_model(ModelState.from_model(Book))\n    self.assertEqual(\n        get_related_models_tuples(project_state.apps.get_model('migrations', 'Author')),\n        {('migrations', 'book')}\n    )\n    self.assertEqual(\n        get_related_models_tuples(project_state.apps.get_model('migrations', 'Book')),\n        {('migrations', 'author')}\n    )\n"], "sample_345": ["    def test_excludes_django_modules(self):\n        django_module = import_module('django.utils.autoreload')\n        self.addCleanup(lambda: sys.path_importer_cache.clear())\n        self.addCleanup(lambda: sys.modules.pop(django_module.__name__, None))\n        files = list(autoreload.iter_all_python_module_files())\n        self.assertNotIn(Path(django_module.__file__), files)\n", "    def test_trigger_reload_calls_sys_exit(self, mocked_exit):\n        autoreload.trigger_reload('test_file.py')\n        self.assertEqual(mocked_exit.call_count, 1)\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_stop_condition(self):\n        reloader = autoreload.BaseReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n", "    def test_is_not_tty(self, mocked_isatty):\n        # No exception should be raised if sys.stdin isn't a tty.\n        autoreload.ensure_echo_on()\n", "    def test_not_tty(self, mocked_isatty):\n        with mock.patch('termios.tcgetattr') as mocked_tcgetattr:\n            autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcgetattr.called)\n"], "sample_346": ["    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n\n        self.assertTrue(my_middleware.sync_capable)\n        self.assertTrue(my_middleware.async_capable)\n", "    def test_middleware_decorator(self):\n        class MyMiddleware:\n                self.view_func = view_func\n\n                return self.view_func(request, *args, **kwargs)\n\n        my_decorator = make_middleware_decorator(MyMiddleware)()\n\n        @my_decorator\n            return HttpResponse()\n\n        self.assertIsInstance(my_view(HttpRequest()), HttpResponse)\n", "    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n\n        self.assertTrue(my_middleware.sync_capable)\n        self.assertTrue(my_middleware.async_capable)\n", "    def test_make_middleware_decorator(self):\n        class MyMiddleware:\n                self.get_response = get_response\n                self.my_arg = my_arg\n\n                return self.get_response(request)\n\n        decorator = make_middleware_decorator(MyMiddleware)\n        decorated_view = decorator('my_arg')(lambda request: HttpResponse())\n        self.assertIsInstance(decorated_view(HttpRequest()), HttpResponse)\n", "    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n\n        self.assertTrue(my_middleware.sync_capable)\n        self.assertTrue(my_middleware.async_capable)\n"], "sample_347": ["def test_make_aware_is_dst_default(self):\n    # Test that is_dst defaults to False for ambiguous times.\n    ambiguous = datetime.datetime(2015, 10, 25, 2, 30)\n\n    tz = pytz.timezone('Europe/Paris')\n    aware = timezone.make_aware(ambiguous, tz)\n    self.assertEqual(aware.tzinfo.utcoffset(aware), datetime.timedelta(hours=1))\n", "def test_get_current_timezone_name(self):\n    \"\"\"\n    Test get_current_timezone_name() returns the correct timezone name.\n    \"\"\"\n    expected_tz_name = 'Africa/Nairobi'\n    with timezone.override(EAT):\n        self.assertEqual(timezone.get_current_timezone_name(), expected_tz_name)\n\n    with timezone.override(datetime.timezone(datetime.timedelta(hours=5), name='Etc/GMT-5')):\n        self.assertEqual(timezone.get_current_timezone_name(), 'Etc/GMT-5')\n\n    # pytz timezone\n    with timezone.override(pytz.timezone('Asia/Bangkok')):\n        self.assertEqual(timezone.get_current_timezone_name(), 'Asia/Bangkok')\n", "def test_localtime(self):\n    aware = datetime.datetime(2015, 1, 1, 0, 0, 1, tzinfo=ICT)\n    self.assertEqual(timezone.localtime(aware, timezone=EAT), datetime.datetime(2014, 12, 31, 21, 0, 1, tzinfo=EAT))\n    with timezone.override(EAT):\n        self.assertEqual(timezone.localtime(aware), datetime.datetime(2014, 12, 31, 21, 0, 1, tzinfo=EAT))\n\n    with mock.patch('django.utils.timezone.now', return_value=aware):\n        self.assertEqual(timezone.localtime(timezone=EAT), datetime.datetime(2014, 12, 31, 21, 0, 1, tzinfo=EAT))\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(), datetime.datetime(2014, 12, 31, 21, 0, 1, tzinfo=EAT))\n", "def test_get_current_timezone_name(self):\n    timezone.activate(ICT)\n    self.assertEqual(timezone.get_current_timezone_name(), 'UTC+07:00')\n    timezone.deactivate()\n    self.assertEqual(timezone.get_current_timezone_name(), 'America/Chicago')\n", "def test_get_current_timezone_name(self):\n    \"\"\"\n    The get_current_timezone_name() function should return the name of the\n    currently active timezone.\n    \"\"\"\n    default_tz = timezone.get_default_timezone()\n    try:\n        timezone.activate(ICT)\n        self.assertEqual(timezone.get_current_timezone_name(), 'UTC+07:00')\n    finally:\n        timezone.deactivate()\n\n    with timezone.override(EAT):\n        self.assertEqual(timezone.get_current_timezone_name(), 'UTC+03:00')\n\n    with timezone.override(default_tz):\n        self.assertEqual(timezone.get_current_timezone_name(), default_tz.tzname(None))\n"], "sample_348": ["    def test_actions_selection_counter_false(self):\n        class BandAdmin(ModelAdmin):\n            actions_selection_counter = False\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_modelform_defines_fields(self):\n        class ValidationTestForm(forms.ModelForm):\n            class Meta:\n                model = ValidationTestModel\n\n        self.assertIsInvalid(\n            ModelAdmin, ValidationTestModel,\n            \"Creating a ModelForm without either the 'fields' attribute \"\n            \"or the 'exclude' attribute is prohibited; form \"\n            \"ValidationTestForm needs updating.\",\n            id='models.E006',\n            invalid_obj=ValidationTestForm\n        )\n", "    def test_model_form_save_with_commit_false(self):\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = ValidationTestModel\n                fields = ('name',)\n\n        form = TestModelForm(data={'name': 'test'})\n        self.assertTrue(form.is_valid())\n        instance = form.save(commit=False)\n        self.assertIsNone(instance.pk)\n", "    def test_choices_with_queryset(self):\n        field = ModelChoiceField(queryset=User.objects.all())\n        self.assertEqual(list(field.choices), [(user.pk, str(user)) for user in User.objects.all()])\n", "    def test_custom_permissions_require_matching_has_method(self):\n        @admin.action(permissions=['custom'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n"], "sample_349": ["def test_optgroups(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'band': beatles.uuid})\n    widget = form['band'].field.widget\n    optgroups = widget.optgroups('band', [beatles.uuid], attrs={})\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 1)\n    self.assertEqual(optgroups[0][1][0]['value'], str(beatles.uuid))\n    self.assertEqual(optgroups[0][1][0]['label'], 'The Beatles')\n    self.assertEqual(optgroups[0][1][0]['selected'], True)\n\n    # Test with multiple values.\n    form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n    widget = form['featuring'].field.widget\n    optgroups = widget.optgroups('featuring', [beatles.pk, who.pk], attrs={})\n    self.assertEqual(len(optgroups), 1)\n    self.assertEqual(len(optgroups[0][1]), 2)\n    self.assertEqual(optgroups[0][1][0]['value'], str(beatles.pk))\n    self.assertEqual(optgroups[0][1][0]['label'], 'The Beatles')\n    self.assertEqual(optgroups[0][1][0]['selected'], True)\n    self.assertEqual(optgroups[0][1][1]['value'], str(who.pk))\n    self.assertEqual(optgroups[0][1][1]['label'], 'The Who')\n    self.assertEqual(optgroups[0][1][1]['selected'], True)\n", "def test_autocomplete_mixin_url_name(self):\n    rel = Album._meta.get_field('band').remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    self.assertEqual(w.url_name % w.admin_site.name, 'admin:autocomplete')\n", "def test_render_options_with_unsaved_instance(self):\n    \"\"\"Options are rendered correctly for an unsaved instance.\"\"\"\n    band = Band(name='The Beatles', style='rock')\n    form = AlbumForm(instance=Album(band=band))\n    output = form.as_table()\n    option = '<option value=\"\" selected>The Beatles</option>'\n    self.assertIn(option, output)\n", "def test_build_attrs_with_custom_language(self):\n    with translation.override('fr'):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['lang'], 'fr')\n", "def test_autocomplete_select_multiple(self):\n    class BandForm(forms.Form):\n        bands = ModelChoiceField(\n            queryset=Band.objects.all(),\n            widget=AutocompleteSelectMultiple(Band._meta.get_field('id'), admin.site),\n        )\n\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n\n    form = BandForm(initial={'bands': [beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option1 = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    selected_option2 = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    self.assertIn(selected_option1, output)\n    self.assertIn(selected_option2, output)\n"], "sample_350": ["def test_unsupported_distinct_on_combined_qs(self):\n    qs = Number.objects.all()\n    msg = 'Calling QuerySet.distinct() after %s() is not supported.'\n    combinators = ['union']\n    if connection.features.supports_select_difference:\n        combinators.append('difference')\n    if connection.features.supports_select_intersection:\n        combinators.append('intersection')\n    for combinator in combinators:\n        with self.subTest(combinator=combinator):\n            with self.assertRaisesMessage(NotSupportedError, msg % combinator):\n                getattr(qs, combinator)(qs).distinct()\n", "def test_union_with_multiple_values_list(self):\n    qs1 = Number.objects.filter(num=1).values_list('num', 'other_num')\n    qs2 = Number.objects.filter(num=2).values_list('num', 'other_num')\n    self.assertEqual(list(qs1.union(qs2)), [(1, 9), (2, 8)])\n", "def test_union_with_subqueries(self):\n    subquery = Number.objects.filter(num__gte=5).values('num')\n    qs1 = Number.objects.filter(num__in=subquery)\n    qs2 = Number.objects.filter(num__lte=3)\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 5, 6, 7, 8, 9], ordered=False)\n", "def test_union_with_outer_ref(self):\n    subquery = Number.objects.filter(num=OuterRef('num'))\n    qs1 = Number.objects.annotate(has_num=Exists(subquery)).filter(num__lte=1)\n    qs2 = Number.objects.annotate(has_num=Exists(subquery)).filter(num__gte=8)\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 8, 9], ordered=False)\n", "def test_union_with_limit(self):\n    qs1 = Number.objects.filter(num__lte=5).order_by('num')[:3]\n    qs2 = Number.objects.filter(num__gt=5).order_by('num')[:3]\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 6, 7, 8])\n"], "sample_351": ["def test_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name__contains': 'test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n    with self.assertRaises(ValidationError):\n        f.clean(self.c1.id)\n\n    # A callable limit_choices_to\n        return {'name__contains': 'third'}\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=limit_choices_to)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c3.pk, 'Third'),\n    ])\n    with self.assertRaises(ValidationError):\n        f.clean(self.c1.id)\n\n    # A callable returning a Q object.\n    from django.db.models import Q\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=lambda: Q(name__contains='test'))\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n\n    # Using a limit_choices_to that is a Q object.\n    from django.db.models import Q\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=Q(name__contains='test'))\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n\n    # Make sure the ModelChoiceField is passed the widget choices explicitly.\n    widget = forms.Select()\n    f = forms.ModelChoiceField(Category.objects.all(), widget=widget, limit_choices_to={'name__contains': 'test'})\n    self.assertEqual(list(f.widget.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_model_choice_field_with_limit_choices_to(self):\n    field = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name__startswith': 'A'})\n    self.assertEqual(len(field.choices), 2)\n    self.assertEqual(list(field.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n\n    # Ensure limit_choices_to works with a callable.\n        return {'name__startswith': 'T'}\n\n    field = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=limit_choices_to)\n    self.assertEqual(len(field.choices), 2)\n    self.assertEqual(list(field.choices), [\n        ('', '---------'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_modelchoicefield_iterator_bool(self):\n    f = forms.ModelChoiceField(Category.objects.all())\n    self.assertIs(bool(f.choices), True)\n    f.queryset = Category.objects.none()\n    self.assertIs(bool(f.choices), False)\n\n    # A ModelChoiceField with an empty_label is always considered True.\n    f = forms.ModelChoiceField(Category.objects.none(), empty_label='--------')\n    self.assertIs(bool(f.choices), True)\n", "def test_model_choice_field_callable_limit_choices_to(self):\n        return {'name__contains': 'test'}\n\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=limit_choices_to)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n\n        return {'name__contains': 'Entertainment'}\n\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=limit_choices_to)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n    ])\n", "def test_model_choice_iterator_len(self):\n    iterator = ModelChoiceIterator(forms.ModelChoiceField(Category.objects.all()))\n    self.assertEqual(len(iterator), 4)\n    Category.objects.create(name='Fourth', slug='fourth', url='fourth')\n    self.assertEqual(len(iterator), 5)\n    Category.objects.filter(slug='fourth').delete()\n    self.assertEqual(len(iterator), 4)\n"], "sample_352": ["    def test_ticket_24798(self):\n        # Test that filtering by a many-to-many field's id doesn't result in a FieldError.\n        self.assertEqual(\n            Item.objects.filter(tags__id=Tag.objects.first().pk).query.where.children[0].lookup_name,\n            'tags__id',\n        )\n", "    def test_ticket_24886(self):\n        \"\"\"\n        Ensure that get() and filter() with multiple exact lookups return the correct result.\n        \"\"\"\n        # Create two objects with different field values\n        obj1 = Tag.objects.create(name='tag1', parent=None)\n        obj2 = Tag.objects.create(name='tag2', parent=obj1)\n\n        # Get an object with multiple exact lookups\n        self.assertEqual(Tag.objects.get(name='tag1', parent=None), obj1)\n\n        # Filter objects with multiple exact lookups\n        self.assertSequenceEqual(Tag.objects.filter(name='tag1', parent=None), [obj1])\n        self.assertSequenceEqual(Tag.objects.filter(name='tag2', parent=obj1), [obj2])\n", "    def test_exists_subquery(self):\n        note = Note.objects.create(note='Test', misc='misc')\n        annotation = Annotation.objects.create(name='Annotation', tag=None)\n        annotation.notes.add(note)\n\n        qs = Annotation.objects.filter(notes__exists=True)\n        self.assertTrue(qs.exists())\n        self.assertEqual(qs.get(), annotation)\n\n        qs = Annotation.objects.filter(notes__exists=False)\n        self.assertFalse(qs.exists())\n\n        # Check that the subquery is not evaluated.\n        with self.assertNumQueries(1):\n            Annotation.objects.filter(notes__exists=True).exists()\n", "    def test_ticket_24863(self):\n        \"\"\"\n        Test that extra select fields are properly aliased when they conflict\n        with model field names.\n        \"\"\"\n        qs = Note.objects.extra(select={'name': 'note'})\n        self.assertSequenceEqual(qs.values('name'), [{'name': 'n1'}, {'name': 'n2'}, {'name': 'n3'}])\n        self.assertEqual(str(qs.query).count('`name`'), 2)\n", "    def test_ticket_24763(self):\n        \"\"\"\n        Ensure that querysets can be pickled after filtering with __in=EmptyQuerySet.\n        \"\"\"\n        qs = Note.objects.filter(pk__in=Note.objects.none())\n        pickle.loads(pickle.dumps(qs))\n"], "sample_353": ["    def test_required_fields_are_asked(self):\n        new_io = StringIO()\n\n        @mock_inputs({\n            'password': 'nopasswd',\n            'username': 'joe',\n            'email': 'joe@example.com',\n            'date_of_birth': '1970-01-01',\n            'first_name': 'Joe',\n        })\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                stdout=new_io,\n                stderr=new_io,\n            )\n            self.assertEqual(new_io.getvalue().strip(), 'Superuser created successfully.')\n\n        test(self)\n", "    def test_environment_variable_with_required_fields_non_interactive(self):\n        call_command('createsuperuser', interactive=False, verbosity=0)\n        user = User.objects.get(username='test_superuser')\n        self.assertEqual(user.email, 'joe@somewhere.org')\n        self.assertTrue(user.check_password('test_password'))\n", "    def test_get_input_data_with_foreign_key(self, mock_get_pass):\n        field = CustomUserWithFK._meta.get_field('group')\n        message = 'Group (Group.id): '\n        input_value = '1'\n        cmd = createsuperuser.Command()\n        result = cmd.get_input_data(field, message, default=None)\n        self.assertEqual(result, input_value)\n\n        # Test with an invalid input\n        input_value = 'non_existent_group'\n        with self.assertRaisesMessage(CommandError, \"Group instance with id '%s' does not exist.\" % input_value):\n            cmd.get_input_data(field, message, default=None)\n", "    def test_get_input_data(self):\n        field = User._meta.get_field('username')\n        message = 'Username: '\n        default = None\n\n        # Test with valid input\n        with mock.patch('builtins.input', return_value='testuser'):\n            value = createsuperuser.Command().get_input_data(field, message, default)\n            self.assertEqual(value, 'testuser')\n\n        # Test with invalid input\n        with mock.patch('builtins.input', return_value='invalid@#$'):\n            with self.assertRaises(exceptions.ValidationError):\n                createsuperuser.Command().get_input_data(field, message, default)\n\n        # Test with default value\n        default = 'defaultuser'\n        with mock.patch('builtins.input', return_value=''):\n            value = createsuperuser.Command().get_input_data(field, message, default)\n            self.assertEqual(value, default)\n", "    def test_createsuperuser_non_ascii_username(self):\n        new_io = StringIO()\n        username = '\u00fcsername'\n\n        @mock_inputs({\n            'password': 'nopasswd',\n            'username': username,\n            'email': '',\n            'date_of_birth': '1976-04-01',\n            'first_name': 'Joe',\n        })\n            call_command(\n                \"createsuperuser\",\n                interactive=True,\n                stdin=MockTTY(),\n                stdout=new_io,\n            )\n            command_output = new_io.getvalue().strip()\n            self.assertEqual(command_output, 'Superuser created successfully.')\n            u = CustomUser._default_manager.get(username=username)\n            self.assertEqual(u.date_of_birth, date(1976, 4, 1))\n\n        test(self)\n"], "sample_354": ["    def test_environment_variable_non_interactive_username_only(self):\n        with mock.patch.dict(os.environ, {\n            'DJANGO_SUPERUSER_USERNAME': 'test_superuser',\n        }):\n            new_io = StringIO()\n            call_command('createsuperuser', interactive=False, stdout=new_io)\n            command_output = new_io.getvalue().strip()\n            self.assertEqual(command_output, '')\n            user = User.objects.get(username='test_superuser')\n            self.assertFalse(user.has_usable_password())\n", "    def setUp(self):\n        self.old_get_system_username = management.get_system_username\n", "    def test_get_input_data(self):\n        command = createsuperuser.Command()\n        field = User._meta.get_field('username')\n        message = 'Username: '\n        default = None\n\n        # Test with valid input\n        with mock.patch('builtins.input', return_value='test_username'):\n            input_data = command.get_input_data(field, message, default)\n            self.assertEqual(input_data, 'test_username')\n\n        # Test with invalid input (validation error)\n        with mock.patch('builtins.input', return_value='invalid_username'):\n            with mock.patch.object(field, 'clean', side_effect=exceptions.ValidationError('Invalid username')):\n                input_data = command.get_input_data(field, message, default)\n                self.assertIsNone(input_data)\n\n        # Test with default value\n        default = 'default_username'\n        with mock.patch('builtins.input', return_value=''):\n            input_data = command.get_input_data(field, message, default)\n            self.assertEqual(input_data, default)\n", "    def setUp(self):\n        self.existing_username = 'existing'\n        User.objects.create(username=self.existing_username)\n", "    def test_validate_username(self):\n        # Test that an existing username returns an error message.\n        User.objects.create_user(username='existing')\n        error_msg = Command()._validate_username('existing', 'username', 'default')\n        self.assertEqual(error_msg, 'Error: That username is already taken.')\n\n        # Test that a blank username returns an error message.\n        error_msg = Command()._validate_username('', 'username', 'default')\n        self.assertEqual(error_msg, 'Username cannot be blank.')\n\n        # Test that a valid username doesn't return an error message.\n        error_msg = Command()._validate_username('valid', 'username', 'default')\n        self.assertIsNone(error_msg)\n"], "sample_355": ["    def setUpTestData(cls):\n        cls.permission = Permission.objects.create(\n            name='test_permission',\n            content_type=ContentType.objects.get_for_model(Group),\n            codename='test_codename'\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        content_type = ContentType.objects.get_for_model(Group)\n        perm1 = Permission.objects.create(name='test1', content_type=content_type, codename='test1')\n        perm2 = Permission.objects.create(name='test2', content_type=content_type, codename='test2')\n        cls.user.user_permissions.add(perm1, perm2)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.perm1 = Permission.objects.create(\n            name='Test permission 1',\n            content_type=ContentType.objects.get_for_model(Group),\n            codename='test_perm1'\n        )\n        cls.perm2 = Permission.objects.create(\n            name='Test permission 2',\n            content_type=ContentType.objects.get_for_model(Group),\n            codename='test_perm2'\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n", "    def test_user_get_all_permissions(self):\n        user = User.objects.create_user('test', 'test@example.com', 'test')\n        content_type = ContentType.objects.get_for_model(Group)\n        perm1 = Permission.objects.create(name='test1', content_type=content_type, codename='test1')\n        perm2 = Permission.objects.create(name='test2', content_type=content_type, codename='test2')\n\n        user.user_permissions.add(perm1)\n        user.save()\n\n        group = Group.objects.create(name='test_group')\n        group.permissions.add(perm2)\n        user.groups.add(group)\n\n        expected_perms = {'auth.test1', 'auth.test2'}\n        self.assertEqual(user.get_all_permissions(), expected_perms)\n"], "sample_356": ["def test_alter_model_options_with_custom_manager(self):\n    \"\"\"Changing a model's options should make a change.\"\"\"\n    changes = self.get_changes([self.author_empty], [ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ], {\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n        \"default_manager_name\": \"objects\",\n        \"base_manager_name\": \"base_objects\"\n    })])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, options={\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n        \"default_manager_name\": \"objects\",\n        \"base_manager_name\": \"base_objects\"\n    })\n", "def test_remove_field_with_index(self):\n    \"\"\"\n    Removing a field that is used in an index should remove the index before\n    removing the field.\n    \"\"\"\n    model_state = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ], {\n        \"indexes\": [models.Index(fields=[\"name\"], name=\"author_name_idx\")],\n    })\n    changes = self.get_changes([model_state], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveIndex\", \"RemoveField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', name='author_name_idx')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', name='name')\n", "def test_alter_field_type_with_choices(self):\n    \"\"\"Tests autodetection of altered field types with choices.\"\"\"\n    changes = self.get_changes(\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=200, choices=[(\"A\", \"A\"), (\"B\", \"B\")]))])],\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.IntegerField(choices=[(1, \"A\"), (2, \"B\")]))])],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n", "def test_m2m_w_through_multistep_remove_with_unique_together(self):\n    \"\"\"\n    A model with a m2m field that specifies a \"through\" model and has unique_together\n    cannot be removed in the same migration as that through model as the schema will\n    pass through an inconsistent state. The autodetector should produce two\n    migrations to avoid this issue.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_m2m_through, self.publisher, self.contract],\n        [self.publisher], MigrationQuestioner({'ask_delete_conflict': True})\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\n        \"RemoveField\", \"RemoveField\", \"AlterUniqueTogether\", \"DeleteModel\", \"DeleteModel\"\n    ])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", model_name='contract')\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"publisher\", model_name='contract')\n    self.assertOperationAttributes(changes, \"testapp\", 0, 2, name=\"Contract\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 3, name=\"Author\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 4, name=\"Contract\")\n", "def test_add_model_with_base_model_field_renamed(self):\n    \"\"\"\n    Renaming a field on a base model takes place before adding a new inherited \n    model that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [], bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('new_title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameField', 'AddField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, old_name='title', new_name='new_title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='title', model_name='book')\n"], "sample_357": ["def test_rename_field_with_m2m_through(self):\n    \"\"\"\n    #27548 - Renaming a field that is referenced in an M2M's through model.\n    \"\"\"\n    before = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.IntegerField()),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE, to_field='field')),\n        ]),\n        ModelState('app', 'FooBar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE)),\n            ('bar', models.ForeignKey('app.Bar', models.CASCADE)),\n        ]),\n        ModelState('app', 'Thing', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foos', models.ManyToManyField('app.Foo', through='app.FooBar')),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('renamed_field', models.IntegerField()),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE, to_field='renamed_field')),\n        ]),\n        ModelState('app', 'FooBar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE)),\n            ('bar', models.ForeignKey('app.Bar', models.CASCADE)),\n        ]),\n        ModelState('app', 'Thing', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foos', models.ManyToManyField('app.Foo', through='app.FooBar')),\n        ]),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameField'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, model_name='foo', old_name='field',\n        new_name='renamed_field',\n    )\n", "def test_generate_altered_db_table_with_mti(self):\n    \"\"\"\n    Tests when model and db_table changes for a model with MTI, autodetector\n    must create two operations.\n    \"\"\"\n    before = [\n        ModelState('testapp', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'B', [], bases=('testapp.A',)),\n    ]\n    after = [\n        ModelState('testapp', 'NewA', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={'db_table': 'new_a'}),\n        ModelState('testapp', 'B', [], bases=('testapp.NewA',)),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename_model': True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"A\", new_name=\"NewA\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"newa\", table=\"new_a\")\n", "def test_add_textfield_and_charfield_with_default(self):\n    \"\"\"\n    #23405 - Adding a NOT NULL and blank `CharField` or `TextField`\n    with default should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default='default_value')),\n            (\"biography\", models.TextField(default='default_biography')),\n        ])\n    ])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_unique_constraint_name(self):\n    changes = self.get_changes(\n        [self.author_empty], \n        [ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        {'constraints': [models.UniqueConstraint(fields=['id'], name='unique_id')]})]\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"CreateModel\", \"AddConstraint\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, model_name=\"author\", constraint=models.UniqueConstraint(fields=['id'], name='unique_id'))\n", "def test_alter_model_options_with_concrete_base(self):\n    \"\"\"Changing a model's options should make a change when the model has a concrete base.\"\"\"\n    before = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        bases=(\"testapp.BaseAuthor\",),\n    )\n    after = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        bases=(\"testapp.BaseAuthor\",),\n        options={\"verbose_name\": \"Authi\"},\n    )\n    changes = self.get_changes([before], [after])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={\"verbose_name\": \"Authi\"})\n"], "sample_358": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            opclasses=['varchar_ops', 'text_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=('ASC', 'DESC'), opclasses=('int4_ops', 'text_ops')\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=['ASC', 'DESC'], opclasses=['varchar_ops', 'text_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=('ASC', 'DESC'), opclasses=('varchar_ops', 'int4_ops')\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=('ASC', 'DESC'), opclasses=('int4_ops', 'text_ops')\n        )\n"], "sample_359": ["def test_create_model_with_db_table(self):\n    \"\"\"\n    Tests the CreateModel operation with a custom db_table.\n    \"\"\"\n    operation = migrations.CreateModel(\n        \"Pony\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"pink\", models.IntegerField(default=1)),\n        ],\n        options={\"db_table\": \"my_custom_pony\"},\n    )\n    project_state = ProjectState()\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_crmo\", new_state)\n    self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].db_table, \"my_custom_pony\")\n    # Test the database alteration\n    self.assertTableNotExists(\"my_custom_pony\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_crmo\", editor, project_state, new_state)\n    self.assertTableExists(\"my_custom_pony\")\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_crmo\", editor, new_state, project_state)\n    self.assertTableNotExists(\"my_custom_pony\")\n    # And deconstruction\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"CreateModel\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(sorted(definition[2]), [\"fields\", \"name\", \"options\"])\n", "def test_references_field_by_through_fields_default(self):\n    operation = FieldOperation('Model', 'field', models.ManyToManyField('Other', through='Through'))\n    self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), True)\n    self.assertIs(operation.references_field('Through', 'model_id', 'migrations'), True)\n    self.assertIs(operation.references_field('Through', 'other_id', 'migrations'), True)\n", "def test_reduce(self):\n    operation = FieldOperation('Model', 'field', models.BooleanField(default=False))\n    other_operation = FieldOperation('Model', 'other_field', models.BooleanField(default=False))\n    same_operation = FieldOperation('Model', 'field', models.BooleanField(default=False))\n    self.assertIs(operation.reduce(other_operation, None), False)\n    self.assertIs(operation.reduce(same_operation, None), True)\n", "def test_references_field_by_from_fields_multiple(self):\n    operation = FieldOperation(\n        'Model', 'field', models.fields.related.ForeignObject('Other', models.CASCADE, ['from1', 'from2'], ['to'])\n    )\n    self.assertIs(operation.references_field('Model', 'from1', 'migrations'), True)\n    self.assertIs(operation.references_field('Model', 'from2', 'migrations'), True)\n    self.assertIs(operation.references_field('Model', 'to', 'migrations'), False)\n    self.assertIs(operation.references_field('Other', 'from1', 'migrations'), False)\n    self.assertIs(operation.references_field('Model', 'to', 'migrations'), False)\n", "def test_rename_model_with_database_table(self):\n    project_state = self.set_up_test_model(\"test_rmwdt\", db_table=\"custom_pony\")\n    operation = migrations.RenameModel(\"Pony\", \"Horse\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_rmwdt\", new_state)\n    self.assertIn((\"test_rmwdt\", \"horse\"), new_state.models)\n    self.assertNotIn((\"test_rmwdt\", \"pony\"), new_state.models)\n    # Test the database alteration\n    self.assertTableExists(\"custom_pony\")\n    self.assertTableNotExists(\"test_rmwdt_horse\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_rmwdt\", editor, project_state, new_state)\n    self.assertTableExists(\"custom_pony\")\n    self.assertTableNotExists(\"test_rmwdt_horse\")\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_rmwdt\", editor, new_state, project_state)\n    self.assertTableExists(\"custom_pony\")\n    self.assertTableNotExists(\"test_rmwdt_horse\")\n"], "sample_360": ["    def tearDown(self):\n        cache.clear()\n", "    def tearDown(self):\n        cache.clear()\n", "    def test_cache_key_generation_with_query_params(self):\n        request = HttpRequest()\n        request.method = 'GET'\n        request.path = '/path/to/resource'\n        request.META['QUERY_STRING'] = 'param1=value1&param2=value2'\n        key = get_cache_key(request)\n        self.assertIn('param1=value1', key)\n        self.assertIn('param2=value2', key)\n", "    def test_process_response_with_no_cache_header(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        get_response = lambda req: response\n\n        middleware = UpdateCacheMiddleware(get_response)\n        processed_response = middleware.process_response(request, response)\n\n        self.assertEqual(processed_response, response)\n        self.assertEqual(len(cache._cache), 0)\n", "    def test_constructor(self):\n        \"\"\"\n        Ensure the constructor is correctly distinguishing between usage of \n        UpdateCacheMiddleware as Middleware vs. usage of UpdateCacheMiddleware \n        as view decorator and setting attributes appropriately.\n        \"\"\"\n        # If only one argument is passed in construction, it's being used as\n        # middleware.\n        middleware = UpdateCacheMiddleware(empty_response)\n\n        # Now test object attributes against values defined in setUp above\n        self.assertEqual(middleware.cache_timeout, None)\n        self.assertIsNone(middleware.page_timeout)\n        self.assertEqual(middleware.key_prefix, '')\n        self.assertEqual(middleware.cache_alias, 'default')\n        self.assertEqual(middleware.cache, caches['default'])\n\n        # If more arguments are being passed in construction, it's being used\n        # as a decorator. First, test with \"defaults\":\n        as_view_decorator = UpdateCacheMiddleware(empty_response, cache_alias=None, key_prefix=None)\n\n        self.assertEqual(as_view_decorator.cache_timeout, None)  \n        self.assertIsNone(as_view_decorator.page_timeout)\n        self.assertEqual(as_view_decorator.key_prefix, '')\n        # Value of DEFAULT_CACHE_ALIAS from django.core.cache\n        self.assertEqual(as_view_decorator.cache_alias, 'default')\n        self.assertEqual(as_view_decorator.cache, caches['default'])\n\n        # Next, test with custom values:\n        as_view_decorator_with_custom = UpdateCacheMiddleware(\n            hello_world_view, cache_timeout=60, cache_alias='other', key_prefix='foo'\n        )\n\n        self.assertEqual(as_view_decorator_with_custom.cache_timeout, 60)\n        self.assertIsNone(as_view_decorator_with_custom.page_timeout)\n        self.assertEqual(as_view_decorator_with_custom.key_prefix, 'foo')\n        self.assertEqual(as_view_decorator_with_custom.cache_alias, 'other')\n        self.assertEqual(as_view_decorator_with_custom.cache, caches['other'])\n"], "sample_361": ["def test_urlize_with_nofollow(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n        ),\n        ('foo@example.com', '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n\n    # Test that rel=\"nofollow\" is not added to mailto links\n    self.assertEqual(\n        urlize('foo@example.com', nofollow=True),\n        '<a href=\"mailto:foo@example.com\">foo@example.com</a>'\n    )\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Search for google.com.', 'Search for <a href=\"http://google.com/\">google.com</a>.'),\n        ('Search for google.com!', 'Search for <a href=\"http://google.com/\">google.com</a>!'),\n        ('Search for google.com?', 'Search for <a href=\"http://google.com/\">google.com</a>?'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Check out google.com.', 'Check out <a href=\"http://google.com\">google.com</a>.'),\n        ('Search for google.com/?q=!', 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!'),\n        ('My email is foo@example.com.', 'My email is <a href=\"mailto:foo@example.com\">foo@example.com</a>.'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Search for google.com/?q=! and see.', 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'),\n        ('Check out www.djangoproject.com.', 'Check out <a href=\"http://www.djangoproject.com\">www.djangoproject.com</a>.'),\n        ('Try foo@example.com?', '<a href=\"mailto:foo@example.com\">foo@example.com</a>?'),\n        ('Visit https://www.djangoproject.com/', '<a href=\"https://www.djangoproject.com/\">https://www.djangoproject.com/</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'\n        ),\n        (\n            'Check out www.google.com.',\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>.'\n        ),\n        (\n            'My email is foo@example.com.',\n            'My email is <a href=\"mailto:foo@example.com\">foo@example.com</a>.'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_362": ["def test_alter_model_options_manager(self):\n    \"\"\"Changing a model's manager should make a change.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.other_pony_food])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n    managers = [\n        ('food_qs', FoodQuerySet.as_manager()),\n        ('food_mgr', FoodManager('a', 'b')),\n        ('food_mgr_kwargs', FoodManager('x', 'y', 3, 4)),\n    ]\n    self.assertEqual([name for name, mgr in changes['otherapp'][0].operations[0].managers], ['food_qs', 'food_mgr', 'food_mgr_kwargs'])\n    # Changing them back to empty should also make a change\n    changes = self.get_changes([self.other_pony_food], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterModelManagers\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"pony\", managers=[])\n", "def test_add_custom_fk_with_hardcoded_to_field(self):\n    class HardcodedForeignKey(models.ForeignKey):\n            kwargs['to'] = 'testapp.Author'\n            kwargs['to_field'] = 'name'\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs['to']\n            del kwargs['to_field']\n            return name, path, args, kwargs\n\n    book_hardcoded_fk_to = ModelState('testapp', 'Book', [\n        ('author', HardcodedForeignKey(on_delete=models.CASCADE)),\n    ])\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_empty, book_hardcoded_fk_to],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book')\n", "def test_alter_model_options_with_manager(self):\n    \"\"\"Changing a model's options should make a change when a manager is present.\"\"\"\n    author_with_manager = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        {\n            \"permissions\": [('can_hire', 'Can hire')],\n            \"verbose_name\": \"Authi\",\n        },\n        managers=[\n            ('objects', models.Manager()),\n        ]\n    )\n    changes = self.get_changes([self.author_empty], [author_with_manager])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n    })\n\n    changes = self.get_changes([author_with_manager], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={})\n", "def test_add_model_with_manager(self):\n    author = ModelState('otherapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ], managers=[\n        ('objects', models.Manager()),\n        ('name_objects', models.Manager()),\n    ])\n    changes = self.get_changes([], [author])\n    # Right number of migrations?\n    self.assertEqual(len(changes['otherapp']), 1)\n    # Right number of actions?\n    migration = changes['otherapp'][0]\n    self.assertEqual(len(migration.operations), 1)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n    self.assertEqual([name for name, mgr in changes['otherapp'][0].operations[0].managers],\n                     ['objects', 'name_objects'])\n", "def test_deep_deconstruct(self):\n    \"\"\"#31068 - Deep deconstruction should work for any type of object.\"\"\"\n    class MyObject:\n            self.value = value\n\n    obj = MyObject(42)\n    deconstructed = MigrationAutodetector.deep_deconstruct(obj)\n    self.assertEqual(deconstructed, obj)\n\n    class DeconstructibleMyObject(MyObject):\n            return (\n                'path.to.MyObject',\n                [],\n                {'value': self.value},\n            )\n\n    obj = DeconstructibleMyObject(42)\n    deconstructed = MigrationAutodetector.deep_deconstruct(obj)\n    self.assertEqual(\n        deconstructed,\n        ('path.to.MyObject', [], {'value': 42}),\n    )\n"], "sample_363": ["    def test_build_attrs(self):\n        mixin = widgets.AutocompleteMixin(None, None)\n        attrs = mixin.build_attrs({'id': 'test'})\n        self.assertIn('data-ajax--url', attrs)\n        self.assertIn('data-app-label', attrs)\n        self.assertIn('data-model-name', attrs)\n        self.assertIn('data-field-name', attrs)\n", "    def test_render(self):\n        w = widgets.AutocompleteSelect('test', admin.site)\n        self.assertHTMLEqual(\n            w.render('test', 'test'),\n            '<input type=\"text\" name=\"test\" class=\"vForeignKeyRawIdAdminField\" '\n            'data-ajax--cache=\"true\" data-ajax--delay=\"250\" '\n            'data-ajax--type=\"GET\" data-ajax--url=\"/admin/autocomplete/\" '\n            'data-app-label=\"\" data-model-name=\"\" data-field-name=\"\" '\n            'data-theme=\"admin-autocomplete\" data-allow-clear=\"true\" '\n            'data-placeholder=\"\" lang=\"\" class=\"admin-autocomplete\">'\n        )\n", "    def test_render(self):\n        w = widgets.AutocompleteSelectMultiple(Album._meta.get_field('band').remote_field, widget_admin_site)\n        self.assertHTMLEqual(\n            w.render('test', [42], attrs={}),\n            '<select multiple name=\"test\" class=\"selectmultiple vManyToManyRawIdAdminField admin-autocomplete '\n            'data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" '\n            'data-ajax--url=\"/admin_widgets/ajax/select/\" data-app-label=\"admin_widgets\" data-model-name=\"band\" '\n            'data-field-name=\"band\" data-theme=\"admin-autocomplete\" data-allow-clear=\"true\" data-placeholder=\"\" '\n            'lang=\"en\" id=\"id_test\"></select>'\n        )\n", "    def test_build_attrs(self):\n        widget = widgets.AutocompleteSelect(ForeignKeyRawIdWidget, admin.site)\n        attrs = widget.build_attrs(base_attrs={'name': 'test'}, extra_attrs=None)\n        self.assertIn('data-ajax--delay', attrs)\n        self.assertIn('data-theme', attrs)\n        self.assertEqual(attrs['data-theme'], 'admin-autocomplete')\n", "    def test_build_attrs(self):\n        w = widgets.AutocompleteSelect(Member._meta.get_field('band').remote_field, admin.site)\n        attrs = w.build_attrs({'id': 'test'})\n        self.assertIn('data-ajax--url', attrs)\n        self.assertIn('data-app-label', attrs)\n        self.assertIn('data-model-name', attrs)\n        self.assertIn('data-field-name', attrs)\n        self.assertIn('data-theme', attrs)\n        self.assertIn('lang', attrs)\n"], "sample_364": ["def test_include_with_missing_namespace(self):\n    msg = (\n        'Specifying a namespace in include() without providing an app_name '\n        'is not supported. Set the app_name attribute in the included '\n        'module, or pass a 2-tuple containing the list of patterns and '\n        'app_name instead.'\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        include('some_module.urls', namespace='my_namespace')\n", "def test_include_with_missing_namespace(self):\n    msg = (\n        'Specifying a namespace in include() without providing an app_name '\n        'is not supported. Set the app_name attribute in the included '\n        'module, or pass a 2-tuple containing the list of patterns and '\n        'app_name instead.',\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        include(('urlpatterns',), namespace='namespace')\n", "def test_path_include_namespace_without_app_name(self):\n    msg = (\n        'Specifying a namespace in include() without providing an app_name '\n        'is not supported. Set the app_name attribute in the included '\n        'module, or pass a 2-tuple containing the list of patterns and '\n        'app_name instead.'\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        include('urlpatterns.include_urls', namespace='namespace')\n", "def test_include_with_namespace(self):\n    msg = (\n        'Specifying a namespace in include() without providing an app_name '\n        'is not supported. Set the app_name attribute in the included '\n        'module, or pass a 2-tuple containing the list of patterns and '\n        'app_name instead.',\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        include('urlpatterns.include_urls', namespace='example')\n", "def test_include_with_empty_namespace(self):\n    msg = (\n        'Specifying a namespace in include() without providing an app_name '\n        'is not supported. Set the app_name attribute in the included '\n        'module, or pass a 2-tuple containing the list of patterns and '\n        'app_name instead.',\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        include('some.urls', namespace='empty-namespace')\n"], "sample_365": ["def test_lazy_hash(self):\n    lazy_obj = lazy(lambda: 'test', str)\n    self.assertEqual(hash(lazy_obj()), hash('test'))\n", "def test_lazy_mod(self):\n    lazy_4 = lazy(lambda: 4, int)\n    self.assertEqual(lazy_4() % 3, 1)\n\n    lazy_str = lazy(lambda: 'hello', str)\n    self.assertEqual(lazy_str() % ' world', 'hello world')\n", "def test_simple_lazy_object_repr(self):\n    class Klazz:\n            return 'Klazz()'\n\n    lazy_obj = SimpleLazyObject(lambda: Klazz())\n    self.assertEqual(repr(lazy_obj), '<SimpleLazyObject: <function test_simple_lazy_object_repr.<locals>.<lambda> at')\n", "def test_lazyobject_repr(self):\n    class Klazz:\n            self.x = 42\n\n            return 'Klazz()'\n\n    lazy_obj = LazyObject()\n    lazy_obj._setupfunc = lambda: Klazz()\n    self.assertEqual(repr(lazy_obj), '<LazyObject: <function test_lazyobject_repr.<locals>.<lambda> at 0x...>>')\n    lazy_obj._setup()\n    self.assertEqual(repr(lazy_obj), '<LazyObject: Klazz()>')\n", "def test_classproperty_setter(self):\n    class Foo:\n        foo_attr = 123\n\n        @classproperty\n            return cls.foo_attr\n\n        @foo.setter\n            cls.foo_attr = value\n\n    self.assertEqual(Foo.foo, 123)\n    Foo.foo = 456\n    self.assertEqual(Foo.foo, 456)\n    self.assertEqual(Foo().foo, 456)\n"], "sample_366": ["def test_invalid_input(self):\n    # Test that invalid inputs return None\n    invalid_inputs = [\n        '2012-04-23T',  # incomplete date\n        '2012-04-23T09:15:',  # incomplete time\n        '2012-04-23T09:15:00 ',  # trailing whitespace\n        ' 2012-04-23T09:15:00',  # leading whitespace\n        '2012-04-32T09:15:00',  # invalid day\n        '2012-13-23T09:15:00',  # invalid month\n        '2012-04-23T25:15:00',  # invalid hour\n        '2012-04-23T09:60:00',  # invalid minute\n        '2012-04-23T09:15:60',  # invalid second\n    ]\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_datetime(source))\n", "def test_parse_duration_invalid_formats(self):\n    # Invalid formats should return None\n    self.assertIsNone(parse_duration('1 day 2'))\n    self.assertIsNone(parse_duration('1:2:3:4'))\n    self.assertIsNone(parse_duration('1.2.3 days'))\n    self.assertIsNone(parse_duration('P1Y2M3D'))  # ISO 8601 format for years and months is not supported\n    self.assertIsNone(parse_duration('PT1H2M3S4'))  # Too many parts\n    self.assertIsNone(parse_duration('PT1H2M3S-4'))  # Negative part at the end\n    self.assertIsNone(parse_duration('PT1H2M-3S'))  # Negative part in the middle\n", "def test_parse_duration_invalid_inputs(self):\n    invalid_inputs = (\n        'abc',\n        '1 year',\n        '1 month',\n        'P1Y2M3W4D',\n        'PT1H2M3S4',\n        'P1DT1H1M1S1',\n        '-P-1D',\n        'P-1DT-1H-1M-1S',\n        'PT-1H-1M-1S-1',\n        'P.5D',\n        'PT.5S',\n        '-P.5D',\n        '-PT.5S',\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "def test_parse_duration_edge_cases(self):\n    # Test parsing of edge cases, such as very large or very small durations\n    test_values = (\n        ('999999999 days', timedelta(days=999999999)),\n        ('-999999999 days', timedelta(days=-999999999)),\n        ('999999999.999999 seconds', timedelta(seconds=999999999, microseconds=999999)),\n        ('-999999999.999999 seconds', timedelta(seconds=-999999999, microseconds=-999999)),\n        ('P1000Y', None),  # This is equivalent to 365250 days, which is too large for timedelta\n        ('P100000000DT100H', None),  # This is equivalent to 100000000 days and 100 hours, which is too large for timedelta\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_duration_edge_cases(self):\n    # Overflow\n    self.assertIsNone(parse_duration('999999999 23:59:59'))\n    self.assertIsNone(parse_duration('P365D 23:59:59'))\n\n    # Invalid input\n    self.assertIsNone(parse_duration('invalid input'))\n    self.assertIsNone(parse_duration('123 invalid seconds'))\n    self.assertIsNone(parse_duration('123 seconds invalid'))\n\n    # Leading and trailing whitespace\n    self.assertEqual(parse_duration('   30  '), timedelta(seconds=30))\n\n    # Mixed positive and negative components\n    self.assertEqual(parse_duration('-1 day +2 hours'), timedelta(days=-1, hours=2))\n    self.assertEqual(parse_duration('+1 day -2 hours'), timedelta(days=1, hours=-2))\n"], "sample_367": ["    def test_cache_page_decorator_with_key_prefix(self):\n        @cache_page(60, key_prefix='test')\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = a_view(request)\n        self.assertEqual(response.status_code, 200)\n", "    def test_cache_page_decorator(self, mock_CacheMiddleware):\n        @cache_page(60)\n            return HttpResponse()\n\n        request = HttpRequest()\n        a_view(request)\n        mock_CacheMiddleware.assert_called_once_with(page_timeout=60)\n", "    def test_cache_page_decorator(self, mock_CacheMiddleware):\n        @cache_page(123)\n            return HttpResponse()\n\n        request = HttpRequest()\n        a_view(request)\n\n        mock_CacheMiddleware.assert_called_once_with(\n            page_timeout=123,\n            cache_alias=None,\n            key_prefix=None,\n        )\n", "    def test_cache_page_decorator_with_cache_alias(self):\n        @cache_page(60, cache='my_cache')\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = a_view(request)\n        self.assertEqual(response.status_code, 200)\n", "    def test_cache_page_decorator(self, mocked_CacheMiddleware):\n        @cache_page(123)\n            return HttpResponse()\n\n        request = HttpRequest()\n        a_view(request)\n\n        mocked_CacheMiddleware.assert_called_once_with(\n            page_timeout=123, cache_alias=None, key_prefix=None\n        )\n"], "sample_368": ["def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test that replaced migrations are properly handled in the migration plan.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    squash_impl = FakeMigration('squash')\n    squash = ('a', 'squash')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(squash, squash_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_replacement(squash, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n\n    plan = executor.migration_plan({squash})\n\n    self.assertEqual(plan, [(squash_impl, False)])\n", "def test_create_project_state_with_applied_migrations(self):\n    executor = MigrationExecutor(connection)\n    # Migrate forwards\n    executor.migrate([('migrations', '0001_initial')])\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Create project state including applied migrations\n    state = executor._create_project_state(with_applied_migrations=True)\n    self.assertIn(('migrations', 'author'), state.models)\n    self.assertIn(('migrations', 'tribble'), state.models)\n    # Cleanup\n    executor.migrate([('migrations', None)])\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Replaced migrations should not be included in the migration plan.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    squash_impl = FakeMigration('squash_0001_0002')\n    squash = ('a', 'squash_0001_0002')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(squash, squash_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_replacement(squash, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n\n    plan = executor.migration_plan([squash])\n\n    self.assertEqual(plan, [(squash_impl, False)])\n", "def test_migrate_with_custom_state(self):\n    \"\"\"\n    #28555 - Tests custom state passed to migrate.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Create initial tables\n    executor.migrate([\n        (\"migrations\", \"0001_initial\"),\n        (\"migrations2\", \"0001_initial\"),\n    ])\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Get the current project state\n    project_state = executor.loader.project_state()\n    # Make a custom state to pass to migrate\n    custom_state = project_state.clone()\n    # Add some apps from other states\n    for app in [\"migrations\", \"migrations2\"]:\n        custom_state.add_model(app, \"testmodel\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n    # Migrate with the custom state\n    executor.migrate([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0002_second\"),\n    ], state=custom_state)\n    # The resulting state should include the custom apps.\n    self.assertIn(('migrations', 'testmodel'), executor.migrate([], state=custom_state).models)\n    self.assertIn(('migrations2', 'testmodel'), executor.migrate([], state=custom_state).models)\n    # Cleanup by unmigrating everything.\n    executor.migrate([\n        (\"migrations\", None),\n        (\"migrations2\", None),\n    ])\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n    self.assertTableNotExists(\"migrations2_otherauthor\")\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test migration plan with replaced migrations.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a1)\n    graph.add_replacement(a3, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n\n    plan = executor.migration_plan({a3})\n\n    self.assertEqual(plan, [(a3_impl, False)])\n"], "sample_369": ["def test_reduce_delete_model(self):\n    operation = DeleteModel(name='model')\n    reduced_operation = operation.reduce(operation, app_label=None)\n    self.assertEqual(reduced_operation, [])\n", "def test_rename_model_with_fks_in_same_app(self):\n    \"\"\"\n    The order of fields in a model does not influence the RenameModel detection.\n    \"\"\"\n    before = [\n        ModelState(\"testapp\", \"EntityA\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ]),\n        ModelState(\"testapp\", \"EntityB\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"entity_a\", models.ForeignKey(\"testapp.EntityA\", models.CASCADE)),\n            (\"some_label\", models.CharField(max_length=255)),\n        ]),\n    ]\n    after = [\n        ModelState(\"testapp\", \"RenamedEntityA\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ]),\n        ModelState(\"testapp\", \"EntityB\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"renamed_entity_a\", models.ForeignKey(\"testapp.RenamedEntityA\", models.CASCADE)),\n            (\"some_label\", models.CharField(max_length=255)),\n        ]),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename_model\": True}))\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"EntityA\", new_name=\"RenamedEntityA\")\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, model_name='entityb', old_name='entity_a',\n        new_name='renamed_entity_a',\n    )\n", "def test_add_model_with_textfield_and_charfield(self):\n    \"\"\"\n    #23405 - Adding a NOT NULL and blank `CharField` or `TextField`\n    without default should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField()),\n            (\"biography\", models.TextField(blank=True)),\n        ])\n    ])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_alter_index_together_with_f_expression(self):\n    initial_author = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n        ('age', models.IntegerField()),\n    ], {\n        'index_together': {('name', models.F('age'))},\n    })\n    author_reversed_constraints = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n        ('age', models.IntegerField()),\n    ], {\n        'index_together': {(models.F('age'), 'name')},\n    })\n    changes = self.get_changes([initial_author], [author_reversed_constraints])\n\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\n        'AlterIndexTogether',\n    ])\n    self.assertOperationAttributes(\n        changes, 'testapp', 0, 0, name='author',\n        index_together={('age', 'name')},\n    )\n", "def test_model_option_operations(self):\n    \"\"\"\n    Tests that model option operations (e.g. AlterModelTable, AlterUniqueTogether)\n    are properly detected.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty],\n        [ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], {\n            \"db_table\": \"my_authors\",\n            \"unique_together\": {(\"id\",)},\n            \"index_together\": {(\"id\",)},\n        })],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\", \"AlterUniqueTogether\", \"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", table=\"my_authors\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"author\", unique_together={(\"id\",)})\n    self.assertOperationAttributes(changes, 'testapp', 0, 2, name=\"author\", index_together={(\"id\",)})\n"], "sample_370": ["    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.tag1 = TaggedItem.objects.create(tag=\"awesome\", content_object=cls.book1)\n        cls.tag2 = TaggedItem.objects.create(tag=\"great\", content_object=cls.book2)\n", "    def setUpTestData(cls):\n        cls.tag = TaggedItem.objects.create(tag=\"django\", content_object=Book.objects.create(title='Poems'))\n        cls.bookmark = Bookmark.objects.create(url='http://www.djangoproject.com/')\n        TaggedItem.objects.create(content_object=cls.bookmark, tag='python')\n", "    def setUpTestData(cls):\n        cls.author = Author.objects.create(name='Author1')\n        cls.book1 = Book.objects.create(title='Book1')\n        cls.book2 = Book.objects.create(title='Book2')\n        cls.book3 = Book.objects.create(title='Book3')\n        cls.author.books.add(cls.book1, cls.book2)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.author2 = Author.objects.create(name='Author 2')\n\n        cls.book1.authors.add(cls.author1, cls.author2)\n        cls.book2.authors.add(cls.author1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n        cls.author3 = Author.objects.create(name='Emily', first_book=cls.book1)\n        cls.author4 = Author.objects.create(name='Jane', first_book=cls.book2)\n\n        cls.book1.authors.add(cls.author1, cls.author2, cls.author3)\n        cls.book2.authors.add(cls.author1)\n"], "sample_371": ["    def test_not_used(self):\n        response = default_urlconf(HttpRequest())\n        self.assertNotContains(response, \"Page not found\", status_code=200)\n", "    def test_get_safe_settings(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        settings_dict = {\n            'SETTING_NAME': 'SETTING_VALUE',\n            'PASSWORD': 'super_secret',\n            'API_KEY': 'another_secret',\n            'ALLOWED_HOSTS': ['example.com'],\n            'DATABASE_URL': 'sqlite:///test.db',\n        }\n        with override_settings(**settings_dict):\n            safe_settings = reporter_filter.get_safe_settings()\n            self.assertEqual(\n                safe_settings,\n                {\n                    'SETTING_NAME': 'SETTING_VALUE',\n                    'PASSWORD': reporter_filter.cleansed_substitute,\n                    'API_KEY': reporter_filter.cleansed_substitute,\n                    'ALLOWED_HOSTS': ['example.com'],\n                    'DATABASE_URL': reporter_filter.cleansed_substitute,\n                },\n            )\n", "    def test_get_traceback_frame_variables(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().get('/')\n        frame = sys._getframe()\n        vars = reporter_filter.get_traceback_frame_variables(request, frame)\n        self.assertIn(('reporter_filter', 'SafeExceptionReporterFilter'), vars)\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = {'SECRET_KEY': 'secret', 'PUBLIC_SETTING': 'public'}\n        safe_settings = filter.get_safe_settings(settings)\n        self.assertEqual(safe_settings['SECRET_KEY'], filter.cleansed_substitute)\n        self.assertEqual(safe_settings['PUBLIC_SETTING'], 'public')\n", "    def test_get_traceback_data_no_request(self):\n        exception = ValueError('Test exception')\n        reporter = ExceptionReporter(None, type(exception), exception, None)\n        data = reporter.get_traceback_data()\n        self.assertEqual(data['request'], None)\n        self.assertEqual(data['request_meta'], {})\n        self.assertEqual(data['filtered_POST_items'], [])\n        self.assertEqual(data['settings'], {})\n        self.assertEqual(data['user_str'], None)\n"], "sample_372": ["    def test_compile(self):\n        p = RegexPattern(r'^test/')\n        self.assertIsInstance(p.regex, re.Pattern)\n", "    def test_check_url_warning(self):\n        resolver = get_resolver()\n        with self.assertWarnsMessage(\n            Warning,\n            \"Your URL pattern 'warning-pattern' [name='warning-pattern'] has a route beginning with a '/'. \"\n            \"Remove this slash as it is unnecessary. If this pattern is targeted in an include(), ensure the \"\n            \"include() pattern has a trailing '/'\"\n        ):\n            resolver.check()\n", "    def test_urlpattern_check(self):\n        resolver = get_resolver()\n        all_warnings = checks.run_checks()\n        url_pattern_warnings = [\n            warning for warning in all_warnings if warning.id == 'urls.W003'\n        ]\n        self.assertEqual(len(url_pattern_warnings), 1)\n        msg = (\n            \"Your URL pattern '^$' [name='named-url1'] has a name including a ':'. \"\n            \"Remove the colon, to avoid ambiguous namespace references.\"\n        )\n        self.assertEqual(url_pattern_warnings[0].msg, msg)\n", "    def test_check_pattern_startswith_slash(self):\n        msg = \"Your URL pattern '^/foo' has a route beginning with a '/'. Remove this slash as it is unnecessary.\"\n        with self.assertRaisesMessage(Warning, msg):\n            check_resolver(get_resolver('urlpatterns_reverse.urls_starting_with_slash'))\n", "    def test_regex_pattern_repr(self):\n        pattern = RegexPattern(r'^example/$', name='example-view')\n        self.assertEqual(repr(pattern), '<RegexPattern ^example/$ (name=\\'example-view\\')>')\n"], "sample_373": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_get_view_name(self):\n        view_path = 'django.contrib.admin.sites.AdminSite.index'\n        self.assertEqual(get_view_name(import_module(view_path)), view_path)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.company = Company.objects.create(name=\"Django\")\n        self.person = Person.objects.create(first_name=\"Human\", last_name=\"User\", company=self.company)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        Site.objects.all().delete()\n        del settings.SITE_ID\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        with captured_stderr() as self.docutils_stderr:\n            self.response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n"], "sample_374": ["    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n", "    def setUpTestData(cls):\n        Teacher.objects_custom.bulk_create([\n            Teacher(name='Teacher1'),\n            Teacher(name='Teacher2'),\n        ])\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.author2 = Author.objects.create(name='Author 2')\n        cls.book1.authors.add(cls.author1, cls.author2)\n        cls.book2.authors.add(cls.author1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n", "    def setUpTestData(cls):\n        cls.author = Author.objects.create(name='TestAuthor')\n        cls.book1 = Book.objects.create(title='Book1')\n        cls.book2 = Book.objects.create(title='Book2')\n        cls.book3 = Book.objects.create(title='Book3')\n        cls.author.books.add(cls.book1, cls.book2)\n"], "sample_375": ["def test_alter_field_with_relation(self):\n    project_state = self.get_base_project_state()\n    self.assertEqual(\n        list(project_state.relations['tests', 'user']),\n        [('tests', 'comment'), ('tests', 'post')],\n    )\n    new_field = models.ForeignKey('tests.comment', models.CASCADE)\n    project_state.alter_field(\n        'tests', 'post', 'authors', new_field, preserve_default=True,\n    )\n    self.assertEqual(\n        list(project_state.relations['tests', 'user']),\n        [('tests', 'comment')],\n    )\n    self.assertEqual(\n        list(project_state.relations['tests', 'comment']),\n        [('tests', 'comment'), ('tests', 'post')],\n    )\n", "def test_get_related_models_tuples(self):\n    \"\"\"\n    get_related_models_tuples returns a set of (app_label, model_name) tuples\n    for all related models.\n    \"\"\"\n    new_apps = Apps()\n\n    class Author(models.Model):\n        name = models.CharField(max_length=255)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class Book(models.Model):\n        title = models.CharField(max_length=1000)\n        author = models.ForeignKey(Author, models.CASCADE)\n        contributors = models.ManyToManyField(Author)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    related_models = get_related_models_tuples(Book)\n    self.assertEqual(related_models, {('migrations', 'author')})\n", "def test_get_related_models_recursive_with_generic_foreign_key(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    B = self.create_model(\"B\")\n    C = self.create_model(\"C\", foreign_keys=[\n        GenericForeignKey(),\n        models.ForeignKey('D', models.CASCADE),\n    ])\n    D = self.create_model(\"D\")\n\n    # Make sure that the generic foreign key does not cause infinite recursion.\n    self.assertRelated(C, [D])\n    self.assertRelated(D, [C])\n", "def test_get_related_models_tuples(self):\n    \"\"\"\n    get_related_models_tuples() returns a set of tuples for all related models.\n    \"\"\"\n    new_apps = Apps()\n\n    class A(models.Model):\n        class Meta:\n            app_label = 'something'\n            apps = new_apps\n\n    class B(models.Model):\n        a = models.ForeignKey(A, models.CASCADE)\n\n        class Meta:\n            app_label = 'something'\n            apps = new_apps\n\n    class C(models.Model):\n        b = models.ForeignKey(B, models.CASCADE)\n\n        class Meta:\n            app_label = 'something'\n            apps = new_apps\n\n    self.assertEqual(\n        get_related_models_tuples(A),\n        {('something', 'b'), ('something', 'c')},\n    )\n    self.assertEqual(\n        get_related_models_tuples(B),\n        {('something', 'a'), ('something', 'c')},\n    )\n    self.assertEqual(\n        get_related_models_tuples(C),\n        {('something', 'a'), ('something', 'b')},\n    )\n", "def test_get_related_models_recursive_with_self_referential_fk(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('self', models.CASCADE)])\n    self.assertRelated(A, [A])\n"], "sample_376": ["def test_update_cookie_empty_messages(self):\n    \"\"\"\n    Test that _update_cookie correctly deletes the cookie when there are no messages.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage._update_cookie(None, response)\n    self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n", "def test_update_cookie_empty(self):\n    \"\"\"\n    Test that an empty list of messages results in a deleted cookie.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage._update_cookie(storage._encode([]), response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n", "def test_update_cookie_empty_value(self):\n    \"\"\"\n    Test that _update_cookie sets the cookie with an empty value when no messages are provided.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage._update_cookie([], response)\n    self.assertEqual(response.cookies['messages'].value, '')\n    self.assertEqual(response.cookies['messages']['domain'], '.example.com')\n    self.assertEqual(response.cookies['messages']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n    self.assertEqual(\n        response.cookies['messages']['samesite'],\n        settings.SESSION_COOKIE_SAMESITE,\n    )\n", "def test_update_cookie_empty_messages(self):\n    \"\"\"\n    If there are no messages, the cookie is deleted.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Store an empty list of messages\n    storage._store([], response)\n\n    # The cookie should be deleted\n    self.assertIn(CookieStorage.cookie_name, response.cookies)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n", "def test_update_cookie(self):\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test setting the cookie with encoded data\n    messages = [Message(constants.INFO, 'test')]\n    encoded_data = storage._encode(messages)\n    storage._update_cookie(encoded_data, response)\n    self.assertEqual(response.cookies['messages'].value, encoded_data)\n\n    # Test deleting the cookie when there is no encoded data\n    storage._update_cookie(None, response)\n    self.assertEqual(response.cookies['messages']['max-age'], 0)\n"], "sample_377": ["    def test_technical_404_response(self):\n        request = self.rf.get(\"/test_view/\")\n        exception = Http404(\"Testing technical 404.\")\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, \"Page not found <span>(404)</span>\", status_code=404)\n", "    def test_technical_404_response(self):\n        exception = Http404(\"Testing technical 404.\")\n        request = RequestFactory().get(\"/technical404/\")\n        response = technical_404_response(request, exception)\n        self.assertContains(\n            response,\n            '<pre class=\"exception_value\">Testing technical 404.</pre>',\n            status_code=404,\n            html=True,\n        )\n", "def test_get_traceback_frames_with_recursive_call(self):\n    try:\n            if n > 0:\n                recursive_call(n-1)\n            else:\n                raise Exception(\"Recursive exception\")\n\n        recursive_call(5)\n    except Exception:\n        exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertGreater(len(frames), 5)  # Ensure all recursive calls are captured\n        for frame in frames:\n            self.assertIn(\"recursive_call\", frame[\"function\"])\n", "    def test_html_response(self):\n        exception = Http404(\"Testing technical 404.\")\n        request = RequestFactory().get(\"/raises404/\")\n        response = technical_404_response(request, exception)\n        self.assertContains(response, \"Page not found <span>(404)</span>\", status_code=404)\n        self.assertContains(\n            response,\n            '<pre class=\"exception_value\">Testing technical 404.</pre>',\n            status_code=404,\n            html=True,\n        )\n        self.assertContains(response, \"Request Method:\", status_code=404)\n        self.assertContains(response, \"Request URL:\", status_code=404)\n        self.assertContains(response, \"Raised by:\", status_code=404)\n", "    def test_no_tried_urls(self):\n        exception = Http404()\n        request = self.rf.get(\"/test_view/\")\n        response = technical_404_response(request, exception)\n        self.assertContains(response, \"No URLs found\", status_code=404)\n"], "sample_378": ["def test_bulk_update_on_model_with_custom_primary_key(self):\n    custom_pks = [\n        CustomPk.objects.create(name='pk-%s' % i, extra='')\n        for i in range(10)\n    ]\n    for model in custom_pks:\n        model.extra = 'extra-%s' % model.pk\n    CustomPk.objects.bulk_update(custom_pks, ['extra'], batch_size=5)\n    self.assertCountEqual(\n        CustomPk.objects.values_list('extra', flat=True),\n        [cat.extra for cat in custom_pks]\n    )\n", "def test_bulk_update_on_fields_with_db_column_name(self):\n    model = CustomDbColumn.objects.create(custom_column=1, custom_db_column='test')\n    model.custom_column = 2\n    model.custom_db_column = 'updated'\n    CustomDbColumn.objects.bulk_update([model], fields=['custom_column', 'custom_db_column'])\n    model.refresh_from_db()\n    self.assertEqual(model.custom_column, 2)\n    self.assertEqual(model.custom_db_column, 'updated')\n", "def test_bulk_update_unique_fields(self):\n    numbers = [Number.objects.create(num=i) for i in range(10)]\n    for number in numbers:\n        number.num += 1\n    Number.objects.bulk_update(numbers, ['num'], unique_fields=['num'])\n    self.assertCountEqual(Number.objects.filter(num__range=[1, 10]), numbers)\n", "def test_unique_fields_on_update(self):\n    custom_pks = [\n        CustomPk.objects.create(name='pk-%s' % i, extra='')\n        for i in range(10)\n    ]\n    for model in custom_pks:\n        model.extra = 'extra-%s' % model.pk\n    CustomPk.objects.bulk_update(custom_pks, ['extra'], unique_fields=['name'])\n    self.assertCountEqual(\n        CustomPk.objects.values_list('extra', flat=True),\n        [cat.extra for cat in custom_pks]\n    )\n", "def test_bulk_update_with_m2m(self):\n    member = Member.objects.create(name='test')\n    details = [\n        Detail.objects.create(data=str(i))\n        for i in range(10)\n    ]\n    member.details.set(details)\n    member.name = 'updated'\n    for detail in details:\n        detail.data = 'updated-%s' % detail.pk\n    with self.assertNumQueries(3):\n        Member.objects.bulk_update([member], ['name'])\n        Detail.objects.bulk_update(details, ['data'])\n    member.refresh_from_db()\n    self.assertEqual(member.name, 'updated')\n    self.assertCountEqual(\n        Detail.objects.filter(data__startswith='updated-'),\n        details\n    )\n"], "sample_379": ["def test_safe_string_addition(self):\n    \"\"\"\n    Test that adding two SafeStrings results in a new SafeString.\n    \"\"\"\n    s1 = mark_safe('a&b')\n    s2 = mark_safe('c&d')\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'a&bc&d')\n\n    # Test that adding a SafeString and a non-SafeString results in a non-SafeString.\n    s3 = 'e&f'\n    result = s1 + s3\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, 'a&be&f')\n", "def test_safe_string_addition(self):\n    safe_str = SafeString('Hello, ')\n    regular_str = 'world!'\n\n    result = safe_str + regular_str\n    self.assertIsInstance(result, str)\n    self.assertNotIsInstance(result, SafeString)\n\n    result = safe_str + SafeString(regular_str)\n    self.assertIsInstance(result, SafeString)\n", "def test_safe_string_addition(self):\n    safe_str = SafeString('Hello, ')\n    other_safe_str = SafeString('world!')\n    result = safe_str + other_safe_str\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'Hello, world!')\n\n    unsafe_str = 'unsafe string'\n    result = safe_str + unsafe_str\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, 'Hello, unsafe string')\n", "def test_safe_string_addition(self):\n    safe_str = SafeString('Hello, ')\n    other_safe_str = SafeString('world!')\n    regular_str = ' This is not safe.'\n\n    # Adding two safe strings should result in a safe string.\n    result = safe_str + other_safe_str\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'Hello, world!')\n\n    # Adding a safe string and a regular string should not be safe.\n    result = safe_str + regular_str\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, 'Hello,  This is not safe.')\n", "def test_safe_string_addition(self):\n    \"\"\"\n    Test that adding two SafeString instances results in a new SafeString.\n    \"\"\"\n    s1 = mark_safe('a&b')\n    s2 = mark_safe('c&d')\n\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'a&bc&d')\n"], "sample_380": ["def test_aggregation_with_filter_and_distinct(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__lt=4.0), distinct=True),\n    )\n    self.assertAlmostEqual(result['value'], Decimal('75.69'), places=2)\n", "def test_aggregation_filter_on_subquery_annotation(self):\n    subquery_qs = Author.objects.filter(\n        pk=OuterRef('pk'),\n        book__name=OuterRef('book__name'),\n    ).values('pk')\n    author_qs = Author.objects.annotate(\n        subquery_id=Subquery(subquery_qs),\n    ).filter(subquery_id__isnull=False).annotate(count=Count('book'))\n    self.assertEqual(author_qs.count(), Author.objects.count())\n", "def test_aggregate_over_subquery_annotation(self):\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).order_by('-pubdate').values('rating')[:1]\n    publishers = Publisher.objects.annotate(latest_rating=Subquery(subquery)).aggregate(avg_latest_rating=Avg('latest_rating'))\n    self.assertEqual(publishers['avg_latest_rating'], Approximate(4.08, places=2))\n", "def test_aggregate_over_subquery_annotation(self):\n    subquery = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publishers = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(subquery)\n    ).aggregate(max_pubdate=Max('latest_book_pubdate'))\n    self.assertEqual(publishers['max_pubdate'], datetime.date(2008, 12, 6))\n", "def test_aggregation_default_in_subquery(self):\n    subquery = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).annotate(\n        max_price=Max('price', default=0)\n    ).values('max_price')[:1]\n    queryset = Publisher.objects.annotate(max_book_price=Subquery(subquery))\n    self.assertEqual(queryset.get(pk=self.p1.pk).max_book_price, 30)\n"], "sample_381": ["def test_alter_field_with_choices(self):\n    \"\"\"Tests autodetection of altered fields with choices.\"\"\"\n    before = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, choices=[(\"A\", \"A\"), (\"B\", \"B\")])),\n    ])\n    after = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, choices=[(\"A\", \"A\"), (\"C\", \"C\")])),\n    ])\n    changes = self.get_changes([before], [after])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n", "def test_m2m_through_intermediate(self):\n    before = [\n        ModelState('app', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'B', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'AB', [\n            ('id', models.AutoField(primary_key=True)),\n            ('a', models.ForeignKey('app.A', models.CASCADE)),\n            ('b', models.ForeignKey('app.B', models.CASCADE)),\n        ]),\n        ModelState('app', 'C', [\n            ('id', models.AutoField(primary_key=True)),\n            ('ab', models.ManyToManyField('app.B', through='app.AB')),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'B', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'C', [\n            ('id', models.AutoField(primary_key=True)),\n            ('ab', models.ManyToManyField('app.B', through='app.AB')),\n        ]),\n        ModelState('app', 'AB', [\n            ('id', models.AutoField(primary_key=True)),\n            ('a', models.ForeignKey('app.A', models.CASCADE)),\n            ('c', models.ForeignKey('app.C', models.CASCADE)),\n            ('b', models.ForeignKey('app.B', models.CASCADE)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='ab', name='a')\n    self.assertEqual(\n        changes['app'][0].operations[0].field.remote_field.model,\n        'app.C',\n    )\n", "def test_add_mti_field_with_existing_field(self):\n    before = [\n        ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.CharField(max_length=10)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.CharField(max_length=10)),\n        ], bases=('otherapp.BaseModel',)),\n        ModelState('otherapp', 'BaseModel', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.CharField(max_length=10)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, name='base_model_ptr', model_name='model'\n    )\n", "def test_alter_field_with_unique(self):\n    changes = self.get_changes([self.author_name], [self.author_with_options])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, options={\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n    })\n\n    changes = self.get_changes([self.author_with_options], [self.author_name])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={})\n", "def test_alter_model_table_with_database_table_name(self):\n    author = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ], options={\"db_table\": \"myapp_author\"})\n    changes = self.get_changes([author], [self.author_with_db_table_options])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"author_one\")\n"], "sample_382": ["def test_get_template_directories_with_locmem_loader(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / 'templates',\n        }\n    )\n", "def test_get_template_directories_with_loader_without_get_dirs(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / 'templates',\n        }\n    )\n", "def test_reset_loaders_with_unresetable_loader(self):\n    with mock.patch('django.template.loaders.locmem.Loader.reset', side_effect=AttributeError):\n        autoreload.reset_loaders()  # Should not raise an exception\n", "def test_get_template_directories_loader_without_get_dirs(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            ROOT / 'templates_extra',\n            ROOT / 'templates',\n        }\n    )\n", "def test_template_changed_with_suffix(self, mock_reset):\n    template_path = Path(__file__).parent / 'templates' / 'index.py'\n    self.assertIsNone(autoreload.template_changed(None, template_path))\n    mock_reset.assert_not_called()\n"], "sample_383": ["    def test_ticket_24863(self):\n        # Test that F expressions in Q objects work with subqueries.\n        Tag.objects.create(name=\"foo\")\n        Annotation.objects.create(tag=Tag.objects.get(name=\"foo\"))\n        self.assertSequenceEqual(\n            Annotation.objects.filter(\n                tag__in=Tag.objects.filter(name=F(\"name\"))\n            ),\n            [Annotation.objects.get()],\n        )\n", "    def test_ticket_25085(self):\n        # Make sure that the ORM can handle a query with an Exists node\n        # containing a subquery that has more than one column in its select clause.\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Exists(RelatedIndividual.objects.values_list(\"id\", \"related\"))\n            ),\n            [i1],\n        )\n", "    def test_related_in_lookup_type(self):\n        # Create a NamedCategory instance with id=1\n        nc = NamedCategory.objects.create(id=1, name=\"test\")\n\n        # Create two DumbCategory instances, one with id matching the NamedCategory instance\n        dc1 = DumbCategory.objects.create(id=1)\n        dc2 = DumbCategory.objects.create(id=2)\n\n        # Use a related 'in' lookup with a list of model instances\n        self.assertSequenceEqual(\n            DumbCategory.objects.filter(namedcategory__in=[nc]), [dc1]\n        )\n\n        # Use a related 'in' lookup with a queryset\n        self.assertSequenceEqual(\n            DumbCategory.objects.filter(namedcategory__in=NamedCategory.objects.all()),\n            [dc1],\n        )\n", "    def test_ticket_24956(self):\n        \"\"\"\n        Ensure that filtering a subquery with an 'in' lookup doesn't cause a \n        FieldError when the subquery contains an annotation.\n        \"\"\"\n        note1 = Note.objects.create(note=\"note1\", misc=\"misc1\")\n        note2 = Note.objects.create(note=\"note2\", misc=\"misc2\")\n        Annotation.objects.create(note=note1, name=\"annotation1\")\n        Annotation.objects.create(note=note2, name=\"annotation2\")\n\n        notes = Note.objects.annotate(has_annotation=Exists(Annotation.objects.filter(note=OuterRef(\"pk\"))))\n        annotated_notes = Note.objects.filter(pk__in=notes.values_list(\"pk\", flat=True))\n        self.assertEqual(len(annotated_notes), 2)\n", "    def test_ticket_24748(self):\n        # Test filtering on a related field with a subquery that has an\n        # __in filter.\n        school1 = School.objects.create()\n        school2 = School.objects.create()\n        student1 = Student.objects.create(school=school1)\n        student2 = Student.objects.create(school=school2)\n        Classroom.objects.create(school=school1, name=\"Class 1\").students.add(\n            student1, student2\n        )\n        self.assertSequenceEqual(\n            Student.objects.filter(classroom__school__in=School.objects.all()),\n            [student1, student2],\n        )\n"], "sample_384": ["def test_bulk_update_on_inheritance_with_parent_link(self):\n    special_category = SpecialCategory.objects.create(name=\"test\", special_name=\"test\")\n    special_category.name = \"new\"\n    special_category.special_name = \"new_special\"\n    SpecialCategory.objects.bulk_update([special_category], fields=[\"name\", \"special_name\"])\n    special_category.refresh_from_db()\n    self.assertEqual(special_category.name, \"new\")\n    self.assertEqual(special_category.special_name, \"new_special\")\n", "def test_bulk_update_with_order_by(self):\n    notes = [\n        Note.objects.create(note=\"test_note\", misc=\"test_misc\") for _ in range(10)\n    ]\n    for i, note in enumerate(notes):\n        note.note = f\"note_{i}\"\n    with self.assertNumQueries(1):\n        Note.objects.order_by(\"id\").bulk_update(notes, [\"note\"])\n    self.assertCountEqual(\n        Note.objects.values_list(\"note\", flat=True),\n        [f\"note_{i}\" for i in range(10)],\n    )\n", "def test_bulk_update_with_nullable_fields(self):\n    articles = [\n        Article.objects.create(name=str(i), created=datetime.datetime.today())\n        for i in range(10)\n    ]\n    for article in articles:\n        article.name = None\n    Article.objects.bulk_update(articles, [\"name\"])\n    self.assertCountEqual(Article.objects.filter(name__isnull=True), articles)\n", "def test_bulk_update_related_objects_with_different_ids(self):\n    related1 = RelatedObject.objects.create()\n    related2 = RelatedObject.objects.create()\n    single = SingleObject.objects.create(related=related1)\n    single.related = related2\n    SingleObject.objects.bulk_update([single], fields=[\"related\"])\n    single.refresh_from_db()\n    self.assertEqual(single.related, related2)\n", "def test_bulk_update_with_expressions(self):\n    notes = [\n        Note.objects.create(note=\"test_note\", misc=\"test_misc\") for _ in range(10)\n    ]\n    for note in notes:\n        note.misc = F(\"note\") + \" updated\"\n    Note.objects.bulk_update(notes, [\"misc\"])\n    self.assertCountEqual(\n        Note.objects.filter(misc=\"test_note updated\"), notes\n    )\n"], "sample_385": ["def test_build_attrs_with_data_app_label(self):\n    form = AlbumForm()\n    attrs = form[\"band\"].field.widget.get_context(\n        name=\"my_field\", value=None, attrs={}\n    )[\"widget\"][\"attrs\"]\n    self.assertEqual(attrs[\"data-app-label\"], \"admin_widgets\")\n\n    # Test with a proxy model.\n    class ProxyBand(Band):\n        class Meta:\n            proxy = True\n\n    proxy_band_field = Album._meta.get_field(\"band\")\n    proxy_band_field.remote_field.model = ProxyBand\n    w = AutocompleteSelect(proxy_band_field, admin.site)\n    attrs = w.build_attrs({})\n    self.assertEqual(attrs[\"data-app-label\"], \"admin_widgets\")\n", "def test_widget_media_css(self):\n    rel = Album._meta.get_field(\"band\").remote_field\n    expected_css = {\n        \"screen\": [\n            \"admin/css/vendor/select2/select2.min.css\",\n            \"admin/css/autocomplete.css\",\n        ]\n    }\n    self.assertEqual(AutocompleteSelect(rel, admin.site).media._css, expected_css)\n", "def test_autocomplete_mixin_url_parameters(self):\n    rel = Album._meta.get_field(\"band\")\n    w = AutocompleteSelect(rel, admin.site)\n    url_params = w.url_parameters()\n    self.assertEqual(url_params, {\"_to_field\": \"uuid\"})\n", "def test_render_options_with_non_ascii_characters(self):\n    \"\"\"Options are correctly rendered with non-ASCII characters.\"\"\"\n    band = Band.objects.create(name=\"Mot\u00f6rhead\", style=\"rock\")\n    form = AlbumForm(initial={\"band\": band.uuid})\n    output = form.as_table()\n    selected_option = (\n        '<option value=\"%s\" selected>Mot\u00f6rhead</option>' % band.uuid\n    )\n    self.assertIn(selected_option, output)\n", "def test_build_attrs_with_custom_language(self):\n    form = AlbumForm()\n    with translation.override(\"fr\"):\n        attrs = form[\"band\"].field.widget.get_context(\n            name=\"my_field\", value=None, attrs={}\n        )[\"widget\"][\"attrs\"]\n    self.assertEqual(attrs[\"lang\"], \"fr\")\n"], "sample_386": ["def test_safe_string_addition(self):\n    \"\"\"\n    Test that adding two SafeString instances results in another SafeString.\n    \"\"\"\n    s1 = mark_safe(\"a&b\")\n    s2 = mark_safe(\"c&d\")\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&bc&d\")\n\n    # Test adding a SafeString to a regular string\n    result = s1 + \"e&f\"\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&be&f\")\n", "def test_safe_string_concatenation(self):\n    safe_str1 = mark_safe(\"<p>Hello</p>\")\n    safe_str2 = mark_safe(\"<p>World!</p>\")\n\n    concatenated_str = safe_str1 + safe_str2\n    self.assertIsInstance(concatenated_str, SafeString)\n    self.assertRenderEqual(\"{{ s }}\", \"<p>Hello</p><p>World!</p>\", s=concatenated_str)\n\n    unsafe_str = \"<script>alert('XSS')</script>\"\n    concatenated_str = safe_str1 + unsafe_str\n    self.assertNotIsInstance(concatenated_str, SafeString)\n    self.assertRenderEqual(\"{{ s }}\", \"<p>Hello</p>&lt;script&gt;alert(&#x27;XSS&#x27;)&lt;/script&gt;\", s=concatenated_str)\n", "def test_safe_string_addition(self):\n    s1 = mark_safe(\"a&b\")\n    s2 = mark_safe(\"<c>\")\n\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(str(result), \"a&b<c>\")\n", "def test_safe_string_addition(self):\n    safe_s = SafeString(\"a&b\")\n    self.assertIsInstance(safe_s + \"c\", SafeString)\n    self.assertIsInstance(safe_s + SafeString(\"c\"), SafeString)\n    self.assertNotIsInstance(safe_s + \"<c>\", SafeString)\n", "def test_safe_string_addition(self):\n    s1 = SafeString(\"a&b\")\n    s2 = SafeString(\"<c>d\")\n\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&b<c>d\")\n\n    result = s1 + \"unsafe string\"\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&bunsafe string\")\n\n    result = \"unsafe string\" + s1\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"unsafe stringa&b\")\n"], "sample_388": ["    def test_configure_user(self):\n        user = User.objects.create(username=\"knownuser\")\n        request = self.client.get(\"/remote_user/\", **{self.header: self.known_user}).wsgi_request\n        backend = RemoteUserBackend()\n        configured_user = backend.configure_user(request, user)\n        self.assertEqual(configured_user, user)\n", "    def test_has_perm(self):\n        user = User.objects.create_user('test', 'test@example.com')\n        self.assertFalse(user.is_superuser)\n        self.assertFalse(user.has_perm('nonexistent.perm'))\n\n        permission = Permission.objects.get(codename='add_user')\n        user.user_permissions.add(permission)\n\n        self.assertTrue(user.has_perm('auth.add_user'))\n        self.assertFalse(user.has_perm('auth.change_user'))\n\n        user.is_superuser = True\n        user.save()\n        self.assertTrue(user.has_perm('auth.add_user'))\n        self.assertTrue(user.has_perm('auth.change_user'))\n", "    def test_inactive_user(self):\n        user = User.objects.create(username=\"knownuser\", is_active=False)\n        user.set_password(\"password\")\n        user.save()\n        self.client.login(username=\"knownuser\", password=\"password\")\n        response = self.client.get(\"/remote_user/\")\n        self.assertEqual(response.context[\"user\"].username, user.username)\n", "    def test_middleware_rejects_invalid_header_names(self):\n        middleware = RemoteUserMiddleware()\n        with self.assertRaises(ValueError):\n            middleware.process_request(request=HttpRequest(), header=\"Invalid Header\")\n", "    def test_has_perm(self):\n        # Create a user with no permissions.\n        user = User.objects.create(username='testuser')\n\n        # The user has no permissions, so this should return False.\n        self.assertFalse(ModelBackend().has_perm(user, 'auth.test_permission'))\n\n        # Give the user a permission.\n        user.user_permissions.create(codename='test_permission', content_type__app_label='auth')\n\n        # Now the user has a permission, so this should return True.\n        self.assertTrue(ModelBackend().has_perm(user, 'auth.test_permission'))\n"], "sample_390": ["def test_was_modified_since_overflow(self):\n    \"\"\"\n    was_modified_since handles timestamps that are close to the maximum Unix time.\n    \"\"\"\n    max_unix_time = 2**31 - 1\n    self.assertFalse(was_modified_since(http_date(max_unix_time), max_unix_time))\n    self.assertTrue(was_modified_since(http_date(max_unix_time), max_unix_time + 1))\n", "def test_was_modified_since_invalid_header_format(self):\n    \"\"\"\n    Test that was_modified_since returns True for an invalid header format.\n    \"\"\"\n    mtime = 1343416141\n    header = \"Invalid header format\"\n    self.assertTrue(was_modified_since(header, mtime))\n", "def test_was_modified_since_overflow(self):\n    \"\"\"\n    An overflow error in the If-Modified-Since header does not disturb was_modified_since.\n    \"\"\"\n    mtime = 1\n    header = \"Thu, 01 Jan 1970 00:00:00 GMT; length=99999999999999999999\"\n    self.assertTrue(was_modified_since(header, mtime))\n", "def test_was_modified_since_invalid_header(self):\n    \"\"\"\n    Test was_modified_since with an invalid header (#18675).\n    \"\"\"\n    mtime = 1343416141\n    header = \"Invalid Header\"\n    self.assertTrue(was_modified_since(header, mtime))\n\n    header = \"Wed, 21-Jan-2015 07:28:00 GMT; length=12345\"  # Invalid length\n    self.assertTrue(was_modified_since(header, mtime))\n\n    header = \"Wed, 21-Jan-2015 07:28:00 GMT; length=\"  # Missing length value\n    self.assertTrue(was_modified_since(header, mtime))\n", "def test_was_modified_since_overflow_error(self):\n    \"\"\"\n    An OverflowError in was_modified_since does not propagate and the function \n    returns True instead (#33348).\n    \"\"\"\n    self.assertTrue(was_modified_since(header=\"Mon, 18 Jan 2038 05:14:07 GMT\", mtime=2**63-1))\n"], "sample_391": ["def test_create_alter_model_table(self):\n    \"\"\"\n    AlterModelTable should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", fields=[]),\n            migrations.AlterModelTable(\"Foo\", \"myapp_foo\"),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", fields=[], options={\"db_table\": \"myapp_foo\"}),\n        ],\n    )\n", "def test_optimize_add_index_remove_index(self):\n    \"\"\"\n    AddIndex and RemoveIndex should collapse into nothing.\n    \"\"\"\n    index = models.Index(fields=[\"name\"], name=\"foo_name_idx\")\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AddIndex(\"Foo\", index),\n            migrations.RemoveIndex(\"Foo\", \"foo_name_idx\"),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n        ],\n    )\n", "def test_optimize_through_alter_index(self):\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"])),\n            migrations.AlterIndexTogether(\"Foo\", [[\"name\"]]),\n            migrations.RemoveIndex(\"Foo\", models.Index(fields=[\"name\"])),\n            migrations.DeleteModel(\"Foo\"),\n        ],\n        [],\n    )\n", "def test_create_model_add_index(self):\n    \"\"\"\n    AddIndex should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", fields=[(\"name\", models.CharField(max_length=255))]),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"], name=\"foo_name_idx\")),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"indexes\": [models.Index(fields=[\"name\"], name=\"foo_name_idx\")]},\n            ),\n        ],\n    )\n", "def test_create_model_add_constraint(self):\n    \"\"\"\n    AddConstraint should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    constraint = models.CheckConstraint(check=models.Q(age__gte=18), name=\"age_gte_18\")\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                    (\"age\", models.IntegerField()),\n                ],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AddConstraint(\"Foo\", constraint),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                    (\"age\", models.IntegerField()),\n                ],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n                constraints=[constraint],\n            ),\n        ],\n    )\n"], "sample_392": ["def test_key_transform_with_none_value(self):\n    obj = NullableJSONModel.objects.create(value={\"a\": None})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__isnull=True),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a=None),\n        [obj],\n    )\n", "    def test_default_value(self):\n        model = JSONModel(value=None)\n        self.assertIsNone(model.value)\n", "def test_key_transform_on_textfield(self):\n    class TextJSONModel(models.Model):\n        value = models.TextField()\n\n    obj = TextJSONModel.objects.create(value='{\"a\": \"b\"}')\n    self.assertEqual(\n        list(\n            TextJSONModel.objects.annotate(\n                key=KeyTransform(\"a\", \"value\"),\n            ).values_list(\"key\", flat=True)\n        ),\n        [\"b\"],\n    )\n", "def test_key_transform_raw_expression_nested(self):\n    expr = RawSQL(self.raw_sql, ['{\"x\": {\"y\": \"bar\"}}'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform(\"y\", KeyTransform(\"x\", expr))),\n        [self.objs[7]],\n    )\n", "    def test_adding_jsonfield(self):\n        # Create the table without the JSONField.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(NullableJSONModelWithoutJSONField)\n\n        # Add the JSONField in a new migration.\n        class Migration(migrations.Migration):\n            initial = True\n\n            operations = [\n                migrations.AddField(\n                    model_name=\"nullablejsonmodelwithoutjsonfield\",\n                    name=\"value\",\n                    field=models.JSONField(null=True),\n                ),\n            ]\n\n        with connection.schema_editor() as schema_editor:\n            migration = Migration(\"add_json_field\", \"tests\")\n            migration.apply(schema_editor, collect_sql=False)\n\n        # Ensure the JSONField was added correctly.\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT value FROM nullablejsonmodelwithoutjsonfield\")\n            self.assertIsNone(cursor.fetchone()[0])\n"], "sample_393": ["    def test_translatable_file_repr(self):\n        translatable_file = MakeMessagesCommand.translatable_file_class(\n            \"/path/to/dir\", \"file.txt\", \"/path/to/locale\"\n        )\n        self.assertEqual(\n            repr(translatable_file), \"<TranslatableFile: /path/to/dir/file.txt>\"\n        )\n", "    def test_no_obsolete_enabled(self):\n        management.call_command(\n            \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n        )\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn(\"#~ msgid \\\"Hello, world!\\\"\", po_contents)\n", "    def test_is_templatized(self):\n        command = MakeMessagesCommand()\n        command.gettext_version = (0, 18, 3)\n        translatable = TranslatableFile(\"path\", \"file.html\", \"locale\")\n        build_file = BuildFile(command, \"django\", translatable)\n        self.assertTrue(build_file.is_templatized)\n\n        build_file = BuildFile(command, \"djangojs\", translatable)\n        self.assertFalse(build_file.is_templatized)\n\n        command.gettext_version = (0, 18, 4)\n        build_file = BuildFile(command, \"djangojs\", translatable)\n        self.assertTrue(build_file.is_templatized)\n\n        build_file = BuildFile(command, \"django\", translatable)\n        self.assertTrue(build_file.is_templatized)\n\n        translatable = TranslatableFile(\"path\", \"file.py\", \"locale\")\n        build_file = BuildFile(command, \"django\", translatable)\n        self.assertFalse(build_file.is_templatized)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command(\n            \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n        )\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn(\"#~ msgid \\\"obsolete message\\\"\", po_contents)\n", "    def test_is_templatized(self):\n        cmd = MakeMessagesCommand()\n        cmd.gettext_version = (0, 18, 3)\n        bf = cmd.build_file_class(cmd, \"djangojs\", cmd.translatable_file_class(\"\", \"\", \"\"))\n        self.assertTrue(bf.is_templatized)\n\n        cmd.gettext_version = (1, 2, 3)\n        bf = cmd.build_file_class(cmd, \"djangojs\", cmd.translatable_file_class(\"\", \"\", \"\"))\n        self.assertFalse(bf.is_templatized)\n\n        bf = cmd.build_file_class(cmd, \"django\", cmd.translatable_file_class(\"\", \"file.py\", \"\"))\n        self.assertFalse(bf.is_templatized)\n\n        bf = cmd.build_file_class(cmd, \"django\", cmd.translatable_file_class(\"\", \"file.html\", \"\"))\n        self.assertTrue(bf.is_templatized)\n"], "sample_395": ["def test_reset_all_loaders_with_locmem_loader(self):\n    with mock.patch(\"django.template.loaders.base.Loader.reset\") as mock_reset:\n        autoreload.reset_loaders()\n        self.assertEqual(mock_reset.call_count, 2)\n", "def test_get_template_directories_with_locmem_loader(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / \"templates\",\n        },\n    )\n", "def test_get_template_directories_with_locmem_loader(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            ROOT / \"templates_extra\",\n            ROOT / \"templates\",\n        },\n    )\n", "    def test_get_template_directories_custom_loader(self):\n        # Custom loader has a get_dirs method.\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                EXTRA_TEMPLATES_DIR,\n                ROOT / \"templates\",\n                Path.cwd() / \"custom_template_dir\",\n            },\n        )\n", "def test_get_template_directories_ignore_loader_without_get_dirs(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / \"templates\",\n        },\n    )\n"], "sample_397": ["def test_get_default(self):\n    \"\"\"\n    Test that Engine.get_default() returns the first DjangoTemplates backend\n    that's configured, or raises ImproperlyConfigured if none are configured.\n    \"\"\"\n    templates = [\n        {\n            \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n            \"OPTIONS\": {},\n        }\n    ]\n    engines = EngineHandler(templates=templates)\n    default_engine = Engine.get_default()\n    self.assertEqual(default_engine, engines[\"django\"].engine)\n\n    # Test that ImproperlyConfigured is raised if no DjangoTemplates backends\n    # are configured.\n    templates = [\n        {\n            \"BACKEND\": \"django.template.backends.jinja2.Jinja2\",\n            \"OPTIONS\": {},\n        }\n    ]\n    engines = EngineHandler(templates=templates)\n    with self.assertRaisesMessage(\n        ImproperlyConfigured, \"No DjangoTemplates backend is configured.\"\n    ):\n        Engine.get_default()\n", "def test_find_template_loader(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"loaders\": [\n                    \"django.template.loaders.filesystem.Loader\",\n                    (\"django.template.loaders.cached.Loader\", [\n                        \"django.template.loaders.app_directories.Loader\"\n                    ]),\n                ],\n            },\n        }\n    )\n\n    # Test single loader as string\n    loader = engine.engine.find_template_loader(\"django.template.loaders.filesystem.Loader\")\n    self.assertIsNotNone(loader)\n\n    # Test loader with args\n    cached_loader = engine.engine.find_template_loader((\"django.template.loaders.cached.Loader\", [\n        \"django.template.loaders.app_directories.Loader\"\n    ]))\n    self.assertIsNotNone(cached_loader)\n\n    # Test invalid loader\n    with self.assertRaises(ImproperlyConfigured):\n        engine.engine.find_template_loader(123)\n", "def test_get_default(self):\n    # Clear the cache to ensure get_default() is called\n    Engine.get_default.cache_clear()\n    \n    # Test that get_default returns the first DjangoTemplates backend\n    templates = [\n        {\n            \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n            \"OPTIONS\": {},\n        },\n        {\n            \"BACKEND\": \"django.template.backends.jinja2.Jinja2\",\n            \"OPTIONS\": {},\n        },\n    ]\n    engines = EngineHandler(templates=templates)\n    self.assertEqual(Engine.get_default(), engines[\"django\"].engine)\n\n    # Test that ImproperlyConfigured is raised when no DjangoTemplates backends are configured\n    templates = [\n        {\n            \"BACKEND\": \"django.template.backends.jinja2.Jinja2\",\n            \"OPTIONS\": {},\n        },\n    ]\n    EngineHandler(templates=templates)\n    with self.assertRaisesMessage(\n        ImproperlyConfigured, \"No DjangoTemplates backend is configured.\"\n    ):\n        Engine.get_default()\n", "def test_find_template_loader(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"loaders\": [\n                    (\"django.template.loaders.cached.Loader\", [\n                        \"django.template.loaders.filesystem.Loader\",\n                    ]),\n                ],\n            },\n        }\n    )\n    loader = engine.engine.find_template_loader(engine.engine.loaders[0])\n    self.assertIsNotNone(loader)\n    self.assertIsInstance(loader, engine.engine.loaders[0][0])\n    invalid_loader = (\"invalid.loader\", ())\n    with self.assertRaises(ImproperlyConfigured) as cm:\n        engine.engine.find_template_loader(invalid_loader)\n    self.assertEqual(str(cm.exception), f\"Invalid value in template loaders configuration: {invalid_loader!r}\")\n", "def test_get_default(self):\n    \"\"\"\n    Test that Engine.get_default() returns the first configured DjangoTemplates\n    backend.\n    \"\"\"\n    templates = [\n        {\n            \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n            \"OPTIONS\": {\"autoescape\": False},\n        },\n        {\n            \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n            \"OPTIONS\": {\"autoescape\": True},\n        },\n    ]\n    engines = EngineHandler(templates=templates)\n    default_engine = Engine.get_default()\n    self.assertEqual(default_engine, engines[\"django\"].engine)\n"], "sample_398": ["    def test_logout_view_dispatch(self):\n        # Test that LogoutView.dispatch requires a POST request.\n        logout_url = reverse(\"logout\")\n\n        # GET request should raise an exception or log a warning in Django 5.0+.\n        with self.assertWarnsMessage(RemovedInDjango50Warning, \"Log out via GET requests is deprecated\"):\n            response = self.client.get(logout_url)\n        self.assertEqual(response.status_code, 200)\n\n        # HEAD request should raise an exception or log a warning in Django 5.0+.\n        with self.assertWarnsMessage(RemovedInDjango50Warning, \"Log out via GET requests is deprecated\"):\n            response = self.client.head(logout_url)\n        self.assertEqual(response.status_code, 200)\n\n        # PUT, DELETE, etc. requests should return 405 Method Not Allowed.\n        response = self.client.put(logout_url)\n        self.assertEqual(response.status_code, 405)\n\n        response = self.client.delete(logout_url)\n        self.assertEqual(response.status_code, 405)\n", "    def test_password_reset_done_view_render(self):\n        response = self.client.get(\"/password_reset/done/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/password_reset_done.html\")\n", "    def test_login_view_redirect_authenticated_user(self):\n        self.login()\n        response = self.client.get(\"/login/redirect_authenticated_user/\")\n        self.assertRedirects(\n            response, \"/accounts/profile/\", fetch_redirect_response=False\n        )\n", "    def test_next_url_with_query_params(self):\n        response = self.client.post(\n            \"/login/\",\n            {\n                \"username\": \"testclient\",\n                \"password\": \"password\",\n                \"next\": \"/path/?param1=value1&param2=value2\",\n            },\n        )\n        self.assertRedirects(response, \"/path/?param1=value1&param2=value2\", fetch_redirect_response=False)\n", "    def test_get_success_url_allowed_hosts(self):\n        mixin = SuccessURLAllowedHostsMixin()\n        mixin.request = HttpRequest()\n        mixin.request.get_host = lambda: \"testserver\"\n        mixin.success_url_allowed_hosts = {\"otherserver\"}\n        self.assertEqual(\n            mixin.get_success_url_allowed_hosts(),\n            {\"testserver\", \"otherserver\"},\n        )\n"], "sample_399": ["def test_aggregation_with_duration_field(self):\n    # Test aggregation with DurationField\n    result = Publisher.objects.aggregate(Sum(\"duration\"))\n    self.assertEqual(result[\"duration__sum\"], datetime.timedelta(days=3))\n", "def test_aggregate_over_unsupported_expression(self):\n    msg = \"Cannot compute Sum('id'): 'id' is an aggregate\"\n    with self.assertRaisesMessage(FieldError, msg):\n        Book.objects.annotate(Max(\"id\")).annotate(Sum(\"id__max\"))\n\n    class MyMax(Max):\n            self.set_source_expressions(self.get_source_expressions()[0:1])\n            return super().as_sql(compiler, connection)\n\n    with self.assertRaisesMessage(\n        FieldError, \"Cannot compute Max('id__max'): 'id__max' is an aggregate\"\n    ):\n        Book.objects.annotate(Max(\"id\")).annotate(my_max=MyMax(\"id__max\", \"price\"))\n", "def test_aggregate_with_output_field(self):\n    result = Book.objects.aggregate(\n        value=Sum(\"price\", output_field=IntegerField())\n    )\n    self.assertIsInstance(result[\"value\"], int)\n\n    result = Book.objects.aggregate(\n        value=Avg(\"price\", output_field=IntegerField())\n    )\n    self.assertIsInstance(result[\"value\"], int)\n", "def test_annotate_with_subquery_and_distinct(self):\n    subquery = Book.objects.filter(publisher=OuterRef(\"pk\")).order_by(\"-pubdate\")[:1]\n    publishers = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(subquery.values(\"pubdate\"))\n    ).distinct()\n    self.assertEqual(len(publishers), len(Publisher.objects.all()))\n", "def test_filtering_on_annotation_with_subquery(self):\n    subquery = Book.objects.filter(\n        publisher=OuterRef(\"pk\"),\n        price__gt=Decimal(\"30.00\"),\n    ).values(\"publisher\")\n    publishers = Publisher.objects.annotate(\n        has_expensive_book=Exists(subquery),\n    ).filter(has_expensive_book=True)\n    self.assertEqual(publishers.count(), 2)\n"], "sample_401": ["def test_cleaned_data_contains_all_fields(self):\n    \"\"\"\n    The cleaned_data of a formset should include all fields, even if the field\n    has no value.\n    \"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"1\",\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.cleaned_data,\n        [{\"choice\": \"Zero\", \"votes\": \"\"}, {\"choice\": \"One\", \"votes\": 1}],\n    )\n", "def test_absolute_max_with_min_num(self):\n    data = {\n        \"form-TOTAL_FORMS\": \"3\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-MIN_NUM_FORMS\": \"2\",  # min number of forms\n        \"form-MAX_NUM_FORMS\": \"1000\",  # max number of forms\n        \"form-0-name\": \"Zero\",\n        \"form-1-name\": \"One\",\n        \"form-2-name\": \"Two\",\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        min_num=2,\n        absolute_max=3,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data)\n    self.assertIs(formset.is_valid(), True)\n    self.assertEqual(len(formset.forms), 3)\n", "def test_empty_formset_total_error_count(self):\n    \"\"\"An empty formset has 0 total errors.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data={\"choices-TOTAL_FORMS\": \"0\", \"choices-INITIAL_FORMS\": \"0\"})\n    self.assertEqual(formset.total_error_count(), 0)\n", "def test_total_error_count_with_deleted_forms(self):\n    \"\"\"Total error count should not include deleted forms.\"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"\",\n        \"choices-1-DELETE\": \"on\",\n    }\n    ChoiceFormSet = formset_factory(Choice, can_delete=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertEqual(formset.total_error_count(), 1)\n", "def test_formset_template_name(self):\n    class CustomFormSet(BaseFormSet):\n        template_name = \"a/custom/formset/template.html\"\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomFormSet)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"a/custom/formset/template.html\")\n\n    # Setting renderer should have no effect on the template_name attribute.\n    class CustomRenderer(TemplatesSetting):\n        formset_template_name = \"another/custom/formset/template.html\"\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomFormSet, renderer=CustomRenderer)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"a/custom/formset/template.html\")\n"], "sample_402": ["def test_prepend_www_append_slash_slashless_unknown(self):\n    \"\"\"\n    PREPEND_WWW and APPEND_SLASH should not redirect to unknown resources.\n    \"\"\"\n    request = self.rf.get(\"/unknown\")\n    response = CommonMiddleware(get_response_404)(request)\n    self.assertEqual(response.status_code, 404)\n", "def test_prepend_www_append_slash_slashless_custom_urlconf_with_query_string(self):\n    request = self.rf.get(\"/customurlconf/slash?test=1\")\n    request.urlconf = \"middleware.extra_urls\"\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/customurlconf/slash/?test=1\")\n", "def test_content_length_header_added_for_file_response(self):\n        response = FileResponse(BytesIO(b\"content\"))\n        self.assertNotIn(\"Content-Length\", response)\n        return response\n\n    response = CommonMiddleware(get_response)(self.rf.get(\"/\"))\n    self.assertEqual(int(response.headers[\"Content-Length\"]), len(response.content))\n", "def test_append_slash_redirect_with_query_string_and_fragment(self):\n    \"\"\"\n    APPEND_SLASH should preserve query strings and fragments when redirecting.\n    \"\"\"\n    request = self.rf.get(\"/slash?test=1#fragment\")\n    resp = CommonMiddleware(get_response_404)(request)\n    self.assertEqual(resp.url, \"/slash/?test=1#fragment\")\n", "def test_prepend_www_append_slash_redirect_querystring(self):\n    \"\"\"\n    Both settings enabled should result in a single redirect with both the www\n    subdomain and a trailing slash.\n    \"\"\"\n    request = self.rf.get(\"/slash?test=1\")\n    resp = CommonMiddleware(get_response_404)(request)\n    self.assertEqual(resp.url, \"http://www.testserver/slash/?test=1\")\n"], "sample_403": ["def test_rename_model_with_m2m_through_field(self):\n    app_label = \"test_rename_model_with_m2m_through_field\"\n\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"PonyRider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"rider\",\n                        models.ForeignKey(\"test_rename_model_with_m2m_through_field.Rider\", models.CASCADE),\n                    ),\n                    (\n                        \"pony\",\n                        models.ForeignKey(\"test_rename_model_with_m2m_through_field.Pony\", models.CASCADE),\n                    ),\n                ],\n            ),\n            migrations.AddField(\n                \"Pony\",\n                \"riders\",\n                models.ManyToManyField(\n                    \"test_rename_model_with_m2m_through_field.Rider\",\n                    through=\"test_rename_model_with_m2m_through_field.PonyRider\",\n                ),\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    PonyRider = project_state.apps.get_model(app_label, \"PonyRider\")\n    pony = Pony.objects.create()\n    rider = Rider.objects.create()\n    PonyRider.objects.create(pony=pony, rider=rider)\n\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameModel(\"Pony\", \"Pony2\"),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"Pony2\")\n    Rider = project_state.apps.get_model(app_label, \"Rider\")\n    PonyRider = project_state.apps.get_model(app_label, \"PonyRider\")\n    pony = Pony.objects.first()\n    rider = Rider.objects.create()\n    PonyRider.objects.create(pony=pony, rider=rider)\n    self.assertEqual(Pony.objects.count(), 1)\n    self.assertEqual(Rider.objects.count(), 2)\n    self.assertEqual(PonyRider.objects.count(), 2)\n    self.assertEqual(pony.riders.count(), 2)\n", "def test_rename_field_references_to_self(self):\n    project_state = self.set_up_test_model(\"test_rnflrtos\", related_model=True)\n    operation = migrations.RenameField(\n        model_name=\"Rider\",\n        old_name=\"friend\",\n        new_name=\"buddy\",\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_rnflrtos\", new_state)\n    Rider = new_state.apps.get_model(\"test_rnflrtos\", \"rider\")\n    self.assertIs(Rider._meta.get_field(\"buddy\").remote_field.model, Rider)\n", "def test_rename_field_with_self_referential_m2m_through(self):\n    app_label = \"test_rename_field_with_self_referential_m2m_through\"\n\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"ReflexivePony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"ReflexivePonyThrough\",\n                fields=[\n                    (\n                        \"from_pony\",\n                        models.ForeignKey(\n                            \"%s.ReflexivePony\" % app_label, models.CASCADE\n                        ),\n                    ),\n                    (\n                        \"to_pony\",\n                        models.ForeignKey(\n                            \"%s.ReflexivePony\" % app_label, models.CASCADE\n                        ),\n                    ),\n                ],\n            ),\n            migrations.AddField(\n                \"ReflexivePony\",\n                \"ponies\",\n                models.ManyToManyField(\n                    \"self\",\n                    through=\"%s.ReflexivePonyThrough\" % app_label,\n                ),\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"ReflexivePony\")\n    pony = Pony.objects.create()\n    PonyThrough = project_state.apps.get_model(app_label, \"ReflexivePonyThrough\")\n    PonyThrough.objects.create(from_pony=pony, to_pony=pony)\n\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameField(\"ReflexivePonyThrough\", \"from_pony\", \"from_pony_2\"),\n        ],\n    )\n    PonyThrough = project_state.apps.get_model(app_label, \"ReflexivePonyThrough\")\n    self.assertEqual(PonyThrough._meta.get_field(\"from_pony_2\").remote_field.model, Pony)\n", "def test_create_model_with_constraint_name(self):\n    project_state = ProjectState()\n    operation = migrations.CreateModel(\n        \"Pony\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"pink\", models.IntegerField(default=3)),\n        ],\n        options={\n            \"constraints\": [\n                models.CheckConstraint(\n                    check=models.Q(pink__gt=2), name=\"test_constraint_pony_pink_gt_2\"\n                )\n            ]\n        },\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_crmo\", new_state)\n    self.assertEqual(len(new_state.models[\"test_crmo\", \"pony\"].options[\"constraints\"]), 1)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_crmo\", editor, project_state, new_state)\n    self.assertTableExists(\"test_crmo_pony\")\n    Pony = new_state.apps.get_model(\"test_crmo\", \"Pony\")\n    Pony.objects.create(pink=4)\n    with self.assertRaises(IntegrityError):\n        Pony.objects.create(pink=1)\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_crmo\", editor, new_state, project_state)\n    self.assertTableNotExists(\"test_crmo_pony\")\n    # Test deconstruction\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"CreateModel\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(\n        definition[2][\"options\"][\"constraints\"][0].name,\n        \"test_constraint_pony_pink_gt_2\",\n    )\n", "def test_references_field_by_mti_parent_ptr(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.OneToOneField(\n            \"Parent\", models.CASCADE, primary_key=True, parent_link=True\n        ),\n    )\n    self.assertIs(operation.references_field(\"Parent\", \"whatever\", \"migrations\"), True)\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n"], "sample_404": ["def test_variable_node_repr(self):\n    engine = self._engine()\n    template = engine.from_string('{{ foo }}')\n    node = template.nodelist[0]\n    self.assertEqual(repr(node), \"<Variable Node: foo>\")\n", "def test_variable_resolve_exception_suppression(self):\n    \"\"\"\n    Variable.resolve() should suppress some exceptions (like AttributeError and\n    KeyError), but not others.\n    \"\"\"\n    class RaisesKeyError:\n            raise KeyError(key)\n\n    class RaisesAttributeError:\n            raise AttributeError(name)\n\n    class RaisesOtherException:\n            raise IndexError(key)\n\n            raise IndexError(name)\n\n    template = self._engine().from_string(\"{{ var.key }}\")\n    context = Context({\"var\": RaisesKeyError()})\n    self.assertEqual(template.render(context), \"\")\n\n    template = self._engine().from_string(\"{{ var.key }}\")\n    context = Context({\"var\": RaisesAttributeError()})\n    self.assertEqual(template.render(context), \"\")\n\n    template = self._engine().from_string(\"{{ var.key }}\")\n    context = Context({\"var\": RaisesOtherException()})\n    with self.assertRaises(IndexError):\n        template.render(context)\n", "def test_variable_resolve(self):\n    template = self._engine().from_string(\"{{ var }}\")\n    context = Context({\"var\": \"test\"})\n    output = template.render(context)\n    self.assertEqual(output, \"test\")\n\n    # Test variable resolution with a dot\n    template = self._engine().from_string(\"{{ var.attr }}\")\n    context = Context({\"var\": {\"attr\": \"test\"}})\n    output = template.render(context)\n    self.assertEqual(output, \"test\")\n\n    # Test variable resolution with a callable\n    template = self._engine().from_string(\"{{ var }}\")\n    context = Context({\"var\": lambda: \"test\"})\n    output = template.render(context)\n    self.assertEqual(output, \"test\")\n\n    # Test variable resolution with a filter\n    template = self._engine().from_string(\"{{ var|upper }}\")\n    context = Context({\"var\": \"test\"})\n    output = template.render(context)\n    self.assertEqual(output, \"TEST\")\n\n    # Test variable resolution with a filter that takes an argument\n    template = self._engine().from_string(\"{{ var|cut:'t' }}\")\n    context = Context({\"var\": \"test\"})\n    output = template.render(context)\n    self.assertEqual(output, \"es\")\n", "def test_template_syntax_error_message(self):\n    \"\"\"\n    Test that TemplateSyntaxError messages include the correct line number and\n    source snippet.\n    \"\"\"\n    engine = self._engine()\n    template_string = \"{% if 1 %}{{ foo@bar }}{% endif %}\"\n    try:\n        engine.from_string(template_string)\n    except TemplateSyntaxError as e:\n        if self.debug_engine:\n            self.assertIn(\"line 1\", str(e))\n            self.assertIn(template_string, str(e))\n        else:\n            self.assertNotIn(\"line 1\", str(e))\n            self.assertNotIn(template_string, str(e))\n    else:\n        self.fail(\"Expected TemplateSyntaxError\")\n", "def test_variable_resolve(self):\n    \"\"\"\n    Test that Variable.resolve() doesn't raise an exception for a variable\n    starting with an underscore.\n    \"\"\"\n    template = self._engine().from_string(\"{{ _var }}\")\n    context = Context({\"_var\": \"value\"})\n    self.assertEqual(template.render(context), \"value\")\n\n    template = self._engine().from_string(\"{{ var._attr }}\")\n    context = Context({\"var\": {\"_attr\": \"value\"}})\n    self.assertEqual(template.render(context), \"value\")\n"], "sample_406": ["    def test_manager_descriptor(self):\n        manager = Article.objects\n        self.assertIsInstance(manager, ManagerDescriptor)\n        self.assertEqual(manager.__get__(None, Article), manager.manager)\n", "    def test_manager_descriptor(self):\n        # Make sure the manager descriptor is working as expected.\n        self.assertEqual(Article.objects.__class__, Manager)\n        self.assertEqual(ManagerDescriptor(Article.objects).manager, Article.objects)\n", "    def test_manager_descriptor(self):\n        # ManagerDescriptor returns the manager instance when accessed on a model class.\n        self.assertIsInstance(Article.objects, Manager)\n", "    def test_manager_descriptor(self):\n        # Make sure the ManagerDescriptor is doing its thing.\n        self.assertEqual(Article.objects.__class__, Manager)\n        self.assertEqual(ManagerDescriptor(Article.objects).manager, Article.objects)\n", "    def test_manager_descriptor(self):\n        model = Article\n        manager = model.objects\n\n        # Test that the manager descriptor returns the manager instance when accessed through the class.\n        self.assertEqual(model.objects, manager)\n\n        # Test that the manager descriptor raises an AttributeError when accessed through an instance.\n        instance = model()\n        with self.assertRaisesMessage(\n            AttributeError, \"Manager isn't accessible via Article instances\"\n        ):\n            getattr(instance, \"objects\")\n\n        # Test that the manager descriptor returns the correct manager when accessed through a subclass.\n        class SubArticle(Article):\n            pass\n\n        self.assertNotEqual(SubArticle.objects, manager)\n\n        # Test that the manager descriptor returns the correct manager when the model is abstract.\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(AbstractModel):\n            pass\n\n        self.assertIsInstance(ConcreteModel.objects, Manager)\n        with self.assertRaisesMessage(\n            AttributeError, \"Manager isn't available; AbstractModel is abstract\"\n        ):\n            getattr(AbstractModel, \"objects\")\n"], "sample_407": ["def test_model_get_state(self):\n    # Test that model's internal state is properly set up after instantiation.\n    self.assertEqual(self.a._state.db, None)\n    self.assertTrue(self.a._state.adding)\n\n    self.a.save()\n    self.assertEqual(self.a._state.db, \"default\")\n    self.assertFalse(self.a._state.adding)\n", "def test_save_foreign_key_with_to_field(self):\n    parent = Parent.objects.create(name=\"jeff\")\n    child = ToFieldChild.objects.create(parent=parent)\n    self.assertEqual(child.parent_id, \"jeff\")\n    parent.name = \"frank\"\n    parent.save()\n    # The child's parent_id should still be \"jeff\" after the parent's name is updated.\n    child.refresh_from_db()\n    self.assertEqual(child.parent_id, \"jeff\")\n    # If the child is saved again, its parent_id will be updated to the current value of the parent's name.\n    child.save()\n    self.assertEqual(child.parent_id, \"frank\")\n    child.refresh_from_db()\n    self.assertEqual(child.parent_id, \"frank\")\n", "def test_reverse_foreign_key_assignment(self):\n    parent = Parent.objects.create(name=\"a\")\n    child = Child.objects.create(parent=parent)\n    self.assertIs(child.parent, parent)\n    new_parent = Parent.objects.create(name=\"b\")\n    child.parent = new_parent\n    child.save()\n    self.assertIs(child.parent, new_parent)\n    # Test assigning None to a non-nullable FK.\n    msg = \"Cannot assign None: \\\"Child.parent\\\" does not allow null values.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        child.parent = None\n    # Test assigning None to a nullable FK.\n    child_nullable = ChildNullableParent.objects.create(parent=parent)\n    child_nullable.parent = None\n    child_nullable.save()\n    self.assertIsNone(child_nullable.parent)\n", "def test_reverse_foreign_key_instance_with_to_field_caching(self):\n    parent = Parent.objects.create(name=\"a\")\n    child = ToFieldChild.objects.create(parent=parent)\n    with self.assertNumQueries(0):\n        self.assertIs(child.parent, parent)\n\n    # Check that the related object is still cached after a save.\n    child.save()\n    with self.assertNumQueries(0):\n        self.assertIs(child.parent, parent)\n", "def test_foreign_key_to_model_with_custom_primary_key(self):\n    school = School.objects.create(is_public=True, school_id=1)\n    student = Student.objects.create(student_id=1, school=school)\n\n    self.assertEqual(student.school, school)\n    self.assertEqual(student.school_id, school.school_id)\n\n    # Test that the foreign key can be accessed and used in queries.\n    self.assertSequenceEqual(\n        Student.objects.filter(school=school),\n        [student],\n    )\n    self.assertSequenceEqual(\n        Student.objects.filter(school_id=school.school_id),\n        [student],\n    )\n\n    # Test that the foreign key can be set to None.\n    student.school = None\n    student.save()\n    self.assertIsNone(student.school)\n    self.assertIsNone(student.school_id)\n"], "sample_409": ["    def test_get_language_info(self):\n        node = GetLanguageInfoNode(parser.compile_filter(\"en\"), \"lang_info\")\n        context = Context()\n        node.render(context)\n        self.assertEqual(context[\"lang_info\"], translation.get_language_info(\"en\"))\n", "    def test_translate_node_repr(self):\n        filter_expression = \"test_filter\"\n        noop = True\n        asvar = \"test_asvar\"\n        message_context = \"test_message_context\"\n\n        translate_node = TranslateNode(\n            filter_expression, noop, asvar=asvar, message_context=message_context\n        )\n\n        expected_repr = (\n            f\"<TranslateNode: filter_expression='{filter_expression}', \"\n            f\"noop={noop}, asvar='{asvar}', message_context='{message_context}'>\"\n        )\n        self.assertEqual(repr(translate_node), expected_repr)\n", "    def test_get_available_languages(self):\n        node = GetAvailableLanguagesNode(variable=\"languages\")\n        context = Context()\n        node.render(context)\n        self.assertEqual(context[\"languages\"], [(\"en\", \"English\")])\n", "    def test_i18n_get_language_info(self):\n        output = self.engine.render_to_string(\"template\")\n        self.assertEqual(output.strip(), \"en English English English uni-directional\")\n", "    def test_translate_tag(self):\n        with translation.override(\"de\"):\n            output = self.engine.render_to_string(\"template\")\n        self.assertEqual(output, \"Seite nicht gefunden\")\n"], "sample_410": ["    def test_normalize_username(self):\n        # Test that the normalize_username method is called when setting the username.\n        class CustomUser(AbstractBaseUser):\n            USERNAME_FIELD = \"username\"\n\n                return username.upper()\n\n        user = CustomUser(username=\"test\")\n        self.assertEqual(user.username, \"TEST\")\n", "    def test_get_email_field_name(self):\n        user = AbstractBaseUser()\n        self.assertEqual(user.get_email_field_name(), \"email\")\n", "    def test_get_email_field_name(self):\n        self.assertEqual(AbstractBaseUser.get_email_field_name(), \"email\")\n", "    def test_normalize_username(self):\n        user = User(username=\"JohnDoe\")\n        self.assertEqual(user.normalize_username(\"JohnDoe\"), \"johndoe\")\n", "    def test_get_email_field_name(self):\n        self.assertEqual(AbstractBaseUser.get_email_field_name(), \"email\")\n"], "sample_411": ["def test_command_add_base_argument(self):\n    parser = BaseCommand().create_parser(\"prog_name\", \"subcommand\")\n    parser.add_argument(\"--foo\", action=\"store_true\")\n    with self.assertRaises(ValueError) as cm:\n        parser.add_argument(\"--foo\", action=\"store_false\")\n    self.assertIn(\"conflicting option string: --foo\", str(cm.exception))\n    BaseCommand.suppressed_base_arguments = {\"--foo\"}\n    parser = BaseCommand().create_parser(\"prog_name\", \"subcommand\")\n    parser.add_argument(\"--foo\", action=\"store_true\")\n    parser.add_argument(\"--foo\", action=\"store_false\")\n    self.assertEqual(len(parser._actions), 3)\n    BaseCommand.suppressed_base_arguments = set()\n", "def test_find_commands_empty_dir(self):\n    \"\"\"\n    find_commands should return an empty list if the directory doesn't contain any commands.\n    \"\"\"\n    with mock.patch(\"os.listdir\", return_value=[]):\n        self.assertEqual(find_commands(\"/path/to/empty/dir\"), [])\n", "def test_outputwrapper_isatty(self):\n    out = StringIO()\n    with mock.patch.object(out, \"isatty\", return_value=True) as mocked_isatty:\n        management.call_command(\"outputwrapper\", stdout=out)\n    self.assertIs(mocked_isatty.called, True)\n    out = StringIO()\n    with mock.patch.object(out, \"isatty\", return_value=False) as mocked_isatty:\n        management.call_command(\"outputwrapper\", stdout=out)\n    self.assertIs(mocked_isatty.called, True)\n", "def test_command_error_returncode(self):\n    error = CommandError(\"message\", returncode=3)\n    self.assertEqual(error.returncode, 3)\n\n    error = CommandError(\"message\")\n    self.assertEqual(error.returncode, 1)\n", "def test_command_error_with_returncode(self):\n    \"\"\"Test CommandError with a custom returncode.\"\"\"\n    with self.assertRaises(CommandError) as cm:\n        raise CommandError(\"An error occurred\", returncode=42)\n    self.assertEqual(cm.exception.returncode, 42)\n"], "sample_412": ["def test_urlize_autoescape(self):\n    tests = (\n        (\"Search for google.com/?q=! and see.\", 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'),\n        (\"Search for google.com/?q=1&lt! and see.\", 'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>! and see.'),\n        (\"foo@example.com\", '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n            self.assertIsInstance(urlize(value, autoescape=True), SafeString)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        (\"Check out google.com.\", 'Check out <a href=\"http://google.com/\">google.com</a>.'),\n        (\"Visit google.com?\", 'Visit <a href=\"http://google.com/\">google.com</a>?'),\n        (\"I love google.com!\", 'I love <a href=\"http://google.com/\">google.com</a>!'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\"Check out www.djangoproject.com.\", 'Check out <a href=\"http://www.djangoproject.com\">www.djangoproject.com</a>.'),\n        (\"Check out www.djangoproject.com!\", 'Check out <a href=\"http://www.djangoproject.com\">www.djangoproject.com</a>!'),\n        (\"Check out www.djangoproject.com?\", 'Check out <a href=\"http://www.djangoproject.com\">www.djangoproject.com</a>?'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_format_html_join(self):\n    values = [\n        (\"first\", \"John\"),\n        (\"second\", \"Jane\"),\n        (\"third\", \"Bob\"),\n    ]\n    expected_output = (\n        \"<ul>\"\n        + \"\\n\".join(f'<li>{name} {position}</li>' for position, name in values)\n        + \"</ul>\"\n    )\n    actual_output = format_html_join(\n        \"\\n\", \"<li>{} {}</li>\", ((name, position) for position, name in values)\n    )\n    self.assertHTMLEqual(f\"<ul>\\n{actual_output}\\n</ul>\", expected_output)\n", "def test_urlize_autoescape(self):\n    tests = (\n        (\n            \"Search for google.com/?q=! and see.\",\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and '\n            \"see.\",\n        ),\n        (\n            \"Search for google.com/?q=1&lt! and see.\",\n            'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt'\n            \"</a>! and see.\",\n        ),\n        (\"foo@example.com\", '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n\n    # Test with a string that needs escaping.\n    value = \"<script>alert('XSS')</script>\"\n    expected_output = \"&lt;script&gt;alert(&#x27;XSS&#x27;)&lt;/script&gt;\"\n    self.assertEqual(urlize(value, autoescape=True), expected_output)\n"], "sample_413": ["    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'django'\",\n                \"'django.templatetags.i18n', 'django.templatetags.i18n'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags', \"\n                \"'custom_library_path.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_3.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n"], "sample_414": ["    def test_render(self):\n        rel = Album._meta.get_field(\"band\").remote_field\n        w = widgets.AdminRadioSelect()\n        wrapper = widgets.RelatedFieldWidgetWrapper(w, rel, admin.site)\n        output = wrapper.render(\"test\", \"value\")\n        self.assertIn('href=\"/admin/admin_widgets/band/add/\"', output)\n        self.assertIn('href=\"/admin/admin_widgets/band/__fk__/\", title=\"Lookup\"', output)\n", "    def test_render(self):\n        widget = widgets.AutocompleteSelect(Band._meta.get_field(\"members\").remote_field, admin.site)\n        output = widget.render(\"test\", None)\n        self.assertHTMLEqual(output, '<input type=\"text\" id=\"id_test\" name=\"test\" class=\"vForeignKeyRawIdAdminField\">')\n", "    def test_get_content_type_for_model(self):\n        from django.contrib.contenttypes.models import ContentType\n\n        self.assertEqual(\n            get_content_type_for_model(Band), ContentType.objects.get_for_model(Band)\n        )\n", "    def test_empty_value_display(self):\n        model_admin = ModelAdmin(Band, admin.site)\n        self.assertEqual(model_admin.get_empty_value_display(), \"-\")\n\n        model_admin.empty_value_display = \"Custom empty value\"\n        self.assertEqual(\n            model_admin.get_empty_value_display(),\n            \"Custom empty value\",\n        )\n\n        with override_settings(EMPTY_VALUE_DISPLAY=\"Custom setting\"):\n            self.assertEqual(\n                model_admin.get_empty_value_display(),\n                \"Custom setting\",\n            )\n", "    def test_formfield_for_dbfield_with_request(self):\n        # Override any settings on the model admin\n        class MyModelAdmin(admin.ModelAdmin):\n            pass\n\n        ma = MyModelAdmin(Car, admin.site)\n        request = self.client.request().wsgi_request\n        request.user = self.superuser\n        ff = ma.formfield_for_dbfield(Car._meta.get_field(\"owner\"), request=request)\n        self.assertIsInstance(ff.widget, widgets.RelatedFieldWidgetWrapper)\n        self.assertEqual(ff.widget.widget.choices, [(self.superuser.pk, \"super\")])\n"], "sample_415": ["def test_validate_expression_with_excluded_fields(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    instance = UniqueConstraintProduct(name=self.p1.name.upper())\n    # Excluding a field used in the expression should skip validation.\n    constraint.validate(UniqueConstraintProduct, instance, exclude={\"name\"})\n    # Excluding another field not used in the expression should still raise an error.\n    with self.assertRaisesMessage(\n        ValidationError, \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    ):\n        constraint.validate(UniqueConstraintProduct, instance, exclude={\"color\"})\n", "def test_clone(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        condition=models.Q(foo=models.F(\"bar\")),\n        deferrable=models.Deferrable.DEFERRED,\n        include=[\"baz_1\", \"baz_2\"],\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n", "    def test_validate_with_exclude_for_expression(self):\n        constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n        instance = UniqueConstraintProduct(name=self.p1.name.upper())\n        with self.assertRaises(ValidationError):\n            constraint.validate(UniqueConstraintProduct, instance)\n        constraint.validate(\n            UniqueConstraintProduct,\n            instance,\n            exclude={\"name\"},\n        )\n", "def test_validate_expression_with_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper())\n    msg = \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"other_field\"},\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n", "def test_validate_excluded_fields_with_condition(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"],\n        name=\"name_without_color_uniq\",\n        condition=models.Q(color__isnull=True),\n    )\n    instance = UniqueConstraintProduct(name=\"test\", color=None)\n    # Field used by the condition is excluded.\n    constraint.validate(UniqueConstraintProduct, instance, exclude={\"color\"})\n    # Field used by the unique constraint is excluded.\n    constraint.validate(UniqueConstraintProduct, instance, exclude={\"name\"})\n    # Both fields are excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        instance,\n        exclude={\"name\", \"color\"},\n    )\n"], "sample_416": ["def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_database_name_not_provided(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n"], "sample_417": ["    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5, \"b\": 8})\n        self.assertEqual(output, \"8 10\")\n", "    def test_slugify01(self):\n        output = self.engine.render_to_string(\"slugify01\", {\"a\": \"Hello, World!\"})\n        self.assertEqual(output, \"hello-world\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 1.23})\n        self.assertEqual(output, \"1.230000E+00\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 1.23})\n        self.assertEqual(output, \"1.230000E+00\")\n", "def test_invalid_precision(self):\n    self.assertEqual(floatformat(1.2345, \"foo\"), \"1.2345\")\n    self.assertEqual(floatformat(15.2042, \"bar\"), \"15.2042\")\n    self.assertEqual(floatformat(Decimal(\"1.2345\"), \"baz\"), \"1.2345\")\n    self.assertEqual(floatformat(Decimal(\"15.2042\"), \"qux\"), \"15.2042\")\n"], "sample_418": ["    def test_floatformat01(self):\n        output = self.engine.render_to_string(\"floatformat01\", {\"float_num\": 7.7})\n        self.assertEqual(output, \"7.7\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"value\": 5})\n        self.assertEqual(output, \"10\")\n", "def test_pluralize01(self):\n    output = self.engine.render_to_string(\"pluralize01\", {\"items\": [\"item1\"]})\n    self.assertEqual(output, \"1 item\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"num1\": 5, \"num2\": 3})\n        self.assertEqual(output, \"8\")\n", "    def test_add_two_numbers(self):\n        output = self.engine.render_to_string(\"add01\", {\"num1\": 4, \"num2\": 6})\n        self.assertEqual(output, \"10\")\n"], "sample_419": ["def test_absolute_max_initial_forms(self):\n    data = {\n        \"form-TOTAL_FORMS\": \"2001\",\n        \"form-INITIAL_FORMS\": \"2000\",\n        \"form-MIN_NUM_FORMS\": \"0\",\n        \"form-MAX_NUM_FORMS\": \"1000\",\n    }\n    initial = [{\"name\": \"Test\"}] * 2000\n    AbsoluteMaxFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        absolute_max=2000,\n    )\n    formset = AbsoluteMaxFavoriteDrinksFormSet(data, initial=initial)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 2000)\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"Please submit at most 1000 forms.\"],\n    )\n", "def test_all_valid_with_non_form_errors(self):\n    \"\"\"all_valid() validates all forms, even when some have non-form errors.\"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"0\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"1\",\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    formset2 = ChoiceFormSetWithNonFormError(data, auto_id=False, prefix=\"choices\")\n    self.assertIs(all_valid((formset1, formset2)), False)\n    expected_errors = [{}, {}]\n    self.assertEqual(formset1._errors, expected_errors)\n    self.assertEqual(formset2._errors, expected_errors)\n    self.assertEqual(len(formset2.non_form_errors()), 1)\n", "def test_formset_template_name_from_renderer(self):\n    \"\"\"A formset's template name can come from the renderer.\"\"\"\n    class CustomRenderer(TemplatesSetting):\n        formset_template_name = \"custom/formset.html\"\n\n    ChoiceFormSet = formset_factory(Choice, renderer=CustomRenderer())\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"custom/formset.html\")\n", "    def test_absolute_max_larger_than_max_num(self):\n        ChoiceFormSet = formset_factory(Choice, max_num=10, absolute_max=20)\n        data = {\n            \"choices-TOTAL_FORMS\": \"15\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"10\",\n        }\n        for i in range(15):\n            data[f\"choices-{i}-choice\"] = f\"Choice {i}\"\n            data[f\"choices-{i}-votes\"] = 0\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(len(formset.forms), 15)\n", "def test_formset_absolute_max_is_integer(self):\n    msg = \"'absolute_max' must be an integer.\"\n    for invalid_value in [\"not an integer\", 1.5, object()]:\n        with self.subTest(absolute_max=invalid_value):\n            with self.assertRaisesMessage(TypeError, msg):\n                formset_factory(FavoriteDrinkForm, absolute_max=invalid_value)\n"], "sample_420": ["    def test_media(self):\n        class MediaModelForm(forms.ModelForm):\n            class Meta:\n                model = TextFile\n                fields = \"__all__\"\n\n            class Media:\n                css = {\"all\": (\"/some/form/css\",)}\n                js = (\"/some/form/javascript\",)\n\n        self.assertEqual(\n            MediaModelForm().media,\n            forms.Media(css={\"all\": (\"/some/form/css\",)}, js=(\"/some/form/javascript\",)),\n        )\n", "    def test_model_formset_with_empty_queryset(self):\n        class BookForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = \"__all__\"\n\n        BookFormSet = forms.modelformset_factory(Book, form=BookForm)\n        formset = BookFormSet(queryset=Book.objects.none())\n        self.assertEqual(formset.initial_form_count(), 0)\n", "    def test_modelformset_validates_unique_fields(self):\n        \"\"\"\n        A model formset should validate unique fields across all its forms.\n        \"\"\"\n\n        class BookForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = \"__all__\"\n\n        formset = modelformset_factory(Book, form=BookForm)\n        data = {\n            \"form-TOTAL_FORMS\": 2,\n            \"form-INITIAL_FORMS\": 0,\n            \"form-MIN_NUM_FORMS\": 0,\n            \"form-MAX_NUM_FORMS\": 1000,\n            \"form-0-title\": \"Test\",\n            \"form-0-author\": \"1\",\n            \"form-1-title\": \"Test\",\n            \"form-1-author\": \"1\",\n        }\n        formset = formset(data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.errors[0],\n            {\"__all__\": [\"Book with this Title and Author already exists.\"]},\n        )\n", "    def test_media_from_parent(self):\n        class Parent(forms.ModelForm):\n            class Media:\n                css = {\"all\": (\"/parent.css\",)}\n                js = (\"/parent.js\",)\n\n            class Meta:\n                model = Person\n                fields = \"__all__\"\n\n        class Child(Parent):\n            pass\n\n        self.assertEqual(Child().media._css[\"all\"], [\"/parent.css\"])\n        self.assertEqual(Child().media._js, [\"/parent.js\"])\n", "    def test_modelform_null_boolean_field(self):\n        class NullBooleanForm(forms.ModelForm):\n            class Meta:\n                model = Homepage\n                fields = \"__all__\"\n\n        form = NullBooleanForm({\"url\": \"http://example.com\", \"active\": \"true\"})\n        self.assertTrue(form.is_valid())\n        self.assertIs(form.cleaned_data[\"active\"], True)\n\n        form = NullBooleanForm({\"url\": \"http://example.com\", \"active\": \"false\"})\n        self.assertTrue(form.is_valid())\n        self.assertIs(form.cleaned_data[\"active\"], False)\n\n        form = NullBooleanForm({\"url\": \"http://example.com\", \"active\": \"\"})\n        self.assertTrue(form.is_valid())\n        self.assertIs(form.cleaned_data[\"active\"], None)\n"], "sample_421": ["    def setUpTestData(cls):\n        cls.o = CaseTestModel.objects.create(integer=1, integer2=1, string=\"1\")\n        FKCaseTestModel.objects.create(fk=cls.o, integer=1)\n", "    def test_type_inference_when_integer(self):\n        expression = Case(When(integer=1, then=1))\n        self.assertIsInstance(expression.output_field, IntegerField)\n", "    def test_expression_wrapper(self):\n        expression = F(\"field\")\n        wrapped_expression = ExpressionWrapper(expression, output_field=IntegerField())\n        self.assertEqual(wrapped_expression.output_field, IntegerField())\n", "    def test_infer_type_from_whens(self):\n        self.assertIsInstance(\n            Case(When(integer=1, then=1), When(integer=2, then=2)).output_field,\n            IntegerField,\n        )\n", "    def setUpTestData(cls):\n        CaseTestModel.objects.create(integer=1, integer2=1)\n        CaseTestModel.objects.create(integer=2, integer2=2)\n        CaseTestModel.objects.create(integer=3, integer2=3)\n"], "sample_422": ["    def setUpTestData(cls):\n        cls.author = Author.objects.create(name=\"Author1\")\n        cls.book1 = Book.objects.create(title=\"Book1\")\n        cls.book2 = Book.objects.create(title=\"Book2\")\n        cls.book1.authors.add(cls.author)\n        cls.book2.authors.add(cls.author)\n", "    def test_m2m_through_fk(self):\n        with self.assertNumQueries(3):\n            qs = Book.objects.prefetch_related(\"authors__addresses\")\n            lists = [\n                [[str(addr) for addr in a.addresses.all()] for a in b.authors.all()]\n                for b in qs\n            ]\n\n        normal_lists = [\n            [[str(addr) for addr in a.addresses.all()] for a in b.authors.all()]\n            for b in Book.objects.all()\n        ]\n        self.assertEqual(lists, normal_lists)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title=\"Les confessions Volume I\")\n        cls.book2 = Book.objects.create(title=\"Candide\")\n        cls.author1 = AuthorWithAge.objects.create(\n            name=\"Rousseau\", first_book=cls.book1, age=70\n        )\n        cls.author2 = AuthorWithAge.objects.create(\n            name=\"Voltaire\", first_book=cls.book2, age=65\n        )\n        cls.book1.authors.add(cls.author1)\n        cls.book2.authors.add(cls.author2)\n", "    def setUpTestData(cls):\n        cls.employee1 = Employee.objects.create(name=\"John\")\n        cls.employee2 = Employee.objects.create(name=\"Jane\", boss=cls.employee1)\n", "    def test_m2m_prefetch_related_with_filtered_queryset(self):\n        # Create a new author that will not be included in the filtered queryset.\n        author5 = Author.objects.create(name=\"Author 5\")\n\n        # Add the new author to a book.\n        self.book1.authors.add(author5)\n\n        # Filter the authors queryset to exclude the new author.\n        filtered_authors = Author.objects.filter(name__in=[\"Charlotte\", \"Anne\", \"Emily\"])\n\n        # Prefetch related authors using the filtered queryset.\n        books = Book.objects.prefetch_related(Prefetch(\"authors\", queryset=filtered_authors))\n\n        with self.assertNumQueries(2):\n            for book in books:\n                authors = list(book.authors.all())\n                self.assertNotIn(author5, authors)\n"], "sample_425": ["def test_serialize_zone_info(self):\n    tz = zoneinfo.ZoneInfo(\"Europe/Paris\")\n    dt = datetime.datetime(2022, 1, 1, tzinfo=tz)\n    self.assertSerializedResultEqual(\n        dt,\n        (\n            \"datetime.datetime(2022, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)\",\n            {\"import datetime\"},\n        ),\n    )\n", "def test_serialize_re_with_flags(self):\n    regex = re.compile(r\"^\\w+$\", flags=re.IGNORECASE | re.MULTILINE)\n    self.assertSerializedResultEqual(\n        regex,\n        (\n            \"re.compile('^\\\\\\\\w+$', flags=re.RegexFlag['IGNORECASE'] | \"\n            \"re.RegexFlag['MULTILINE'])\",\n            {\"import re\"},\n        ),\n    )\n", "def test_deconstruct_nested_class_arguments(self):\n    # Test deconstruction of a class argument that contains another class.\n    class NestedClass:\n        pass\n\n    class DeconstructibleClassWithNestedClass:\n            self.nested_class = nested_class\n\n            return (\n                \"DeconstructibleClassWithNestedClass\",\n                [self.nested_class],\n                {},\n            )\n\n    string = MigrationWriter.serialize(\n        models.CharField(default=DeconstructibleClassWithNestedClass(NestedClass))\n    )[0]\n    self.assertEqual(\n        string,\n        \"models.CharField(default=migrations.test_writer.\"\n        \"DeconstructibleClassWithNestedClass(migrations.test_writer.NestedClass))\",\n    )\n", "def test_serialize_function_type(self):\n    func = lambda x: x**2\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize function: lambda\"):\n        MigrationWriter.serialize(func)\n\n        return x**2\n\n    string, imports = MigrationWriter.serialize(my_func)\n    self.assertEqual(string, \"migrations.test_writer.WriterTests.test_serialize_function_type.<locals>.my_func\")\n    self.assertEqual(imports, {\"import migrations.test_writer\"})\n\n    class MyClass:\n            return x**2\n\n    obj = MyClass()\n    method = obj.my_method\n    string, imports = MigrationWriter.serialize(method)\n    self.assertEqual(string, \"migrations.test_writer.WriterTests.test_serialize_function_type.<locals>.MyClass.my_method\")\n    self.assertEqual(imports, {\"import migrations.test_writer\"})\n", "def test_serialize_lambda_function_in_partial(self):\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize function: lambda\"):\n        value = functools.partial(lambda x: x**2, 4)\n        MigrationWriter.serialize(value)\n"], "sample_426": ["def test_timeuntil_depth(self):\n    t = (\n        self.t\n        + self.oneyear\n        + self.onemonth\n        + self.oneweek\n        + self.oneday\n        + self.onehour\n    )\n    tests = [\n        (t, 1, \"1\\xa0year\"),\n        (t, 2, \"1\\xa0year, 1\\xa0month\"),\n        (t, 3, \"1\\xa0year, 1\\xa0month, 1\\xa0week\"),\n        (t, 4, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day\"),\n        (t, 5, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n        (t, 6, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n        (self.t + self.onehour, 5, \"1\\xa0hour\"),\n        (self.t + (4 * self.oneminute), 3, \"4\\xa0minutes\"),\n        (self.t + self.onehour + self.oneminute, 1, \"1\\xa0hour\"),\n        (self.t + self.oneday + self.onehour, 1, \"1\\xa0day\"),\n        (self.t + self.oneweek + self.oneday, 1, \"1\\xa0week\"),\n        (self.t + self.onemonth + self.oneweek, 1, \"1\\xa0month\"),\n        (self.t + self.oneyear + self.onemonth, 1, \"1\\xa0year\"),\n        (self.t + self.oneyear + self.oneweek + self.oneday, 3, \"1\\xa0year\"),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timeuntil(self.t, value, depth=depth), expected)\n", "def test_time_strings_override(self):\n    time_strings = {\n        \"year\": npgettext_lazy(\"naturaltime\", \"%(num)d annum\", \"%(num)d years\", \"num\"),\n        \"month\": npgettext_lazy(\"naturaltime\", \"%(num)d moon\", \"%(num)d months\", \"num\"),\n    }\n    t = datetime.datetime(2022, 1, 1)\n    tests = [\n        (datetime.datetime(2023, 1, 1), \"1\\xa0annum\"),\n        (datetime.datetime(2022, 2, 1), \"1\\xa0moon\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value, time_strings=time_strings), expected)\n", "def test_timesince_depth_too_large(self):\n    t = (\n        self.t\n        + self.oneyear\n        + self.onemonth\n        + self.oneweek\n        + self.oneday\n        + self.onehour\n    )\n    tests = [\n        (t, 10, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(self.t, value, depth=depth), expected)\n            self.assertEqual(timeuntil(value, self.t, depth=depth), expected)\n", "def test_timesince_invalid_time_strings(self):\n    msg = \"'time_strings' must be a dictionary\"\n    with self.assertRaisesMessage(TypeError, msg):\n        timesince(self.t, self.t, time_strings=\"invalid\")\n", "def test_years_edge(self):\n    t = datetime.datetime(2022, 1, 1)\n    tests = [\n        (datetime.datetime(2023, 1, 1), \"1\\xa0year\"),\n        (datetime.datetime(2024, 1, 1), \"2\\xa0years\"),\n        (datetime.datetime(2025, 1, 1), \"3\\xa0years\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value), expected)\n"], "sample_427": ["def test_management_form_error_class(self):\n    \"\"\"ManagementForm's error_class is the same as the formset.\"\"\"\n    formset = ArticleFormSet(\n        {}, error_class=ErrorList, prefix=\"form\"\n    )\n    self.assertEqual(formset.management_form.error_class, ErrorList)\n", "    def test_formset_context(self):\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet()\n        context = formset.get_context()\n        self.assertIn(\"formset\", context)\n        self.assertEqual(context[\"formset\"], formset)\n", "def test_get_context(self):\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    context = formset.get_context()\n    self.assertEqual(context[\"formset\"], formset)\n", "def test_formset_with_empty_data(self):\n    \"\"\"A formset with empty data should be invalid.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data={})\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            \"ManagementForm data is missing or has been tampered with. \"\n            \"Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS. \"\n            \"You may need to file a bug report if the issue persists.\",\n        ],\n    )\n", "def test_add_fields(self):\n    class DynamicBaseFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields[\"custom\"] = CharField()\n\n    ChoiceFormSet = formset_factory(Choice, formset=DynamicBaseFormSet)\n    formset = ChoiceFormSet()\n    self.assertIn(\"custom\", formset.forms[0].fields)\n\n    ChoiceFormSet = formset_factory(Choice, extra=1, can_delete=True, can_order=True)\n    formset = ChoiceFormSet(initial=[{\"choice\": \"Zero\", \"votes\": \"1\"}])\n    self.assertEqual(len(formset.forms), 2)\n    self.assertIn(\"DELETE\", formset.forms[0].fields)\n    self.assertIn(\"ORDER\", formset.forms[0].fields)\n    self.assertIn(\"DELETE\", formset.forms[1].fields)\n    self.assertIn(\"ORDER\", formset.forms[1].fields)\n\n    ChoiceFormSet = formset_factory(\n        Choice, extra=1, can_delete=True, can_delete_extra=False\n    )\n    formset = ChoiceFormSet(initial=[{\"choice\": \"Zero\", \"votes\": \"1\"}])\n    self.assertEqual(len(formset.forms), 2)\n    self.assertIn(\"DELETE\", formset.forms[0].fields)\n    self.assertNotIn(\"DELETE\", formset.forms[1].fields)\n\n    ChoiceFormSet = formset_factory(Choice, extra=1, can_order=True)\n    formset = ChoiceFormSet(initial=[{\"choice\": \"Zero\", \"votes\": \"1\"}])\n    self.assertEqual(len(formset.forms), 2)\n    self.assertIn(\"ORDER\", formset.forms[0].fields)\n    self.assertIn(\"ORDER\", formset.forms[1].fields)\n"], "sample_428": ["def test_non_uniform_grouping(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"12,34,56,789\",\n    )\n    self.assertEqual(\n        nformat(\n            123456789,\n            \".\",\n            grouping=(3, 2, 0),\n            thousand_sep=\",\",\n            decimal_pos=2,\n            force_grouping=True,\n        ),\n        \"12,34,56,789.00\",\n    )\n    self.assertEqual(\n        nformat(-123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"-12,34,56,789\",\n    )\n    self.assertEqual(\n        nformat(\n            Decimal(\"123456789\"),\n            \".\",\n            grouping=(3, 2, 0),\n            thousand_sep=\",\",\n            force_grouping=True,\n        ),\n        \"12,34,56,789\",\n    )\n", "def test_mark_safe(self):\n    self.assertIsInstance(nformat(1234, \".\"), str)\n    self.assertIsInstance(nformat(\"1234\", \".\"), str)\n    self.assertIsInstance(nformat(None, \".\"), str)\n    self.assertIsInstance(nformat(\"\", \".\"), str)\n    value = \"<script>alert('XSS')</script>\"\n    self.assertNotEqual(nformat(value, \".\"), mark_safe(value))\n    self.assertEqual(str(nformat(value, \".\")), value)\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2), thousand_sep=\",\", force_grouping=True),\n        \"123,45,6789\",\n    )\n    self.assertEqual(\n        nformat(\n            123456789,\n            \".\",\n            grouping=(3, 2, 0),\n            thousand_sep=\",\",\n            force_grouping=True,\n        ),\n        \"123,45,6789\",\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(1, 2, 3), thousand_sep=\",\", force_grouping=True),\n        \"1,23,456789\",\n    )\n", "def test_non_uniform_digit_grouping(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"123,45,6789\",\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2), thousand_sep=\",\", force_grouping=True),\n        \"123,45,6789\",\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(4, 3, 2), thousand_sep=\",\", force_grouping=True),\n        \"1234,567,89\",\n    )\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"12,34,56,789\"\n    )\n    self.assertEqual(\n        nformat(1234567890, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"1,23,45,67,890\"\n    )\n    self.assertEqual(\n        nformat(1234, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"1,234\"\n    )\n"], "sample_429": ["def test_url_validator_with_non_ascii_characters(self):\n    valid_urls = [\n        \"http://.ws/\u4a39\",\n        \"http://\u27a1.ws/\u4a39\",\n        \"http://\u2318.ws\",\n        \"http://\u263a.damowmow.com/\",\n    ]\n    for url in valid_urls:\n        with self.subTest(url=url):\n            try:\n                URLValidator()(url)\n            except ValidationError:\n                self.fail(f\"URL '{url}' unexpectedly failed validation\")\n", "    def test_limit_value_callable(self):\n        class DynamicLimit:\n                return 10\n\n        limit = DynamicLimit()\n        validator = BaseValidator(limit_value=limit)\n        self.assertEqual(validator.limit_value, 10)\n\n        limit = lambda: 20\n        validator = BaseValidator(limit_value=limit)\n        self.assertEqual(validator.limit_value, 20)\n", "def test_validator_deconstruction(self):\n    validator = RegexValidator(r\"^(?:[a-z0-9\\.\\-]*)://\")\n    path, args, kwargs = validator.deconstruct()\n    self.assertEqual(path, 'django.core.validators.RegexValidator')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'regex': r'^(?:[a-z0-9\\.\\-]*)://', 'message': 'Enter a valid value.', 'code': 'invalid', 'inverse_match': False, 'flags': 0})\n", "    def test_compare(self):\n        class CustomValidator(BaseValidator):\n                return a == b\n\n        validator = CustomValidator(limit_value=5)\n        with self.assertRaisesMessage(\n            ValidationError,\n            \"Ensure this value is not equal to 5 (it is 5).\",\n        ):\n            validator(5)\n\n        try:\n            validator(3)\n        except ValidationError:\n            self.fail(\"Should not have raised an exception.\")\n", "    def test_custom_schemes(self):\n        validator = URLValidator(schemes=[\"custom1\", \"custom2\"])\n        self.assertEqual(validator(\"custom1://example.com\"), None)\n        self.assertEqual(validator(\"custom2://example.com\"), None)\n        with self.assertRaisesMessage(ValidationError, \"Enter a valid URL.\"):\n            validator(\"http://example.com\")\n"], "sample_431": ["    def test_model_clean_is_called(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10)\n\n                self.full_clean_called = True\n\n        tm = TestModel(name=\"foo\")\n        tm.full_clean()\n        self.assertTrue(tm.full_clean_called)\n", "    def test_serialize_model_instance(self):\n        a = Article.objects.create(headline=\"Parrot programs in Python\", pub_date=datetime(2005, 7, 28))\n        data = a.__getstate__()\n        self.assertIn(\"headline\", data)\n        self.assertIn(\"pub_date\", data)\n", "    def test_delete(self):\n        pub_date = datetime.now()\n        a1 = Article.objects.create(headline=\"Article 1\", pub_date=pub_date)\n        a2 = Article.objects.create(headline=\"Article 2\", pub_date=pub_date)\n        self.assertEqual(Article.objects.count(), 2)\n\n        # Delete an object.\n        a1.delete()\n\n        # After deleting an object, its id is not set to None.\n        self.assertIsNotNone(a1.id)\n\n        # You can still access the attributes of the object after it's been\n        # deleted from the database.\n        self.assertEqual(a1.headline, \"Article 1\")\n\n        # If you try to save the object again, Django will throw an error.\n        msg = (\n            \"Cannot force both insert and updating in model saving.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            a1.save(force_insert=True)\n\n        # The article is no longer in the database.\n        self.assertEqual(Article.objects.count(), 1)\n        self.assertEqual(Article.objects.all()[0].headline, \"Article 2\")\n\n        # Trying to get it through the manager raises an exception.\n        msg = \"Article matching query does not exist.\"\n        with self.assertRaisesMessage(ObjectDoesNotExist, msg):\n            Article.objects.get(pk=a1.id)\n", "    def test_full_clean(self):\n        pub_date = datetime.now()\n        article = Article(pub_date=pub_date)\n        article.full_clean()\n        article.save()\n        self.assertEqual(article.pub_date, pub_date)\n", "    def test_full_clean_calls_clean(self):\n        class TestModel(models.Model):\n                self.clean_called = True\n\n        obj = TestModel()\n        obj.full_clean()\n        self.assertTrue(obj.clean_called)\n"], "sample_432": ["def test_list_editable_foreign_key(self):\n    band = Band.objects.create(name=\"Test Band\", nr_of_members=3)\n    event = Event.objects.create(name=\"Test Event\", band=band)\n\n    superuser = self._create_superuser(\"superuser\")\n    self.client.force_login(superuser)\n    changelist_url = reverse(\"admin:admin_changelist_event_changelist\")\n\n    # Send the POST with a new band ID.\n    new_band = Band.objects.create(name=\"New Band\", nr_of_members=1)\n    data = {\n        \"form-TOTAL_FORMS\": \"1\",\n        \"form-INITIAL_FORMS\": \"1\",\n        \"form-MIN_NUM_FORMS\": \"0\",\n        \"form-MAX_NUM_FORMS\": \"1000\",\n        \"form-0-id\": str(event.pk),\n        \"form-0-name\": \"Test Event\",\n        \"form-0-band\": str(new_band.pk),\n        \"_save\": \"Save\",\n    }\n    response = self.client.post(changelist_url, data)\n    self.assertRedirects(response, changelist_url)\n    event.refresh_from_db()\n    self.assertEqual(event.band, new_band)\n", "def test_dynamic_sortable_by(self):\n    \"\"\"\n    Regression tests for ticket #17646: dynamic sortable_by support.\n    \"\"\"\n    parent = Parent.objects.create(name=\"parent\")\n    for i in range(10):\n        Child.objects.create(name=\"child %s\" % i, parent=parent)\n\n    user_noparents = self._create_superuser(\"noparents\")\n    user_parents = self._create_superuser(\"parents\")\n\n    # Test with user 'noparents'\n    m = DynamicListDisplayChildAdmin(Child, custom_site)\n    m.sortable_by = (\"name\", \"age\")\n    request = self._mocked_authenticated_request(\"/child/\", user_noparents)\n    response = m.changelist_view(request)\n    self.assertEqual(response.context_data[\"cl\"].sortable_by, (\"name\", \"age\"))\n\n    # Test with user 'parents'\n    m = DynamicListDisplayChildAdmin(Child, custom_site)\n    m.sortable_by = (\"name\", \"age\")\n    request = self._mocked_authenticated_request(\"/child/\", user_parents)\n    response = m.changelist_view(request)\n    self.assertEqual(\n        response.context_data[\"cl\"].sortable_by, (\"parent\", \"name\", \"age\")\n    )\n", "def test_lookup_allowed(self):\n    m = ChildAdmin(Child, custom_site)\n\n    request = self.factory.get(\"/child/\")\n    request.user = self.superuser\n\n    tests = [\n        # Simple lookup\n        (\"age__exact\", \"0\"),\n        # Simple lookup with value that needs quoting\n        (\"name__contains\", \"some,value\"),\n        # Related lookup\n        (\"parent__name__exact\", \"some_value\"),\n        # Related lookup with value that needs quoting\n        (\"parent__name__contains\", \"some,value\"),\n        # Related lookup across multiple models\n        (\"parent__parent__name__exact\", \"some_value\"),\n        # Related lookup across multiple models with value that needs quoting\n        (\"parent__parent__name__contains\", \"some,value\"),\n    ]\n\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertTrue(m.lookup_allowed(lookup, value))\n", "def test_sortable_by_dynamic(self):\n    parent = Parent.objects.create(name=\"parent\")\n    for i in range(10):\n        Child.objects.create(name=\"child %s\" % i, parent=parent)\n\n    user_noparents = self._create_superuser(\"noparents\")\n    user_parents = self._create_superuser(\"parents\")\n\n    m = DynamicListDisplayChildAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request(\"/child/\", user_parents)\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.sortable_by, [\"parent__id\", \"name\", \"age\"])\n\n    m = DynamicListDisplayChildAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request(\"/child/\", user_noparents)\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.sortable_by, [\"name\", \"age\"])\n", "def test_custom_change_list_template(self):\n    self.client.force_login(self.superuser)\n    request = self.factory.get(\"/child/\")\n    request.user = self.superuser\n    m = ChildAdmin(Child, custom_site)\n    m.change_list_template = \"custom_change_list.html\"\n    response = m.changelist_view(request)\n    self.assertTemplateUsed(response, \"custom_change_list.html\")\n"], "sample_433": ["def test_suggest_name_with_custom_name(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.DeleteModel(\"Animal\"),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    custom_name = \"custom_name\"\n    self.assertEqual(\n        migration.suggest_name(custom_name=custom_name), f\"0001_{custom_name}\"\n    )\n", "def test_suggest_name_long_fragment(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.CreateModel(\"AnotherPerson\", fields=[]),\n            migrations.CreateModel(\"YetAnotherPerson\", fields=[]),\n            migrations.CreateModel(\"EvenMorePerson\", fields=[]),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    self.assertEqual(\n        migration.suggest_name(),\n        \"person_anotherperson_yetanotherperson_and_more\"\n    )\n", "def test_suggest_name_custom_migration(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.DeleteModel(\"Animal\"),\n        ]\n\n    migration_name = \"custom_name\"\n    migration = Migration(\"some_migration\", \"test_app\")\n    suggest_name = migration.suggest_name(migration_name)\n    self.assertEqual(suggest_name, f\"{migration_name}_person_delete_animal\")\n", "def test_custom_migration_name_with_initial(self):\n    \"\"\"Tests custom naming of migrations for graph matching with initial=True.\"\"\"\n    # Make a fake graph\n    graph = MigrationGraph()\n    graph.add_node((\"testapp\", \"0001_initial\"), None)\n    graph.add_node((\"testapp\", \"0002_foobar\"), None)\n    graph.add_node((\"otherapp\", \"0001_initial\"), None)\n    graph.add_dependency(\n        \"testapp.0002_foobar\",\n        (\"testapp\", \"0002_foobar\"),\n        (\"testapp\", \"0001_initial\"),\n    )\n\n    # Use project state to make a new migration change set\n    before = self.make_project_state([])\n    after = self.make_project_state(\n        [self.author_empty, self.other_pony, self.other_stable]\n    )\n    autodetector = MigrationAutodetector(before, after, MigrationQuestioner({\"ask_initial\": True}))\n    changes = autodetector._detect_changes()\n\n    # Run through arrange_for_graph\n    migration_name = \"custom_name\"\n    changes = autodetector.arrange_for_graph(changes, graph, migration_name)\n\n    # Make sure there's a new name, deps match, etc.\n    self.assertEqual(changes[\"testapp\"][0].name, \"0003_%s\" % migration_name)\n    self.assertEqual(\n        changes[\"testapp\"][0].dependencies, [(\"testapp\", \"0002_foobar\")]\n    )\n    self.assertEqual(changes[\"otherapp\"][0].name, \"0002_%s\" % migration_name)\n    self.assertEqual(\n        changes[\"otherapp\"][0].dependencies, [(\"otherapp\", \"0001_initial\")]\n    )\n", "def test_custom_migration_name_with_initial_true(self):\n    class Migration(migrations.Migration):\n        initial = True\n        operations = [migrations.CreateModel(\"Person\", fields=[])]\n\n    migration = Migration(\"0001_initial\", \"test_app\")\n    custom_name = \"custom_name\"\n    self.assertEqual(migration.suggest_name(custom_name), custom_name)\n"], "sample_434": ["    def test_render_to_response(self):\n        mixin = TemplateResponseMixin()\n        mixin.template_name = \"template.html\"\n        mixin.request = RequestFactory().get(\"/\")\n        response = mixin.render_to_response({})\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, \"template.html\")\n", "    def test_template_view_renders_correctly(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/\")\n        response = TestTemplateView.as_view()(request)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, [\"test_template.html\"])\n", "def test_as_view_signature(self):\n    \"\"\"\n    as_view() should return a function with the correct signature.\n    \"\"\"\n    view_cls = View\n    view_func = view_cls.as_view()\n    self.assertEqual(view_func.__name__, 'view')\n    self.assertEqual(view_func.__module__, view_cls.__module__)\n    self.assertEqual(view_func.__annotations__, view_cls.dispatch.__annotations__)\n    self.assertEqual(view_func.__doc__, view_cls.__doc__)\n", "    def test_template_view_renders_template(self):\n        class MyTemplateView(TemplateView):\n            template_name = \"template.html\"\n\n        view = MyTemplateView.as_view()\n        request = RequestFactory().get(\"/\")\n        response = view(request)\n\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, [\"template.html\"])\n", "    def test_template_view_renders_template(self):\n        class TemplateViewTest(TemplateView):\n            template_name = \"test_template.html\"\n\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/\")\n        response = TemplateViewTest.as_view()(request)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, \"test_template.html\")\n"], "sample_435": ["    def test_normalize_username(self):\n        field = UsernameField()\n        self.assertEqual(field.to_python(\"test\u2126\"), \"test\u03a9\")\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + \"unusable\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_get_context(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(name=\"password\", value=None, attrs={})\n        self.assertEqual(context[\"summary\"], [{\"label\": _(\"No password set.\")}])\n        \n        value = (\n            \"pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5u\"\n            \"dm0=\"\n        )\n        context = widget.get_context(name=\"password\", value=value, attrs={})\n        self.assertEqual(len(context[\"summary\"]), 4)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable_password\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_user_change_form_init(self):\n        # Test that the UserChangeForm initializes correctly with an instance\n        user = User.objects.create_user(\"testuser\", \"test@example.com\", \"testpassword\")\n        form = UserChangeForm(instance=user)\n        self.assertEqual(form.instance, user)\n"], "sample_436": ["    def test_help_text(self):\n        args = [\"runserver\", \"--help\"]\n        out, err = self.run_manage(args)\n        self.assertNoOutput(err)\n        self.assertOutput(out, \"usage: manage.py runserver\")\n        self.assertOutput(out, \"optional arguments:\")\n        self.assertOutput(out, \"--addrport\")\n        self.assertOutput(out, \"--ipv6\")\n        self.assertOutput(out, \"--nothreading\")\n        self.assertOutput(out, \"--noreload\")\n        self.assertOutput(out, \"--skip-checks\")\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\"settings.py\")\n", "    def test_runserver_with_extra_args(self):\n        args = [\"runserver\", \"--settings=test_project.settings\", \"0.0.0.0:8000\"]\n        out, err = self.run_manage(args)\n        self.assertNoOutput(err)\n        self.assertOutput(out, \"Starting development server at http://0.0.0.0:8000/\")\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\"settings.py\")\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\"settings.py\")\n"], "sample_437": ["    def test_copy(self):\n        conn = connections[DEFAULT_DB_ALIAS]\n        copy_conn = conn.copy()\n        self.assertEqual(copy_conn.settings_dict, conn.settings_dict)\n        self.assertEqual(copy_conn.alias, conn.alias)\n", "    def test_thread_sharing_validation(self):\n        # Create a connection in the main thread.\n        conn = connections[\"default\"]\n        with self.assertRaises(DatabaseError):\n            # Attempt to use the connection from a different thread.\n                conn.cursor()\n\n            thread = threading.Thread(target=target)\n            thread.start()\n            thread.join()\n", "    def test_savepoint_rollback_with_autocommit(self):\n        with self.assertRaises(TransactionManagementError):\n            connection.savepoint()\n            connection.savepoint_rollback(connection.savepoint_ids[0])\n", "    def test_validate_thread_sharing(self):\n        # Create a new connection.\n        connection = connections[DEFAULT_DB_ALIAS]\n        connection.connect()\n\n        # Initialize the thread sharing properties.\n        connection.inc_thread_sharing()\n        self.assertTrue(connection.allow_thread_sharing)\n        self.assertEqual(connection._thread_sharing_count, 1)\n\n        # Try to access the connection from a different thread.\n        with patch.object(_thread, \"get_ident\", return_value=\"different_thread\"):\n            connection.validate_thread_sharing()\n            # No exception is raised.\n\n        # Decrement the thread sharing count and try again.\n        connection.dec_thread_sharing()\n        self.assertFalse(connection.allow_thread_sharing)\n        self.assertEqual(connection._thread_sharing_count, 0)\n\n        with patch.object(_thread, \"get_ident\", return_value=\"different_thread\"):\n            with self.assertRaises(DatabaseError):\n                connection.validate_thread_sharing()\n", "    def test_timezone_property(self):\n        with override_settings(USE_TZ=True):\n            self.assertIsNone(connection.timezone)\n\n        with override_settings(USE_TZ=True, TIME_ZONE=\"America/New_York\"):\n            self.assertIsNotNone(connection.timezone)\n\n        with override_settings(USE_TZ=False):\n            self.assertIsNone(connection.timezone)\n"], "sample_438": ["def test_generic_foreign_key_checks(self):\n    class Model(models.Model):\n        field = GenericForeignKey()\n\n    errors = Model.check()\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], models.checks.Error)\n    self.assertEqual(errors[0].id, \"fields.E300\")\n    self.assertEqual(\n        errors[0].msg,\n        \"Field defines a relation with model 'Model', which is either not installed, or is abstract.\",\n    )\n", "def test_generic_foreign_key_representation(self):\n    class Model(models.Model):\n        field = GenericForeignKey()\n\n    model_instance = Model()\n    representation = repr(model_instance.field)\n    self.assertEqual(representation, \"contenttypes_tests.Model.field\")\n", "    def test_fields_cache(self):\n        model = Model()\n        self.assertEqual(model._state.fields_cache, {})\n        model._state.fields_cache[\"test\"] = \"value\"\n        self.assertEqual(model._state.fields_cache, {\"test\": \"value\"})\n", "    def test_model_full_clean_checks_unique_constraints(self):\n        class UniqueModel(models.Model):\n            unique_field = models.CharField(max_length=10, unique=True)\n\n        UniqueModel.objects.create(unique_field=\"test\")\n        instance = UniqueModel(unique_field=\"test\")\n\n        with self.assertRaises(ValidationError) as cm:\n            instance.full_clean()\n\n        self.assertEqual(len(cm.exception.error_dict), 1)\n        self.assertIn(\"unique_field\", cm.exception.error_dict)\n", "def test_generic_foreign_key_field_get_attname(self):\n    class Model(models.Model):\n        field = GenericForeignKey()\n\n    self.assertEqual(Model.field.get_attname(), \"field_id\")\n"], "sample_440": ["def test_update_conflicts_with_deferred_fields(self):\n    # Create an instance of UpsertConflict with deferred fields.\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 1)\n\n    # Attempt to update the instance with a conflicting object that has deferred fields.\n    conflicting_object = UpsertConflict(number=1, rank=4, name=\"Steve\")\n    conflicting_object._state.adding = False\n    UpsertConflict.objects.bulk_create(\n        [conflicting_object],\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 1)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": 4, \"name\": \"Steve\"},\n        ],\n    )\n", "def test_bulk_create_on_model_with_non_concrete_fields(self):\n    \"\"\"\n    Test that bulk_create() can be used on models with non-concrete fields.\n    \"\"\"\n    # Create a list of Country objects\n    countries = [\n        Country(name=\"United States of America\", iso_two_letter=\"US\"),\n        Country(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n    ]\n    \n    # Use bulk_create() to create the countries in the database\n    Country.objects.bulk_create(countries)\n    \n    # Check that the countries were created successfully\n    self.assertEqual(Country.objects.count(), 2)\n    self.assertQuerySetEqual(\n        Country.objects.order_by(\"-name\"),\n        [\n            \"United States of America\",\n            \"The Netherlands\",\n        ],\n        attrgetter(\"name\"),\n    )\n    \n    # Create a list of RelatedModel objects, which have a non-concrete field (country)\n    related_models = [\n        RelatedModel(country=countries[0], name=\"Related US\"),\n        RelatedModel(country=countries[1], name=\"Related NL\"),\n    ]\n    \n    # Use bulk_create() to create the related models in the database\n    RelatedModel.objects.bulk_create(related_models)\n    \n    # Check that the related models were created successfully\n    self.assertEqual(RelatedModel.objects.count(), 2)\n    self.assertQuerySetEqual(\n        RelatedModel.objects.order_by(\"-name\"),\n        [\n            \"Related US\",\n            \"Related NL\",\n        ],\n        attrgetter(\"name\"),\n    )\n", "def test_update_conflicts_with_custom_primary_key(self):\n    Country.objects.bulk_create(\n        [\n            Country(name=\"Germany\", iso_two_letter=\"DE\"),\n            Country(name=\"Czech Republic\", iso_two_letter=\"CZ\"),\n        ]\n    )\n    self.assertEqual(Country.objects.count(), 2)\n\n    conflicting_objects = [\n        Country(name=\"Germany\", iso_two_letter=\"XX\"),\n        Country(name=\"Czech Republic\", iso_two_letter=\"YY\"),\n    ]\n    Country.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"name\"],\n    )\n    self.assertEqual(Country.objects.count(), 2)\n    self.assertCountEqual(\n        Country.objects.values(\"iso_two_letter\", \"name\"),\n        [\n            {\"iso_two_letter\": \"XX\", \"name\": \"Germany\"},\n            {\"iso_two_letter\": \"YY\", \"name\": \"Czech Republic\"},\n        ],\n    )\n", "def test_update_conflicts_primary_keys_in_update_fields(self):\n    msg = \"bulk_create() cannot be used with primary key fields in update_fields.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n            ],\n            update_conflicts=True,\n            update_fields=[\"number\", \"name\"],\n        )\n", "def test_update_conflicts_with_pre_save(self):\n        instance.name = instance.name.upper()\n        return instance\n\n    # Connect the pre-save signal.\n    models.signals.pre_save.connect(update_name, sender=UpsertConflict)\n\n    try:\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n            ]\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n\n        conflicting_objects = [\n            UpsertConflict(number=1, rank=4, name=\"Steve\"),\n            UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n            UpsertConflict(number=3, rank=1, name=\"Hannah\"),\n        ]\n        UpsertConflict.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=[\"name\", \"rank\"],\n            unique_fields=[\"number\"],\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n        self.assertCountEqual(\n            UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n            [\n                {\"number\": 1, \"rank\": 4, \"name\": \"STEVE\"},\n                {\"number\": 2, \"rank\": 2, \"name\": \"OLIVIA\"},\n                {\"number\": 3, \"rank\": 1, \"name\": \"HANNAH\"},\n            ],\n        )\n    finally:\n        # Disconnect the pre-save signal.\n        models.signals.pre_save.disconnect(update_name, sender=UpsertConflict)\n"], "sample_441": ["    def test_render(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable password\"\n        html = widget.render(\"name\", value, {\"id\": \"id_password\"})\n        self.assertHTMLEqual(\n            html,\n            '<div id=\"id_password\">'\n            + str(_(\"No password set.\"))\n            + \"</div>\",\n        )\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable password\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_autocapitalize_none(self):\n        field = UsernameField()\n        self.assertEqual(field.widget_attrs({\"autocapitalize\": \"on\"})[\"autocapitalize\"], \"none\")\n", "    def test_widget_attrs(self):\n        field = UsernameField()\n        widget = field.widget\n        self.assertEqual(widget.attrs[\"autocapitalize\"], \"none\")\n        self.assertEqual(widget.attrs[\"autocomplete\"], \"username\")\n", "    def test_widget_attrs(self):\n        field = UsernameField()\n        widget = field.widget\n        self.assertEqual(\n            widget.attrs,\n            {\n                \"autocapitalize\": \"none\",\n                \"autocomplete\": \"username\",\n            },\n        )\n"], "sample_442": ["    def test_custom_serializer(self):\n        class CustomSerializer(signing.JSONSerializer):\n                return json.dumps(obj, separators=(\",\", \": \"), indent=4).encode(\"latin-1\")\n\n                return json.loads(data.decode(\"latin-1\"))\n\n        value = {\"foo\": \"bar\"}\n        signed = signing.dumps(value, serializer=CustomSerializer)\n        self.assertEqual(signing.loads(signed, serializer=CustomSerializer), value)\n", "    def test_custom_serializer(self):\n        class CustomSerializer:\n                return json.dumps(obj, separators=(\",\", \":\")).encode(\"utf-8\")\n\n                return json.loads(data.decode(\"utf-8\"))\n\n        signer = signing.Signer(key=\"predictable-secret\", serializer=CustomSerializer)\n        value = {\"foo\": \"bar\"}\n        signed_value = signer.sign_object(value)\n        self.assertEqual(signer.unsign_object(signed_value), value)\n", "    def test_dumps_loads_fallback_keys(self):\n        old_signer = signing.Signer(key=\"oldsecret\")\n        value = {\"foo\": \"bar\"}\n        signed_value = old_signer.sign_object(value)\n        self.assertEqual(signing.loads(signed_value), value)\n", "    def test_dumps_loads(self):\n        serializer = signing.JSONSerializer()\n        tests = [\n            [\"a\", \"list\"],\n            \"a string \\u2019\",\n            {\"a\": \"dictionary\"},\n        ]\n        for obj in tests:\n            with self.subTest(obj=obj):\n                dumped_obj = serializer.dumps(obj)\n                self.assertIsInstance(dumped_obj, bytes)\n                loaded_obj = serializer.loads(dumped_obj)\n                self.assertEqual(obj, loaded_obj)\n", "    def test_custom_serializer(self):\n        class CustomSerializer(signing.JSONSerializer):\n                return super().dumps(obj).upper()\n\n                return super().loads(data.lower())\n\n        signer = signing.Signer(key=\"secret\", serializer=CustomSerializer)\n        obj = {\"a\": \"b\"}\n        signed_obj = signer.sign_object(obj)\n        self.assertEqual(obj, signer.unsign_object(signed_obj))\n"], "sample_443": ["    def test_cache_non_existent_directory(self):\n        with self.assertRaises(OSError):\n            cache.set(\"key\", \"value\")\n", "    def test_max_entries(self):\n        cache = caches[\"default\"]\n        self.assertEqual(cache._max_entries, 1000)\n", "    def test_mkdir(self):\n        cache_dir = cache._dir\n        shutil.rmtree(cache_dir)\n        cache.set(\"key\", \"value\")\n        self.assertTrue(os.path.exists(cache_dir))\n", "def test_filesystem_caches_createdir_permissions(self):\n    \"\"\"Test that filesystem cache dir is created with correct permissions.\"\"\"\n    dir_name = tempfile.mkdtemp()\n    cache_dir = os.path.join(dir_name, \"test_cache\")\n    params = {\n        \"BACKEND\": \"django.core.cache.backends.filebased.FileBasedCache\",\n        \"LOCATION\": cache_dir,\n    }\n    with self.settings(CACHES={\"default\": params}):\n        cache.get(\"foo\")  # Trigger cache directory creation.\n    self.assertEqual(os.stat(cache_dir).st_mode & 0o777, 0o700)\n", "    def test_cache_key_length(self):\n        key = \"a\" * 250\n        cache_key = cache._key_to_file(key)\n        self.assertLessEqual(len(cache_key), 255)\n"], "sample_444": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, \"test\"))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def test_thread_safety(self):\n        import threading\n\n        threads = []\n        for _ in range(10):\n            t = threading.Thread(target=self.run_collectstatic)\n            threads.append(t)\n            t.start()\n\n        for t in threads:\n            t.join()\n\n        # If all threads finished without exceptions, it's a good sign that the\n        # storage is thread-safe. We can also check the manifest file to make\n        # sure everything was collected correctly.\n        manifest, _ = storage.staticfiles_storage.load_manifest()\n        self.assertEqual(len(manifest), len(storage.staticfiles_storage.hashed_files))\n", "    def test_init_with_manifest_storage(self):\n        manifest_storage = storage.StaticFilesStorage(location=\"/path/to/manifest\")\n        storage_instance = storage.ManifestStaticFilesStorage(manifest_storage=manifest_storage)\n        self.assertEqual(storage_instance.manifest_storage, manifest_storage)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, \"test\"))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, \"test\"))\n        self.addCleanup(shutil.rmtree, temp_dir)\n"], "sample_445": ["def test_time_strings_passed(self):\n    time_strings = {\n        \"year\": npgettext_lazy(\n            \"naturaltime-years\", \"%(num)d year\", \"%(num)d years\", \"num\"\n        ),\n        \"month\": npgettext_lazy(\n            \"naturaltime-months\", \"%(num)d month\", \"%(num)d months\", \"num\"\n        ),\n        \"week\": npgettext_lazy(\n            \"naturaltime-weeks\", \"%(num)d week\", \"%(num)d weeks\", \"num\"\n        ),\n        \"day\": npgettext_lazy(\n            \"naturaltime-days\", \"%(num)d day\", \"%(num)d days\", \"num\"\n        ),\n        \"hour\": npgettext_lazy(\n            \"naturaltime-hours\", \"%(num)d hour\", \"%(num)d hours\", \"num\"\n        ),\n        \"minute\": npgettext_lazy(\n            \"naturaltime-minutes\", \"%(num)d minute\", \"%(num)d minutes\", \"num\"\n        ),\n    }\n    with translation.override(\"cs\"):\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=time_strings), \"1\\xa0t\u00fdden\")\n        self.assertEqual(timeuntil(self.t + self.oneweek, self.t, time_strings=time_strings), \"1\\xa0t\u00fdden\")\n", "def test_invalid_input(self):\n    msg = \"timesince expected a datetime or date, got str.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        timesince(\"2022-01-01\", self.t)\n", "def test_depth_too_large(self):\n    t = (\n        self.t\n        + self.oneyear\n        + self.onemonth\n        + self.oneweek\n        + self.oneday\n        + self.onehour\n    )\n    expected = \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"\n    self.assertEqual(timesince(self.t, t, depth=10), expected)\n", "def test_depth_too_large(self):\n    t = (\n        self.t\n        + self.oneyear\n        + self.onemonth\n        + self.oneweek\n        + self.oneday\n        + self.onehour\n    )\n    tests = [\n        (t, 10, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n        (self.t + self.onehour, 10, \"1\\xa0hour\"),\n        (self.t + (4 * self.oneminute), 10, \"4\\xa0minutes\"),\n        (self.t + self.onehour + self.oneminute, 10, \"1\\xa0hour\"),\n        (self.t + self.oneday + self.onehour, 10, \"1\\xa0day\"),\n        (self.t + self.oneweek + self.oneday, 10, \"1\\xa0week\"),\n        (self.t + self.onemonth + self.oneweek, 10, \"1\\xa0month\"),\n        (self.t + self.oneyear + self.onemonth, 10, \"1\\xa0year\"),\n        (self.t + self.oneyear + self.oneweek + self.oneday, 10, \"1\\xa0year\"),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(self.t, value, depth=depth), expected)\n            self.assertEqual(timeuntil(value, self.t, depth=depth), expected)\n", "def test_years_edge(self):\n    t = datetime.datetime(2022, 1, 1)\n    tests = [\n        (datetime.datetime(2023, 1, 1), \"1\\xa0year\"),\n        (datetime.datetime(2023, 12, 31), \"1\\xa0year, 11\\xa0months\"),\n        (datetime.datetime(2024, 1, 1), \"2\\xa0years\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value), expected)\n"], "sample_446": ["    def test_add(self):\n        output = self.engine.render_to_string(\"add01\", {\"num\": 5})\n        self.assertEqual(output, \"10\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 4})\n        self.assertEqual(output, \"6\")\n", "    def test_floatformat_round_half_up(self):\n        # Test that floatformat rounds numbers ending in 5 away from zero.\n        self.assertEqual(floatformat(1.005, 2), \"1.01\")\n        self.assertEqual(floatformat(-1.005, 2), \"-1.01\")\n        self.assertEqual(floatformat(0.0005, 4), \"0.0005\")\n        self.assertEqual(floatformat(-0.0005, 4), \"-0.0005\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5})\n        self.assertEqual(output, \"10\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 1, \"b\": 2})\n        self.assertEqual(output, \"3\")\n"], "sample_447": ["def test_alias_subquery_outerref_transform(self):\n    qs = Book.objects.alias(\n        top_rating_year=Subquery(\n            Book.objects.filter(pubdate__year=OuterRef(\"pubdate__year\"))\n            .order_by(\"-rating\")\n            .values(\"rating\")[:1]\n        ),\n    ).values(\"pubdate__year\", \"top_rating_year\")\n    self.assertCountEqual(\n        qs,\n        [\n            {\"pubdate__year\": 1991, \"top_rating_year\": 5.0},\n            {\"pubdate__year\": 1995, \"top_rating_year\": 4.0},\n            {\"pubdate__year\": 2007, \"top_rating_year\": 4.5},\n            {\"pubdate__year\": 2008, \"top_rating_year\": 4.0},\n            {\"pubdate__year\": 2008, \"top_rating_year\": 4.0},\n            {\"pubdate__year\": 2008, \"top_rating_year\": 4.0},\n        ],\n    )\n", "def test_resolve_expression(self):\n    expr = F(\"rating\")\n    resolved = expr.resolve_expression(query=Book.objects.all())\n    self.assertIsInstance(resolved, Col)\n    self.assertEqual(resolved.alias, \"annotations_book\")\n    self.assertEqual(resolved.target.column, \"rating\")\n\n    # Ensure the output field is correctly set.\n    output_field = expr.output_field\n    self.assertIsNone(output_field)\n    self.assertIsInstance(resolved.output_field, fields.FloatField)\n\n    # If the output field is already set, it's not changed.\n    expr = F(\"rating\", output_field=fields.IntegerField())\n    resolved = expr.resolve_expression(query=Book.objects.all())\n    self.assertIsInstance(resolved, Col)\n    self.assertEqual(resolved.alias, \"annotations_book\")\n    self.assertEqual(resolved.target.column, \"rating\")\n    self.assertIsInstance(resolved.output_field, fields.IntegerField)\n", "def test_alias_with_subquery(self):\n    subquery = Book.objects.filter(pages__gt=400).values(\"publisher\")\n    qs = Publisher.objects.alias(has_long_books=Exists(subquery)).filter(\n        has_long_books=True\n    )\n    self.assertCountEqual(qs, [self.p1, self.p2, self.p3, self.p4])\n", "def test_alias_subquery(self):\n    subquery = Book.objects.filter(pages__gt=400).values(\"publisher\")\n    qs = Publisher.objects.alias(\n        has_long_books=Exists(subquery.filter(publisher=OuterRef(\"pk\")))\n    ).filter(has_long_books=True)\n    self.assertSequenceEqual(qs, [self.p1, self.p2, self.p3, self.p4])\n", "def test_resolve_expression_with_rawsql(self):\n    raw_sql = RawSQL(\"SELECT 1\", ())\n    resolved = raw_sql.resolve_expression()\n    self.assertEqual(resolved, raw_sql)\n"], "sample_448": ["def test_validate_with_exclude_and_deferrable(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"],\n        name=\"name_unique\",\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    instance = UniqueConstraintDeferrable(name=\"test\", shelf=\"front\")\n    # Validation should ignore excluded fields.\n    constraint.validate(UniqueConstraintDeferrable, instance, exclude={\"name\"})\n    # Validation should raise an error if the constraint is deferred and not excluded.\n    with self.assertRaisesMessage(\n        ValidationError, \"Constraint \u201cname_unique\u201d is violated.\"\n    ):\n        constraint.validate(UniqueConstraintDeferrable, instance)\n", "def test_validate_expression_condition_with_exclude(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_without_color_uniq\",\n        condition=models.Q(color__isnull=True),\n    )\n    non_unique_product = UniqueConstraintProduct(name=self.p2.name.upper())\n    msg = \"Constraint \u201cname_lower_without_color_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    # Field from a condition is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"color\"},\n    )\n    # Expression field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n", "def test_validate_expression_with_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    instance = UniqueConstraintProduct(name=self.p1.name.upper())\n    # Exclude should ignore the case.\n    constraint.validate(\n        UniqueConstraintProduct,\n        instance,\n        exclude={\"name\"},\n    )\n    with self.assertRaises(ValidationError):\n        constraint.validate(\n            UniqueConstraintProduct,\n            instance,\n            exclude={\"other_field\"},\n        )\n", "def test_clone(self):\n    constraint = models.UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n\n    constraint_with_condition = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        condition=models.Q(foo=models.F(\"bar\")),\n    )\n    cloned_constraint_with_condition = constraint_with_condition.clone()\n    self.assertEqual(constraint_with_condition, cloned_constraint_with_condition)\n    self.assertIsNot(constraint_with_condition, cloned_constraint_with_condition)\n\n    constraint_with_deferrable = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    cloned_constraint_with_deferrable = constraint_with_deferrable.clone()\n    self.assertEqual(constraint_with_deferrable, cloned_constraint_with_deferrable)\n    self.assertIsNot(constraint_with_deferrable, cloned_constraint_with_deferrable)\n\n    constraint_with_include = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        include=[\"baz_1\", \"baz_2\"],\n    )\n    cloned_constraint_with_include = constraint_with_include.clone()\n    self.assertEqual(constraint_with_include, cloned_constraint_with_include)\n    self.assertIsNot(constraint_with_include, cloned_constraint_with_include)\n\n    constraint_with_opclasses = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        opclasses=[\"varchar_pattern_ops\", \"text_pattern_ops\"],\n    )\n    cloned_constraint_with_opclasses = constraint_with_opclasses.clone()\n    self.assertEqual(constraint_with_opclasses, cloned_constraint_with_opclasses)\n    self.assertIsNot(constraint_with_opclasses, cloned_constraint_with_opclasses)\n\n    constraint_with_expressions = models.UniqueConstraint(\n        Lower(\"title\"),\n        F(\"author\"),\n        name=\"book_func_uq\",\n    )\n    cloned_constraint_with_expressions = constraint_with_expressions.clone()\n    self.assertEqual(constraint_with_expressions, cloned_constraint_with_expressions)\n    self.assertIsNot(constraint_with_expressions, cloned_constraint_with_expressions)\n", "def test_validate_expression_with_database_value(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    p1 = UniqueConstraintProduct.objects.create(name=\"Hello\")\n    # Simulate database value.\n    p1.name = \"hello\"\n    constraint.validate(UniqueConstraintProduct, p1)\n    # Different case should be considered equal.\n    with self.assertRaisesMessage(\n        ValidationError, \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    ):\n        constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=\"hELLo\"))\n    # Different value is valid.\n    constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=\"World\"))\n"], "sample_449": ["    def test_connections_override(self):\n        \"\"\"ThreadedWSGIServer overrides connections for new threads.\"\"\"\n        server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler)\n        connections_override = {\"default\": \"test connection\"}\n        server.connections_override = connections_override\n\n        class MockRequest:\n                self.server = server\n\n        class MockThread:\n                self.target = target\n                self.args = args\n\n                self.target(*self.args)\n\n        original_thread = socketserver.ThreadingMixIn.process_request_thread\n        socketserver.ThreadingMixIn.process_request_thread = MockThread\n\n        try:\n            request = MockRequest(server)\n            client_address = (\"192.168.2.0\", 8080)\n            server.process_request_thread(request, client_address)\n            self.assertEqual(connections_override, connections)\n        finally:\n            socketserver.ThreadingMixIn.process_request_thread = original_thread\n            server.server_close()\n", "def test_address_string(self):\n    \"\"\"WSGIRequestHandler returns the client address without calling socket.getfqdn.\"\"\"\n    request = WSGIRequest(self.request_factory.get(\"/\").environ)\n    client_address = (\"192.168.2.0\", 8080)\n    handler = WSGIRequestHandler(request, *client_address)\n    self.assertEqual(handler.address_string(), client_address[0])\n", "    def test_finish_response_head_request(self):\n        \"\"\"ServerHandler finishes response for HEAD requests.\"\"\"\n        environ = self.request_factory.head(\"/\").environ\n        rfile = BytesIO()\n        wfile = UnclosableBytesIO()\n\n            return wfile.write\n\n        handler = ServerHandler(rfile, wfile, None, environ)\n        handler.result = [b\"Hello World\"]\n        handler.finish_response()\n\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n        self.assertEqual(lines[-1], b\"\\r\\n\")\n        self.assertIs(\n            any([line.startswith(b\"Content-Length:\") for line in lines]), False\n        )\n        self.assertNotIn(b\"Connection: close\\r\\n\", lines)\n", "    def test_close_connections(self):\n        \"\"\"ThreadedWSGIServer closes database connections.\"\"\"\n        server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler)\n        try:\n            # Simulate a request that opens database connections.\n            with self.settings(DATABASES={\n                'default': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                    'NAME': ':memory:',\n                }\n            }):\n                request = WSGIRequest(self.request_factory.get(\"/\").environ)\n                client_address = (\"192.168.2.0\", 8080)\n                server.process_request_thread(request, client_address)\n\n            # Check that the database connections are closed after the request.\n            from django.db import connections\n            self.assertTrue(all(not conn.connection for conn in connections.all()))\n        finally:\n            server.server_close()\n", "    def test_finish_response_head_request(self):\n        \"\"\"ServerHandler finishes response for HEAD requests.\"\"\"\n        hello_world_body = b\"<!DOCTYPE html><html><body>Hello World</body></html>\"\n        content_length = len(hello_world_body)\n\n            \"\"\"A WSGI app that returns a hello world.\"\"\"\n            start_response(\"200 OK\", [])\n            return [hello_world_body]\n\n        rfile = BytesIO(b\"HEAD / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        handler = ServerHandler(rfile, wfile, None, request.get_environ())\n        handler.request_handler = Stub(server=server)\n        handler.run(server.get_app())\n\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n        body = lines[-1]\n        # The body is not returned in a HEAD response.\n        self.assertEqual(body, b\"\\r\\n\")\n        self.assertIs(\n            any([line.startswith(b\"Content-Length:\") for line in lines]), False\n        )\n        self.assertNotIn(b\"Connection: close\\r\\n\", lines)\n"], "sample_450": ["def test_get_admin_log_template_tag(self):\n    \"\"\"\n    The get_admin_log template tag returns a list of LogEntry objects for the \n    given criteria.\n    \"\"\"\n    # Create some log entries\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something\",\n    )\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something else\",\n    )\n\n    # Test the template tag\n    template = Template('{% load admin_list %}{% get_admin_log 10 as admin_log for_user user %}{{ admin_log|length }}')\n    context = Context({'user': self.user})\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n\n    # Test the template tag with no user\n    template = Template('{% load admin_list %}{% get_admin_log 10 as admin_log %}{{ admin_log|length }}')\n    context = Context({})\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n\n    # Test the template tag with a different user\n    other_user = User.objects.create_superuser(\n        username=\"other\", password=\"secret\", email=\"other@example.com\"\n    )\n    template = Template('{% load admin_list %}{% get_admin_log 10 as admin_log for_user other_user %}{{ admin_log|length }}')\n    context = Context({'other_user': other_user})\n    rendered = template.render(context)\n    self.assertEqual(rendered, '0')\n", "def test_get_admin_log_template_tag(self):\n    \"\"\"\n    The get_admin_log template tag returns a list of LogEntry objects for the \n    given criteria.\n    \"\"\"\n    # Create some log entries\n    LogEntry.objects.log_action(\n        self.user.pk,\n        ContentType.objects.get_for_model(Article).pk,\n        self.a1.pk,\n        \"Article changed\",\n        CHANGE,\n        change_message=\"Article changed message\",\n    )\n    LogEntry.objects.log_action(\n        self.user.pk,\n        ContentType.objects.get_for_model(Article).pk,\n        self.a1.pk,\n        \"Article changed again\",\n        CHANGE,\n        change_message=\"Article changed again message\",\n    )\n\n    # Test the template tag\n    template = Template('{% load admin_utils %}{% get_admin_log 10 as admin_log for_user user %}{{ admin_log|length }}')\n    context = Context({'user': self.user, 'log_entries': LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n\n    # Test the template tag with a specific user\n    template = Template('{% load admin_utils %}{% get_admin_log 10 as admin_log for_user user.pk %}{{ admin_log|length }}')\n    context = Context({'user': self.user, 'log_entries': LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n\n    # Test the template tag without a user\n    template = Template('{% load admin_utils %}{% get_admin_log 10 as admin_log %}{{ admin_log|length }}')\n    context = Context({'log_entries': LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n", "def test_get_admin_log_template_tag(self):\n    LogEntry.objects.log_action(\n        self.user.pk,\n        ContentType.objects.get_for_model(Article).pk,\n        self.a1.pk,\n        \"Article changed\",\n        CHANGE,\n        change_message=\"Article changed message\",\n    )\n    template = Template(\"{% load admin_list %}{% get_admin_log 10 as admin_log for_user user %}{{ admin_log|length }}\")\n    context = Context({\"user\": self.user, \"log_entries\": LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"1\")\n\n    # test with no entries\n    LogEntry.objects.all().delete()\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"0\")\n\n    # test with different user\n    other_user = User.objects.create_superuser(\"other\", \"secret\", \"other@example.com\")\n    LogEntry.objects.log_action(\n        other_user.pk,\n        ContentType.objects.get_for_model(Article).pk,\n        self.a1.pk,\n        \"Article changed\",\n        CHANGE,\n        change_message=\"Article changed message\",\n    )\n    context[\"user\"] = other_user\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"1\")\n\n    # test without for_user\n    template = Template(\"{% load admin_list %}{% get_admin_log 10 as admin_log %}{{ admin_log|length }}\")\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"1\")\n", "def test_get_admin_log_template_tag(self):\n    \"\"\"\n    The get_admin_log template tag returns a list of LogEntry objects for the\n    given user and content type.\n    \"\"\"\n    log_entries = LogEntry.objects.all()\n    self.assertEqual(len(log_entries), 1)\n\n    # Create a new log entry for a different user\n    user2 = User.objects.create_superuser(\n        username=\"super2\", password=\"secret\", email=\"super2@example.com\"\n    )\n    LogEntry.objects.log_action(\n        user2.pk,\n        ContentType.objects.get_for_model(Article).pk,\n        self.a1.pk,\n        \"Article changed\",\n        CHANGE,\n        change_message=\"Article changed message\",\n    )\n\n    # Test the template tag with a specific user\n    template = Template('{% load admin_utils_tags %}{% get_admin_log 10 as admin_log for_user user %}{{ admin_log|length }}')\n    context = Context({'user': self.user})\n    rendered = template.render(context)\n    self.assertEqual(rendered, '1')\n\n    # Test the template tag with no user specified\n    template = Template('{% load admin_utils_tags %}{% get_admin_log 10 as admin_log %}{{ admin_log|length }}')\n    context = Context({})\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n", "def test_get_admin_log_tag(self):\n    \"\"\"\n    The get_admin_log tag should render a list of admin log entries.\n    \"\"\"\n    LogEntry.objects.log_action(\n        self.user.pk,\n        ContentType.objects.get_for_model(Article).pk,\n        self.a1.pk,\n        \"Article changed\",\n        CHANGE,\n        change_message=\"Article changed message\",\n    )\n    template = Template(\"{% load admin_log %}{% get_admin_log 10 as admin_log for_user user %}{{ admin_log|length }}\")\n    context = Context({\"user\": self.user, \"log_entries\": LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"1\")\n\n    # Test without user\n    template = Template(\"{% load admin_log %}{% get_admin_log 10 as admin_log %}{{ admin_log|length }}\")\n    context = Context({\"log_entries\": LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"1\")\n\n    # Test with incorrect syntax\n    with self.assertRaises(TemplateSyntaxError):\n        Template(\"{% load admin_log %}{% get_admin_log %}\")\n\n    with self.assertRaises(TemplateSyntaxError):\n        Template(\"{% load admin_log %}{% get_admin_log 10 as admin_log for_user %}\")\n\n    with self.assertRaises(TemplateSyntaxError):\n        Template(\"{% load admin_log %}{% get_admin_log 10 as admin_log for_user user foo %}\")\n"], "sample_451": ["def test_get_view_name(self):\n    from django.views.generic import View\n\n    class TestView(View):\n        pass\n\n    view_func = TestView.as_view()\n    self.assertEqual(get_view_name(view_func), f\"{__name__}.TestView\")\n\n        pass\n\n    self.assertEqual(get_view_name(test_function_view), f\"{__name__}.test_function_view\")\n", "def test_replace_metacharacters(self):\n    pattern = r\"^Hello, world\\?$\"\n    expected_output = r\"^Hello, world$\"\n    self.assertEqual(replace_metacharacters(pattern), expected_output)\n", "def test_replace_metacharacters(self):\n    pattern = r\"^Hello, world\\?$\"\n    expected_output = \"^Hello, world$\"\n    self.assertEqual(replace_metacharacters(pattern), expected_output)\n\n    pattern = r\"^\\(Hello, world\\)\\?$\"\n    expected_output = \"\\(Hello, world\\)$\"\n    self.assertEqual(replace_metacharacters(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r\"^/(?P<a>\\w+)/b/(?P<c>\\w+)/$\"\n    expected_output = \"^/<a>/b/<c>/$\"\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r\"^/(?P<a>\\w+)/b/(?P<c>\\w+)/$\"\n    expected_output = \"^/<a>/b/<c>/$\"\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r\"^/(?P<a>\\w+)/b/(\\w+)$\"\n    expected_output = \"^/<a>/b/(\\\\w+)$\"\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n"], "sample_453": ["def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should correctly generate JSON data.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n    try:\n        json.loads(template_context[\"prepopulated_fields_json\"])\n    except ValueError as e:\n        self.fail(f\"Invalid JSON: {e}\")\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should return a context with the\n    correct prepopulated fields.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn(\"prepopulated_fields\", template_context)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    self.assertIsInstance(template_context[\"prepopulated_fields\"], list)\n    self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should include the necessary fields.\n    \"\"\"\n    admin = UserAdmin(User, site)\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn(\"prepopulated_fields\", template_context)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should return a list of dictionaries \n    containing field information.\n    \"\"\"\n    admin = UserAdmin(User, site)\n    admin.prepopulated_fields = {\"username\": (\"first_name\", \"last_name\")}\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    prepopulated_fields = template_context[\"prepopulated_fields_json\"]\n    self.assertIsInstance(prepopulated_fields, str)\n    fields = json.loads(prepopulated_fields)\n    self.assertEqual(len(fields), 1)\n    field = fields[0]\n    self.assertIn(\"id\", field)\n    self.assertIn(\"name\", field)\n    self.assertIn(\"dependency_ids\", field)\n    self.assertIn(\"dependency_list\", field)\n    self.assertIn(\"maxLength\", field)\n    self.assertIn(\"allowUnicode\", field)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should pass whole context and\n    correctly populate fields.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    extra_context = {\"extra\": True}\n    response = admin.change_view(\n        request, str(self.superuser.pk), extra_context=extra_context\n    )\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIs(template_context[\"extra\"], True)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n\n    # Check that fields are correctly populated\n    fields = json.loads(template_context[\"prepopulated_fields_json\"])\n    for field in fields:\n        self.assertIn(\"id\", field)\n        self.assertIn(\"name\", field)\n        self.assertIn(\"dependency_ids\", field)\n        self.assertIn(\"dependency_list\", field)\n        self.assertIn(\"maxLength\", field)\n        self.assertIn(\"allowUnicode\", field)\n"], "sample_455": ["def test_clone(self):\n    constraint = UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n\n    constraint_with_condition = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        condition=models.Q(foo=models.F(\"bar\")),\n    )\n    cloned_constraint_with_condition = constraint_with_condition.clone()\n    self.assertEqual(constraint_with_condition, cloned_constraint_with_condition)\n    self.assertIsNot(constraint_with_condition, cloned_constraint_with_condition)\n\n    constraint_with_deferrable = UniqueConstraint(\n        fields=[\"foo\"],\n        name=\"unique\",\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    cloned_constraint_with_deferrable = constraint_with_deferrable.clone()\n    self.assertEqual(constraint_with_deferrable, cloned_constraint_with_deferrable)\n    self.assertIsNot(constraint_with_deferrable, cloned_constraint_with_deferrable)\n\n    constraint_with_include = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"include\",\n        include=[\"baz_1\", \"baz_2\"],\n    )\n    cloned_constraint_with_include = constraint_with_include.clone()\n    self.assertEqual(constraint_with_include, cloned_constraint_with_include)\n    self.assertIsNot(constraint_with_include, cloned_constraint_with_include)\n\n    constraint_with_opclasses = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"opclasses\",\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n    )\n    cloned_constraint_with_opclasses = constraint_with_opclasses.clone()\n    self.assertEqual(constraint_with_opclasses, cloned_constraint_with_opclasses)\n    self.assertIsNot(constraint_with_opclasses, cloned_constraint_with_opclasses)\n\n    constraint_with_expressions = UniqueConstraint(\n        Lower(\"title\"),\n        F(\"author\"),\n        name=\"book_func_uq\",\n    )\n    cloned_constraint_with_expressions = constraint_with_expressions.clone()\n    self.assertEqual(constraint_with_expressions, cloned_constraint_with_expressions)\n    self.assertIsNot(constraint_with_expressions, cloned_constraint_with_expressions)\n", "def test_clone(self):\n    constraint = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique_fields\",\n        condition=models.Q(foo=models.F(\"bar\")),\n        deferrable=models.Deferrable.IMMEDIATE,\n        include=[\"baz_1\", \"baz_2\"],\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n        violation_error_message=\"Custom message\",\n        violation_error_code=\"custom_code\",\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n", "def test_validate_expression_with_database_function(self):\n    constraint = models.UniqueConstraint(\n        models.F(\"name__lower\"),\n        name=\"name_lower_uniq\",\n    )\n    msg = \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper()),\n        exclude={\"name\"},\n    )\n", "    def test_exclude_works_with_field_name(self):\n        constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n        instance = UniqueConstraintProduct(name=self.p1.name)\n        with self.assertRaises(ValidationError):\n            constraint.validate(UniqueConstraintProduct, instance)\n        constraint.validate(\n            UniqueConstraintProduct,\n            instance,\n            exclude={\"name\"},\n        )\n", "def test_validate_with_exclude(self):\n    constraint = UniqueConstraintProduct._meta.constraints[0]\n    # Exclude one of the fields in the unique constraint.\n    exclude = {\"name\"}\n    non_unique_product = UniqueConstraintProduct(\n        name=self.p1.name, color=self.p1.color\n    )\n    constraint.validate(UniqueConstraintProduct, non_unique_product, exclude=exclude)\n    # Exclude all fields in the unique constraint.\n    exclude = {\"name\", \"color\"}\n    constraint.validate(UniqueConstraintProduct, non_unique_product, exclude=exclude)\n"], "sample_456": ["def test_cleaned_data_with_deleted_forms(self):\n    \"\"\"cleaned_data doesn't include deleted forms.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n    ]\n    data = {\n        \"choices-TOTAL_FORMS\": \"3\",  # the number of forms rendered\n        \"choices-INITIAL_FORMS\": \"2\",  # the number of forms with initial data\n        \"choices-MIN_NUM_FORMS\": \"0\",  # min number of forms\n        \"choices-MAX_NUM_FORMS\": \"0\",  # max number of forms\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-0-DELETE\": \"\",\n        \"choices-1-choice\": \"Fergie\",\n        \"choices-1-votes\": \"900\",\n        \"choices-1-DELETE\": \"on\",\n        \"choices-2-choice\": \"The Decemberists\",\n        \"choices-2-votes\": \"500\",\n        \"choices-2-DELETE\": \"\",\n    }\n    formset = ChoiceFormSet(data, initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.forms],\n        [\n            {\"votes\": 100, \"DELETE\": False, \"choice\": \"Calexico\"},\n            {\"votes\": 900, \"DELETE\": True, \"choice\": \"Fergie\"},\n            {\"votes\": 500, \"DELETE\": False, \"choice\": \"The Decemberists\"},\n        ],\n    )\n    self.assertEqual(\n        formset.cleaned_data,\n        [{\"votes\": 100, \"DELETE\": False, \"choice\": \"Calexico\"}, {\"votes\": 500, \"DELETE\": False, \"choice\": \"The Decemberists\"}],\n    )\n", "def test_all_valid_empty_formset(self):\n    \"\"\"all_valid() works with an empty formset.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet()\n    formset2 = ChoiceFormSet()\n    self.assertIs(all_valid((formset1, formset2)), False)\n", "def test_non_field_errors_are_caught_by_formset(self):\n    data = {\n        \"form-TOTAL_FORMS\": \"2\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-0-title\": \"Test\",\n        \"form-0-pub_date\": \"1904-06-16\",\n        \"form-1-title\": \"Test\",\n        \"form-1-pub_date\": \"1904-06-16\",\n    }\n    formset = ArticleFormSet(data)\n    formset.forms[0].add_error(None, \"Non-field error\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        [{}, {}], formset.errors\n    )  # errors are empty because they're stored in _non_form_errors\n    self.assertEqual([\"Non-field error\"], formset.non_form_errors())\n", "def test_cleaned_data_after_error(self):\n    \"\"\"If a formset has errors, its cleaned_data should be an empty list.\"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"1\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"\",  # missing value\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.cleaned_data, [])\n", "def test_formset_with_disabled_fields(self):\n    \"\"\"Formsets with disabled fields.\"\"\"\n    class DisabledForm(Form):\n        name = CharField(disabled=True)\n\n    DisabledFormSet = formset_factory(DisabledForm)\n    data = {\n        \"form-TOTAL_FORMS\": \"1\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-0-name\": \"Test\",\n    }\n    formset = DisabledFormSet(data)\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([{}], formset.cleaned_data)\n"], "sample_457": ["def test_validate_expression_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    existing_product = UniqueConstraintProduct.objects.create(name=\"Existing Product\")\n    new_product = UniqueConstraintProduct(name=\"existing product\")\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, new_product)\n    constraint.validate(\n        UniqueConstraintProduct,\n        new_product,\n        exclude={\"name\"},\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        existing_product,\n    )\n", "def test_validate_nullable_field_with_none(self):\n    constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n    instance = UniqueConstraintProduct(name=None)\n    # Validation should pass when the field value is None.\n    constraint.validate(UniqueConstraintProduct, instance)\n", "def test_clone(self):\n    constraint = UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n\n    constraint_with_condition = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        condition=models.Q(foo=models.F(\"bar\")),\n    )\n    cloned_constraint_with_condition = constraint_with_condition.clone()\n    self.assertEqual(constraint_with_condition, cloned_constraint_with_condition)\n    self.assertIsNot(constraint_with_condition, cloned_constraint_with_condition)\n\n    constraint_with_deferrable = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    cloned_constraint_with_deferrable = constraint_with_deferrable.clone()\n    self.assertEqual(constraint_with_deferrable, cloned_constraint_with_deferrable)\n    self.assertIsNot(constraint_with_deferrable, cloned_constraint_with_deferrable)\n\n    constraint_with_include = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        include=[\"baz_1\", \"baz_2\"],\n    )\n    cloned_constraint_with_include = constraint_with_include.clone()\n    self.assertEqual(constraint_with_include, cloned_constraint_with_include)\n    self.assertIsNot(constraint_with_include, cloned_constraint_with_include)\n\n    constraint_with_opclasses = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n    )\n    cloned_constraint_with_opclasses = constraint_with_opclasses.clone()\n    self.assertEqual(constraint_with_opclasses, cloned_constraint_with_opclasses)\n    self.assertIsNot(constraint_with_opclasses, cloned_constraint_with_opclasses)\n\n    constraint_with_expressions = UniqueConstraint(\n        Lower(\"title\"),\n        F(\"author\"),\n        name=\"book_func_uq\",\n    )\n    cloned_constraint_with_expressions = constraint_with_expressions.clone()\n    self.assertEqual(constraint_with_expressions, cloned_constraint_with_expressions)\n    self.assertIsNot(constraint_with_expressions, cloned_constraint_with_expressions)\n", "def test_contains_expressions(self):\n    constraint = UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n    self.assertIs(constraint.contains_expressions, False)\n    constraint = UniqueConstraint(Lower(\"name\"), name=\"unique_name\")\n    self.assertIs(constraint.contains_expressions, True)\n", "def test_clone_with_violation_error_message(self):\n    constraint = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        violation_error_message=\"custom error\",\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(\n        cloned_constraint.violation_error_message,\n        constraint.violation_error_message,\n    )\n"], "sample_458": ["def test_non_numeric_input(self):\n    non_numeric_inputs = [\"foo\", \"\u00bfC\u00f3mo esta usted?\", None]\n    for input_value in non_numeric_inputs:\n        with self.subTest(input_value=input_value):\n            self.assertEqual(floatformat(input_value), \"\")\n            self.assertEqual(floatformat(input_value, 2), \"\")\n            self.assertEqual(floatformat(input_value, -2), \"\")\n            self.assertEqual(floatformat(input_value, \"g\"), \"\")\n            self.assertEqual(floatformat(input_value, \"u\"), \"\")\n", "    def test_slugify01(self):\n        output = self.engine.render_to_string(\"slugify01\", {\"a\": \"Hello, World!\"})\n        self.assertEqual(output, \"hello-world\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 12345})\n        self.assertEqual(output, \"1.234500e+04\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5})\n        self.assertEqual(output, \"7\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 4})\n        self.assertEqual(output, \"6\")\n"], "sample_459": ["    def test_get_prep_lookup(self):\n        lookup = In(lhs=models.IntegerField(), rhs=[1, 2, 3])\n        prep_lookup = lookup.get_prep_lookup()\n        self.assertEqual(prep_lookup, [1, 2, 3])\n", "    def test_uuidfield_cleans_valid_string(self):\n        f = models.UUIDField()\n        import uuid\n        value = str(uuid.uuid4())\n        self.assertEqual(f.clean(value, None), uuid.UUID(value))\n", "    def test_uuid_field_lookups(self):\n        from django.db.models import UUIDField\n\n        uuid_value = \"01234567-89ab-cdef-0123-456789abcdef\"\n\n        class UUIDModel(models.Model):\n            value = UUIDField()\n\n        UUIDModel.objects.create(value=uuid_value)\n\n        self.assertEqual(UUIDModel.objects.filter(value__exact=uuid_value).count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(value__iexact=uuid_value).count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(value__contains=\"0123\").count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(value__icontains=\"0123\").count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(value__startswith=\"0123\").count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(value__istartswith=\"0123\").count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(value__endswith=\"abcdef\").count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(value__iendswith=\"abcdef\").count(), 1)\n", "    def test_uuidfield_cleans_valid_string(self):\n        f = models.UUIDField()\n        self.assertEqual(f.clean(\"550e8400-e29b-11d4-a716-446655440000\", None), uuid.UUID('550e8400-e29b-11d4-a716-446655440000'))\n", "    def test_uuid_field_cleans_valid_uuid(self):\n        f = models.UUIDField()\n        self.assertEqual(f.clean(\"01234567-89ab-cdef-0123-456789abcdef\", None), uuid.UUID('01234567-89ab-cdef-0123-456789abcdef'))\n"], "sample_461": ["    def test_uuidfield_widget(self):\n        f = UUIDField()\n        self.assertWidgetRendersTo(f, '<input type=\"text\" name=\"f\" id=\"id_f\" required>')\n", "    def test_genericipaddressfield_widget(self):\n        f = GenericIPAddressField()\n        self.assertWidgetRendersTo(f, '<input type=\"text\" name=\"f\" id=\"id_f\" required>')\n", "    def test_uuidfield_widget(self):\n        f = UUIDField()\n        self.assertWidgetRendersTo(f, '<input type=\"text\" name=\"f\" id=\"id_f\" required>')\n", "def test_urlfield_empty_value(self):\n    f = URLField(required=False, empty_value=\"https://example.com\")\n    self.assertEqual(f.clean(\"\"), \"https://example.com\")\n    self.assertEqual(f.clean(None), \"https://example.com\")\n", "    def test_urlfield_max_length(self):\n        f = URLField(max_length=10)\n        msg = \"'Ensure this value has at most 10 characters (it has 11).'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"http://example\")\n        self.assertEqual(f.clean(\"http://exam\"), \"http://exam\")\n"], "sample_462": ["def test_choicefield_initial(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], initial=\"2\")\n    self.assertEqual(f.initial, \"2\")\n    self.assertWidgetRendersTo(\n        f,\n        '<select id=\"id_f\" name=\"f\">'\n        '<option value=\"1\">One</option>'\n        '<option value=\"2\" selected>Two</option>'\n        \"</select>\",\n    )\n", "def test_choicefield_empty_value(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], empty_value=None)\n    self.assertIsNone(f.clean(\"\"))\n    self.assertIsNone(f.clean(None))\n    self.assertEqual(\"1\", f.clean(1))\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_with_initial(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], initial=\"2\")\n    self.assertEqual(\"2\", f.clean(\"2\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n    # If the initial value isn't in the choices, it should be ignored.\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], initial=\"3\")\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_error_messages(self):\n    f = ChoiceField(\n        choices=[(\"1\", \"One\"), (\"2\", \"Two\")],\n        error_messages={\"required\": \"You must select a choice.\"},\n    )\n    with self.assertRaisesMessage(ValidationError, \"You must select a choice.\"):\n        f.clean(\"\")\n", "def test_choicefield_widget_choices(self):\n    f = ChoiceField(choices=[(\"J\", \"John\"), (\"P\", \"Paul\")])\n    self.assertEqual(f.widget.choices, [(\"J\", \"John\"), (\"P\", \"Paul\")])\n    f.widget.choices = [(\"J\", \"Jane\")]\n    self.assertEqual(f.widget.choices, [(\"J\", \"Jane\")])\n    self.assertEqual(f.choices, [(\"J\", \"John\"), (\"P\", \"Paul\")])\n"], "sample_464": ["def test_file_response_with_async_iterator(self):\n    class AsyncFile:\n            self.content = content\n\n        async def read(self, n_bytes=-1):\n            return self.content\n\n        async def __aiter__(self):\n            yield self.content\n\n    file = AsyncFile(b\"binary content\")\n    response = FileResponse(file)\n    self.assertEqual(list(response.streaming_content), [b\"binary content\"])\n", "def test_set_signed_cookie(self):\n    response = HttpResponse()\n    response.set_signed_cookie(\"test\", \"value\")\n    self.assertIn('Set-Cookie', response.headers)\n    cookie = response.cookies['test']\n    self.assertEqual(cookie.value, response.cookies['test'].value)\n    self.assertTrue(cookie.value.startswith('value:'))\n", "def test_unicode_filename(self):\n    response = FileResponse(\n        ContentFile(b\"binary content\", name=\"filename with \u00fcnicode ch\u00e4r\u00e4cters.txt\"),\n        as_attachment=True,\n    )\n    self.assertEqual(\n        response.headers[\"Content-Disposition\"],\n        \"attachment; filename*=utf-8''filename%20with%20%C3%BCnicode%20ch%C3%A4r%C3%A4cters.txt\",\n    )\n", "def test_close_called_on_file(self):\n    class TestFile:\n            self.closed = False\n\n            return b\"binary content\"\n\n            self.closed = True\n\n    file = TestFile()\n    response = FileResponse(file)\n    response.close()\n    self.assertTrue(file.closed)\n", "def test_file_response_context_manager(self):\n    with tempfile.NamedTemporaryFile(mode=\"wb\") as tmp:\n        tmp.write(b\"binary content\")\n        tmp.flush()\n        with FileResponse(tmp) as response:\n            self.assertEqual(list(response), [b\"binary content\"])\n        self.assertTrue(response.closed)\n"], "sample_465": ["def test_to_field_allowed(self):\n    class BandAdmin(ModelAdmin):\n            return to_field == \"name\"\n\n    ma = BandAdmin(Band, self.site)\n    self.assertIs(ma.to_field_allowed(request, \"name\"), True)\n    self.assertIs(ma.to_field_allowed(request, \"bio\"), False)\n", "def test_get_empty_value_display(self):\n    ma = ModelAdmin(Band, AdminSite())\n    self.assertEqual(ma.get_empty_value_display(), \"-\")\n\n    class CustomModelAdmin(ModelAdmin):\n        empty_value_display = \"Not available\"\n\n    ma = CustomModelAdmin(Band, AdminSite())\n    self.assertEqual(ma.get_empty_value_display(), \"Not available\")\n", "def test_get_inline_formsets(self):\n    class ConcertInline(TabularInline):\n        model = Concert\n        fk_name = \"main_band\"\n\n    class BandAdmin(ModelAdmin):\n        inlines = [ConcertInline]\n\n    ma = BandAdmin(Band, self.site)\n    formsets, inline_instances = ma.get_inline_formsets(request)\n    self.assertEqual(len(formsets), 1)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(formsets[0], BaseInlineFormSet)\n    self.assertIsInstance(inline_instances[0], ConcertInline)\n", "def test_has_view_or_change_permission(self):\n    \"\"\"\n    has_view_or_change_permission returns True for users who can view or edit objects and\n    False for users who can't.\n    \"\"\"\n    ma = ModelAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockViewUser()\n    self.assertIs(ma.has_view_or_change_permission(request), True)\n    request.user = self.MockAddUser()\n    self.assertIs(ma.has_view_or_change_permission(request), False)\n    request.user = self.MockChangeUser()\n    self.assertIs(ma.has_view_or_change_permission(request), True)\n    request.user = self.MockDeleteUser()\n    self.assertIs(ma.has_view_or_change_permission(request), False)\n", "def test_get_changeform_initial_data(self):\n    class BandAdmin(ModelAdmin):\n            return {\"name\": \"The Beatles\"}\n\n    ma = BandAdmin(Band, self.site)\n    initial = ma.get_changeform_initial_data(request)\n    self.assertEqual(initial, {\"name\": \"The Beatles\"})\n"], "sample_466": ["def test_serialize_time_zone(self):\n    self.assertSerializedEqual(datetime.timezone.utc)\n    string, imports = MigrationWriter.serialize(datetime.timezone.utc)\n    self.assertEqual(string, \"datetime.timezone.utc\")\n    self.assertEqual(imports, {\"import datetime\"})\n\n    tz = zoneinfo.ZoneInfo(\"Europe/Paris\")\n    string, imports = MigrationWriter.serialize(tz)\n    self.assertEqual(string, \"zoneinfo.ZoneInfo('Europe/Paris')\")\n    self.assertEqual(imports, {\"import zoneinfo\"})\n\n    field = models.DateTimeField(default=datetime.datetime.now, tzinfo=tz)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.DateTimeField(default=datetime.datetime.now, tzinfo=zoneinfo.ZoneInfo('Europe/Paris'))\",\n    )\n", "def test_serialize_none_as_default(self):\n    self.assertSerializedEqual(models.CharField(null=True, blank=True, default=None))\n    string, imports = MigrationWriter.serialize(\n        models.CharField(null=True, blank=True, default=None)\n    )\n    self.assertEqual(\n        string,\n        \"models.CharField(blank=True, default=None, null=True)\",\n    )\n    self.assertEqual(imports, {\"from django.db import models\"})\n", "def test_serialize_timezones(self):\n    self.assertSerializedEqual(get_default_timezone())\n    self.assertSerializedResultEqual(\n        get_default_timezone(),\n        (\"pytz.timezone('UTC')\", {\"import pytz\"}),\n    )\n    self.assertSerializedEqual(get_fixed_timezone(-300))\n    self.assertSerializedResultEqual(\n        get_fixed_timezone(180),\n        (\"datetime.timezone(datetime.timedelta(seconds=64800))\", {\"import datetime\"}),\n    )\n    self.assertSerializedEqual(timezone(\"Europe/Paris\"))\n    self.assertSerializedResultEqual(\n        timezone(\"America/New_York\"),\n        (\"pytz.timezone('America/New_York')\", {\"import pytz\"}),\n    )\n", "def test_unregister_non_registered_serializer(self):\n    with self.assertRaisesMessage(\n        ValueError, \"Serializer for 'complex' does not exist.\"\n    ):\n        MigrationWriter.unregister_serializer(complex)\n", "def test_serialize_datetime_timezone_info(self):\n    dt = datetime.datetime(2012, 1, 1, 1, 1, tzinfo=datetime.timezone.utc)\n    self.assertSerializedResultEqual(\n        dt,\n        (\n            \"datetime.datetime(2012, 1, 1, 1, 1, tzinfo=datetime.timezone.utc)\",\n            {\"import datetime\"},\n        ),\n    )\n    dt = datetime.datetime(2012, 1, 1, 1, 1, tzinfo=get_default_timezone())\n    string, imports = MigrationWriter.serialize(dt)\n    self.assertEqual(string, \"datetime.datetime(2012, 1, 1, 1, 1)\")\n    self.assertEqual(imports, {\"import datetime\"})\n"], "sample_467": ["def test_use_required_attribute(self):\n    class TestForm(Form):\n        required_field = DateField(widget=SelectDateWidget, required=True)\n        non_required_field = DateField(widget=SelectDateWidget, required=False)\n\n    form = TestForm()\n    self.assertTrue(form.fields[\"required_field\"].widget.use_required_attribute(None))\n    self.assertFalse(\n        form.fields[\"non_required_field\"].widget.use_required_attribute(None)\n    )\n    # 'required' is always okay for <select multiple>.\n    self.assertTrue(\n        SelectDateWidget(required=True).use_required_attribute(initial=None)\n    )\n    self.assertTrue(\n        SelectDateWidget(required=True).use_required_attribute(initial=\"2010-04-15\")\n    )\n", "def test_value_from_datadict_with_invalid_date(self):\n    data = {\"date_year\": \"2010\", \"date_month\": \"13\", \"date_day\": \"32\"}\n    self.assertEqual(self.widget.value_from_datadict(data, {}, \"date\"), \"0-0-0\")\n", "def test_selectdate_widget_required_attribute(self):\n    # The 'required' attribute should only be rendered if the widget is required.\n    class TestForm(Form):\n        required_field = DateField(widget=self.widget)\n        not_required_field = DateField(widget=self.widget, required=False)\n\n    form = TestForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        \"\"\"\n        <p><select name=\"required_field_month\" id=\"id_required_field_month\" required>\n            <option value=\"1\">January</option>\n            ...\n        </select>\n        <select name=\"required_field_day\" id=\"id_required_field_day\" required>\n            <option value=\"1\">1</option>\n            ...\n        </select>\n        <select name=\"required_field_year\" id=\"id_required_field_year\" required>\n            <option value=\"2007\">2007</option>\n            ...\n        </select></p>\n        <p><select name=\"not_required_field_month\" id=\"id_not_required_field_month\">\n            <option value=\"1\">January</option>\n            ...\n        </select>\n        <select name=\"not_required_field_day\" id=\"id_not_required_field_day\">\n            <option value=\"1\">1</option>\n            ...\n        </select>\n        <select name=\"not_required_field_year\" id=\"id_not_required_field_year\">\n            <option value=\"2007\">2007</option>\n            ...\n        </select></p>\n        \"\"\",\n    )\n", "def test_format_value_edge_cases(self):\n    \"\"\"\n    Test edge cases for the format_value method.\n    \"\"\"\n    widget = SelectDateWidget()\n    self.assertEqual(widget.format_value(None), {\"year\": None, \"month\": None, \"day\": None})\n    self.assertEqual(widget.format_value(\"\"), {\"year\": None, \"month\": None, \"day\": None})\n    self.assertEqual(widget.format_value(\"invalid\"), {\"year\": None, \"month\": None, \"day\": None})\n    self.assertEqual(widget.format_value(\"2022-02-30\"), {\"year\": 2022, \"month\": 2, \"day\": 30})\n    self.assertEqual(widget.format_value(\"2022-13-01\"), {\"year\": 2022, \"month\": 13, \"day\": 1})\n    self.assertEqual(widget.format_value(\"2022-01-32\"), {\"year\": 2022, \"month\": 1, \"day\": 32})\n", "def test_value_from_datadict_edge_cases(self):\n    tests = [\n        ((\"2000\", \"12\", \"\"), None),\n        ((\"2000\", \"\", \"1\"), None),\n        ((\"\", \"12\", \"1\"), None),\n        ((None, \"12\", \"1\"), None),\n        ((\"2000\", None, \"1\"), None),\n        ((\"2000\", \"12\", None), None),\n        ((str(sys.maxsize + 1), str(sys.maxsize + 1), str(sys.maxsize + 1)), \"0-0-0\"),\n    ]\n    for values, expected in tests:\n        with self.subTest(values=values):\n            data = {}\n            for field_name, value in zip((\"year\", \"month\", \"day\"), values):\n                if value is not None:\n                    data[\"field_%s\" % field_name] = value\n            self.assertEqual(\n                self.widget.value_from_datadict(data, {}, \"field\"), expected\n            )\n"], "sample_469": ["def test_alias_forbidden_chars(self):\n    tests = [\n        'al\"ias',\n        \"a'lias\",\n        \"ali`as\",\n        \"alia s\",\n        \"alias\\t\",\n        \"ali\\nas\",\n        \"alias--\",\n        \"ali/*as\",\n        \"alias*/\",\n        \"alias;\",\n        # [] are used by MSSQL.\n        \"alias[\",\n        \"alias]\",\n    ]\n    msg = (\n        \"Column aliases cannot contain whitespace characters, quotation marks, \"\n        \"semicolons, or SQL comments.\"\n    )\n    for crafted_alias in tests:\n        with self.subTest(crafted_alias):\n            with self.assertRaisesMessage(ValueError, msg):\n                Book.objects.alias(**{crafted_alias: Value(1)})\n", "def test_resolve_expression(self):\n    class CustomExpression(BaseExpression):\n            self.query = query\n            self.allow_joins = allow_joins\n            self.reuse = reuse\n            self.summarize = summarize\n            self.for_save = for_save\n            return self\n\n    qs = Book.objects.filter(isbn=\"159059725\")\n    expression = CustomExpression()\n    qs.resolve_expression(expression)\n    self.assertEqual(expression.query, qs.query)\n    self.assertIs(expression.allow_joins, True)\n    self.assertIs(expression.reuse, None)\n    self.assertIs(expression.summarize, False)\n    self.assertIs(expression.for_save, False)\n", "def test_alias_forbidden_chars(self):\n    tests = [\n        'al\"ias',\n        \"a'lias\",\n        \"ali`as\",\n        \"alia s\",\n        \"alias\\t\",\n        \"ali\\nas\",\n        \"alias--\",\n        \"ali/*as\",\n        \"alias*/\",\n        \"alias;\",\n        # [] are used by MSSQL.\n        \"alias[\",\n        \"alias]\",\n    ]\n    msg = (\n        \"Column aliases cannot contain whitespace characters, quotation marks, \"\n        \"semicolons, or SQL comments.\"\n    )\n    for crafted_alias in tests:\n        with self.subTest(crafted_alias):\n            with self.assertRaisesMessage(ValueError, msg):\n                Book.objects.alias(**{crafted_alias: Value(1)})\n", "def test_alias_with_m2m(self):\n    qs = Author.objects.alias(\n        friend_names=F(\"friends__name\"),\n    ).annotate(friend_name=F(\"friend_names\"))\n    self.assertIs(hasattr(qs.first(), \"friend_names\"), False)\n    for author in qs:\n        with self.subTest(author=author):\n            if author.friends.exists():\n                self.assertEqual(author.friend_name, author.friends.first().name)\n", "def test_alias_in_f_grouped_by_annotation(self):\n    qs = (\n        Publisher.objects.alias(multiplier=Value(3))\n        .values(\"name\")\n        .annotate(multiplied_value_sum=Sum(F(\"multiplier\") * F(\"num_awards\")))\n        .order_by()\n    )\n    self.assertCountEqual(\n        qs,\n        [\n            {\"multiplied_value_sum\": 9, \"name\": \"Apress\"},\n            {\"multiplied_value_sum\": 0, \"name\": \"Jonno's House of Books\"},\n            {\"multiplied_value_sum\": 27, \"name\": \"Morgan Kaufmann\"},\n            {\"multiplied_value_sum\": 21, \"name\": \"Prentice Hall\"},\n            {\"multiplied_value_sum\": 3, \"name\": \"Sams\"},\n        ],\n    )\n"], "sample_468": ["def test_make_context(self):\n    request = self.request_factory.get(\"/\")\n    context = make_context({\"a\": 1}, request)\n    self.assertIsInstance(context, RequestContext)\n    self.assertEqual(context[\"a\"], 1)\n\n    context = make_context({\"a\": 1})\n    self.assertIsInstance(context, Context)\n    self.assertEqual(context[\"a\"], 1)\n\n    with self.assertRaises(TypeError):\n        make_context(\"not a dict\")\n\n    context = make_context(None, request)\n    self.assertIsInstance(context, RequestContext)\n\n    context = make_context(None)\n    self.assertIsInstance(context, Context)\n", "def test_make_context(self):\n    request = self.request_factory.get(\"/\")\n    context = make_context({\"a\": 1}, request)\n    self.assertIsInstance(context, RequestContext)\n    self.assertEqual(context[\"a\"], 1)\n\n    context = make_context({\"a\": 1})\n    self.assertIsInstance(context, Context)\n    self.assertEqual(context[\"a\"], 1)\n\n    with self.assertRaises(TypeError):\n        make_context(\"not a dict\")\n", "def test_render_context_push_state(self):\n    test_context = RenderContext({\"fruit\": \"papaya\"})\n\n    with test_context.push_state(template=\"test_template\", isolated_context=True):\n        self.assertEqual(test_context.template, \"test_template\")\n        test_context[\"vegetable\"] = \"artichoke\"\n        self.assertEqual(list(test_context), [\"vegetable\"])\n\n    self.assertIsNone(test_context.template)\n    self.assertNotIn(\"vegetable\", test_context)\n    self.assertEqual(list(test_context), [\"fruit\"])\n", "def test_make_context(self):\n    request = self.request_factory.get(\"/\")\n    context = make_context({\"a\": 1}, request)\n    self.assertIsInstance(context, RequestContext)\n    self.assertEqual(context[\"a\"], 1)\n\n    context = make_context({\"a\": 1})\n    self.assertIsInstance(context, Context)\n    self.assertEqual(context[\"a\"], 1)\n\n    with self.assertRaises(TypeError):\n        make_context(\"not a dict\")\n", "def test_make_context(self):\n    context = make_context({\"a\": 1})\n    self.assertIsInstance(context, Context)\n    self.assertEqual(context[\"a\"], 1)\n\n    request = self.request_factory.get(\"/\")\n    context = make_context({\"a\": 1}, request)\n    self.assertIsInstance(context, RequestContext)\n    self.assertEqual(context[\"a\"], 1)\n    self.assertEqual(context.request, request)\n\n    context = make_context()\n    self.assertIsInstance(context, Context)\n    self.assertEqual(context.dicts, [{\"False\": False, \"None\": None, \"True\": True}])\n"], "sample_470": ["def test_simple_lazy_object_repr(self):\n    class Klazz:\n            self.value = value\n\n            return f\"Klazz({self.value})\"\n\n    lazy_obj = SimpleLazyObject(lambda: Klazz(42))\n    self.assertEqual(repr(lazy_obj), \"<SimpleLazyObject: <function test_simple_lazy_object_repr.<locals>.<lambda> at\")\n    lazy_obj._setup()\n    self.assertEqual(repr(lazy_obj), \"<SimpleLazyObject: Klazz(42)>\")\n", "def test_lazy_object_hash(self):\n    original_object = \"Lazy translation text\"\n    lazy_obj = lazy(lambda: original_object, str)\n    self.assertEqual(hash(original_object), hash(lazy_obj()))\n", "def test_partition(self):\n    values = [1, 2, 3, 4, 5]\n    predicate = lambda x: x > 3\n    left, right = partition(predicate, values)\n    self.assertEqual(left, [1, 2, 3])\n    self.assertEqual(right, [4, 5])\n", "def test_lazyobject_unpickle(self):\n    obj = SimpleLazyObject(lambda: \"Hello\")\n    unpickled_obj = unpickle_lazyobject(obj._wrapped)\n    self.assertEqual(unpickled_obj, \"Hello\")\n", "def test_lazyobject_unpickle(self):\n    class Klazz:\n            self.value = value\n\n    obj = SimpleLazyObject(lambda: Klazz(42))\n    unpickled_obj = unpickle_lazyobject(obj._wrapped)\n    self.assertEqual(unpickled_obj.value, 42)\n"], "sample_471": ["def test_integerfield_strip(self):\n    f = IntegerField()\n    self.assertEqual(1, f.clean(\" 1 \"))\n    self.assertEqual(1, f.clean(\"\\t1\\r\\n\"))\n    self.assertEqual(1, f.clean(\" \\t1\\r\\n \"))\n\n    f = IntegerField(strip=False)\n    self.assertEqual(1, f.clean(\" 1 \"))\n    self.assertEqual(1, f.clean(\"\\t1\\r\\n\"))\n    self.assertEqual(1, f.clean(\" \\t1\\r\\n \"))\n", "def test_integerfield_max_length(self):\n    f = IntegerField(max_length=5)\n    self.assertWidgetRendersTo(\n        f,\n        '<input type=\"number\" name=\"f\" id=\"id_f\" required maxlength=\"5\">',\n    )\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(\"\")\n    self.assertEqual(1, f.clean(\"1\"))\n    with self.assertRaisesMessage(\n        ValidationError,\n        \"Ensure this filename has at most 5 characters (it has 6).\",\n    ):\n        f.clean(\"123456\")\n", "def test_integerfield_subclass_widget_instance(self):\n    \"\"\"\n    Class-defined widget instance is not overwritten by __init__() (#22245).\n    \"\"\"\n\n    class MyIntegerField(IntegerField):\n        widget = Textarea(attrs={\"rows\": 5})\n\n    f = MyIntegerField()\n    self.assertEqual(f.widget.__class__, Textarea)\n    self.assertEqual(f.widget.attrs, {\"rows\": 5})\n    f = MyIntegerField(localize=True)\n    self.assertEqual(f.widget.__class__, Textarea)\n    self.assertEqual(f.widget.attrs, {\"rows\": 5})\n", "def test_integerfield_widget_attrs(self):\n    f = IntegerField(min_value=10, max_value=20)\n    widget_attrs = f.widget_attrs(f.widget)\n    self.assertEqual(widget_attrs[\"min\"], 10)\n    self.assertEqual(widget_attrs[\"max\"], 20)\n\n    f = IntegerField(step_size=3)\n    widget_attrs = f.widget_attrs(f.widget)\n    self.assertEqual(widget_attrs[\"step\"], 3)\n", "def test_integerfield_disabled(self):\n    f = IntegerField(disabled=True)\n    self.assertWidgetRendersTo(\n        f, '<input type=\"number\" name=\"f\" id=\"id_f\" disabled>'\n    )\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(\"\")\n    self.assertIsNone(f.clean(None))\n    self.assertEqual(1, f.clean(\"1\"))\n    self.assertIsInstance(f.clean(\"1\"), int)\n    self.assertEqual(23, f.clean(\"23\"))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n        f.clean(\"a\")\n    self.assertEqual(42, f.clean(42))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n        f.clean(3.14)\n    self.assertEqual(1, f.clean(\"1 \"))\n    self.assertEqual(1, f.clean(\" 1\"))\n    self.assertEqual(1, f.clean(\" 1 \"))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n        f.clean(\"1a\")\n"], "sample_472": ["def test_paginating_non_integer_per_page(self):\n    paginator = Paginator([1, 2, 3], \"a\")\n    with self.assertRaisesMessage(TypeError, \"'str' object cannot be interpreted as an integer\"):\n        paginator.per_page\n\n    paginator = Paginator([1, 2, 3], 2.5)\n    self.assertEqual(paginator.per_page, 2)\n\n    paginator = Paginator([1, 2, 3], float(\"inf\"))\n    with self.assertRaisesMessage(OverflowError, \"cannot convert float infinity to integer\"):\n        paginator.per_page\n", "def test_page_repr(self):\n    paginator = Paginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertEqual(repr(page), \"<Page 1 of 2>\")\n", "def test_page_representation(self):\n    paginator = Paginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertEqual(str(page), \"<Page 1 of 2>\")\n    repr_output = repr(page)\n    self.assertIn(\"<Page 1 of 2>\", repr_output)\n    self.assertIn(\"object_list\", repr_output)\n    self.assertIn(\"number\", repr_output)\n    self.assertIn(\"paginator\", repr_output)\n", "def test_paginating_object_list_with_ordered_attribute_does_not_warn(self):\n    \"\"\"\n    No unordered object list warning with an object that has an ordered\n    attribute set to True.\n    \"\"\"\n\n    class ObjectList:\n        ordered = True\n\n    object_list = ObjectList()\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator(object_list, 5)\n    self.assertEqual(len(recorded), 0)\n", "def test_get_elided_page_range_with_invalid_number(self):\n    paginator = Paginator(range(5000), 100)\n    msg = \"That page number is not an integer\"\n    with self.assertRaisesMessage(PageNotAnInteger, msg):\n        list(paginator.get_elided_page_range(\"a\"))\n\n    msg = \"That page number is less than 1\"\n    with self.assertRaisesMessage(EmptyPage, msg):\n        list(paginator.get_elided_page_range(0))\n\n    msg = \"That page contains no results\"\n    with self.assertRaisesMessage(EmptyPage, msg):\n        list(paginator.get_elided_page_range(51))\n"], "sample_473": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_474": ["    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena\", alias=\"elena\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n", "    def setUpTestData(cls):\n        cls.author = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n"], "sample_475": ["    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            id=\"admin.E034\",\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"hello\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_476": ["    def test_closed_attribute(self):\n        p = Person(name=\"Joe\")\n        p.mugshot.save(\"mug\", self.file1)\n        self.assertIs(p.mugshot.closed, True)\n\n        file = p.mugshot.open()\n        self.assertIs(file.closed, False)\n        file.close()\n        self.assertIs(file.closed, True)\n", "    def test_close_called(self):\n        \"\"\"\n        Test that the underlying file of an ImageFieldFile is closed when it goes\n        out of scope.\n        \"\"\"\n        instance = Person()\n        instance.mugshot.save(\"mug\", self.file1)\n\n        file = instance.mugshot.file\n        file.close = MagicMock()\n\n        with instance.mugshot as f:\n            pass\n\n        file.close.assert_called_once()\n", "    def test_image_field_form(self):\n        \"\"\"\n        An ImageField in a form behaves like a FileField.\n        \"\"\"\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = Person\n                fields = (\"mugshot\",)\n\n        form = TestForm(files={\"mugshot\": self.file1})\n        self.assertTrue(form.is_valid())\n        instance = form.save()\n        self.assertEqual(instance.mugshot, self.file1)\n", "    def test_deletion(self):\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1)\n        p.save()\n        path = p.mugshot.path\n        self.assertTrue(os.path.exists(path))\n        p.delete()\n        self.assertFalse(os.path.exists(path))\n", "    def test_close_called(self):\n        \"\"\"\n        The underlying file should be closed when the FieldFile is garbage\n        collected or its close method is called.\n        \"\"\"\n        person = Person(name=\"Joe\")\n        person.mugshot.save(\"mug\", self.file1)\n        mugshot = person.mugshot\n\n        # Get a new File object from the field. We can't just use\n        # person.mugshot.file because that would just give us whatever was\n        # already cached.\n        mugshot_file = mugshot.storage.open(mugshot.name, \"rb\")\n\n        # Open the file (if it wasn't already) and check it's not closed.\n        file_obj = mugshot.file\n        self.assertIs(file_obj.closed, False)\n\n        # Delete the FieldFile and check the file is closed.\n        del mugshot\n        self.assertIs(file_obj.closed, True)\n\n        # Check closing the file manually also works.\n        mugshot_file = person.mugshot.storage.open(person.mugshot.name, \"rb\")\n        file_obj = person.mugshot.file\n        self.assertIs(file_obj.closed, False)\n        person.mugshot.close()\n        self.assertIs(file_obj.closed, True)\n"], "sample_477": ["def test_add01(self):\n    output = self.engine.render_to_string(\"add01\", {\"num\": 5})\n    self.assertEqual(output, \"10\")\n", "def test_dictsort(self):\n    output = self.engine.render_to_string(\n        \"dictsort\", {\"value\": [{\"name\": \"John\", \"age\": 25}, {\"name\": \"Alice\", \"age\": 30}]}\n    )\n    self.assertEqual(output, \"[{'name': 'John', 'age': 25}, {'name': 'Alice', 'age': 30}]\")\n", "def test_pluralize01(self):\n    output = self.engine.render_to_string(\"pluralize01\", {\"num\": 1})\n    self.assertEqual(output, \"\")\n", "def test_dictsort01(self):\n    output = self.engine.render_to_string(\n        \"dictsort01\", {\"dict\": [{\"key\": 2}, {\"key\": 1}, {\"key\": 3}]}\n    )\n    self.assertEqual(output, \"{'key': 1}{'key': 2}{'key': 3}\")\n", "def test_floatformat01(self):\n    output = self.engine.render_to_string(\"floatformat01\", {\"num\": 34.23234})\n    self.assertEqual(output, \"34.2\")\n"], "sample_478": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_view_on_site_is_not_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"not callable\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_invalid_type(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_479": ["def test_create_model_add_remove_constraint(self):\n    \"\"\"\n    AddConstraint and RemoveConstraint should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", fields=[]),\n            migrations.AddConstraint(\n                \"Foo\",\n                models.CheckConstraint(check=models.Q(age__gte=18), name=\"age_gte_18\"),\n            ),\n            migrations.RemoveConstraint(\"Foo\", \"age_gte_18\"),\n        ],\n        [migrations.CreateModel(\"Foo\", fields=[])],\n    )\n\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", fields=[]),\n            migrations.AddConstraint(\n                \"Foo\",\n                models.CheckConstraint(check=models.Q(age__gte=18), name=\"age_gte_18\"),\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[],\n                options={\n                    \"constraints\": [\n                        models.CheckConstraint(check=models.Q(age__gte=18), name=\"age_gte_18\")\n                    ]\n                },\n            ),\n        ],\n    )\n", "def test_create_alter_model_table(self):\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AlterModelTable(\"Foo\", \"myapp_foo\"),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))], options={\"db_table\": \"myapp_foo\"}\n            ),\n        ],\n    )\n", "def test_rename_index_same_name(self):\n    self.assertOptimizesTo(\n        [\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_name=\"old_name\"\n            ),\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_name=\"new_name\"\n            ),\n        ],\n        [migrations.RenameIndex(\"Pony\", new_name=\"new_name\", old_name=\"old_name\")],\n    )\n", "def test_rename_index_through_model_rename(self):\n    self.assertOptimizesTo(\n        [\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_fields=(\"weight\", \"pink\")\n            ),\n            migrations.RenameModel(\"Pony\", \"Horse\"),\n        ],\n        [\n            migrations.RenameModel(\"Pony\", \"Horse\"),\n            migrations.RenameIndex(\n                \"Horse\", new_name=\"new_name\", old_fields=(\"weight\", \"pink\")\n            ),\n        ],\n    )\n    self.assertOptimizesTo(\n        [\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_name=\"old_name\"\n            ),\n            migrations.RenameModel(\"Pony\", \"Horse\"),\n        ],\n        [\n            migrations.RenameModel(\"Pony\", \"Horse\"),\n            migrations.RenameIndex(\n                \"Horse\", new_name=\"new_name\", old_name=\"old_name\"\n            ),\n        ],\n    )\n", "def test_add_remove_constraint(self):\n    self.assertOptimizesTo(\n        [\n            migrations.AddConstraint(\n                \"Pony\",\n                models.CheckConstraint(check=models.Q(weight__gt=0), name=\"chk_pony_weight\"),\n            ),\n            migrations.RemoveConstraint(\"Pony\", \"chk_pony_weight\"),\n        ],\n        [],\n    )\n"], "sample_480": ["def test_key_transform_in_expression(self):\n    obj = NullableJSONModel.objects.create(value={\"a\": 1, \"b\": 2})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.annotate(\n            key=KeyTransform(\"a\", \"value\"),\n        ).filter(key__in=[1, 3]),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.annotate(\n            key=KeyTransform(\"b\", \"value\"),\n        ).filter(key__in=[1, 3]),\n        [],\n    )\n", "    def setUpTestData(cls):\n        cls.objs = [\n            NullableJSONModel.objects.create(value={\"a\": \"b\"}),\n            NullableJSONModel.objects.create(value={\"a\": \"c\"}),\n            NullableJSONModel.objects.create(value={\"b\": \"d\"}),\n        ]\n", "    def test_subquery_json_field(self):\n        obj1 = JSONModel.objects.create(value={\"a\": 1, \"b\": 2})\n        obj2 = JSONModel.objects.create(value={\"a\": 2, \"b\": 3})\n        subquery = JSONModel.objects.filter(value__a=1).values(\"value\")\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__in=subquery),\n            [obj1],\n        )\n", "    def test_lookup_type_error(self):\n        tests = [\n            (\"value__contains\", \"a\"),\n            (\"value__contained_by\", \"b\"),\n            (\"value__has_key\", 1),\n            (\"value__has_keys\", 2),\n            (\"value__has_any_keys\", 3),\n        ]\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup, value=value):\n                msg = f\"Lookup type error: {lookup} must be {type(value).__name__}\"\n                with self.assertRaisesMessage(TypeError, msg):\n                    NullableJSONModel.objects.filter(**{lookup: value})\n", "def test_key_transform_with_custom_decoder(self):\n    obj = NullableJSONModel.objects.create(value_custom={\"a\": \"b\"})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value_custom__a=\"b\"),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value_custom__has_key=\"a\"),\n        [obj],\n    )\n"], "sample_481": ["    def test_pprint01(self):\n        output = self.engine.render_to_string(\"pprint01\", {\"a\": [1, 2, 3]})\n        self.assertEqual(output, \"[1, 2, 3]\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5})\n        self.assertEqual(output, \"10\")\n", "    def test_join_empty_list(self):\n        output = self.engine.render_to_string(\"join_empty_list\", {\"a\": []})\n        self.assertEqual(output, \"\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"i\": 5})\n        self.assertEqual(output, \"6\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 10})\n        self.assertEqual(output, \"15\")\n"], "sample_482": ["    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n"], "sample_483": ["def test_list_filter_works_on_related_fields(self):\n    \"\"\"\n    Ensure list_filter can access related fields.\n    \"\"\"\n\n    class BookAdminWithListFilter(admin.ModelAdmin):\n        list_filter = [\"authors__name\"]\n\n    errors = BookAdminWithListFilter(Book, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_list_filter_item_is_not_a_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [\"nonexistent\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent', which does not \"\n            \"refer to a Field.\",\n            obj=SongAdmin,\n            id=\"admin.E116\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_list_filter_item(self):\n    from django.contrib.admin import ListFilter\n\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [\"title\", \"album__name\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class InvalidListFilter(ListFilter):\n        pass\n\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [(\"title\", InvalidListFilter)]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0][1]' must inherit from 'FieldListFilter'.\",\n            obj=SongAdmin,\n            id=\"admin.E115\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_works_on_non_field(self):\n    \"\"\"\n    Ensure list_filter can access non-fields (e.g. a method on the ModelAdmin).\n    \"\"\"\n\n    class BookAdminWithListFilter(admin.ModelAdmin):\n            return queryset.filter(name=\"Test\")\n\n        list_filter = [book_test]\n\n    errors = BookAdminWithListFilter(Book, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_list_filter_item_tuple(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        list_filter = [(\"album__title\", admin.AllValuesFieldListFilter)]\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_484": ["    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n"], "sample_485": ["def test_html_safe_subclass_override_html(self):\n    class BaseClass:\n            return \"some html content\"\n\n            return \"some non html content\"\n\n    @html_safe\n    class Subclass(BaseClass):\n            return \"overridden html content\"\n\n            return \"some html safe content\"\n\n    subclass_obj = Subclass()\n    self.assertEqual(str(subclass_obj), \"overridden html content\")\n    self.assertEqual(subclass_obj.__html__(), \"overridden html content\")\n", "def test_urlize_nofollow(self):\n    tests = (\n        (\n            \"Search for google.com/?q=! and see.\",\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and '\n            \"see.\",\n        ),\n        (\n            lazystr(\"Search for google.com/?q=!\"),\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!',\n        ),\n        (\n            \"foo@example.com\",\n            '<a href=\"mailto:foo@example.com\" rel=\"nofollow\">foo@example.com</a>',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_nofollow(self):\n    value = \"Search for google.com and see.\"\n    expected_output = 'Search for <a href=\"http://google.com\" rel=\"nofollow\">google.com</a> and see.'\n    self.assertEqual(urlize(value, nofollow=True), expected_output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\"Search for google.com/?q=! and see.\", 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'),\n        (\"Search for google.com/?q=1&lt! and see.\", 'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>! and see.'),\n        (\"Search for google.com/?q=! and see...\", 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see...'),\n        (\"Search for google.com/?q=1&lt! and see...\", 'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>! and see...'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_avoid_wrapping(self):\n    tests = (\n        (\"foo bar\", \"foo\\xa0bar\"),\n        (\"foo  bar\", \"foo\\xa0\\xa0bar\"),\n        (\"foo\\nbar\", \"foo\\nbar\"),  # Don't replace newlines.\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(avoid_wrapping(value), output)\n"], "sample_486": ["def test_inlineformset_factory_uses_model_field_for_parent_instance(self):\n    \"\"\"\n    #27592 - Inline formsets use the parent instance's model field for the\n    foreign key value, not its pk attribute.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        ParentWithUUIDAlternateKey, ChildRelatedViaAK, fields=\"__all__\"\n    )\n    parent = ParentWithUUIDAlternateKey.objects.create()\n    formset = FormSet(instance=parent)\n    self.assertEqual(formset.forms[0].fields[\"parent\"].initial, parent.uuid)\n", "def test_inlineformset_factory_nulls_default_pks_on_save(self):\n    \"\"\"\n    #24377 - If we're adding a new object, a parent's auto-generated pk from\n    the model field default should be ignored as it's regenerated on the save\n    request.\n    \"\"\"\n    FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n    formset = FormSet()\n    self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n\n    instance = formset.instance\n    instance.save()\n\n    self.assertIsNotNone(instance.uuid)\n    self.assertNotEqual(formset.forms[0].fields[\"parent\"].initial, instance.uuid)\n", "def test_inlineformset_factory_uses_provided_queryset(self):\n    \"\"\"\n    #26515 - Make sure that the inline formset factory uses the provided queryset.\n    \"\"\"\n    parent = UUIDPKParent.objects.create()\n    child1 = UUIDPKChild.objects.create(parent=parent, name=\"child1\")\n    child2 = UUIDPKChild.objects.create(parent=parent, name=\"child2\")\n\n    FormSet = inlineformset_factory(\n        UUIDPKParent, UUIDPKChild, fields=\"__all__\", queryset=UUIDPKChild.objects.filter(name=\"child1\")\n    )\n    formset = FormSet(instance=parent)\n\n    self.assertEqual(len(formset.forms), 1)\n    self.assertEqual(formset.forms[0].instance, child1)\n", "def test_inlineformset_factory_nulls_default_pks_parent_instance_with_pk(self):\n    \"\"\"\n    If a parent instance with a primary key is passed to the inline formset,\n    the primary key should be set on the child instances.\n    \"\"\"\n    parent = UUIDPKParent.objects.create(uuid=\"123e4567-e89b-12d3-a456-426655440000\")\n    FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n    formset = FormSet(instance=parent)\n    self.assertEqual(formset.forms[0].fields[\"parent\"].initial, parent.uuid)\n", "def test_inlineformset_factory_validates_primary_key(self):\n    \"\"\"\n    #27519 - If a form in an inline formset has its primary key set to an\n    existing instance, the formset should raise a ValidationError.\n    \"\"\"\n    parent = UUIDPKParent.objects.create()\n    child = UUIDPKChild.objects.create(parent=parent)\n\n    FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n    formset = FormSet(\n        {\n            \"uuidpkchild_set-TOTAL_FORMS\": 1,\n            \"uuidpkchild_set-INITIAL_FORMS\": 0,\n            \"uuidpkchild_set-MAX_NUM_FORMS\": \"\",\n            \"uuidpkchild_set-0-id\": str(child.pk),\n            \"uuidpkchild_set-0-name\": \"Foo\",\n        },\n        instance=parent,\n    )\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.errors[0][\"id\"][0], \"Select a valid choice. That choice is not one of the available choices.\"\n    )\n"], "sample_487": ["    def test_readonly_fields(self):\n        class Admin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            Admin,\n            ValidationTestModel,\n            msg=\"The value of 'readonly_fields' must be a list or tuple.\",\n            id=\"admin.E034\",\n            invalid_obj=Admin,\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_readonly_fields_is_none(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = None\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n"], "sample_488": ["def test_get_containing_app_config(self):\n    \"\"\"\n    apps.get_containing_app_config() returns the app config for the inner \n    application in case of nesting.\n    \"\"\"\n    self.assertIsNone(apps.get_containing_app_config(\"nonexistent.module\"))\n\n    # Create a mock module within an app.\n    app_name = \"apps\"\n    app_config = apps.get_app_config(app_name)\n    module_name = f\"{app_config.name}.tests.module\"\n    with self.settings(INSTALLED_APPS=[app_name]):\n        self.assertEqual(\n            apps.get_containing_app_config(module_name), app_config\n        )\n", "def test_set_available_apps(self):\n    \"\"\"\n    Tests apps.set_available_apps().\n    \"\"\"\n    # Use the original apps registry, not a new Apps instance.\n    original_app_configs = apps.app_configs.copy()\n\n    # Restrict available apps.\n    available = [\"apps\", \"django.contrib.contenttypes\"]\n    apps.set_available_apps(available)\n\n    try:\n        self.assertEqual(\n            {app_config.name for app_config in apps.get_app_configs()}, set(available)\n        )\n\n        # Test get_model() with an app that is no longer available.\n        with self.assertRaises(LookupError):\n            apps.get_model(\"admin\", \"LogEntry\")\n\n        # Test that set_available_apps() can be called multiple times.\n        available = [\"apps\"]\n        apps.set_available_apps(available)\n        self.assertEqual(\n            {app_config.name for app_config in apps.get_app_configs()}, set(available)\n        )\n    finally:\n        # Restore original app_configs to avoid breaking other tests.\n        apps.app_configs = original_app_configs\n        apps.clear_cache()\n", "def test_lazy_model_operation_with_function(self):\n    \"\"\"\n    Tests apps.lazy_model_operation() with a function that takes no arguments.\n    \"\"\"\n    initial_pending = set(apps._pending_operations)\n\n        pass\n\n    apps.lazy_model_operation(test_func)\n\n    self.assertEqual(set(apps._pending_operations), initial_pending)\n    self.assertIsNone(apps._pending_operations.get((), None))\n", "def test_set_available_apps(self):\n    \"\"\"\n    Tests apps.set_available_apps().\n    \"\"\"\n    available = [\"apps.apps.MyAdmin\", \"apps.apps.MyAuth\"]\n    apps.set_available_apps(available)\n    self.assertEqual(\n        [app_config.name for app_config in apps.get_app_configs()],\n        available,\n    )\n    apps.unset_available_apps()\n    self.assertEqual(\n        [app_config.name for app_config in apps.get_app_configs()],\n        SOME_INSTALLED_APPS_NAMES,\n    )\n\n    # Apps argument must be a subset of installed apps.\n    with self.assertRaises(ValueError):\n        apps.set_available_apps([\"nonexistent.app\"])\n", "    def test_get_models(self):\n        # Test that get_models() returns all models, and that they're ready.\n        models = apps.get_models()\n        self.assertGreater(len(models), 0)\n        for model in models:\n            self.assertIs(model._meta.apps, apps)\n"], "sample_489": ["def test_bulk_create_with_database_router(self):\n    router = \"tests.routers.TestRouter\"\n    with override_settings(DATABASE_ROUTERS=[router]):\n        Country.objects.using(\"other\").bulk_create(self.data)\n        self.assertEqual(Country.objects.using(\"other\").count(), 4)\n", "def test_bulk_create_with_fields_subset(self):\n    data = [\n        TwoFields(f1=1, f2=1),\n        TwoFields(f1=2, f2=2),\n    ]\n    TwoFields.objects.bulk_create(data, fields=(\"f1\",))\n    self.assertEqual(TwoFields.objects.count(), 2)\n    for instance in TwoFields.objects.all():\n        self.assertIsNone(instance.f2)\n", "def test_bulk_create_large_batch_mixed_with_pk_set(self):\n    batch_size = 1000\n    objects = [\n        TwoFields(id=i if i % 2 == 0 else None, f1=i, f2=i + 1)\n        for i in range(100000, 101000)\n    ]\n    with self.assertNumQueries(ceil(len(objects) / batch_size)):\n        TwoFields.objects.bulk_create(objects, batch_size=batch_size)\n    self.assertEqual(TwoFields.objects.count(), len(objects))\n", "def test_update_conflicts_unique_fields_update_fields_foreign_key(self):\n    RelatedModel.objects.bulk_create(\n        [\n            RelatedModel(country=Country.objects.create(name=\"France\")),\n            RelatedModel(country=Country.objects.create(name=\"Spain\")),\n        ]\n    )\n    self.assertEqual(RelatedModel.objects.count(), 2)\n\n    conflicting_objects = [\n        RelatedModel(id=1, country=Country.objects.create(name=\"Germany\")),\n        RelatedModel(id=2, country=Country.objects.create(name=\"Italy\")),\n    ]\n    results = RelatedModel.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"id\"],\n        update_fields=[\"country\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(RelatedModel.objects.count(), 2)\n    self.assertCountEqual(\n        RelatedModel.objects.values(\"id\", \"country__name\"),\n        [\n            {\"id\": 1, \"country__name\": \"Germany\"},\n            {\"id\": 2, \"country__name\": \"Italy\"},\n        ],\n    )\n", "def test_update_conflicts_unique_fields_update_fields_non_unique(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(rank=1, name=\"a\"),\n            FieldsWithDbColumns(rank=2, name=\"b\"),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n\n    conflicting_objects = [\n        FieldsWithDbColumns(rank=1, name=\"c\"),\n        FieldsWithDbColumns(rank=2, name=\"d\"),\n    ]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"rank\"],\n        update_fields=[\"name\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"rank\", \"name\"),\n        [\n            {\"rank\": 1, \"name\": \"c\"},\n            {\"rank\": 2, \"name\": \"d\"},\n        ],\n    )\n\n    # Test update fields that are not unique.\n    results = FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(rank=1, name=\"e\", description=\"desc e\"),\n            FieldsWithDbColumns(rank=2, name=\"f\", description=\"desc f\"),\n        ],\n        update_conflicts=True,\n        unique_fields=[\"rank\"],\n        update_fields=[\"name\", \"description\"],\n    )\n    self.assertEqual(len(results), 2)\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"rank\", \"name\", \"description\"),\n        [\n            {\"rank\": 1, \"name\": \"e\", \"description\": \"desc e\"},\n            {\"rank\": 2, \"name\": \"f\", \"description\": \"desc f\"},\n        ],\n    )\n"], "sample_490": ["def test_clone(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique_fields\",\n        condition=models.Q(foo=models.F(\"bar\")),\n        deferrable=models.Deferrable.IMMEDIATE,\n        include=[\"baz_1\", \"baz_2\"],\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n        nulls_distinct=True,\n        violation_error_message=\"Custom message\",\n        violation_error_code=\"custom_code\",\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertEqual(cloned_constraint.name, \"unique_fields\")\n    self.assertEqual(\n        cloned_constraint.condition, models.Q(foo=models.F(\"bar\"))\n    )\n    self.assertEqual(cloned_constraint.deferrable, models.Deferrable.IMMEDIATE)\n    self.assertEqual(cloned_constraint.include, [\"baz_1\", \"baz_2\"])\n    self.assertEqual(\n        cloned_constraint.opclasses,\n        [\"text_pattern_ops\", \"varchar_pattern_ops\"],\n    )\n    self.assertEqual(cloned_constraint.nulls_distinct, True)\n    self.assertEqual(cloned_constraint.violation_error_message, \"Custom message\")\n    self.assertEqual(cloned_constraint.violation_error_code, \"custom_code\")\n", "def test_clone(self):\n    constraint = UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique_fields\")\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n", "def test_clone(self):\n    constraint = UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n    self.assertEqual(cloned_constraint.name, \"unique\")\n    self.assertEqual(cloned_constraint.fields, (\"foo\", \"bar\"))\n", "def test_unique_constraint_with_multiple_expressions(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        F(\"color\"),\n        name=\"multi_expression_uniq\",\n    )\n    self.assertEqual(\n        repr(constraint),\n        \"<UniqueConstraint: expressions=(Lower(F(name)), F(color)) \"\n        \"name='multi_expression_uniq'>\",\n    )\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(\n            name=self.p1.name.upper(), color=self.p1.color\n        )\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    with self.assertRaisesMessage(ValidationError, \"Constraint \u201cmulti_expression_uniq\u201d is violated.\"):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper(), color=self.p1.color),\n        )\n", "def test_validate_expression_with_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper())\n    # Exclude field used in the expression.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n    # Exclude field not used in the expression.\n    with self.assertRaises(ValidationError):\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"color\"},\n        )\n"], "sample_492": ["def test_serialize_recursive_data_structures(self):\n    # Test that MigrationWriter can serialize recursive data structures.\n    recursive_list = [1, 2]\n    recursive_list.append(recursive_list)\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize:\"):\n        MigrationWriter.serialize(recursive_list)\n\n    recursive_dict = {\"a\": 1}\n    recursive_dict[\"b\"] = recursive_dict\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize:\"):\n        MigrationWriter.serialize(recursive_dict)\n\n    recursive_tuple = (1, 2)\n    recursive_tuple += (recursive_tuple,)\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize:\"):\n        MigrationWriter.serialize(recursive_tuple)\n", "def test_serialize_set_with_unserializable_elements(self):\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: <lambda>\"):\n        self.assertSerializedEqual({lambda x: x})\n", "def test_serialize_type_type(self):\n    self.assertSerializedEqual(type)\n    self.assertSerializedResultEqual(\n        type,\n        (\"type\", set()),\n    )\n", "def test_serialize_decimal_context(self):\n    context = decimal.getcontext()\n    self.assertSerializedEqual(context)\n    string, imports = MigrationWriter.serialize(context)\n    self.assertEqual(\n        string,\n        \"decimal.Context(prec=28, rounding=decimal.ROUND_HALF_EVEN, \"\n        \"Emin=-999999, Emax=999999, capitals=1, clamp=0, flags=[], traps=[\"\n        \"decimal.InvalidOperation, decimal.DivisionByZero, decimal.Overflow])\",\n    )\n    self.assertEqual(imports, {\"import decimal\"})\n", "def test_serialize_custom_timezone(self):\n    class CustomTimeZone(datetime.tzinfo):\n            super().__init__()\n            self.minutes = minutes\n\n            return datetime.timedelta(minutes=self.minutes)\n\n            return datetime.timedelta(0)\n\n            return \"Custom\"\n\n            return (\n                \"%s.%s\" % (self.__class__.__module__, self.__class__.__name__),\n                [self.minutes],\n                {},\n            )\n\n    dt = datetime.datetime(2012, 1, 1, 1, 1, tzinfo=CustomTimeZone(30))\n    self.assertSerializedResultEqual(\n        dt,\n        (\n            \"datetime.datetime(2012, 1, 1, 1, 1, tzinfo=migrations.test_writer.\"\n            \"WriterTests.test_serialize_custom_timezone.<locals>.CustomTimeZone(30))\",\n            {\"import datetime\", \"import migrations.test_writer\"},\n        ),\n    )\n"], "sample_493": ["def test_annotate_with_case(self):\n    qs = Book.objects.annotate(\n        is_expensive=Case(When(price__gt=30, then=True), default=False)\n    )\n    self.assertEqual(qs.get(pk=self.b1.pk).is_expensive, False)\n    self.assertEqual(qs.get(pk=self.b5.pk).is_expensive, True)\n\n    # Use a database function in the Case expression.\n    qs = Book.objects.annotate(\n        is_long=Case(When(pages__gt=400, then=Length(\"name\")), default=0)\n    )\n    self.assertGreater(qs.get(pk=self.b5.pk).is_long, 0)\n    self.assertEqual(qs.get(pk=self.b6.pk).is_long, 0)\n\n    # Use an aggregate in the Case expression.\n    qs = Book.objects.annotate(\n        num_authors=Count(\"authors\"),\n        has_multiple_authors=Case(When(num_authors__gt=1, then=True), default=False),\n    )\n    self.assertEqual(qs.get(pk=self.b1.pk).has_multiple_authors, True)\n    self.assertEqual(qs.get(pk=self.b3.pk).has_multiple_authors, False)\n", "def test_aggregate_filter_subquery(self):\n    \"\"\"An aggregate filter with a subquery should be pushed into the subquery.\"\"\"\n    subquery = Book.objects.filter(publisher=OuterRef(\"pk\"))\n    publishers = Publisher.objects.annotate(\n        total_pages=Sum(\n            \"book__pages\",\n            filter=Q(id__in=subquery.values(\"publisher\")),\n        ),\n    )\n    self.assertEqual(len(publishers), 5)\n", "def test_aggregation_empty_result_set_with_non_null_default(self):\n    result = Publisher.objects.filter(num_awards__gt=100).aggregate(\n        value=Sum(\"duration\", default=datetime.timedelta(days=1)),\n    )\n    self.assertEqual(result[\"value\"], datetime.timedelta(days=1))\n", "def test_aggregation_group_by_external(self):\n    \"\"\"Group by clause contains external column references when grouping\n    against a subquery.\"\"\"\n    subquery = Book.objects.filter(publisher=OuterRef(\"pk\")).values_list(\n        \"publisher\", flat=True\n    )\n    publisher_qs = (\n        Publisher.objects.annotate(book_id=Subquery(subquery))\n        .values(\"book_id\")\n        .annotate(count=Count(\"*\"))\n        .order_by()\n    )\n    self.assertEqual(\n        list(publisher_qs),\n        [{\"book_id\": 1, \"count\": 2}, {\"book_id\": 2, \"count\": 1}, {\"book_id\": 3, \"count\": 2}],\n    )\n", "def test_aggregation_over_subquery_annotation(self):\n    \"\"\"Aggregate over subquery annotation should work.\"\"\"\n    subquery = Book.objects.filter(\n        publisher=OuterRef(\"pk\")\n    ).values(\"publisher\").annotate(c=Count(\"*\")).values(\"c\")\n    publishers = Publisher.objects.annotate(count=Subquery(subquery)).aggregate(\n        total_count=Sum(\"count\")\n    )\n    self.assertEqual(publishers[\"total_count\"], 6)\n"], "sample_494": ["def test_serialize_type_with_module(self):\n    self.assertSerializedEqual(type(None))\n    string, imports = MigrationWriter.serialize(type(None))\n    self.assertEqual(string, \"types.NoneType\")\n    self.assertEqual(imports, {\"import types\"})\n\n    self.assertSerializedEqual(models.Model)\n    string, imports = MigrationWriter.serialize(models.Model)\n    self.assertEqual(string, \"models.Model\")\n    self.assertEqual(imports, {\"from django.db import models\"})\n", "def test_serialize_type_with_module(self):\n    class TestClass:\n        pass\n\n    string, imports = MigrationWriter.serialize(type(TestClass))\n    self.assertEqual(string, \"type\")\n    self.assertEqual(imports, set())\n", "def test_serialize_regex_pattern(self):\n    pattern = re.compile(r\"^\\w+$\").pattern\n    self.assertSerializedEqual(pattern)\n    string, imports = MigrationWriter.serialize(pattern)\n    self.assertEqual(string, repr(pattern))\n    self.assertEqual(imports, set())\n", "def test_serialize_model_field_with_choices(self):\n    choices = [\n        (1, \"one\"),\n        (2, \"two\"),\n    ]\n    field = models.IntegerField(choices=choices)\n    string, imports = MigrationWriter.serialize(field)\n    self.assertEqual(\n        string,\n        \"models.IntegerField(choices=[(1, 'one'), (2, 'two')])\",\n    )\n    self.assertEqual(imports, {\"from django.db import models\"})\n", "def test_serialize_timezone(self):\n    self.assertSerializedEqual(datetime.timezone.utc)\n    self.assertSerializedResultEqual(\n        datetime.timezone.utc,\n        (\"datetime.timezone.utc\", {\"import datetime\"}),\n    )\n    self.assertSerializedEqual(zoneinfo.ZoneInfo(\"America/New_York\"))\n    self.assertSerializedResultEqual(\n        zoneinfo.ZoneInfo(\"America/New_York\"),\n        (\"zoneinfo.ZoneInfo('America/New_York')\", {\"import zoneinfo\"}),\n    )\n"], "sample_495": ["def test_paginating_ordered_object_list_does_not_warn(self):\n    \"\"\"\n    No unordered object list warning with an object that has an ordered\n    attribute and is set to True.\n    \"\"\"\n    class ObjectList:\n        ordered = True\n    object_list = ObjectList()\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator(object_list, 5)\n    self.assertEqual(len(recorded), 0)\n", "def test_page_representation(self):\n    paginator = Paginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertEqual(repr(page), '<Page 1 of 2>')\n    self.assertEqual(str(page), '<Page 1 of 2>')\n", "def test_paginating_object_list_with_ordered_attribute_does_not_warn(self):\n    \"\"\"\n    No warning is raised if the object list has an ordered attribute that is True.\n    \"\"\"\n    class ObjectList:\n        ordered = True\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator(ObjectList(), 5)\n    self.assertEqual(len(recorded), 0)\n", "def test_page_representation(self):\n    paginator = Paginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertEqual(repr(page), '<Page 1 of 2>')\n", "def test_paginating_ordered_object_list_does_not_warn(self):\n    \"\"\"\n    Ordered object list does not raise a warning.\n    \"\"\"\n    class ObjectList:\n        ordered = True\n    object_list = ObjectList()\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator(object_list, 5)\n    self.assertEqual(len(recorded), 0)\n"], "sample_496": ["    def test_styles(self):\n        wrapper = OutputWrapper(StringIO())\n        self.assertEqual(wrapper.style_func('Hello, world!'), 'Hello, world!')\n        wrapper.style_func = color.make_style('nocolor')\n        self.assertEqual(wrapper.style_func('Hello, world!'), 'Hello, world!')\n", "    def setUp(self):\n        self.write_settings('settings.py')\n", "    def setUp(self):\n        self.write_settings('settings.py')\n", "    def test_command_error(self):\n        args = ['nonexistent_command']\n        out, err = self.run_manage(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'nonexistent_command'\")\n", "    def test_handle_default_options(self):\n        options = {'settings': 'test_project.settings', 'pythonpath': '/tmp'}\n        handle_default_options(options)\n        self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], 'test_project.settings')\n        self.assertEqual(sys.path[0], '/tmp')\n"], "sample_497": ["def test_get_tick_space():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    xaxis = ax.xaxis\n    yaxis = ax.yaxis\n\n    # Test the tick space for x-axis\n    ends = ax.transAxes.transform([[0, 0], [1, 0]])\n    length = ((ends[1][0] - ends[0][0]) / fig.dpi) * 72\n    tick = xaxis._get_tick(True)\n    size = tick.label1.get_size() * 3.0\n    if size > 0:\n        expected = int(np.floor(length / size))\n    else:\n        expected = 2**31 - 1\n    assert xaxis.get_tick_space() == expected\n\n    # Test the tick space for y-axis\n    ends = ax.transAxes.transform([[0, 0], [0, 1]])\n    length = ((ends[1][1] - ends[0][1]) / fig.dpi) * 72\n    tick = yaxis._get_tick(True)\n    size = tick.label1.get_size() * 2.0\n    if size > 0:\n        expected = int(np.floor(length / size))\n    else:\n        expected = 2**31 - 1\n    assert yaxis.get_tick_space() == expected\n", "def test_axis_remove_overlapping_locs():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.xaxis.set_major_locator(mticker.MultipleLocator(1))\n    ax.xaxis.remove_overlapping_locs = True\n\n    # Create overlapping ticks by setting a small figure size\n    fig.set_size_inches(2, 2)\n\n    fig.canvas.draw()\n\n    ticks = ax.xaxis.get_major_ticks()\n    ticklabels = [tick.label1 for tick in ticks]\n    assert len(ticks) > len([label for label in ticklabels if label.get_visible()])\n", "def test_axis_remove_overlapping_locs():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.xaxis.set_major_locator(mticker.MultipleLocator(1))\n    ax.xaxis.set_minor_locator(mticker.MultipleLocator(0.5))\n\n    # Check that the remove_overlapping_locs parameter is used correctly.\n    ax.xaxis.remove_overlapping_locs = False\n    assert len(ax.xaxis.get_minorticklocs()) == 19\n\n    ax.xaxis.remove_overlapping_locs = True\n    assert len(ax.xaxis.get_minorticklocs()) < 19\n\n    # Check that the getter and setter work correctly.\n    ax.xaxis.remove_overlapping_locs = False\n    assert not ax.xaxis.get_remove_overlapping_locs()\n\n    ax.xaxis.remove_overlapping_locs = True\n    assert ax.xaxis.get_remove_overlapping_locs()\n\n    # Check that setting the property to None resets it to the default value.\n    ax.xaxis.remove_overlapping_locs = None\n    assert ax.xaxis.get_remove_overlapping_locs() is True\n", "def test_axis_label_position():\n    fig, ax = plt.subplots()\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n\n    # Test default positions\n    assert ax.xaxis.label.get_position() == (0.5, 0)\n    assert ax.yaxis.label.get_position() == (0, 0.5)\n\n    # Test setting positions\n    ax.xaxis.set_label_position('top')\n    ax.yaxis.set_label_position('right')\n    assert ax.xaxis.label.get_position() == (0.5, 1)\n    assert ax.yaxis.label.get_position() == (1, 0.5)\n\n    # Test invalid position\n    with pytest.raises(ValueError):\n        ax.xaxis.set_label_position('invalid')\n", "    def test_tick_remove(self):\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1])\n        ax.set_xticks([0.5])\n        assert len(ax.xaxis.majorTicks) == 1\n        ax.xaxis.majorTicks[0].remove()\n        assert len(ax.xaxis.majorTicks) == 0\n"], "sample_498": ["def test_legend_handler_map():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], label='test')\n    handler_map = {mlines.Line2D: legend_handler.HandlerLine2D()}\n    with mock.patch('matplotlib.legend._get_legend_handles_labels') as handles_labels:\n        handles_labels.return_value = ([ax.lines[0]], ['test'])\n        ax.legend(handler_map=handler_map)\n    handles_labels.assert_called_with([ax], handler_map)\n", "def test_legend_with_fancybox_and_framealpha():\n    # Test legend with fancybox and framealpha\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    leg = ax.legend(fancybox=True, framealpha=0.5)\n    assert leg.get_frame().get_alpha() == 0.5\n", "def test_legend_with_fancybox_and_shadow():\n    # Test that legend with fancybox and shadow is correctly rendered.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='Line')\n    legend = ax.legend(fancybox=True, shadow=True)\n    assert legend.get_frame().get_path_effects()[0].offset == (2, -2)\n", "def test_legend_set_title():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n    new_title = \"New Legend Title\"\n    leg.set_title(new_title)\n    assert leg.get_title().get_text() == new_title\n    leg.remove()\n    ax.legend(title=new_title)\n    assert ax.get_legend().get_title().get_text() == new_title\n", "def test_legend_handler_map():\n    # Test that legend handler map is properly updated\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    handler_map = {mlines.Line2D: mlegend.HandlerLine2D()}\n    leg = ax.legend(handler_map=handler_map)\n    assert leg.get_legend_handler_map()[mlines.Line2D] == handler_map[mlines.Line2D]\n"], "sample_499": ["def test_legend_labelspacing():\n    # Test that labelspacing is used correctly when creating a legend.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='Line 1')\n    ax.plot([0, 1], [1, 0], label='Line 2')\n    leg = ax.legend(labelspacing=2)\n    assert leg.labelspacing == 2\n    fig.canvas.draw()  # Needed to update the text positions.\n    y1 = leg.get_texts()[0].get_position()[1]\n    y2 = leg.get_texts()[1].get_position()[1]\n    assert abs(y2 - y1) > leg._fontsize * 1.5  # Should be more than default spacing.\n", "def test_legend_bbox_to_anchor():\n    # Test that legend can be placed at specified bbox_to_anchor.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    assert leg.get_bbox_to_anchor().x0 == 1.05\n    assert leg.get_bbox_to_anchor().y0 == 1\n", "def test_legend_with_no_parent():\n    legend = mlegend.Legend(None, [], [], loc='upper right')\n    assert legend.parent is None\n    assert legend.isaxes is False\n\n    with pytest.raises(TypeError):\n        legend.set_figure(None)\n\n    with pytest.raises(TypeError):\n        legend._set_loc('upper left')\n\n    with pytest.raises(AttributeError):\n        legend.get_bbox_to_anchor()\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='line')\n    leg = ax.legend()\n    leg.set_bbox_to_anchor((0.5, 0.5))\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n", "def test_legend_bbox_to_anchor():\n    # Test that legend can be placed using bbox_to_anchor argument\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    assert leg.get_bbox_to_anchor().x0 == 1.05\n    assert leg.get_bbox_to_anchor().y0 == 1\n"], "sample_500": ["def test_colorbar_with_lines():\n    fig, ax = plt.subplots()\n    x = np.arange(-3.0, 4.001)\n    y = np.arange(-4.0, 3.001)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    levels = [-12, -5, 0, 5, 12]\n    colors = ['r', 'g', 'b', 'c']\n    cs = ax.contourf(X, Y, Z, levels=levels, colors=colors, extend='neither')\n    cbar = fig.colorbar(cs, ax=ax, orientation='horizontal')\n    cbar.add_lines(cs, colors='black')\n    assert len(cbar.lines) == 1\n", "def test_colorbar_with_no_mappable(fig_ref, fig_test):\n    # Test that a colorbar can be created without a mappable\n    ax = fig_test.add_subplot()\n    cax = fig_test.add_axes([0.9, 0.1, 0.03, 0.8])\n    cb = Colorbar(cax, cmap='viridis', norm=Normalize(vmin=0, vmax=10))\n    cb.set_ticks([0, 5, 10])\n", "def test_colorbar_with_log_norm(fig_ref, fig_test):\n    # Test that colorbar with log norm is correctly plotted\n    np.random.seed(19680801)\n    data = np.random.rand(30, 40)\n    data[data < 0.1] = 0\n\n    ax = fig_ref.add_subplot()\n    im = ax.pcolormesh(data, norm=LogNorm(vmin=0.01, vmax=1))\n    fig_ref.colorbar(im)\n\n    ax = fig_test.add_subplot()\n    im = ax.pcolormesh(data, norm=LogNorm(vmin=0.01, vmax=1))\n    cbar = fig_test.colorbar(im)\n    cbar.ax.set_yscale('log')\n", "def test_colorbar_alpha(fig_ref, fig_test):\n    np.random.seed(0)\n    data = np.random.rand(10, 10)\n    alpha = np.random.rand(10, 10)\n\n    ax = fig_ref.add_subplot()\n    pc = ax.pcolormesh(data, alpha=alpha)\n    fig_ref.colorbar(pc)\n\n    ax = fig_test.add_subplot()\n    pc = ax.pcolormesh(data, alpha=alpha)\n    cbar = fig_test.colorbar(pc)\n    assert cbar.alpha is None\n    assert pc.get_alpha() is alpha\n", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10), alpha=0.5)\n    cb = fig.colorbar(pc)\n    assert cb.alpha == 0.5\n\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10), alpha=np.linspace(0, 1, 100).reshape(10, 10))\n    cb = fig.colorbar(pc)\n    assert cb.alpha is None\n"], "sample_501": ["def test_legend_bbox_to_anchor_transform():\n    # Test that legend's bbox_to_anchor argument can accept a transform.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5), loc='center',\n                    bbox_transform=ax.transAxes)\n    assert leg.get_bbox_to_anchor().transform == ax.transAxes\n", "def test_legend_bbox_to_anchor_transformed():\n    # Test that legend's bbox_to_anchor is correctly transformed.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\",\n                    bbox_transform=ax.transAxes)\n    assert leg.get_bbox_to_anchor().x0 > ax.bbox.x1\n", "def test_legend_remove_with_empty_list():\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10))\n    leg = fig.legend(lines, \"test\")\n    fig.legends = []\n    leg.remove()\n    assert fig.legends == []\n", "def test_legend_handler_map_update():\n    # Test that the legend handler map is updated correctly\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(handler_map={mlines.Line2D: mlegend.HandlerLine2D()})\n    assert isinstance(leg.legendHandles[0], mlines.Line2D)\n", "def test_legend_with_empty_label():\n    # Test that a legend can be created with an empty label.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='')\n    leg = ax.legend()\n    assert len(leg.get_texts()) == 0\n"], "sample_502": ["def test_subplot_mosaic():\n    # Simple grid\n    fig, axd = plt.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    assert len(axd) == 4\n    assert set(axd.keys()) == {'A', 'B', 'C', 'D'}\n\n    # Grid with some empty cells\n    fig, axd = plt.subplot_mosaic([['A', 'B'], [None, 'D']])\n    assert len(axd) == 3\n    assert set(axd.keys()) == {'A', 'B', 'D'}\n\n    # Nested grid\n    inner = [['1', '2'], ['3', '4']]\n    fig, axd = plt.subplot_mosaic([[inner, 'B'], ['C', 'D']])\n    assert len(axd) == 6\n    assert set(axd.keys()) == {'B', 'C', 'D', '1', '2', '3', '4'}\n\n    # Nested grid with some empty cells\n    inner = [[None, '2'], [None, '4']]\n    fig, axd = plt.subplot_mosaic([[inner, 'B'], ['C', 'D']])\n    assert len(axd) == 5\n    assert set(axd.keys()) == {'B', 'C', 'D', '2', '4'}\n", "def test_subplot_mosaic():\n    # Create a figure and axis using subplot_mosaic\n    fig, ax_dict = plt.subplot_mosaic(\n        [['left', 'right'], ['bottom', 'bottom']],\n        figsize=(7, 5)\n    )\n\n    # Check that the axes are in the correct position\n    assert ax_dict['left'].get_position().x0 < ax_dict['right'].get_position().x0\n    assert ax_dict['left'].get_position().y0 > ax_dict['bottom'].get_position().y0\n\n    # Check that the axes are not overlapping\n    assert not ax_dict['left'].get_position().overlaps(ax_dict['right'].get_position())\n    assert not ax_dict['left'].get_position().overlaps(ax_dict['bottom'].get_position())\n    assert not ax_dict['right'].get_position().overlaps(ax_dict['bottom'].get_position())\n\n    # Check that the axes are accessible by their labels\n    assert 'left' in ax_dict\n    assert 'right' in ax_dict\n    assert 'bottom' in ax_dict\n\n    # Check that the axes have the correct number of ticks\n    assert len(ax_dict['left'].get_xticks()) > 0\n    assert len(ax_dict['right'].get_xticks()) > 0\n    assert len(ax_dict['bottom'].get_xticks()) > 0\n", "def test_subplot2grid():\n    ax = plt.subplot2grid((1, 1), (0, 0))\n    assert isinstance(ax, mpl.axes.Axes)\n    ax = plt.subplot2grid((3, 3), (0, 0), colspan=2, rowspan=2)\n    assert isinstance(ax, mpl.axes.Axes)\n    with pytest.raises(ValueError):\n        plt.subplot2grid((1, 1), (0, 0), colspan=2)\n    with pytest.raises(ValueError):\n        plt.subplot2grid((1, 1), (0, 0), rowspan=2)\n", "def test_subplot_mosaic():\n    fig, axd = plt.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    assert len(axd) == 4\n    for k in 'ABCD':\n        assert k in axd\n    assert all(isinstance(ax, mpl.axes.Axes) for ax in axd.values())\n    assert fig is plt.gcf()\n", "def test_subplot_mosaic():\n    fig, axd = plt.subplot_mosaic([['left', 'right'], ['bottom', 'bottom']],\n                                   empty_sentinel='.')\n    assert len(axd) == 3\n    assert 'left' in axd\n    assert 'right' in axd\n    assert 'bottom' in axd\n\n    # Check that the axes are laid out as expected.\n    bb_left = axd['left'].get_position()\n    bb_right = axd['right'].get_position()\n    bb_bottom_left = axd['bottom'].get_position()\n    assert bb_left.x0 < bb_right.x0\n    assert bb_left.y1 > bb_bottom_left.y0\n    assert bb_right.y1 > bb_bottom_left.y0\n    assert bb_bottom_left.width == bb_right.width + bb_left.width\n"], "sample_503": ["def test_axline(fig_test, fig_ref):\n    fig_test.add_subplot().axline((1, 1), (2, 2), color='r')\n    fig_ref.add_subplot().plot([0, 3], [0, 3], color='r')\n", "def test_transformed_path(fig_test, fig_ref):\n    ax = fig_test.add_subplot()\n    x = np.arange(10)\n    y = np.sin(x)\n    line, = ax.plot(x, y)\n\n    # Trigger creation of transformed path\n    line._transform_path()\n\n    ax_ref = fig_ref.add_subplot()\n    line_ref, = ax_ref.plot(x, y)\n    line_ref.set_transform(line.get_transform())\n", "def test_dash_joinstyle(fig_test, fig_ref):\n    fig_test.add_subplot().plot([1, 2], linestyle=(0, (3, 3)), linewidth=10,\n                                 dash_joinstyle='round', solid_joinstyle='miter')\n    fig_ref.add_subplot().plot([1, 2], linestyle=(0, (3, 3)), linewidth=10,\n                                dash_joinstyle='round', solid_joinstyle='miter')\n", "def test_axline(fig_test, fig_ref):\n    fig_test.add_subplot().axline((1, 2), (3, 4))\n    fig_ref.add_subplot().plot([0, 10], [1, 9], transform=mtransforms.IdentityTransform())\n", "def test_linestyle_none(fig_test, fig_ref):\n    fig_test.add_subplot().plot([1, 2], linestyle=None)\n    fig_ref.add_subplot().plot([1, 2], linestyle='None')\n"], "sample_504": ["def test_colorbar_with_empty_mappable(fig_test, fig_ref):\n    ax = fig_test.add_subplot()\n    mappable = cm.ScalarMappable(cmap='viridis')\n    cb = fig_test.colorbar(mappable, cax=ax)\n    cb.set_ticks([0.2, 0.5, 0.8])\n", "def test_colorbar_with_no_mappable():\n    fig, ax = plt.subplots()\n    with pytest.raises(TypeError):\n        Colorbar(ax)\n    with pytest.raises(TypeError):\n        Colorbar(ax, cmap='viridis')\n    with pytest.raises(TypeError):\n        Colorbar(ax, norm=Normalize())\n    Colorbar(ax, cmap='viridis', norm=Normalize())\n    Colorbar(ax, mappable=cm.ScalarMappable(cmap='viridis'))\n", "def test_colorbar_with_lines():\n    # Test colorbar with contour lines.\n    fig, ax = plt.subplots()\n    x = y = np.arange(-3.0, 4.001)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    levels = [-12, -5, 0, 5, 12]\n    cs = ax.contourf(X, Y, Z, levels=levels, extend='both')\n    ax.contour(X, Y, Z, levels=[-10, 0, 10], colors='black')\n    fig.colorbar(cs)\n", "def test_colorbar_update_normal():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    cs = ax.contourf(data, levels=levels)\n    cbar = fig.colorbar(cs)\n\n    # Update the norm of the mappable and check if colorbar is updated\n    cs.set_norm(Normalize(vmin=0, vmax=1500))\n    fig.canvas.draw()\n\n    # Check if the ticks and limits are updated correctly\n    assert len(cbar.ax.yaxis.get_ticklocs()) == 7\n    np.testing.assert_almost_equal(cbar.ax.yaxis.get_ticklocs(),\n                                   np.linspace(0, 1500, 7))\n\n    # Update the norm again and check if colorbar is updated\n    cs.set_norm(Normalize(vmin=-500, vmax=1500))\n    fig.canvas.draw()\n\n    # Check if the ticks and limits are updated correctly\n    assert len(cbar.ax.yaxis.get_ticklocs()) == 7\n    np.testing.assert_almost_equal(cbar.ax.yaxis.get_ticklocs(),\n                                   np.linspace(-500, 1500, 7))\n", "def test_colorbar_label_rotation():\n    \"\"\"\n    Test the label rotation parameter for horizontal and vertical colorbars.\n    \"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, orientation='horizontal')\n    cbar.set_label('horizontal label', rotation=45)\n    assert cbar.ax.get_xlabel() == 'horizontal label'\n    assert cbar.ax.xaxis.label.get_rotation() == 45\n\n    cbar = fig.colorbar(im, orientation='vertical')\n    cbar.set_label('vertical label', rotation=90)\n    assert cbar.ax.get_ylabel() == 'vertical label'\n    assert cbar.ax.yaxis.label.get_rotation() == 90\n"], "sample_505": ["def test_date_ticker_factory():\n    # Test date_ticker_factory function.\n    fig, ax = plt.subplots()\n    t0 = datetime.datetime(2017, 1, 1)\n    t1 = datetime.datetime(2018, 12, 31)\n    ax.set_xlim(t0, t1)\n\n    locator, formatter = mdates.date_ticker_factory(t1 - t0, numticks=5)\n\n    ax.xaxis.set_major_locator(locator)\n    ax.xaxis.set_major_formatter(formatter)\n\n    ticks = ax.get_xticks()\n    labels = [ax.xaxis.get_major_formatter()(tick) for tick in ticks]\n\n    assert len(ticks) == 5\n    assert len(labels) == 5\n", "def test_date2num_microseconds():\n    dt = datetime.datetime(2020, 1, 1, 12, 30, 30, 123456)\n    num = mdates.date2num(dt)\n    assert np.isclose(num % 1, (12 * 3600 + 30 * 60 + 30 + 123456 / 1e6) / 86400)\n", "def test_datestr2num():\n    # test with single date string\n    date_str = '2022-01-01'\n    expected_result = mdates.date2num(datetime.datetime(2022, 1, 1))\n    assert mdates.datestr2num(date_str) == expected_result\n\n    # test with list of date strings\n    date_strs = ['2022-01-01', '2022-01-02']\n    expected_results = [mdates.date2num(datetime.datetime(2022, 1, 1)),\n                        mdates.date2num(datetime.datetime(2022, 1, 2))]\n    np.testing.assert_array_equal(mdates.datestr2num(date_strs), expected_results)\n\n    # test with default parameter\n    date_str = '01/01/22'\n    default = datetime.datetime(2020, 1, 1)\n    expected_result = mdates.date2num(datetime.datetime(2022, 1, 1))\n    assert mdates.datestr2num(date_str, default=default) == expected_result\n", "def test_datestr2num():\n    date_str = \"2020-01-01\"\n    date_num = mdates.datestr2num(date_str)\n    assert isinstance(date_num, float)\n\n    date_str_list = [\"2020-01-01\", \"2020-01-02\"]\n    date_num_list = mdates.datestr2num(date_str_list)\n    assert isinstance(date_num_list, np.ndarray)\n    assert len(date_num_list) == 2\n", "def test_datestr2num():\n    dates = ['2020-01-01', '2020-02-01']\n    nums = mdates.datestr2num(dates)\n    expected = mdates.date2num([datetime.datetime(2020, 1, 1), datetime.datetime(2020, 2, 1)])\n    np.testing.assert_equal(nums, expected)\n\n    default = datetime.datetime(2020, 1, 1)\n    dates = ['01-02', '02-03']\n    nums = mdates.datestr2num(dates, default)\n    expected = mdates.date2num([datetime.datetime(2020, 1, 2), datetime.datetime(2020, 2, 3)])\n    np.testing.assert_equal(nums, expected)\n"], "sample_506": ["def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() is None\n\n    spine.set_bounds((0, 1))\n    assert spine.get_bounds() == (0, 1)\n\n    spine.set_bounds(low=2)\n    assert spine.get_bounds() == (2, 1)\n\n    spine.set_bounds(high=3)\n    assert spine.get_bounds() == (2, 3)\n\n    with pytest.raises(ValueError):\n        spine.set_bounds('circle', 0, 1)\n", "def test_spine_get_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() is None\n    spine.set_bounds(0, 1)\n    assert spine.get_bounds() == (0, 1)\n    spine.set_bounds(None, None)\n    assert spine.get_bounds() == (0, 1)\n    spine.set_bounds(2, None)\n    assert spine.get_bounds() == (2, 1)\n    spine.set_bounds(None, 3)\n    assert spine.get_bounds() == (2, 3)\n", "def test_spine_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() is None\n\n    spine.set_bounds(0, 10)\n    assert spine.get_bounds() == (0, 10)\n\n    spine.set_bounds(None, 20)\n    assert spine.get_bounds() == (0, 20)\n\n    spine.set_bounds(5, None)\n    assert spine.get_bounds() == (5, 20)\n\n    spine.set_bounds((3, 15))\n    assert spine.get_bounds() == (3, 15)\n\n    with pytest.raises(ValueError):\n        spine.set_bounds('foo', 10)\n", "def test_spine_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() is None\n\n    spine.set_bounds(0, 1)\n    assert spine.get_bounds() == (0, 1)\n\n    spine.set_bounds(low=2)\n    assert spine.get_bounds() == (2, 1)\n\n    spine.set_bounds(high=3)\n    assert spine.get_bounds() == (2, 3)\n\n    spine.set_bounds(None, 4)\n    assert spine.get_bounds() == (2, 4)\n\n    spine.set_bounds(5, None)\n    assert spine.get_bounds() == (5, 4)\n\n    spine.set_bounds((6, 7))\n    assert spine.get_bounds() == (6, 7)\n", "def test_spine_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() is None\n\n    spine.set_bounds(0, 1)\n    assert spine.get_bounds() == (0, 1)\n\n    spine.set_bounds(low=2)\n    assert spine.get_bounds() == (2, 1)\n\n    spine.set_bounds(high=3)\n    assert spine.get_bounds() == (2, 3)\n\n    spine.set_bounds((4, 5))\n    assert spine.get_bounds() == (4, 5)\n"], "sample_507": ["def test_StrCategoryFormatter_empty_values():\n    unit = cat.UnitData(['a', 'b', 'c'])\n    labels = cat.StrCategoryFormatter(unit._mapping)\n    assert labels([], None) == []\n    assert labels([], []) == []\n    assert labels([1, 2, 3], None) == ['b', 'c', '']\n    assert labels([0, 1, 2], [0, 1, 2]) == ['a', 'b', 'c']\n", "def test_StrCategoryConverter_default_units():\n    cc = cat.StrCategoryConverter()\n    ax = FakeAxis(None)\n    data = ['a', 'b', 'c']\n    units = cc.default_units(data, ax)\n    assert isinstance(units, cat.UnitData)\n    assert list(units._mapping.keys()) == data\n    assert list(units._mapping.values()) == range(len(data))\n    # check that axis.units is updated\n    assert ax.units is units\n", "def test_StrCategoryFormatter_empty_mapping():\n    unit = cat.UnitData()\n    labels = cat.StrCategoryFormatter(unit._mapping)\n    assert labels(0, 0) == ''\n    assert labels(1, None) == ''\n", "def test_StrCategoryConverter_validate_unit():\n    class InvalidUnit:\n        pass\n\n    unit = InvalidUnit()\n    with pytest.raises(ValueError):\n        cat.StrCategoryConverter._validate_unit(unit)\n\n    valid_unit = cat.UnitData()\n    cat.StrCategoryConverter._validate_unit(valid_unit)  # Should not raise\n", "    def test_convert_pass_through(self):\n        # test that convert() passes through sequence of non-binary numbers\n        cc = cat.StrCategoryConverter()\n        unit = cat.UnitData()\n        ax = FakeAxis(unit)\n        with pytest.warns(MatplotlibDeprecationWarning):\n            actual = cc.convert([1.0, 2.0, 3.0], unit, ax)\n        np.testing.assert_allclose(actual, np.array([1., 2., 3.]))\n"], "sample_508": ["def test_artist_findobj():\n    fig, ax = plt.subplots()\n    ln, = ax.plot([1, 2, 3])\n\n    # Find all Artists in the figure.\n    assert len(fig.findobj(martist.Artist)) > 0\n\n    # Find by class.\n    assert len(fig.findobj(mlines.Line2D)) == 1\n\n    # Find using match function.\n        return isinstance(artist, mlines.Line2D)\n\n    assert len(fig.findobj(match)) == 1\n\n    # Include self in search.\n    assert fig.findobj(mlines.Line2D, include_self=True) == [ln]\n\n    # Don't include self in search.\n    assert fig.findobj(mlines.Line2D, include_self=False) == [ln]\n", "def test_artist_stale_callback():\n    fig, ax = plt.subplots()\n    artist = martist.Artist()\n    artist.axes = ax\n\n        assert val\n        artist.stale_callback_called = True\n\n    artist.stale_callback = callback\n    artist.stale = True\n\n    assert hasattr(artist, 'stale_callback_called')\n    assert artist.stale_callback_called\n", "def test_artist_stale_callback():\n    class ArtistSubclass(martist.Artist):\n            super().__init__()\n            self.stale_callback_count = 0\n\n            self.stale_callback_count += 1\n            assert val == self.stale\n\n    artist = ArtistSubclass()\n    artist.stale_callback = artist.stale_callback\n\n    # Check that stale_callback is called when stale is set to True.\n    artist.stale = True\n    assert artist.stale_callback_count == 1\n\n    # Check that stale_callback is not called again if stale is already True.\n    artist.stale = True\n    assert artist.stale_callback_count == 1\n\n    # Check that stale_callback is called when stale is set to False.\n    artist.stale = False\n    assert artist.stale_callback_count == 2\n\n    # Check that stale_callback is not called again if stale is already False.\n    artist.stale = False\n    assert artist.stale_callback_count == 2\n", "def test_artist_set():\n    art = martist.Artist()\n    with pytest.raises(AttributeError):\n        art.set(invalid_property=True)\n    with pytest.raises(TypeError):\n        art.set(zorder='not an integer')\n", "def test_setp_invalid_property():\n    fig, ax = plt.subplots()\n    line, = ax.plot(range(3))\n    with pytest.raises(AttributeError):\n        plt.setp(line, 'invalid_property', 42)\n"], "sample_509": ["def test_num2date_out_of_range():\n    with pytest.raises(ValueError, match=\"Matplotlib dates must be between\"):\n        mdates.num2date(1e16)\n    with pytest.raises(ValueError, match=\"Matplotlib dates must be between\"):\n        mdates.num2date(-1e16)\n", "def test_rrulewrapper_set():\n    rule = mdates.rrulewrapper(dateutil.rrule.DAILY, dtstart=datetime.datetime(2017, 1, 1))\n    assert rule._rrule._dtstart == datetime.datetime(2017, 1, 1)\n    rule.set(dtstart=datetime.datetime(2018, 1, 1))\n    assert rule._rrule._dtstart == datetime.datetime(2018, 1, 1)\n", "def test_date2num_roundtrip():\n    dates = [datetime.datetime(2022, 1, 1, 12, 0, 0),\n             datetime.datetime(2022, 1, 1, 12, 0, 1),\n             datetime.datetime(2022, 1, 1, 12, 0, 1, 100),\n             datetime.datetime(2022, 1, 1, 12, 0, 1, 100, tzinfo=datetime.timezone.utc)]\n    for date in dates:\n        assert mdates.num2date(mdates.date2num(date)) == date\n", "def test_DateFormatter_format_data_short():\n    dt = datetime.datetime(2022, 1, 10)\n    assert mdates.DateFormatter('%Y').format_data_short(mdates.date2num(dt)) == '2022-01-10 00:00:00'\n", "def test_date_formatter_timezone():\n    # Test that DateFormatter correctly handles timezones\n    dt = datetime.datetime(2022, 1, 10, 12, 0, tzinfo=datetime.timezone.utc)\n    formatter = mdates.DateFormatter('%Y-%m-%d %H:%M %Z%z')\n    assert formatter(dt) == '2022-01-10 12:00 UTC+0000'\n"], "sample_510": ["def test_figure_label():\n    fig = plt.figure('Test Figure')\n    assert fig.get_label() == 'Test Figure'\n    assert plt.gcf().get_label() == 'Test Figure'\n\n    # Check that creating a figure with the same label doesn't create a new one\n    plt.figure('Test Figure')\n    assert len(plt.get_fignums()) == 1\n\n    # Check that creating a figure with a different label creates a new one\n    plt.figure('New Figure')\n    assert len(plt.get_fignums()) == 2\n", "def test_subplot_mosaic():\n    fig, axes = plt.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    assert len(axes) == 4\n    for key in ['A', 'B', 'C', 'D']:\n        assert key in axes\n\n    # Test that specifying the gridspec_kw argument works.\n    fig, axes = plt.subplot_mosaic(\n        [['A', 'B'], ['C', 'D']], gridspec_kw={'height_ratios': [1, 3]}\n    )\n    assert axes['A'].get_position().height < axes['C'].get_position().height\n", "def test_subplot_mosaic():\n    # create a mosaic layout and check that the axes are created correctly\n    fig, ax_dict = plt.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    assert len(ax_dict) == 4\n    assert all(isinstance(ax, mpl.axes.Axes) for ax in ax_dict.values())\n\n    # check that the axes are in the correct positions\n    for key, ax in ax_dict.items():\n        assert ax.get_label() == key\n\n    # check that the axes are in the correct grid positions\n    for i, row in enumerate([['A', 'B'], ['C', 'D']]):\n        for j, key in enumerate(row):\n            ax = ax_dict[key]\n            assert ax.get_subplotspec().get_position(fig).bounds == (j/2, i/2, 1/2, 1/2)\n", "def test_subplot_mosaic():\n    fig, axd = plt.subplot_mosaic([['A panel', 'A panel', 'edge'],\n                                    ['C panel', '.',       'edge']],\n                                   empty_sentinel='.')\n    assert len(axd) == 3\n    assert set(axd.keys()) == {'A panel', 'C panel', 'edge'}\n", "def test_tight_layout_with_constrained_layout():\n    # Test that calling tight_layout() on a figure with constrained_layout\n    # enabled does not cause an error.\n    fig = plt.figure(constrained_layout=True)\n    ax = fig.add_subplot(111)\n    fig.tight_layout()\n"], "sample_511": ["def test_switch_backend():\n    original_backend = plt.get_backend()\n    try:\n        plt.switch_backend('pdf')\n        assert plt.get_backend() == 'pdf'\n    finally:\n        plt.switch_backend(original_backend)\n", "def test_subplot_mosaic_sharex_sharey():\n    fig, axes = plt.subplot_mosaic(\n        [['A', 'B'], ['C', 'D']], sharex=True, sharey=True)\n    ax_a, ax_b, ax_c, ax_d = axes['A'], axes['B'], axes['C'], axes['D']\n\n    # Test that x-axis is shared\n    ax_b.set_xlim(0, 1)\n    assert ax_a.get_xlim() == (0, 1)\n\n    # Test that y-axis is shared\n    ax_c.set_ylim(0, 1)\n    assert ax_a.get_ylim() == (0, 1)\n", "def test_subplot2grid():\n    fig = plt.figure()\n    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3)\n    ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)\n    ax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n    ax4 = plt.subplot2grid((3, 3), (2, 0))\n    ax5 = plt.subplot2grid((3, 3), (2, 1))\n\n    assert ax1.get_position().width > ax2.get_position().width\n    assert ax3.get_position().height > ax4.get_position().height\n    assert ax5.get_position().height == ax4.get_position().height\n\n    # Check that attempting to create overlapping axes will delete the old one.\n    with pytest.warns(MatplotlibDeprecationWarning):\n        ax6 = plt.subplot2grid((3, 3), (0, 0), colspan=3)\n\n    assert ax1 not in fig.axes\n    assert ax6 in fig.axes\n", "def test_subplot_tool():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    tool = plt.subplot_tool(ax)\n    assert tool is not None\n", "def test_subplot_tool():\n    fig, ax = plt.subplots()\n    tool = plt.subplot_tool()\n    assert isinstance(tool, mpl.widgets.SubplotTool)\n    plt.close(fig)\n"], "sample_512": ["def test_fignum_exists():\n    fig1 = plt.figure()\n    fig2 = plt.figure()\n    assert plt.fignum_exists(1)\n    assert plt.fignum_exists(2)\n    assert not plt.fignum_exists(3)\n    assert not plt.fignum_exists('nonexistent')\n    plt.close(fig1)\n    assert not plt.fignum_exists(1)\n    assert plt.fignum_exists(2)\n    plt.close(fig2)\n    assert not plt.fignum_exists(2)\n    fig3 = plt.figure()\n    fig3.set_label('existing')\n    assert plt.fignum_exists('existing')\n    assert not plt.fignum_exists('nonexistent')\n    plt.close(fig3)\n    assert not plt.fignum_exists('existing')\n", "def test_switch_backend_interactive_framework():\n    # This test case will ensure that the switch_backend function raises an\n    # ImportError when trying to switch to a backend that requires a different\n    # interactive framework than the one currently running.\n\n    # Get the current backend and its required interactive framework\n    original_backend = get_backend()\n    original_framework = _get_required_interactive_framework(\n        _get_backend_mod())\n\n    # Define the backends and their required interactive frameworks to test\n    test_backends = {\n        'TkAgg': 'tk',\n        'GTK3Agg': 'gtk3',\n        'Qt5Agg': 'qt5',\n        'MacOSX': None,\n    }\n\n    # Try switching to each backend and check if it raises an ImportError\n    for backend, framework in test_backends.items():\n        try:\n            switch_backend(backend)\n        except ImportError as e:\n            # Check if the error message indicates that the backend requires\n            # a different interactive framework\n            if (original_framework is not None and\n                    framework is not None and\n                    framework != original_framework):\n                assert str(e).startswith(\"Cannot load backend\")\n            else:\n                raise\n        else:\n            # If no ImportError was raised, switch back to the original backend\n            switch_backend(original_backend)\n", "def test_fignum_exists():\n    fig = plt.figure()\n    assert plt.fignum_exists(fig.number)\n    assert not plt.fignum_exists(fig.number + 1)\n    plt.close(fig)\n    assert not plt.fignum_exists(fig.number)\n\n    fig = plt.figure(\"Test figure\")\n    assert plt.fignum_exists(\"Test figure\")\n    assert not plt.fignum_exists(\"Non-existent figure\")\n    plt.close(fig)\n    assert not plt.fignum_exists(\"Test figure\")\n", "def test_subplot2grid():\n    fig = plt.figure()\n    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2)\n    ax2 = plt.subplot2grid((3, 3), (0, 2))\n    ax3 = plt.subplot2grid((3, 3), (1, 0), colspan=2, rowspan=2)\n    ax4 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\n    assert ax1.get_position().width > ax2.get_position().width\n    assert ax3.get_position().height > ax1.get_position().height\n    assert ax4.get_position().height == ax3.get_position().height\n\n    # Test with fig given\n    fig2 = plt.figure()\n    ax5 = plt.subplot2grid((1, 1), (0, 0), fig=fig2)\n    assert ax5.figure is fig2\n\n    # Test with projection\n    ax6 = plt.subplot2grid((1, 1), (0, 0), fig=fig2, projection='polar')\n    assert ax6.name == 'polar'\n", "def test_fignum_exists():\n    fig = plt.figure()\n    assert plt.fignum_exists(fig.number)\n    assert not plt.fignum_exists(fig.number + 1)\n    assert plt.fignum_exists(fig)\n    assert not plt.fignum_exists(\"nonexistent figure\")\n"], "sample_513": ["def test_legend_handles_when_no_label():\n    # Test that legend handles are correctly generated when some artists have no label.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line1')\n    ax.plot([1, 2], [5, 6])  # no label\n    ax.plot([1, 2], [7, 8], label='line3')\n    handles, labels = ax.get_legend_handles_labels()\n    assert len(handles) == 2\n    assert len(labels) == 2\n    assert labels == ['line1', 'line3']\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n\n    # Test setting bbox_to_anchor with a BboxBase instance\n    bbox = mtransforms.Bbox.from_bounds(0, 0, 1, 1)\n    leg.set_bbox_to_anchor(bbox)\n    assert leg.get_bbox_to_anchor() == bbox\n\n    # Test setting bbox_to_anchor with a tuple of four values\n    leg.set_bbox_to_anchor((0, 0, 1, 1))\n    assert leg.get_bbox_to_anchor().bounds == (0, 0, 1, 1)\n\n    # Test setting bbox_to_anchor with a tuple of two values\n    leg.set_bbox_to_anchor((0, 0))\n    assert leg.get_bbox_to_anchor().bounds == (0, 0, 0, 0)\n\n    # Test setting bbox_to_anchor to None\n    leg.set_bbox_to_anchor(None)\n    assert leg.get_bbox_to_anchor() is not None\n", "def test_legend_handler_map():\n    # Test that legend handler map is properly updated\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    handler_map = {mlines.Line2D: mlegend.HandlerLine2D()}\n    leg = ax.legend(handler_map=handler_map)\n    assert leg.get_legend_handler_map()[mlines.Line2D] is handler_map[mlines.Line2D]\n", "def test_legend_bbox_to_anchor():\n    # Test that legend can be placed at specified bbox_to_anchor\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5), loc='center')\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n", "def test_legend_get_texts():\n    # Test that legend.get_texts() returns the correct number of Text objects\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n    assert len(leg.get_texts()) == 1\n"], "sample_514": ["def test_colorbar_alpha():\n    # Test that colorbar correctly handles array-like alphas.\n    fig, ax = plt.subplots()\n    cmap = cm.get_cmap('viridis')\n    norm = Normalize(vmin=0, vmax=1)\n    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n    mappable.set_array([0.5])\n    cbar = fig.colorbar(mappable, ax=ax)\n    cbar.set_alpha([0.5, 0.7])  # This should not raise an error\n    assert cbar.alpha is None\n    fig.draw_without_rendering()\n", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    pc = ax.contourf(data, levels=levels, alpha=0.5)\n    fig.colorbar(pc)\n", "def test_colorbar_no_mappable():\n    # Check that a colorbar can be created without an attached mappable.\n    fig, ax = plt.subplots()\n    cmap = plt.get_cmap(\"viridis\")\n    norm = mcolors.Normalize(vmin=0, vmax=1)\n    cb = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n    assert cb.mappable is not None\n    assert cb.mappable not in ax.collections\n    assert cb.mappable not in ax.images\n    assert cb.mappable not in ax.patches\n", "def test_colorbar_with_no_norm_or_cmap():\n    fig, ax = plt.subplots()\n    with pytest.raises(TypeError):\n        Colorbar(ax)\n", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    np.random.seed(seed=19680808)\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, location='right')\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n    cb.set_alpha([0.2, 0.3, 0.4])\n    assert cb.alpha is None\n"], "sample_515": ["def test_colorbar_set_alpha():\n    # Test that setting the alpha value of a colorbar works correctly.\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    assert cb.alpha is None\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n    cb.set_alpha([0.5, 0.6])\n    assert cb.alpha is None\n", "def test_colorbar_no_mappable():\n    # Test that a colorbar can be created without a mappable.\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps[\"viridis\"]\n    norm = Normalize(vmin=0, vmax=1)\n    Colorbar(ax, cmap=cmap, norm=norm)\n", "def test_orientation(fig_ref, fig_test):\n    ax_ref = fig_ref.add_subplot()\n    im = ax_ref.imshow([[0, 1], [2, 3]])\n    ax_ref.figure.colorbar(im, orientation=\"horizontal\")\n    \n    ax_test = fig_test.add_subplot()\n    im = ax_test.imshow([[0, 1], [2, 3]])\n    ax_test.figure.colorbar(im, location=\"bottom\")\n", "def test_colorbar_cmap_with_under_over_badge():\n    # Check that colormaps with under/over set still produce the correct\n    # colorbar (issue #23358).\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps['RdBu'].copy()\n    cmap.set_under('black')\n    cmap.set_over('white')\n    cmap.set_bad('red')\n    data = np.random.randn(10, 10)\n    im = ax.pcolormesh(data, cmap=cmap)\n    fig.colorbar(im)\n", "def test_colorbar_shrink_aspect():\n    fig, ax = plt.subplots(figsize=(6, 6))\n    pc = ax.imshow(np.random.randn(10, 10), cmap='RdBu_r')\n    cb = fig.colorbar(pc, shrink=0.5, aspect=10)\n    np.testing.assert_allclose(cb.ax.get_position().width / cb.ax.get_position().height, 10)\n"], "sample_516": ["def test_multipage_pagecount_with_metadata():\n    md = {\n        'Author': 'me',\n        'Title': 'Multipage PDF',\n        'Subject': 'Test page',\n        'Keywords': 'test,pdf,multipage',\n        'ModDate': datetime.datetime(\n            1968, 8, 1, tzinfo=datetime.timezone(datetime.timedelta(0))),\n        'Trapped': 'True'\n    }\n    with PdfPages(io.BytesIO(), metadata=md) as pdf:\n        assert pdf.get_pagecount() == 0\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3])\n        fig.savefig(pdf, format=\"pdf\")\n        assert pdf.get_pagecount() == 1\n        pdf.savefig()\n        assert pdf.get_pagecount() == 2\n", "def test_encode_string():\n    # Test that the encode_string function in the Pdf backend can handle\n    # different types of strings and font types.\n    from matplotlib.backends.backend_pdf import RendererPdf\n\n    renderer = RendererPdf(io.BytesIO(), dpi=72, height=10, width=10)\n\n    fonttype = mpl.rcParams['pdf.fonttype']\n\n    s = \"Hello, World!\"\n    encoded_s = renderer.encode_string(s, fonttype)\n    assert isinstance(encoded_s, bytes)\n\n    s = \"\u00a1Hola, Mundo!\"\n    encoded_s = renderer.encode_string(s, fonttype)\n    assert isinstance(encoded_s, bytes)\n\n    s = \"\"\n    encoded_s = renderer.encode_string(s, fonttype)\n    assert isinstance(encoded_s, bytes)\n\n    fonttype = 1\n    s = \"Hello, World!\"\n    encoded_s = renderer.encode_string(s, fonttype)\n    assert isinstance(encoded_s, bytes)\n\n    fonttype = 3\n    s = \"Hello, World!\"\n    encoded_s = renderer.encode_string(s, fonttype)\n    assert isinstance(encoded_s, bytes)\n", "def test_text_with_urls():\n    fig, ax = plt.subplots()\n    ax.text(0.1, 0.6, 'Hello', url='https://matplotlib.org')\n    ax.text(0.4, 0.6, 'World', url='https://matplotlib.org')\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n", "def test_pdf_pages_close():\n    # Check that PdfPages close properly when used with 'with' statement\n    pdfio = io.BytesIO()\n    with PdfPages(pdfio) as pdf:\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3])\n        pdf.savefig(fig)\n\n    assert pdf._file is None\n    assert pdfio.getvalue().count(b'startxref') == 1\n", "def test_alpha_too_low():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], color=(0.5, 0.5, 0.5, 1e-10))\n    fig.savefig(io.BytesIO(), format=\"pdf\")\n"], "sample_517": ["def test_text_position_with_transform():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n    trans = mtransforms.Affine2D().translate(5, 5) + ax.transData\n    ax.text(0, 0, 'test', transform=trans)\n\n    ax.plot([0, 10], [5, 5], 'k-')\n    ax.plot([5, 5], [0, 10], 'k-')\n", "def test_text_with_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Hello, World!', ha='center', va='center',\n                   bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"k\"))\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n", "def test_set_text_invalid_input():\n    text = Text(0, 0, 'foo')\n    with pytest.raises(TypeError):\n        text.set_text(None)\n", "def test_update_from():\n    t1 = Text(0, 0, 'test')\n    t2 = Text(0, 0, 'test2')\n    t2.update_from(t1)\n    assert t2.get_text() == t1.get_text()\n    assert t2.get_rotation() == t1.get_rotation()\n    assert t2.get_horizontalalignment() == t1.get_horizontalalignment()\n", "def test_rotation_mode():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"test\", rotation=45, rotation_mode='anchor')\n    assert text.get_rotation_mode() == 'anchor'\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n    with pytest.raises(ValueError):\n        text.set_rotation_mode('invalid')\n"], "sample_518": ["def test_patch_transform():\n    # Test that patches have the correct transform when added to axes.\n    fig, ax = plt.subplots()\n    patch = Patch()\n    assert patch.get_transform() == mtransforms.IdentityTransform()\n    ax.add_patch(patch)\n    assert patch.get_transform() == (ax.transData + ax.transAxes.inverted())\n", "def test_patch_alpha():\n    patch = Patch()\n    assert patch.get_alpha() is None\n    patch.set_alpha(0.5)\n    assert patch.get_alpha() == 0.5\n    patch.set_alpha(None)\n    assert patch.get_alpha() is None\n", "def test_boxstyle_custom():\n    class CustomBoxStyle(BoxStyle._Base):\n            self.pad = pad\n\n            # This custom box style just adds some padding to the rectangle.\n            x0, y0, x1, y1 = path.get_extents().extents\n            pad = mutation_size * self.pad\n            x0 -= pad\n            y0 -= pad\n            x1 += pad\n            y1 += pad\n            return mpath.Path([[x0, y0],\n                               [x1, y0],\n                               [x1, y1],\n                               [x0, y1],\n                               [x0, y0]],\n                              closed=True), True\n\n    fig, ax = plt.subplots()\n    patch = FancyBboxPatch((0.2, 0.2), 0.5, 0.5,\n                           boxstyle=CustomBoxStyle(pad=0.4))\n    ax.add_patch(patch)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_boxstyle_get_styles():\n    styles = BoxStyle.get_styles()\n    assert isinstance(styles, dict)\n    for name, style in styles.items():\n        assert isinstance(name, str)\n        assert issubclass(style, BoxStyle._Base)\n", "def test_patch_transform():\n    patch = Patch()\n    transform = mtransforms.Affine2D().translate(1, 2).scale(3, 4)\n    patch.set_transform(transform)\n    assert patch.get_transform() == transform\n"], "sample_519": ["def test_subfigure_repr():\n    fig = Figure()\n    subfig = fig.subfigures(1, 1)\n    assert repr(subfig[0]) == \"<SubFigure size 640x480 with 0 Axes>\"\n", "def test_pickle_layout_engine():\n    fig = Figure(layout='constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert isinstance(fig2.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig = Figure(layout='tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert isinstance(fig2.get_layout_engine(), TightLayoutEngine)\n", "def test_pickle_layout_engine():\n    fig = Figure(layout='constrained')\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert isinstance(fig2.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig = Figure(layout='tight')\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert isinstance(fig2.get_layout_engine(), TightLayoutEngine)\n", "def test_draw_artist():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([0, 1])\n    line2, = ax.plot([1, 0], color='r')\n    fig.draw_artist(ax.patch)\n    fig.draw_artist(line1)\n    assert line1.stale == False\n    assert line2.stale == True\n", "def test_supxlabel():\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.plot([0, 1])\n    fig.supxlabel('supxlabel')\n"], "sample_520": ["def test_line_collection_2d_to_3d(fig_test, fig_ref):\n    x = np.linspace(0, 1, 10)\n    y = np.sin(x)\n    z = 0.5\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    lc = LineCollection([np.column_stack((x, y))], colors='C0')\n    lc.set_zorder(1)\n    art3d.line_collection_2d_to_3d(lc, z)\n    ax_ref.add_collection3d(lc)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.plot(x, y, np.full_like(x, z), 'C0', zorder=1)\n", "def test_line3d_modification(fig_test, fig_ref):\n    # Test that modifying Line3D properties after creation works.\n    x, y, z = [0, 1], [2, 3], [4, 5]\n    ax_test = fig_test.add_subplot(projection='3d')\n    line = ax_test.plot(x, y, z)[0]\n    line.set_color('C2')\n    line.set_linewidth(3)\n    line.set_alpha(0.7)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.plot(x, y, z, color='C2', lw=3, alpha=0.7)\n", "def test_dir_vector():\n    # Test that get_dir_vector returns the correct direction vector.\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array((1, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array((0, 1, 0)))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array((0, 0, 1)))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array((0, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array((1, 2, 3)))\n\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('a')\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector((1, 2))\n", "def test_patch_collection_alpha():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    patches = [Circle((0, 0), 1), Circle((2, 2), 1)]\n    collection = PatchCollection(patches, alpha=0.5)\n    art3d.patch_collection_2d_to_3d(collection, z=0, zdir='z')\n    ax.add_collection(collection)\n    fig.canvas.draw()\n    assert collection.get_alpha() == 0.5\n", "def test_text3d_position_modification(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot(projection='3d')\n    t = art3d.Text3D(0.5, 0.5, 1, 'Hello World!')\n    ax_test.add_artist(t)\n    t.set_position_3d((0.7, 0.7, 1))\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    t = art3d.Text3D(0.7, 0.7, 1, 'Hello World!')\n    ax_ref.add_artist(t)\n"], "sample_521": ["def test_line3d_collection_modification(fig_test, fig_ref):\n    # Test that modifying Line3DCollection properties after creation works.\n    fig_test = plt.figure()\n    ax_test = fig_test.add_subplot(projection='3d')\n    line1 = art3d.Line3D([0, 1], [0, 1], [0, 1])\n    line2 = art3d.Line3D([1, 2], [1, 2], [1, 2])\n    colors = np.array([[0., 0.5, 0., 1.], [0.5, 0., 0., 0.5]])\n    c = art3d.Line3DCollection([line1, line2], linewidths=3,\n                               colors=colors)\n\n    ax_test.add_collection3d(c)\n    c.set_edgecolor('C2')\n    c.set_facecolor(colors)\n    c.set_alpha(0.7)\n    assert c.get_depthshade()\n    c.set_depthshade(False)\n    assert not c.get_depthshade()\n\n    fig_ref = plt.figure()\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    line1 = art3d.Line3D([0, 1], [0, 1], [0, 1])\n    line2 = art3d.Line3D([1, 2], [1, 2], [1, 2])\n    colors = np.array([[0., 0.5, 0., 1.], [0.5, 0., 0., 0.5]])\n    c = art3d.Line3DCollection([line1, line2], linewidths=3,\n                               edgecolor='C2', facecolor=colors,\n                               alpha=0.7, depthshade=False)\n    ax_ref.add_collection3d(c)\n", "def test_line_collection_2d_to_3d(fig_test, fig_ref):\n    x = np.linspace(-1, 1, 100)\n    y = np.sin(x)\n    z = np.cos(x)\n\n    lc = LineCollection([np.column_stack([x, y])])\n    art3d.line_collection_2d_to_3d(lc, zs=z, zdir='y')\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(lc)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.plot(x, z, y)\n", "def test_text3d_positioning():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    txt = art3d.Text3D(0.5, 0.5, 1, 'Test text')\n    ax.add_artist(txt)\n    assert txt.get_position_3d() == (0.5, 0.5, 1)\n\n    # Test that setting position works\n    txt.set_position_3d((1, 2, 3))\n    assert txt.get_position_3d() == (1, 2, 3)\n\n    # Test that setting z position separately works\n    txt.set_z(4)\n    assert txt.get_position_3d() == (1, 2, 4)\n", "def test_linecollection_2d_to_3d(fig_test, fig_ref):\n    # Create a figure with a single axis in 3D\n    ax = fig_test.add_subplot(projection='3d')\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n\n    # Create some 2D data for a LineCollection\n    x = np.array([0.5, 0.6, 0.7])\n    y = np.array([0.5, 0.6, 0.7])\n    line = [(x[0], y[0]), (x[1], y[1])]\n    line2 = [(x[0], y[0]), (x[2], y[2])]\n\n    # Create the LineCollection and add it to the axis\n    lc = LineCollection([line, line2], colors=['r', 'b'])\n    art3d.line_collection_2d_to_3d(lc, zs=0.5)\n    ax.add_collection3d(lc)\n\n    # Reference figure\n    ax = fig_ref.add_subplot(projection='3d')\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n    ax.plot3D(x[:2], y[:2], [0.5, 0.5], c='r')\n    ax.plot3D([x[0], x[2]], [y[0], y[2]], [0.5, 0.5], c='b')\n", "def test_line3d_collection_modification(fig_test, fig_ref):\n    # Test that modifying Line3DCollection properties after creation works.\n    xs = np.array([0, 1])\n    ys = np.array([2, 3])\n    zs = np.array([4, 5])\n    segments = [(xs, ys, zs)]\n    colors = np.array([[0., 0.5, 0., 1.]])\n\n    lc_test = art3d.Line3DCollection(segments, colors=colors, linewidths=3)\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(lc_test)\n    lc_test.set_edgecolor('C2')\n    lc_test.set_linewidths(5)\n    assert lc_test.get_depthshade()\n    lc_test.set_depthshade(False)\n    assert not lc_test.get_depthshade()\n\n    lc_ref = art3d.Line3DCollection(segments, edgecolor='C2', linewidths=5,\n                                    depthshade=False)\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.add_collection3d(lc_ref)\n"], "sample_522": ["def test_colorbar_with_unevenly_spaced_boundaries():\n    fig, ax = plt.subplots(figsize=(2, 2))\n    data = np.random.randn(10, 10)\n    cmap = mpl.colormaps[\"viridis\"].resampled(7)\n    norm = BoundaryNorm([data.min(), -1.5, -0.5, 0.0, 0.5, 1.5, data.max()],\n                        cmap.N)\n    pc = ax.pcolormesh(data, cmap=cmap, norm=norm)\n    cb = fig.colorbar(pc, ax=ax)\n", "def test_colorbar_set_ticks_with_minorlocator():\n    # check that setting ticks with minorlocator specified will not raise an error\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.ax.yaxis.set_minor_locator(FixedLocator(np.arange(0, 10, 0.2)))\n    cb.set_ticks(np.arange(10))\n", "def test_colorbar_update_ticks():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_ticks([0, 5, 10])\n    np.testing.assert_array_equal(cb.ax.yaxis.get_ticklocs(), [0, 5, 10])\n    cb.update_ticks()\n    np.testing.assert_array_equal(cb.ax.yaxis.get_ticklocs(), [0, 5, 10])\n    cb.set_ticks(None)\n    assert len(cb.ax.yaxis.get_ticklocs()) > 3\n    cb.update_ticks()\n    assert len(cb.ax.yaxis.get_ticklocs()) > 3\n", "def test_colorbar_with_no_norm_or_cmap():\n    # Test that a colorbar can be created without a norm or cmap.\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable()\n    with pytest.raises(ValueError):\n        fig.colorbar(sm, cax=ax)\n", "def test_set_in_layout():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cb = fig.colorbar(im)\n    assert cb.ax.in_layout\n    cb.ax.set_in_layout(False)\n    assert not cb.ax.in_layout\n    cb.ax.set_in_layout(True)\n    assert cb.ax.in_layout\n"], "sample_523": ["def test_legend_contains():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n\n    event = mock.Mock()\n    event.xdata = 0.5\n    event.ydata = 0.5\n    event.inaxes = ax\n\n    contains, info = leg.contains(event)\n    assert contains is not None\n", "def test_legend_bbox_transform():\n    # Test that legend's bbox_to_anchor argument accepts a transform\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    bbox = mtransforms.Bbox.from_bounds(0.5, 0.5, 0.2, 0.2)\n    leg = ax.legend(bbox_to_anchor=bbox, loc='center', bbox_transform=ax.transAxes)\n    assert leg.get_bbox_to_anchor() == bbox\n    assert leg._bbox_to_anchor.transform == ax.transAxes\n", "def test_legend_handler_map_update():\n    # Test that updating the legend handler map updates the default handler map.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    mlegend.Legend.update_default_handler_map({mlines.Line2D: mlegend_handler.HandlerLine2D()})\n    leg = ax.legend()\n    assert isinstance(leg.get_legend_handler_map()[mlines.Line2D], mlegend_handler.HandlerLine2D)\n", "def test_legend_get_texts():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n    texts = leg.get_texts()\n    assert len(texts) == 1\n    assert texts[0].get_text() == 'line'\n", "def test_legend_with_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n"], "sample_524": ["def test_colorbar_no_warning_rcparams_grid_true_with_subplots():\n    # github issue #21723 - If mpl style has 'axes.grid' = True,\n    # fig.colorbar raises a warning about Auto-removal of grids\n    # by pcolor() and pcolormesh(). This is fixed by PR #22216.\n    plt.rcParams['axes.grid'] = True\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.grid(False)\n        im = ax.pcolormesh([0, 1], [0, 1], [[1]])\n        fig.colorbar(im, ax=ax)\n", "def test_figure_repr():\n    fig = plt.figure()\n    assert \"Figure\" in repr(fig)\n    assert \"size\" in repr(fig)\n    assert \"Axes\" in repr(fig)\n", "def test_colorbar_with_subplots():\n    fig, axs = plt.subplots(1, 2)\n    pc = axs[0].pcolormesh(np.random.randn(10, 10))\n    fig.colorbar(pc, ax=axs[0])\n    pc = axs[1].pcolormesh(np.random.randn(10, 10))\n    fig.colorbar(pc, ax=axs[1])\n", "def test_colorbar_empty_array():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.empty((0, 0)))\n    with pytest.warns(UserWarning, match=\"No data for colormapping\"):\n        fig.colorbar(pc)\n", "def test_colorbar_orientation():\n    fig, axs = plt.subplots(2, 2)\n    pc = axs[0, 0].pcolormesh(np.random.randn(10, 10))\n    fig.colorbar(pc, ax=axs[0, 0], orientation='horizontal')\n    pc = axs[0, 1].pcolormesh(np.random.randn(10, 10))\n    fig.colorbar(pc, ax=axs[0, 1], orientation='vertical')\n    pc = axs[1, 0].pcolormesh(np.random.randn(10, 10))\n    fig.colorbar(pc, ax=axs[1, 0], orientation='horizontal', pad=0.1)\n    pc = axs[1, 1].pcolormesh(np.random.randn(10, 10))\n    fig.colorbar(pc, ax=axs[1, 1], orientation='vertical', pad=0.1)\n"], "sample_525": ["def test_subfigure_label():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1)\n    sub_fig.set_label('sub figure label')\n    assert sub_fig.get_label() == 'sub figure label'\n    fig.set_label('figure label')\n    assert fig.get_label() == 'figure label'\n", "def test_subfigure_repr():\n    fig = Figure()\n    subfig = fig.subfigures(1, 1)[0]\n    assert repr(subfig) == f\"<{type(subfig).__name__} size {subfig.get_size_inches()[0]:g}x{subfig.get_size_inches()[1]:g} with 0 Axes>\"\n", "def test_set_tight_layout():\n    # Test that setting tight_layout still works\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3])\n    fig.set_tight_layout(True)\n    assert fig.get_tight_layout()\n    fig.set_tight_layout(False)\n    assert not fig.get_tight_layout()\n", "def test_supxlabel_ylabel_title():\n    fig, ax = plt.subplots()\n    fig.supxlabel(\"xlabel\")\n    fig.supylabel(\"ylabel\")\n    fig.suptitle(\"title\")\n\n    assert fig._supxlabel.get_text() == \"xlabel\"\n    assert fig._supylabel.get_text() == \"ylabel\"\n    assert fig._suptitle.get_text() == \"title\"\n\n    fig.supxlabel(\"new_xlabel\")\n    fig.supylabel(\"new_ylabel\")\n    fig.suptitle(\"new_title\")\n\n    assert fig._supxlabel.get_text() == \"new_xlabel\"\n    assert fig._supylabel.get_text() == \"new_ylabel\"\n    assert fig._suptitle.get_text() == \"new_title\"\n\n    # test that removing them works...\n    fig.supxlabel(None)\n    fig.supylabel(None)\n    fig.suptitle(None)\n\n    assert fig._supxlabel is None\n    assert fig._supylabel is None\n    assert fig._suptitle is None\n\n    # and that setting to none doesn't raise...\n    fig.supxlabel(None)\n    fig.supylabel(None)\n    fig.suptitle(None)\n", "def test_subfigure_repr():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1)\n    assert repr(sub_fig[0]).startswith(\"<SubFigure\")\n"], "sample_526": ["def test_date2num_vectorized():\n    # Test that date2num can handle vectorized inputs.\n    dates = np.array([\n        datetime.datetime(2022, 1, 1),\n        datetime.datetime(2022, 1, 2),\n        datetime.datetime(2022, 1, 3)\n    ])\n    expected = np.array([18993., 18994., 18995.])\n    assert np.allclose(mdates.date2num(dates), expected)\n\n    # Test that it can also handle numpy arrays of datetime64 objects.\n    dates = np.array([\n        np.datetime64('2022-01-01'),\n        np.datetime64('2022-01-02'),\n        np.datetime64('2022-01-03')\n    ])\n    assert np.allclose(mdates.date2num(dates), expected)\n\n    # Test that it can handle mixed input types.\n    dates = np.array([\n        datetime.datetime(2022, 1, 1),\n        np.datetime64('2022-01-02'),\n        datetime.date(2022, 1, 3)\n    ])\n    assert np.allclose(mdates.date2num(dates), expected)\n", "def test_DateFormatter_tzinfo():\n    time = mdates.date2num(datetime.datetime(2011, 1, 1, 0, 0,\n                                             tzinfo=mdates.UTC))\n    formatter = mdates.DateFormatter('%Y-%b-%d %H:%M', tz=mdates.UTC)\n    assert formatter(time) == '2011-Jan-01 00:00'\n\n    # Set tzinfo to Pacific/Kiritimati\n    formatter = mdates.DateFormatter('%Y-%b-%d %H:%M', tz='Pacific/Kiritimati')\n    assert formatter(time) == '2011-Jan-01 14:00'\n", "def test_MicrosecondLocator():\n    fig, ax = plt.subplots()\n    locator = mdates.MicrosecondLocator(interval=1000)\n    ax.xaxis.set_major_locator(locator)\n    ax.plot([datetime.datetime(2022, 1, 1, 0, 0, 0, 1000),\n             datetime.datetime(2022, 1, 1, 0, 0, 0, 2000)], [0, 1])\n    fig.canvas.draw()\n    ticks = [t.get_text() for t in ax.get_xticklabels()]\n    assert len(ticks) == 2\n    assert ticks[0] == '2022-01-01 00:00:00.001000'\n    assert ticks[1] == '2022-01-01 00:00:00.002000'\n", "def test_num2date_list():\n    dates = [datetime.datetime(2022, 1, 1), datetime.datetime(2022, 1, 2)]\n    nums = mdates.date2num(dates)\n    converted_dates = mdates.num2date(nums)\n    assert converted_dates == dates\n", "def test_date2num_roundtrip():\n    # Test that date2num and num2date are inverses of each other.\n    dates = [datetime.datetime(2022, 1, 1),\n             datetime.datetime(2022, 6, 15, 12, 30, 45),\n             datetime.datetime(2022, 12, 31, 23, 59, 59)]\n    nums = mdates.date2num(dates)\n    roundtripped_dates = mdates.num2date(nums)\n    for original, roundtripped in zip(dates, roundtripped_dates):\n        assert original == roundtripped\n"], "sample_527": ["def test_subfigure_add_subplot():\n    fig = plt.figure()\n    subfigs = fig.subfigures(1, 2)\n    axs = subfigs[0].subplots(2, 2)\n    assert len(axs) == 4\n    for ax in axs.flat:\n        assert ax.get_figure() is fig\n    ax sf = subfigs[0].add_subplot(111)\n    assert sf.get_figure() is fig\n    with pytest.raises(ValueError):\n        subfigs[0].add_subplot(axs[0, 0])\n", "def test_figure_show_warns():\n    fig = Figure()\n    with pytest.warns(UserWarning, match=\"Matplotlib is currently using\"):\n        fig.show(warn=True)\n", "def test_subplots_adjust():\n    fig, axs = plt.subplots(2, 2)\n    old_positions = [ax.get_position() for ax in axs.flat]\n    fig.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8,\n                        wspace=0.5, hspace=0.5)\n    new_positions = [ax.get_position() for ax in axs.flat]\n    assert all(o != n for o, n in zip(old_positions, new_positions))\n", "def test_subplots_adjust():\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(wspace=0.5)\n    assert fig.subplotpars.wspace == 0.5\n    fig.subplots_adjust(hspace=0.3)\n    assert fig.subplotpars.hspace == 0.3\n\n    # Test that subplots_adjust can be called with only one argument.\n    fig.subplots_adjust(left=0.1)\n    assert fig.subplotpars.left == 0.1\n    fig.subplots_adjust(right=0.9)\n    assert fig.subplotpars.right == 0.9\n    fig.subplots_adjust(top=0.8)\n    assert fig.subplotpars.top == 0.8\n    fig.subplots_adjust(bottom=0.2)\n    assert fig.subplotpars.bottom == 0.2\n\n    # Test that subplots_adjust can be called with multiple arguments.\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.8, bottom=0.2)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.8\n    assert fig.subplotpars.bottom == 0.2\n\n    # Test that subplots_adjust can be called with no arguments.\n    fig.subplots_adjust()\n    # Should not raise any errors.\n", "def test_figure_show():\n    fig = plt.figure()\n    with pytest.raises(AttributeError):\n        fig.show()\n\n    # make sure that figure can't be shown if it's not managed by a\n    # figure manager\n    from matplotlib._pylab_helpers import Gcf\n    assert len(Gcf.figs) == 0\n\n    fig.canvas.manager = None\n    with pytest.raises(AttributeError):\n        fig.show()\n"], "sample_528": ["def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library_with_invalid_file(caplog):\n    with temp_style('invalid', {' invalid': 'setting'}):\n        style.reload_library()\n    assert len(caplog.records) == 1\n    assert \"In\" in caplog.records[0].getMessage()\n    assert \"invalid\" in caplog.records[0].getMessage()\n", "def test_reload_library_with_empty_user_library():\n    original_library = style.library.copy()\n    USER_LIBRARY_PATHS.clear()\n    style.reload_library()\n    assert style.library == original_library\n    USER_LIBRARY_PATHS.append('non_existent_path')\n    style.reload_library()\n    assert style.library == original_library\n"], "sample_529": ["def test_legend_with_transform():\n    # Test legend with a custom transform.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    ax.set_xlim(0, 5)\n    ax.set_ylim(0, 5)\n    trans = mtransforms.Affine2D().translate(0.5, 0.5) + ax.transAxes\n    leg = ax.legend(loc='upper left', bbox_to_anchor=(0, 0), bbox_transform=trans)\n    fig.canvas.draw()\n    assert leg.get_window_extent().x0 > 0\n    assert leg.get_window_extent().y0 > 0\n", "def test_legend_get_children():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([0, 1], [0, 1], label='line1')\n    line2, = ax.plot([0, 1], [1, 0], label='line2')\n    leg = ax.legend()\n    children = leg.get_children()\n    assert len(children) == 2\n    assert children[0] is leg.get_frame()\n    assert children[1] is leg._legend_box\n", "def test_legend_bbox_to_anchor():\n    # Test that bbox_to_anchor is correctly set for the legend\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5))\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5, 0.2, 0.2))\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n    assert leg.get_bbox_to_anchor().width == 0.2\n    assert leg.get_bbox_to_anchor().height == 0.2\n\n    leg = ax.legend(bbox_to_anchor=[0.5, 0.5])\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n", "def test_legend_bbox_to_anchor_transform():\n    # Test that the bbox_to_anchor argument can be specified with a transform.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5), loc='center',\n                    bbox_transform=fig.transFigure)\n    assert leg.get_bbox_to_anchor().transform == fig.transFigure\n", "def test_legend_with_empty_string_label():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='')\n    leg = ax.legend()\n    assert len(leg.get_texts()) == 0\n"], "sample_530": ["def test_anchoredoffsetbox_locations(loc):\n    fig, ax = plt.subplots()\n    anchored_box = AnchoredOffsetbox(\n        loc=loc,\n        child=DrawingArea(10, 10),\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.\n    )\n    ax.add_artist(anchored_box)\n    fig.canvas.draw()\n    assert not fig.stale\n", "def test_offsetbox_repr():\n    # Test that the repr of OffsetBox classes are reasonable and do not error.\n    fig, ax = plt.subplots()\n    da = DrawingArea(10, 20)\n    hp = HPacker(children=[da])\n    vp = VPacker(children=[da])\n    ap = AnchoredOffsetbox(loc='upper left', child=da)\n\n    for ob in [da, hp, vp, ap]:\n        repr(ob)  # Check that this does not error.\n", "def test_offsetbox_repr():\n    ob = OffsetBox()\n    assert repr(ob) == \"OffsetBox()\"\n", "def test_offsetbox_stale():\n    # Test that changing the offset box's child or offset makes it stale.\n    fig, ax = plt.subplots()\n    child = TextArea(\"Text\")\n    box = AnchoredOffsetbox(loc=\"upper left\", child=child)\n\n    assert not box.stale\n\n    box.set_child(TextArea(\"New text\"))\n    assert box.stale\n\n    box.stale = False\n    box.set_offset((10, 20))\n    assert box.stale\n", "def test_textarea_multilinebaseline():\n    fig, ax = plt.subplots(figsize=(4, 3), dpi=100)\n    ta1 = TextArea(\"Single line of text\", multilinebaseline=True)\n    ta2 = TextArea(\"Two lines of\\ntext\", multilinebaseline=True)\n    ta3 = TextArea(\"Three lines\\nof\\ntext\", multilinebaseline=True)\n\n    ab1 = AnnotationBbox(ta1, (.5, .9), xycoords='data', boxcoords='data')\n    ab2 = AnnotationBbox(ta2, (.5, .7), xycoords='data', boxcoords='data')\n    ab3 = AnnotationBbox(ta3, (.5, .5), xycoords='data', boxcoords='data')\n\n    ax.add_artist(ab1)\n    ax.add_artist(ab2)\n    ax.add_artist(ab3)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    bb1 = ta1.get_window_extent(renderer)\n    bb2 = ta2.get_window_extent(renderer)\n    bb3 = ta3.get_window_extent(renderer)\n\n    assert_allclose(bb1.height, bb2.height / 2, atol=2)\n    assert_allclose(bb1.height, bb3.height / 3, atol=2)\n"], "sample_531": ["def test_savefig_bbox_extra_artists():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    text = fig.text(0.5, 0.5, 'extra artist', ha='center')\n    with io.BytesIO() as buf:\n        fig.savefig(buf, format='png', bbox_inches='tight',\n                    bbox_extra_artists=[text])\n", "def test_get_default_bbox_extra_artists():\n    fig, ax = plt.subplots()\n    assert fig.get_default_bbox_extra_artists() == [ax]\n    ax.set_in_layout(False)\n    assert fig.get_default_bbox_extra_artists() == []\n    ax.set_in_layout(True)\n\n    fig2 = plt.figure()\n    fig2.add_artist(plt.Line2D([0], [0]))\n    assert len(fig2.get_default_bbox_extra_artists()) == 1\n\n    fig3, ax3 = plt.subplots(constrained_layout=True)\n    ax3.set_in_layout(False)\n    assert ax3 not in fig3.get_default_bbox_extra_artists()\n\n    ax4 = fig3.add_axes([0.1, 0.1, 0.8, 0.8])\n    ax4.set_in_layout(False)\n    assert ax4 not in fig3.get_default_bbox_extra_artists()\n", "def test_figure_savefig_kwargs():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    with io.BytesIO() as buf:\n        fig.savefig(buf, format='png', dpi=100)\n        assert buf.getvalue() is not None\n\n    # Check that we can pass a file-like object to savefig.\n    with open('test.png', 'wb') as f:\n        fig.savefig(f, format='png', dpi=100)\n\n    # Check that we can pass a PathLike object to savefig.\n    fig.savefig(Path('test.png'), format='png', dpi=100)\n\n    # Check that passing a string to savefig does not raise an error.\n    fig.savefig('test.png', format='png', dpi=100)\n", "def test_supxlabel_ylabel(fig_test, fig_ref):\n    fig_test.supxlabel(\"X Label\")\n    fig_test.supylabel(\"Y Label\")\n\n    ax = fig_ref.add_subplot(111)\n    ax.set_xlabel(\"X Label\")\n    ax.set_ylabel(\"Y Label\")\n    ax.xaxis.tick_top()\n    ax.yaxis.tick_right()\n", "def test_layout_engine_compatibility():\n    # Test that we get a warning when switching between incompatible layout engines.\n    fig = plt.figure()\n    fig.set_layout_engine('tight')\n    with pytest.warns(UserWarning, match='not compatible'):\n        fig.set_layout_engine('constrained')\n"], "sample_532": ["def test_contourf_lognormextend():\n    fig, ax = plt.subplots()\n    x = np.linspace(-3.0, 3.0, 100)\n    y = np.linspace(-2.0, 2.0, 100)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-(X**2 + Y**2) / 2) / (2 * np.pi)\n    Z2 = (np.exp(-(((X - 1) / 1.5)**2 + ((Y - 1) / 0.5)**2) / 2) /\n          (2 * np.pi * 0.5 * 1.5))\n\n    Z = 10.0 * (Z2 - Z1)\n\n    pcm = ax.contourf(X, Y, Z, norm=LogNorm(), extend='both')\n    assert pcm.norm.vmin == Z.min()\n    assert pcm.norm.vmax == Z.max()\n\n    pcm = ax.contourf(X, Y, Z, norm=LogNorm(vmin=0.01, vmax=10), extend='both')\n    assert pcm.norm.vmin == 0.01\n    assert pcm.norm.vmax == 10\n\n    pcm = ax.contourf(X, Y, Z, levels=[0.01, 0.1, 1, 10], norm=LogNorm(), extend='both')\n    assert pcm.norm.vmin == 0.01\n    assert pcm.norm.vmax == 10\n", "def test_contour_zorder():\n    fig, ax = plt.subplots()\n    z = np.arange(9).reshape((3, 3))\n    cs1 = ax.contour(z, levels=[3, 6], zorder=10)\n    cs2 = ax.contour(z, levels=[4, 7], zorder=5)\n    assert cs1.collections[0].get_zorder() == 10\n    assert cs2.collections[0].get_zorder() == 5\n", "def test_contourf_hatches():\n    # Test that hatching works for filled contours.\n    fig, ax = plt.subplots()\n    data = np.arange(12).reshape((3, 4))\n    cs = ax.contourf(data, levels=[2, 4, 6, 10, 20], hatches=['/', '\\\\', 'x', '-'])\n    assert len(cs.collections) == 4\n    for collection, hatch in zip(cs.collections, ['/', '\\\\', 'x', '-']):\n        assert collection.get_hatch() == hatch\n", "def test_contourf_empty():\n    fig, ax = plt.subplots()\n    z = np.array([[1.0, 2.0], [3.0, 4.0]])\n    cs = ax.contourf(z, levels=[5.0, 6.0])\n    assert len(cs.collections) == 1\n    assert len(cs.collections[0].get_paths()) == 0\n    assert len(ax.get_children()) == 2  # QuadContourSet and AxesImage are present\n", "def test_contour_label_text():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    clabels = cs.clabel()\n    for text in clabels:\n        assert isinstance(text, mpl.text.Text)\n        assert text.get_text() != \"\"\n"], "sample_533": ["def test_contourf_drawedges():\n    # Test drawedges parameter for contourf function\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    ax.contourf(x, y, z, drawedges=True)\n    assert len(ax.collections) == 11  # One for each level and one for edges\n", "def test_contour_zorder():\n    ax = plt.figure().add_subplot()\n    cs1 = ax.contour(np.arange(16).reshape((4, 4)), zorder=2)\n    cs2 = ax.contour(np.arange(16).reshape((4, 4)), zorder=1)\n    assert cs1.collections[0].get_zorder() == 2\n    assert cs2.collections[0].get_zorder() == 1\n", "def test_contour_label_inline_spacing():\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    cs = ax.contour(x, y, z)\n    label = cs.clabel(cs.levels[0], inline=True, inline_spacing=5)[0]\n    assert label.get_text() == f'{cs.levels[0]:.6g}'\n    # Check the path of the contour is broken at the correct points\n    vertices = cs.collections[0].get_paths()[0].vertices\n    dist = np.linalg.norm(vertices - vertices[0], axis=1)\n    mask = dist < 5\n    assert np.any(mask)  # at least one point within inline_spacing of label\n    assert not np.all(mask)  # but not all points are within inline_spacing\n", "def test_contourf_log_scale():\n    x = np.linspace(0, 10, 100)\n    y = np.linspace(0, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.exp(-(X-5)**2 - (Y-5)**2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(X, Y, Z, levels=np.logspace(-1, 2, 10), norm=LogNorm())\n    assert_array_almost_equal(cs.levels, np.logspace(-1, 2, 10))\n    cbar = fig.colorbar(cs)\n    assert cbar.norm.type == 'log'\n", "def test_contour_add_label_near():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    ax.clabel(cs, manual=False)\n\n    # Add a label near a specific point.\n    cs.add_label_near(5.0, 5.0, inline=True)\n    assert len(cs.labelTexts) > 0\n"], "sample_534": ["def test_contour_zero_level():\n    x = np.linspace(-1, 1, 10)\n    y = np.linspace(-1, 1, 10)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z, levels=[0])\n    ax.clabel(cs, inline=True, fontsize=10)\n    ax.set_title(\"Zero level contour\")\n", "def test_contourf_alpha():\n    fig, ax = plt.subplots()\n    z = np.random.rand(10, 10)\n    cs = ax.contourf(z, alpha=0.5)\n    assert cs.alpha == 0.5\n    for collection in cs.collections:\n        assert collection.get_alpha() == 0.5\n", "def test_contour_label_rotation():\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    cs = ax.contour(x, y, z)\n    for label in cs.labelTexts:\n        rotation = label.get_rotation()\n        assert isinstance(rotation, (int, float))\n", "def test_given_colors_levels_and_extends_with_hatches():\n    # Remove this line when this test image is regenerated.\n    plt.rcParams['pcolormesh.snap'] = False\n\n    _, axs = plt.subplots(2, 4)\n\n    data = np.arange(12).reshape(3, 4)\n\n    colors = ['red', 'yellow', 'pink', 'blue', 'black']\n    levels = [2, 4, 8, 10]\n\n    hatches = [None, '/', '\\\\', '//']\n\n    for i, ax in enumerate(axs.flat):\n        filled = i % 2 == 0.\n        extend = ['neither', 'min', 'max', 'both'][i // 2]\n\n        if filled:\n            # If filled, we have 3 colors with no extension,\n            # 4 colors with one extension, and 5 colors with both extensions\n            first_color = 1 if extend in ['max', 'neither'] else None\n            last_color = -1 if extend in ['min', 'neither'] else None\n            c = ax.contourf(data, colors=colors[first_color:last_color],\n                            levels=levels, extend=extend, hatches=hatches)\n        else:\n            # If not filled, we have 4 levels and 4 colors\n            c = ax.contour(data, colors=colors[:-1],\n                           levels=levels, extend=extend)\n\n        plt.colorbar(c, ax=ax)\n", "def test_contour_corner_mask():\n    x, y = np.meshgrid(np.linspace(0, 1, 10), np.linspace(0, 1, 10))\n    z = np.sin(x)*np.cos(y)\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(x, y, z, corner_mask=True)\n    assert cs._corner_mask is True\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(x, y, z, corner_mask=False)\n    assert cs._corner_mask is False\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(x, y, z)\n    assert cs._corner_mask == mpl.rcParams['contour.corner_mask']\n"], "sample_535": ["def test_table_visible_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    cell = table.add_cell(1, 2, 1, 1)\n\n    # Check default value\n    assert cell.visible_edges == 'closed'\n\n    # Check setting with a substring of 'BRTL'\n    cell.visible_edges = 'RT'\n    assert cell.visible_edges == 'RT'\n\n    # Check setting with an alias\n    cell.visible_edges = 'horizontal'\n    assert cell.visible_edges == 'BT'\n\n    # Check invalid value\n    with pytest.raises(ValueError):\n        cell.visible_edges = ' invalid'\n\n    # Check that the edge is correctly updated in the path\n    codes = tuple(s for _, s in cell.get_path().iter_segments())\n    assert Path.MOVETO in codes\n    assert Path.LINETO in codes\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    table.edges = 'horizontal'\n    assert table.edges == 'BT'\n\n    table.edges = 'vertical'\n    assert table.edges == 'RL'\n\n    table.edges = 'open'\n    assert table.edges == ''\n\n    table.edges = 'closed'\n    assert table.edges == 'BRTL'\n\n    table.edges = None\n    assert table.edges == 'BRTL'\n\n    with pytest.raises(ValueError):\n        table.edges = ' invalid'\n", "def test_table_get_celld():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    cell1 = table.add_cell(1, 2, 1, 1)\n    cell2 = table.add_cell(2, 1, 1, 2)\n\n    celld = table.get_celld()\n    assert isinstance(celld, dict)\n    assert (1, 2) in celld\n    assert (2, 1) in celld\n    assert celld[(1, 2)] is cell1\n    assert celld[(2, 1)] is cell2\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    # Test setting edges for a cell\n    cell = table.add_cell(1, 2, 1, 1)\n    cell.visible_edges = 'horizontal'\n    assert cell.get_path().codes.tolist() == [\n        Path.MOVETO, Path.LINETO, Path.MOVETO, Path.MOVETO, Path.LINETO]\n\n    # Test setting edges for the entire table\n    table.edges = 'closed'\n    assert table._edges == 'BRTL'\n\n    # Test invalid edge value\n    with pytest.raises(ValueError):\n        cell.visible_edges = 'invalid'\n\n    # Test that auto font size does not affect cell edges\n    table.auto_set_font_size(True)\n    assert cell.get_path().codes.tolist() == [\n        Path.MOVETO, Path.LINETO, Path.MOVETO, Path.MOVETO, Path.LINETO]\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    ax.axis('off')\n\n    table = Table(ax)\n\n    for i in range(3):\n        for j in range(3):\n            table.add_cell(i, j, 1, 1, edges='open')\n\n    table[0, 0].visible_edges = 'B'\n    table[0, 1].visible_edges = 'T'\n    table[0, 2].visible_edges = 'L'\n    table[1, 0].visible_edges = 'R'\n    table[1, 1].visible_edges = 'horizontal'\n    table[1, 2].visible_edges = 'vertical'\n    table[2, 0].visible_edges = 'closed'\n    table[2, 1].visible_edges = 'open'\n    table[2, 2].visible_edges = 'none'\n\n    ax.add_table(table)\n"], "sample_537": ["def test_csd_onesided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    v = np.array([0, 2, 4, 6, 2, 4, 2])\n    dt = 1.0\n    Su = np.fft.fft(u) * dt\n    Sv = np.fft.fft(v) * dt\n    Csuv = (Su * np.conj(Sv)) / (dt * u.size)\n    C, f = mlab.csd(u, v, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='onesided')\n    Csuv_1side = np.append([Csuv[0]], Csuv[1:4] + Csuv[4:][::-1])\n    assert_allclose(C, Csuv_1side, atol=1e-06)\n", "def test_detrend_linear_axis(self):\n    x = np.linspace(0, 10, 20).reshape((4, 5))\n    y = mlab.detrend_linear(x, axis=0)\n    assert_array_almost_equal(y.mean(axis=0), np.zeros(5))\n    y = mlab.detrend_linear(x, axis=1)\n    assert_array_almost_equal(y.mean(axis=1), np.zeros(4))\n\n    with pytest.raises(ValueError):\n        mlab.detrend_linear(x, axis=2)\n\n    x = np.linspace(0, 10, 20)\n    y = mlab.detrend_linear(x)\n    assert_array_almost_equal(y.mean(), 0)\n", "def test_window_hanning():\n    \"\"\"Test window_hanning function.\"\"\"\n    x = np.arange(10)\n    y = mlab.window_hanning(x)\n    expected = np.hanning(len(x)) * x\n    assert_array_equal(y, expected)\n\n    # Test 0D input\n    x = 5.5\n    with pytest.raises(TypeError):\n        mlab.window_hanning(x)\n", "def test_psd_csd_with_zero_length_input():\n    \"\"\"Test that psd and csd can handle zero-length input arrays.\"\"\"\n    x = np.array([])\n    y = np.array([])\n\n    Pxx, freqs = mlab.psd(x)\n    assert_array_equal(Pxx, np.array([]))\n    assert_array_equal(freqs, np.array([]))\n\n    Pxy, freqs = mlab.csd(x, y)\n    assert_array_equal(Pxy, np.array([]))\n    assert_array_equal(freqs, np.array([]))\n", "def test_psd_csd_window_normalization(self):\n    freqs = self.freqs_density\n    spec, fsp = mlab.psd(x=self.y,\n                         NFFT=self.NFFT_density,\n                         Fs=self.Fs,\n                         noverlap=self.nover_density,\n                         pad_to=self.pad_to_density,\n                         sides=self.sides,\n                         window=mlab.window_hanning)\n    spec2, fsp2 = mlab.csd(x=self.y, y=self.y,\n                           NFFT=self.NFFT_density,\n                           Fs=self.Fs,\n                           noverlap=self.nover_density,\n                           pad_to=self.pad_to_density,\n                           sides=self.sides,\n                           window=mlab.window_hanning)\n    assert_allclose(spec, np.real(spec2), atol=1e-06)\n    assert_array_equal(fsp, fsp2)\n"], "sample_538": ["def test_blended_transform():\n    # Create a blended transform with different x and y transforms\n    x_transform = mtransforms.Affine2D().scale(2, 1)\n    y_transform = mtransforms.Affine2D().scale(1, 3)\n    blended_transform = mtransforms.blended_transform_factory(x_transform, y_transform)\n\n    # Test the blended transform on some points\n    points = np.array([[1, 2], [3, 4]])\n    transformed_points = blended_transform.transform(points)\n\n    # Check that the x and y coordinates were transformed correctly\n    assert_array_almost_equal(transformed_points[:, 0], x_transform.transform(points)[:, 0])\n    assert_array_almost_equal(transformed_points[:, 1], y_transform.transform(points)[:, 1])\n", "def test_blended_transform_contains_branch_seperately():\n    trans_x = mtransforms.Affine2D().translate(10, 0)\n    trans_y = mtransforms.Affine2D().translate(0, 20)\n\n    blended_trans = mtransforms.blended_transform_factory(trans_x, trans_y)\n    x_contains, y_contains = blended_trans.contains_branch_seperately(trans_x)\n\n    assert x_contains\n    assert not y_contains\n\n    x_contains, y_contains = blended_trans.contains_branch_seperately(trans_y)\n\n    assert not x_contains\n    assert y_contains\n", "def test_interval_contains():\n    interval = (1.0, 5.0)\n    assert mtransforms.interval_contains(interval, 3.0)\n    assert not mtransforms.interval_contains(interval, 0.0)\n    assert not mtransforms.interval_contains(interval, 6.0)\n    assert mtransforms.interval_contains(interval, 1.0)\n    assert mtransforms.interval_contains(interval, 5.0)\n", "def test_AffineDeltaTransform():\n    t = mtransforms.Affine2D().translate(1, 2).scale(3, 4)\n    dt = mtransforms.AffineDeltaTransform(t)\n    points = np.array([[0, 0], [1, 1]])\n\n    # Check that the translation part of t is ignored.\n    assert_array_almost_equal(dt.transform(points), t.get_matrix()[:, :2] @ points.T).T\n\n    # Check that applying the delta transform twice is equivalent to adding the\n    # transformed points.\n    double_dt = mtransforms.AffineDeltaTransform(t) + mtransforms.AffineDeltaTransform(t)\n    assert_array_almost_equal(double_dt.transform(points), 2 * dt.transform(points))\n", "def test_transform_wrapper_equality():\n    # Test that TransformWrapper instances can be compared for equality.\n    t1 = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    t2 = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    assert t1 == t2\n\n    t3 = mtransforms.TransformWrapper(mtransforms.Affine2D().translate(1, 2))\n    assert t1 != t3\n\n    # Test that TransformWrapper instances can be compared to other transforms.\n    t4 = mtransforms.Affine2D()\n    assert t1 == t4\n\n    t5 = mtransforms.Affine2D().translate(1, 2)\n    assert t1 != t5\n"], "sample_540": ["def test_movie_writer_invalid_bitrate(anim):\n    with pytest.raises(ValueError, match=\"Bitrate must be a non-negative number\"):\n        _ = anim.save(\"test.mp4\", writer=animation.FFMpegFileWriter(), bitrate=-2)\n", "def test_movie_writer_metadata(tmpdir, anim):\n    writer = animation.FFMpegWriter(metadata={'title': 'Test Animation'})\n    with tmpdir.as_cwd():\n        anim.save('test.mp4', writer=writer)\n    # Check if metadata was correctly passed to ffmpeg\n    with open(tmpdir.join('test.mp4'), 'rb') as f:\n        assert b'title' in f.read()\n", "def test_html_writer_invalid_path(anim):\n    if sys.platform == \"win32\":\n        match_str = re.escape(\"[WinError 3] The system cannot find the path specified:\")\n    else:\n        match_str = re.escape(\"[Errno 2] No such file or directory: '/foo\")\n    with pytest.raises(FileNotFoundError, match=match_str):\n        _ = anim.save(\"/foo/bar/aardvark/thiscannotreallyexist.html\",\n                      writer=animation.HTMLWriter())\n", "def test_to_jshtml(anim):\n    jshtml = anim.to_jshtml()\n    assert '<script>' in jshtml\n    assert '</script>' in jshtml\n    assert 'animation.js' in jshtml\n", "def test_html_writer_embed_limit(tmpdir):\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=5)\n    writer = animation.HTMLWriter(fps=30, embed_limit=1e-6)  # ~1 byte.\n    with tmpdir.as_cwd():\n        with pytest.warns(UserWarning, match=\"Animation size has reached\"):\n            anim.save(\"test.html\", writer=writer)\n"], "sample_542": ["def test_text_bbox_alignment():\n    fig, ax = plt.subplots()\n    text = \"This is some example text\"\n    for x in [0.1, 0.5, 0.9]:\n        for ha in ['left', 'center', 'right']:\n            ax.text(x, 0.8, text, ha=ha, bbox=dict(facecolor='red'))\n    for y in [0.2, 0.5, 0.8]:\n        for va in ['bottom', 'center', 'top']:\n            ax.text(0.5, y, text, va=va, bbox=dict(facecolor='blue'))\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n", "def test_linespacing():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'foo\\nbar', linespacing=1.2)\n    assert text.get_linespacing() == 1.2\n\n    text.set_linespacing(2.5)\n    assert text.get_linespacing() == 2.5\n\n    with pytest.raises(TypeError):\n        text.set_linespacing('invalid')\n", "def test_linespacing():\n    fig, ax = plt.subplots()\n    text1 = ax.text(0.5, 0.5, \"Line1\\nLine2\", linespacing=1)\n    text2 = ax.text(0.5, 0.3, \"Line1\\nLine2\", linespacing=2)\n    fig.canvas.draw()\n\n    ext1 = text1.get_window_extent()\n    ext2 = text2.get_window_extent()\n    assert ext2.height > ext1.height\n", "def test_text_get_rotation_mode():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    assert text.get_rotation_mode() is None\n\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n\n    text.set_rotation_mode('anchor')\n    assert text.get_rotation_mode() == 'anchor'\n", "def test_set_linespacing():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Hello\\nWorld\")\n    text.set_linespacing(2)\n    assert text.get_linespacing() == 2\n    fig.canvas.draw()\n    # Check that the height of the text is correctly updated.\n    bbox = text.get_window_extent(fig.canvas.renderer)\n    assert bbox.height > 20  # exact value depends on the font size\n"], "sample_544": ["def test_axesimage_get_extent():\n    # generate dummy image to test get_extent method\n    ax = plt.gca()\n    im = AxesImage(ax)\n    extent = (1, 2, 3, 4)\n    im.set_extent(extent)\n    assert im.get_extent() == extent\n\n    # Test that get_extent returns None when no extent is set\n    im2 = AxesImage(ax)\n    assert im2.get_extent() is not None\n\n    # Test that get_extent returns the correct default extent for an image\n    # with a data array but no explicitly set extent.\n    im3 = AxesImage(ax)\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im3.set_data(z)\n    assert im3.get_extent() == (-0.5, 2.5, -0.5, 3.5)\n\n    # Test that get_extent returns the correct extent when origin='upper'\n    im4 = AxesImage(ax, origin='upper')\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im4.set_data(z)\n    assert im4.get_extent() == (-0.5, 2.5, 3.5, -0.5)\n", "def test_imshow_stage(fig_test, fig_ref):\n    # Test that data and rgba stages produce different results for large range.\n    data = np.random.default_rng().integers(0, 2**16, (10, 10), dtype=np.uint16)\n    ax = fig_test.subplots()\n    ax.imshow(data, interpolation=\"bilinear\", interpolation_stage='data')\n    ax = fig_ref.subplots()\n    ax.imshow(data, interpolation=\"bilinear\", interpolation_stage='rgba')\n", "def test_pcolorimage(fig_test, fig_ref):\n    ax_test = fig_test.subplots()\n    ax_ref = fig_ref.subplots()\n\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = X + Y\n\n    ax_test.pcolorfast(X, Y, Z)\n\n    ax_ref.imshow(Z, origin='lower', extent=(x.min(), x.max(), y.min(), y.max()))\n", "def test_nonuniformimage_extent(fig_test, fig_ref):\n    ax_test = fig_test.subplots()\n    ax_ref = fig_ref.subplots()\n\n    x = np.arange(5)\n    y = np.arange(7)\n    X, Y = np.meshgrid(x, y)\n    Z = X + Y\n\n    im_test = NonUniformImage(ax_test, interpolation='nearest')\n    im_test.set_data(x, y, Z)\n    ax_test.add_image(im_test)\n\n    ax_ref.imshow(Z, interpolation='nearest', extent=(0, 4, 0, 6))\n", "def test_axesimage_write_png(fig_test, fig_ref):\n    # Test that AxesImage.write_png produces the same result as saving\n    # a figure with only that image.\n    ax = fig_test.subplots()\n    im = np.arange(100).reshape((10, 10))\n    img = ax.imshow(im)\n    ax.axis('off')\n\n    buf = io.BytesIO()\n    img.write_png(buf)\n\n    ax = fig_ref.subplots()\n    ax.imshow(im)\n    ax.axis('off')\n    fig_ref.savefig(buf := io.BytesIO())\n\n    assert buf.getvalue() == buf.getvalue()\n"], "sample_545": ["def test_add_artist(fig_test, fig_ref):\n    fig_test.subplots()\n    l1 = plt.Line2D([.2, .7], [.7, .7], gid='l1')\n    l2 = plt.Line2D([.2, .7], [.8, .8], gid='l2')\n    r1 = plt.Circle((20, 20), 100, transform=None, gid='C1')\n    r2 = plt.Circle((.7, .5), .05, gid='C2')\n    r3 = plt.Circle((4.5, .8), .55, transform=fig_test.dpi_scale_trans,\n                    facecolor='crimson', gid='C3')\n    for a in [l1, l2, r1, r2, r3]:\n        fig_test.add_artist(a)\n    l2.remove()\n\n    ax2 = fig_ref.subplots()\n    l1 = plt.Line2D([.2, .7], [.7, .7], transform=fig_ref.transFigure,\n                    gid='l1', zorder=21)\n    r1 = plt.Circle((20, 20), 100, transform=None, clip_on=False, zorder=20,\n                    gid='C1')\n    r2 = plt.Circle((.7, .5), .05, transform=fig_ref.transFigure, gid='C2',\n                    zorder=20)\n    r3 = plt.Circle((4.5, .8), .55, transform=fig_ref.dpi_scale_trans,\n                    facecolor='crimson', clip_on=False, zorder=20, gid='C3')\n    for a in [l1, r1, r2, r3]:\n        ax2.add_artist(a)\n\n    # Add another artist with a different transform to ensure that it's applied\n    # correctly.\n    fig_test.add_artist(plt.Text(.5, .9, 'text', transform=fig_test.transFigure))\n    fig_ref.add_artist(plt.Text(.5, .9, 'text', transform=fig_ref.transFigure))\n", "def test_sup_title_xlabel_ylabel(fig_test, fig_ref):\n    fig_test.supxlabel(\"Xlabel\")\n    fig_test.supylabel(\"Ylabel\")\n    fig_test.suptitle(\"Title\")\n\n    ax = fig_ref.add_subplot(111)\n    ax.set_xlabel(\"Xlabel\")\n    ax.set_ylabel(\"Ylabel\")\n    fig_ref.suptitle(\"Title\")\n", "def test_subfigure_repr():\n    fig = plt.figure()\n    subfig = fig.subfigures(1, 1)\n    assert repr(subfig[0]).startswith(\"<SubFigure\")\n", "def test_subfigure_remove(fig_test, fig_ref):\n    fig_test.subplots()\n    sub = fig_test.subfigures(1, 2)[0]\n    sub.remove()\n\n    fig_ref.subplots()\n", "def test_supxlabel_ylabel(fig_test, fig_ref):\n    fig_test.supxlabel(\"Xlabel\")\n    fig_test.supylabel(\"Ylabel\")\n    ax = fig_ref.add_subplot()\n    ax.set_xlabel(\"Xlabel\")\n    ax.set_ylabel(\"Ylabel\")\n"], "sample_546": ["def test_subplots_adjust():\n    fig = plt.figure()\n    n = 10\n    fig.subplots(n, 1, sharex=True)\n    fig.subplots_adjust(hspace=0)\n    for i in range(1, n):\n        ax = fig.axes[i]\n        assert ax.get_position().p0[1] == fig.axes[i-1].get_position().p1[1]\n", "def test_subfigure_add_subplot():\n    fig = plt.figure()\n    subfigs = fig.subfigures(1, 2)\n    ax1 = subfigs[0].add_subplot(111)\n    ax2 = subfigs[1].add_subplot(111)\n    assert ax1.get_position().x0 < ax2.get_position().x0\n", "def test_figure_set_constrained_layout():\n    fig = plt.figure()\n    assert not fig.get_constrained_layout()\n\n    fig.set_constrained_layout(True)\n    assert fig.get_constrained_layout()\n\n    fig.set_constrained_layout(False)\n    assert not fig.get_constrained_layout()\n\n    # Test that it accepts a dictionary of parameters\n    fig.set_constrained_layout(True, w_pad=1.0, h_pad=2.0)\n    assert fig.get_constrained_layout()\n\n    # Test that it warns if the layout engine is changed after a colorbar has\n    # been added.\n    fig.set_constrained_layout(False)\n    fig.subplots(1, 1).colorbar(mpl.cm.ScalarMappable())\n    with pytest.warns(RuntimeError):\n        fig.set_constrained_layout(True)\n", "def test_figure_constrained_layout():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    fig.set_layout_engine('constrained')\n    assert fig.get_layout_engine() is not None\n    assert isinstance(fig.get_layout_engine(), type(fig.get_layout_engine()))\n    assert fig.get_constrained_layout()\n    fig.set_layout_engine(None)\n    assert not fig.get_constrained_layout()\n", "def test_figure_subfigures():\n    fig = plt.figure()\n    sfigs = fig.subfigures(1, 2)\n    assert len(sfigs) == 2\n    assert sfigs[0].get_parent() is fig\n    assert sfigs[1].get_parent() is fig\n\n    axsL = sfigs[0].subplots(1, 2)\n    axsR = sfigs[1].subplots(2, 1)\n\n    assert len(axsL) == 2\n    assert len(axsR) == 2\n\n    for ax in axsL + axsR:\n        assert ax.get_figure() is fig\n"], "sample_547": ["def test_offsetbox_alignment():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test\\ntest long text\", loc=\"center left\",\n                         pad=0.2, prop={\"ha\": \"left\", \"va\": \"baseline\"},\n                         frameon=False)\n    ax.add_artist(text0)\n\n    text1 = AnchoredText(\"test\\ntest long text\", loc=\"center\",\n                         pad=0.2, prop={\"ha\": \"center\", \"va\": \"baseline\"},\n                         frameon=False)\n    ax.add_artist(text1)\n\n    text2 = AnchoredText(\"test\\ntest long text\", loc=\"center right\",\n                         pad=0.2, prop={\"ha\": \"right\", \"va\": \"baseline\"},\n                         frameon=False)\n    ax.add_artist(text2)\n\n    text3 = AnchoredText(\"test\\ntest long text\", loc=\"upper left\",\n                         pad=0.2, prop={\"ha\": \"left\", \"va\": \"top\"},\n                         frameon=False)\n    ax.add_artist(text3)\n\n    text4 = AnchoredText(\"test\\ntest long text\", loc=\"lower right\",\n                         pad=0.2, prop={\"ha\": \"right\", \"va\": \"bottom\"},\n                         frameon=False)\n    ax.add_artist(text4)\n", "def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    box = OffsetBox()\n    ax.add_artist(box)\n    event = MouseEvent(\"motion_notify_event\", fig.canvas, 0, 0)\n    contains, _ = box.contains(event)\n    assert not contains\n\n    # Add a child artist and check again.\n    child = mpatches.Rectangle((0, 0), 1, 1)\n    box._children = [child]\n    contains, _ = box.contains(event)\n    assert contains\n", "def test_auxtransformbox():\n    fig, ax = plt.subplots()\n    aux_transform = mtransforms.Affine2D().scale(1.5).rotate_deg(45)\n    child = TextArea(\"Hello\")\n    box = AuxTransformBox(aux_transform)\n    box.add_artist(child)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bbox = box.get_bbox(renderer)\n\n    assert isinstance(bbox, Bbox)\n    assert bbox.width > 0\n    assert bbox.height > 0\n", "def test_drawingarea_transform():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    ax.add_artist(da)\n    fig.canvas.draw()\n\n    assert not da.get_transform().is_affine\n    assert isinstance(da.get_transform(), mtransforms.CompositeGenericTransform)\n", "def test_offsetbox_repr():\n    ob = OffsetBox()\n    assert 'OffsetBox' in repr(ob)\n\n    anchored_ob = AnchoredOffsetbox(loc='center', child=ob)\n    assert 'AnchoredOffsetbox' in repr(anchored_ob)\n    assert 'OffsetBox' in repr(anchored_ob)\n\n    at = AnchoredText('test', loc='center')\n    assert 'AnchoredText' in repr(at)\n    assert 'TextArea' in repr(at)\n    assert 'test' in repr(at)\n\n    da = DrawingArea(100, 100)\n    assert 'DrawingArea' in repr(da)\n    assert '100' in repr(da)\n\n    hp = HPacker()\n    assert 'HPacker' in repr(hp)\n\n    vp = VPacker()\n    assert 'VPacker' in repr(vp)\n\n    pb = PaddedBox(child=ob)\n    assert 'PaddedBox' in repr(pb)\n    assert 'OffsetBox' in repr(pb)\n\n    ab = AnnotationBbox(offsetbox=ob, xy=(0.5, 0.5))\n    assert 'AnnotationBbox' in repr(ab)\n    assert 'OffsetBox' in repr(ab)\n    assert '0.5' in repr(ab)\n"], "sample_548": ["def test_colorbar_label_rotation():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cbar = fig.colorbar(im)\n    cbar.set_label('Test Label', rotation=45)\n", "def test_colorbar_ticklabels_rotation(fig_ref, fig_test):\n    ax_ref = fig_ref.add_subplot()\n    im = ax_ref.imshow([[0, 1], [2, 3]])\n    cbar_ref = ax_ref.figure.colorbar(im)\n    cbar_ref.ax.set_xticklabels(cbar_ref.ax.get_xticklabels(), rotation=45)\n\n    ax_test = fig_test.add_subplot()\n    im = ax_test.imshow([[0, 1], [2, 3]])\n    cbar_test = ax_test.figure.colorbar(im)\n    cbar_test.ax.tick_params(axis='x', labelrotation=45)\n", "def test_colorbar_extend_triangles():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]], interpolation=\"none\")\n    fig.colorbar(im, extend='both', extendfrac=(0.05, 0.05))\n    fig.colorbar(im, extend='both', extendfrac=(0.05, 0.15))\n", "def test_colorbar_alpha(fig_ref, fig_test):\n    ax_ref = fig_ref.add_subplot()\n    im = ax_ref.imshow([[0, 1], [2, 3]], alpha=0.5)\n    fig_ref.colorbar(im)\n\n    ax_test = fig_test.add_subplot()\n    im = ax_test.imshow([[0, 1], [2, 3]])\n    cb = fig_test.colorbar(im)\n    cb.set_alpha(0.5)\n", "def test_colorbar_set_ticks():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    ticks = [0.5, 1.5, 2.5]\n    labels = ['a', 'b', 'c']\n    cb.set_ticks(ticks, labels=labels)\n    np.testing.assert_array_equal(cb.get_ticks(), ticks)\n    assert cb.ax.yaxis.get_ticklabels()[0].get_text() == labels[0]\n"], "sample_549": ["def test_unikey_or_keysym_to_mplkey():\n    # Test with some known values\n    assert cbook._unikey_or_keysym_to_mplkey(\"a\", \"a\") == \"a\"\n    assert cbook._unikey_or_keysym_to_mplkey(\"\", \"space\") == \"space\"\n    assert cbook._unikey_or_keysym_to_mplkey(\"\", \"ctrl_l\") == \"ctrl\"\n    assert cbook._unikey_or_keysym_to_mplkey(\"\", \"alt_r\") == \"alt\"\n\n    # Test with unknown keysyms\n    assert cbook._unikey_or_keysym_to_mplkey(\"\", \"EuroSign\") == \"eurosign\"\n\n    # Test with modifier keys\n    assert cbook._unikey_or_keysym_to_mplkey(\"\", \"Shift_L\") == \"shift\"\n    assert cbook._unikey_or_keysym_to_mplkey(\"\", \"Control_R\") == \"ctrl\"\n\n    # Test with None input\n    assert cbook._unikey_or_keysym_to_mplkey(None, \"a\") == \"a\"\n", "def test_is_math_text():\n    assert cbook.is_math_text('$1$')\n    assert not cbook.is_math_text('1')\n    assert cbook.is_math_text('$1$ and $2$')\n    assert not cbook.is_math_text('$1 and $2')\n", "def test_grouper():\n    class Dummy:\n        pass\n    a, b, c, d, e, f = objs = [Dummy() for _ in range(6)]\n    g = cbook.Grouper()\n    g.join(a, b)\n    g.join(b, c)\n    g.join(d, e)\n\n    assert len(list(g)) == 2\n    assert set(list(g)[0]) == {a, b, c}\n    assert set(list(g)[1]) == {d, e}\n\n    assert g.joined(a, b)\n    assert g.joined(a, c)\n    assert not g.joined(a, d)\n\n    g.remove(b)\n    assert len(list(g)) == 3\n    assert set(list(g)[0]) == {a}\n    assert set(list(g)[1]) == {c}\n    assert set(list(g)[2]) == {d, e}\n", "def test_get_sample_data():\n    # test get_sample_data with asfileobj=True\n    file_obj = cbook.get_sample_data('README.txt', asfileobj=True)\n    assert isinstance(file_obj, bytes)\n\n    # test get_sample_data with asfileobj=False\n    file_path = cbook.get_sample_data('README.txt', asfileobj=False)\n    assert isinstance(file_path, str)\n\n    # test get_sample_data with non-existent file\n    with pytest.raises(FileNotFoundError):\n        cbook.get_sample_data('non_existent_file.txt')\n", "def test_get_sample_data():\n    # Test that get_sample_data returns the correct data for a given file\n    fname = 'goog.npy'\n    datafile = cbook.get_sample_data(fname, asfileobj=False)\n    assert isinstance(datafile, str)\n    assert datafile.endswith(fname)\n\n    datafile = cbook.get_sample_data(fname, asfileobj=True)\n    assert hasattr(datafile, 'read')\n"], "sample_550": ["def test_artist_list():\n    fig, ax = plt.subplots()\n    line1 = ax.plot([1, 2, 3])[0]\n    line2 = ax.plot([4, 5, 6])[0]\n    patch1 = ax.add_patch(plt.Rectangle((1, 1), 1, 1))\n    patch2 = ax.add_patch(plt.Rectangle((2, 2), 1, 1))\n\n    assert len(ax.lines) == 2\n    assert len(ax.patches) == 2\n\n    assert line1 in ax.lines\n    assert line2 in ax.lines\n    assert patch1 in ax.patches\n    assert patch2 in ax.patches\n\n    # Test indexing and slicing\n    assert ax.lines[0] is line1\n    assert ax.lines[1] is line2\n    assert ax.patches[0] is patch1\n    assert ax.patches[1] is patch2\n\n    assert ax.lines[:1] == [line1]\n    assert ax.lines[1:] == [line2]\n    assert ax.patches[:1] == [patch1]\n    assert ax.patches[1:] == [patch2]\n\n    # Test addition\n    lines = ax.lines + [line1, line2]\n    assert len(lines) == 4\n    assert lines[0] is line1\n    assert lines[1] is line2\n    assert lines[2] is line1\n    assert lines[3] is line2\n\n    patches = ax.patches + [patch1, patch2]\n    assert len(patches) == 4\n    assert patches[0] is patch1\n    assert patches[1] is patch2\n    assert patches[2] is patch1\n    assert patches[3] is patch2\n\n    # Test iteration\n    for line in ax.lines:\n        assert line in [line1, line2]\n    for patch in ax.patches:\n        assert patch in [patch1, patch2]\n", "def test_axes_locator():\n    fig, ax = plt.subplots()\n    locator = _TransformedBoundsLocator([0.1, 0.1, 0.8, 0.8], fig.transFigure)\n    ax.set_axes_locator(locator)\n    fig.canvas.draw()\n    assert ax.get_position().bounds == pytest.approx((0.1, 0.1, 0.8, 0.8))\n", "def test_axes_children():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    ax.text(0.5, 0.5, 'test')\n    ax.set_title('title')\n    ax.set_xlabel('xlabel')\n    ax.set_ylabel('ylabel')\n\n    children = ax.get_children()\n\n    assert isinstance(children[0], plt.Line2D)\n    assert isinstance(children[1], plt.Text)\n    assert isinstance(children[-4], plt.Title)\n    assert isinstance(children[-3], plt.Text)\n    assert isinstance(children[-2], plt.Text)\n    assert isinstance(children[-1], plt.Patch)\n", "def test_axes_get_window_extent():\n    fig, ax = plt.subplots()\n    assert isinstance(ax.get_window_extent(), transforms.Bbox)\n    renderer = fig.canvas.get_renderer()\n    assert isinstance(ax.get_window_extent(renderer), transforms.Bbox)\n", "def test_axes_remove():\n    fig, ax = plt.subplots()\n    ax2 = ax.twinx()\n    assert ax2 in ax._twinned_axes.get_siblings(ax)\n    ax.remove()\n    assert ax2 not in ax._twinned_axes.get_siblings(ax)\n"], "sample_551": ["def test_patch3d_collection():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    patches = [plt.Circle((0, 0), 1), plt.Rectangle((1, 1), 2, 2)]\n    collection = art3d.Patch3DCollection(patches)\n    ax.add_collection(collection)\n    assert len(ax.collections) == 1\n    assert isinstance(ax.collections[0], art3d.Patch3DCollection)\n", "def test_pathpatch_2d_to_3d():\n    from matplotlib.patches import Circle\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    pathpatch = Circle((0, 0), 1, edgecolor='black', facecolor='none')\n    art3d.pathpatch_2d_to_3d(pathpatch, z=0.5)\n    ax.add_patch(pathpatch)\n    assert pathpatch.get_facecolor()[0] == (0, 0, 0, 0)\n    assert pathpatch.get_edgecolor()[0] == (0, 0, 0, 1)\n", "def test_linecollection_2d_to_3d():\n    from matplotlib.lines import Line2D\n    from mpl_toolkits.mplot3d import art3d\n\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    line = Line2D([0, 1], [0, 1])\n    art3d.line_collection_2d_to_3d(line, zs=5)\n    ax.add_collection(line)\n\n    assert isinstance(line, art3d.Line3DCollection)\n    assert line.get_zs() == 5\n", "def test_text_2d_to_3d():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    text = ax.text(0.5, 0.5, 'text')\n    art3d.text_2d_to_3d(text, z=1)\n    assert isinstance(text, art3d.Text3D)\n    assert text._z == 1\n", "def test_linecollection_get_edgecolor():\n    lines1 = [[(0, .5), (.5, 1)], [(.3, .6), (.2, .2)]]\n    lc1 = art3d.Line3DCollection(lines1, colors='red', lw=3)\n    assert lc1.get_edgecolor() == [(1.0, 0.0, 0.0, 1.0)]\n"], "sample_552": ["def test_figure_supxlabel():\n    fig, axs = plt.subplots(2, 2)\n    fig.supxlabel('xlabel')\n    assert fig._supxlabel is not None\n    assert fig._supxlabel.get_text() == 'xlabel'\n    fig.supxlabel('new_xlabel')\n    assert fig._supxlabel.get_text() == 'new_xlabel'\n\n    # Test supxlabel with kwargs\n    fig.supxlabel('xlabel', fontsize=20, color='red')\n    assert fig._supxlabel.get_fontsize() == 20\n    assert fig._supxlabel.get_color() == 'red'\n", "def test_set_layout_engine_invalid_kwargs():\n    fig = Figure()\n    with pytest.raises(ValueError, match=\"Invalid value for 'layout'\"):\n        fig.set_layout_engine('tight', unknown_kwarg=True)\n", "def test_add_subplot_axes_class():\n    class MyAxes(Axes):\n        pass\n\n    fig = Figure()\n    ax = fig.add_subplot(111, axes_class=MyAxes)\n    assert isinstance(ax, MyAxes)\n\n    with pytest.raises(ValueError):\n        fig.add_subplot(111, axes_class=MyAxes, projection='3d')\n\n    with pytest.raises(ValueError):\n        fig.add_subplot(111, axes_class=MyAxes, polar=True)\n", "def test_subfigure_repr():\n    fig = plt.figure()\n    subfig = fig.subfigures(1, 2)\n    assert repr(subfig[0]) == \"<SubFigure size 640x480 with 0 Axes>\"\n", "def test_get_children():\n    fig, ax = plt.subplots()\n    children = fig.get_children()\n    assert ax in children\n    assert fig.patch in children\n"], "sample_553": ["def test_movie_writer_adjusted_figsize():\n    fig = plt.figure(figsize=(10.85, 9.21))\n    writer = animation.FFMpegWriter()\n    writer.setup(fig, \"test.mp4\", dpi=100)\n    assert fig.get_size_inches() != (10.85, 9.21)\n    writer.finish()\n", "def test_html_writer_options(anim):\n    writer = animation.HTMLWriter(fps=30, embed_frames=True,\n                                  default_mode='once')\n    anim.save('unused.html', writer=writer)\n    # Check that options are properly set in the generated HTML.\n    with open('unused.html', 'r') as f:\n        html = f.read()\n    assert '1000 / 30' in html  # interval\n    assert 'embed_frames = True' not in html  # This should be removed.\n    assert 'default_mode = \"once\"' not in html  # This should be removed.\n    assert 'loop' not in html  # We're in once mode.\n", "def test_animation_to_jshtml(anim):\n    html = anim.to_jshtml()\n    assert \"<script>\" in html\n    assert \"<video\" not in html\n    # Test without fps, with interval set to 100\n    anim._interval = 100\n    html = anim.to_jshtml(fps=None)\n    assert \"<script>\" in html\n    assert \"<video\" not in html\n", "def test_html_writer_invalid_extra_args(anim):\n    writer = animation.HTMLWriter(extra_args=['invalid_arg'])\n    with pytest.warns(UserWarning, match='HTMLWriter ignores'):\n        anim.save(\"test.html\", writer=writer)\n", "def test_invalid_save_count():\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    with pytest.raises(ValueError, match='save_count must be a positive integer'):\n        animation.FuncAnimation(fig, animate, init_func=init, save_count=-1)\n\n    with pytest.raises(ValueError, match='save_count must be a positive integer'):\n        animation.FuncAnimation(fig, animate, init_func=init, save_count=0)\n\n    with pytest.raises(TypeError, match='save_count must be an integer'):\n        animation.FuncAnimation(fig, animate, init_func=init, save_count='10')\n"], "sample_554": ["def test_update_from():\n    # Test that update_from correctly copies properties from another Text instance.\n    text1 = Text(0.5, 0.5, \"Test\")\n    text1.set_color(\"red\")\n    text1.set_fontproperties(FontProperties(weight=\"bold\"))\n\n    text2 = Text(0.5, 0.5, \"Test\")\n    text2.update_from(text1)\n\n    assert text2.get_color() == text1.get_color()\n    assert text2.get_fontproperties().get_weight() == text1.get_fontproperties().get_weight()\n", "def test_text_math_antialiased_off(fig_test, fig_ref):\n    fig_test.text(0.5, 0.5, r'$\\sum_{i=0}^{j}$', antialiased=False)\n\n    mpl.rcParams['text.antialiased'] = False\n    fig_ref.text(0.5, 0.5, r'$\\sum_{i=0}^{j}$')\n", "def test_text_set_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    bbox = dict(facecolor='red', alpha=0.5)\n    text.set_bbox(bbox)\n    assert text.get_bbox_patch() is not None\n\n    # Check that the bbox is correctly updated when changing the text properties\n    text.set_fontsize(20)\n    fig.canvas.draw()\n    text.update_bbox_position_size(fig.canvas.renderer)\n    assert text.get_bbox_patch().get_window_extent() != Bbox.unit()\n\n    # Check that the bbox is correctly removed when setting it to None\n    text.set_bbox(None)\n    assert text.get_bbox_patch() is None\n", "def test_set_backgroundcolor():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    text.set_backgroundcolor('red')\n    assert text.get_bbox_patch().get_facecolor() == (1, 0, 0, 1)\n", "def test_text_wrap_long_word():\n    fig = plt.figure(figsize=(6, 4))\n    text = fig.text(0.5, 0.5, 'This is a very long word that should be wrapped', wrap=True)\n    fig.canvas.draw()\n    assert len(text._get_wrapped_text().split('\\n')) > 1\n"], "sample_555": ["def test_modifying_ellipse(fig_test, fig_ref):\n    ell1 = Ellipse([.5, .5], .5, 1, angle=20)\n    ell2 = Ellipse([.5, .5], 1.5, 1, angle=10)\n    fig_ref.subplots().add_patch(ell1)\n    fig_test.subplots().add_patch(ell2)\n    ell2.set_width(.5)\n    ell2.set_angle(20)\n", "def test_arc_get_path():\n    arc = Arc([.5, .5], .5, 1, theta1=0, theta2=60, angle=20)\n    path = arc.get_path()\n    assert isinstance(path, mpath.Path)\n", "def test_get_patch_transform():\n    rect = Rectangle((1, 2), 3, 4)\n    assert isinstance(rect.get_patch_transform(), mtransforms.IdentityTransform)\n    ell = Ellipse((1, 2), 3, 4)\n    assert not isinstance(ell.get_patch_transform(), mtransforms.IdentityTransform)\n", "def test_fill_between_alpha():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 1, 100)\n    y1 = np.sin(x)\n    y2 = np.cos(x)\n\n    ax.fill_between(x, y1, y2, alpha=0.5)\n\n    # check that the patch is created with correct alpha\n    assert ax.patches[0].get_alpha() == 0.5\n", "def test_fancybox():\n    # Test that fancybox is drawn correctly\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.text(0.5, 0.5, 'Test', bbox=dict(boxstyle='round'))\n"], "sample_556": ["def test_figure_repr_empty():\n    fig = plt.figure()\n    assert repr(fig) == \"<Figure size 640x480 with 0 Axes>\"\n", "def test_subplots_parsing():\n    # Testing the subplots parsing function with different input types.\n    fig = Figure()\n\n    # Test with a single number\n    axes = fig.subplots(2)\n    assert len(axes) == 2\n\n    # Test with a tuple of numbers\n    axes = fig.subplots(2, 3)\n    assert axes.shape == (2, 3)\n\n    # Test with a tuple of numbers and sharex=True\n    axes = fig.subplots(2, 3, sharex=True)\n    assert axes.shape == (2, 3)\n    for ax in axes[1, :]:\n        assert ax.xaxis.get_visible() is False\n\n    # Test with a tuple of numbers and sharey=True\n    axes = fig.subplots(2, 3, sharey=True)\n    assert axes.shape == (2, 3)\n    for ax in axes[:, 1:].flat:\n        assert ax.yaxis.get_visible() is False\n\n    # Test with a tuple of numbers and squeeze=False\n    axes = fig.subplots(2, 3, squeeze=False)\n    assert axes.shape == (2, 3)\n\n    # Test with a tuple of numbers and subplot_kw\n    axes = fig.subplots(2, 3, subplot_kw={'facecolor': 'red'})\n    assert axes.shape == (2, 3)\n    for ax in axes.flat:\n        assert ax.patch.get_facecolor() == (1.0, 0.0, 0.0, 1.0)\n\n    # Test with a tuple of numbers and gridspec_kw\n    axes = fig.subplots(2, 3, gridspec_kw={'width_ratios': [1, 2, 3]})\n    assert axes.shape == (2, 3)\n", "def test_supxlabel_and_supylabel(fig_test, fig_ref):\n    fig_test.subplots(2, 1)\n    fig_test.supxlabel(\"supxlabel\")\n    fig_test.supylabel(\"supylabel\")\n\n    fig_ref.subplots(2, 1)\n    fig_ref.text(0.5, 0.02, \"supxlabel\", ha=\"center\")\n    fig_ref.text(0.02, 0.5, \"supylabel\", va=\"center\", rotation=\"vertical\")\n", "def test_add_artist_duplicate():\n    fig = Figure()\n    artist = plt.Line2D([.2, .7], [.7, .7])\n    fig.add_artist(artist)\n    with pytest.raises(ValueError):\n        fig.add_artist(artist)\n", "def test_figure_suptitle():\n    fig = Figure()\n    title = \"Test Title\"\n    suptitle = fig.suptitle(title)\n    assert suptitle.get_text() == title\n    assert fig._suptitle == suptitle\n\n    # Test that setting a new title removes the old one\n    new_title = \"New Title\"\n    new_suptitle = fig.suptitle(new_title)\n    assert new_suptitle.get_text() == new_title\n    assert fig._suptitle == new_suptitle\n    assert suptitle not in fig.texts\n"], "sample_557": ["def test_sup_xlabel_title(fig_test, fig_ref):\n    fig_ref.supxlabel(\"X-Label\")\n    fig_ref.suptitle(\"Title\")\n\n    fig_test.supxlabel(\"X-Label\")\n    fig_test.suptitle(\"Title\")\n\n    # Check that suptitle and supxlabel are correctly positioned.\n    renderer = fig_test.canvas.get_renderer()\n    assert (fig_test._suptitle.get_window_extent(renderer).y0 <\n            fig_test._supxlabel.get_window_extent(renderer).y0)\n", "def test_subfigure_repr():\n    fig = Figure()\n    subfig = fig.subfigures(1)[0]\n    assert repr(subfig) == \"<SubFigure size 640x480 with 0 Axes>\"\n", "def test_subfigure_repr():\n    fig = plt.figure()\n    subfig = fig.subfigures(1, 1)[0]\n    assert repr(subfig) == \"<SubFigure size 640x480 with 0 Axes>\"\n", "def test_sup_xlabel_ylabel_title(fig_test, fig_ref):\n    fig_ref.supxlabel(\"xlabel\", x=0.6)\n    fig_ref.supylabel(\"ylabel\", y=0.4)\n    fig_ref.suptitle(\"Title\", x=0.5, y=0.95)\n\n    fig_test.supxlabel(\"xlabel\")\n    fig_test.supylabel(\"ylabel\")\n    fig_test.suptitle(\"Title\")\n\n    fig_test._supxlabel.set_position((0.6, None))\n    fig_test._supylabel.set_position((None, 0.4))\n    fig_test._suptitle.set_position((0.5, 0.95))\n", "def test_get_size_inches():\n    fig = Figure(figsize=(10, 5))\n    assert fig.get_size_inches() == (10, 5)\n    fig.set_size_inches(8, 6)\n    assert fig.get_size_inches() == (8, 6)\n"], "sample_558": ["def test_cbar_axes_base_toggle_label():\n    fig = plt.figure()\n    cax = CbarAxesBase(fig, [0.1, 0.1, 0.8, 0.05], orientation='horizontal')\n    fig.add_axes(cax)\n    cax.toggle_label(True)\n    cax.toggle_label(False)\n", "def test_imagegrid_cbar_mode_single():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(8, 6))\n    grid = ImageGrid(fig, 111,\n                     nrows_ncols=(2, 2),\n                     cbar_location='right',\n                     cbar_size='20%',\n                     cbar_mode='single')\n    ax1, ax2, ax3, ax4 = grid\n\n    ax1.imshow(arr, cmap='nipy_spectral')\n    ax2.imshow(arr.T, cmap='hot')\n    ax3.imshow(np.hypot(arr, arr.T), cmap='jet')\n    ax4.imshow(np.arctan2(arr, arr.T), cmap='hsv')\n\n    # Clear out the axes first.\n    for ax in grid:\n        ax.cax.cla()\n    cb = ax1.cax.colorbar(ax1.images[0])\n", "def test_grid_with_cbar():\n    fig = plt.figure(figsize=(6, 6))\n    grid = Grid(fig, 111, (2, 2), axes_pad=0.1)\n    for ax in grid:\n        im = ax.imshow(np.random.rand(10, 10), interpolation='nearest')\n        cax = grid.cbar_axes[0]\n        fig.colorbar(im, cax=cax)\n    grid.cbar_axes[0].set_visible(True)\n", "def test_cbar_axes_base_toggle_label():\n    fig = plt.figure()\n    ax = CbarAxesBase(fig, [0.1, 0.1, 0.8, 0.8], orientation='vertical')\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        ax.toggle_label(True)\n    axis = ax.axis[ax.orientation]\n    assert axis.major_ticklabels.get_visible()\n    assert axis.label.get_visible()\n", "def test_imagegrid_cbar_mode_single():\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, nrows_ncols=(2, 2), cbar_mode='single',\n                     cbar_location='right')\n    for ax in grid:\n        im = ax.imshow(np.random.rand(10, 10))\n    grid.cbar_axes[0].colorbar(im)\n    assert len(grid.cbar_axes) == 1\n    assert all(ax.cax is grid.cbar_axes[0] for ax in grid)\n"], "sample_560": ["def test_legend_edgecolor_inherit():\n    \"\"\"Test legend edge color inheritance.\"\"\"\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], label='test')\n    ax.set_facecolor('red')\n    ax.legend(edgecolor='inherit')\n", "def test_set_title_fontproperties(fig_test, fig_ref):\n    # Test setting the title font properties using a FontProperties object.\n    fig_test.legend(title=\"Test\", title_fontproperties=FontProperties(size=22))\n    fig_ref.legend(title=\"Test\", title_fontsize=22)\n", "def test_legend_title_fontproperties():\n    # test the title_fontproperties kwarg\n    plt.plot(range(10))\n    leg = plt.legend(title='Aardvark',\n                     title_fontproperties={'family': 'serif', 'size': 22})\n    assert leg.get_title().get_family()[0] == 'serif'\n    assert leg.get_title().get_size() == 22\n", "def test_legend_multiple_scatter_plots():\n    fig, ax = plt.subplots()\n    ax.scatter([1, 2], [3, 4], label='scatter1')\n    ax.scatter([5, 6], [7, 8], label='scatter2')\n    leg = ax.legend()\n    assert len(leg.legend_handles) == 2\n    assert leg.legend_handles[0].get_label() == 'scatter1'\n    assert leg.legend_handles[1].get_label() == 'scatter2'\n", "def test_loc_validation_outside_string_value():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=(\"outside' option for loc='...' \"\n                       \"keyword argument only works for figure legends\")):\n        ax.legend(loc='outside upper right')\n"], "sample_561": ["def test_marker_get_path():\n    marker = markers.MarkerStyle(\"o\")\n    path = marker.get_path()\n    assert isinstance(path, Path)\n    assert len(path.vertices) > 0\n\n    marker = markers.MarkerStyle(\"\")\n    path = marker.get_path()\n    assert isinstance(path, Path)\n    assert len(path.vertices) == 0\n", "def test_marker_style_copy():\n    marker = markers.MarkerStyle(\"o\", fillstyle=\"left\")\n    new_marker = markers.MarkerStyle(marker)\n    assert new_marker.get_marker() == \"o\"\n    assert new_marker.get_fillstyle() == \"left\"\n    assert new_marker.get_transform() == marker.get_transform()\n    assert new_marker.get_joinstyle() == marker.get_joinstyle()\n    assert new_marker.get_capstyle() == marker.get_capstyle()\n\n    # Test that modifying the original marker doesn't affect the new one.\n    marker._set_marker(\"+\")\n    assert new_marker.get_marker() == \"o\"\n", "def test_marker_get_alt_path():\n    marker = markers.MarkerStyle(\"o\", fillstyle=\"left\")\n    assert marker.get_alt_path() is not None\n    assert isinstance(marker.get_alt_path(), Path)\n\n    marker = markers.MarkerStyle(\"o\", fillstyle=\"full\")\n    assert marker.get_alt_path() is None\n\n    marker = markers.MarkerStyle(\"o\", fillstyle=\"none\")\n    assert marker.get_alt_path() is None\n", "def test_marker_get_path():\n    marker = markers.MarkerStyle(\"o\")\n    assert isinstance(marker.get_path(), Path)\n\n    marker = markers.MarkerStyle(\"None\")\n    assert marker.get_path() == Path(np.empty((0, 2)))\n", "def test_marker_path():\n    \"\"\"Test that the path of a marker is correct.\"\"\"\n    marker = markers.MarkerStyle(\"o\")\n    path = marker.get_path()\n    assert isinstance(path, Path)\n    assert len(path.vertices) > 0\n\n    # Test that the path of a marker with a transform is correct.\n    t = Affine2D().translate(1, 1)\n    t_marker = markers.MarkerStyle(\"o\", transform=t)\n    t_path = t_marker.get_path()\n    assert isinstance(t_path, Path)\n    assert len(t_path.vertices) > 0\n    assert np.allclose(path.vertices + [1, 1], t_path.vertices)\n\n    # Test that the path of a marker with a fillstyle is correct.\n    marker = markers.MarkerStyle(\"o\", fillstyle=\"left\")\n    path = marker.get_path()\n    assert isinstance(path, Path)\n    assert len(path.vertices) > 0\n"], "sample_562": ["def test_axline_transform(fig_test, fig_ref):\n    ax1 = fig_test.add_subplot()\n    ax2 = fig_ref.add_subplot()\n\n    # Create an AxLine instance with slope and xy1\n    line1 = mlines.AxLine((.1, .1), slope=0.6)\n    line1.set_transform(ax1.transAxes)\n    ax1.add_line(line1)\n\n    # Create a Line2D instance for reference\n    line2 = mlines.Line2D([0, 1], [0.1 - 0.6 * 0.1, 0.1 + 0.6 * (1 - 0.1)])\n    line2.set_transform(ax2.transAxes)\n    ax2.add_line(line2)\n", "def test_line2d_subslice(fig_test, fig_ref):\n    x = np.linspace(0, 10, 10000)\n    y = np.sin(x)\n\n    fig_test.add_subplot().plot(x, y)\n    fig_ref.add_subplot().plot(x, y)\n\n    # Trigger subslicing in the test figure.\n    fig_test.canvas.draw()\n", "def test_set_markevery_invalid_input():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3])\n    with pytest.raises(ValueError):\n        line.set_markevery((0, 1, 2))\n    with pytest.raises(ValueError):\n        line.set_markevery(\"invalid\")\n    with pytest.raises(TypeError):\n        line.set_markevery(1.5, 2)\n", "def test_axline_transform():\n    fig, ax = plt.subplots()\n    line1 = ax.axline((.1, .1), slope=0.6)\n    line2 = ax.axline((.1, .1), (.8, .4))\n    # Testing get_transform method.\n    # Should return a Transform instance.\n    assert isinstance(line1.get_transform(), mtransforms.Transform)\n    assert isinstance(line2.get_transform(), mtransforms.Transform)\n", "def test_axline_transform(fig_test, fig_ref):\n    # Testing axline with custom transform\n    fig_test.add_subplot().axline((0.2, 0.3), (0.5, 0.8))\n    fig_ref.add_subplot().plot([0.2, 0.5], [0.3, 0.8])\n"], "sample_563": ["def test_offsetbox_repr():\n    ob = OffsetBox()\n    assert repr(ob) == \"OffsetBox()\"\n", "def test_auxtransformbox():\n    fig, ax = plt.subplots()\n    aux_transform = mtransforms.Affine2D().translate(10, 20)\n    atb = AuxTransformBox(aux_transform)\n    text = TextArea(\"Hello\")\n    atb.add_artist(text)\n    ax.add_artist(atb)\n    fig.canvas.draw()\n    assert not fig.stale\n    assert_allclose(text.get_window_extent().extents, (10, 0, 40, 12))\n", "def test_annotationbbox_with_nondefault_figure():\n    fig1, ax1 = plt.subplots()\n    fig2, ax2 = plt.subplots()\n\n    ab = AnnotationBbox(DrawingArea(20, 20, 0, 0, clip=True), (0.5, 0.5),\n                        xycoords='data')\n    ax2.add_artist(ab)\n\n    assert ab.figure == fig2\n\n    # switch figure of axes, make sure ab still uses original figure\n    ax2.figure = fig1\n    assert ab.figure == fig2\n", "def test_offsetbox_set_offset():\n    ob = OffsetBox()\n    xy = (10, 20)\n    ob.set_offset(xy)\n    assert ob.get_offset() == xy\n\n    # test setting offset with callable\n        return (width, height)\n\n    ob.set_offset(get_offset)\n    assert ob._offset == get_offset\n\n    # test that invalid inputs raise errors\n    with pytest.raises(TypeError):\n        ob.set_offset('invalid')\n\n    with pytest.raises(ValueError):\n        ob.set_offset((1, 2, 3))\n", "def test_offsetbox_offset():\n    # Test offset is calculated correctly for different coordinate systems.\n    fig, ax = plt.subplots()\n    box = AnchoredOffsetbox('upper left', child=DrawingArea(10, 10))\n    ax.add_artist(box)\n    ax.set_xlim((0, 1))\n    ax.set_ylim((0, 1))\n\n    # data coordinates\n    box.loc = 'upper right'\n    box.set_offset((0.5, 0.5))\n    assert_allclose(box.get_offset(), (60, -20))\n\n    # axes fraction\n    box.set_bbox_to_anchor((0.5, 0.5), transform=ax.transAxes)\n    assert_allclose(box.get_offset(), (30, -30))\n\n    # display coordinates\n    box.set_bbox_to_anchor((100, 100), transform=None)\n    assert_allclose(box.get_offset(), (-40, -120))\n"], "sample_565": ["def test_inset_locator_bounding_boxes():\n    fig, ax = plt.subplots(figsize=[5, 4])\n\n    # prepare the demo image\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n\n    ax.imshow(Z, extent=extent, interpolation=\"nearest\", origin=\"lower\")\n\n    # inset axes with different bounding box parameters\n    axins1 = inset_axes(ax, width=1., height=1., bbox_to_anchor=(200, 100), loc=3)\n    axins2 = inset_axes(ax, width=\"40%\", height=\"30%\", bbox_to_anchor=(0.4, 0.5))\n    axins3 = inset_axes(ax, width=1., height=1., bbox_to_anchor=(0.7, 0.8))\n\n    for axin in [axins1, axins2, axins3]:\n        axin.imshow(Z, extent=extent, interpolation=\"nearest\", origin=\"lower\")\n        axin.yaxis.get_major_locator().set_params(nbins=7)\n        axin.xaxis.get_major_locator().set_params(nbins=7)\n        axin.set_xlim(-1.5, -0.9)\n        axin.set_ylim(-2.5, -1.9)\n\n    plt.xticks(visible=False)\n    plt.yticks(visible=False)\n", "def test_inset_axes_locator():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, width=1.3, height=0.9)\n    assert isinstance(inset_ax.get_axes_locator(), AnchoredSizeLocator)\n    zoomed_ax = zoomed_inset_axes(ax, zoom=2.5, loc='lower right')\n    assert isinstance(zoomed_ax.get_axes_locator(), AnchoredZoomLocator)\n    ip = InsetPosition(ax, [0.2, 0.25, 0.5, 0.4])\n    ax_ins = plt.axes([0, 0, 1, 1])\n    ax_ins.set_axes_locator(ip)\n    assert ax_ins.get_axes_locator() is ip\n", "def test_anchored_size_locator():\n    fig = plt.figure(figsize=(3, 3))\n    fig1, fig2 = fig.subfigures(nrows=2, ncols=1)\n\n    ax = fig1.subplots()\n    ax.set(aspect=1, xlim=(-15, 15), ylim=(-20, 5))\n    ax.set(xticks=[], yticks=[])\n\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n\n    axins = inset_axes(ax, width=\"40%\", height=\"30%\", loc=\"upper left\")\n    axins.set(xticks=[], yticks=[])\n\n    axins.imshow(Z, extent=extent, origin=\"lower\")\n", "def test_inset_axes_with_zoomed_inset():\n    fig, ax = plt.subplots(figsize=[5, 4])\n\n    # prepare the demo image\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n    Z2 = np.zeros((150, 150))\n    ny, nx = Z.shape\n    Z2[30:30+ny, 30:30+nx] = Z\n\n    ax.imshow(Z2, extent=extent, interpolation=\"nearest\",\n              origin=\"lower\")\n\n    axins = inset_axes(ax, width=1., height=1., bbox_to_anchor=(1, 1),\n                       bbox_transform=ax.transAxes)\n\n    axins.imshow(Z2, extent=extent, interpolation=\"nearest\",\n                 origin=\"lower\")\n    axins.yaxis.get_major_locator().set_params(nbins=7)\n    axins.xaxis.get_major_locator().set_params(nbins=7)\n    # sub region of the original image\n    x1, x2, y1, y2 = -1.5, -0.9, -2.5, -1.9\n    axins.set_xlim(x1, x2)\n    axins.set_ylim(y1, y2)\n\n    plt.xticks(visible=False)\n    plt.yticks(visible=False)\n\n    # draw a bbox of the region of the inset axes in the parent axes and\n    # connecting lines between the bbox and the inset axes area\n    mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n\n    axins_zoom = zoomed_inset_axes(axins, zoom=2, loc='upper right')\n    axins_zoom.imshow(Z2, extent=extent, interpolation=\"nearest\",\n                      origin=\"lower\")\n    axins_zoom.yaxis.get_major_locator().set_params(nbins=7)\n    axins_zoom.xaxis.get_major_locator().set_params(nbins=7)\n    axins_zoom.set_xlim(-1.2, -1.1)\n    axins_zoom.set_ylim(-2.2, -2.1)\n\n    mark_inset(axins, axins_zoom, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n", "def test_inset_axes_with_rectangle():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, width=1, height=1, loc='upper right')\n    inset_ax.set_xlim(0, 1)\n    inset_ax.set_ylim(0, 1)\n    rect = Rectangle((0.2, 0.2), 0.5, 0.5, fill=False)\n    inset_ax.add_patch(rect)\n    fig.canvas.draw()\n    assert inset_ax.get_xlim() == (0, 1)\n    assert inset_ax.get_ylim() == (0, 1)\n    assert rect.get_extents().extents == (0.2, 0.7, 0.2, 0.7)\n"], "sample_566": ["def test_subfigure_suptitle(fig_test, fig_ref):\n    fig_test.subplots()\n    fig_test.suptitle(\"Figure suptitle\")\n\n    fig_ref.subplots()\n    fig_ref.text(0.5, 0.98, \"Figure suptitle\",\n                 transform=fig_ref.transFigure,\n                 verticalalignment=\"top\",\n                 horizontalalignment=\"center\")\n", "def test_get_constrained_layout():\n    fig = plt.figure(layout='constrained')\n    assert fig.get_constrained_layout() is True\n\n    fig = plt.figure(layout='compressed')\n    assert fig.get_constrained_layout() is True\n\n    fig = plt.figure(layout='tight')\n    assert fig.get_constrained_layout() is False\n\n    fig = plt.figure(layout=None)\n    assert fig.get_constrained_layout() is False\n", "def test_savefig_metadata_supported():\n    fig = Figure()\n    for fmt in ['png', 'pdf', 'svg']:\n        with io.BytesIO() as file:\n            fig.savefig(file, format=fmt, metadata={'Author': 'matplotlib'})\n            file.seek(0)\n            if fmt == 'png':\n                img = Image.open(file)\n                assert img.info['Author'] == 'matplotlib'\n            elif fmt == 'pdf':\n                # Unfortunately, PyPDF2 does not support reading file-like objects.\n                # Thus we have to write the PDF to a temporary file first.\n                import tempfile\n                with tempfile.NamedTemporaryFile(suffix='.pdf') as tmp:\n                    tmp.write(file.read())\n                    import PyPDF2\n                    pdf = PyPDF2.PdfFileReader(tmp.name)\n                    assert pdf.getDocumentInfo()['/Author'] == 'matplotlib'\n            elif fmt == 'svg':\n                import xml.etree.ElementTree as ET\n                svg = ET.parse(file)\n                assert svg.find('.//{http://purl.org/dc/elements/1.1/}Creator').text == 'matplotlib'\n", "def test_subfigure_repr():\n    fig = plt.figure(figsize=(10, 20), dpi=10)\n    subfig = fig.subfigures(1)[0]\n    assert repr(subfig) == \"<SubFigure size 100x200 with 0 Axes>\"\n", "def test_subfigure_repr():\n    fig = plt.figure(figsize=(10, 20), dpi=10)\n    subfig = fig.subfigures(1, 2)[0]\n    assert repr(subfig) == \"<SubFigure size 500x1000 with 0 Axes>\"\n"], "sample_567": ["def test_get_window_extent_with_linespacing():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"foo\\nbar\", linespacing=2)\n    fig.canvas.draw()\n\n    extent1 = text.get_window_extent()\n    extent2 = text.get_window_extent(linespacing=3)\n\n    assert extent1.height != extent2.height\n", "def test_get_rotation_mode():\n    txt = Text(.5, .5, \"foo\")\n    assert txt.get_rotation_mode() == \"default\"\n    txt.set_rotation_mode(\"anchor\")\n    assert txt.get_rotation_mode() == \"anchor\"\n    txt.set_rotation_mode(None)\n    assert txt.get_rotation_mode() == \"default\"\n", "def test_text_bbox_clip_on():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"test\", clip_on=True)\n    assert text.get_clip_on()\n    text.set_clip_on(False)\n    assert not text.get_clip_on()\n", "def test_text_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Hello\", bbox=dict(facecolor='red'))\n    assert isinstance(text.get_bbox_patch(), mpatches.FancyBboxPatch)\n    text.set_bbox(None)\n    assert text.get_bbox_patch() is None\n", "def test_text_wrap_rotated():\n    fig = plt.figure(figsize=(6, 4))\n    s = 'This is a very long text that should be wrapped multiple times.'\n    text = fig.text(0.7, 0.7, s, wrap=True, rotation=45)\n    fig.canvas.draw()\n    assert len(text._get_wrapped_text().split('\\n')) > 1\n"], "sample_568": ["def test_collection3d_zorder(fig_test, fig_ref):\n    fig_test.subplots(subplot_kw={\"projection\": \"3d\"})\n    x = [1, 2, 3]\n    y = [1, 2, 3]\n    z = [1, 2, 3]\n    line1 = art3d.Line3DCollection([np.column_stack([x, y, z])], colors='r', zorder=1)\n    line2 = art3d.Line3DCollection([np.column_stack([x, y, [i + 0.1 for i in z]])], colors='b', zorder=2)\n\n    ax = fig_test.get_axes()[0]\n    ax.add_collection3d(line1)\n    ax.add_collection3d(line2)\n\n    fig_ref.subplots(subplot_kw={\"projection\": \"3d\"})\n    ax = fig_ref.get_axes()[0]\n    ax.plot(x, y, z, color='r')\n    ax.plot(x, y, [i + 0.1 for i in z], color='b')\n", "def test_scatter3d_cmap(fig_test, fig_ref):\n    x, y, z = np.random.rand(3, 100)\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.scatter(x, y, z, c=z, cmap='viridis')\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.scatter(x, y, z, c=plt.cm.viridis(z))\n", "def test_plot_surface_alpha():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Create a surface with alpha values\n    x = np.linspace(-1, 1, 100)\n    y = np.linspace(-1, 1, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + Y**2\n    A = np.abs(Z) / np.max(np.abs(Z))  # Alpha values based on Z\n\n    ax.plot_surface(X, Y, Z, facecolors=mpl.cm.plasma(A), alpha=0.5)\n\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(0, 2)\n", "def test_collection_2d_to_3d_modification(fig_test, fig_ref):\n    # Test that modifying a 2D collection converted to 3D works.\n    circle = Circle((0, 0))\n    col = PolyCollection([circle], facecolor='C0', edgecolor='C1')\n    art3d.poly_collection_2d_to_3d(col, zs=0, zdir='z')\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(col)\n    col.set_facecolor('C2')\n    col.set_edgecolor('C3')\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    circle = Circle((0, 0))\n    col = PolyCollection([circle], facecolor='C2', edgecolor='C3')\n    art3d.poly_collection_2d_to_3d(col, zs=0, zdir='z')\n    ax_ref.add_collection3d(col)\n", "def test_line3d_set_data_3d(fig_test, fig_ref):\n    ax = fig_test.add_subplot(projection='3d')\n    line = art3d.Line3D([], [], [])\n    ax.add_artist(line)\n    line.set_data_3d([1, 2], [3, 4], [5, 6])\n\n    ax = fig_ref.add_subplot(projection='3d')\n    line = art3d.Line3D([1, 2], [3, 4], [5, 6])\n    ax.add_artist(line)\n"], "sample_569": ["def test_regplot_ylim(self):\n\n    f, ax = plt.subplots()\n    x, y1, y2 = np.random.randn(3, 50)\n    lm.regplot(x=x, y=y1)\n    lm.regplot(x=x, y=y2)\n    line1, line2 = ax.lines\n    assert np.array_equal(line1.get_ydata(), line2.get_ydata())\n", "def test_lmplot_hue_order(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", hue=\"h\", data=self.df, hue_order=[\"y\", \"x\"])\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 2\n    assert len(ax.collections) == 4\n\n    # Check that the hue order is respected\n    assert ax.collections[0].get_facecolors()[0, :3] == color_palette()[1]\n    assert ax.collections[2].get_facecolors()[0, :3] == color_palette()[0]\n", "def test_lmplot_x_estimator(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, x_estimator=np.mean)\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, np.unique(self.df.x))\n    npt.assert_array_almost_equal(y, self.df.groupby(\"x\")[\"y\"].mean())\n", "def test_residplot_x_is_categorical(self):\n\n    df = pd.DataFrame({\n        \"x\": np.tile([\"a\", \"b\", \"c\"], 10),\n        \"y\": np.random.randn(30)\n    })\n\n    with pytest.raises(TypeError):\n        lm.residplot(x=\"x\", y=\"y\", data=df)\n", "def test_lmplot_x_partial(self):\n\n    df = pd.DataFrame({\n        'x': np.random.randn(100),\n        'y': np.random.randn(100),\n        'z': np.random.randn(100)\n    })\n\n    g1 = lm.lmplot(x=\"x\", y=\"y\", data=df)\n    g2 = lm.lmplot(x=\"x\", y=\"y\", data=df, x_partial=\"z\")\n\n    assert not np.array_equal(g1.axes[0, 0].lines[0].get_ydata(),\n                              g2.axes[0, 0].lines[0].get_ydata())\n"], "sample_570": ["def test_units_in_bootstrapping(self, long_df):\n\n    agg = EstimateAggregator(\"mean\", \"ci\", n_boot=100, units=\"x\")\n    out_with_units = agg(long_df, \"y\")\n\n    agg = EstimateAggregator(\"mean\", \"ci\", n_boot=100)\n    long_df_no_units = long_df.drop(columns=[\"units\"])\n    out_without_units = agg(long_df_no_units, \"y\")\n\n    assert out_with_units[\"ymin\"] != out_without_units[\"ymin\"]\n    assert out_with_units[\"ymax\"] != out_without_units[\"ymax\"]\n", "def test_cached_bin_params(self, x):\n\n    h = Histogram()\n    h.define_bin_params(x)\n    _, edges = h(x[x > 0])\n    assert_array_equal(edges, h.bin_kws[\"bins\"])\n", "def test_cached_bin_params(self, x):\n\n    h = Histogram()\n    h.define_bin_params(x)\n    _, edges = h(x[x > 0])\n    assert_array_equal(edges, h.bin_kws[\"bins\"])\n", "def test_units_option_in_EstimateAggregator(long_df):\n    # Test that EstimateAggregator correctly handles the \"units\" option\n\n    agg = EstimateAggregator(\"mean\", \"ci\", n_boot=1000, seed=0, units=\"y\")\n    out = agg(long_df, \"y\")\n\n    assert \"ymin\" in out\n    assert \"ymax\" in out\n\n    # Check that error bars are estimated separately for each unit\n    unique_units = long_df[\"units\"].unique()\n    for unit in unique_units:\n        unit_df = long_df[long_df[\"units\"] == unit]\n        unit_agg = EstimateAggregator(\"mean\", \"ci\", n_boot=1000, seed=0)\n        unit_out = unit_agg(unit_df, \"y\")\n\n        unit_ymin = out.loc[out.index.get_level_values(\"units\") == unit, \"ymin\"]\n        unit_ymax = out.loc[out.index.get_level_values(\"units\") == unit, \"ymax\"]\n\n        assert_array_almost_equal(unit_ymin, unit_out[\"ymin\"])\n        assert_array_almost_equal(unit_ymax, unit_out[\"ymax\"])\n", "    def test_units_option(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", n_boot=1000, units=\"x\")\n        out = agg(long_df, \"y\")\n\n        assert out[\"y\"] == long_df.groupby(\"x\")[\"y\"].mean().mean()\n        assert out[\"ymin\"] < out[\"y\"]\n        assert out[\"ymax\"] > out[\"y\"]\n"], "sample_571": ["def test_lmplot_hue_order(self):\n\n    hue_order = list(\"yx\")\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", hue_order=hue_order)\n    ax = g.axes[0, 0]\n    assert [collection.get_facecolors()[0, :3] for collection in ax.collections[:2]] == [\n        color_palette(n_colors=2)[i] for i in [1, 0]\n    ]\n", "def test_lmplot_hue_order(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", hue=\"h\", data=self.df, hue_order=[\"y\", \"x\"])\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 2\n    assert len(ax.collections) == 4\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, self.df.x[self.df.h == \"y\"])\n    npt.assert_array_equal(y, self.df.y[self.df.h == \"y\"])\n\n    x, y = ax.collections[2].get_offsets().T\n    npt.assert_array_equal(x, self.df.x[self.df.h == \"x\"])\n    npt.assert_array_equal(y, self.df.y[self.df.h == \"x\"])\n", "def test_lmplot_scatter_kws_alpha(self):\n\n    f, ax = plt.subplots()\n    color = np.array([[0.3, 0.8, 0.5, 0.5]])\n    lm.lmplot(x=\"x\", y=\"y\", data=self.df,\n              scatter_kws={'color': color}, ax=ax)\n    assert ax.collections[0]._alpha is None\n    assert ax.collections[0]._facecolors[0, 3] == 0.5\n\n    f, ax = plt.subplots()\n    color = np.array([[0.3, 0.8, 0.5]])\n    lm.lmplot(x=\"x\", y=\"y\", data=self.df,\n              scatter_kws={'color': color}, ax=ax)\n    assert ax.collections[0]._alpha == 0.8\n\n    f, ax = plt.subplots()\n    color = np.array([[0.3, 0.8, 0.5]])\n    lm.lmplot(x=\"x\", y=\"y\", data=self.df,\n              scatter_kws={'color': color, 'alpha': 0.4}, ax=ax)\n    assert ax.collections[0]._alpha == 0.4\n\n    f, ax = plt.subplots()\n    color = 'r'\n    lm.lmplot(x=\"x\", y=\"y\", data=self.df,\n              scatter_kws={'color': color}, ax=ax)\n    assert ax.collections[0]._alpha == 0.8\n\n    f, ax = plt.subplots()\n    alpha = .3\n    lm.lmplot(x=\"x\", y=\"y\", data=self.df, x_bins=5, fit_reg=False,\n              scatter_kws={\"alpha\": alpha}, ax=ax)\n    for line in ax.lines:\n        assert line.get_alpha() == alpha\n", "def test_regplot_x_estimator(self):\n\n    x, y = self.df.x, self.df.y\n    ax = lm.regplot(x=x, y=y, x_estimator=np.mean)\n    assert len(ax.lines) == 6  # 5 bins + regression line\n    assert len(ax.collections) == 2\n\n    with pytest.raises(ValueError):\n        lm.regplot(x=x, y=y, x_estimator=\"not a callable\")\n", "def test_lmplot_logx(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, logx=True)\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, self.df.x)\n    npt.assert_array_equal(y, self.df.y)\n"], "sample_572": ["    def test_univariate_discrete_data(self, rng):\n\n        x = rng.binomial(20, .5, 100)\n        kde = KDE()\n        density, support = kde(x)\n\n        assert support.min() <= x.min()\n        assert support.max() >= x.max()\n", "    def test_units_option(self, long_df):\n\n        units = np.repeat([1, 2, 3], len(long_df) // 3)\n        long_df[\"units\"] = units\n\n        agg = EstimateAggregator(\"mean\", \"ci\", n_boot=100000, seed=0, units=\"units\")\n        out = agg(long_df, \"y\")\n\n        assert out[\"ymin\"].notna().all()\n        assert out[\"ymax\"].notna().all()\n", "def test_units_in_boostrap(self, long_df):\n    # Test that the \"units\" parameter is passed to bootstrap when error_method is \"ci\"\n    agg = EstimateAggregator(\"mean\", \"ci\", n_boot=100000, seed=0)\n    out = agg(long_df, \"y\")\n    assert \"units\" not in agg.boot_kws\n\n    agg = EstimateAggregator(\"mean\", \"ci\", n_boot=100000, seed=0, units=\"some_column\")\n    out = agg(long_df.assign(some_column=np.random.choice([1, 2, 3], size=len(long_df))), \"y\")\n    assert agg.boot_kws[\"units\"] == \"some_column\"\n", "    def test_units(self, long_df):\n\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"x\", units=\"cm\")\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert out.attrs[\"units\"] == \"cm\"\n", "    def test_units_single_level(self, long_df):\n\n        units = [\"a\", \"b\", \"c\"] * 10\n        long_df[\"units\"] = units\n\n        agg = EstimateAggregator(\"mean\", (\"se\", 2))\n        out = agg(long_df, \"x\")\n\n        assert out[\"x\"] == long_df.groupby(\"units\")[\"x\"].mean().mean()\n        assert out[\"xmin\"] == (long_df.groupby(\"units\")[\"x\"].mean().mean() - \n                               2 * long_df.groupby(\"units\")[\"x\"].sem().mean())\n        assert out[\"xmax\"] == (long_df.groupby(\"units\")[\"x\"].mean().mean() + \n                               2 * long_df.groupby(\"units\")[\"x\"].sem().mean())\n"], "sample_573": ["def test_insufficient_unique_x(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = np.repeat([0, 1], len(df) // 2)\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\", \"group\"])\n\n    for _, part in res.groupby(\"group\"):\n        assert part.empty\n", "def test_insufficient_data(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df = df.head(2)  # Not enough unique x values for a 2nd order polyfit\n    res = PolyFit()(df[[\"x\", \"y\"]], groupby, \"x\", {})\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n    assert res.empty\n", "def test_insufficient_unique_values(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df.loc[:, \"x\"] = np.repeat(1, len(df))\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n    \n    assert_array_equal(res.columns, [\"x\", \"y\", \"group\"])\n    assert res.empty\n", "def test_insufficient_data(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df.loc[df[\"group\"] == \"x\", \"x\"] = 1\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n    for _, part in res.groupby(\"group\"):\n        if part[\"group\"].iloc[0] == \"x\":\n            assert_array_equal(part[\"x\"], [])\n            assert_array_equal(part[\"y\"], [])\n        else:\n            gridsize = 100\n            grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), gridsize)\n            assert_array_equal(part[\"x\"], grid)\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n", "def test_order_too_high(self, df):\n\n    groupby = GroupBy([\"group\"])\n    order = df[\"x\"].nunique()\n    res = PolyFit(order=order)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n    assert res.empty\n"], "sample_574": ["    def test_set_view_interval(self):\n\n        ax = PseudoAxis(mpl.scale.LinearScale())\n        ax.set_view_interval(1, 10)\n        assert ax.get_view_interval() == (1, 10)\n", "    def x(self):\n        return pd.Series([\"a\", \"c\", \"b\", \"c\"], name=\"x\")\n", "    def test_identity_scale(self):\n        s = Scale._identity()\n        assert s._pipeline == []\n        assert s._spacer is None\n        assert s._legend is None\n        assert s._matplotlib_scale is None\n", "    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n", "def test_tick_at(self, x):\n\n    locs = [.2, .5, .9]\n    a = self.setup_ticks(x, at=locs)\n    assert_array_equal(a.major.locator(), locs)\n\n    # TODO: Add more assertions to check the tick labels and other properties\n    # For example:\n    # assert_array_equal(a.major.formatter.format_ticks(locs), [\"0.2\", \"0.5\", \"0.9\"])\n"], "sample_575": ["    def test_coordinate(self):\n\n        scale = Continuous()._setup(pd.Series([1, 2, 3], name=\"x\"), Coordinate())\n        ax = PseudoAxis(scale._matplotlib_scale)\n        ax.set_view_interval(0, 10)\n        assert_array_equal(ax.major.locator(), [0, 2.5, 5, 7.5, 10])\n", "    def x(self):\n        return pd.Series([\"a\", \"c\", \"b\", \"c\"], name=\"x\")\n", "    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n", "    def test_identity(self):\n\n        s = Scale._identity()\n        assert s._pipeline == []\n        assert s._spacer is None\n        assert s._legend is None\n        assert s._matplotlib_scale is None\n", "    def test_identity_scale(self):\n        s = Scale._identity()\n        x = pd.Series([1, 2, 3], name=\"x\")\n        assert_series_equal(s(x), x)\n"], "sample_576": ["def test_facet_wrap_suppresses_axis_labels(self, long_df):\n\n    p = Plot(long_df).facet(\"a\", wrap=2).plot()\n    axs = p._figure.axes\n\n    for ax in axs[:-2]:\n        assert not ax.get_xlabel()\n\n    for ax in axs[2:]:\n        assert not ax.get_ylabel()\n", "    def test_plotter_repr_png(self):\n\n        p = Plot().plot()\n        data, metadata = p._repr_png_()\n        img = Image.open(io.BytesIO(data))\n        assert img.format == \"PNG\"\n        assert sorted(metadata) == [\"height\", \"width\"]\n", "    def test_plot_on_with_target_axes(self, long_df):\n\n        ax = mpl.figure.Figure().subplots()\n        p = Plot(long_df, x=\"x\", y=\"y\").on(ax).add(MockMark())\n        assert not hasattr(p, \"_figure\")\n        p = p.plot()\n        assert p._figure is ax.figure\n", "    def test_layout_engine(self, engine):\n\n        p = Plot().layout(engine=engine).plot()\n        assert p._layout_spec[\"engine\"] == engine\n", "    def test_target_type_check(self):\n\n        p = Plot()\n        with pytest.raises(TypeError, match=\"The `Plot.on` target must be an instance of\"):\n            p.on(object())\n"], "sample_577": ["    def test_repr_png(self, long_df):\n\n        m = MockMark()\n        p = Plot(long_df, x=\"x\", y=\"y\").add(m).plot()\n        data, metadata = p._repr_png_()\n        img = Image.open(io.BytesIO(data))\n\n        assert not hasattr(p, \"_figure\")\n        assert isinstance(data, bytes)\n        assert img.format == \"PNG\"\n        assert sorted(metadata) == [\"height\", \"width\"]\n", "def test_label_dict(self, long_df):\n\n    label = {\"a\": \"A\", \"b\": \"B\"}\n    p = Plot(long_df).facet(col=\"a\", row=\"b\").label(col=label, row=label).plot()\n    for subplot in p._subplots:\n        ax = subplot[\"ax\"]\n        title_parts = ax.get_title().split(\" | \")\n        col, row = subplot[\"col\"], subplot[\"row\"]\n        assert title_parts == [f\"{label[col]} {col}\", f\"{label[row]} {row}\"]\n", "def test_figure_size_default(self):\n\n    p = Plot().plot()\n    assert p._figure.get_size_inches() == (6.4, 4.8)\n", "    def test_plotter_repr(self):\n\n        p = Plot().plot()\n        assert repr(p) == f\"Plotter(pyplot={p._pyplot}, theme={p._theme})\"\n", "    def test_unsupported_operand_type(self, long_df):\n        p = Plot(long_df)\n        with pytest.raises(TypeError, match=\"Unsupported operand type\"):\n            p + \"a\"\n"], "sample_578": ["def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    assert len(paths) == len(x)\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 0] == pytest.approx(x[i] - .5)\n        assert verts[1, 0] == pytest.approx(x[i] + .5)\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == y[i] + baseline\n", "def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == y[i] + baseline\n", "def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == pytest.approx(baseline)\n        assert verts[3, 1] == pytest.approx(y[i] + baseline)\n", "def test_baseline(self, x, y):\n\n    baseline = 5\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    assert len(paths) == len(x)\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 0] == pytest.approx(x[i] - .5)\n        assert verts[1, 0] == pytest.approx(x[i] + .5)\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == y[i] + baseline\n", "def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == y[i] + baseline\n"], "sample_579": ["def test_clustermap_default_colormap():\n    df = self.df_norm.copy()\n    g = mat.clustermap(df)\n    cmap = g.ax_heatmap.collections[0].get_cmap()\n    assert cmap.name == \"rocket\"\n", "def test_clustermap_masked_values():\n    df = self.df_norm.copy()\n    df.iloc[0, 0] = np.nan\n\n    g = mat.clustermap(df)\n    npt.assert_array_equal(g.data2d.index, g.mask.index)\n    npt.assert_array_equal(g.data2d.columns, g.mask.columns)\n\n    assert g.mask.iloc[0, 0]\n", "def test_clustermap_dataframe_index_name():\n    df = self.df_norm.copy()\n    df.index.name = \"Index Name\"\n    g = mat.clustermap(df)\n    assert g.ax_heatmap.get_ylabel() == \"Index Name\"\n", "def test_clustermap_savefig_tight_layout(self):\n    cg = mat.clustermap(self.df_norm, **self.default_kws)\n    cg.savefig(tempfile.NamedTemporaryFile(), format='png', bbox_inches='tight')\n    # Check if the dendrograms are still visible after saving with tight layout\n    assert len(cg.ax_row_dendrogram.collections[0].get_paths()) > 0\n    assert len(cg.ax_col_dendrogram.collections[0].get_paths()) > 0\n", "def test_clustermap_cbar_kws(self):\n    g = mat.clustermap(self.df_norm, cbar_kws=dict(shrink=0.5))\n    assert g.ax_cbar.get_position().height < 0.2\n"], "sample_580": ["def test_variable_type_edge_cases():\n\n    # Test all-na data with different dtypes\n    s = pd.Series([np.nan, np.nan], dtype=\"float64\")\n    assert variable_type(s) == \"numeric\"\n    s = pd.Series([np.nan, np.nan], dtype=\"Int64\")\n    assert variable_type(s) == \"numeric\"\n    s = pd.Series([pd.NA, pd.NA], dtype=\"object\")\n    assert variable_type(s) == \"numeric\"\n\n    # Test empty data\n    s = pd.Series([], dtype=\"float64\")\n    assert variable_type(s) == \"numeric\"\n    s = pd.Series([], dtype=\"object\")\n    assert variable_type(s) == \"numeric\"\n\n    # Test single-element data\n    s = pd.Series([1])\n    assert variable_type(s) == \"numeric\"\n    s = pd.Series([\"a\"])\n    assert variable_type(s) == \"categorical\"\n\n    # Test mixed-type data\n    s = pd.Series([1, \"a\", 2.5])\n    assert variable_type(s) == \"categorical\"\n", "def test_variable_type_strict_boolean():\n\n    s = pd.Series([True, False, False])\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([1, 0, 0])\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"numeric\"\n\n    s = pd.Series([True, False, False], dtype=\"category\")\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"categorical\"\n\n    s = pd.Series([pd.NA, True, False], dtype=\"boolean\")\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n", "def test_variable_type_strict_boolean():\n\n    s = pd.Series([True, False, True])\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([1, 0, 1], dtype='int64')\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"numeric\"\n\n    s = pd.Series([True, False, True], dtype='object')\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"numeric\"\n\n    s = pd.Series(pd.array([True, False, True], dtype='boolean))\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n", "def test_variable_type_all_na_with_non_numeric_dtype():\n\n    s = pd.Series([np.nan, np.nan], dtype=object)\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([np.nan, np.nan], dtype=\"category\")\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([np.nan, np.nan], dtype=\"datetime64[ns]\")\n    assert variable_type(s) == \"numeric\"\n", "def test_variable_type_strict_boolean():\n\n    s = pd.Series([True, False], dtype='boolean')\n    assert variable_type(s) == \"numeric\"\n    assert variable_type(s, boolean_type=\"boolean\") == \"boolean\"\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([True, False], dtype='bool)\n    assert variable_type(s) == \"numeric\"\n    assert variable_type(s, boolean_type=\"boolean\") == \"boolean\"\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([1, 0])\n    assert variable_type(s, boolean_type=\"boolean\") == \"boolean\"\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"numeric\"\n"], "sample_581": ["def test_blueprint_registration_options(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/foo\")\n        return \"foo\"\n\n    app.register_blueprint(bp, options={\"foo\": \"bar\"})\n\n    assert client.get(\"/foo\").data == b\"foo\"\n    assert bp.options == {\"foo\": \"bar\"}\n", "def test_blueprint_url_processor_registration(app, client) -> None:\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.url_defaults\n        values[\"lang\"] = \"en\"\n\n    @bp.url_value_preprocessor\n        values.pop(\"lang\")\n\n    @bp.route(\"/<lang>/\")\n        return \"index\"\n\n    app.register_blueprint(bp)\n\n    response = client.get(\"/en/\")\n    assert response.status_code == 200\n\n    with app.test_request_context(\"/en/\"):\n        assert flask.url_for(\"bp.index\") == \"/en/\"\n", "def test_blueprint_url_value_preprocessor_exception(app, client) -> None:\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.url_value_preprocessor\n        raise Exception(\"Test exception\")\n\n    @bp.route(\"/<int:id>\")\n        return \"View\"\n\n    app.register_blueprint(bp)\n\n    with pytest.raises(Exception, match=\"Test exception\"):\n        client.get(\"/1\")\n", "def test_blueprint_cli_group(app, cli_runner) -> None:\n    bp = flask.Blueprint(\"bp\", __name__, cli_group=None)\n    app.register_blueprint(bp)\n\n    @bp.cli.command()\n        click.echo(\"Hello World!\")\n\n    result = cli_runner.invoke(cmd)\n    assert result.exit_code == 0\n    assert result.output == \"Hello World!\\n\"\n\n    bp2 = flask.Blueprint(\"bp2\", __name__, cli_group=\"sub\")\n    app.register_blueprint(bp2)\n\n    @bp2.cli.command()\n        click.echo(\"Hello World!\")\n\n    result = cli_runner.invoke(cmd)\n    assert result.exit_code == 0\n    assert result.output == \"Hello World!\\n\"\n\n    result = cli_runner.invoke(sub_cmd, [\"--help\"])\n    assert result.exit_code == 0\n    assert \"(sub)\" in result.output\n", "def test_blueprint_name_prefix(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/test\")\n        return \"test\"\n\n    app.register_blueprint(bp, name_prefix=\"prefix.\")\n\n    assert client.get(\"/test\").data == b\"test\"\n    assert app.view_functions[\"prefix.bp.test\"] == test\n"], "sample_582": ["def test_cli_group_name_conflict(app):\n    \"\"\"Test that registering a CLI group with an existing name raises an error.\"\"\"\n    bp1 = Blueprint(\"blue\", __name__, cli_group=\"conflict\")\n    bp2 = Blueprint(\"green\", __name__, cli_group=\"conflict\")\n\n    app.register_blueprint(bp1)\n\n    with pytest.raises(ValueError):\n        app.register_blueprint(bp2)\n", "def test_cli_with_appcontext(app):\n    \"\"\"Test that a command with the @with_appcontext decorator can access\n    the current app instance.\"\"\"\n    called = [False]\n\n    @app.cli.command()\n    @with_appcontext\n        assert current_app._get_current_object() is app\n        called[0] = True\n\n    runner = app.test_cli_runner()\n    runner.invoke(args=[\"test_command\"])\n    assert called[0]\n", "def test_cli_group_merge(app):\n    \"\"\"Test blueprint commands can be merged into the application's CLI\"\"\"\n    bp = Blueprint(\"bp\", __name__, cli_group=None)\n    app.register_blueprint(bp)\n\n    @app.cli.command(\"app_cmd\")\n        click.echo(\"app_result\")\n\n    @bp.cli.command(\"bp_cmd\")\n        click.echo(\"bp_result\")\n\n    result = app.test_cli_runner().invoke(args=[\"app_cmd\"])\n    assert \"app_result\" in result.output\n\n    result = app.test_cli_runner().invoke(args=[\"bp_cmd\"])\n    assert \"bp_result\" in result.output\n", "def test_cli_appctx_push_pop(runner):\n    \"\"\"Test that the app context is pushed and popped when running a CLI command.\"\"\"\n    app = Flask(__name__)\n\n    @app.cli.command()\n        click.echo(current_app.name)\n\n    cli = FlaskGroup(create_app=lambda: app)\n    result = runner.invoke(cli, [\"check\"], standalone_mode=False)\n    assert result.return_value is None\n    assert not current_app:\n", "def test_cli_help(app):\n    \"\"\"Test help messages for commands and groups\"\"\"\n    app_runner = app.test_cli_runner()\n\n    # Test app-level help message\n    result = app_runner.invoke(args=[\"--help\"])\n    assert \"Usage: flask [OPTIONS] COMMAND [ARGS]...\" in result.output\n\n    # Test command-level help message\n    result = app_runner.invoke(args=[\"run\", \"--help\"])\n    assert \"Usage: flask run [OPTIONS]\" in result.output\n\n    # Test group-level help message\n    bp = Blueprint(\"blue\", __name__, cli_group=\"blue\")\n    @bp.cli.command(\"cmd\")\n        pass\n    app.register_blueprint(bp)\n    result = app_runner.invoke(args=[\"blue\", \"--help\"])\n    assert \"Usage: flask blue [OPTIONS] COMMAND [ARGS]...\" in result.output\n"], "sample_583": ["def test_decompose_indexer_vectorized_with_slice():\n    shape = (10, 20, 30)\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), slice(None), np.array([0, 1, -1])))\n    backend_ind, np_ind = indexing.decompose_indexer(\n        indexer, shape, indexing.IndexingSupport.OUTER)\n    expected = indexing.OuterIndexer((np.array([0, 2]), slice(None), np.array([0, 1])))\n    assert backend_ind.tuple == expected.tuple\n    np.testing.assert_array_equal(np_ind.tuple[0], np.array([0, -1, 2]))\n    np.testing.assert_array_equal(np_ind.tuple[1], slice(None))\n    np.testing.assert_array_equal(np_ind.tuple[2], np.array([0, 1, -1]))\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = indexing.OuterIndexer((np.array([0, 0, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert actual.tuple == expected.tuple\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = indexing.VectorizedIndexer(\n        (np.array([0, 0, 2]), np.array([0, 1, 1])))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert actual.tuple == expected.tuple\n\n    indexer = indexing.BasicIndexer((-1,))\n    expected = indexing.BasicIndexer((0,))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert actual.tuple == expected.tuple\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = indexing.OuterIndexer((np.array([0, 0, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, indexing.OuterIndexer)\n    np.testing.assert_array_equal(expected.tuple[0], actual.tuple[0])\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = indexing.VectorizedIndexer(\n        (np.array([0, 0, 2]), np.array([0, 1, 0])))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, indexing.VectorizedIndexer)\n    np.testing.assert_array_equal(expected.tuple[0], actual.tuple[0])\n    np.testing.assert_array_equal(expected.tuple[1], actual.tuple[1])\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = indexing.OuterIndexer((np.array([0, 0, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, indexing.OuterIndexer)\n    np.testing.assert_array_equal(expected.tuple[0], actual.tuple[0])\n\n    indexer = indexing.VectorizedIndexer((np.array([0, -1, 2]), slice(None)))\n    expected = indexing.VectorizedIndexer((np.array([0, 0, 2]), slice(None)))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, indexing.VectorizedIndexer)\n    np.testing.assert_array_equal(expected.tuple[0], actual.tuple[0])\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = indexing.OuterIndexer((np.array([0, 0, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, indexing.OuterIndexer)\n    np.testing.assert_array_equal(expected.tuple[0], actual.tuple[0])\n\n    indexer = indexing.VectorizedIndexer((np.array([0, -1, 2]),))\n    expected = indexing.VectorizedIndexer((np.array([0, 0, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, indexing.VectorizedIndexer)\n    np.testing.assert_array_equal(expected.tuple[0], actual.tuple[0])\n"], "sample_584": ["    def test_empty_input(self):\n        assert_identical(Dataset(), combine_nested([], concat_dim='x'))\n", "def test_combine_nested_error_message(self):\n    objs = [Dataset({'x': [0], 'y': [0]}), Dataset({'y': [1], 'z': [1]})]\n    with pytest.raises(ValueError, match=\"These objects cannot be combined\"):\n        combine_nested(objs, concat_dim='x')\n", "    def test_combine_nd_invalid_input_type(self):\n        ds = create_test_data(0)\n        combined_ids = {(0,): ds}\n        with raises_regex(TypeError, 'concat_dims must be a list or tuple'):\n            _combine_nd(combined_ids, concat_dims='x', data_vars='all',\n                        coords='different', fill_value=dtypes.NA)\n", "def test_combine_nested_fill_value_edge_cases(self):\n    datasets = [Dataset({'a': ('x', [2, 3]), 'x': [1, 2]}),\n                Dataset({'a': ('x', [1, 2]), 'x': [0, 1]})]\n    with pytest.raises(TypeError, match=\"fill_value must be a scalar\"):\n        combine_nested(datasets, concat_dim='t', fill_value=[2])\n", "def test_combine_by_coords_with_empty_datasets(self):\n    objs = [Dataset(), Dataset({'x': [1]})]\n    actual = combine_by_coords(objs)\n    expected = Dataset({'x': [1]})\n    assert_identical(expected, actual)\n\n    objs = [Dataset({'x': [0]}), Dataset()]\n    actual = combine_by_coords(objs)\n    expected = Dataset({'x': [0]})\n    assert_identical(expected, actual)\n"], "sample_585": ["def test_groupby_reduce_dim_order():\n    array = xr.DataArray([1, 2, 3], [('x', [1, 2, 3]), ('y', [4, 5, 6])])\n    expected = xr.DataArray([6], [('x', [1])])\n    actual = array.groupby('x').sum('y')\n    assert_identical(expected, actual)\n", "def test_groupby_assign_coords():\n    array = xr.DataArray([1, 2, 3], [('x', [1, 2, 3])])\n    expected = array.assign_coords(y=('x', [4, 5, 6]))\n    actual = array.groupby('x').assign_coords(y=('x', [4, 5, 6]))\n    assert_identical(expected, actual)\n", "def test_groupby_reduce_dim_order():\n    # regression test for GH1685\n\n        array = xr.DataArray(\n            np.random.randn(3, 4, 5),\n            dims=['x', 'y', 'z'],\n            coords={'x': range(3), 'y': range(4), 'z': range(5)}\n        )\n        expected = array.reduce(np.sum, dim=reduce_dim)\n        actual = array.groupby(groupby_dim).reduce(np.sum, dim=reduce_dim)\n        assert_identical(expected, actual)\n\n    check_reduce_dim_order('x', 'y')\n    check_reduce_dim_order('y', 'x')\n    check_reduce_dim_order('x', ('y', 'z'))\n    check_reduce_dim_order(('x', 'y'), 'z')\n", "def test_da_groupby_fillna():\n    array = xr.DataArray([1, 2, np.nan, 4, np.nan, 6],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n\n    # Scalar fill value\n    expected = xr.DataArray([1, 2, 0, 4, 0, 6], [('x', [1, 1, 1, 2, 2, 2])])\n    actual = array.groupby('x').fillna(0)\n    assert_identical(expected, actual)\n\n    # Vector fill value\n    fill_value = xr.DataArray([0, 1], [('x', [1, 2])])\n    expected = xr.DataArray([1, 2, 0, 4, 1, 6], [('x', [1, 1, 1, 2, 2, 2])])\n    actual = array.groupby('x').fillna(fill_value)\n    assert_identical(expected, actual)\n", "def test_groupby_concat_dim_order():\n    # GH3374\n    da = xr.DataArray(\n        np.arange(6).reshape(2, 3),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": [\"a\", \"b\"], \"y\": [1, 2, 3]},\n    )\n    group = xr.DataArray([\"X\", \"X\"], dims=\"x)\n\n    result = da.groupby(group).sum()\n    expected = xr.DataArray([9], dims=\"group\", coords={\"group\": [\"X\"]})\n    assert_identical(result, expected)\n\n    # Check that the dimension order is preserved when grouping by multiple variables\n    da = xr.DataArray(\n        np.arange(12).reshape(2, 2, 3),\n        dims=[\"x\", \"z\", \"y\"],\n        coords={\"x\": [\"a\", \"b\"], \"z\": [10, 20], \"y\": [1, 2, 3]},\n    )\n    group1 = xr.DataArray([\"X\", \"X\"], dims=\"x\")\n    group2 = xr.DataArray([\"Z\", \"Z\"], dims=\"z\")\n\n    result = da.groupby((group1, group2)).sum()\n    expected = xr.DataArray(\n        [[18]], dims=(\"group_x\", \"group_z\"), coords={\"group_x\": [\"X\"], \"group_z\": [\"Z\"]}\n    )\n    assert_identical(result, expected)\n"], "sample_586": ["def test_concat_coords_conflict(self):\n    ds1 = Dataset({}, coords={\"x\": 0})\n    ds2 = Dataset({}, coords={\"x\": (\"y\", [1, 2])})\n    with raises_regex(ValueError, \"conflicting values\"):\n        concat([ds1, ds2], dim=\"z\", coords=\"minimal\")\n", "def test_concat_positions(self):\n    data = create_test_data()\n    split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n    positions = [0, 3]\n    actual = concat(split_data, dim=\"dim1\", positions=positions)\n    assert_identical(data, actual)\n\n    # Test with non-integer positions\n    positions = [0.0, 3.0]\n    with raises_regex(TypeError, \"positions must be integers\"):\n        concat(split_data, dim=\"dim1\", positions=positions)\n\n    # Test with incorrect number of positions\n    positions = [0, 3, 6]\n    with raises_regex(ValueError, \"length of positions must match\"):\n        concat(split_data, dim=\"dim1\", positions=positions)\n", "def test_concat_positions(self):\n    # Test concatenating with positions argument\n    data = create_test_data()\n    split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n    positions = [[0, 1, 2], [3, 4, 5]]\n    actual = concat(split_data, dim=\"dim1\", positions=positions)\n    assert_identical(data, actual)\n\n    # Test concatenating with overlapping positions\n    positions = [[0, 1, 2], [2, 3, 4]]\n    with raises_regex(ValueError, \"overlapping positions\"):\n        concat(split_data, dim=\"dim1\", positions=positions)\n\n    # Test concatenating with non-integer positions\n    positions = [[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]]\n    with raises_regex(TypeError, \"positions must be integer\"):\n        concat(split_data, dim=\"dim1\", positions=positions)\n", "def test_concat_positions(self):\n    # Test that the positions argument works as expected\n    data = create_test_data()\n    split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n    positions = [[0, 1, 2], [3, 4, 5]]\n    actual = concat(split_data, \"dim1\", positions=positions)\n    assert_identical(data, actual)\n\n    # Test that the positions argument raises an error if it's not the same length as the input datasets\n    with raises_regex(ValueError, \"Positions must be the same length as the input datasets\"):\n        concat(split_data, \"dim1\", positions=[[0, 1, 2]])\n\n    # Test that the positions argument raises an error if it's not a list of lists or integers\n    with raises_regex(ValueError, \"Positions must be a list of lists or integers\"):\n        concat(split_data, \"dim1\", positions=\"foo\")\n\n    # Test that the positions argument works with a single dataset\n    actual = concat([data], \"dim1\", positions=[range(data.dims[\"dim1\"])])\n    assert_identical(data, actual)\n", "def test_concat_empty_datasets(self):\n    # Test that concatenating empty datasets returns an empty dataset\n    ds1 = Dataset()\n    ds2 = Dataset()\n    actual = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset()\n    assert_identical(actual, expected)\n"], "sample_587": ["def test_merge_coord_dtype(self):\n    ds1 = xr.Dataset(coords={\"x\": [1, 2]})\n    ds2 = xr.Dataset(coords={\"x\": [2, 3.0]})  # float dtype\n    actual = xr.merge([ds1, ds2])\n    expected = xr.Dataset(coords={\"x\": [1, 2, 3.0]})  # should use float dtype\n    assert actual.identical(expected)\n", "def test_merge_override(self):\n    ds1 = xr.Dataset({\"x\": 0})\n    ds2 = xr.Dataset({\"x\": 1})\n    actual = ds1.merge(ds2, compat=\"override\")\n    assert ds1.identical(actual)\n\n    ds1 = xr.Dataset({\"x\": ((), 0, {\"foo\": \"bar\"})})\n    ds2 = xr.Dataset({\"x\": ((), 1, {\"baz\": \"qux\"})})\n    actual = ds1.merge(ds2, compat=\"override\")\n    assert ds1.identical(actual)\n", "def test_merge_update_no_conflicts(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"x\": [0, 1, 2]})\n    ds1.update(ds2, compat=\"no_conflicts\")\n    assert expected.identical(ds1)\n", "def test_merge_no_conflicts_fill_value(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [np.nan, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"x\": [0, 1, 2]})\n\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\"))\n\n    expected2 = xr.Dataset({\"a\": (\"x\", [1, 2, 10]), \"x\": [0, 1, 2]})\n    assert expected2.identical(\n        ds1.merge(ds2, compat=\"no_conflicts\", fill_value=10)\n    )\n    assert expected2.identical(\n        ds2.merge(ds1, compat=\"no_conflicts\", fill_value=10)\n    )\n", "def test_merge_no_conflicts_multi_coord(self):\n    ds1 = xr.Dataset(coords={\"x\": [1, 2], \"y\": [3, 4]})\n    ds2 = xr.Dataset(coords={\"x\": [2, 3], \"y\": [4, 5]})\n    expected = xr.Dataset(coords={\"x\": [1, 2, 3], \"y\": [3, 4, 5]})\n\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\"))\n\n    assert ds1.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"left\"))\n\n    assert ds2.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"right\"))\n\n    expected2 = xr.Dataset(coords={\"x\": [2], \"y\": [4]})\n    assert expected2.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"inner\"))\n"], "sample_588": ["def test_combine_nested_data_vars_and_coords(self):\n    objs = [\n        Dataset({\"a\": (\"x\", [0]), \"b\": (\"x\", [1])}, coords={\"x\": [0]}),\n        Dataset({\"a\": (\"x\", [2]), \"c\": (\"x\", [3])}, coords={\"x\": [1]}),\n    ]\n    actual = combine_nested(objs, concat_dim=\"x\", data_vars=\"all\", coords=\"all\")\n    expected = Dataset(\n        {\"a\": (\"x\", [0, 2]), \"b\": (\"x\", [1, np.nan]), \"c\": (\"x\", [np.nan, 3])},\n        coords={\"x\": [0, 1]},\n    )\n    assert_identical(expected, actual)\n", "def test_combine_by_coords_with_different_coordinate_values(self):\n    # Test that combine_by_coords works when the coordinate values are different\n    # between datasets, but still monotonic\n    ds1 = Dataset({\"x\": [0, 1], \"y\": (\"x\", [10, 20])})\n    ds2 = Dataset({\"x\": [2, 3], \"y\": (\"x\", [30, 40])})\n    expected = Dataset({\"x\": [0, 1, 2, 3], \"y\": (\"x\", [10, 20, 30, 40])})\n    actual = combine_by_coords([ds1, ds2])\n    assert_identical(expected, actual)\n", "    def test_concat_dim_type(self):\n        shape = (2,)\n        combined_ids = _create_combined_ids(shape)\n        with raises_regex(TypeError, \"concat_dims must be a list of\"):\n            _combine_nd(combined_ids, concat_dims=\"x\")\n", "    def test_combine_nd_with_none_concat_dim(self):\n        combined_ids = {(0,): Dataset(), (1,): Dataset()}\n        with pytest.raises(ValueError):\n            _combine_nd(combined_ids, concat_dims=[None])\n", "def test_combine_nested_edge_cases(self):\n    # Test combine_nested with edge cases: empty list, single element list, etc.\n    assert_identical(Dataset(), combine_nested([], concat_dim=\"x\"))\n    assert_identical(Dataset(), combine_nested([Dataset()], concat_dim=\"x\"))\n\n    ds = Dataset()\n    assert_identical(ds, combine_nested([ds], concat_dim=None))\n    assert_identical(ds, combine_nested([ds], concat_dim=\"x\"))\n"], "sample_589": ["def test_interpolate_na_max_gap_integer(da):\n    da = xr.DataArray(\n        [np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10],\n        dims=[\"x\"],\n    )\n    expected = da.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da.interpolate_na(\"x\", max_gap=2)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_float(da):\n    da[\"x\"] = np.arange(len(da))\n    expected = da.copy(data=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    actual = da.interpolate_na(\"x\", max_gap=3.0)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_float(da):\n    da = xr.DataArray([np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10], dims=[\"x\"])\n    expected = da.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da.interpolate_na(\"x\", max_gap=2.0)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_float(da):\n    da[\"x\"] = np.arange(len(da))\n    expected = da.copy(\n        data=[0, 1, 2, 3, np.nan, 5, 6, 7, np.nan, 9, 10]\n    )\n    actual = da.interpolate_na(\"x\", max_gap=2.5)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_int(da):\n    da[\"t\"] = np.arange(11)\n    expected = da.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da.interpolate_na(\"t\", max_gap=3)\n    assert_equal(actual, expected)\n"], "sample_590": ["def test_concat_positions(self):\n    # Test that the `positions` argument works as expected\n    data = create_test_data()\n    split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n    positions = [[0, 1, 2], [4, 5, 6]]\n\n    actual = concat(split_data, dim=\"dim1\", positions=positions)\n    expected = data.copy(deep=True)\n    expected[\"dim1\"] = [0, 1, 2, 4, 5, 6]\n    assert_identical(actual, expected)\n\n    # Test that an error is raised if the lengths of the positions do not match\n    with raises_regex(ValueError, \"Lengths of positions do not match\"):\n        concat(split_data, dim=\"dim1\", positions=[[0, 1, 2], [4, 5]])\n\n    # Test that an error is raised if the positions are not lists or tuples\n    with raises_regex(TypeError, \"Positions must be lists or tuples\"):\n        concat(split_data, dim=\"dim1\", positions=[np.array([0, 1, 2]), np.array([4, 5, 6])])\n", "def test_concat_positions(self):\n    # Test that the `positions` argument works as expected\n    foo = DataArray([1, 2], coords=[(\"x\", [1, 2])])\n    bar = DataArray([3, 4], coords=[(\"x\", [3, 4])])\n\n    # Concatenate with positions\n    actual = concat((foo, bar), dim=\"y\", positions=[0, 1])\n    expected = DataArray(\n        [[1, 2], [3, 4]],\n        dims=[\"y\", \"x\"],\n        coords={\"x\": [1, 2], \"y\": [0, 1]},\n    )\n    assert_identical(actual, expected)\n\n    # Concatenate with default positions (should be the same as above)\n    actual = concat((foo, bar), dim=\"y\")\n    assert_identical(actual, expected)\n\n    # Concatenate with custom positions\n    actual = concat((foo, bar), dim=\"y\", positions=[10, 20])\n    expected = DataArray(\n        [[1, 2], [3, 4]],\n        dims=[\"y\", \"x\"],\n        coords={\"x\": [1, 2], \"y\": [10, 20]},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    # Test that the `positions` argument is respected.\n    datasets = [\n        Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]}),\n        Dataset({\"a\": (\"x\", [3, 4])}, coords={\"x\": [2, 3]}),\n    ]\n    actual = concat(datasets, dim=\"x\", positions=[[0, 1], [2, 4]])\n    expected = Dataset({\"a\": (\"x\", [1, 2, np.nan, 3, 4])}, coords={\"x\": [0, 1, 2, 3, 4]})\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    # Test that positions are used correctly when concatenating datasets.\n    ds1 = Dataset({\"x\": [0, 1]})\n    ds2 = Dataset({\"x\": [2, 3]})\n\n    # Specify positions explicitly.\n    actual = concat([ds1, ds2], dim=\"y\", positions=[0, 1])\n    expected = Dataset({\"x\": ((\"y\", \"x\"), [[0, 1], [2, 3]])}, coords={\"y\": [0, 1]})\n    assert_identical(actual, expected)\n\n    # Do not specify positions; they should default to the dimension length.\n    actual = concat([ds1, ds2], dim=\"y\")\n    expected = Dataset({\"x\": ((\"y\", \"x\"), [[0, 1], [2, 3]])}, coords={\"y\": [0, 1]})\n    assert_identical(actual, expected)\n\n    # Specify positions with different lengths.\n    with raises_regex(ValueError, \"Length of positions does not match\"):\n        concat([ds1, ds2], dim=\"y\", positions=[0, 1, 2])\n\n    # Specify positions with negative values.\n    with raises_regex(ValueError, \"Positions must be non-negative\"):\n        concat([ds1, ds2], dim=\"y\", positions=[-1, 1])\n", "def test_concat_dim_order(self, data):\n    # Regression test for GH1072\n    ds1 = data.isel(dim1=0)\n    ds2 = data.isel(dim1=1)\n\n    # Reorder the dimensions in the second dataset\n    ds2 = ds2.transpose(\"dim2\", \"dim3\", \"dim1\")\n\n    actual = concat([ds1, ds2], dim=\"dim1\")\n    expected = data\n\n    assert_identical(actual, expected)\n"], "sample_592": ["def test_short_data_repr():\n    # Test short_data_repr with different types of data\n    array = np.random.randn(100)\n    expected = limit_lines(repr(array), limit=40)\n    assert formatting.short_data_repr(array) == expected\n\n    da = xr.DataArray(np.random.randn(100))\n    expected = limit_lines(repr(da.data), limit=40)\n    assert formatting.short_data_repr(da) == expected\n\n    var = xr.Variable((\"x\",), np.random.randn(100))\n    expected = limit_lines(repr(var.data), limit=40)\n    assert formatting.short_data_repr(var) == expected\n\n    # Test short_data_repr with dask array\n    import dask.array as da\n\n    dask_array = da.from_array(np.random.randn(100), chunks=10)\n    expected = limit_lines(repr(dask_array), limit=40)\n    assert formatting.short_data_repr(dask_array) == expected\n", "def test_short_data_repr():\n    # Test for DataArray and Variable\n    array = xr.DataArray(np.random.randn(100, 5, 1))\n    result = formatting.short_data_repr(array)\n    assert len(result.splitlines()) < 50\n\n    # Test for dask array\n    import dask.array as da\n\n    dask_array = da.from_array(np.random.randn(100, 5, 1), chunks=(10, 5, 1))\n    result = formatting.short_data_repr(dask_array)\n    assert len(result.splitlines()) < 50\n\n    # Test for sparse array\n    import sparse\n\n    sparse_array = sparse.COO(np.random.randn(100, 5, 1))\n    result = formatting.short_data_repr(sparse_array)\n    assert len(result.splitlines()) < 50\n\n    # Test for large numpy array\n    large_array = np.random.randn(1000, 1000)\n    result = formatting.short_data_repr(large_array)\n    assert len(result.splitlines()) < 50\n", "def test_format_timestamp_with_timezone():\n    import pytz\n\n    dt = pd.Timestamp(\"2000-01-01T12\", tzinfo=pytz.timezone(\"US/Eastern\"))\n    expected = \"2000-01-01T12:00:00-05:00\"\n    result = formatting.format_timestamp(dt)\n    assert result == expected\n", "def test_inline_variable_array_repr():\n    import dask.array as da\n\n    x = xr.Variable((\"x\",), np.array([1, 2, 3]))\n    assert formatting.inline_variable_array_repr(x, 80) == \"[1 2 3]\"\n\n    y = xr.Variable((\"y\",), da.from_array(np.array([1, 2, 3]), chunks=2))\n    assert \"dask.array\" in formatting.inline_variable_array_repr(y, 80)\n\n    z = xr.Variable((\"z\",), np.array([1.0, 2.0, 3.0], dtype=\"float64\"))\n    assert formatting.inline_variable_array_repr(z, 80) == \"[1. 2. 3.]\"\n", "def test_short_data_repr():\n    # Test that short_data_repr returns the correct representation for different types of data\n    da = xr.DataArray(np.array([1, 2, 3]))\n    assert formatting.short_data_repr(da) == \"[1 2 3]\"\n\n    da = xr.DataArray(np.array([1.0, 2.0, 3.0]))\n    assert formatting.short_data_repr(da) == \"[1. 2. 3.]\"\n\n    da = xr.DataArray(np.array([\"a\", \"b\", \"c\"]))\n    assert formatting.short_data_repr(da) == \"['a' 'b' 'c']\"\n\n    da = xr.DataArray(np.array([True, False, True]))\n    assert formatting.short_data_repr(da) == \"[ True False  True]\"\n"], "sample_593": ["def test_summarize_coord_name_safe():\n    var = xr.Variable([\"x\"], [1], {\"foo\": \"bar\"})\n    coord_name = \"test\"\n    formatted = fh.summarize_coord(coord_name, var)\n    assert f\"<div class='xr-var-name'><span>{coord_name}</span></div>\" in formatted.values()\n", "def test_summarize_coord_with_multiindex(multiindex):\n    coord = multiindex.coords[\"x\"]\n    formatted = fh.summarize_coord(\"x\", coord)\n    assert \"(level_1, level_2)\" in formatted\n    assert \"MultiIndex\" in formatted\n    assert \"<span class='xr-has-index'>x</span>\" in formatted\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Details\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n\n    assert f\"<input id='section-\" in section\n    assert f\"<label for='section-\" in section\n    assert name in section\n    assert inline_details in section\n    assert details in section\n    assert str(n_items) in section\n    assert \"disabled\" not in section\n    assert \"checked\" not in section\n", "def test_collapsible_section():\n    section = fh.collapsible_section(\n        name=\"Test Section\", inline_details=\"\", details=\"\", n_items=None, enabled=True, collapsed=False\n    )\n    assert \"class='xr-section-summary-in'\" in section\n    assert \"class='xr-section-inline-details'\" in section\n    assert \"class='xr-section-details'\" in section\n\n    section = fh.collapsible_section(\n        name=\"Test Section\", inline_details=\"\", details=\"\", n_items=5, enabled=False, collapsed=True\n    )\n    assert \"disabled\" in section\n    assert \"checked\" in section\n", "def test_summarize_variable():\n    v = xr.Variable([\"time\", \"x\"], [[1, 2, 3], [4, 5, 6]], {\"foo\": \"bar\"})\n    formatted = fh.summarize_variable(\"my_var\", v)\n    assert \"<div class='xr-var-name'><span>my_var</span></div>\" in formatted\n    assert \"<div class='xr-var-dims'>(time, x)</div>\" in formatted\n    assert \"<div class='xr-var-dtype'>int64</div>\" in formatted\n    assert \"<div class='xr-var-preview xr-preview'>\" in formatted\n    assert \"<input id=\" in formatted\n    assert \"<label for=\" in formatted\n"], "sample_594": ["def test_wrap_indent():\n    text = \"This is a very long string that needs to be wrapped and indented.\"\n    expected = \"    This is a very long string that needs to\\n    be wrapped and indented.\"\n    actual = formatting.wrap_indent(text, start=\"    \", length=4)\n    assert actual == expected\n", "def test_short_data_repr(self):\n    array = np.arange(1000)\n    expected = \"[1000 values with dtype='int64']\"\n    actual = formatting.short_data_repr(array)\n    assert actual == expected\n\n    array = np.arange(5)\n    expected = \"[0 1 2 3 4]\"\n    actual = formatting.short_data_repr(array)\n    assert actual == expected\n\n    array = np.arange(1000.0)\n    expected = \"[1000 values with dtype='float64']\"\n    actual = formatting.short_data_repr(array)\n    assert actual == expected\n", "def test_limit_lines():\n    string = \"\\n\".join(str(i) for i in range(100))\n    expected = \"\\n\".join(\n        [\"0\", \"1\", \"2\", \"...\", \"97\", \"98\", \"99\"]\n    )\n    actual = formatting.limit_lines(string, limit=7)\n    assert actual == expected\n", "def test_short_data_repr_ndarray():\n    array = np.array([1, 2, 3])\n    actual = formatting.short_data_repr(array)\n    expected = \"array([1, 2, 3])\"\n    assert actual == expected\n\n    array = np.array([1, 2, 3], dtype=np.float64)\n    actual = formatting.short_data_repr(array)\n    expected = \"array([1., 2., 3.])\"\n    assert actual == expected\n", "def test_inline_dask_repr():\n    import dask.array as da\n\n    x = da.from_array(np.random.randn(1000), chunks=100)\n    var = xr.Variable(\"x\", x)\n    actual = formatting.inline_dask_repr(var.data)\n    expected = \"dask.array<chunksize=(100,), meta='numpy.ndarray'>\"\n    assert actual == expected\n"], "sample_595": ["def test_isdecimal(dtype):\n    values = xr.DataArray([\"123\", \"123.45\", \"123,456\", \"0x10\", \"-1\"]).astype(dtype)\n    expected = xr.DataArray([True, False, False, False, False])\n    assert_equal(values.str.isdecimal(), expected)\n", "def test_split(dtype):\n    values = xr.DataArray([\"a,b\", \"c,d,e\", \"\", \"a,b,c,d\"]).astype(dtype)\n\n    result = values.str.split(\",\")\n    expected = xr.DataArray([[\"a\", \"b\"], [\"c\", \"d\", \"e\"], [], [\"a\", \"b\", \"c\", \"d\"]])\n    assert_equal(result, expected)\n", "def test_getitem(dtype):\n    values = xr.DataArray([\"a_b_c\", \"c_d_e\", \"f_g_h\"]).astype(dtype)\n\n    result = values.str[1:3]\n    expected = xr.DataArray([\"_b\", \"_d\", \"_g\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str[-2:]\n    expected = xr.DataArray([\"_c\", \"_e\", \"_h\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str[::-1]\n    expected = xr.DataArray([\"c_b_a\", \"e_d_c\", \"h_g_f\"]).astype(dtype)\n    assert_equal(result, expected)\n", "def test_isdecimal():\n    # 0x00bc: \u00bc VULGAR FRACTION ONE QUARTER\n    # 0x2605: \u2605 not number\n    # 0x1378: \u1378 ETHIOPIC NUMBER SEVENTY\n    # 0xFF13: \uff13 Em 3\n    values = [\"A\", \"3\", \"\u00bc\", \"\u2605\", \"\u1378\", \"\uff13\", \"four\"]\n    s = xr.DataArray(values)\n    decimal_e = [False, True, False, False, False, True, False]\n    assert_equal(s.str.isdecimal(), xr.DataArray(decimal_e))\n", "def test_partition(dtype):\n    values = xr.DataArray([\"abXcd\", \"efXgh\", \"ijXkl\", \"mnXop\", \"qrstu\"]).astype(dtype)\n    result = values.str.partition(\"X\")\n    expected = xr.DataArray([tuple(x.partition(dtype(\"X\"))) for x in values.values])\n    assert_equal(result, expected)\n\n    result = values.str.rpartition(\"X\")\n    expected = xr.DataArray([tuple(x.rpartition(dtype(\"X\"))) for x in values.values])\n    assert_equal(result, expected)\n"], "sample_596": ["def test_concat_variables_with_different_attributes():\n    # Test that variables with different attributes are concatenated correctly\n\n    # Create two DataArrays with different attributes\n    da1 = DataArray([1, 2, 3], dims=\"x\", attrs={\"units\": \"m\"})\n    da2 = DataArray([4, 5, 6], dims=\"x\", attrs={\"units\": \"cm\"})\n\n    # Concatenate the DataArrays\n    actual = concat([da1, da2], dim=\"x\")\n\n    # Check that the resulting DataArray has the correct attributes\n    expected_attrs = {\"units\": \"m\"}\n    assert actual.attrs == expected_attrs\n\n    # Check that the values are correct\n    expected_values = np.array([1, 2, 3, 4, 5, 6])\n    assert_array_equal(actual.values, expected_values)\n", "def test_concat_dim_order():\n    # Test that the order of dimensions is preserved when concatenating\n    ds1 = Dataset(\n        {\"data\": ([\"x\", \"y\"], [[1, 2], [3, 4]])},\n        coords={\"x\": [0, 1], \"y\": [0, 1]},\n    )\n    ds2 = Dataset(\n        {\"data\": ([\"x\", \"y\"], [[5, 6], [7, 8]])},\n        coords={\"x\": [2, 3], \"y\": [0, 1]},\n    )\n\n    expected = Dataset(\n        {\"data\": ([\"x\", \"y\"], [[1, 2], [3, 4], [5, 6], [7, 8]])},\n        coords={\"x\": [0, 1, 2, 3], \"y\": [0, 1]},\n    )\n\n    actual = concat([ds1, ds2], dim=\"x\")\n    assert_identical(actual, expected)\n", "def test_concat_coords_with_conflicting_values():\n    ds1 = Dataset({}, coords={\"x\": 1})\n    ds2 = Dataset({}, coords={\"x\": 2})\n    with raises_regex(merge.MergeError, \"conflicting values\"):\n        concat([ds1, ds2], dim=\"y\", coords=\"minimal\")\n", "def test_concat_fill_value_with_dtype():\n    da1 = DataArray([1, 2], dims=\"x\", coords={\"x\": [1, 2]})\n    da2 = DataArray([3, 4], dims=\"x\", coords={\"x\": [3, 4]})\n\n    # Test with a float fill value and integer data\n    expected = DataArray([1, 2, np.nan, 3, 4], dims=\"x\", coords={\"x\": [1, 2, 2.5, 3, 4]})\n    actual = concat([da1, da2], dim=\"x\", fill_value=np.nan)\n    assert_identical(actual, expected)\n\n    # Test with an integer fill value and float data\n    da1 = DataArray([1.0, 2.0], dims=\"x\", coords={\"x\": [1, 2]})\n    da2 = DataArray([3.0, 4.0], dims=\"x\", coords={\"x\": [3, 4]})\n    expected = DataArray([1.0, 2.0, 0.0, 3.0, 4.0], dims=\"x\", coords={\"x\": [1, 2, 2.5, 3, 4]})\n    actual = concat([da1, da2], dim=\"x\", fill_value=0)\n    assert_identical(actual, expected)\n\n    # Test with a NaN fill value and object data\n    da1 = DataArray([\"a\", \"b\"], dims=\"x\", coords={\"x\": [1, 2]})\n    da2 = DataArray([\"c\", \"d\"], dims=\"x\", coords={\"x\": [3, 4]})\n    expected = DataArray([\"a\", \"b\", None, \"c\", \"d\"], dims=\"x\", coords={\"x\": [1, 2, 2.5, 3, 4]})\n    actual = concat([da1, da2], dim=\"x\", fill_value=None)\n    assert_identical(actual, expected)\n", "def test_concat_no_conflicts_data_vars():\n    ds1 = Dataset(\n        {\n            \"a\": ((\"x\",), [1, 2]),\n            \"b\": ((\"x\",), [3, 4]),\n        },\n        coords={\"x\": [0, 1]},\n    )\n    ds2 = Dataset(\n        {\n            \"a\": ((\"x\",), [5, 6]),\n            \"c\": ((\"x\",), [7, 8]),\n        },\n        coords={\"x\": [0, 1]},\n    )\n\n    expected = Dataset(\n        {\n            \"a\": ((\"concat_dim\", \"x\"), [[1, 2], [5, 6]]),\n            \"b\": ((\"concat_dim\", \"x\"), [[3, 4], [np.nan, np.nan]]),\n            \"c\": ((\"concat_dim\", \"x\"), [[np.nan, np.nan], [7, 8]]),\n        },\n        coords={\"concat_dim\": [0, 1], \"x\": [0, 1]},\n    )\n\n    actual = concat([ds1, ds2], dim=\"concat_dim\", data_vars=\"no_conflicts\")\n    assert_identical(actual, expected)\n"], "sample_597": ["def test_merge_dict_of_dataarrays(self):\n    data = create_test_data()\n    dict_of_da = {\"var1\": data.var1, \"var2\": data.var2}\n    expected = data[[\"var1\", \"var2\"]]\n    actual = xr.merge(dict_of_da)\n    assert actual.identical(expected)\n", "def test_merge_update(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [0, 1]})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])}, coords={\"x\": [0, 1]}\n    )\n    assert expected.identical(ds1.merge(ds2))\n\n    ds3 = xr.Dataset({\"c\": (\"y\", [5, 6]), \"y\": [0, 1]})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4]), \"c\": (\"y\", [5, 6])},\n        coords={\"x\": [0, 1], \"y\": [0, 1]},\n    )\n    assert expected.identical(ds1.merge([ds2, ds3]))\n\n    ds4 = xr.Dataset({\"d\": (\"z\", [7, 8]), \"z\": [0, 1]})\n    expected = xr.Dataset(\n        {\n            \"a\": (\"x\", [1, 2]),\n            \"b\": (\"x\", [3, 4]),\n            \"c\": (\"y\", [5, 6]),\n            \"d\": (\"z\", [7, 8]),\n        },\n        coords={\"x\": [0, 1], \"y\": [0, 1], \"z\": [0, 1]},\n    )\n    assert expected.identical(ds1.merge([ds2, ds3, ds4]))\n", "def test_merge_no_conflicts_empty(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset()\n\n    expected = ds1.copy()\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\"))\n\n    ds3 = xr.Dataset(coords={\"y\": [2, 3]})\n    expected = xr.Dataset(coords={\"x\": [0, 1], \"y\": [2, 3]})\n    expected[\"a\"] = (\"x\", [1, 2])\n    assert expected.identical(ds1.merge(ds3, compat=\"no_conflicts\"))\n    assert expected.identical(ds3.merge(ds1, compat=\"no_conflicts\"))\n", "def test_merge_update_coords(self):\n    ds1 = xr.Dataset(coords={\"x\": [1, 2]})\n    ds2 = xr.Dataset(coords={\"x\": [3, 4], \"y\": [5, 6]})\n\n    expected = xr.Dataset(coords={\"x\": [1, 2], \"y\": [5, 6]})\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"x\"]))\n", "def test_merge_update(self):\n    ds1 = xr.Dataset({\"a\": 0, \"b\": 1})\n    ds2 = xr.Dataset({\"b\": 2, \"c\": 3})\n\n    expected = xr.Dataset({\"a\": 0, \"b\": 2, \"c\": 3})\n    assert_identical(ds1.merge(ds2, combine_attrs=\"override\"), expected)\n    assert_identical(ds1.update(ds2), expected)\n\n    ds1 = xr.Dataset({\"a\": 0, \"b\": 1}, attrs={\"foo\": \"bar\"})\n    ds2 = xr.Dataset({\"b\": 2, \"c\": 3}, attrs={\"foo\": \"baz\"})\n\n    expected = xr.Dataset({\"a\": 0, \"b\": 2, \"c\": 3}, attrs={\"foo\": \"bar\"})\n    assert_identical(ds1.merge(ds2, combine_attrs=\"override\"), expected)\n    assert_identical(ds1.update(ds2), expected)\n"], "sample_598": ["def test_unindexed_dims_repr():\n    ds = xr.Dataset(coords={\"foo\": [1, 2, 3]})\n    ds[\"bar\"] = xr.DataArray([4, 5, 6], dims=\"foo\")\n    ds[\"baz\"] = xr.DataArray([7, 8, 9], dims=\"qux\")\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    expected = \"Dimensions without coordinates: qux\"\n    assert actual == expected\n\n    ds.coords[\"qux\"] = [10, 11, 12]\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    assert actual is None\n", "def test_limit_lines():\n    lines = [\"line1\", \"line2\", \"line3\", \"line4\", \"line5\"]\n    string = \"\\n\".join(lines)\n    limit = 3\n    expected = \"line1\\n...\\nline5\"\n    actual = formatting.limit_lines(string, limit=limit)\n    assert actual == expected\n", "def test__summarize_coord_multiindex(self):\n    coord = xr.Variable(\n        (\"x\",),\n        np.array([1, 2, 3]),\n        attrs={\"units\": \"m\"},\n        dims=(\"x\",),\n    ).to_index_variable()\n    coord.level_names = [\"level_1\", \"level_2\"]\n\n    actual = formatting._summarize_coord_multiindex(coord, col_width=10)\n    expected = pretty_print(\"  * x       \", 10) + \"(x) MultiIndex\"\n    assert actual == expected\n", "def test_unindexed_dims_repr():\n    dims = (\"x\", \"y\", \"z\")\n    coords = {\"x\": np.array([1, 2]), \"y\": np.array([3, 4])}\n    expected = \"Dimensions without coordinates: z\"\n    actual = formatting.unindexed_dims_repr(dims, coords)\n    assert actual == expected\n\n    dims = (\"x\", \"y\")\n    coords = {\"x\": np.array([1, 2]), \"y\": np.array([3, 4])}\n    expected = None\n    actual = formatting.unindexed_dims_repr(dims, coords)\n    assert actual == expected\n", "def test_format_timestamp_valid_range():\n    date = pd.Timestamp(\"1678-01-01\")\n    expected = \"1678-01-01\"\n    result = formatting.format_timestamp(date)\n    assert result == expected\n\n    date = pd.Timestamp(\"2262-04-11\")\n    expected = \"2262-04-11\"\n    result = formatting.format_timestamp(date)\n    assert result == expected\n"], "sample_599": ["def test_UnsignedIntegerCoder_roundtrip():\n    original = xr.Variable((\"x\",), np.array([0, 1, 2], dtype=\"uint8\"))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == \"int8\"\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == \"uint8\"\n", "def test_scaling_with_offset_converts_to_float64(dtype):\n    original = xr.Variable(\n        (\"x\",), np.arange(10, dtype=dtype), encoding=dict(scale_factor=10, add_offset=0.1)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float64\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == np.float64\n", "def test_UnsignedIntegerCoder():\n    original = xr.Variable((\"x\",), np.array([0, 255], dtype=\"uint8))\n    original.encoding[\"_Unsigned\"] = \"true\"\n    original.attrs[\"_FillValue\"] = 255\n\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n\n    expected = xr.Variable(\n        (\"x\",), np.array([0, -1], dtype=\"int8\"), attrs={\"_FillValue\": -1}\n    )\n    assert_identical(encoded, expected)\n\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n", "def test_UnsignedIntegerCoder_decode():\n    original = xr.Variable((\"x\",), np.array([-1, 0, 1], dtype=\"int8\"))\n    original.attrs[\"_Unsigned\"] = \"true\"\n    expected = xr.Variable((\"x\",), np.array([255, 0, 1], dtype=\"uint8\"))\n    coder = variables.UnsignedIntegerCoder()\n    decoded = coder.decode(original)\n    assert_identical(expected, decoded)\n", "def test_UnsignedIntegerCoder_roundtrip():\n    original = xr.Variable((\"x\",), [0, 1, 2], {\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n"], "sample_600": ["def test_unpack_for_encoding_with_missing_values():\n    var = xr.Variable((\"x\",), [0, -1, 1], {\"_FillValue\": -1})\n    dims, data, attrs, encoding = variables.unpack_for_encoding(var)\n    assert \"missing_value\" not in attrs\n    assert \"_FillValue\" in attrs\n\n    var = xr.Variable((\"x\",), [0, -1, 1], {\"missing_value\": -1})\n    dims, data, attrs, encoding = variables.unpack_for_encoding(var)\n    assert \"missing_value\" in attrs\n    assert \"_FillValue\" not in attrs\n\n    var = xr.Variable(\n        (\"x\",), [0, -1, 1], {\"_FillValue\": -1, \"missing_value\": -1}\n    )\n    dims, data, attrs, encoding = variables.unpack_for_encoding(var)\n    assert \"missing_value\" in attrs\n    assert \"_FillValue\" in attrs\n", "compilation error", "def test_encode_unsigned_to_signed():\n    original = xr.Variable((\"x\",), [0, 255], dtype=\"uint8)\n    expected = xr.Variable((\"x\",), [0, -1], dtype=\"int8\")\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_scaling_with_offset_converts_to_float64(dtype):\n    original = xr.Variable(\n        (\"x\",), np.arange(10, dtype=dtype), encoding=dict(scale_factor=10, add_offset=0.1)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float64\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == np.float64\n", "def test_scaling_converts_to_float64(dtype):\n    original = xr.Variable(\n        (\"x\",), np.arange(10, dtype=dtype), encoding=dict(scale_factor=10, add_offset=1000)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float64\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == np.float64\n"], "sample_601": ["def test_field_access_on_coordinate_with_no_name(data, field):\n    if field == \"dayofyear\" or field == \"dayofweek\":\n        pytest.importorskip(\"cftime\", minversion=\"1.0.2.1\")\n    result = getattr(data.time.dt, field)\n    expected = xr.DataArray(\n        getattr(xr.coding.cftimeindex.CFTimeIndex(data.time.values), field),\n        dims=[\"time\"],\n    )\n\n    assert_equal(result, expected)\n", "def test_accessor_method_cftime(cftime_date_type, method, parameters):\n    dates = [\n        cftime_date_type(1, 1, 1, 1),\n        cftime_date_type(1, 1, 1, 15),\n        cftime_date_type(1, 1, 2, 1),\n    ]\n    xdates = xr.DataArray(dates, dims=[\"time\"])\n    expected = getattr(xr.coding.cftimeindex.CFTimeIndex(dates), method)(parameters)\n    actual = getattr(xdates.dt, method)(parameters)\n    assert_array_equal(expected, actual)\n", "def test_cftime_invalid_rounding_frequency(cftime_date_type):\n    da = xr.DataArray([cftime_date_type(1, 1, 1, 1)])\n    with raises_regex(ValueError, \"Invalid frequency\"):\n        da.dt.floor(\"invalid\")\n    with raises_regex(ValueError, \"Invalid frequency\"):\n        da.dt.ceil(\"invalid\")\n    with raises_regex(ValueError, \"Invalid frequency\"):\n        da.dt.round(\"invalid\")\n", "def test_cftime_invalid_accessor(data):\n    with raises_regex(\n        AttributeError, \"CFTimeIndex object has no attribute 'invalid_accessor'\"\n    ):\n        data.time.dt.invalid_accessor\n", "def test_cftime_strftime_method(cftime_rounding_dataarray):\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    expected = xr.DataArray(\n        [\n            [\"0001-01-01 01:00:00\", \"0001-01-01 15:00:00\"],\n            [\"0001-01-01 23:00:00\", \"0001-01-02 01:00:00\"],\n        ],\n        name=\"strftime\",\n    )\n    result = cftime_rounding_dataarray.dt.strftime(date_format)\n    assert_identical(result, expected)\n"], "sample_602": ["def test_open_dataset_invalid_engine():\n    with pytest.raises(ValueError) as excinfo:\n        xr.open_dataset(\"example.nc\", engine=\"invalid\")\n    assert \"unrecognized engine\" in str(excinfo.value)\n", "def test_open_dataset_with_pathlib_path():\n    expected = xr.Dataset(\n        dict(a=2 * np.arange(5)), coords=dict(x=(\"x\", np.arange(5), dict(units=\"s\")))\n    )\n\n    import tempfile\n    with tempfile.NamedTemporaryFile(suffix=\".nc\") as tmp:\n        expected.to_netcdf(tmp.name)\n        actual = xr.open_dataset(tmp.name)\n        assert_identical(expected, actual)\n\n        import pathlib\n        actual = xr.open_dataset(pathlib.Path(tmp.name))\n        assert_identical(expected, actual)\n", "def test_open_dataset_invalid_engine():\n    with pytest.raises(ValueError):\n        xr.open_dataset(\"example.nc\", engine=\"invalid_engine\")\n", "def test_to_netcdf_invalid_engine():\n    ds = xr.Dataset()\n    with pytest.raises(ValueError) as excinfo:\n        ds.to_netcdf(\"example.nc\", engine=\"invalid\")\n    assert \"unrecognized engine\" in str(excinfo.value)\n", "def test_open_dataset_invalid_engine():\n    with pytest.raises(ValueError) as excinfo:\n        xr.open_dataset(\"example.nc\", engine=\"invalid\")\n    assert \"unrecognized engine\" in str(excinfo.value)\n"], "sample_603": ["def test_summarize_variable_with_unsafe_name_and_dtype():\n    var = xr.Variable([\"x\"], [1, 2, 3], {\"foo\": \"bar\"}, dtype=\"float64\")\n    var.name = \"<unsafe_name>\"\n    formatted = fh.summarize_variable(var.name, var)\n    assert \"&lt;unsafe_name&gt;\" in formatted\n    assert \"float64\" in formatted\n    assert \"xr-var-name\" in formatted\n    assert \"xr-var-dtype\" in formatted\n    assert \"xr-var-preview\" in formatted\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"More details\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details, details, n_items, enabled, collapsed\n    )\n\n    assert f\"<input id='section-\" in section\n    assert f\"<label for='section-\" in section\n    assert name in section\n    assert inline_details in section\n    assert details in section\n    assert str(n_items) in section\n", "def test_summarize_variable_with_unsafe_name():\n    var = xr.Variable([\"x\"], [1, 2, 3], {\"foo\": \"bar\"})\n    var.name = \"<unsafe_name>\"\n    formatted = fh.summarize_variable(\"foo\", var)\n    assert \"&lt;unsafe_name&gt;\" in formatted\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    details = \"<p>This is a test section.</p>\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details=\"\", details=details, n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n\n    assert f\"<input id='section-\" in section\n    assert f\"type='checkbox' {'disabled' if not enabled else ''} {'checked' if collapsed else ''}>\" in section\n    assert f\"<label for='section-\" in section\n    assert f\"class='xr-section-summary' title='Expand/collapse section'>{name}:</label>\" in section\n    assert f\"<div class='xr-section-inline-details'></div>\" in section\n    assert f\"<div class='xr-section-details'>{details}</div>\" in section\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Detailed information\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n\n    assert f\"<input id='section-\" in section\n    assert f\"class='xr-section-summary-in' type='checkbox' {'disabled' if not enabled else ''} {'checked' if not collapsed else ''}>\" in section\n    assert f\"<label for='section-\" in section\n    assert f\"class='xr-section-summary' title='Expand/collapse section'>{name}: <span>({n_items})</span></label>\" in section\n    assert f\"<div class='xr-section-inline-details'>{inline_details}</div>\" in section\n    assert f\"<div class='xr-section-details'>{details}</div>\" in section\n"], "sample_604": ["def test__mapping_repr_empty():\n    with xr.set_options(display_max_rows=10):\n        actual = formatting._mapping_repr({}, \"Coordinates\", summarize_coord, expand_option_name=\"display_expand_coords\")\n        expected = dedent(\n            \"\"\"\\\n            Coordinates:\n                *empty*\"\"\"\n        )\n        assert actual == expected\n\n        actual = formatting._mapping_repr({}, \"Data variables\", summarize_datavar, expand_option_name=\"display_expand_data_vars\")\n        expected = dedent(\n            \"\"\"\\\n            Data variables:\n                *empty*\"\"\"\n        )\n        assert actual == expected\n\n        actual = formatting._mapping_repr({}, \"Attributes\", summarize_attr, expand_option_name=\"display_expand_attrs\")\n        expected = dedent(\n            \"\"\"\\\n            Attributes:\n                *empty*\"\"\"\n        )\n        assert actual == expected\n", "def test_wrap_indent():\n    lines = [\n        \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. \"\n        \"Praesent et dignissim diam.\",\n        \"Sed eleifend eros sed felis laoreet ultrices. \"\n        \"Donec et posuere neque.\",\n    ]\n    expected = (\n        \"    This is a long text:\\n\"\n        \"        Lorem ipsum dolor sit amet, consectetur adipiscing elit. \"\n        \"Praesent et dignissim diam.\\n\"\n        \"        Sed eleifend eros sed felis laoreet ultrices. \"\n        \"Donec et posuere neque.\"\n    )\n    actual = formatting.wrap_indent(lines, start=\"    This is a long text: \")\n    assert actual == expected\n", "def test_limit_lines():\n    string = \"a\\n\" * 1000\n    limited_string = formatting.limit_lines(string, limit=5)\n    assert len(limited_string.splitlines()) == 7  # includes ellipsis line and two padding lines\n\n    string = \"a\\n\" * 3\n    limited_string = formatting.limit_lines(string, limit=5)\n    assert len(limited_string.splitlines()) == 3  # no change if under limit\n", "def test_unindexed_dims_repr():\n    dims = (\"x\", \"y\", \"z\")\n    coords = {\"x\": [1, 2], \"y\": [3, 4]}\n    expected = \"Dimensions without coordinates: z\"\n    actual = formatting.unindexed_dims_repr(dims, coords)\n    assert actual == expected\n\n    coords = {\"x\": [1, 2], \"y\": [3, 4], \"z\": [5, 6]}\n    expected = None\n    actual = formatting.unindexed_dims_repr(dims, coords)\n    assert actual == expected\n\n    dims = (\"x\",)\n    coords = {}\n    expected = \"Dimensions without coordinates: x\"\n    actual = formatting.unindexed_dims_repr(dims, coords)\n    assert actual == expected\n", "def test__summarize_coord_levels():\n    coord = xr.DataArray(\n        np.array([1, 2, 3]),\n        dims=\"x\",\n        name=\"foo\",\n        attrs={\"units\": \"m\"},\n    ).to_index_variable()\n\n    actual = formatting._summarize_coord_levels(coord, col_width=80)\n    expected = dedent(\n        \"\"\"\\\n        * foo       (x) int64 1 2 3\n            units: m\"\"\"\n    )\n    assert actual == expected\n\n    # MultiIndex levels\n    coord = xr.DataArray(\n        pd.MultiIndex.from_product([range(3), range(4)], names=(\"a\", \"b\")),\n        dims=\"x\",\n        name=\"foo\",\n    ).to_index_variable()\n\n    actual = formatting._summarize_coord_levels(coord, col_width=80)\n    expected = dedent(\n        \"\"\"\\\n        * foo       (x) MultiIndex\n          * a       (x) int64 0 0 0 0 1 1 1 1 2 2 2 2\n          * b       (x) int64 0 1 2 3 0 1 2 3 0 1 2 3\"\"\"\n    )\n    assert actual == expected\n"], "sample_605": ["def test_groupby_fillna():\n    array = xr.DataArray(\n        [[1, 2, np.nan], [3, np.nan, 5], [np.nan, 7, 8]], dims=[\"x\", \"y\"]\n    )\n    expected = xr.DataArray(\n        [[1.0, 2.0, 5.0], [3.0, 7.0, 5.0], [3.0, 7.0, 8.0]], dims=[\"x\", \"y\"]\n    )\n    actual = array.groupby(\"x\").fillna(array.mean(\"x\"))\n    assert_identical(actual, expected)\n", "def test_groupby_assign_coords():\n    actual = xr.Dataset(\n        data_vars={\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [1, 2, 3]}\n    ).groupby(\"x\")\n    assigned = actual.assign_coords({\"y\": (\"x\", [4, 5, 6])})\n    expected = xr.Dataset(\n        data_vars={\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [1, 2, 3], \"y\": [4, 5, 6]}\n    )\n    assert_identical(assigned, expected)\n", "def test_groupby_quantile_dim_check():\n    array = xr.DataArray([1, 2, 3], dims=\"x\")\n    with pytest.raises(ValueError):\n        array.groupby(\"x\").quantile(0.5, dim=\"y\")\n", "def test_groupby_squeeze():\n    # Test squeezing for DataArray and Dataset\n    array = xr.DataArray([1, 2, 3], dims=[\"x\"], coords={\"x\": [1, 1, 2]})\n    dataset = array.to_dataset(name=\"foo\")\n\n    # Squeeze True\n    expected_array = xr.DataArray([2, 3], dims=[\"x\"], coords={\"x\": [1, 2]})\n    expected_dataset = expected_array.to_dataset(name=\"foo\")\n    assert_identical(array.groupby(\"x\", squeeze=True).sum(), expected_array)\n    assert_identical(dataset.groupby(\"x\", squeeze=True).sum(), expected_dataset)\n\n    # Squeeze False\n    expected_array = xr.DataArray([2, 3], dims=[\"x\"], coords={\"x\": [1, 2]})\n    expected_dataset = expected_array.to_dataset(name=\"foo\")\n    assert_identical(array.groupby(\"x\", squeeze=False).sum(), expected_array)\n    assert_identical(dataset.groupby(\"x\", squeeze=False).sum(), expected_dataset)\n", "def test_groupby_map_type_preservation():\n    # Test that map preserves the type of the input object\n    array = xr.DataArray([1, 2, 3], dims=[\"x\"], coords={\"x\": [1, 2, 3]})\n    dataset = array.to_dataset(name=\"foo\")\n\n        return x\n\n    assert isinstance(array.groupby(\"x\").map(identity), xr.DataArray)\n    assert isinstance(dataset.groupby(\"x\").map(identity), xr.Dataset)\n"], "sample_607": ["def test_detect_parameters():\n    backend = DummyBackendEntrypoint1()\n    parameters = plugins.detect_parameters(backend.open_dataset)\n    assert parameters == (\"filename_or_obj\", \"decoder\")\n\n    backend = DummyBackendEntrypointArgs()\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(backend.open_dataset)\n\n    backend = DummyBackendEntrypointKwargs()\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(backend.open_dataset)\n", "def test_get_backend():\n    backend = plugins.get_backend(\"netcdf4\")\n    assert isinstance(backend, common.BackendEntrypoint)\n\n    with pytest.raises(ValueError):\n        plugins.get_backend(\"nonexistent-engine\")\n\n    with pytest.raises(TypeError):\n        plugins.get_backend(object())\n", "def test_guess_engine():\n    store_spec = \"foo.nc\"\n    with mock.patch.object(DummyBackendEntrypoint1, \"guess_can_open\", return_value=True):\n        assert plugins.guess_engine(store_spec) == \"dummy\"\n\n    with mock.patch.object(DummyBackendEntrypoint1, \"guess_can_open\", side_effect=Exception()):\n        with pytest.warns(RuntimeWarning):\n            assert plugins.guess_engine(store_spec) is None\n", "def test_get_backend_str():\n    backend = plugins.get_backend(\"dummy\")\n    assert isinstance(backend, DummyBackendEntrypoint1)\n\n", "def test_guess_engine_match():\n    engine = plugins.guess_engine(\"example.nc\")\n    assert engine == \"netcdf4\"\n"], "sample_608": ["def test__element_formatter_single_element() -> None:\n    elements = [\"dim_0: 3\"]\n    intro = \"Dimensions without coordinates: \"\n    values = xr.core.formatting._element_formatter(\n        elements, col_width=len(intro), max_rows=None\n    )\n    actual = intro + values\n    expected = \"Dimensions without coordinates: dim_0: 3\"\n    assert expected == actual\n", "def test__element_formatter_single_line(n_elements: int = 5) -> None:\n    expected = \"Dimensions without coordinates: dim_0: 3, dim_1: 3, dim_2: 3, dim_3: 3, dim_4: 3\"\n\n    intro = \"Dimensions without coordinates: \"\n    elements = [\n        f\"{k}: {v}\" for k, v in {f\"dim_{k}\": 3 for k in np.arange(n_elements)}.items()\n    ]\n    values = xr.core.formatting._element_formatter(\n        elements, col_width=len(intro), max_rows=None\n    )\n    actual = intro + values\n    assert expected == actual\n", "def test__element_formatter_small_elements(n_elements: int = 5) -> None:\n    expected = \"\"\"\\", "def test__dim_summary_limited() -> None:\n    ds = xr.Dataset(coords={\"foo\": [1, 2, 3], \"bar\": [1, 2, 3]})\n    actual = formatting.dim_summary_limited(ds, col_width=10)\n    expected = \"foo: 3, bar: 3\"\n    assert actual == expected\n\n    actual = formatting.dim_summary_limited(ds, col_width=5)\n    expected = \"foo: 3\\nbar: 3\"\n    assert actual == expected\n\n    actual = formatting.dim_summary_limited(ds, col_width=5, max_rows=1)\n    expected = \"foo: 3\"\n    assert actual == expected\n", "def test__element_formatter_single_row(n_elements: int = 100) -> None:\n    expected = \"\"\"\\"], "sample_609": ["def test_apply_ufunc_with_dtype_change() -> None:\n    # Test apply_ufunc with a function that changes the dtype of the input.\n    # In particular, we're testing for GH4031, where apply_ufunc would fail if\n    # the input had a chunk size of 1 in any dimension.\n\n    import dask.array as da\n\n    array = da.from_array(np.arange(4), chunks=(2,))\n    data_array = xr.DataArray(array, dims=(\"x\",))\n\n        return x.astype(\"float64\")\n\n    result = apply_ufunc(change_dtype, data_array, dask=\"parallelized\")\n    assert result.dtype == \"float64\"\n    assert isinstance(result.data, da.Array)\n    xr.testing.assert_allclose(result.compute(), data_array.compute().astype(\"float64\"))\n", "def test_where_dataarray_coords() -> None:\n    cond = xr.DataArray([True, False], dims=\"x\")\n    x = xr.DataArray([1, 2], dims=\"x\")\n    y = xr.DataArray([0, 3], dims=\"x\")\n    actual = xr.where(cond, x, y)\n    expected = xr.DataArray([1, 3], dims=\"x\")\n    assert_identical(expected, actual)\n", "def test_polyval_degree_coord_not_found() -> None:\n    da = xr.DataArray(\n        np.stack((1.0 + np.arange(10) + 2.0 * np.arange(10) ** 2,)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": np.arange(10), \"d\": [0]},\n    )\n    coeffs = xr.DataArray(\n        [[2, 1, 1]],\n        dims=(\"d\", \"poly_dim\"),\n        coords={\"d\": [0], \"poly_dim\": [2, 1, 0]},\n    )\n\n    with pytest.raises(KeyError, match=r\"Dimension 'degree' not in degree\"):\n        xr.polyval(da.x, coeffs)\n", "def test_unify_chunks() -> None:\n    # Test unify_chunks with DataArray objects\n    da1 = xr.DataArray(np.arange(10), dims=\"x\", chunks=5)\n    da2 = xr.DataArray(np.arange(10), dims=\"x\", chunks=2)\n    unified_da1, unified_da2 = unify_chunks(da1, da2)\n    assert unified_da1.chunks == ((2, 2, 2, 2, 2),)\n    assert unified_da2.chunks == ((2, 2, 2, 2, 2),)\n\n    # Test unify_chunks with Dataset objects\n    ds1 = xr.Dataset()\n    ds1[\"var\"] = da1\n    ds2 = xr.Dataset()\n    ds2[\"var\"] = da2\n    unified_ds1, unified_ds2 = unify_chunks(ds1, ds2)\n    assert unified_ds1[\"var\"].chunks == ((2, 2, 2, 2, 2),)\n    assert unified_ds2[\"var\"].chunks == ((2, 2, 2, 2, 2),)\n\n    # Test unify_chunks with mixed DataArray and Dataset objects\n    unified_da1, unified_ds1 = unify_chunks(da1, ds1)\n    assert unified_da1.chunks == ((5,),)\n    assert unified_ds1[\"var\"].chunks == ((5,),)\n", "def test_polyval_chunks() -> None:\n    import dask.array as da\n\n    x = np.arange(10)\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n    ).chunk({\"d\": 2})\n    expected = xr.polyval(x, coeffs.compute())\n    actual = xr.polyval(x, coeffs)\n    assert isinstance(actual.data, da.Array)\n    xr.testing.assert_allclose(expected, actual)\n"], "sample_610": ["def test_cftimeindex_repr_with_name():\n    index = xr.cftime_range(\"2000\", periods=5, name=\"time\")\n    expected = f\"\"\"\\", "def test_cftimeindex_get_value(series, index):\n    result = index.get_value(series, \"0001\")\n    expected = series[\"0001\"]\n    assert result.equals(expected)\n", "def test_cftimeindex_repr_unsorted():\n    index = xr.CFTimeIndex(\n        [\n            cftime.DatetimeNoLeap(1, 1, 2),\n            cftime.DatetimeNoLeap(1, 1, 1),\n            cftime.DatetimeNoLeap(1, 1, 3),\n        ]\n    )\n    assert \"length=3\" in repr(index)\n    assert \"calendar='noleap'\" in repr(index)\n    assert \"[0001-01-02 00:00:00, 0001-01-01 00:00:00, 0001-01-03 00:00:00]\" in repr(index)\n", "def test_cftimeindex_repr_with_name(date_type):\n    index = xr.CFTimeIndex(\n        [date_type(1, 1, 1), date_type(1, 2, 1)], name=\"time\"\n    )\n    expected = f\"\"\"\\", "def test_cftimeindex_to_pandas_index_roundtrip(index):\n    result = CFTimeIndex.from_pandas_index(index.to_pandas_index())\n    assert result.equals(index)\n"], "sample_611": ["def test_cftime_range_tz_error():\n    with pytest.raises(ValueError, match=\"Time zone is not supported\"):\n        cftime_range(start=\"2000\", periods=4, tz=\"UTC\")\n", "def test_cftime_range_error_without_calendar():\n    with pytest.raises(ValueError, match=\"'calendar' must be specified\"):\n        cftime_range(start=\"2000\", periods=14, freq=\"D\")\n", "def test_date_range_like_unsupported_calendar():\n    src = date_range(\"2000-01-01\", periods=12, freq=\"M\")\n    with pytest.raises(\n        ValueError,\n        match=\"Unsupported calendar 'unsupported'.\",\n    ):\n        date_range_like(src, \"unsupported\")\n", "def test_date_range_like_non_iso8601(start, freq, cal_src, cal_tgt, use_cftime, exp0, exp_pd):\n    source = date_range(start, periods=12, freq=freq, calendar=cal_src)\n\n    out = date_range_like(DataArray(source), cal_tgt, use_cftime=use_cftime)\n\n    assert len(out) == 12\n    assert infer_freq(out) == freq\n\n    assert out[0].isoformat().startswith(exp0)\n\n    if exp_pd:\n        assert isinstance(out, pd.DatetimeIndex)\n    else:\n        assert isinstance(out, CFTimeIndex)\n        assert out.calendar == cal_tgt\n", "def test_date_range_like_cftime_index():\n    src = date_range(\"2000-01-01\", periods=12, freq=\"M\", calendar=\"noleap\")\n    out = date_range_like(src, \"gregorian\")\n    assert len(out) == 12\n    assert infer_freq(out) == \"M\"\n    assert isinstance(out, CFTimeIndex)\n    assert out.calendar == \"gregorian\"\n"], "sample_612": ["def test_groupby_map_empty_groups():\n    array = xr.DataArray([1, 2, 3], dims=\"x\")\n    array[\"x\"] = pd.date_range(\"2000-01-01\", periods=3)\n    grouped = array.groupby(\"time.month\")\n\n        return x + 1\n\n    expected = xr.DataArray([2, 3, 4], dims=\"x\", coords={\"time\": array.time})\n    actual = grouped.map(func)\n    assert_identical(expected, actual)\n\n    # make sure the grouped object still has the correct groups\n    assert len(grouped.groups) == 1\n    np.testing.assert_array_equal(next(iter(grouped.groups.values())), np.arange(3))\n", "def test_groupby_fillna_without_coords():\n    array = DataArray([1, np.nan, 3], dims=\"x\")\n    filled = array.groupby(\"x\").fillna(0)\n    expected = DataArray([1, 0, 3], dims=\"x\")\n    assert_identical(expected, filled)\n", "def test_groupby_bins_errors(array):\n    with pytest.raises(ValueError, match=r\"All bin edges are NaN.\"):\n        array.groupby_bins(\"x\", bins=[np.nan, np.nan])\n\n    with pytest.raises(ValueError, match=r\"Failed to group data.\"):\n        array.groupby_bins(\"x\", bins=[0, 1, 2], include_lowest=True)\n\n    with pytest.raises(\n        ValueError, match=r\"None of the data falls within bins with edges\"\n    ):\n        array.groupby_bins(\"x\", bins=[-1, -0.5])\n", "def test_groupby_reduce_dim_order(array):\n    array = array.rename({\"x\": \"a\", \"y\": \"b\", \"z\": \"c\"})\n    dims = list(array.dims)\n    # Reorder the dimensions to be in a different order than the array's dims\n    dims.reverse()\n    expected = array.reduce(np.mean, dim=dims)\n    actual = array.groupby(\"b\").reduce(np.mean, dim=dims)\n    assert_identical(expected, actual)\n", "def test_groupby_dataset_sortby_map(self):\n    # GH: 4507\n    times = pd.date_range(\"2000-01-01\", freq=\"D\", periods=30)\n    ds = Dataset(\n        {\n            \"foo\": (\"time\", np.random.randn(30)),\n            \"bar\": (\"time\", np.random.randn(30)),\n        },\n        {\"time\": times},\n    )\n\n    ds_grouped = ds.groupby(\"time.month\").map(lambda x: x.sortby(\"time\"))\n    expected_ds = ds.copy()\n    for month in ds_grouped.groups:\n        inds = ds[\"time.month\"] == month\n        expected_ds[inds] = ds[inds].sortby(\"time\")\n\n    assert_identical(ds_grouped, expected_ds)\n"], "sample_613": ["def test_groupby_dataarray_getitem(dataset) -> None:\n    array = dataset[\"foo\"]\n    grouped = array.groupby(\"x\")\n    expected = array.isel(x=0)\n    actual = grouped[0]\n    assert_identical(expected, actual)\n\n    expected = array.isel(x=[1, 2])\n    actual = grouped[[1, 2]]\n    assert_identical(expected, actual)\n\n    with pytest.raises(IndexError):\n        grouped[3]\n\n    with pytest.raises(IndexError):\n        grouped[-4]\n\n    with pytest.raises(IndexError):\n        grouped[[1, 2, 3]]\n", "def test_groupby_map_no_side_effects() -> None:\n    # Test that map does not modify the original array.\n    da = xr.DataArray([1, 2, 3], dims=\"x\")\n    da_copy = da.copy()\n\n        x.values[:] = 0\n        return x\n\n    da.groupby(\"x\").map(func)\n    assert_identical(da, da_copy)\n", "def test_groupby_squeeze():\n    # Test that groupby with squeeze=True removes singleton dimensions\n    array = xr.DataArray(\n        np.arange(10),\n        dims=\"x\",\n        coords={\"x\": (\"x\", [1, 2, 2, 3, 3, 3, 4, 4, 4, 4])},\n    )\n    actual = array.groupby(\"x\", squeeze=True).mean()\n    expected = xr.DataArray([0, 2.5, 6, 8.5], dims=\"x\", coords={\"x\": [1, 2, 3, 4]})\n    assert_identical(actual, expected)\n\n    # Test that groupby with squeeze=False keeps singleton dimensions\n    array = xr.DataArray(\n        np.arange(10),\n        dims=\"x\",\n        coords={\"x\": (\"x\", [1, 2, 2, 3, 3, 3, 4, 4, 4, 4])},\n    )\n    actual = array.groupby(\"x\", squeeze=False).mean()\n    expected = xr.DataArray(\n        [0, 2.5, 2.5, 6, 6, 6, 8.5, 8.5, 8.5, 8.5],\n        dims=\"x\",\n        coords={\"x\": [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]},\n    )\n    assert_identical(actual, expected)\n", "def test_groupby_fillna_dataset():\n    # Test fillna with a dataset\n    ds = xr.Dataset()\n    ds[\"a\"] = (\"x\", [1, np.nan, 3])\n    ds[\"b\"] = (\"x\", [np.nan, 2, 3])\n    ds.coords[\"x\"] = [0, 1, 0]\n    \n    actual = ds.groupby(\"x\").fillna(0)\n    expected = xr.Dataset()\n    expected[\"a\"] = (\"x\", [1, 0, 3])\n    expected[\"b\"] = (\"x\", [0, 2, 3])\n    expected.coords[\"x\"] = [0, 1, 0]\n    \n    assert_identical(actual, expected)\n", "def test_groupby_bins_empty_bin():\n    array = DataArray(np.arange(4), dims=\"dim_0\")\n    bins = [0, 2, 5]\n    bin_coords = pd.cut(array[\"dim_0\"], bins).categories\n    actual = array.groupby_bins(\"dim_0\", bins).map(lambda x: x.sum())\n    expected = DataArray([1, 7], dims=\"dim_0_bins\", coords={\"dim_0_bins\": bin_coords})\n    assert_identical(expected, actual)\n"], "sample_614": ["def test__calculate_col_width() -> None:\n    assert formatting._calculate_col_width([\"a\", \"bb\", \"ccc\"]) == 9\n    assert formatting._calculate_col_width([]) == 0\n    assert formatting._calculate_col_width([\"short\", \"very_long_variable_name\"]) == 24\n", "def test__calculate_col_width() -> None:\n    variables = [\"short\", \"long_variable_name\"]\n    actual = formatting._calculate_col_width(variables)\n    expected = 18  # max_name_length (17) + 1\n    assert actual == expected\n\n    variables = []\n    actual = formatting._calculate_col_width(variables)\n    expected = 7  # default value\n    assert actual == expected\n", "def test_short_data_repr() -> None:\n    array = xr.DataArray(np.array([1, 2, 3]), dims=\"x\")\n    expected = \"array([1, 2, 3], dtype=int64)\"\n    actual = formatting.short_data_repr(array)\n    assert actual == expected\n\n    # Test with numpy arrays\n    array = np.array([1, 2, 3])\n    expected = \"array([1, 2, 3], dtype=int64)\"\n    actual = formatting.short_data_repr(array)\n    assert actual == expected\n\n    # Test with duck arrays\n    class DuckArray:\n            return \"DuckArray([1, 2, 3])\"\n\n            return NotImplemented\n\n    array = DuckArray()\n    expected = \"DuckArray([1, 2, 3])\"\n    actual = formatting.short_data_repr(array)\n    assert actual == expected\n\n    # Test with internal xarray array type\n    array = xr.Variable(\"x\", [1, 2, 3])\n    expected = \"[3 values with dtype=int64]\"\n    actual = formatting.short_data_repr(array)\n    assert actual == expected\n", "def test__element_formatter_empty() -> None:\n    elements = []\n    col_width = 10\n    actual = formatting._element_formatter(elements, col_width=col_width)\n    expected = \"\"\n    assert actual == expected\n\n    actual = formatting._element_formatter(elements, col_width=col_width, max_rows=5)\n    assert actual == expected\n", "def test__calculate_col_width() -> None:\n    cases = [\n        ({\"a\": 1, \"b\": 2}, 7),\n        ({\"long_variable_name\": 1, \"short\": 2}, 20),\n        ({}, 7),\n        ({\"a\": 1}, 7),\n    ]\n\n    for mapping, expected in cases:\n        actual = formatting._calculate_col_width(mapping)\n        assert actual == expected\n"], "sample_615": ["def test_cross_broadcast() -> None:\n    # Test broadcasting\n    a = xr.DataArray(\n        [[1, 2], [3, 4]],\n        dims=[\"x\", \"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    expected_a = np.array([[1, 2, 0], [3, 4, 0]])\n    expected_b = np.array([4, 5, 6])\n    expected = np.cross(expected_a, expected_b)\n\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_chunked():\n    # Test cross with chunked data\n    a = xr.DataArray(\n        np.arange(0, 5 * 3 * 4).reshape((5, 3, 4)),\n        dims=[\"time\", \"cartesian\", \"var\"],\n        coords=dict(\n            time=([\"time\"], np.arange(0, 5)),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n            var=([\"var\"], [1, 1.5, 2, 2.5]),\n        ),\n    ).chunk()\n    b = xr.DataArray(\n        np.arange(0, 5 * 3 * 4).reshape((5, 3, 4)) + 1,\n        dims=[\"time\", \"cartesian\", \"var\"],\n        coords=dict(\n            time=([\"time\"], np.arange(0, 5)),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n            var=([\"var\"], [1, 1.5, 2, 2.5]),\n        ),\n    ).chunk()\n\n    expected = np.cross(a.data, b.data, axis=1)\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_1d() -> None:\n    # Test cross product with 1D arrays\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"])\n\n    expected = np.cross(a.values, b.values)\n    actual = xr.cross(a, b, dim=\"x\")\n\n    assert_array_equal(actual.values, expected)\n", "def test_cross_2d_vectors() -> None:\n    # Test that the cross product of two 2D vectors returns a scalar\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"])\n    b = xr.DataArray([4, 5], dims=[\"cartesian\"])\n    expected = np.cross(a.values, b.values)\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert actual.dims == ()\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_no_coords(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    a = xr.DataArray([1, 2], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"])\n\n    if use_dask:\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=\"x\")\n    assert isinstance(actual, xr.DataArray)\n    assert actual.dims == ()\n"], "sample_616": ["def test_unify_chunks() -> None:\n    da1 = xr.DataArray(\n        np.random.rand(10, 20),\n        dims=(\"x\", \"y\"),\n        chunks=(5, 10),\n    )\n    da2 = xr.DataArray(\n        np.random.rand(10, 20),\n        dims=(\"x\", \"y\"),\n        chunks=(3, 7),\n    )\n\n    unified_da1, unified_da2 = unify_chunks(da1, da2)\n\n    assert unified_da1.chunks == ((5, 5), (10, 10))\n    assert unified_da2.chunks == ((5, 5), (10, 10))\n\n    # Test with datasets\n    ds1 = xr.Dataset({\"a\": da1})\n    ds2 = xr.Dataset({\"b\": da2})\n\n    unified_ds1, unified_ds2 = unify_chunks(ds1, ds2)\n\n    assert unified_ds1[\"a\"].chunks == ((5, 5), (10, 10))\n    assert unified_ds2[\"b\"].chunks == ((5, 5), (10, 10))\n", "def test_cross_array_output() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"])\n\n    actual = xr.cross(a, b, dim=\"x\")\n    assert isinstance(actual, xr.DataArray)\n", "def test_cross_dataset() -> None:\n    ds_a = xr.Dataset(dict(x=(\"dim_0\", [1]), y=(\"dim_0\", [2]), z=(\"dim_0\", [3])))\n    ds_b = xr.Dataset(dict(x=(\"dim_0\", [4]), y=(\"dim_0\", [5]), z=(\"dim_0\", [6])))\n\n    expected = xr.DataArray([9, 2 + 6 + 16, 2 + 9 + 36], dims=\"dim_0\")\n    actual = xr.cross(ds_a.to_array(\"cartesian\"), ds_b.to_array(\"cartesian\"), dim=\"cartesian\")\n    assert_identical(expected, actual)\n", "def test_cross_broadcasting() -> None:\n    # Test broadcasting with different shapes and sizes\n    a = xr.DataArray(\n        [1, 2, 3],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    b = xr.DataArray(\n        [[4, 5, 6], [7, 8, 9]],\n        dims=[\"time\", \"cartesian\"],\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n\n    expected = np.cross([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\n\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_broadcasting() -> None:\n    # Test broadcasting for cross product\n    a = xr.DataArray(\n        np.array([[1, 2], [3, 4]]),\n        dims=(\"x\", \"cartesian\"),\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    b = xr.DataArray(\n        np.array([4, 5, 6]),\n        dims=\"cartesian\",\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n\n    expected = np.cross(a.values[:, np.newaxis, :], b.values)\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n"], "sample_618": ["def test_unify_chunks() -> None:\n    import dask.array as da\n\n    # create dataarrays with different chunk sizes\n    a = xr.DataArray(da.from_array([1, 2, 3], chunks=(1,)))\n    b = xr.DataArray(da.from_array([4, 5, 6], chunks=(3,)))\n\n    # unify chunks should rechunk to the largest chunk size\n    a_unified, b_unified = unify_chunks(a, b)\n    assert a_unified.chunks == ((3,),)\n    assert b_unified.chunks == ((3,),)\n\n    # test that unify_chunks handles non-dask arrays\n    c = xr.DataArray([7, 8, 9])\n    c_unified, = unify_chunks(c)\n    assert isinstance(c_unified.data, np.ndarray)\n\n    # test that unify_chunks handles multiple types of inputs\n    a_unified, b_unified, c_unified = unify_chunks(a, b, c)\n    assert a_unified.chunks == ((3,),)\n    assert b_unified.chunks == ((3,),)\n    assert isinstance(c_unified.data, da.Array)\n    assert c_unified.chunks == ((3,),)\n\n    # test that unify_chunks handles dataset inputs\n    ds = xr.Dataset()\n    ds_unified, = unify_chunks(ds)\n    assert isinstance(ds_unified, xr.Dataset)\n", "def test_unify_chunks() -> None:\n    # Test with DataArray\n    da1 = xr.DataArray(np.random.rand(10, 10), dims=[\"x\", \"y\"]).chunk((5, 5))\n    da2 = xr.DataArray(np.random.rand(10, 10), dims=[\"x\", \"y\"]).chunk((3, 3))\n    unified_da1, unified_da2 = unify_chunks(da1, da2)\n    assert unified_da1.chunks == ((5, 5), (5, 5))\n    assert unified_da2.chunks == ((5, 5), (5, 5))\n\n    # Test with Dataset\n    ds1 = xr.Dataset()\n    ds1[\"var1\"] = xr.DataArray(np.random.rand(10, 10), dims=[\"x\", \"y\"]).chunk((5, 5))\n    ds2 = xr.Dataset()\n    ds2[\"var1\"] = xr.DataArray(np.random.rand(10, 10), dims=[\"x\", \"y\"]).chunk((3, 3))\n    unified_ds1, unified_ds2 = unify_chunks(ds1, ds2)\n    assert unified_ds1[\"var1\"].chunks == ((5, 5), (5, 5))\n    assert unified_ds2[\"var1\"].chunks == ((5, 5), (5, 5))\n\n    # Test with both DataArray and Dataset\n    unified_da1, unified_ds1 = unify_chunks(da1, ds1)\n    assert unified_da1.chunks == ((5, 5), (5, 5))\n    assert unified_ds1[\"var1\"].chunks == ((5, 5), (5, 5))\n", "def test_cross_broadcasting() -> None:\n    a = xr.DataArray(\n        [[1, 2], [3, 4]],\n        dims=[\"x\", \"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    \n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"cartesian\")\n        \n    b = xr.DataArray(\n        [[4, 5], [6, 7]],\n        dims=[\"y\", \"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert actual.dims == (\"x\", \"y\", \"cartesian\")\n", "def test_cross_broadcasting():\n    a = xr.DataArray(\n        [[1, 2], [3, 4]],\n        dims=[\"time\", \"cartesian\"],\n        coords=dict(time=([\"time\"], [0, 1]), cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    b = xr.DataArray(\n        [4, 5, 6],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    expected = np.cross(\n        a.values[..., np.newaxis], b.values[np.newaxis, ...], axis=-1\n    )\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_broadcasting():\n    a = xr.DataArray(\n        np.array([1, 2, 3]),\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    b = xr.DataArray(\n        np.array([[4, 5, 6], [7, 8, 9]]),\n        dims=[\"time\", \"cartesian\"],\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n\n    expected = np.cross(a.values, b.values)\n\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n"], "sample_619": ["def test_encode_cf_datetime_non_standard_calendar() -> None:\n    import cftime\n\n    calendar = \"360_day\"\n    times = cftime.num2date(\n        [0, 1], \"hours since 2000-01-01\", calendar, only_use_cftime_datetimes=True\n    )\n    units = \"days since 2000-01-01\"\n    encoded, _, _ = encode_cf_datetime(times, units, calendar)\n\n    expected = cftime.date2num(times, units, calendar)\n    np.testing.assert_equal(encoded, expected)\n", "def test_cftime_to_nptime_year_zero() -> None:\n    # Test for fix to GH 5503\n    import cftime\n\n    date = cftime.DatetimeGregorian(0, 1, 1)\n    with pytest.raises(ValueError):\n        cftime_to_nptime(date)\n", "def test_encode_cf_datetime_invalid_calendar() -> None:\n    dates = pd.date_range(\"2000\", periods=3)\n    units = \"days since 2000-01-01\"\n    calendar = \" invalid_calendar\"\n    with pytest.raises(ValueError, match=\"invalid calendar\"):\n        encode_cf_datetime(dates, units, calendar)\n", "def test_encode_cf_datetime_cftime_with_time_zone() -> None:\n    import cftime\n\n    calendar = \"gregorian\"\n    times = cftime.num2date([0, 1], \"hours since 2000-01-01\", calendar)\n    units = \"days since 2000-01-01T00:00:00+05:00\"\n\n    encoded, _, _ = encode_cf_datetime(times, units, calendar)\n\n    expected = np.array([0.0, 0.04166667])\n    np.testing.assert_almost_equal(encoded, expected)\n", "def test_infer_datetime_units_with_cftime_dates() -> None:\n    import cftime\n\n    calendar = \"360_day\"\n    units = \"days since 2000-01-01\"\n    times = cftime.num2date([1, 2, 3], units, calendar)\n    expected = f\"{units} 00:00:00\"\n    assert expected == coding.times.infer_datetime_units(times)\n"], "sample_620": ["def test_concat_index_order() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [3, 4])})\n\n    # Make sure that index order is preserved\n    actual = concat([ds1, ds2], dim=\"x\")\n    assert list(actual.indexes) == [\"x\"]\n\n    # Check that index order is still preserved if an index is not present in all datasets\n    ds3 = Dataset(coords={\"y\": (\"y\", [5, 6])})\n    actual = concat([ds1, ds3], dim=\"x\")\n    assert list(actual.indexes) == [\"x\", \"y\"]\n", "def test_concat_combine_attrs_drop_conflicts() -> None:\n    ds1 = Dataset(attrs={\"a\": 1, \"b\": 2})\n    ds2 = Dataset(attrs={\"b\": 3, \"c\": 4})\n\n    expected = Dataset(attrs={\"a\": 1, \"c\": 4})\n    actual = concat([ds1, ds2], dim=\"new_dim\", combine_attrs=\"drop_conflicts\")\n\n    assert_identical(actual, expected)\n", "def test_concat_along_new_dim_with_coordinate() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [3, 4])})\n\n    new_dim = DataArray([0, 1], dims=\"y\")\n    actual = concat([ds1, ds2], dim=new_dim)\n\n    expected = Dataset(coords={\"x\": ((\"y\", \"x\"), [[1, 2], [3, 4]]), \"y\": [0, 1]})\n    assert_identical(actual, expected)\n", "def test_concat_dim_order() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2]), \"y\": (\"y\", [3, 4])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [5, 6]), \"y\": (\"y\", [7, 8])})\n\n    # swap dimension order\n    ds2 = ds2.transpose(\"y\", \"x\")\n\n    expected = Dataset(\n        coords={\"x\": (\"x\", [1, 2, 5, 6]), \"y\": (\"y\", [3, 4, 7, 8])}\n    )\n\n    actual = concat([ds1, ds2], dim=\"x\")\n    assert_identical(actual, expected)\n", "def test_concat_positions() -> None:\n    # Test that the `positions` argument works as expected.\n    da1 = DataArray([1, 2], dims=\"x\")\n    da2 = DataArray([3, 4], dims=\"x\")\n\n    # Concatenate along a new dimension.\n    actual = concat([da1, da2], dim=\"y\", positions=[[0], [1]])\n    expected = DataArray([[1, 2], [3, 4]], dims=[\"y\", \"x\"])\n    assert_identical(actual, expected)\n\n    # Concatenate along an existing dimension.\n    actual = concat([da1, da2], dim=\"x\", positions=[[0, 1], [2, 3]])\n    expected = DataArray([1, 2, 3, 4], dims=[\"x\"])\n    assert_identical(actual, expected)\n\n    # Test with different lengths of positions.\n    da3 = DataArray([5, 6, 7], dims=\"x\")\n    actual = concat([da1, da3], dim=\"x\", positions=[[0, 1], [2, 3, 4]])\n    expected = DataArray([1, 2, 5, 6, 7], dims=[\"x\"])\n    assert_identical(actual, expected)\n\n    # Test with overlapping positions.\n    with pytest.raises(ValueError, match=r\"positions overlap\"):\n        concat([da1, da2], dim=\"x\", positions=[[0, 1], [1, 2]])\n"], "sample_621": ["def test_create_default_index_implicit(indexes) -> None:\n    index, index_vars = create_default_index_implicit(\n        indexes.variables[\"x\"], indexes.variables\n    )\n\n    assert isinstance(index, PandasIndex)\n    assert set(index_vars) == {\"x\"}\n\n    # multi-index case (this should be removed once we remove implicit index)\n    pd_midx = pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=(\"foo\", \"bar\"))\n    var = xr.Variable(\"z\", pd_midx)\n\n    index, index_vars = create_default_index_implicit(var, indexes.variables)\n    assert isinstance(index, PandasMultiIndex)\n    assert set(index_vars) == {\"foo\", \"bar\"}\n", "def test_assert_no_index_corrupted(indexes) -> None:\n    coord_names = {\"x\", \"y\"}\n    assert_no_index_corrupted(indexes, coord_names)\n\n    with pytest.raises(\n        ValueError,\n        match=r\"cannot remove coordinate\\(s\\) \\{'z'\\}, which would corrupt the following index\",\n    ):\n        coord_names = {\"x\", \"z\"}\n        assert_no_index_corrupted(indexes, coord_names)\n", "def test_filter_indexes_from_coords(self, indexes) -> None:\n    filtered_coord_names = {\"x\", \"y\", \"one\"}\n    expected = {\n        \"x\": indexes[\"x\"],\n        \"y\": indexes[\"y\"],\n        \"z\": indexes[\"z\"],\n        \"one\": indexes[\"one\"],\n    }\n    actual = filter_indexes_from_coords(indexes, filtered_coord_names)\n    assert actual == expected\n\n    filtered_coord_names = {\"x\", \"y\"}\n    expected = {\"x\": indexes[\"x\"], \"y\": indexes[\"y\"]}\n    actual = filter_indexes_from_coords(indexes, filtered_coord_names)\n    assert actual == expected\n", "def test_remove_unused_levels_categories(self) -> None:\n    pd_midx = pd.MultiIndex.from_product(\n        [[\"a\", \"b\"], [1, 2]], names=[\"one\", \"two\"]\n    )\n    midx = PandasMultiIndex(pd_midx, \"x\")\n\n    # create index with unused levels and categories\n    index = midx.index[[0, 1]]\n    assert len(index.levels[0]) == 2\n    assert len(index.levels[1]) == 2\n\n    new_index = remove_unused_levels_categories(index)\n    assert len(new_index.levels[0]) == 2\n    assert len(new_index.levels[1]) == 2\n\n    # create categorical index with unused categories\n    cat_idx = pd.CategoricalIndex([\"a\", \"b\", \"a\"], name=\"cat\", categories=[\"a\", \"b\", \"c\"])\n    assert len(cat_idx.categories) == 3\n\n    new_cat_idx = remove_unused_levels_categories(cat_idx)\n    assert len(new_cat_idx.categories) == 2\n", "def test_copy_indexes_deep(self, indexes) -> None:\n    copied, index_vars = indexes.copy_indexes(deep=True)\n\n    assert copied.keys() == indexes.keys()\n    for new, original in zip(copied.values(), indexes.values()):\n        assert new.equals(original)\n        assert new is not original  # check deep copy\n\n    assert index_vars.keys() == indexes.variables.keys()\n    for new, original in zip(index_vars.values(), indexes.variables.values()):\n        assert_identical(new, original)\n        assert new is not original  # check deep copy\n"], "sample_622": ["def test_encode_dataset_coordinates_with_global_coordinates() -> None:\n    # regression test for GH4490\n    original = Dataset(\n        coords={\n            \"time\": pd.date_range(\"2000-01-01\", periods=3),\n            \"lat\": (\"y\", [1, 2, 3]),\n            \"lon\": (\"x\", [4, 5, 6]),\n        },\n        data_vars={\n            \"a\": ([\"time\", \"y\", \"x\"], np.zeros((3, 3, 3)), {\"coordinates\": \"time lat lon\"}),\n            \"b\": ([\"time\", \"y\"], np.zeros((3, 3)), {\"coordinates\": \"time lat\"}),\n        },\n        attrs={\"coordinates\": \"time\"},\n    )\n    encoded, _ = conventions.encode_dataset_coordinates(original)\n    assert encoded[\"a\"].attrs.get(\"coordinates\") == \"time lat lon\"\n    assert encoded[\"b\"].attrs.get(\"coordinates\") == \"time lat\"\n    assert \"coordinates\" not in encoded[\"time\"].attrs\n", "def test_decode_cf_with_coordinates_attr_set_to_none() -> None:\n    original = Dataset(\n        {\n            \"t\": (\"t\", [0, 1, 2], {\"units\": \"days since 2000-01-01\"}),\n            \"foo\": (\"t\", [0, 0, 0], {\"coordinates\": None, \"units\": \"bar\"}),\n            \"y\": (\"t\", [5, 10, -999], {\"_FillValue\": -999}),\n        }\n    )\n    expected = Dataset(\n        {\"foo\": (\"t\", [0, 0, 0], {\"units\": \"bar\"})},\n        {\n            \"t\": pd.date_range(\"2000-01-01\", periods=3),\n            \"y\": (\"t\", [5.0, 10.0, np.nan]),\n        },\n    )\n    actual = conventions.decode_cf(original)\n    assert_identical(expected, actual)\n", "def test_decode_cf_variables_with_bounds() -> None:\n    original = Dataset(\n        {\n            \"t\": (\"t\", [0, 1, 2], {\"units\": \"days since 2000-01-01\"}),\n            \"foo\": (\"t\", [0, 0, 0], {\"coordinates\": \"y\", \"units\": \"bar\"}),\n            \"y\": (\n                \"t\",\n                [5, 10, -999],\n                {\"_FillValue\": -999, \"bounds\": \"y_bnds\"},\n            ),\n            \"y_bnds\": ((\"t\", \"bnds\"), [[4, 6], [9, 11], [-999, -999]]),\n        }\n    )\n    expected = Dataset(\n        {\"foo\": (\"t\", [0, 0, 0], {\"units\": \"bar\"})},\n        {\n            \"t\": pd.date_range(\"2000-01-01\", periods=3),\n            \"y\": (\n                \"t\",\n                [5.0, 10.0, np.nan],\n                {\"bounds\": \"y_bnds\"},\n            ),\n            \"y_bnds\": ((\"t\", \"bnds\"), [[4, 6], [9, 11], [np.nan, np.nan]]),\n        },\n    )\n    actual = conventions.decode_cf(original)\n    assert_identical(expected, actual)\n", "def test_encode_cf_variable_with_bounds_attribute() -> None:\n    # Test that the \"bounds\" attribute is preserved when encoding a variable\n    original = Variable(\n        [\"t\"], [0, 1, 2], {\"units\": \"days since 2000-01-01\", \"bounds\": \"time_bounds\"}\n    )\n    encoded = conventions.encode_cf_variable(original)\n    assert encoded.attrs[\"bounds\"] == \"time_bounds\"\n", "def test_decode_cf_bounds_attributes() -> None:\n    # Test that decode_cf handles bounds variables with attributes correctly\n    original = Dataset(\n        {\n            \"t\": (\"t\", [0, 1, 2], {\"units\": \"days since 2000-01-01\"}),\n            \"t_bnds\": (\n                (\"t\", \"bnds\"),\n                [[-1, 1], [0, 2], [1, 3]],\n                {\"units\": \"days since 2000-01-01\"},\n            ),\n        }\n    )\n    expected = Dataset(\n        {\"t\": pd.date_range(\"2000-01-01\", periods=3)},\n        {\"t_bnds\": ((\"t\", \"bnds\"), [[-1, 1], [0, 2], [1, 3]], {\"units\": \"days since 2000-01-01\"})},\n    )\n    actual = conventions.decode_cf(original)\n    assert_identical(expected, actual)\n"], "sample_623": ["def test_to_zarr_append_dim() -> None:\n    # GH4562: Test that to_zarr works with append_dim.\n    import dask.array as da\n\n    # Create a sample dataset.\n    data = da.random.random((10, 10), chunks=(5, 5))\n    ds = xr.Dataset()\n    ds[\"data\"] = ((\"x\", \"y\"), data)\n\n    # Write the dataset to zarr.\n    store = \"test.zarr\"\n    ds.to_zarr(store, mode=\"w\")\n\n    # Append new data along the x dimension.\n    new_data = da.random.random((5, 10), chunks=(5, 5))\n    new_ds = xr.Dataset()\n    new_ds[\"data\"] = ((\"x\", \"y\"), new_data)\n    new_ds.to_zarr(store, mode=\"a\", append_dim=\"x\")\n\n    # Check the resulting zarr store.\n    final_ds = xr.open_zarr(store)\n    assert final_ds.dims[\"x\"] == 15\n    assert final_ds.dims[\"y\"] == 10\n", "def test_open_dataset_chunking() -> None:\n    \"\"\"Test that open_dataset correctly handles chunking.\"\"\"\n    # Create a sample dataset with dask arrays.\n    da = xr.DataArray(\n        np.random.rand(10, 10),\n        dims=(\"x\", \"y\"),\n        coords={\"x\": np.arange(10), \"y\": np.arange(10)},\n    ).chunk(chunks=(5, 5))\n    ds = xr.Dataset({\"a\": da})\n\n    # Test that the chunks argument is applied.\n    chunks = {\"x\": 2, \"y\": 3}\n    ds_reopened = xr.open_dataset(ds, engine=PassThroughBackendEntrypoint, chunks=chunks)\n    assert ds_reopened[\"a\"].chunks == ((2, 2, 2, 2, 2), (3, 3, 3, 1))\n\n    # Test that the chunks argument can be 'auto'.\n    ds_reopened = xr.open_dataset(ds, engine=PassThroughBackendEntrypoint, chunks=\"auto\")\n    assert ds_reopened[\"a\"].chunks\n\n    # Test that the chunks argument can be None.\n    ds_reopened = xr.open_dataset(ds, engine=PassThroughBackendEntrypoint, chunks=None)\n    assert not ds_reopened[\"a\"].chunks\n\n    # Test that the chunks argument can be an int.\n    chunks = 2\n    ds_reopened = xr.open_dataset(ds, engine=PassThroughBackendEntrypoint, chunks=chunks)\n    assert ds_reopened[\"a\"].chunks == ((2, 2, 2, 2, 2), (2, 2, 2, 2, 2))\n", "def test_open_dataset_with_chunks_minus_one() -> None:\n    \"\"\"Test that chunks=-1 works as expected.\"\"\"\n    dataset = xr.Dataset(\n        dict(a=(\"x\", np.arange(10))), coords=dict(x=(\"x\", np.arange(10)))\n    )\n    with assert_no_warnings():\n        actual = xr.open_dataset(\n            dataset, engine=PassThroughBackendEntrypoint, chunks=-1\n        )\n    assert_identical(dataset, actual)\n    assert actual[\"a\"].chunks == ((10,),)\n", "def test_open_dataset_chunks_auto() -> None:\n    \"\"\"Test that chunks='auto' raises a warning when using engine='scipy'.\"\"\"\n    with pytest.warns(UserWarning, match=\"`chunks='auto'` cannot be used with `engine='scipy'`.\"):\n        xr.open_dataset(\"example.nc\", engine=\"scipy\", chunks=\"auto\")\n", "def test_to_zarr_region() -> None:\n    # GH3818\n    data = np.random.rand(10, 10)\n    dataset = xr.Dataset(dict(data=((\"x\", \"y\"), data)), coords=dict(x=range(10), y=range(10)))\n\n    store = \"test_to_zarr_region.zarr\"\n    region1 = dict(x=slice(0, 5), y=slice(0, 5))\n    region2 = dict(x=slice(5, 10), y=slice(5, 10))\n\n    dataset.isel(region1).to_zarr(store, mode=\"w\")\n    dataset.isel(region2).to_zarr(store, mode=\"a\", append_dim=None, region=region2)\n\n    loaded = xr.open_zarr(store)\n    assert_identical(dataset, loaded)\n"], "sample_624": ["def test_short_data_repr_for_duck_array() -> None:\n    class CustomArray:\n            return NotImplemented\n\n            return \"CustomArray\"\n\n    var = xr.Variable(\"x\", CustomArray())\n    actual = formatting.short_data_repr(var)\n    assert actual == \"CustomArray\"\n", "def test_limit_lines() -> None:\n    string = \"\\n\".join(str(i) for i in range(100))\n    actual = formatting.limit_lines(string, limit=5)\n    expected = \"0\\n1\\n...\\n98\\n99\"\n    assert actual == expected\n\n    string = \"\\n\".join(str(i) for i in range(3))\n    actual = formatting.limit_lines(string, limit=5)\n    expected = \"0\\n1\\n2\"\n    assert actual == expected\n", "def test_inline_variable_array_repr_timedelta64() -> None:\n    import numpy as np\n\n    variable = xr.Variable(\"x\", np.array([np.timedelta64(1, \"D\")]))\n    actual = formatting.inline_variable_array_repr(variable, max_width=30)\n    assert actual == \"[1 days]\"\n", "def test_format_timedelta() -> None:\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\"),\n        (np.timedelta64(1, \"D\"), \"1 days 00:00:00\"),\n        (np.timedelta64(-1, \"D\"), \"-1 days +00:00:00\"),\n        (np.timedelta64(3, \"h\"), \"0 days 03:00:00\"),\n        (np.array([pd.Timedelta(\"10 days 1 hour\")]), \"10 days 01:00:00\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n\n    # Test with different format options\n    timedelta_str = formatting.format_timedelta(pd.Timedelta(\"10 days 1 hour\"), \"date\")\n    assert timedelta_str == \"10 days\"\n\n    timedelta_str = formatting.format_timedelta(pd.Timedelta(\"10 days 1 hour\"), \"time\")\n    assert timedelta_str == \"01:00:00\"\n", "def test_inline_dask_repr() -> None:\n    import dask.array as da\n\n    array = da.from_array(np.arange(24).reshape(2, 3, 4), chunks=(1, 3, 2))\n    actual = formatting.inline_dask_repr(array)\n    expected = \"dask.array<chunksize=(1, 3, 2)>\"\n    assert actual == expected\n\n    # Test with a meta attribute\n    array._meta = np.zeros(())\n    actual = formatting.inline_dask_repr(array)\n    expected = \"dask.array<chunksize=(1, 3, 2), meta=np.ndarray>\"\n    assert actual == expected\n"], "sample_625": ["def test_cross_broadcasting() -> None:\n    # Test broadcasting of cross product along common dimensions\n    a = xr.DataArray(\n        np.array([[1, 2], [3, 4]]),\n        dims=[\"x\", \"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    b = xr.DataArray(\n        np.array([4, 5]),\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n\n    expected = np.cross(a.values, b.values)\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n\n    # Test broadcasting with different shapes\n    a = xr.DataArray(\n        np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]),\n        dims=[\"time\", \"x\", \"cartesian\"],\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            x=([\"x\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\"]),\n        ),\n    )\n    b = xr.DataArray(\n        np.array([[4, 5], [6, 7]]),\n        dims=[\"x\", \"cartesian\"],\n        coords=dict(x=([\"x\"], [0, 1]), cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n\n    expected = np.cross(a.values, b.values[:, np.newaxis, :])\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_missing_dims() -> None:\n    # Test that cross raises an error when input arrays are missing dimensions\n    a = xr.DataArray([1, 2], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"y\"])\n\n    with pytest.raises(ValueError, match=r\"Dimension 'x' not on b\"):\n        xr.cross(a, b, dim=\"x\")\n\n    with pytest.raises(ValueError, match=r\"Dimension 'y' not on a\"):\n        xr.cross(a, b, dim=\"y\")\n", "def test_cross_vectorized() -> None:\n    # Test that xr.cross is vectorized along non-core dimensions\n    a = xr.DataArray(\n        np.random.rand(2, 3, 4),\n        dims=[\"time\", \"cartesian\", \"var\"],\n        coords=dict(\n            time=([\"time\"], np.arange(0, 2)),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n            var=([\"var\"], [1, 1.5, 2, 2.5]),\n        ),\n    )\n    b = xr.DataArray(\n        np.random.rand(2, 3, 4),\n        dims=[\"time\", \"cartesian\", \"var\"],\n        coords=dict(\n            time=([\"time\"], np.arange(0, 2)),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n            var=([\"var\"], [1, 1.5, 2, 2.5]),\n        ),\n    )\n\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    expected = np.cross(a.values, b.values, axis=1)\n\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_3d() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n\n    expected = np.cross(a.values, b.values)\n    actual = xr.cross(a, b, dim=\"cartesian\")\n\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_2d() -> None:\n    a = xr.DataArray(\n        [[1, 2], [3, 4]], dims=(\"x\", \"y\"), coords={\"x\": [0, 1], \"y\": [0, 1]}\n    )\n    b = xr.DataArray(\n        [[5, 6], [7, 8]], dims=(\"x\", \"y\"), coords={\"x\": [0, 1], \"y\": [0, 1]}\n    )\n\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"z\")\n\n    actual = xr.cross(a, b, dim=\"x\")\n    expected = np.cross(a.values, b.values, axis=0)\n    assert_array_equal(actual.values, expected)\n\n    actual = xr.cross(a, b, dim=\"y\")\n    expected = np.cross(a.values, b.values, axis=1)\n    assert_array_equal(actual.values, expected)\n"], "sample_626": ["def test_restore_dtype_on_multiindexes_with_missing_values(dtype: str) -> None:\n    foo = xr.Dataset(coords={\"bar\": (\"bar\", np.array([0, np.nan], dtype=dtype))})\n    foo = foo.stack(baz=(\"bar\",))\n    assert str(foo[\"bar\"].values.dtype) == dtype\n", "def test_as_indexable() -> None:\n    array = np.array([1, 2, 3])\n    assert isinstance(as_indexable(array), NumpyIndexingAdapter)\n\n    array = pd.Index([1, 2, 3])\n    assert isinstance(as_indexable(array), PandasIndexingAdapter)\n\n    import dask.array as da\n    array = da.from_array(np.array([1, 2, 3]))\n    assert isinstance(as_indexable(array), DaskIndexingAdapter)\n", "def test_expanded_indexer():\n    key = (1, slice(None), Ellipsis, [1, 2, 3])\n    ndim = 5\n\n    expanded_key = expanded_indexer(key, ndim)\n    expected_key = (1, slice(None), slice(None), [1, 2, 3], slice(None))\n    assert expanded_key == expected_key\n", "def test_create_mask():\n    shape = (3, 4)\n    indexer1 = VectorizedIndexer((np.array([0, 1]), np.array([-1, 2])))\n    mask1 = create_mask(indexer1, shape)\n    expected_mask1 = np.array([[True, False], [False, True]])\n    assert_array_equal(mask1, expected_mask1)\n\n    indexer2 = OuterIndexer((np.array([0, -1]), slice(None)))\n    mask2 = create_mask(indexer2, shape)\n    expected_mask2 = np.array([[True, True, True, True], [False, False, False, False]])\n    assert_array_equal(mask2, expected_mask2)\n\n    indexer3 = BasicIndexer((-1, slice(None)))\n    mask3 = create_mask(indexer3, shape)\n    expected_mask3 = False\n    assert mask3 == expected_mask3\n", "def test_LazilyIndexedArray_getitem() -> None:\n    array = np.array([1, 2, 3, 4])\n    lazily_indexed_array = LazilyIndexedArray(array)\n\n    # BasicIndexer with slice\n    key = BasicIndexer((slice(1, 3),))\n    result = lazily_indexed_array[key]\n    expected = np.array([2, 3])\n    assert_array_equal(result, expected)\n\n    # BasicIndexer with integer\n    key = BasicIndexer((1,))\n    result = lazily_indexed_array[key]\n    expected = np.array([2])\n    assert_array_equal(result, expected)\n\n    # OuterIndexer with 1d numpy array\n    key = OuterIndexer((np.array([1, 3]),))\n    result = lazily_indexed_array[key]\n    expected = np.array([2, 4])\n    assert_array_equal(result, expected)\n\n    # VectorizedIndexer\n    key = VectorizedIndexer((np.array([1, 3]),))\n    result = lazily_indexed_array[key]\n    expected = np.array([2, 4])\n    assert_array_equal(result, expected)\n"], "sample_627": ["def test_concat_positions() -> None:\n    ds1 = Dataset()\n    ds2 = Dataset()\n    ds3 = Dataset()\n\n    # Specify positions\n    actual = concat([ds1, ds2, ds3], dim=\"new_dim\", positions=[[0], [1], [2]])\n    expected = Dataset(coords={\"new_dim\": [0, 1, 2]})\n    assert_identical(actual, expected)\n\n    # No positions\n    actual = concat([ds1, ds2, ds3], dim=\"new_dim\")\n    expected = Dataset(coords={\"new_dim\": [0, 1, 2]})\n    assert_identical(actual, expected)\n\n    # Positions with gaps\n    actual = concat([ds1, ds2, ds3], dim=\"new_dim\", positions=[[0], [5], [9]])\n    expected = Dataset(coords={\"new_dim\": [0, 5, 9]})\n    assert_identical(actual, expected)\n\n    # Duplicate positions\n    with pytest.raises(ValueError, match=r\"Positions must be unique\"):\n        concat([ds1, ds2, ds3], dim=\"new_dim\", positions=[[0], [0], [2]])\n", "def test_concat_dataarray_with_new_dim() -> None:\n    da1 = DataArray([1, 2], dims=\"x\")\n    da2 = DataArray([3, 4], dims=\"x\")\n\n    expected = DataArray([[1, 2], [3, 4]], dims=[\"y\", \"x\"])\n    actual = concat([da1, da2], dim=\"y\")\n    assert_identical(actual, expected)\n", "def test_concat_no_conflict_attrs() -> None:\n    ds1 = Dataset(attrs={\"a\": 1, \"b\": 2})\n    ds2 = Dataset(attrs={\"a\": 1, \"c\": 3})\n\n    expected = Dataset(attrs={\"a\": 1, \"b\": 2, \"c\": 3})\n    actual = concat([ds1, ds2], dim=\"new_dim\", combine_attrs=\"no_conflicts\")\n\n    assert_identical(actual, expected)\n", "def test_concat_along_new_dim_with_name() -> None:\n    ds1 = Dataset()\n    ds2 = Dataset()\n    dim = DataArray([0, 1], dims=\"new_dim\", name=\"new_dim\")\n    actual = concat([ds1, ds2], dim=dim)\n    expected = Dataset(coords={\"new_dim\": [0, 1]})\n    assert_identical(actual, expected)\n", "def test_concat_dim_index() -> None:\n    # Test that the dim index is properly updated when concatenating along a new dimension.\n    da1 = DataArray([1, 2, 3], dims=\"x\")\n    da2 = DataArray([4, 5, 6], dims=\"x\")\n\n    actual = concat([da1, da2], dim=\"y\")\n    expected = DataArray([[1, 2, 3], [4, 5, 6]], dims=[\"y\", \"x\"])\n\n    assert_identical(actual, expected)\n"], "sample_628": ["def test_spelling_store_unknown_words(self):\n    self.checker.config.spelling_store_unknown_words = True\n    with tempfile.NamedTemporaryFile(mode=\"w\") as tmp_file:\n        self.checker.config.spelling_private_dict_file = tmp_file.name\n        self.checker.open()\n        stmt = astroid.extract_node(\n            'class ComentAbc(object):\\n   \"\"\"This is a bad coment\"\"\"\\n   pass'\n        )\n        self.checker.visit_classdef(stmt)\n        tmp_file.flush()\n        with open(tmp_file.name, \"r\") as private_dict_file:\n            assert private_dict_file.read() == \"coment\\n\"\n", "def test_store_unknown_words(self):\n    self.checker.config.spelling_store_unknown_words = True\n    stmt = astroid.extract_node(\n        '''def f():\n    \"\"\"This is a tezt\"\"\"", "def test_store_unknown_words(self):\n    self.checker.config.spelling_store_unknown_words = True\n    self.checker.open()\n    with open(self.checker.config.spelling_private_dict_file, \"w\") as f:\n        f.write(\"\")\n    stmt = astroid.extract_node(\n        'class ComentAbc(object):\\n   \"\"\"This is a bad coment\"\"\"\\n   pass'\n    )\n    self.checker.visit_classdef(stmt)\n    with open(self.checker.config.spelling_private_dict_file, \"r\") as f:\n        unknown_words = f.read().strip().splitlines()\n    assert \"coment\" in unknown_words\n", "def test_store_unknown_words(self):\n    self.checker.config.spelling_store_unknown_words = True\n    with tempfile.NamedTemporaryFile(mode=\"w\") as tmp_file:\n        self.checker.config.spelling_private_dict_file = tmp_file.name\n        self.checker.open()\n        self.checker.process_tokens(_tokenize_str(\"# msitake\"))\n        tmp_file.seek(0)\n        assert tmp_file.read() == \"msitake\\n\"\n", "def test_store_unknown_words(self):\n    self.checker.config.spelling_store_unknown_words = True\n    self.checker.open()\n    stmt = astroid.extract_node(\n        'class ComentAbc(object):\\n   \"\"\"ComentAbc with a bad coment\"\"\"\\n   pass'\n    )\n    self.checker.visit_classdef(stmt)\n    assert \"coment\" in self.checker.unknown_words\n"], "sample_629": ["def test_expand_modules_with_ignore_list():\n    ignore_list = [\"unittest_lint.py\"]\n    ignore_list_re, ignore_list_paths_re = [], []\n    files_or_modules = [Path(__file__).parent]\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    expected = [init_of_package, this_file_from_init]\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_ignore_list():\n    ignore_list = [\"unittest_lint.py\"]\n    ignore_list_re, ignore_list_paths_re = [], []\n    files_or_modules = [Path(__file__).parent]\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    expected = [init_of_package, this_file_from_init]\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_namespace_package():\n    namespace_package_path = str(TEST_DIRECTORY / \"lint/namespace_package\")\n    ignore_list, ignore_list_re, ignore_list_paths_re = [], [], []\n    modules, errors = expand_modules(\n        [namespace_package_path], ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    assert len(modules) > 1  # A namespace package should expand to multiple modules\n    assert not errors\n\n    # Check that all expanded modules have the correct basepath and basename\n    for module in modules:\n        assert module[\"basepath\"] == str(TEST_DIRECTORY / \"lint/namespace_package/__init__.py\")\n        assert module[\"basename\"] == \"lint.namespace_package\"\n", "def test_expand_modules_with_ignore_list():\n    ignore_list = [\"lint.unittest_lint\"]\n    ignore_list_re, ignore_list_paths_re = [], []\n    files_or_modules = [Path(__file__).parent]\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    expected = [init_of_package, this_file_from_init]\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_ignore_list():\n    files_or_modules = [__file__]\n    ignore_list = [\"lint.unittest_expand_modules\"]\n    ignore_list_re, ignore_list_paths_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    assert not modules\n    assert not errors\n"], "sample_630": ["def test_get_annotation_label(func, return_type, expected):\n    \"\"\"Test get_annotation_label function\"\"\"\n    node = astroid.extract_node(func)\n    got = get_annotation_label(node.returns)\n    assert got == expected, f\"got {got} instead of {expected} for value {node}\"\n", "def test_get_values(assign, expected):\n    \"\"\"Test get_values method of DotWriter\"\"\"\n    node = astroid.extract_node(assign)\n    writer = DotWriter(Config())\n    got = writer.get_values(node)\n    assert got == expected, f\"got {got} instead of {expected} for value {node}\"\n", "def test_get_annotation_label(func, expected):\n    \"\"\"Test get_annotation_label function\"\"\"\n    node = astroid.extract_node(func)\n    got = get_annotation_label(node.returns)\n    assert got == expected, f\"got {got} instead of {expected} for value {node}\"\n", "def test_get_annotation_label(func, label):\n    \"\"\"Test get_annotation_label function\"\"\"\n    node = astroid.extract_node(func)\n    got = get_annotation_label(node.returns)\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_function(func, label):\n    \"\"\"Function node\"\"\"\n    node = astroid.extract_node(func)\n    args = node.args.args\n    annotations = node.args.annotations\n    for arg, annotation in zip(args, annotations):\n        got = get_annotation_label(annotation).name\n        assert isinstance(arg, astroid.AssignName)\n        assert got == label, f\"got {got} instead of {label} for value {node}\"\n"], "sample_631": ["    def test_type_annotation_names(self):\n        \"\"\"Make sure type annotations don't cause false positives\"\"\"\n        node = astroid.parse(\n            \"\"\"\n        from typing import List\n\n            baz = 1\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n", "def test_undefined_variable_in_type_annotation(self):\n    \"\"\"Make sure undefined variables in type annotations are reported\"\"\"\n    node = astroid.parse(\n        \"\"\"\n    a: UndefinedVariable = 1\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"undefined-variable\", node=node, args=\"UndefinedVariable\")\n    ):\n        self.walk(node)\n", "def test_unpacking_non_sequence_with_subscript(self):\n    \"\"\"Make sure unpacking non-sequence raises an error when subscript is used\"\"\"\n    node = astroid.parse(\n        \"\"\"\n    a, b = [1, 2][0]\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"unpacking-non-sequence\", node=node)\n    ):\n        self.walk(node)\n", "def test_import_inside_with_statement(self):\n    \"\"\"Make sure imports inside a with statement are properly checked\"\"\"\n    node = astroid.parse(\n        \"\"\"\n    with open('foo.txt', 'r') as f:\n        import os\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"unused-import\", node=node.body[0].body[0], args=\"import os\")\n    ):\n        self.walk(node)\n", "    def test_unpacking_non_sequence(self):\n        \"\"\"Make sure unpacking non-sequence variables are properly checked\"\"\"\n        node = astroid.parse(\n            \"\"\"\n        a, b = 1\n        c, d = \"test\"\n        e, f = object()\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"unpacking-non-sequence\", node=node.body[0], args=\"\"),\n            Message(\"unpacking-non-sequence\", node=node.body[1], args=\"\"),\n            Message(\"unpacking-non-sequence\", node=node.body[2], args=\"\"),\n        ):\n            self.walk(node)\n"], "sample_632": ["def test_combine_mapreduce_data():\n    \"\"\"Tests that combine_mapreduce_data correctly recombines data\"\"\"\n    linter = PyLinter(reporter=Reporter())\n    sim = similar.SimilarChecker(linter)\n\n    # Manually perform a 'map' type function\n    source_streams = (\n        str(INPUT / \"similar_lines_a.py\"),\n        str(INPUT / \"similar_lines_b.py\"),\n    )\n    data = []\n    for source_fname in source_streams:\n        sim_part = similar.SimilarChecker(linter)\n        with open(source_fname) as stream:\n            sim_part.append_stream(source_fname, stream)\n        # The map bit, can you tell? ;)\n        data.extend(sim_part.get_map_data())\n\n    # Now reduce and recombine the data\n    sim.combine_mapreduce_data(data)\n\n    # Check that we have the correct number of linesets after recombining\n    assert len(source_streams) == len(sim.linesets)\n\n    # Check that each lineset has the correct name after recombining\n    for source_fname, lineset_obj in zip(source_streams, sim.linesets):\n        assert source_fname == lineset_obj.name\n", "def test_append_stream_encoding():\n    sim = similar.Similar()\n    with open(SIMILAR1, \"rb\") as stream:\n        sim.append_stream(SIMILAR1, stream, encoding=\"utf-8\")\n    assert len(sim.linesets) == 1\n    lineset = sim.linesets[0]\n    assert lineset.name == SIMILAR1\n    assert len(lineset._real_lines) > 0\n", "def test_display_sims_with_no_similarities():\n    sim = similar.Similar()\n    sim._display_sims([])\n    # No output should be displayed when there are no similarities\n    assert True\n", "def test_get_similarity_report():\n    \"\"\"Tests that _get_similarity_report returns the expected report\"\"\"\n    sim = similar.Similar()\n    sim.linesets = [\n        similar.LineSet(\"file1\", [\"line1\", \"line2\", \"line3\"], ignore_comments=False),\n        similar.LineSet(\"file2\", [\"line1\", \"line2\", \"line4\"], ignore_comments=False),\n    ]\n    similarities = [(2, {(sim.linesets[0], 0), (sim.linesets[1], 0)})]\n    report = sim._get_similarity_report(similarities)\n    expected_report = (\n        \"\\n2 similar lines in 2 files\\n\"\n        \"==file1:0\\n\"\n        \"==file2:0\\n\"\n        \"   line1\\n\"\n        \"   line2\\n\"\n        \"TOTAL lines=6 duplicates=2 percent=33.33\\n\"\n    )\n    assert report == expected_report\n", "def test_combine_mapreduce_data():\n    \"\"\"Tests that a SimilarChecker respects the MapReduceMixin interface\"\"\"\n    linter = PyLinter(reporter=Reporter())\n\n    # Add a parallel checker to ensure it can map and reduce\n    sim = similar.SimilarChecker(linter)\n\n    source_streams = (\n        str(INPUT / \"similar_lines_a.py\"),\n        str(INPUT / \"similar_lines_b.py\"),\n    )\n\n    data = []\n\n    # Manually perform a 'map' type function\n    for source_fname in source_streams:\n        with open(source_fname) as stream:\n            sim.append_stream(source_fname, stream)\n        # The map bit, can you tell? ;)\n        data.extend(sim.get_map_data())\n\n    # Now, manually 'reduce' this data into our checker.\n    sim.combine_mapreduce_data([data])\n\n    assert len(data) == len(sim.linesets)\n    for expected_lineset, actual_lineset in zip(data, sim.linesets):\n        assert expected_lineset.name == actual_lineset.name\n        # There doesn't seem to be a faster way of doing this, yet.\n        lines = (line for idx, line in actual_lineset.enumerate_stripped())\n        assert tuple(expected_lineset._stripped_lines) == tuple(lines)\n"], "sample_633": ["def test_get_similarity_report() -> None:\n    sim = similar.Similar()\n    sim.linesets = [\n        similar.LineSet(\"file1\", [\"import os\", \"print('Hello World')\"]),\n        similar.LineSet(\"file2\", [\"import os\", \"print('Hello World')\"]),\n    ]\n    similarities = sim._compute_sims()\n    report = sim._get_similarity_report(similarities)\n    assert report.startswith(\"2 similar lines in 2 files\")\n    assert \"TOTAL lines=4 duplicates=2 percent=50.00\" in report\n", "def test_stripped_lines() -> None:\n    lines = [\n        \"import os\",\n        \"\",\n        \"def foo():\",\n        '    \"\"\"Docstring\"\"\"',\n        \"    # Comment\",\n        \"    pass\",\n        \"\",\n        \"# Another comment\",\n        \"class Bar:\",\n        '    \"\"\"Another docstring\"\"\"',\n        \"    def baz(self):\",\n        \"        # Yet another comment\",\n        \"        pass\",\n    ]\n\n    expected_stripped_lines = [\n        LineSpecifs(text=\"import os\", line_number=0),\n        LineSpecifs(text=\"def foo():\", line_number=2),\n        LineSpecifs(text=\"pass\", line_number=5),\n        LineSpecifs(text=\"class Bar:\", line_number=8),\n        LineSpecifs(text=\"def baz(self):\", line_number=10),\n        LineSpecifs(text=\"pass\", line_number=12),\n    ]\n\n    stripped_lines_result = similar.stripped_lines(\n        lines, ignore_comments=True, ignore_docstrings=True, ignore_imports=False, ignore_signatures=False\n    )\n\n    assert len(stripped_lines_result) == len(expected_stripped_lines)\n    for i in range(len(stripped_lines_result)):\n        assert stripped_lines_result[i].text == expected_stripped_lines[i].text\n        assert stripped_lines_result[i].line_number == expected_stripped_lines[i].line_number\n", "def test_set_duplicate_lines_to_negative() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--duplicates=-1\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert output.getvalue() == \"\"\n", "def test_run_with_empty_lineset() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        sim = similar.Similar()\n        sim.append_stream(\"test_file\", StringIO(\"\"))\n        sim.run()\n    assert ex.value.code == 0\n    assert output.getvalue().strip() == \"TOTAL lines=0 duplicates=0 percent=0.00\"\n", "def test_similarities_with_min_lines() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--duplicates=3\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\""], "sample_634": ["    def test_expand_modules_with_non_existent_modules(self, files_or_modules, expected):\n        \"\"\"Test expand_modules with non-existent modules\"\"\"\n        ignore_list, ignore_list_re = [], []\n        _, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self.checker, \"ignore-paths\"),\n        )\n        assert len(errors) == len(expected)\n        for error in errors:\n            assert error[\"key\"] == \"fatal\"\n            assert error[\"mod\"] in [e[\"mod\"] for e in expected]\n", "def test_expand_modules_with_ignore_list(self, files_or_modules, ignore_list, expected):\n    \"\"\"Test expand_modules with an ignore-list\"\"\"\n    ignore_list_re = []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    assert modules == expected\n    assert not errors\n", "    def test_expand_modules_with_non_existent_module(self, files_or_modules, expected):\n        \"\"\"Test expand_modules with a non-existent module\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        assert modules == expected\n        assert len(errors) == 1\n        assert errors[0][\"key\"] == \"fatal\"\n", "    def test_expand_modules_with_ignore_list(self, files_or_modules, ignore_list, expected):\n        \"\"\"Test expand_modules with an ignore list\"\"\"\n        ignore_list_re = []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "def test_expand_modules_with_import_error(files_or_modules, ignore_list, expected):\n    \"\"\"Test expand_modules with import errors\"\"\"\n    ignore_list_re = []\n    ignore_paths_re = get_global_option(None, \"ignore-paths\")\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        ignore_paths_re,\n    )\n    assert modules == expected\n    assert len(errors) == len(files_or_modules)\n    for error in errors:\n        assert error[\"key\"] == \"fatal\"\n"], "sample_635": ["def test_match_param_docs_numpy(self) -> None:\n    \"\"\"Tests that match_param_docs correctly handles Numpy style docstrings\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''Prints this, other and that\n\n        Parameters\n        ----------\n        this : str\n            Printed first\n        other : int\n            Other args\n\n        Keyword Parameters\n        -----------------\n        that : bool\n            Printed second\n        '''\n        print(this, that, other)\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_google_style_with_optional_type(self) -> None:\n    \"\"\"Example of a function with Google style parameter documentation in the docstring.\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''Function foo ...\n\n        Args:\n            x (int): bla x\n            y (str, optional): bla y. Defaults to 'default'.\n\n        Returns:\n            bool: result\n        '''\n        return True\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_finds_property_return_type_with_annotation_numpy(self) -> None:\n    \"\"\"Example of a property having return documentation in\n    a numpy style docstring with type annotation\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    class Foo(object):\n        @property\n            '''docstring ...\n\n            Raises\n            ------\n                RuntimeError\n                    Always\n            '''\n            raise RuntimeError()\n            return 10\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_numpy_property_return_type(self) -> None:\n    \"\"\"Example of a property having return documentation in\n    a numpy style docstring\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    class Foo(object):\n        @property\n            '''int: docstring ...\n            '''\n            return 10\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_numpy_docstring_with_colon_in_parameter_description(self) -> None:\n    \"\"\"Example of a function with a Numpy style docstring and a colon in the parameter description\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''docstring ...\n\n        Parameters\n        ----------\n        x: int\n            Parameter with a colon: in its description\n        y: str\n            Another parameter\n\n        Returns\n        -------\n        bool\n            Whether something is true or not\n        '''\n        return True\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n"], "sample_636": ["def test_duplicate_code_ignored_comments(self) -> None:\n    \"\"\"Tests that comments are ignored when checking for duplicate code.\"\"\"\n    path = join(DATA, \"ignored_comments\")\n    self._runtest([path, \"--disable=all\", \"--enable=duplicate-code\"], code=0)\n", "def test_duplicate_code_with_ignore_imports(self) -> None:\n    \"\"\"Tests duplicate code detection with ignore-imports option.\"\"\"\n    path = join(DATA, \"duplicate_code_with_imports\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_with_ignored_imports(self) -> None:\n    \"\"\"Tests ignoring imports in duplicate code.\"\"\"\n    path = join(DATA, \"duplicate_code_with_imports\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [\n            path,\n            \"--disable=all\",\n            \"--enable=duplicate-code\",\n            \"--ignore-imports=yes\",\n        ],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_min_similarity_lines(self) -> None:\n    \"\"\"Tests that duplicate code is not reported when min-similarity-lines is higher than the duplicate block.\"\"\"\n    path = join(DATA, \"raw_strings_all\")\n    expected_output = \"\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--min-similarity-lines=10\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_with_ignore_comments(self) -> None:\n    \"\"\"Tests that duplicate code is found even when comments are ignored.\"\"\"\n    path = join(DATA, \"ignore_comments\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-comments=y\"],\n        expected_output=expected_output,\n    )\n"], "sample_637": ["def test_fixme_pattern_with_notes_rgx(self) -> None:\n    code = \"\"\"a = 1\n            # BUG1234\n            \"\"\"\n    with set_config(notes=(), notes_rgx=\"BUG\\\\d+\"):\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"BUG1234\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "def test_fixme_pattern_with_custom_notes_rgx(self) -> None:\n    code = \"\"\"a = 1\n            # CUSTOM_FIXME\n            \"\"\"\n    set_config(notes_rgx=\"CUSTOM_FIXME\")(self)\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"CUSTOM_FIXME\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_fixme_pattern_with_notes_rgx(self) -> None:\n    code = \"\"\"a = 1\n            # NOTE123\n            \"\"\"\n    set_config(notes=(), notes_rgx=\"NOTE\\\\d+\")\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"NOTE123\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_disable_option_with_notes_codetag(self) -> None:\n    code = \"\"\"a = 1\n            # pylint: disable=fixme\n            # FIXME\n            \"\"\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(code))\n\n    code = \"\"\"a = 1\n            # pylint: disable-all\n            # FIXME\n            \"\"\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(code))\n\n    code = \"\"\"a = 1\n            # pylint: disable=fixme,missing-docstring\n            # FIXME\n            \"\"\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_disable_option(self) -> None:\n    code = \"\"\"# pylint: disable=fixme\n              # FIXME this should not trigger a fixme\"\"\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(code))\n"], "sample_638": ["def test_run_without_args(mock_writer, capsys):\n    \"\"\"Test that running pyreverse without arguments prints the help message and exits.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the help message is shown to the user\n    assert \"Usage:\" in capsys.readouterr().out\n    # Check that we exited with the expected error code\n    assert wrapped_sysexit.value.code == 1\n    # Check that no diagram was created\n    mock_writer.DiagramWriter().write.assert_not_called()\n", "def test_run_with_no_arguments(capsys):\n    \"\"\"Test that running pyreverse with no arguments shows the help message.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the help message is shown to the user\n    assert \"usage: pyreverse\" in capsys.readouterr().out\n    # Check that we exited with the expected error code\n    assert wrapped_sysexit.value.code == 1\n", "def test_run_without_arguments(capsys):\n    \"\"\"Test running pyreverse without any arguments.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the help message is printed and we exit with the expected error code\n    assert \"Usage:\" in capsys.readouterr().out\n    assert wrapped_sysexit.value.code == 1\n", "def test_output_directory_option(mock_writer, capsys):\n    \"\"\"Test that the output directory option works as expected.\"\"\"\n    output_dir = \"test_output\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([\"-o\", \"png\", \"-d\", output_dir, TEST_DATA_DIR])\n    mock_writer.DiagramWriter.assert_called_once_with(main.Run([TEST_DATA_DIR]).config)\n    assert wrapped_sysexit.value.code == 0\n    assert f\"Output directory set to '{output_dir}'\" in capsys.readouterr().out\n", "def test_run_without_arguments(mock_writer, capsys):\n    \"\"\"Test that running without arguments prints the help message and exits.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the help message is printed\n    assert \"usage: pyreverse\" in capsys.readouterr().out\n    # Check that we exited cleanly\n    assert wrapped_sysexit.value.code == 1\n    # Check that no diagram was written\n    mock_writer.DiagramWriter().write.assert_not_called()\n"], "sample_639": ["def test_base_checker_get_full_documentation() -> None:\n    \"\"\"Test get_full_documentation method of BaseChecker.\"\"\"\n    basic = OtherBasicChecker()\n    expected_doc = str(basic)\n    assert basic.get_full_documentation(\n        msgs=basic.msgs, options=basic.options_and_values(), reports=basic.reports\n    ) == expected_doc\n    assert basic.get_full_documentation(\n        msgs=basic.msgs, options=basic.options_and_values(), reports=basic.reports, doc=\"Test Doc\"\n    ).startswith(\"Basic checker Documentation\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nTest Doc\")\n", "def test_base_checker_get_full_documentation() -> None:\n    \"\"\"Test BaseChecker.get_full_documentation method.\"\"\"\n    checker = OtherBasicChecker()\n    doc = \"This is a test documentation.\"\n    result = checker.get_full_documentation(\n        msgs=checker.msgs, options=checker.options_and_values(), reports=checker.reports, doc=doc\n    )\n    expected = f\"\"\"\\", "def test_base_checker_create_message_definition_from_tuple() -> None:\n    \"\"\"Test creating a MessageDefinition from a tuple.\"\"\"\n    checker = OtherBasicChecker()\n    msgid = \"W0001\"\n    msg_tuple = (\n        \"Basic checker has an example.\",\n        \"basic-checker-example\",\n        \"Used nowhere and serves no purpose.\",\n    )\n    message_definition = checker.create_message_definition_from_tuple(msgid, msg_tuple)\n    assert message_definition.msgid == msgid\n    assert message_definition.msg == msg_tuple[0]\n    assert message_definition.symbol == msg_tuple[1]\n    assert message_definition.help == msg_tuple[2]\n\n    # Test with options\n    msg_tuple_with_options = (\n        \"Basic checker has an example.\",\n        \"basic-checker-example\",\n        \"Used nowhere and serves no purpose.\",\n        {\"scope\": \"module\"},\n    )\n    message_definition_with_options = checker.create_message_definition_from_tuple(\n        msgid, msg_tuple_with_options\n    )\n    assert message_definition_with_options.msgid == msgid\n    assert message_definition_with_options.msg == msg_tuple_with_options[0]\n    assert message_definition_with_options.symbol == msg_tuple_with_options[1]\n    assert message_definition_with_options.help == msg_tuple_with_options[2]\n    assert message_definition_with_options.scope == \"module\"\n\n    # Test with invalid message tuple\n    invalid_msg_tuple = (\"Basic checker has an example.\", \"basic-checker-example\")\n    try:\n        checker.create_message_definition_from_tuple(msgid, invalid_msg_tuple)\n        assert False, \"Expected InvalidMessageError\"\n    except InvalidMessageError as e:\n        assert str(e).startswith(\"Messages should have a msgid and a symbol.\")\n", "def test_base_checker_create_message_definition_from_tuple() -> None:\n    \"\"\"Test create_message_definition_from_tuple method of BaseChecker.\"\"\"\n    basic = OtherBasicChecker()\n    msgid = \"W0001\"\n    msg_tuple = (\n        \"Basic checker has an example.\",\n        \"basic-checker-example\",\n        \"Used nowhere and serves no purpose.\",\n    )\n    message_definition = basic.create_message_definition_from_tuple(msgid, msg_tuple)\n    assert message_definition.msgid == msgid\n    assert message_definition.msg == msg_tuple[0]\n    assert message_definition.symbol == msg_tuple[1]\n    assert message_definition.help == msg_tuple[2]\n\n    # Test with invalid msg_tuple length\n    msg_tuple_invalid = (\"Invalid message\",)\n    try:\n        basic.create_message_definition_from_tuple(msgid, msg_tuple_invalid)\n        assert False, \"Expected InvalidMessageError to be raised\"\n    except InvalidMessageError as e:\n        assert str(e).startswith(\"Messages should have a msgid and a symbol.\")\n", "def test_base_checker_messages() -> None:\n    \"\"\"Test messages property of BaseChecker class.\"\"\"\n    checker = OtherBasicChecker()\n    assert len(checker.messages) == 1\n    message = checker.messages[0]\n    assert message.msgid == \"W0001\"\n    assert message.msg == \"Basic checker has an example.\"\n    assert message.descr == \"Used nowhere and serves no purpose.\"\n    assert message.symbol == \"basic-checker-example\"\n\n    # Test get_message_definition method\n    message_definition = checker.get_message_definition(\"W0001\")\n    assert message_definition.msgid == \"W0001\"\n    assert message_definition.msg == \"Basic checker has an example.\"\n    assert message_definition.descr == \"Used nowhere and serves no purpose.\"\n    assert message_definition.symbol == \"basic-checker-example\"\n\n    # Test create_message_definition_from_tuple method\n    msg_tuple = (\n        \"Basic checker has an example.\",\n        \"basic-checker-example\",\n        \"Used nowhere and serves no purpose.\",\n    )\n    message_definition = checker.create_message_definition_from_tuple(\"W0001\", msg_tuple)\n    assert message_definition.msgid == \"W0001\"\n    assert message_definition.msg == \"Basic checker has an example.\"\n    assert message_definition.descr == \"Used nowhere and serves no purpose.\"\n    assert message_definition.symbol == \"basic-checker-example\"\n"], "sample_640": ["def test_is_test_condition() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    if True:  #@\n        pass\n\n    while False:  #@\n        pass\n\n    assert True  #@\n\n    a = True if False else True  #@\n    \"\"\"\n    )\n    assert isinstance(code, list) and len(code) == 4\n\n    assert utils.is_test_condition(code[0])\n    assert utils.is_test_condition(code[1])\n    assert utils.is_test_condition(code[2])\n    assert utils.is_test_condition(code[3])\n\n    not_test_conditions = astroid.extract_node(\n        \"\"\"\n    True  #@\n\n    a = True  #@\n\n\n        pass\n    \"\"\"\n    )\n    assert isinstance(not_test_conditions, list) and len(not_test_conditions) == 3\n\n    assert not utils.is_test_condition(not_test_conditions[0])\n    assert not utils.is_test_condition(not_test_conditions[1])\n    assert not utils.is_test_condition(not_test_conditions[2])\n", "def test_get_node_first_ancestor_of_type() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n                pass\n        \"\"\"\n    )\n    assert utils.get_node_first_ancestor_of_type(code, nodes.FunctionDef).name == \"outer_function\"\n    assert utils.get_node_first_ancestor_of_type(code, (nodes.FunctionDef, nodes.Module)) is not None\n    assert utils.get_node_first_ancestor_of_type(code, nodes.Module) is not None\n    assert utils.get_node_first_ancestor_of_type(code, nodes.ClassDef) is None\n", "def test_is_overload_stub() -> None:\n    node = astroid.extract_node(\n        \"\"\"\n        from typing import overload\n\n        @overload\n            ...\n        \"\"\"\n    )\n    assert utils.is_overload_stub(node) is True\n", "def test_is_call_of_name() -> None:\n    node = astroid.extract_node(\"foo()\")\n    assert utils.is_call_of_name(node, \"foo\") is True\n\n    node = astroid.extract_node(\"bar.foo()\")\n    assert utils.is_call_of_name(node, \"foo\") is False\n\n    node = astroid.extract_node(\"foo.bar()\")\n    assert utils.is_call_of_name(node, \"foo\") is False\n", "def test_is_overload_stub() -> None:\n    node = astroid.extract_node(\n        \"\"\"\n    from typing import overload\n\n    @overload\n    \"\"\"\n    )\n    assert utils.is_overload_stub(node)\n"], "sample_641": ["def test_load_result_file_does_not_exist(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n\n", "def test_load_result_file_does_not_exist(path: str) -> None:\n    assert load_results(path) is None\n\n", "def test_load_result_does_not_exist(path: str) -> None:\n    assert load_results(path) is None\n\n", "def test_load_non_existent_result(path: str) -> None:\n    assert load_results(path) is None\n", "def test_load_non_existent_result(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n"], "sample_642": ["def test_preprocess_options() -> None:\n    \"\"\"Test that pre-processable options are correctly handled.\"\"\"\n    run = Run([\"--init-hook\", \"import os\"])\n    with mock.patch(\"pylint.lint.run.Run._rcfile\") as rcfile_mock:\n        processed_args = _preprocess_options(run, [\"--init-hook\", \"import os\"])\n        assert processed_args == []\n        rcfile_mock.assert_not_called()\n\n    run = Run([\"--rcfile\", \"path/to/rcfile\"])\n    with mock.patch(\"pylint.lint.run.Run._rcfile\") as rcfile_mock:\n        processed_args = _preprocess_options(run, [\"--rcfile\", \"path/to/rcfile\"])\n        assert processed_args == []\n        rcfile_mock.assert_called_once_with(\"path/to/rcfile\")\n\n    run = Run([\"--load-plugins\", \"plugin1,plugin2\"])\n    with mock.patch(\"pylint.lint.run.Run._plugins\") as plugins_mock:\n        processed_args = _preprocess_options(run, [\"--load-plugins\", \"plugin1,plugin2\"])\n        assert processed_args == []\n        plugins_mock.extend.assert_called_once_with([\"plugin1\", \"plugin2\"])\n\n    run = Run([\"--verbose\"])\n    with mock.patch(\"pylint.lint.run.Run.verbose\") as verbose_mock:\n        processed_args = _preprocess_options(run, [\"--verbose\"])\n        assert processed_args == []\n        verbose_mock.assert_called_once_with(True)\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that pre-process options are correctly handled.\"\"\"\n    with tempdir() as chroot:\n        chroot_path = Path(chroot)\n        testutils.create_files([\"a/b/c/d/__init__.py\"])\n        os.chdir(chroot_path / \"a/b/c\")\n\n        run = Run([\"--verbose\", \"--init-hook\", \"import os\"])\n        processed_args = _preprocess_options(run, [\"--verbose\", \"--init-hook=import os\"])\n        assert processed_args == []\n\n        run = Run([\"--enable-all-extensions\"])\n        processed_args = _preprocess_options(run, [\"--enable-all-extensions\"])\n        assert processed_args == []\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that we correctly pre-process options.\"\"\"\n    with tempdir() as chroot:\n        chroot_path = Path(chroot)\n        testutils.create_files([\"a/b/c/d/__init__.py\"])\n        os.chdir(chroot_path / \"a/b/c\")\n        run = Run([\"--verbose\", \"--init-hook=import os\"])\n        processed_args = _preprocess_options(run, [\"--verbose\", \"--init-hook=import os\"])\n        assert len(processed_args) == 0\n        assert run.verbose\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that options are pre-processed correctly.\"\"\"\n    run = Run()\n    args = [\"--init-hook\", \"import os\", \"--verbose\", \"--rcfile\", \"path/to/rcfile\"]\n    processed_args = _preprocess_options(run, args)\n    assert len(processed_args) == 0\n    assert run._rcfile == \"path/to/rcfile\"\n    assert run.verbose\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that pre-processable options are correctly handled.\"\"\"\n    run = Run([\"--init-hook\", \"import os\", \"--rcfile\", \"path/to/rcfile\"])\n    with mock.patch(\"pylint.lint.run.Run._rcfile\", new_callable=mock.PropertyMock) as mock_rcfile:\n        processed_args = _preprocess_options(run, run.args)\n        assert mock_rcfile.call_count == 1\n        assert mock_rcfile.call_args[0][0] == \"path/to/rcfile\"\n        assert processed_args == []\n"], "sample_643": ["def test_colorized_output_deprecation() -> None:\n    \"\"\"TODO remove in 3.0.\"\"\"\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        ColorizedTextReporter(color_mapping={\"I\": (\"green\", \"bold\")})\n\n    assert len(cm) == 1\n    assert isinstance(cm[0].message, DeprecationWarning)\n", "def test_colorized_output_deprecated() -> None:\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        ColorizedTextReporter(color_mapping={\"I\": (\"green\", \"\")})  # type: ignore[arg-type]\n\n    assert len(cm) == 1\n    assert isinstance(cm[0].message, DeprecationWarning)\n", "def test_colorized_text_reporter() -> None:\n    \"\"\"Test ColorizedTextReporter.\"\"\"\n    output = StringIO()\n    reporter = ColorizedTextReporter(output)\n    message = Message(\n        symbol=\"missing-docstring\",\n        msg_id=\"C0123\",\n        location=MessageLocationTuple(\"abspath\", \"path\", \"module\", \"obj\", 1, 2, 1, 3),\n        msg=\"Not modified\",\n        confidence=HIGH,\n    )\n\n    reporter.handle_message(message)\n\n    assert output.getvalue().startswith(\"\\033[\")\n", "def test_colorized_reporter_output(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter = ColorizedTextReporter(output)\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\n        \"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4)\n    )\n    out_lines = output.getvalue().split(\"\\n\")\n    assert len(out_lines) == 3\n    assert out_lines[0].startswith(\"\\033[\")\n    assert out_lines[1].startswith(\"\\033[\")\n    assert out_lines[2] == \"\"\n", "def test_colorized_text_reporter_message_style() -> None:\n    \"\"\"Test ColorizedTextReporter message style.\"\"\"\n    reporter = ColorizedTextReporter()\n    msg_style = reporter._get_decoration(\"I\")\n    assert msg_style == MessageStyle(\"green\")\n\n    msg_style = reporter._get_decoration(\"C\")\n    assert msg_style == MessageStyle(None, (\"bold\",))\n\n    msg_style = reporter._get_decoration(\"R\")\n    assert msg_style == MessageStyle(\"magenta\", (\"bold\", \"italic\"))\n\n    msg_style = reporter._get_decoration(\"W\")\n    assert msg_style == MessageStyle(\"magenta\")\n\n    msg_style = reporter._get_decoration(\"E\")\n    assert msg_style == MessageStyle(\"red\", (\"bold\",))\n\n    msg_style = reporter._get_decoration(\"F\")\n    assert msg_style == MessageStyle(\"red\", (\"bold\", \"underline\"))\n\n    msg_style = reporter._get_decoration(\"S\")\n    assert msg_style == MessageStyle(\"yellow\", (\"inverse\",))\n\n    msg_style = reporter._get_decoration(\"X\")  # Unknown message type\n    assert msg_style == MessageStyle(None)\n"], "sample_644": ["def test_allow_wildcard_with_all(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"wildcard_with_all\", REGR_DATA)\n    import_from = module.body[0]\n\n    self.checker.linter.config.allow_wildcard_with_all = True\n\n    with self.assertNoMessages():\n        self.checker.visit_importfrom(import_from)\n\n    self.checker.linter.config.allow_wildcard_with_all = False\n\n    msg = MessageTest(\n        msg_id=\"wildcard-import\",\n        node=import_from,\n        args=\"module_with_all\",\n        confidence=UNDEFINED,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=24,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_importfrom(import_from)\n", "def test_import_outside_toplevel(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel\", REGR_DATA)\n    function_def = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=function_def.body[0],\n        args=\"os, sys\",\n        confidence=UNDEFINED,\n        line=3,\n        col_offset=4,\n        end_line=3,\n        end_col_offset=19,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_functiondef(function_def)\n", "def test_allow_any_import_level(self) -> None:\n    \"\"\"Test --allow-any-import-level option.\"\"\"\n    self.checker._allow_any_import_level = {\"module1\", \"module2\"}\n    node = astroid.extract_node(\n        \"\"\"\n            import module1\n            from module2 import something\n            import module3\n        \"\"\"\n    )\n    with self.assertNoMessages():\n        for child in node.body[0].body:\n            self.checker.visit_import(child)\n", "def test_import_outside_toplevel(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel\", REGR_DATA)\n    function_def = module.body[0]\n    import_node = function_def.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=import_node,\n        args=\"os\",\n        confidence=UNDEFINED,\n        line=3,\n        col_offset=4,\n        end_line=3,\n        end_col_offset=14,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n", "def test_useless_import_alias(self) -> None:\n    \"\"\"Test useless-import-alias message\"\"\"\n    module = astroid.MANAGER.ast_from_module_name(\"useless_alias\", REGR_DATA)\n    import_node = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"useless-import-alias\",\n        node=import_node,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=len(import_node.as_string()),\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n\n    # Should not emit if it's a re-export from a package __init__.py\n    package_init = astroid.MANAGER.ast_from_module_name(\"package_init\", REGR_DATA)\n    reexport_node = package_init.body[0]\n    with self.assertNoMessages():\n        self.checker.visit_import(reexport_node)\n"], "sample_645": ["def test_log_format_with_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            logger = logging.getLogger('catchlog')\n            logger.info(\"INFO message with\\\\n multiple lines\")\n\n            expected_format = \"INFO     catchlog:test_log_format.py:5 INFO message with\\\\n                 multiple lines\"\n            assert expected_format in caplog.text\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_level=INFO\n        log_format=%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\n        log_auto_indent=True\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_percent_style_multiline_auto_indent(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\")\n\n    # Test with auto-indent on.\n    caplog.handler.formatter._style._auto_indent = -1\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\")\n    lines = caplog.text.splitlines()\n    assert len(lines) == 2\n    assert lines[0].startswith(\"INFO\")\n    assert lines[1].startswith(\" \" * (len(lines[0]) - len(\"INFO boo arg1\")))\n\n    # Test with auto-indent off.\n    caplog.handler.formatter._style._auto_indent = 0\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\")\n    lines = caplog.text.splitlines()\n    assert len(lines) == 2\n    assert lines[0].startswith(\"INFO\")\n    assert not lines[1].startswith(\" \")\n\n    # Test with fixed auto-indent.\n    caplog.handler.formatter._style._auto_indent = 10\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\")\n    lines = caplog.text.splitlines()\n    assert len(lines) == 2\n    assert lines[0].startswith(\"INFO\")\n    assert lines[1].startswith(\" \" * 10)\n", "def test_caplog_records_multiple_messages(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"message1\")\n    logger.info(\"message2\")\n    assert len(caplog.records) == 2\n    assert caplog.records[0].getMessage() == \"message1\"\n    assert caplog.records[1].getMessage() == \"message2\"\n", "def test_percent_style_multiline_auto_indent(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo\\narg\")\n    assert \"boo\" in caplog.text\n    assert \"arg\" in caplog.text\n    lines = caplog.text.splitlines()\n    assert len(lines) == 2\n    assert lines[1].startswith(\" \" * (len(lines[0]) - len(lines[0].lstrip())))\n", "def test_log_report_captures_according_to_config_option_upon_success(\n    pytester: Pytester,"], "sample_646": ["def test_unittest_testcase_with_init(pytester: Pytester) -> None:\n    \"\"\"Test that __init__ is called for unittest.TestCase subclasses.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                super().__init__(*args, **kwargs)\n                self.init_called = True\n\n                assert self.init_called\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"* 1 passed in*\"])\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestIt(unittest.TestCase):\n                for x in range(3):\n                    with self.subTest(x=x):\n                        self.assertEqual(x % 2, 0)\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*::test_it?::subTest=x=0 PASSED*\",\n            \"*::test_it?::subTest=x=1 FAILED*\",\n            \"*::test_it?::subTest=x=2 PASSED*\",\n            \"*1 failed, 2 passed in *\",\n        ]\n    )\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytest.importorskip(\"unittest\")\n\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestIt(unittest.TestCase):\n                for i in range(5):\n                    with self.subTest(i=i):\n                        if i == 3:\n                            assert False\n                        else:\n                            assert True\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed, 4 passed*\"])\n", "def test_unittest_expected_failure_with_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.expectedFailure(reason=\"expected failure reason\")\n                assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines(\n        [\"*XFAIL*MyTestCase*test_failing_test*\", \"*1 xfailed*\"]\n    )\n    assert result.ret == 0\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestIt(unittest.TestCase):\n                for i in range(5):\n                    with self.subTest(i=i):\n                        if i == 3:\n                            assert False\n                        else:\n                            assert True\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 1\n    assert failed == 1\n    assert reprec.ret == 1\n"], "sample_647": ["def test_warning_types_messages(warning_class: type, expected_message: str) -> None:\n    \"\"\"Test the messages of some warnings to ensure they are correctly formatted.\"\"\"\n    if warning_class == warning_types.PytestExperimentalApiWarning:\n        warning = warning_class.simple(\"test_api\")\n    else:\n        warning = warning_class(\"test_mark\")\n    assert str(warning) == expected_message\n", "def test_unformatted_warning_formatting() -> None:\n    \"\"\"Test that UnformattedWarning formats the message correctly.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"Hello {name}!\"\n    )\n    formatted_warning = warning.format(name=\"World\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"Hello World!\"\n", "def test_unformatted_warning() -> None:\n    \"\"\"Test that UnformattedWarning formats the message correctly.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"This is a {adjective} warning\"\n    )\n    formatted_warning = warning.format(adjective=\"test\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"This is a test warning\"\n", "def test_warning_types_messages(warning_class: type, expected_message: str) -> None:\n    \"\"\"Test that warnings have the correct messages.\"\"\"\n    warning = warning_class(\"some_api\")\n    if isinstance(warning, warning_types.UnformattedWarning):\n        warning = warning.format(apiname=\"some_api\", markname=\"some_mark\")\n    assert str(warning) == expected_message\n", "def test_unformatted_warning() -> None:\n    \"\"\"Test that UnformattedWarning formats correctly.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"Some warning: {message}\"\n    )\n    formatted_warning = warning.format(message=\"test message\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"Some warning: test message\"\n"], "sample_648": ["def test_get_empty_parameterset_mark_reason(pytester: Pytester) -> None:\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    mark = get_empty_parameterset_mark(config, [\"a\", \"b\"], lambda: None)\n    assert mark.kwargs[\"reason\"] == \"got empty parameter set ['a', 'b'], function <lambda> at {}:{}\".format(\n        mark.kwargs[\"reason\"].split(\":\")[-2], mark.kwargs[\"reason\"].split(\":\")[-1]\n    )\n", "def test_get_empty_parameterset_mark_reason(pytester: Pytester) -> None:\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    mark = get_empty_parameterset_mark(config, [\"a\", \"b\"], all)\n    assert \"a\" in mark.kwargs[\"reason\"]\n    assert \"b\" in mark.kwargs[\"reason\"]\n    assert \"all\" in mark.kwargs[\"reason\"]\n    assert str(mark.kwargs[\"reason\"]).startswith(\"got empty parameter set\")\n", "def test_get_empty_parameterset_mark_uses_run_false_for_xfail(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        {}=xfail\n        \"\"\".format(\n            EMPTY_PARAMETERSET_OPTION\n        )\n    )\n\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    result_mark = get_empty_parameterset_mark(config, [\"a\"], all)\n    assert result_mark.name == \"xfail\"\n    assert result_mark.kwargs[\"reason\"].startswith(\"got empty parameter set \")\n    assert result_mark.kwargs.get(\"run\") is False\n", "def test_get_empty_parameterset_mark_reason(pytester: Pytester) -> None:\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    mark = get_empty_parameterset_mark(config, [\"a\", \"b\"], lambda: None)\n    assert mark.kwargs[\"reason\"] == \"got empty parameter set ['a', 'b'], function <lambda> at {}:{}\".format(\n        __file__, get_empty_parameterset_mark.__code__.co_firstlineno + 1\n    )\n", "def test_get_empty_parameterset_mark_reason(pytester: Pytester) -> None:\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    mark = get_empty_parameterset_mark(config, [\"a\", \"b\"], lambda x: x)\n    assert \"lambda\" in mark.kwargs[\"reason\"]\n    assert \"at\" in mark.kwargs[\"reason\"]\n    assert \":0\" in mark.kwargs[\"reason\"]  # line number may vary\n"], "sample_649": ["def test_caplog_messages(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info('info message')\n            logging.warning('warning message')\n            assert caplog.messages == ['info message', 'warning message']\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_caplog_clear(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger()\n            logger.warning('warning message')\n            caplog.clear()\n            assert not caplog.records\n            logger.info('info message')\n            assert len(caplog.records) == 1\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_format_with_percent_style_multiline(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"Line 1\\\\nLine 2\")\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-format=%(message)s\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured log call -*\",\n            \"Line 1\",\n            \"Line 2\",\n        ]\n    )\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Test that log auto-indent works correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info('line 1\\\\n  line 2')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\", \"--log-auto-indent\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured log call -*\",\n            \"INFO    *test_log_auto_indent.py* line 1\",\n            \"  line 2\",\n        ]\n    )\n", "def test_log_file_cli_custom_format(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"Custom format warning message\")\n            logging.error(\"Custom format error message\")\n    \"\"\"\n    )\n\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n\n    result = pytester.runpytest(\n        f\"--log-file={log_file}\",\n        \"--log-file-format=%(asctime)s %(levelname)s [%(name)s:%(lineno)d] %(message)s\",\n    )\n\n    assert result.ret == 0\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"Custom format warning message\" in contents\n        assert \"Custom format error message\" in contents\n        assert \"[test_log_file_cli_custom_format:2]\" in contents\n        assert \"[test_log_file_cli_custom_format:3]\" in contents\n"], "sample_650": ["def test_colored_level_formatter_add_color_level(pytester: Pytester) -> None:\n    \"\"\"Test that the add_color_level method of ColoredLevelFormatter works.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            caplog.handler.setFormatter(ColoredLevelFormatter(create_terminal_writer(pytester.config), \"%(levelname)s%(message)s\"))\n            caplog.handler.add_color_level(logging.INFO, \"green\")\n            logger.info('text going to logger from call')\n            assert \"\\x1b[32mINFO\\x1b[0m\" in caplog.text\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--color=yes\")\n    assert result.ret == 0\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Check that log messages are properly auto-indented.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('First line\\\\nSecond line')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=True\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    lines = result.stdout.get_lines_after(\"WARNING\")\n    assert lines[0].startswith(\"First line\")\n    assert lines[1].startswith(\" \" * (len(lines[0]) - len(\"First line\")) + \"Second line\")\n", "def test_log_file_formatting(pytester: Pytester) -> None:\n    \"\"\"Check that log_file_format affects output.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text')\n            assert False\n        \"\"\"\n    )\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_format=%(levelname)s; %(message)s\n        log_file_date_format=%Y-%m-%d %H:%M:%S\n    \"\"\".format(\n            log_file\n        )\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"WARNING; text\" in contents\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Check that log messages are properly auto-indented.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('Line 1\\\\n  Line 2\\\\nLine 3')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-auto-indent\", \"--log-cli-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- live log call --*\",\n            \"*INFO     *test_log_auto_indent:test_log_auto_indent.py:*\",\n            \"  Line 1\",\n            \"    Line 2\",\n            \"  Line 3\",\n        ]\n    )\n", "def test_log_auto_indent_option(pytester: Pytester) -> None:\n    \"\"\"Check that log_auto_indent option affects output.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text\\\\nwith multiple lines')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=true\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*WARNING*test_log_auto_indent_option:test_log_auto_indent_option.py:6 text\",\n            \"    with multiple lines\",\n        ]\n    )\n"], "sample_651": ["def test_warns_match_string(self) -> None:\n    with pytest.warns(UserWarning, match=\"must be 42\"):\n        warnings.warn(\"value must be 42\", UserWarning)\n\n    with pytest.warns():\n        with pytest.raises(pytest.fail.Exception):\n            with pytest.warns(UserWarning, match=\"must be 42\"):\n                warnings.warn(\"this is not here\", UserWarning)\n", "compilation error", "def test_warns_message_repr(self) -> None:\n    with pytest.warns(UserWarning) as record:\n        warnings.warn(\"user warning\", UserWarning)\n    assert repr(record[0].message) == \"UserWarning('user warning')\"\n", "def test_warns_message_with_non_ascii_characters() -> None:\n    with pytest.warns(UserWarning, match=\"\u00e9\"):\n        warnings.warn(\"\u00e9\", UserWarning)\n\n    with pytest.warns(UserWarning, match=r\"\\u00e9\"):\n        warnings.warn(\"\u00e9\", UserWarning)\n\n    with pytest.raises(pytest.fail.Exception):\n        with pytest.warns(UserWarning, match=\"e\"):\n            warnings.warn(\"\u00e9\", UserWarning)\n", "def test_warns_message_str_representation() -> None:\n    \"\"\"Test that warnings messages have a nice string representation.\"\"\"\n    with pytest.warns(UserWarning) as record:\n        warnings.warn(\"user warning\", UserWarning)\n\n    assert str(record[0].message) == \"user warning\"\n    assert repr(record[0].message) == \"UserWarning('user warning')\"\n"], "sample_652": ["def test_fixture_finalizer_with_warnings(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n\n        @pytest.fixture\n            request.addfinalizer(lambda: warnings.warn(\"teardown warning\"))\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    result.stdout.fnmatch_lines([\"*warnings summary*\", \"*teardown warning*\"])\n", "def test_fixture_scope_overrides_parametrized_fixture(testdir):\n    \"\"\"Test that a fixture's scope can override the scope of a parametrized fixture (#2485)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=[1, 2], scope=\"function\")\n            return request.param\n\n        @pytest.fixture(scope=\"module\")\n            return fix\n\n            assert fix_module in [1, 2]\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n", "def test_funcarg_repr():\n        pass\n\n    assert not fixtures.getfuncargnames(func)\n\n        pass\n\n    assert fixtures.getfuncargnames(g) == (\"arg\",)\n\n        pass\n\n    assert fixtures.getfuncargnames(h) == (\"arg1\",)\n\n        pass\n\n    assert fixtures.getfuncargnames(h) == (\"arg1\", \"arg2\")\n\n    class A(object):\n            pass\n\n        @staticmethod\n            pass\n\n    assert fixtures.getfuncargnames(A().f) == (\"arg1\",)\n    assert fixtures.getfuncargnames(A.static, cls=A) == (\"arg1\", \"arg2\")\n", "def test_fixture_scope_function_with_module_parametrization(testdir):\n    \"\"\"Test that a fixture with function scope is executed once per parameter when used in conjunction with a module-scoped parametrization (#4464)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=[1, 2], scope='module')\n            return request.param\n\n        @pytest.fixture\n            return module_scope + 1\n\n            assert function_scope in [2, 3]\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*2 passed*\"])\n", "def test_fixture_function_with_yield_and_return():\n    \"\"\"Check if an error is raised if a fixture function has both yield and return statements (#4545)\"\"\"\n\n    @pytest.fixture\n        yield 1\n        return 2\n\n    with pytest.raises(pytest.fail.Exception):\n        assert fix() == 1\n"], "sample_653": ["def test_get_option_ini_default(testdir):\n    \"\"\"Test that get_option_ini returns the default value when no option or ini value is set.\"\"\"\n    config = testdir.parseconfig()\n    assert get_option_ini(config, \"log_print\") is True\n    assert get_option_ini(config, \"non_existent_option\", \"non_existent_ini\") is None\n", "def test_log_cli_format(testdir):\n    # Default log file level\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.formatter._fmt == '%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s'\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\", \"--log-cli-format=%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_format.py*This log message will be shown\",\n            \"PASSED\",  # 'PASSED' on its own line because the log message prints a new line\n        ]\n    )\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_format_with_invalid_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.log(100, 'log message with invalid level')\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=DEBUG\", \"--log-format=%(levelname)s\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"*Level 100*\", \"*log message with invalid level*\"])\n", "def test_log_cli_custom_format(testdir):\n    \"\"\"Test that log-cli-format is properly applied.\"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_level=INFO\n        log_cli_format=%(asctime)s - %(message)s\n        log_cli_date_format=%H:%M\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info(\"log message from test_custom_log_format\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*::test_custom_log_format \",\n            \"*-- live log call --*\",\n            \"*:* - log message from test_custom_log_format\",\n            \"PASSED *100%*\",\n            \"=* 1 passed in *=\",\n        ]\n    )\n", "def test_get_option_ini_not_found(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        \"\"\"\n    )\n    config = testdir.parseconfigure()\n    assert get_option_ini(config, \"log_format\") is None\n    assert get_option_ini(config, \"log_format\", \"log_date_format\") == DEFAULT_LOG_FORMAT\n"], "sample_654": ["def test_fixture_param_shadowing_with_autouse_fixture(testdir):\n    \"\"\"Parametrized arguments would be shadowed if a fixture with the same name also exists (#5036)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=['a', 'b'], autouse=True)\n            return request.param\n\n        @pytest.fixture\n            return argroot\n\n        # This should only be parametrized directly\n        @pytest.mark.parametrize(\"arg\", [1])\n            assert arg == 1\n\n        # This should be parametrized based on the fixtures\n            assert isinstance(arg, str)\n\n        # Indirect should still work:\n\n        @pytest.fixture\n            return 2*request.param\n\n        @pytest.mark.parametrize(\"arg2\", [1], indirect=True)\n            assert arg2 == 2\n    \"\"\"\n    )\n    # Only one test should have run\n    result = testdir.runpytest(\"-v\")\n    result.assert_outcomes(passed=4)\n    result.stdout.fnmatch_lines([\"*::test_direct[[]1[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]a[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]b[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_indirect[[]1[]]*\"])\n", "def test_fixture_function_warning(testdir):\n    \"\"\"Check if a warning is raised when a fixture function is defined without the @pytest.fixture decorator\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            return 1\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*Fixture 'fix' is not properly defined.*\"])\n", "def test_fixture_param_shadowing_with_default(testdir):\n    \"\"\"Parametrized arguments would be shadowed if a fixture with the same name also exists (#5036)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=['a', 'b'])\n            return request.param\n\n        @pytest.fixture\n            return argroot\n\n        # This should only be parametrized directly\n        @pytest.mark.parametrize(\"arg\", [1], ids=['custom-id'])\n            assert arg == 1\n\n        # This should be parametrized based on the fixtures\n            assert isinstance(arg, str)\n\n        # Indirect should still work:\n\n        @pytest.fixture\n            return 2*request.param\n\n        @pytest.mark.parametrize(\"arg2\", [1], indirect=True)\n            assert arg2 == 2\n    \"\"\"\n    )\n    # Only one test should have run\n    result = testdir.runpytest(\"-v\")\n    result.assert_outcomes(passed=4)\n    result.stdout.fnmatch_lines([\"*::test_direct[custom-id]*\"])\n    result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]a[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]b[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_indirect[[]1[]]*\"])\n", "def test_fixture_with_yield_and_params(testdir):\n    \"\"\"Test that a fixture with yield and params is executed correctly.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=[1, 2])\n            yield request.param\n            assert request.param in [1, 2]\n\n            assert my_fix in [1, 2]\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=2)\n", "def test_fixture_function_has_docstring(testdir):\n    \"\"\"Check that a fixture function has a docstring (#5070)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            '''Fixture docstring'''\n            return 1\n\n            assert fix == 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--fixtures\")\n    result.stdout.fnmatch_lines([\"*fix*\", \"*Fixture docstring*\"])\n"], "sample_655": ["def test_capturing_and_flushing(testdir):\n    \"\"\"Test that flushing of captured output works correctly (#5021).\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            print(\"hello\", end=\"\")\n            sys.stdout.flush()\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\"\n            print(\" world\")\n            captured = capsys.readouterr()\n            assert captured.out == \" world\\\\n\"\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n", "def test_capturing_check_bytes_only(tmpfile):\n    fd = tmpfile.fileno()\n    cap = capture.FDCaptureBinary(fd)\n    data = b\"hello\"\n    os.write(fd, data)\n    s = cap.snap()\n    cap.done()\n    assert not s\n    cap = capture.FDCaptureBinary(fd)\n    cap.start()\n    os.write(fd, data)\n    s = cap.snap()\n    cap.done()\n    assert s == b\"hello\"\n", "def test_capturing_with_redirecting_output_to_same_file_descriptor(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import os\n            with open(\"output.txt\", \"w\") as f:\n                os.dup2(f.fileno(), 1)\n                print(\"captured\")\n            with open(\"output.txt\", \"r\") as f:\n                assert f.read() == \"captured\\\\n\"\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n", "def test_captured_outerr_unicode(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            print(\"hx\u0105\u0107\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines(\n        [\n            \"*hx\u0105\u0107*\",\n        ]\n    )\n", "def test_capturing_in_nested_contexts(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import contextlib\n        import sys\n\n        @contextlib.contextmanager\n            with capsys.disabled():\n                yield\n\n            print(\"before\")\n            with capture_context():\n                print(\"in context\")\n            print(\"after\")\n\n            captured = capsys.readouterr()\n            assert captured.out == \"before\\\\nafter\\\\n\"\n        \"\"\"\n    )\n    result = testdir.runpytest(str(p))\n    assert result.ret == 0\n"], "sample_656": ["def test_capturemanager_is_globally_capturing(capsys):\n    capman = CaptureManager(\"sys\")\n    assert not capman.is_globally_capturing()\n    capman.start_global_capturing()\n    assert capman.is_globally_capturing()\n    capman.stop_global_capturing()\n    assert not capman.is_globally_capturing()\n", "def test_close_twice_encoded_file():\n    ef = capture.EncodedFile(None, None)\n    ef.close()\n    ef.close()  # should not raise an exception\n", "def test_capturing_binary_data_with_text_fixtures(testdir):\n    \"\"\"Check that capturing binary data with text fixtures doesn't crash (#6246)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            import sys\n            sys.stdout.buffer.write(b'\\\\x80abc')\n            captured = capsys.readouterr()\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.assert_outcomes(passed=1)\n", "def test_capturing_plugin_resets_capturing_after_internal_error(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            raise Exception(\"boom\")\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*Exception: boom*\", \"*1 error*\"])\n    assert \"Captured\" not in result.stdout.str()\n", "def test_capture_with_suspend_resume(testdir):\n    # Issue 4144\n    # capture should work with suspend and resume\n\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n\n            capsys.suspend()\n            print(\"suspend\")\n            sys.stderr.write(\"suspend err\\\\n\")\n\n            capsys.resume()\n            captured = capsys.readouterr()\n            assert captured.out == \"\"\n            assert captured.err == \"\"\n\n            print(\"resume\")\n            sys.stderr.write(\"resume err\\\\n\")\n            captured = capsys.readouterr()\n            assert captured.out == \"resume\\\\n\"\n            assert captured.err == \"resume err\\\\n\"\n    \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n"], "sample_657": ["def test_mark_decorator_with_args(testdir):\n    \"\"\"Test that mark decorators with arguments are correctly applied\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo(\"bar\", baz=\"qux\")\n            pass\n    \"\"\"\n    )\n    items, rec = testdir.inline_genitems(p)\n    item = items[0]\n    marks = [mark for mark in item.iter_markers(name=\"foo\")]\n    assert len(marks) == 1\n    mark = marks[0]\n    assert mark.args == (\"bar\",)\n    assert mark.kwargs == {\"baz\": \"qux\"}\n", "def test_node_keywords_contains_markers_from_ancestors(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class BaseTests(object):\n            pytestmark = pytest.mark.base\n\n                pass\n\n        @pytest.mark.FOO\n        class TestFooClass(BaseTests):\n            pytestmark = pytest.mark.foo\n\n                pass\n\n        @pytest.mark.BAR\n        class TestBarClass(TestFooClass):\n            pytestmark = pytest.mark.bar\n\n                pass\n    \"\"\"\n    )\n\n    reprec = testdir.inline_run()\n    passed, skipped, failed = reprec.countoutcomes()\n    dlist = reprec.getcalls(\"pytest_deselected\")\n    assert passed == 3\n    assert skipped == failed == 0\n    item = reprec.items[2]\n    assert \"base\" in item.keywords\n    assert \"foo\" in item.keywords\n    assert \"bar\" in item.keywords\n    assert \"FOO\" in item.keywords\n    assert \"BAR\" in item.keywords\n", "def test_mark_decorator_with_args(testdir):\n    \"\"\"Test that the MarkDecorator can be used with arguments\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo('bar', baz='qux')\n            pass\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 1\n    assert skipped == failed == 0\n", "def test_mark_decorator_with_single_callable_argument(testdir):\n    \"\"\"Test that a MarkDecorator with a single callable argument does not treat it as a test function.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n        mark = pytest.mark.foo(some_function)\n        assert mark.mark.name == 'foo'\n        assert len(mark.mark.args) == 1\n        assert mark.mark.args[0] is some_function\n    \"\"\"\n    )\n    reprec = testdir.inline_run(p)\n    reprec.assertoutcome(passed=1)\n", "def test_pytest_param_mark_decorator_as_marks():\n    mark = pytest.mark.xfail()\n    param = pytest.param(1, marks=mark)\n    assert param.marks == (mark,)\n"], "sample_658": ["def test_doctest_unexpected_exception_dont_crash_when_no_traceback(self, testdir):\n    testdir.maketxtfile(\n        \"\"\"\n        >>> raise SystemExit(0)\n        >>> \n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*UNEXPECTED EXCEPTION: SystemExit*\", \"*1 failed*\"])\n", "def test_doctest_continue_on_failure_with_usepdb(self, testdir):\n    testdir.maketxtfile(\n        \"\"\"\n        >>> i = 5\n        >>> 0 / i\n        2\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\", \"--pdb\")\n    result.stdout.fnmatch_lines([\"*ZeroDivisionError*\", \"*>   0 / i\"])\n", "def test_doctest_item_node_id(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n            '''\n            >>> 1 + 1\n            2\n            '''\n    \"\"\"\n    )\n    items, _ = testdir.inline_genitems(\"--doctest-modules\")\n    assert len(items) == 1\n    item = items[0]\n    assert item.nodeid.endswith(\"foo\")\n", "def test_doctest_namespace_fixture_is_module_scope(self, testdir):\n    \"\"\"Test that the doctest_namespace fixture is module scope.\"\"\"\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True)\n            doctest_namespace['foo'] = 'bar'\n    \"\"\"\n    )\n    p1 = testdir.makepyfile(\n        \"\"\"\n            '''\n            >>> foo\n            'bar'\n            '''\n    \"\"\"\n    )\n    p2 = testdir.makepyfile(\n        \"\"\"\n            '''\n            >>> foo\n            'bar'\n            '''\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"* 2 passed *\"])\n", "def test_doctest_continue_on_failure_with_usepdb(self, testdir):\n    p = testdir.maketxtfile(\n        test_something=\"\"\"\n        >>> i = 5\n        >>> def foo():\n        ...     raise ValueError('error1')\n        >>> foo()\n        >>> i\n        >>> i + 2\n        7\n        >>> i + 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\", \"--pdb\")\n    result.assert_outcomes(passed=0, failed=1)\n    # The lines that contains the failure are 4, 5, and 8.  The first one\n    # is a stack trace and the other two are mismatches.\n    result.stdout.fnmatch_lines(\n        [\"*4: UnexpectedException*\", \"*5: DocTestFailure*\", \"*8: DocTestFailure*\"]\n    )\n"], "sample_659": ["def test_traceback_entry_getsource(self):\n    # Create a simple function that raises an exception\n        raise Exception(\"Test exception\")\n\n    try:\n        foo()\n    except Exception as e:\n        # Get the traceback entry for the exception\n        exc_info = sys.exc_info()\n        tb = exc_info[2]\n        entry = TracebackEntry(tb)\n\n        # Test getsource method\n        source = entry.getsource()\n        assert source is not None\n        assert \"raise Exception\" in str(source)\n", "def test_traceback_recursion(self):\n    \"\"\"Test that a recursion error is properly handled in the traceback.\"\"\"\n        recursive_function()\n\n    with pytest.raises(RecursionError):\n        recursive_function()\n", "def test_repr_traceback_entry_source(self):\n    \"\"\"Test that ReprEntry.source is set correctly.\"\"\"\n    excinfo = pytest.raises(ZeroDivisionError, lambda: 1 / 0)\n    entry = excinfo.traceback[-1]\n    reprentry = ReprEntry([], None, None, None, \"long\")\n    reprentry.reprfileloc = ReprFileLocation(entry.path, entry.lineno + 1, \"\")\n    assert reprentry._getentrysource(entry) is not None\n", "def test_traceback_entry_set_repr_style(self):\n    entry = TracebackEntry(None)\n    assert entry._repr_style is None\n    entry.set_repr_style(\"short\")\n    assert entry._repr_style == \"short\"\n    with pytest.raises(AssertionError):\n        entry.set_repr_style(\"invalid_style\")\n", "def test_repr_traceback_with_recursive_frames(self):\n    class RecursiveError(Exception):\n        pass\n\n        if n == 0:\n            raise RecursiveError(\"Maximum recursion depth exceeded\")\n        recursive_function(n - 1)\n\n    try:\n        recursive_function(10)\n    except RecursiveError as e:\n        excinfo = pytest.ExceptionInfo.from_current()\n        formatted_excinfo = FormattedExcinfo()\n        repr_traceback = formatted_excinfo.repr_traceback(excinfo)\n        assert \"Recursion detected\" in str(repr_traceback)\n"], "sample_660": ["def test_log_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    system_out_nodes = node.find_by_tag(\"system-out\")\n    assert len(system_out_nodes) == 1\n    assert \"This is stdout\" in system_out_nodes[0].text\n    system_err_nodes = node.find_by_tag(\"system-err\")\n    assert len(system_err_nodes) == 1\n    assert \"This is stderr\" in system_err_nodes[0].text\n    assert \"hello\" in system_out_nodes[0].text\n", "def test_add_stats(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(tests=1)\n    log = LogXML(str(testdir.tmpdir.join(\"test_add_stats.xml\")), None)\n    log.add_stats(\"passed\")\n    assert log.stats[\"passed\"] == 1\n    log.add_stats(\"skipped\")\n    assert log.stats[\"skipped\"] == 1\n    log.add_stats(\"failure\")\n    assert log.stats[\"failure\"] == 1\n    log.add_stats(\"error\")\n    assert log.stats[\"error\"] == 1\n", "def test_node_reporter_finalized(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node_reporter = LogXML(str(testdir.tmpdir.join(\"test.xml\")), None).node_reporter(\n        \"test_func\"\n    )\n    node_reporter.finalize()\n    with pytest.raises(AttributeError):\n        node_reporter.to_xml()\n", "def test_xfail_strict_with_custom_message(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(strict=True, reason=\"Custom message\")\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    fnode = tnode.find_first_by_tag(\"failure\")\n    fnode.assert_attr(message=\"[XPASS(strict)] Custom message\")\n", "def test_escaped_classname_issue3533(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        class TestClass:\n                pass\n    \"\"\"\n    )\n    _, dom = runandparse(testdir, \"--junitprefix=<>\")\n    node = dom.find_first_by_tag(\"testcase\")\n    assert \"<>test_escaped_classname_issue3533.TestClass\" in node[\"classname\"]\n"], "sample_661": ["def test_node_reporter_record_testreport_location_is_none(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    assert \"file\" in node.attributes.keys()\n    assert \"line\" not in node.attributes.keys()\n", "def test_escaped_systemout_issue3533(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n            print('1 <> 2')\n            assert False\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    snode = node.find_first_by_tag(\"system-out\")\n    assert \"1 <> 2\" in snode.text\n", "def test_mangle_test_address_with_brackets():\n    from _pytest.junitxml import mangle_test_address\n\n    address = \"::\".join([\"a/my.py.thing.py\", \"Class\", \"()\", \"method\", \"[a-1-::]\"])\n    newnames = mangle_test_address(address)\n    assert newnames == [\"a.my.py.thing\", \"Class\", \"method\", \"[a-1-::]\"]\n\n    address = \"::\".join([\"a/my.py.thing.py\", \"Class\", \"()\", \"method\", \"[a-1-::]\", \"more\"])\n    newnames = mangle_test_address(address)\n    assert newnames == [\"a.my.py.thing\", \"Class\", \"method\", \"[a-1-::]\", \"more\"]\n", "def test_junit_log_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    assert len(node.find_by_tag(\"system-err\")) == 1\n    assert len(node.find_by_tag(\"system-out\")) == 1\n", "def test_escaped_skipreason_with_newlines_issue3533(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason='1 <> 2\\\\nnext line')\n            pass\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    snode = node.find_first_by_tag(\"skipped\")\n    assert \"1 <> 2\" in snode.text\n    assert \"next line\" in snode.text\n    snode.assert_attr(message=\"1 <> 2\\nnext line\")\n"], "sample_662": ["def test_report_head_line(self, testdir):\n    \"\"\"Test that the head_line property returns the correct domain.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    report = reports[1]\n    assert report.head_line == \"test_a\"\n", "def test_count_towards_summary(self, testdir):\n    \"\"\"Test that the count_towards_summary property works correctly.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 6\n    for rep in reports:\n        if rep.when == \"setup\" or rep.when == \"teardown\":\n            assert not rep.count_towards_summary\n        else:\n            assert rep.count_towards_summary\n", "def test_report_head_line(self, testdir):\n    \"\"\"Test that the head_line property returns the correct domain.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    report = reports[1]\n    assert report.head_line == \"test_a\"\n", "def test_report_get_sections(self, testdir):\n    \"\"\"Test that the get_sections method returns the correct sections.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    report = reports[1]\n    report.sections = [(\"prefix1\", \"content1\"), (\"prefix2\", \"content2\")]\n    sections = list(report.get_sections(\"prefix1\"))\n    assert len(sections) == 1\n    assert sections[0] == (\"prefix1\", \"content1\")\n", "def test_report_head_line(self, testdir):\n    \"\"\"Test that the head_line property returns the expected value.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    report = reports[1]\n    assert report.head_line == \"test_a\"\n"], "sample_663": ["def test_collectignoreglob_via_conftest(testdir):\n    \"\"\"collect_ignore_glob in parent conftest skips importing child (issue #4592).\"\"\"\n    tests = testdir.mkpydir(\"tests\")\n    tests.ensure(\"conftest.py\").write(\"collect_ignore_glob = ['ignore_me*']\")\n\n    ignore_me = tests.mkdir(\"ignore_me\")\n    ignore_me.ensure(\"__init__.py\")\n    ignore_me.ensure(\"conftest.py\").write(\"assert 0, 'should_not_be_called'\")\n\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n", "def test_collect_file_with_bom(testdir):\n    \"\"\"Test that collecting a file with a BOM (Byte Order Mark) works correctly (#4757).\"\"\"\n    p = testdir.makepyfile(\n        \"\\xef\\xbb\\xbf\" + \"def test_func(): pass\"\n    )  # UTF-8 BOM\n    result = testdir.runpytest(str(p))\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n    assert result.ret == 0\n", "def test_session_collects_from_conftest_files(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.join(\"conftest.py\").write(\n        textwrap.dedent(\n            \"\"\"\n                if path.basename == \"special.txt\":\n                    return MyFile(path, parent)\n            class MyFile(pytest.File):\n                    return [Item(\"special\", self)]\n            class Item(pytest.Item):\n                    pass\n            \"\"\"\n        )\n    )\n    sub.join(\"special.txt\").ensure(file=True)\n    result = testdir.runpytest(\"-v\", \"--collect-only\", str(sub))\n    result.stdout.fnmatch_lines([\"*MyFile*special.txt*\", \"*Item*special*\"])\n", "def test_collect_session_with_recursive_symlinks(testdir):\n    \"\"\"Test collection with recursive symlinks (#4426).\"\"\"\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Create a recursive symlink.\n    sub.join(\"recursive\").mksymlinkto(sub)\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub/test_file.py::test_file PASSED*\",\n            \"*1 passed in*\",\n        ]\n    )\n", "def test_matchnodes_no_conftest(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n    \"\"\"\n    )\n    items, hookrec = testdir.inline_genitems()\n    assert len(items) == 1\n    item, = items\n    newitems = Session(testdir.config).matchnodes([item], [\"test_func\"])\n    assert len(newitems) == 1\n    newitem, = newitems\n    assert newitem.name == \"test_func\"\n"], "sample_664": ["def test_fixture_positional_arguments_warning(pytestconfig):\n    \"\"\"Check that passing arguments to pytest.fixture() as positional arguments raises a warning\"\"\"\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        pytest.fixture(autouse=True)\n", "def test_fixture_positional_arguments_warn(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(42)\n            return 42\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_warning():\n    \"\"\"Check that passing arguments to pytest.fixture() as positional arguments raises a warning.\"\"\"\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        pytest.fixture(autouse=True)\n", "def test_funcargnames_attribute_deprecation():\n    \"\"\"Test that the `funcargnames` attribute raises a deprecation warning.\"\"\"\n    with pytest.warns(deprecated.FUNCARGNAMES):\n        class FakeRequest:\n            funcargnames = None\n\n        request = FakeRequest()\n        getattr(request, 'funcargnames')\n", "def test_fixture_positional_arguments_warning():\n    \"\"\"Check that a warning is emitted when passing positional arguments to pytest.fixture()\"\"\"\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        pytest.fixture(autouse=True)\n"], "sample_665": ["def test_collector_respects_pytestignore(testdir):\n    p1 = testdir.makepyfile(\"assert 0\")\n    testdir.makefile(\".pytestignore\", \"test_collector_respects_pytestignore.py\")\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n", "def test_collect_with_import_mode_append(testdir):\n    testdir.makepyfile(\"def test_foo(): pass\")\n    testdir.makeconftest(\n        \"\"\"\n        import sys\n            sys.path.insert(0, 'non_existent_dir')\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--import-mode=append\")\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n    assert result.ret == 0\n", "def test_parametrize_id_func_is_called_with_value(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            return \"id_\" + str(val)\n\n        @pytest.mark.parametrize(\"arg\", [1, 2], ids=id_func)\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--collect-only\")\n    result.stdout.fnmatch_lines([\"*test_foo[id_1]*\", \"*test_foo[id_2]*\"])\n", "def test_nodeid_with_parametrize_and_strange_characters(testdir):\n    \"\"\"Check that using parametrize with test nodeids that contain strange characters works (#6937).\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"p\", [\"[]\", \"()\", \"{}\"])\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*3 passed in*\"])\n", "def test_package_visible_to_module_file(testdir):\n    \"\"\"Test that a package is visible to a module file within it, even if the\n    package is not visible via sys.path.\"\"\"\n    pkg = testdir.mkpydir(\"foopkg\")\n    pkg.join(\"__init__.py\").write(\"\")\n    testdir.makepyfile(\n        \"import foopkg\\n\"\n        \"def test():\\n\"\n        \"    assert hasattr(foopkg, '__package__')\\n\"\n        \"    assert foopkg.__package__ == 'foopkg'\\n\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.OK\n"], "sample_666": ["def test_percent_style_multiline_auto_indent():\n    style = PercentStyleMultiline(\"%(message)s\", auto_indent=True)\n    record_dict = {\"message\": \"Hello\\nWorld\"}\n    formatted = style.format(record_dict)\n    assert formatted == \"Hello\\n    World\"\n", "def test_capture_and_log_file(testdir):\n    \"\"\"Test that log messages are written to the log file even when capture is enabled.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\\\n        import logging\n\n            logging.warning(\"warning message\")\n            logging.info(\"info message\")\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--log-file=log.txt\", \"--log-file-level=DEBUG\")\n    assert result.ret == ExitCode.TESTS_FAILED\n    with open(\"log.txt\") as f:\n        log_contents = f.read()\n    assert \"warning message\" in log_contents\n    assert \"info message\" in log_contents\n", "def test_percent_style_multiline_auto_indent(testdir):\n    \"\"\"Test the auto-indent feature of PercentStyleMultiline.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import logging\n\n            logging.basicConfig(format='%(message)s')\n            logging.info('Hello\\\\n  World')\n            logging.info('Hello\\\\n World')\n            logging.info('Hello\\\\n   World')\n            logging.info('Hello\\\\nWorld')\n        \"\"\"\n    )\n    result = testdir.runpytest(p, '--log-auto-indent', 'on')\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO* Hello\",\n            \"  World*\",\n            \"*INFO* Hello\",\n            \"    World*\",\n            \"*INFO* Hello\",\n            \"      World*\",\n            \"*INFO* Hello\",\n            \"World*\",\n        ]\n    )\n\n    result = testdir.runpytest(p, '--log-auto-indent', 'off')\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO* Hello\",\n            \"  World*\",\n            \"*INFO* Hello\",\n            \" World*\",\n            \"*INFO* Hello\",\n            \"   World*\",\n            \"*INFO* Hello\",\n            \"World*\",\n        ]\n    )\n\n    result = testdir.runpytest(p, '--log-auto-indent', '10')\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO* Hello\",\n            \"         World*\",\n            \"*INFO* Hello\",\n            \"         World*\",\n            \"*INFO* Hello\",\n            \"         World*\",\n            \"*INFO* Hello\",\n            \"         World*\",\n        ]\n    )\n", "def test_colored_level_formatter():\n    formatter = ColoredLevelFormatter(create_terminal_writer(None), \"%(levelname)s\", \"%H:%M:%S\")\n    record = logging.LogRecord(\"name\", logging.INFO, \"path\", 1, \"message\", None, None)\n    formatted = formatter.format(record)\n    assert \"\\x1b[32mINFO\\x1b[0m\" in formatted\n", "def test_percent_style_multiline():\n    style = PercentStyleMultiline(\"%(message)s\", auto_indent=True)\n    record = logging.LogRecord(\"name\", logging.INFO, \"path\", 1, \"Hello\\nWorld\", None, None)\n    formatted = style.format(record)\n    assert formatted == \"Hello\\n    World\"\n"], "sample_667": ["def test_getbasetemp_with_custom_basetemp(tmp_path):\n    \"\"\"Test that getbasetemp returns the custom basetemp when provided\"\"\"\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(tmp_path)\n    t = TempPathFactory.from_config(config)\n    assert t.getbasetemp() == tmp_path\n", "def test_getbasetemp_custom_removes_old_basetemp_dir_with_read_only_file(testdir):\n    mytemp = testdir.tmpdir.join(\"xyz\")\n    p = testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    adir = mytemp.mkdir(\"adir\")\n    afile = adir / \"afile\"\n    afile.write_bytes(b\"aa\")\n    mode = os.stat(str(afile)).st_mode\n    os.chmod(str(afile), mode & ~stat.S_IWRITE)\n\n    testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert not mytemp.check()\n", "def test_tmp_path_factory_basetemp_with_trailing_slash(tmp_path_factory):\n    \"\"\"Test that the tmp_path_factory works correctly when the basetemp has a trailing slash.\"\"\"\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(str(tmp_path_factory.getbasetemp()) + \"/\")\n    t = TempPathFactory.from_config(config)\n    assert t.getbasetemp().resolve() == tmp_path_factory.getbasetemp().resolve()\n", "def test_getbasetemp_with_custom_basetemp(tmp_path):\n    \"\"\"Test that getbasetemp returns the custom basetemp when provided\"\"\"\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(tmp_path / \"custom_basetemp\")\n    t = TempPathFactory.from_config(config)\n    assert t.getbasetemp() == tmp_path / \"custom_basetemp\"\n", "def test_tmpdir_factory_cleanup_lock(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope='session')\n            return tmp_path_factory.mktemp('data', numbered=False)\n\n            assert session_dir.isdir()\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    assert result.ret == 0\n    # run again to check that the cleanup lock is properly released\n    result = testdir.runpytest(p)\n    assert result.ret == 0\n"], "sample_668": ["def test_fixture_positional_arguments_warning():\n    with pytest.warns(pytest.PytestDeprecationWarning):\n        @pytest.fixture(autouse=True)\n            pass\n", "def test_fixture_positional_arguments_warning():\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture\\\\(\\\\) as positional arguments is deprecated - pass them as a keyword argument instead.\",\n    ):\n        pytest.fixture(autouse=True)\n", "def test_fixture_positional_arguments_warning():\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.\",\n    ):\n        pytest.fixture(autouse=True)\n", "def test_fixture_positional_arguments_warning():\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.\",\n    ):\n        pytest.fixture(\"function\", lambda: None)\n", "def test_funcargnames_attribute_deprecation():\n    \"\"\"Test that the 'funcargnames' attribute deprecation warning is raised.\"\"\"\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"The `funcargnames` attribute was an alias for `fixturenames`\"):\n        deprecated.FUNCARGNAMES\n"], "sample_669": ["def test_capture_manager_is_globally_capturing():\n    capman = CaptureManager(\"fd\")\n    assert not capman.is_globally_capturing()\n    capman.start_global_capturing()\n    assert capman.is_globally_capturing()\n    capman.stop_global_capturing()\n    assert not capman.is_globally_capturing()\n", "def test_fdcapture_binary_mode(tmpfile):\n    fd = tmpfile.fileno()\n    cap = capture.FDCaptureBinary(fd)\n    data = b\"hello\"\n    os.write(fd, data)\n    s = cap.snap()\n    cap.done()\n    assert not s\n    cap = capture.FDCaptureBinary(fd)\n    cap.start()\n    os.write(fd, data)\n    s = cap.snap()\n    cap.done()\n    assert s == data\n", "def test_captured_outerr_aliasing(capsys):\n    \"\"\"Verify captured out and err are not aliased (#5938)\"\"\"\n    print(\"out\")\n    sys.stderr.write(\"err\")\n    captured = capsys.readouterr()\n    captured.out += \"mutated\"\n    assert captured.err == \"err\"\n", "def test_capture_manager_is_globally_capturing():\n    capman = CaptureManager(\"fd\")\n    assert not capman.is_globally_capturing()\n    capman.start_global_capturing()\n    assert capman.is_globally_capturing()\n    capman.stop_global_capturing()\n    assert not capman.is_globally_capturing()\n", "def test_encodedfile_write_bytes_error(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    with pytest.raises(TypeError):\n        ef.write(b\"hello\")\n    assert ef.write(\"world\") == 5\n    ef.flush()\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"world\"\n    tmpfile.close()\n    with pytest.raises(ValueError):\n        ef.read()\n"], "sample_670": ["def test_matcher(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_matcher_logic(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_nested_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_evaluate_with_matcher(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_unbalanced_parens(expr: str) -> None:\n    with pytest.raises(ParseError):\n        evaluate(expr, lambda ident: True)\n"], "sample_671": ["def test_mark_xfail_reason_from_func(testdir):\n    \"\"\"Ensure pytest.mark.xfail reason can be obtained from a function\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            return \"Expected failure\"\n\n        @pytest.mark.xfail(True, reason=reason_func())\n            assert False\n    \"\"\"\n    )\n    result = testdir.inline_run()\n    passed, skipped, failed = result.listoutcomes()\n    assert not failed\n    xfailed = [r for r in skipped if hasattr(r, \"wasxfail\")]\n    assert xfailed[0].wasxfail == \"Expected failure\"\n", "def test_xfail_strict_from_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_default_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_with_unexpected_success(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(strict=True)\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_ini_config(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n"], "sample_672": ["def test_saferepr_recursive():\n    class Recursive:\n            return saferepr(self)\n\n    assert saferepr(Recursive()).startswith(\n        \"<[RecursionError('maximum recursion depth exceeded' raised in repr()] Recursive object at 0x\"\n    )\n", "def test_saferepr_recursive():\n    \"\"\"Test saferepr() with recursive objects.\"\"\"\n\n    class Recursive:\n            self.name = name\n            self.children = []\n\n            return f\"Recursive({self.name})\"\n\n    obj1 = Recursive(\"obj1\")\n    obj2 = Recursive(\"obj2\")\n    obj1.children.append(obj2)\n    obj2.children.append(obj1)\n\n    # The repr of obj1 will cause a recursion error if not handled properly.\n    assert saferepr(obj1).startswith(\"Recursive(obj1)\")\n", "def test_saferepr_with_recursive_structure():\n    \"\"\"Test saferepr() with recursive structures.\"\"\"\n    r = [1, 2]\n    r.append(r)\n    assert saferepr(r).startswith(\"[1, 2, [...\")\n", "def test_saferepr_recursive():\n    class Recursive:\n            return saferepr(self)\n\n    assert saferepr(Recursive()).startswith(\n        \"<[RecursionError('maximum recursion depth exceeded') raised in repr()]\"\n    )\n", "def test_saferepr_recursive():\n    class RecursiveRepr:\n            return saferepr(self)\n\n    assert \"RecursionError\" in saferepr(RecursiveRepr())\n"], "sample_673": ["def test_is_setup_py_empty(tmpdir):\n    setup_py = tmpdir.join(\"setup.py\")\n    setup_py.write('')\n    assert not _is_setup_py(setup_py)\n", "def test_doctest_continue_on_failure_with_usepdb(testdir):\n    testdir.maketxtfile(\n        \"\"\"\n        >>> 1 / 0\n        >>> 1 + 1\n        2\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\", \"--pdb\")\n    result.stdout.fnmatch_lines([\"*ZeroDivisionError*\", \"*Entering PDB*\", \"*2\"])\n    assert result.ret == 1\n", "def test_is_setup_py_empty_file(tmpdir):\n    setup_py = tmpdir.join(\"setup.py\")\n    setup_py.write(\"\")\n    assert not _is_setup_py(setup_py)\n", "def test_get_optionflags(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        doctest_optionflags = ELLIPSIS NORMALIZE_WHITESPACE\n    \"\"\"\n    )\n    assert get_optionflags(testdir) == (\n        doctest.ELLIPSIS | doctest.NORMALIZE_WHITESPACE\n    )\n", "def test_doctest_namespace_fixture_autouse(request, tmpdir):\n    \"\"\"\n    Test that the doctest_namespace fixture can be used with autouse.\n    \"\"\"\n    tmpdir.join(\"conftest.py\").write(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True)\n            doctest_namespace['myvar'] = 42\n    \"\"\"\n    )\n    tmpdir.join(\"test_docstring.txt\").write(\n        \"\"\"\n        >>> myvar\n        42\n    \"\"\"\n    )\n    result = pytest.runpytest(tmpdir)\n    result.stdout.fnmatch_lines([\"* 1 passed *\"])\n"], "sample_674": ["def test_node_repr_failure(testdir):\n    \"\"\"Test that Node._repr_failure_py returns the expected representation.\"\"\"\n    item = testdir.getitems(\n        \"\"\"\n            assert False\n    \"\"\"\n    )[0]\n    excinfo = pytest.raises(AssertionError, item.runtest)\n    repr_failure = item._repr_failure_py(excinfo)\n    assert isinstance(repr_failure, str)\n    assert \"AssertionError\" in repr_failure\n    assert \"assert False\" in repr_failure\n", "def test_get_fslocation_from_item():\n    class FakeItem:\n            self.location = location\n            self.obj = obj\n            self.fspath = fspath\n\n    item = FakeItem(location=(\"path/to/file.py\", 10))\n    assert nodes.get_fslocation_from_item(item) == (\"path/to/file.py\", 10)\n\n    item = FakeItem(obj=\"some_object\")\n    with pytest.raises(TypeError, match=\".*has no attribute.*\"):\n        nodes.get_fslocation_from_item(item)\n\n    item = FakeItem(fspath=\"path/to/file.py\")\n    assert nodes.get_fslocation_from_item(item) == (\"path/to/file.py\", -1)\n", "def test_node_keywords():\n    node = nodes.Node.from_parent(None, name=\"test_node\")\n    assert isinstance(node.keywords, nodes.NodeKeywords)\n    assert not node.keywords\n\n    node.keywords[\"mark1\"] = \"marker1\"\n    assert node.keywords == {\"mark1\": \"marker1\"}\n", "def test_get_fslocation_from_item():\n    \"\"\"Test that get_fslocation_from_item returns the correct location for an item.\"\"\"\n    class FakeItem:\n        fspath = py.path.local(\"path/to/file.py\")\n        obj = None\n        location = (\"path/to/file.py\", 10, \"\")\n\n    assert nodes.get_fslocation_from_item(FakeItem()) == (\"path/to/file.py\", 10)\n\n    class FakeItem:\n        fspath = py.path.local(\"path/to/file.py\")\n        obj = \"object\"\n        location = None\n\n    assert nodes.get_fslocation_from_item(FakeItem()) == (\"unknown location\", -1)\n", "def test_get_fslocation_from_item():\n    \"\"\"Test that get_fslocation_from_item returns the expected location.\"\"\"\n    class FakeItem:\n            self.fspath = fspath\n\n    item = FakeItem(py.path.local(\"/path/to/file.py\"))\n    assert nodes.get_fslocation_from_item(item) == (\"/path/to/file.py\", None)\n\n    class FakeItemWithObj:\n            self.obj = obj\n\n    class FakeObj:\n            self.__code__ = FakeCode(fspath)\n\n    class FakeCode:\n            self.co_filename = co_filename\n            self.co_firstlineno = 1\n\n    item = FakeItemWithObj(FakeObj(\"/path/to/file.py\"))\n    assert nodes.get_fslocation_from_item(item) == (\"/path/to/file.py\", 1)\n"], "sample_675": ["def test_log_cli_auto_indent(testdir):\n    \"\"\"Test that log messages are properly indented when --log-auto-indent is used.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info('This is a multiline log message\\\\nwith multiple lines')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=INFO\", \"--log-auto-indent\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- live log call --*\",\n            \"*INFO* This is a multiline log message\",\n            \"          with multiple lines\",\n        ]\n    )\n", "def test_log_auto_indent_default(testdir):\n    \"\"\"Test that log auto-indent is off by default.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info(\"Multiline message\\\\nLine 2\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-format=%(message)s\")\n    result.stdout.fnmatch_lines([\"*Multiline message*\", \"*Line 2*\"])\n    assert result.stdout.str().count(\"  Line 2\") == 0\n", "def test_logging_plugin_set_log_path(testdir: Testdir) -> None:\n    \"\"\"\n    Test that the set_log_path method of the LoggingPlugin sets the log file path correctly.\n    \"\"\"\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level=DEBUG\n        \"\"\".format(\n            log_file\n        )\n    )\n\n    plugin = testdir.config.pluginmanager.getplugin(\"logging-plugin\")\n    new_log_file = testdir.tmpdir.join(\"new_pytest.log\").strpath\n    plugin.set_log_path(new_log_file)\n\n    assert plugin.log_file_handler.stream.name == new_log_file\n    assert os.path.exists(new_log_file)\n", "def test_log_cli_format(testdir):\n    # Default log file level\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.formatter._style._fmt == \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"\n            logging.getLogger('catchlog').debug(\"This log message won't be shown\")\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format = %(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\", \"--log-cli-level=INFO\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_format.py*This log message will be shown\",\n            \"PASSED\",  # 'PASSED' on its own line because the log message prints a new line\n        ]\n    )\n    result.stdout.no_fnmatch_line(\"*This log message won't be shown*\")\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_file_set_path_and_log_level(testdir):\n    report_dir_base = testdir.tmpdir.strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            import os\n            import pytest\n            @pytest.hookimpl(hookwrapper=True, tryfirst=True)\n                config = item.config\n                logging_plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n                report_file = os.path.join({}, item._request.node.name)\n                logging_plugin.set_log_path(report_file)\n                logging_plugin.log_file_handler.setLevel(logging.DEBUG)\n                yield\n        \"\"\".format(\n            repr(report_dir_base)\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                logger.debug(\"debug message from test 1\")\n                assert True\n\n                logger.debug(\"message from test 2\")\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(os.path.join(report_dir_base, \"test_first\")) as rfh:\n        content = rfh.read()\n        assert \"message from test 1\" in content\n        assert \"debug message from test 1\" in content\n\n    with open(os.path.join(report_dir_base, \"test_second\")) as rfh:\n        content = rfh.read()\n        assert \"message from test 2\" in content\n"], "sample_676": ["def test_terminal_summary_warnings_counting(testdir):\n    \"\"\"Test that warnings emitted during pytest_terminal_summary are counted.\n    (#1305).\n    \"\"\"\n    testdir.makeconftest(\n        \"\"\"\n        import warnings\n            warnings.warn(UserWarning('internal warning'))\n            warnings.warn(UserWarning('another internal warning'))\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import warnings\n            warnings.warn(\"warning_from_\" + \"test\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-ra\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*= warnings summary =*\",\n            \"*warning_from_test*\",\n            \"*= short test summary info =*\",\n            \"*= warnings summary (final) =*\",\n            \"*conftest.py:3:*internal warning\",\n            \"*conftest.py:4:*another internal warning\",\n            \"*== 1 failed, 3 warnings in *\",\n        ]\n    )\n    assert \"None\" not in result.stdout.str()\n    stdout = result.stdout.str()\n    assert stdout.count(\"warning_from_test\") == 1\n    assert stdout.count(\"=== warnings summary \") == 2\n", "def test_summary_counts_with_xdist_shards_count(testdir):\n    testdir.makepyfile(\n        \"\"\"\n                pass\n                assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--shard-count=2\")\n    result.stdout.fnmatch_lines([\"*1 failed, 1 passed in*\"])\n", "def test_build_summary_stats_line_with_duplicates(self):\n    stats = {\"failed\": [1, 2, 3], \"passed\": [1, 2, 3, 4]}\n    (line, color) = build_summary_stats_line(stats)\n    assert line == \"3 failed, 4 passed\"\n    assert color == \"red\"\n", "def test_terminal_summary_verbosity(testdir):\n    testdir.makeconftest(\n        \"\"\"\n            terminalreporter.write_line(\"hello world\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-v\")\n    result.stdout.fnmatch_lines([\"*hello world*\"])\n", "def test_terminal_summary_with_no_tests_collected(testdir):\n    \"\"\"Test that pytest_terminal_summary is called even if no tests were collected.\"\"\"\n    testdir.makeconftest(\n        \"\"\"\n            terminalreporter.write_line(\"no tests were collected\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"no tests were collected\"])\n"], "sample_677": ["def test_expression_code_generation() -> None:\n    expression = Expression.compile(\"true and false\")\n    assert isinstance(expression.code, type(compile(\"\", \"\", \"eval\")))\n    assert expression.code.co_filename == \"<pytest match expression>\"\n    assert expression.code.co_name == \"<module>\"\n", "def test_python_keywords(expr: str, expected: bool) -> None:\n    matcher = {\"True\": True, \"False\": False, \"None\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_evaluation_with_matcher(expr: str, ident: str, expected: bool) -> None:\n    matcher = {ident: True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_expression_repr() -> None:\n    expr = Expression.compile(\"true\")\n    assert repr(expr) == f\"<Expression('{expr.code.co_name}')>\"\n", "def test_builtin_idents(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n"], "sample_678": ["def test_create_cleanup_lock(tmp_path):\n    \"\"\"Test that create_cleanup_lock creates a lock file with the correct content.\"\"\"\n    path = tmp_path / \"dir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    lock_file = create_cleanup_lock(path)\n    assert lock_file == lock_path\n    assert lock_path.is_file()\n    with open(str(lock_path), \"r\") as f:\n        assert f.read() == str(os.getpid())\n", "def test_cleanup_candidates(tmp_path):\n    \"\"\"Test that cleanup_candidates returns the correct directories for cleanup.\"\"\"\n    root = tmp_path\n    prefix = \"temp-\"\n    paths = [root / f\"{prefix}{i}\" for i in range(5)]\n    for path in paths:\n        path.mkdir()\n\n    keep = 3\n    candidates = list(cleanup_candidates(root, prefix, keep))\n    assert len(candidates) == 2\n    assert candidates == [paths[0], paths[1]]\n", "def test_register_cleanup_lock_removal(tmp_path, monkeypatch):\n    \"\"\"Ensure that the cleanup lock removal function is registered and called correctly.\"\"\"\n    path = tmp_path / \"dir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    create_cleanup_lock(path)\n\n        assert func.__name__ == \"cleanup_on_exit\"\n\n    monkeypatch.setattr(atexit, \"unregister\", mock_unregister)\n    register_cleanup_lock_removal(lock_path)\n    atexit.unregister(cleanup_on_exit)\n", "def test_ensure_extended_length_path(tmp_path):\n    \"\"\"Ensure that ensure_extended_length_path returns the correct path.\"\"\"\n    path = tmp_path / (\"a\" * 250)\n    extended_path = ensure_extended_length_path(path)\n    if sys.platform == \"win32\":\n        assert str(extended_path).startswith(\"\\\\\\\\?\\\\\")\n    else:\n        assert extended_path == path\n", "def test_cleanup_numbered_dir(tmp_path):\n    \"\"\"Test that cleanup_numbered_dir removes old numbered directories.\"\"\"\n    root = tmp_path / \"root\"\n    root.mkdir()\n    prefix = \"temp-\"\n    keep = 2\n\n    # Create some numbered directories\n    for i in range(5):\n        path = root / f\"{prefix}{i}\"\n        path.mkdir()\n\n    # Create a lock file for one of the directories\n    lock_path = get_lock_path(root / f\"{prefix}3\")\n    lock_path.touch()\n\n    # Call cleanup_numbered_dir\n    cleanup_numbered_dir(root, prefix, keep, 0)\n\n    # Check that the old directories were removed\n    for i in range(3):\n        assert not (root / f\"{prefix}{i}\").exists()\n\n    # Check that the lock file was removed\n    assert not lock_path.exists()\n"], "sample_679": ["def test_mark_evaluator_istrue():\n    item = mock.Mock()\n    item.iter_markers.return_value = [Mark(name=\"test_mark\", args=(\"condition\",))]\n    evaluator = MarkEvaluator(item, \"test_mark\")\n    assert evaluator.istrue() is True\n    item.iter_markers.assert_called_once_with(name=\"test_mark\")\n", "def test_mark_evaluator_istrue():\n    item = mock.Mock(spec=Item)\n    item.obj = None\n    item.cls = None\n    item.module = mock.Mock()\n    item.module.__dict__ = {}\n    mark_evaluator = MarkEvaluator(item, \"some_mark\")\n    assert not mark_evaluator.istrue()\n    item.obj = mock.Mock()\n    item.obj.__globals__ = {\"some_var\": True}\n    mark_evaluator = MarkEvaluator(item, \"some_mark\")\n    mark_evaluator._marks = [Mark(\"some_mark\", (), {\"condition\": \"some_var\"})]\n    assert mark_evaluator.istrue()\n", "def test_mark_evaluator_with_no_marks(testdir):\n    item = testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    evaluator = MarkEvaluator(item, \"my_mark\")\n    assert not evaluator\n    assert not evaluator.wasvalid()\n    assert evaluator.getexplanation() == \"\"\n", "def test_mark_evaluator_istrue_with_invalid_syntax(testdir):\n    \"\"\"Test that MarkEvaluator.istrue handles invalid syntax correctly\"\"\"\n    item = mock.Mock()\n    item.obj = None\n    item.keywords = []\n    item.cls = None\n    item.func = None\n\n    evaluator = MarkEvaluator(item, \"test_mark\")\n    with pytest.raises(SyntaxError):\n        evaluator._istrue(\"invalid syntax\")\n", "def test_compiled_eval_scope():\n    # Test that compiled_eval evaluates in the correct scope.\n    d = {\"x\": 10}\n    result = compiled_eval(\"x\", d)\n    assert result == 10\n\n    # Test that compiled_eval does not leak names in the scope.\n    with pytest.raises(NameError):\n        compiled_eval(\"y\", d)\n\n    # Test that compiled_eval raises SyntaxError for invalid syntax.\n    with pytest.raises(SyntaxError):\n        compiled_eval(\"invalid syntax\", d)\n"], "sample_680": ["def test_skipif_with_invalid_condition(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*SyntaxError: invalid syntax*\", \"*1 error*\"])\n", "def test_skipif_with_invalid_condition(testdir) -> None:\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n\n        class InvalidCondition:\n                raise TypeError(\"INVALID\")\n\n        @pytest.mark.skipif(InvalidCondition(), reason=\"xxx\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'skipif' condition as a boolean\" in excinfo.value.msg\n    assert \"INVALID\" in excinfo.value.msg\n", "def test_xfail_strict_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\", \"*1 failed*\"])\n    assert result.ret == 1\n", "def test_mark_xfail_item_strict(testdir):\n    # Ensure pytest.mark.xfail works with non-Python Item and strict=True\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        class MyItem(pytest.Item):\n            nodeid = 'foo'\n                marker = pytest.mark.xfail(\"1 == 2\", reason=\"Expected failure - false\", strict=True)\n                self.add_marker(marker)\n                assert True\n\n            return MyItem(\"foo\", parent)\n    \"\"\"\n    )\n    result = testdir.inline_run()\n    passed, skipped, failed = result.listoutcomes()\n    assert failed\n    assert not skipped\n", "def test_xfail_strict_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n"], "sample_681": ["def test_log_format_with_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('message\\\\nwith multiple lines')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO*message\",\n            \"    with multiple lines\",\n        ]\n    )\n", "def test_log_file_cli_subdirectories_with_absolute_path_are_successfully_created(testdir):\n    path = testdir.makepyfile(\"\"\" def test_logger(): pass \"\"\")\n    expected = os.path.join(os.path.dirname(str(path)), \"foo\", \"bar\")\n    result = testdir.runpytest(\"--log-file={}/logf.log\".format(os.path.abspath(expected)))\n    assert \"logf.log\" in os.listdir(expected)\n    assert result.ret == ExitCode.OK\n", "def test_percent_style_multiline(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('line1\\\\nline2')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-format=%(message)s\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured log call -*\",\n            \"line1\",\n            \"line2\",\n        ]\n    )\n\n", "def test_percent_style_multiline(testdir):\n    \"\"\"\n    Test that PercentStyleMultiline formats multiline messages as expected.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('Line 1\\\\nLine 2\\\\nLine 3')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO    *test_percent_style_multiline.py* Line 1\",\n            \"Line 2\",\n            \"Line 3\",\n        ]\n    )\n", "def test_log_file_cli_given_directory_is_respected(testdir):\n    log_dir = testdir.mkdir(\"logs\")\n    log_file = log_dir.join(\"log.txt\")\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"Test warning\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-file={}\".format(log_file))\n    assert log_file.exists()\n    with open(str(log_file)) as f:\n        contents = f.read()\n        assert \"Test warning\" in contents\n    assert result.ret == 0\n"], "sample_682": ["def test_xfail_strict_with_invalid_boolean(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class InvalidBool:\n                raise TypeError(\"INVALID\")\n\n        @pytest.mark.xfail(InvalidBool(), reason=\"xxx\", strict=True)\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*ERROR*test_func*\", \"*Error evaluating 'xfail' condition as a boolean*\"])\n", "def test_xfail_strict_with_multiple_conditions(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(strict=True, condition=\"True or False\")\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_default_from_ini_file(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\"])\n    assert result.ret == 1\n", "def test_skipif_with_invalid_condition_type(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(None)\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*Error evaluating 'skipif' condition as a boolean*\", \"*1 error*\"]\n    )\n", "def test_xfail_strict_from_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\", \"*1 failed*\"])\n"], "sample_683": ["def test_readouterr_unicode_decoding_error(tmpfile):\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.write(\"hx\u0105\u0107\\n\")\n    ef.flush()\n    tmpfile.seek(0)\n    tmpfile.write(b\"invalid utf-8: \\xff\\xff\")\n    tmpfile.seek(0)\n    cap = capture.FDCapture(ef.fileno())\n    cap.start()\n    out, err = cap.readouterr()\n    assert out == \"hx\u0105\u0107\\n\"\n    assert \"invalid\" in out  # the invalid utf-8 should be replaced\n", "def test_capturing_enabled_globally_and_fixture(tmpdir):\n    p = tmpdir.makepyfile(\n        \"\"\"\n            print(\"hello\")\n            out, err = capsys.readouterr()\n            assert out == \"hello\\\\n\"\n        \"\"\"\n    )\n    result = tmpdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_encodedfile_mode(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert ef.mode == \"w\"\n    tmpfile.close()\n    with pytest.raises(ValueError):\n        ef.read()\n", "def test_captured_outerr_unicode_decoding_failure(testdir):\n    \"\"\"Test that captured out/err handles decoding failures (#3815).\"\"\"\n    p = testdir.makepyfile(\n        r\"\"\"\n            import os\n            os.write(1, b'\\xff\\n')\n            os.write(2, b'\\xfe\\n')\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*1 failed*\",\n            \"====* FAILURES *====\",\n            \"____*____\",\n            \"*--- Captured stdout *call*\",\n            \"*--- Captured stderr *call*\",\n        ]\n    )\n", "def test_global_capture_with_live_logging_and_capture_fixture(testdir):\n    # Issue 3819\n    # capture should work with live cli logging and capture fixture\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n        @pytest.fixture\n            print(\"fix setup\")\n            logging.info(\"fix setup\")\n            yield\n            logging.info(\"fix teardown\")\n            print(\"fix teardown\")\n\n            print(\"begin test\")\n            logging.info(\"something in test\")\n            print(\"end test\")\n            captured = capsys.readouterr()\n            assert \"begin test\" in captured.out\n            assert \"end test\" in captured.out\n            assert \"something in test\" not in captured.out\n            assert \"something in test\" not in captured.err\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n"], "sample_684": ["    def test_reprlocals_truncates_long_list(self, tw_mock) -> None:\n        lines = [\"variable1 = [1, 2, 3, 4, 5]\", \"variable2 = 'hello'\"]\n        r = ReprLocals(lines)\n        r.toterminal(tw_mock)\n        assert len(tw_mock.lines) == 2\n        assert tw_mock.lines[0] == \"variable1 = [1, 2, 3, ...]\"\n        assert tw_mock.lines[1] == \"variable2 = 'hello'\"\n", "def test_traceback_recursion_index() -> None:\n        if n > 0:\n            recursive_function(n - 1)\n        else:\n            raise Exception(\"Base case\")\n\n    try:\n        recursive_function(10)\n    except Exception:\n        excinfo = ExceptionInfo.from_current()\n\n    tb = excinfo.traceback\n    assert tb.recursionindex() is not None\n    assert tb.recursionindex() == 9\n", "def test_getfslineno_with_decorators() -> None:\n    @pytest.mark.usefixtures(\"tmpdir\")\n            return func(*args, **kwargs)\n\n        return inner\n\n    @wrapper\n        pass\n\n    path, lineno = getfslineno(example_function)\n    assert path.basename == \"test_code.py\"\n    assert lineno > 0\n", "def test_traceback_entry_ishidden() -> None:\n        __tracebackhide__ = True\n        bar()\n\n        raise NotImplementedError()\n\n    with pytest.raises(NotImplementedError):\n        foo()\n\n    excinfo = ExceptionInfo.from_current()\n    entry = excinfo.traceback[0]\n    assert entry.ishidden()\n\n        raise NotImplementedError()\n\n    with pytest.raises(NotImplementedError):\n        baz()\n\n    excinfo = ExceptionInfo.from_current()\n    entry = excinfo.traceback[0]\n    assert not entry.ishidden()\n", "def test_frame_repr(self) -> None:\n        return sys._getframe(0)\n\n    f = Frame(func())\n    assert repr(f.statement) == \"Source('        return sys._getframe(0)')\"\n    assert f.repr(\"test\") == \"'test'\"\n"], "sample_685": ["def test_log_cli_enabled(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_level is not None\n            logger = logging.getLogger('catchlog')\n            logger.warning(\"WARNING message will be shown\")\n            assert 'WARNING' in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_cli_enabled(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger()\n            logger.info(\"INFO message\")\n            assert 'INFO' in caplog.text\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-cli-level=INFO\")\n    result.stdout.fnmatch_lines([\"*INFO message*\"])\n    assert result.ret == 0\n", "def test_caplog_captures_multiple_loggers(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"message from main logger\")\n    sublogger.info(\"message from sublogger\")\n    assert \"message from main logger\" in caplog.text\n    assert \"message from sublogger\" in caplog.text\n    assert len(caplog.records) == 2\n    assert len(caplog.messages) == 2\n    assert len(caplog.record_tuples) == 2\n", "def test_log_file_path_with_directory_creation(testdir):\n    log_file_path = \"logs/test.log\"\n    testdir.makepyfile(\n        f\"\"\"\n        import logging\n            logging.info(\"Test log message\")\n        \"\"\"\n    )\n    result = testdir.runpytest(f\"--log-file={log_file_path}\")\n    assert result.ret == 0\n    assert testdir.tmpdir.join(log_file_path).exists()\n", "def test_percent_style_multiline_auto_indent(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo\\n%s\", \"arg\")\n    assert \"boo\\n    arg\" in caplog.text\n\n    logger.info(\"boo\\n%s\", \"arg\", extra={\"auto_indent\": True})\n    assert \"boo\\n    arg\" in caplog.text\n\n    logger.info(\"boo\\n%s\", \"arg\", extra={\"auto_indent\": False})\n    assert \"boo\\narg\" in caplog.text\n\n    logger.info(\"boo\\n%s\", \"arg\", extra={\"auto_indent\": 4})\n    assert \"boo\\n    arg\" in caplog.text\n\n    logger.info(\"boo\\n%s\", \"arg\", extra={\"auto_indent\": 0})\n    assert \"boo\\narg\" in caplog.text\n"], "sample_686": ["def test_warning_captured_hook_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"The pytest_warning_captured is deprecated and will be removed in a future release\",\n    ):\n        pytest.hookimpl(tryfirst=True)(deprecated.WARNING_CAPTURED_HOOK)\n", "def test_warning_captured_hook_deprecated(testdir) -> None:\n    testdir.makeconftest(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated*\"])\n", "def test_fixture_positional_arguments_warn(testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(\"function\", 1)\n            return 1\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: Passing arguments to pytest.fixture() as positional arguments is deprecated*\"]\n    )\n", "def test_fixture_positional_arguments(testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(\"function\")\n            return 1\n\n            pass\n    \"\"\"\n    )\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"Passing arguments to pytest.fixture() as positional arguments is deprecated\"):\n        testdir.parseconfig()\n", "def test_fixture_positional_arguments_warning() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture\\\\(\\\\) as positional arguments is deprecated\",\n    ):\n        pytest.fixture(\"module\")\n"], "sample_687": ["def test_log_cli_enabled(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin._log_cli_enabled()\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=INFO\")\n    result.assert_outcomes(passed=1)\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=True\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_cli_enabled(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli = True\n        log_level = DEBUG\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger('catchlog')\n            logger.debug(\"DEBUG message will be shown\")\n            assert \"DEBUG message will be shown\" in caplog.text\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*DEBUG message will be shown*\"])\n    assert result.ret == 0\n", "def test_caplog_captures_multiline_log_message(caplog):\n    logger = logging.getLogger(__name__)\n    caplog.set_level(logging.INFO)\n\n    logger.info(\"Line 1\\nLine 2\\nLine 3\")\n    assert len(caplog.records) == 1\n    assert \"Line 1\" in caplog.text\n    assert \"Line 2\" in caplog.text\n    assert \"Line 3\" in caplog.text\n\n    # Ensure the log message is properly formatted.\n    formatted_log_message = caplog.records[0].getMessage()\n    assert formatted_log_message == \"Line 1\\nLine 2\\nLine 3\"\n", "def test_log_file_handler_level_control(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_level == logging.WARNING\n\n            logger = logging.getLogger('catchlog')\n            logger.info(\"INFO message won't be shown in file\")\n            logger.warning(\"WARNING message will be shown in file\")\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level=WARNING\n        log_file=test.log\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n\n    with open(\"test.log\", \"r\") as f:\n        log_content = f.read()\n\n    assert \"INFO message won't be shown in file\" not in log_content\n    assert \"WARNING message will be shown in file\" in log_content\n", "def test_log_report_captures_according_to_config_option_upon_error(testdir):\n    \"\"\"Test that upon an error in a test:\n    (1) `caplog` succeeded to capture the DEBUG message and assert on it => No `Exception` is raised.\n    (2) The `DEBUG` message does NOT appear in the `Captured log call` report.\n    (3) The stdout, `INFO`, and `WARNING` messages DO appear in the test reports due to `--log-level=INFO`.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            if 'DEBUG log ' + 'message' not in caplog.text:\n                raise Exception('caplog failed to ' + 'capture DEBUG')\n\n            assert 1 / 0\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-level=INFO\")\n    result.stdout.no_fnmatch_line(\"*Exception: caplog failed to capture DEBUG*\")\n    result.stdout.no_fnmatch_line(\"*DEBUG log message*\")\n    result.stdout.fnmatch_lines(\n        [\"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n    )\n    assert result.ret == 1\n"], "sample_688": ["def test_parts_function(tmpdir):\n    \"\"\"Test the parts function with different inputs.\"\"\"\n    s = str(tmpdir.join(\"a\", \"b\", \"c\"))\n    assert parts(s) == {str(tmpdir), str(tmpdir.join(\"a\")), str(tmpdir.join(\"a\", \"b\")), s}\n    s = str(tmpdir.join(\"a\", \"b\", \"c\", \"d\"))\n    assert parts(s) == {\n        str(tmpdir),\n        str(tmpdir.join(\"a\")),\n        str(tmpdir.join(\"a\", \"b\")),\n        str(tmpdir.join(\"a\", \"b\", \"c\")),\n        s,\n    }\n", "def test_absolutepath(testdir):\n    \"\"\"Test that absolutepath works as expected.\"\"\"\n    testdir.makepyfile(\"def test_foo(): pass\")\n    p = testdir.tmpdir.join(\"test_foo.py\")\n    assert absolutepath(p) == Path(os.path.abspath(str(p)))\n    assert absolutepath(str(p)) == Path(os.path.abspath(str(p)))\n", "def test_parts_function(tmpdir):\n    s = \"path/to/somewhere\"\n    expected = {\"path\", \"path/to\", \"path/to/somewhere\"}\n    assert parts(s) == expected\n\n    # Test with trailing path separator.\n    s_trailing = s + os.path.sep\n    assert parts(s_trailing) == expected\n\n    # Test with absolute path.\n    abs_s = str(tmpdir.join(s))\n    expected_abs = {os.path.sep, os.path.join(os.path.sep, \"path\"), abs_s}\n    assert parts(abs_s) == expected_abs\n\n    # Test with absolute path and trailing path separator.\n    abs_s_trailing = abs_s + os.path.sep\n    assert parts(abs_s_trailing) == expected_abs\n", "def test_find_prefixed(root: Path, prefix: str) -> None:\n    \"\"\"Test find_prefixed function with various prefixes and paths.\"\"\"\n    # Create some sample files\n    root.joinpath(\"test_file1.txt\").touch()\n    root.joinpath(\"test_file2.txt\").touch()\n    root.joinpath(\"other_file.txt\").touch()\n\n    # Test with a prefix that matches some files\n    result = list(find_prefixed(root, \"test_\"))\n    assert len(result) == 2\n    assert result[0].name == \"test_file1.txt\"\n    assert result[1].name == \"test_file2.txt\"\n\n    # Test with a prefix that doesn't match any files\n    result = list(find_prefixed(root, \"nonexistent_\"))\n    assert len(result) == 0\n\n    # Test with an empty prefix (should return all files)\n    result = list(find_prefixed(root, \"\"))\n    assert len(result) == 3\n    assert result[0].name == \"other_file.txt\"\n    assert result[1].name == \"test_file1.txt\"\n    assert result[2].name == \"test_file2.txt\"\n", "def test_import_path_mode_append(tmpdir):\n    \"\"\"Test that import_path with mode='append' correctly handles importing a module from a package.\"\"\"\n    pkg = tmpdir.mkdir(\"pkg\")\n    pkg.join(\"__init__.py\").write(\"\")\n    module = pkg.join(\"module.py\")\n    module.write(\"def test_function(): pass\")\n    sys.path.append(str(tmpdir))\n    imported_module = import_path(module, mode=\"append\")\n    assert imported_module.__name__ == \"module\"\n    assert imported_module.test_function.__name__ == \"test_function\"\n"], "sample_689": ["def test_warning_captured_hook_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            \"The pytest_warning_captured is deprecated and will be removed in a future release.\\n\"\n            \"Please use pytest_warning_recorded instead.\"\n        ),\n    ):\n        warnings.warn(deprecated.WARNING_CAPTURED_HOOK)\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    \"\"\"The pytest_warning_captured hook is deprecated (#7530).\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.hookimpl\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated and will be removed in a future release.\",\n            \"*Please use pytest_warning_recorded instead.\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            \"The pytest_warning_captured is deprecated and will be removed in a future release.\\n\"\n            \"Please use pytest_warning_recorded instead.\"\n        ),\n    ):\n        warnings.warn(deprecated.WARNING_CAPTURED_HOOK)\n", "def test_warning_captured_hook_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            \"The pytest_warning_captured is deprecated and will be removed in a future release.\\n\"\n            \"Please use pytest_warning_recorded instead.\"\n        ),\n    ):\n        warnings.warn(deprecated.WARNING_CAPTURED_HOOK, stacklevel=2)\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    \"\"\"pytest_warning_captured is deprecated and will be removed in a future release (#7530).\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import warnings\n\n            warnings.warn(\"Using pytest_warning_captured\")\n        \"\"\"\n    )\n    pytester.makepyfile(\"def test_foo(): pass\")\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\"])\n"], "sample_690": ["def test_xfail_strict_from_ini(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_strict_ini(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_strict_with_multiple_conditions(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(\"True or False\", reason=\"Expected failure\", strict=True)\n            assert 1\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*Expected failure*\", \"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_mark_with_multiple_conditions(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(condition=\"1 == 2\", reason=\"first reason\")\n        @pytest.mark.xfail(condition=\"3 == 4\", reason=\"second reason\")\n            assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines(\n        [\"*XFAIL*test_func*\", \"*first reason*\", \"*second reason*\"]\n    )\n", "def test_xfail_strict_from_ini(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n"], "sample_691": ["def test_unconfigure(pytester: Pytester, monkeypatch) -> None:\n    \"\"\"Test that faulthandler is properly unconfigured after pytest finishes.\"\"\"\n    import faulthandler\n    from _pytest.faulthandler import FaultHandlerHooks\n\n    called = []\n\n    monkeypatch.setattr(faulthandler, \"disable\", lambda: called.append(1))\n    monkeypatch.setattr(FaultHandlerHooks, \"_get_stderr_fileno\", lambda _: 2)\n\n    pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n    assert called == [1]\n", "def test_fault_handler_stderr_key(pytester: Pytester) -> None:\n    \"\"\"Test fault handler stderr key is properly stored and cleaned up.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            import faulthandler\n            assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\"-o\", \"faulthandler_timeout=1\")\n    result.stdout.fnmatch_lines(\"*1 passed*\")\n    assert result.ret == 0\n\n    # check that the stderr key is removed after the test run\n    config = pytester.parseconfigure()\n    assert fault_handler_stderr_key not in config._store\n", "def test_fault_handler_stderr_key(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler's stderr key is properly stored and cleaned up.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n\n    # Check that the stderr key is removed from the config after the test run\n    from _pytest.faulthandler import fault_handler_stderr_key\n    from _pytest.config import Config\n\n    config = Config.fromdictargs({}, [\"-vv\"])\n    assert fault_handler_stderr_key not in config._store\n", "def test_get_stderr_fileno_attribute_error() -> None:\n    \"\"\"Test for faulthandler being able to handle AttributeError when accessing stderr (#8249).\"\"\"\n    from _pytest.faulthandler import FaultHandlerHooks\n\n    class StdErrWrapper:\n        \"\"\"\n        Mimic an object that does not have a fileno attribute.\n        \"\"\"\n\n            if name == \"fileno\":\n                raise AttributeError(\"No fileno attribute\")\n            return super().__getattr__(name)\n\n    wrapper = StdErrWrapper()\n\n    with pytest.MonkeyPatch.context() as mp:\n        mp.setattr(\"sys.stderr\", wrapper)\n\n        # Even when the stderr wrapper raises an AttributeError,\n        # ``_get_stderr_fileno()`` should return the real one.\n        assert FaultHandlerHooks._get_stderr_fileno() == sys.__stderr__.fileno()\n", "def test_unconfigure(pytester: Pytester) -> None:\n    \"\"\"Test that faulthandler is properly unconfigured after pytest finishes.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    with pytest.MonkeyPatch.context() as mp:\n        import faulthandler\n\n        original_stderr = sys.stderr\n        mp.setattr(\"sys.stderr\", io.StringIO())\n\n        try:\n            result = pytester.runpytest_subprocess()\n            assert result.ret == 0\n        finally:\n            # restore stderr to check for file descriptor leaks\n            sys.stderr = original_stderr\n\n        # faulthandler should be disabled after pytest finishes\n        assert not faulthandler.is_enabled()\n\n        # verify that the file descriptor is not leaked\n        import os\n\n        open_files = [f for f in os.listdir(\"/proc/self/fd\") if f.isdigit()]\n        assert len(open_files) == 3  # stdin, stdout, stderr\n"], "sample_692": ["def test_tmp_path_factory_uses_existing_basetemp(tmp_path_factory: TempPathFactory) -> None:\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp.exists()\n    new_factory = TempPathFactory.from_config(FakeConfig(basetemp), _ispytest=True)\n    new_basetemp = new_factory.getbasetemp()\n    assert new_basetemp == basetemp\n", "def test_temp_path_factory_getbasetemp_with_given_basetemp(tmp_path):\n    given_basetemp = tmp_path / \"given-basetemp\"\n    given_basetemp.mkdir()\n    temp_path_factory = TempPathFactory(given_basetemp, trace=None)\n    basetemp = temp_path_factory.getbasetemp()\n    assert basetemp == given_basetemp.resolve()\n", "def test_tmp_path_factory_uses_env_var(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_temp_path_factory_from_config_creates_basetemp_with_proper_permissions(\n    tmp_path: Path, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_uses_configured_basetemp(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    basetemp = Path(\"my_basetemp\")\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", basetemp)\n    p = tmp_path_factory.getbasetemp()\n    assert p == basetemp.resolve()\n"], "sample_693": ["def test_unittest_subtest(pytester: Pytester) -> None:\n    pytest.importorskip(\"unittest\")\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestExample(unittest.TestCase):\n\n                for i in range(5):\n                    with self.subTest(i=i):\n                        self.assertEqual(i % 2, 0)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*3 failed, 2 passed*\"])\n", "def test_is_skipped(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        @unittest.skip(\"skip reason\")\n        class MyTestCase(unittest.TestCase):\n                pass\n\n        class MyOtherTestCase(unittest.TestCase):\n            @unittest.skip(\"skip reason\")\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*SKIP*[1]*MyTestCase*skip reason*\",\n            \"*SKIP*[1]*MyOtherTestCase*test_func*skip reason*\",\n            \"*2 skipped*\",\n        ]\n    )\n    assert result.ret == 0\n", "def test_pytest_unittest_parametrize(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        class MyTestCase(unittest.TestCase):\n\n            @pytest.mark.parametrize(\"arg1,arg2\", [(\"a\", \"b\"), (\"c\", \"d\")])\n                assert arg1 != arg2\n\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines([\"*MyTestCase::test_func[a-b]*\", \"*MyTestCase::test_func[c-d]*\"])\n    result.assert_outcomes(passed=2)\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytest.importorskip(\"unittest\")\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                with self.subTest(msg=\"subtest\"):\n                    assert 1 == 2\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*MyTestCase::test_foo*\"], consecutive=True)\n    assert result.ret == 1\n", "def test_unittest_assertion_helper(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                self.assert_(False)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*AssertionError*\",\n            \"*assert False*\",\n        ]\n    )\n"], "sample_694": ["def test_deprecation_of_type_argument_in_addoption(tmp_path, pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"string\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'string', \"\n            \"but when supplied should be a type (for example `str` or `int`). (options: ['--foo'])\",\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", default=\"bar\", help=\"%%default == bar\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: pytest now uses argparse. \\\"%%default\\\" should be changed to \\\"%%(default)s\\\"\",\n        ]\n    )\n", "def test_unformatted_warning_argtype_str_choice():\n    with pytest.warns(\n        PytestRemovedIn8Warning,\n        match=r\"`type` argument to addoption\\(\\) is the string 'choice'.*\",\n    ):\n        deprecated.ARGUMENT_TYPE_STR_CHOICE.format(typ=\"choice\", names=[\"-a\", \"--arg\"])\n", "def test_addoption_type_is_string(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"string\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'string', \"\n            \"but when supplied should be a type (for example `str` or `int`). (options: ['--foo'])\",\n        ]\n    )\n", "def test_addoption_type_argument_deprecated() -> None:\n    with pytest.warns(\n        PytestRemovedIn8Warning,\n        match=r\"`type` argument to addoption\\(\\) is the string\",\n    ):\n        pytest.addoption(\"--foo\", type=\"string\")\n\n    with pytest.warns(\n        PytestRemovedIn8Warning,\n        match=r\"For choices this is optional and can be omitted\",\n    ):\n        pytest.addoption(\"--bar\", type=\"choice\", choices=[\"a\", \"b\"])\n\n    # Doesn't warn.\n    pytest.addoption(\"--baz\", type=str)\n"], "sample_695": ["def test_get_fslocation_from_item() -> None:\n    node = nodes.Node.from_parent(None, name=\"test_node\")\n    assert get_fslocation_from_item(node) == (\"unknown location\", -1)\n\n    class MockNode(nodes.Node):\n            self.fspath = fspath\n\n    node = MockNode(\"path/to/node\")\n    assert get_fslocation_from_item(node) == (\"path/to/node\", -1)\n\n    class MockObjNode(nodes.Node):\n            self.obj = obj\n\n    obj = lambda: None\n    obj.__code__ = type(obj.__code__)(\n        obj.__code__.co_argcount,\n        obj.__code__.co_code,\n        obj.__code__.co_consts,\n        obj.__code__.co_filename,\n        \"mock_name\",\n        obj.__code__.co_firstlineno,\n        obj.__code__.co_flags,\n        obj.__code__.co_freevars,\n        obj.__code__.co_kwonlyargcount,\n        obj.__code__.co_lnotab,\n        obj.__code__.co_name,\n        obj.__code__.co_names,\n        obj.__code__.co_nlocals,\n        obj.__code__.co_posonlyargcount,\n        obj.__code__.co_stacksize,\n        obj.__code__.co_varnames,\n    )\n    node = MockObjNode(obj)\n    assert get_fslocation_from_item(node) == (obj.__code__.co_filename, 0)\n", "def test_add_marker() -> None:\n    \"\"\"Test that a marker can be added to a node.\"\"\"\n    node = nodes.Node.from_parent(parent=None, name=\"test_node\")\n    marker = nodes.Mark(name=\"test_marker\", args=(), kwargs={})\n\n    node.add_marker(marker)\n\n    assert \"test_marker\" in node.keywords\n    assert node.own_markers == [marker]\n", "def test_node_repr_failure_with_changed_cwd(pytester: Pytester) -> None:\n    \"\"\"\n    Test failure lines should use absolute paths if cwd has changed since\n    invocation, so the path is correct (#6428).\n    \"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import os\n        import pytest\n\n        @pytest.fixture\n            out_dir = 'ddd'\n            os.mkdir(out_dir)\n            old_dir = os.getcwd()\n            os.chdir(out_dir)\n            yield out_dir\n            os.chdir(old_dir)\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--tb=long\")\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\"])\n    # Ensure we display full absolute path to avoid confusion.\n    assert str(p.absolute()) in result.stdout.str()\n", "def test_item_location(tmp_path: Path, pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    items = pytester.getitems(p)\n    item = items[0]\n    assert item.location[0] == str(p.relative_to(pytester.path))\n    assert item.location[1] is not None\n", "def test_node_location_to_relpath_trailing_slash(tmp_path: Path, pytester: Pytester) -> None:\n    \"\"\"Ensure that trailing slashes are handled correctly when converting a node location to a relative path.\"\"\"\n    p = tmp_path / \"dir\"\n    p.mkdir()\n    item = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\",\n        path=p,\n    )[0]\n    session = item.session\n    assert session._node_location_to_relpath(str(p)) == \"dir\"\n    assert session._node_location_to_relpath(str(p) + os.sep) == \"dir\"\n"], "sample_696": ["def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    \"\"\"Test that pytest warns about %default in addoption.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption(\"--foo\", default=\"bar\", help=\"Foo option (%default)\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: pytest now uses argparse. \\\"%(default)s\\\" should be used instead of \\\"%default\\\"*\"]\n    )\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n            parser.addoption(\"--foo\", type=str, default=\"bar\", help=\"%%default == 'bar'\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%%default\\\" should be changed to \\\"%%(default)s\\\"\",\n        ]\n    )\n", "def test_check_ispytest_legacy_use_case() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning, match=\"private pytest class or function\"\n    ):\n        deprecated.check_ispytest(False)\n\n    # Doesn't warn.\n    deprecated.check_ispytest(True)\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n            parser.addoption(\"--foo\", action=\"store\", default=\"bar\", help=\"%%default\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%%default\\\" should be changed to \\\"%%(default)s\\\"\",\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=r\"pytest now uses argparse. %default should be changed to %(default)s\",\n    ):\n        warn(deprecated.ARGUMENT_PERCENT_DEFAULT, stacklevel=3)\n"], "sample_697": ["def test_tmp_path_factory_with_basetemp_as_pathlib_path(\n    tmp_path: Path, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_custom_basetemp_with_symlink(tmp_path, monkeypatch):\n    # Create a custom basetemp directory with a symlink in its path.\n    real_basetemp = tmp_path / \"real-basetemp\"\n    real_basetemp.mkdir()\n    link_basetemp = tmp_path / \"link-basetemp\"\n    attempt_symlink_to(link_basetemp, str(real_basetemp))\n    \n    # Use the symlinked basetemp directory.\n    monkeypatch.setattr(\"os.environ\", {\"PYTEST_DEBUG_TEMPROOT\": str(link_basetemp)})\n    tmp_factory = TempPathFactory(None, lambda *args: None, _ispytest=True)\n    basetemp = tmp_factory.getbasetemp()\n\n    # Verify that the basetemp directory is the real one, not the symlink.\n    assert basetemp.resolve() == real_basetemp.resolve()\n", "def test_tmp_path_factory_uses_given_basetemp(tmp_path_factory: TempPathFactory, tmp_path: Path) -> None:\n    given_basetemp = tmp_path / \"given_basetemp\"\n    given_basetemp.mkdir()\n    tmp_factory = TempPathFactory(given_basetemp, lambda *args: None, _ispytest=True)\n    basetemp = tmp_factory.getbasetemp()\n\n    assert basetemp == given_basetemp.resolve()\n    assert (basetemp.stat().st_mode & 0o077) == 0\n", "def test_tmp_path_factory_basetemp_already_exists(tmp_path: Path, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Verify that if the basetemp directory already exists, it is reused.\"\"\"\n    # Use the test's tmp_path as the system temproot (/tmp).\n    monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(tmp_path))\n    existing_basetemp = tmp_path / \"pytest-of-foo\"\n    existing_basetemp.mkdir()\n    tmp_factory = TempPathFactory(None, lambda *args: None, _ispytest=True)\n    basetemp = tmp_factory.getbasetemp()\n\n    assert basetemp == existing_basetemp\n    assert (basetemp.stat().st_mode & 0o077) == 0\n", "def test_tmp_path_factory_uses_given_basetemp(tmp_path_factory: TempPathFactory, tmp_path: Path) -> None:\n    given_basetemp = tmp_path / \"given-basetemp\"\n    tmp_path_factory = TempPathFactory(given_basetemp, lambda *args: None, _ispytest=True)\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp == given_basetemp\n    assert basetemp.exists()\n"], "sample_698": ["def test_percent_style_multiline_get_auto_indent() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    # Test that get_auto_indent handles all the possible values correctly\n    style = PercentStyleMultiline(\"%(message)s\", auto_indent=None)\n    assert style._get_auto_indent(True) == -1\n    assert style._get_auto_indent(False) == 0\n    assert style._get_auto_indent(\"True\") == -1\n    assert style._get_auto_indent(\"False\") == 0\n    assert style._get_auto_indent(\"On\") == -1\n    assert style._get_auto_indent(\"Off\") == 0\n    assert style._get_auto_indent(5) == 5\n    assert style._get_auto_indent(\"5\") == 5\n    assert style._get_auto_indent({}) == 0\n    assert style._get_auto_indent(\"junk\") == 0\n", "def test_get_option_ini() -> None:\n    from _pytest.logging import get_option_ini\n\n    class Config:\n            if name == \"log_format\":\n                return \"%(levelname)s %(message)s\"\n            return None\n\n            if name == \"log_format\":\n                return \"%(levelname)s %(message)s\"\n            elif name == \"log_date_format\":\n                return \"%H:%M\"\n\n    config = Config()\n    assert get_option_ini(config, \"log_format\") == \"%(levelname)s %(message)s\"\n    assert get_option_ini(config, \"log_date_format\") == \"%H:%M\"\n    assert get_option_ini(config, \"non_existing_option\") is None\n", "def test_percent_style_multiline_with_fixed_indentation() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record: Any = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message line1\\nline2\",\n        args=(),\n        exc_info=None,\n    )\n    # this is called by logging.Formatter.format\n    record.message = record.getMessage()\n\n    ai_on_style = PercentStyleMultiline(logfmt, 5)\n    output = ai_on_style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\n\"\n        \"                                    line2\"\n    )\n\n    ai_off_style = PercentStyleMultiline(logfmt, 0)\n    output = ai_off_style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\nline2\"\n    )\n\n    ai_neg_style = PercentStyleMultiline(logfmt, -1)\n    output = ai_neg_style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\n\"\n        \"                                        line2\"\n    )\n", "def test_coloredlogformatter_with_invalid_log_level() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=100,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 NOTSET   Test Message\"\n    )\n", "def test_coloredlogformatter_no_color_if_no_terminalwriter_markup() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    class NoMarkupTerminalWriter:\n            self.hasmarkup = False\n\n            return text\n\n    tw = NoMarkupTerminalWriter()\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 INFO     Test Message\")\n"], "sample_699": ["def test_doctest_namespace_is_not_modified(pytester: Pytester) -> None:\n    \"\"\"Ensure that doctest_namespace fixture does not modify the namespace.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True)\n            doctest_namespace['foo'] = 'bar'\n        \"\"\"\n    )\n    pytester.maketxtfile(\n        test_doc=\"\"\"\n        >>> foo\n        'bar'\n        >>> 'new_value' in globals()\n        False\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_namespace_is_not_shared_between_doctest_items(pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n            '''\n            >>> doctest_namespace['x'] = 1\n            >>> x\n            1\n            '''\n\n            '''\n            >>> 'x' in doctest_namespace\n            False\n            >>> doctest_namespace['y'] = 2\n            >>> y\n            2\n            '''\n    \"\"\"\n    )\n    reprec = pytester.inline_run(\"--doctest-modules\")\n    reprec.assertoutcome(passed=2)\n", "def test_doctest_unexpected_exception_traceback(pytester: Pytester):\n    pytester.maketxtfile(\n        \"\"\"\n        >>> i = 0\n        >>> 0 / i\n        2\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\n            \"test_doctest_unexpected_exception_traceback.txt F *\",\n            \"\",\n            \"*= FAILURES =*\",\n            \"*_ [[]doctest[]] test_doctest_unexpected_exception_traceback.txt _*\",\n            \"001 >>> i = 0\",\n            \"002 >>> 0 / i\",\n            \"UNEXPECTED EXCEPTION: ZeroDivisionError*\",\n            \"Traceback (most recent call last):\",\n            '  File \"*/doctest.py\", line *, in __run',\n            \"    *\",\n            '  File \"<doctest test_doctest_unexpected_exception_traceback.txt[1]>\", line 1, in <module>',\n            \"ZeroDivisionError: division by zero\",\n            \"*/test_doctest_unexpected_exception_traceback.txt:2: UnexpectedException\",\n        ],\n        consecutive=True,\n    )\n", "def test_doctest_get_optionflags(tmp_path: Path, pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        doctest_optionflags =\n            ELLIPSIS\n            NORMALIZE_WHITESPACE\n    \"\"\"\n    )\n    pytester.maketxtfile(\n        test_doc=\"\"\"\n        >>> import sys\n        >>> sys.stdout.write('Hello\\\\n')\n        Hello...\n        >>> sys.stdout.write('   a  \\\\t')\n         a...\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_unex_importerror_with_namespace(self, pytester: Pytester):\n    pytester.makeconftest(\n        \"\"\"\n        class MyException(Exception):\n            pass\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import asdalsdkjaslkdjasd\n\n            '''\n            >>> 1 + 1\n            2\n            '''\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        filterwarnings =\n            error::pytest.PytestUnraisableExceptionWarning\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\n            \"ERROR collecting *\",\n            \"ModuleNotFoundError: No module named *asdals*\",\n            \"*= 1 error in *\",\n        ]\n    )\n"], "sample_700": ["def test_xfail_markeval_namespace_keyerror(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            return {\"color\": \"green\"}\n        \"\"\"\n    )\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(\"nonexisting == 'green'\")\n            assert False\n\n        @pytest.mark.xfail(\"nonexisting == 'red'\")\n            assert False\n    \"\"\"\n    )\n    res = pytester.runpytest(p)\n    assert res.ret == 1\n    res.stdout.fnmatch_lines([\"*1 failed*\"])\n    res.stdout.fnmatch_lines([\"*1 xfailed*\"])\n", "def test_xfail_imperative_in_setup_function_with_strict(\n    pytester: Pytester, monkeypatch: pytest.MonkeyPatch", "def test_importorskip_with_custom_message() -> None:\n    with pytest.raises(\n        pytest.skip.Exception,\n        match=\"^could not import 'doesnotexist': custom message$\",\n    ):\n        pytest.importorskip(\"doesnotexist\", reason=\"custom message\")\n", "def test_bestrelpath_with_rootdir(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        **{\n            \"tests/test_1.py\": \"\"\"\n        import pytest\n        @pytest.mark.skip()\n            pass\n            \"\"\",\n        }\n    )\n    result = pytester.runpytest(\"-rs\", \"tests/test_1.py\", \"--rootdir=tests\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n    # Check that bestrelpath is used in the skip message.\n    assert \"tests\" not in result.stdout.str()\n", "def test_pytest_skip_marked_function(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip(reason=\"my reason\")\n            assert 0\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*SKIP*my reason*\"])\n"], "sample_701": ["def test_argument_type_str_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"str\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str', \"\n            \"but when supplied should be a type (for example `str` or `int`).*\",\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\"pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"\"),\n    ):\n        from _pytest.deprecated import ARGUMENT_PERCENT_DEFAULT\n        warn(ARGUMENT_PERCENT_DEFAULT)\n", "def test_argument_type_str_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"str\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str',*\",\n            \"*but when supplied should be a type (for example `str` or `int`).*\",\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", action=\"store\", default=\"bar\", help=\"%%default == bar\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%%default\\\" should be changed to \\\"%%(default)s\\\"\",\n        ]\n    )\n", "def test_argument_type_str_choice_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            \"`type` argument to addoption() is the string 'choice'. \"\n            \"For choices this is optional and can be omitted, \"\n            \"but when supplied should be a type (for example `str` or `int`).\"\n        ),\n    ):\n        pytest.addoption(\"--foo\", type=\"choice\", choices=[\"bar\", \"baz\"])\n"], "sample_702": ["def test_pytester_run_with_timeout_float(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\"import time; def test_sleep(): time.sleep(2)\")\n    result = pytester.runpytest_subprocess(testfile, timeout=0.5)\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_pytester_run_with_close_stdin(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import sys\n\n            assert sys.stdin.closed\n    \"\"\"\n    )\n    result = pytester.runpytest_inprocess()\n    result.assert_outcomes(passed=1)\n", "def test_pytester_copy_example(pytester: Pytester, tmp_path_factory: TempPathFactory) -> None:\n    example_dir = tmp_path_factory.mktemp(\"example\")\n    pytester._request.config._inicfg[\"pytester_example_dir\"] = str(example_dir)\n    example_file = example_dir / \"example.txt\"\n    example_file.write_text(\"example content\")\n    dest = pytester.copy_example(\"example.txt\")\n    assert dest.read_text() == \"example content\"\n", "def test_pytester_spawn_with_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n\n            time.sleep(10)\n        \"\"\"\n    )\n    child = pytester.spawn_pytest(str(pytester.path), expect_timeout=0.5)\n    assert child.isalive()\n    child.terminate(force=True)\n", "def test_run_result_duration(pytester: Pytester) -> None:\n    pytester.makepyfile(\"def test_pass(): pass\")\n    result = pytester.runpytest()\n    assert isinstance(result.duration, float)\n    assert result.duration > 0.0\n"], "sample_703": ["def test_builtin_idents(input: str, expected: bool) -> None:\n    assert evaluate(input, lambda ident: False) is expected\n", "def test_matcher_adapter() -> None:\n    matcher = {\"foo\": True, \"bar\": False}.__getitem__\n    adapter = MatcherAdapter(matcher)\n    assert adapter[\"$foo\"] is True\n    assert adapter[\"$bar\"] is False\n    with pytest.raises(KeyError):\n        adapter[\"$baz\"]\n", "def test_expression_class() -> None:\n    expr = Expression.compile(\"true\")\n    assert isinstance(expr, Expression)\n    assert hasattr(expr, \"code\")\n    assert callable(expr.evaluate)\n\n        return ident == \"true\"\n\n    assert expr.evaluate(matcher)\n", "def test_expression_repr() -> None:\n    expr = Expression.compile(\"true\")\n    assert repr(expr) == f\"Expression({expr.code!r})\"\n", "def test_expression_compile_cache() -> None:\n    expr1 = Expression.compile(\"true\")\n    expr2 = Expression.compile(\"true\")\n    assert expr1.code is expr2.code\n    expr3 = Expression.compile(\"false\")\n    assert expr1.code is not expr3.code\n"], "sample_704": ["def test_node_repr_failure_no_traceback(pytester: Pytester) -> None:\n    \"\"\"Test that Node._repr_failure_py handles an empty traceback.\"\"\"\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    excinfo = pytest.raises(AssertionError, match=\"assert False\")\n    excinfo.traceback = []\n    result = items[0]._repr_failure_py(excinfo)\n    assert \"AssertionError\" in str(result)\n", "def test_node_get_closest_marker() -> None:\n    class DummyNode(nodes.Node):\n            super().__init__(name, parent=parent)\n\n    root = DummyNode(\"root\")\n    child = DummyNode(\"child\", parent=root)\n    grandchild = DummyNode(\"grandchild\", parent=child)\n\n    mark = pytest.mark.my_mark\n    child.add_marker(mark)\n\n    assert grandchild.get_closest_marker(\"my_mark\") == mark\n    assert grandchild.get_closest_marker(\"nonexistent_mark\") is None\n    assert grandchild.get_closest_marker(\"nonexistent_mark\", default=\"default\") == \"default\"\n", "def test_get_fslocation_from_item(tmp_path: Path) -> None:\n    node = nodes.Item(\"test\", parent=nodes.Node(\"parent\", nodeid=\"nodeid\"))\n    node.location = (\"path/to/test.py\", 10, \"test_function\")\n    assert nodes.get_fslocation_from_item(node) == (\"path/to/test.py\", 10)\n\n    node = nodes.Item(\"test\", parent=nodes.Node(\"parent\", nodeid=\"nodeid\"))\n    obj = tmp_path / \"test.py\"\n    obj.touch()\n    node.obj = obj\n    assert nodes.get_fslocation_from_item(node) == (str(obj), 0)\n\n    node = nodes.Item(\"test\", parent=nodes.Node(\"parent\", nodeid=\"nodeid\"))\n    fspath = legacy_path(tmp_path / \"test.py\")\n    node.fspath = fspath\n    assert nodes.get_fslocation_from_item(node) == (fspath, -1)\n", "def test_node_listextrakeywords() -> None:\n    \"\"\"Test that Node.listextrakeywords returns a set of all extra keywords.\"\"\"\n    node = nodes.Node.from_parent(None, name=\"test_node\")\n    node.extra_keyword_matches = {\"extra1\", \"extra2\"}\n    child_node = nodes.Node.from_parent(node, name=\"child_node\")\n    child_node.extra_keyword_matches = {\"extra3\"}\n    grandchild_node = nodes.Node.from_parent(child_node, name=\"grandchild_node\")\n    assert grandchild_node.listextrakeywords() == {\"extra1\", \"extra2\", \"extra3\"}\n", "def test_node_repr_failure_non_ascii_error_message(pytester: Pytester) -> None:\n    \"\"\"Test that node._repr_failure can handle non-ascii characters in the error message.\"\"\"\n    items = pytester.getitems(\n        \"\"\"\n            raise AssertionError('\u20ac')\n    \"\"\"\n    )\n    excinfo = pytest.raises(AssertionError, items[0].obj)\n    representation = items[0]._repr_failure_py(excinfo)\n    assert \"\u20ac\" in str(representation)\n"], "sample_705": ["def test_pytester_parse_summary_nouns_no_summary(pytester: Pytester) -> None:\n    lines = [\n        \"some output 1\",\n        \"some output 2\",\n        \"done.\",\n    ]\n    with pytest.raises(ValueError, match=\"Pytest terminal summary report not found\"):\n        pytester_mod.RunResult.parse_summary_nouns(lines)\n", "def test_pytester_parse_summary_nouns(pytester: Pytester) -> None:\n    lines = [\n        \"some output 1\",\n        \"some output 2\",\n        \"======= 1 failed, 1 passed, 1 warning, 1 error in 0.13s ====\",\n        \"done.\",\n    ]\n    assert pytester_mod.RunResult.parse_summary_nouns(lines) == {\n        \"errors\": 1,\n        \"failed\": 1,\n        \"passed\": 1,\n        \"warnings\": 1,\n    }\n\n    lines = [\n        \"some output 1\",\n        \"some output 2\",\n        \"======= 1 failed, 1 passed, 2 warnings, 2 errors in 0.13s ====\",\n        \"done.\",\n    ]\n    assert pytester_mod.RunResult.parse_summary_nouns(lines) == {\n        \"errors\": 2,\n        \"failed\": 1,\n        \"passed\": 1,\n        \"warnings\": 2,\n    }\n    # Test case for no summary line found\n    lines = [\"some output 1\", \"some output 2\"]\n    with pytest.raises(ValueError):\n        pytester_mod.RunResult.parse_summary_nouns(lines)\n", "def test_pytester_makeconftest(pytester: Pytester) -> None:\n    pytester.makeconftest(\"print('Hello, World!')\")\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"Hello, World!*\"])\n", "def test_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n\n            time.sleep(2)\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(timeout=1)\n    assert result.ret == ExitCode.INTERRUPTED\n    assert \"KeyboardInterrupt\" in str(result.stdout)\n", "def test_pytester_run_with_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n\n            time.sleep(10)\n        \"\"\"\n    )\n    result = pytester.runpytest(timeout=1)\n    assert result.ret == ExitCode.INTERRUPTED\n    assert \"Timeout expired after 1 seconds\" in result.stderr.str()\n"], "sample_706": ["def test_matcher_adapter() -> None:\n    matcher = {\"foo\": True, \"bar\": False}.__getitem__\n\n    adapter = MatcherAdapter(matcher)\n\n    assert adapter[\"$foo\"]\n    assert not adapter[\"$bar\"]\n    with pytest.raises(KeyError):\n        adapter[\"baz\"]\n\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n\n    with pytest.raises(NotImplementedError):\n        iter(adapter)\n", "def test_expression_compile_once() -> None:\n    expr = Expression.compile(\"true\")\n    assert expr.evaluate({\"true\": True}.__getitem__)\n    assert expr.evaluate({\"true\": False}.__getitem__) is False\n    assert expr.evaluate({\"true\": True}.__getitem__)\n", "def test_matcher_adapter() -> None:\n    matcher = {\"foo\": True, \"bar\": False}.__getitem__\n\n    adapter = MatcherAdapter(matcher)\n\n    assert adapter[\"$foo\"]\n    assert not adapter[\"$bar\"]\n\n    with pytest.raises(KeyError):\n        adapter[\"$baz\"]\n\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n\n    with pytest.raises(NotImplementedError):\n        iter(adapter)\n", "def test_expression_compile_error_handling() -> None:\n    with pytest.raises(ParseError):\n        Expression.compile(\"(\")\n    with pytest.raises(ParseError):\n        Expression.compile(\")\")\n    with pytest.raises(ParseError):\n        Expression.compile(\"not\")\n    with pytest.raises(ParseError):\n        Expression.compile(\"and\")\n    with pytest.raises(ParseError):\n        Expression.compile(\"or\")\n", "def test_matcher_adapter() -> None:\n    matcher = {\"foo\": True, \"bar\": False}.__getitem__\n    adapter = Expression.MatcherAdapter(matcher)\n    assert adapter[\"$foo\"]\n    assert not adapter[\"$bar\"]\n    with pytest.raises(KeyError):\n        adapter[\"$baz\"]\n"], "sample_707": ["def test_get_fslocation_from_item() -> None:\n    class FakeNode:\n            self.location = location\n            self.obj = obj\n            self.fspath = fspath\n\n    node = FakeNode(location=(\"path/to/file.py\", 10))\n    assert nodes.get_fslocation_from_item(node) == (\"path/to/file.py\", 10)\n\n    node = FakeNode(obj=\"some_object\")\n    with pytest.raises(TypeError, match=\".* is not a code object\"):\n        nodes.get_fslocation_from_item(node)\n\n    node = FakeNode(fspath=\"path/to/file.py\")\n    assert nodes.get_fslocation_from_item(node) == (\"path/to/file.py\", -1)\n", "def test_get_fslocation_from_item() -> None:\n    \"\"\"Test that get_fslocation_from_item works correctly for different node types.\"\"\"\n    class FakeNode(nodes.Node):\n            super().__init__(\"fake_node\", parent=None)\n            self.location = location\n            self.obj = obj\n            self.fspath = fspath\n\n    # Test with location attribute\n    node = FakeNode(location=(\"path/to/location\", 10))\n    assert nodes.get_fslocation_from_item(node) == (\"path/to/location\", 10)\n\n    # Test with obj attribute\n    obj = object()\n    node = FakeNode(obj=obj)\n    assert nodes.get_fslocation_from_item(node) == (None, -1)  # Since getfslineno returns None\n\n    # Test with fspath attribute\n    node = FakeNode(fspath=\"path/to/fspath\")\n    assert nodes.get_fslocation_from_item(node) == (\"path/to/fspath\", -1)\n", "def test_node_repr_failure_when_fulltrace(pytester: Pytester) -> None:\n    \"\"\"Test that Node._repr_failure respects the --full-trace option.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--full-trace\")\n    result.stdout.fnmatch_lines([\"*test_repr_failure*\", \"*1 failed in *\"])\n    assert \"short\" not in result.stdout.str()\n", "def test_get_fslocation_from_item() -> None:\n    \"\"\"Test that get_fslocation_from_item returns the correct location.\"\"\"\n    node = nodes.Node.from_parent(None, name=\"test_node\", nodeid=\"node_id\")\n    node.location = (\"path/to/node\", 10, \"node_name\")\n\n    location = nodes.get_fslocation_from_item(node)\n    assert location == (\"path/to/node\", 10)\n\n    node.location = None\n    node.obj = \"path/to/obj\"\n    location = nodes.get_fslocation_from_item(node)\n    assert location == (\"path/to/obj\", -1)\n\n    node.obj = None\n    node.fspath = \"path/to/fspath\"\n    location = nodes.get_fslocation_from_item(node)\n    assert location == (\"path/to/fspath\", -1)\n", "def test_node_repr_failure(pytester: Pytester) -> None:\n    \"\"\"Test that node._repr_failure_py handles different exception types.\"\"\"\n    items = pytester.getitems(\n        \"\"\"\n            import pytest\n            pytest.fail(\"pytest fail\")\n\n            import warnings\n            warnings.warn(UserWarning(\"user warning\"))\n\n            raise RuntimeError(\"runtime error\")\n    \"\"\"\n    )\n    for item in items:\n        try:\n            item.runtest()\n        except Exception as e:\n            excinfo = pytest.ExceptionInfo.from_exc_info((type(e), e, e.__traceback__))\n            repr_failure = item._repr_failure_py(excinfo)\n            assert isinstance(repr_failure, pytest.TerminalRepr)\n"], "sample_708": ["def test_getstatementrange_with_nested_blocks() -> None:\n    source = Source(\n        \"\"\"\\\n            if x > 0:\n                try:\n                    y = 1 / x\n                except ZeroDivisionError:\n                    pass\n            else:\n                y = 0\n        \"\"\"\n    )\n    assert len(source) == 8\n    assert source.getstatementrange(2) == (2, 6)\n    assert source.getstatementrange(4) == (4, 5)\n    assert source.getstatementrange(6) == (6, 7)\n", "def test_getstatement_with_empty_lines() -> None:\n    source = \"\"\"\\", "def test_getstatementrange_ast_with_nested_try_except() -> None:\n    source = Source(\n        \"\"\"\\\n        try:\n            try:\n                x = 1\n            except ValueError:\n                pass\n        except Exception:\n            pass\n        \"\"\"\n    )\n    ast, start, end = getstatementrange_ast(2, source)\n    assert start == 2\n    assert end == 4\n", "def test_getstatement_with_triple_quotes_inside_string() -> None:\n    source = getstatement(0, 'assert \"hello \\\\\"\"\" != \\'hello \\\\\\'\\'\\'')\n    assert str(source) == 'assert \"hello \\\\\"\"\" != \\'hello \\\\\\'\\'\\''\n", "def test_getstatementrange_ast_with_nested_blocks() -> None:\n    source = Source(\n        \"\"\"\\"], "sample_709": ["def test_pytester_parseconfig_with_plugin(pytester: Pytester) -> None:\n    plugin1 = pytester.makepyfile(\n        **{\n            \"plugin1\": \"\"\"\n                assert config.option.verbose == 2\n            \"\"\"\n        }\n    )\n    pytester.syspathinsert()\n    import plugin1  # noqa\n\n    pytester.plugins = [\"plugin1\"]\n    config = pytester.parseconfig(\"-vv\")\n    config._do_configure()\n", "def test_pytester_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n\n            time.sleep(10)\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess(timeout=1)\n    assert isinstance(result, pytester_mod.RunResult)\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_pytester_copy_example(pytester: Pytester, tmp_path_factory: TempPathFactory) -> None:\n    example_dir = tmp_path_factory.mktemp(\"example\")\n    example_file = example_dir / \"example.py\"\n    example_file.write_text(\"def test_example(): pass\")\n\n    pytester._request.config._inicache[\"pytester_example_dir\"] = str(example_dir)\n\n    pytester.copy_example()\n    assert (pytester.path / \"example.py\").is_file()\n\n    pytester.copy_example(\"example.py\")\n    assert (pytester.path / \"example.py\").is_file()\n", "def test_pytester_run_with_stdin(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import sys\n\n            assert sys.stdin.read() == 'input'\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess(stdin=b\"input\")\n    assert result.ret == 0\n", "def test_pytester_getmodulecol_withinit(pytester: Pytester) -> None:\n    \"\"\"Test pytester.getmodulecol() with the `withinit` parameter.\"\"\"\n    pytester.makepyfile(__init__=\"\")\n    modcol = pytester.getmodulecol(\"\")\n    assert modcol.parent.parent.name == \"__init__.py\"\n    modcol = pytester.getmodulecol(\"\", withinit=False)\n    assert modcol.parent.parent.name != \"__init__.py\"\n    modcol = pytester.getmodulecol(\"\")\n    assert modcol.parent.parent.name != \"__init__.py\"\n"], "sample_710": ["def test_unittest_subtest(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                for i in range(5):\n                    with self.subTest(i=i):\n                        if i == 3:\n                            assert False\n                        else:\n                            assert True\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 4\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytest.importorskip(\"unittest\")\n\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestExample(unittest.TestCase):\n\n                for i in range(5):\n                    with self.subTest(i=i):\n                        self.assertEqual(i % 2, 0)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                for i in range(5):\n                    with self.subTest(i=i):\n                        if i == 3:\n                            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*MyTestCase.test_subtest[i=0]*\",\n            \"*MyTestCase.test_subtest[i=1]*\",\n            \"*MyTestCase.test_subtest[i=2]*\",\n            \"*MyTestCase.test_subtest[i=3]*\",\n            \"*MyTestCase.test_subtest[i=4]*\",\n            \"*1 failed, 4 passed*\",\n        ]\n    )\n", "def test_addCleanup_order(pytester: Pytester) -> None:\n    \"\"\"Ensure addCleanup calls are executed in the correct order (#6947)\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        cleanups = []\n\n        class Test(unittest.TestCase):\n\n                self.addCleanup(cleanups.append, \"first\")\n                self.addCleanup(cleanups.append, \"second\")\n\n            assert cleanups == [\"second\", \"first\"]\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*::test_func PASSED *\",\n            \"*::test_check_cleanups PASSED *\",\n        ]\n    )\n", "def test_unittest_TestCase_with___call__(pytester: Pytester) -> None:\n    \"\"\"Test that a unittest.TestCase subclass with a __call__ method is collected and run correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n                self.assertEqual(1, 1)\n                self.assertEqual(1, 1)\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=2)\n"], "sample_711": ["def test_add_marker_append_prepend(pytester: Pytester) -> None:\n    \"\"\"Test that markers can be added to a node with append and prepend.\"\"\"\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    marker1 = pytest.mark.skip(reason=\"skip reason\")\n    marker2 = pytest.mark.xfail(reason=\"xfail reason\")\n\n    item.add_marker(marker1, append=True)\n    item.add_marker(marker2, append=False)\n\n    assert list(item.iter_markers()) == [marker2, marker1]\n", "def test_node_repr_failure_with_style_override(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    with pytest.raises(AssertionError):\n        items[0].repr_failure(excinfo=pytest.raises(Exception).excinfo, style=\"short\")\n", "def test_node_repr_failure_with_changed_cwd(pytester: Pytester) -> None:\n    \"\"\"\n    Test failure lines should use absolute paths if cwd has changed since\n    invocation, so the path is correct (#6428).\n    \"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import os\n        import pytest\n\n        @pytest.fixture\n            out_dir = 'ddd'\n            os.mkdir(out_dir)\n            old_dir = os.getcwd()\n            os.chdir(out_dir)\n            yield out_dir\n            os.chdir(old_dir)\n\n            raise Exception(\"Something went wrong\")\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([str(p) + \":*: Exception\", \"*1 failed in *\"])\n", "def test_node_listchain() -> None:\n    \"\"\"Test that listchain returns all parent collectors up to self.\"\"\"\n    class DummyNode(nodes.Node):\n            super().__init__(name, parent=parent)\n\n    grandparent = DummyNode(\"grandparent\")\n    parent = DummyNode(\"parent\", parent=grandparent)\n    child = DummyNode(\"child\", parent=parent)\n\n    chain = child.listchain()\n    assert len(chain) == 3\n    assert chain[0] is child\n    assert chain[1] is parent\n    assert chain[2] is grandparent\n", "def test_node_repr_failure_with_changed_cwd(pytester: Pytester) -> None:\n    \"\"\"\n    Test failure lines should use absolute paths if cwd has changed since\n    invocation, so the path is correct (#6428).\n    \"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import os\n        import pytest\n\n        @pytest.fixture\n            out_dir = 'ddd'\n            os.mkdir(out_dir)\n            old_dir = os.getcwd()\n            os.chdir(out_dir)\n            yield out_dir\n            os.chdir(old_dir)\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--tb=short\")\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\"])\n    assert \"ddd\" not in result.stdout.str()\n"], "sample_712": ["def test_ordinal_encoder_set_params():\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[[0, 1, 2, 3]])\n    assert enc.get_params()['categories'] == [[0, 1, 2, 3]]\n    assert enc.fit_transform(X).shape == (2, 1)\n    # set params on already fitted object\n    enc.set_params(categories=[[0, 1, 2, 3, 4]])\n    assert enc.fit_transform(X).shape == (2, 1)\n", "def test_ordinal_encoder_handle_missing():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X)\n    X_test = [[np.nan, 2, 55], ['def', np.nan, 55]]\n    X_tr_test = enc.transform(X_test)\n    assert_array_equal(X_tr_test, [[-1, 1, 0], [1, -1, 0]])\n", "def test_ordinal_encoder_categories():\n    # Test that categories parameter is handled correctly\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder(categories=[['abc', 'def'], [2, 1], [55]])\n\n    exp = np.array([[0, 0, 0],\n                    [1, 1, 0]], dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp.astype('float64'))\n\n    # Check that incorrect categories raises an error\n    enc = OrdinalEncoder(categories=[['abc', 'de'], [2, 1], [55]])\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit(X)\n\n    # Check that categories are preserved when fitting another time\n    enc = OrdinalEncoder(categories=[['abc', 'def'], [2, 1], [55]])\n    enc.fit(X)\n    assert_array_equal(enc.categories_, [['abc', 'def'], [2, 1], [55]])\n    enc.fit(X)\n    assert_array_equal(enc.categories_, [['abc', 'def'], [2, 1], [55]])\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 'b'], ['c', 'd']])\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X)\n\n    # test that an error is raised when an unknown category is encountered\n    X_test = np.array([['e', 'f']])\n    msg = \"Found unknown categories\"\n    assert_raises_regex(ValueError, msg, enc.transform, X_test)\n\n    # test that handle_unknown='ignore' correctly ignores unknown categories\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n    X_test_tr = enc.transform(X_test)\n    assert_array_equal(X_test_tr, [[np.nan, np.nan]])\n", "def test_ordinal_encoder_categories():\n    # Test that categories are returned correctly\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    assert len(enc.categories_) == 3\n    assert_array_equal(enc.categories_[0], ['abc', 'def'])\n    assert_array_equal(enc.categories_[1], [1, 2])\n    assert_array_equal(enc.categories_[2], [55])\n"], "sample_713": ["def test_ridgecv_saga():\n    # Test that RidgeCV works with 'saga' solver\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n    ridge_cv = RidgeCV(solver='saga')\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'coef_')\n    assert hasattr(ridge_cv, 'intercept_')\n    assert hasattr(ridge_cv, 'alpha_')\n", "def test_ridge_saga_solver_convergence():\n    # Test that saga solver converges to the same solution as other solvers.\n    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1,\n                           random_state=42)\n\n    ridge_svd = Ridge(solver='svd', tol=1e-12)\n    ridge_svd.fit(X, y)\n\n    ridge_saga = Ridge(solver='saga', max_iter=10000, tol=1e-12)\n    ridge_saga.fit(X, y)\n\n    assert_array_almost_equal(ridge_svd.coef_, ridge_saga.coef_)\n    assert_almost_equal(ridge_svd.intercept_, ridge_saga.intercept_)\n", "def test_ridge_saga_solver_convergence():\n    # Test that the SAGA solver converges to the same solution as the other\n    # solvers for a multi-target problem.\n\n    X, y = make_regression(n_samples=1000, n_features=10, n_targets=5,\n                           random_state=42)\n\n    # Reference solution using the Cholesky solver\n    ridge_chol = Ridge(alpha=1.0, solver='cholesky', fit_intercept=False)\n    ridge_chol.fit(X, y)\n\n    # Solution using the SAGA solver with a large number of iterations\n    ridge_saga = Ridge(alpha=1.0, solver='saga', max_iter=10000, tol=1e-10,\n                       fit_intercept=False)\n    ridge_saga.fit(X, y)\n\n    assert_array_almost_equal(ridge_saga.coef_, ridge_chol.coef_)\n", "def test_ridgecv_non_uniform_sample_weights():\n    # Test that RidgeCV can handle non-uniform sample weights.\n    rng = np.random.RandomState(0)\n    X, y = make_regression(n_samples=100, n_features=10, random_state=rng)\n    sample_weight = rng.rand(100)\n\n    # Compare the result of fitting on the whole dataset with individual\n    # sample weights and fitting each sample repeated according to its weight.\n    ridge = RidgeCV()\n    ridge.fit(X, y, sample_weight=sample_weight)\n\n    X_repeated = np.repeat(X, (sample_weight * 10).astype(int), axis=0)\n    y_repeated = np.repeat(y, (sample_weight * 10).astype(int))\n    ridge_repeated = RidgeCV()\n    ridge_repeated.fit(X_repeated, y_repeated)\n\n    assert_almost_equal(ridge.coef_, ridge_repeated.coef_)\n    assert_almost_equal(ridge.intercept_, ridge_repeated.intercept_)\n", "def test_saga_solver_convergence():\n    # Test that the SAGA solver converges to the same solution as the other\n    # solvers for a given dataset.\n\n    X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n    alpha = 1.0\n\n    # Reference solution using svd solver\n    ridge_svd = Ridge(alpha=alpha, solver='svd', fit_intercept=True,\n                      max_iter=None, tol=1e-8)\n    ridge_svd.fit(X, y)\n\n    # Solution with saga solver\n    ridge_saga = Ridge(alpha=alpha, solver='saga', fit_intercept=True,\n                       max_iter=10000, tol=1e-8)\n    ridge_saga.fit(X, y)\n\n    assert_array_almost_equal(ridge_svd.coef_, ridge_saga.coef_)\n    assert_almost_equal(ridge_svd.intercept_, ridge_saga.intercept_)\n"], "sample_714": ["def test_log_loss_sample_weight():\n    # binary case with symbolic labels (\"no\" < \"yes\")\n    y_true = [\"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\"]\n    y_pred = np.array([[0.5, 0.5], [0.1, 0.9], [0.01, 0.99],\n                       [0.9, 0.1], [0.75, 0.25], [0.001, 0.999]])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.1, 0.2, 0.1])\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(loss, 1.653318)\n\n    # multiclass case; adapted from http://bit.ly/RJJHWA\n    y_true = [1, 0, 2]\n    y_pred = [[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]]\n    sample_weight = np.array([0.5, 0.3, 0.2])\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(loss, 0.786184)\n\n    # check that we got all the shapes and axes right\n    # by doubling the length of y_true and y_pred\n    y_true *= 2\n    y_pred *= 2\n    sample_weight *= 2\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight, normalize=False)\n    assert_almost_equal(loss, 0.786184 * 6, decimal=6)\n", "def test_jaccard_similarity_score_binary_data_non_binary_average():\n    # Error if user does not explicitly set non-binary average mode\n    y_true_bin = [0, 1, 1, 0]\n    y_pred_bin = [0, 1, 0, 1]\n    assert_raise_message(ValueError,\n                         \"Target is binary but average='binary'. Please \"\n                         \"choose another average setting.\",\n                         jaccard_similarity_score, y_true_bin, y_pred_bin)\n", "def test_balanced_accuracy_score():\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 0, 1]\n\n    # true negatives = 3, false positives = 1\n    specificity = 3 / (3 + 1)\n    # true positives = 1, false negatives = 1\n    recall = 1 / (1 + 1)\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred),\n                        (specificity + recall) / 2)\n", "def test_brier_score_loss_binary_averaged():\n    # Test brier_score_loss with binary averaged case\n    y_true = np.array([0, 1, 0, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n\n    score = brier_score_loss(y_true, y_pred, pos_label=1)\n    assert_almost_equal(score, 0.14222222)\n\n    score = brier_score_loss(y_true, y_pred, pos_label=0)\n    assert_almost_equal(score, 0.63777778)\n\n    # Test averaging\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.1, 0.8, 0.1], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7],\n                       [0.3, 0.5, 0.2], [0.4, 0.6, 0.0], [0.1, 0.3, 0.6]])\n\n    score = brier_score_loss(y_true, y_pred)\n    assert_almost_equal(score, 0.44916667)\n", "def test_brier_score_loss_multiclass():\n    # Check brier_score_loss function for multiclass case\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.9, 0.05, 0.05], [0.05, 0.9, 0.05],\n                       [0.05, 0.05, 0.9], [0.8, 0.1, 0.1],\n                       [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]])\n    true_score = np.mean(np.sum((y_pred - np.eye(3)[y_true]) ** 2,\n                                axis=1)) / 2\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n"], "sample_715": ["def test_cross_validate_return_estimator():\n    # Test that cross_validate returns the estimator when return_estimator is True\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    scores = cross_validate(clf, X, y, return_estimator=True)\n    assert 'estimator' in scores\n    assert isinstance(scores['estimator'], list)\n    assert len(scores['estimator']) == scores['test_score'].shape[0]\n    for est in scores['estimator']:\n        assert isinstance(est, MockClassifier)\n", "def test_cross_val_score_warn_on_scores():\n    # test that cross_val_score will warn when scores are not\n    # of the correct type or range.\n    X, y = make_classification(n_samples=50, random_state=0)\n\n    # create an estimator that returns a non-float score\n    class NonFloatEstimator(BaseEstimator):\n            pass\n\n            return \"this is not a float\"\n\n    clf = NonFloatEstimator()\n    assert_warns_message(RuntimeWarning,\n                         \"scoring must return a number, got 'this is not a float' \"\n                         \"(scorer=accuracy) for sample 0\",\n                         cross_val_score, clf, X, y, cv=5)\n\n    # create an estimator that returns a score outside [0,1] range\n    class InvalidRangeEstimator(BaseEstimator):\n            pass\n\n            return -1.0\n\n    clf = InvalidRangeEstimator()\n    assert_warns_message(RuntimeWarning,\n                         \"Estimator score method should return values within the range [0, 1]\",\n                         cross_val_score, clf, X, y, cv=5)\n", "def test_cross_val_score_with_string_target():\n    # Test that cross_val_score works with string targets\n    iris = load_iris()\n    X, y = iris.data, iris.target_names[iris.target]\n    clf = SVC(kernel='linear')\n    scores = cross_val_score(clf, X, y, cv=5)\n    assert_array_almost_equal(scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n", "def test_cross_val_score_with_object_dtype():\n    # check cross_val_score works with object dtype\n    X = np.array([['a', 'b'], ['c', 'd'], ['e', 'f']])\n    y = np.array([0, 1, 2])\n    clf = MockClassifier()\n    scores = cross_val_score(clf, X, y)\n    assert_array_equal(scores, [1.0, 1.0, 1.0])\n", "def test_cross_validate_dict_scoring_and_return_train_score_warn():\n    # Test that warnings are raised. Will be removed in 0.21\n\n    X, y = make_classification(random_state=0)\n    estimator = MockClassifier()\n\n    result = {}\n    scoring = {'score': 'accuracy'}\n    for val in [False, True, 'warn']:\n        result[val] = assert_no_warnings(cross_validate, estimator, X, y,\n                                         scoring=scoring,\n                                         return_train_score=val)\n\n    msg = (\n        'You are accessing a training score ({!r}), '\n        'which will not be available by default '\n        'any more in 0.21. If you need training scores, '\n        'please set return_train_score=True').format('train_score')\n    train_score = assert_warns_message(FutureWarning, msg,\n                                       result['warn'].get, 'train_score')\n    assert np.allclose(train_score, result[True]['train_score'])\n    assert 'train_score' not in result[False]\n"], "sample_716": ["def test_ridge_regression_sparse_float32():\n    # Test that ridge regression with sparse data and float32 works\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]), dtype=np.float32)\n    y = np.array([5, 6], dtype=np.float32)\n\n    ridge = Ridge(alpha=1.0, solver='sparse_cg')\n    ridge.fit(X, y)\n\n    assert_equal(ridge.coef_.dtype, np.float32)\n    assert_equal(ridge.intercept_.dtype, np.float32)\n    assert_equal(ridge.predict(X).dtype, np.float32)\n", "def test_ridge_intercept_dtype():\n    # Test that the intercept is of the correct dtype, even when X is not\n    # of the same type as y (e.g. X is float32 and y is int64).\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 5).astype(np.float32)\n    y = rng.randint(0, 2, size=10).astype(np.int64)\n\n    ridge = Ridge()\n    ridge.fit(X, y)\n\n    assert ridge.intercept_.dtype == y.dtype\n", "def test_ridgecv_scorer():\n    # Test that RidgeCV can handle a custom scorer\n    X, y = make_regression(n_samples=100, n_features=10)\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n\n    # With a scorer\n    ridgecv = RidgeCV(scorer=scorer)\n    ridgecv.fit(X, y)\n\n    # Without a scorer\n    ridgecv_no_scorer = RidgeCV()\n    ridgecv_no_scorer.fit(X, y)\n\n    assert_array_almost_equal(ridgecv.coef_, ridgecv_no_scorer.coef_)\n", "def test_ridge_multitarget_dtype_match():\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features, n_targets = 6, 5, 2\n    X_64 = rng.randn(n_samples, n_features)\n    y_64 = rng.randn(n_samples, n_targets)\n    X_32 = X_64.astype(np.float32)\n    y_32 = y_64.astype(np.float32)\n\n    solvers = [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\"]\n    for solver in solvers:\n\n        # Check type consistency 32bits\n        ridge_32 = Ridge(alpha=alpha, solver=solver)\n        ridge_32.fit(X_32, y_32)\n        coef_32 = ridge_32.coef_\n\n        # Check type consistency 64 bits\n        ridge_64 = Ridge(alpha=alpha, solver=solver)\n        ridge_64.fit(X_64, y_64)\n        coef_64 = ridge_64.coef_\n\n        # Do the actual checks at once for easier debug\n        assert coef_32.dtype == X_32.dtype\n        assert coef_64.dtype == X_64.dtype\n        assert ridge_32.predict(X_32).dtype == X_32.dtype\n        assert ridge_64.predict(X_64).dtype == X_64.dtype\n        assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)\n", "def test_ridgecv_with_scorer():\n    # Test RidgeCV with a custom scorer\n    X, y = make_regression(n_samples=100, n_features=10)\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n    ridgecv = RidgeCV(scoring=scorer)\n    ridgecv.fit(X, y)\n    assert hasattr(ridgecv, 'best_score_')\n"], "sample_717": ["def test_load_lfw_pairs_subset():\n    # Test that the subset parameter is correctly handled\n    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                      subset='train',\n                                      download_if_missing=False)\n\n    lfw_pairs_test = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                     subset='test',\n                                     download_if_missing=False)\n\n    lfw_pairs_10_folds = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                         subset='10_folds',\n                                         download_if_missing=False)\n\n    assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs_test.pairs.shape, (0, 2, 62, 47))  # Fake data has no pairsDevTest.txt\n    assert_equal(lfw_pairs_10_folds.pairs.shape, (0, 2, 62, 47))  # Fake data has no pairs.txt\n\n    # Check that an invalid subset raises a ValueError\n    assert_raises(ValueError, fetch_lfw_pairs, data_home=SCIKIT_LEARN_DATA,\n                  subset='invalid_subset', download_if_missing=False)\n", "def test_load_fake_lfw_pairs_subset():\n    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                      subset='train', download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 62, 47))\n\n    # the target is whether the person is the same or not\n    assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # names of the persons can be found using the target_names array\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs_train.target_names, expected_classes)\n\n    # Test with subset='test'\n    lfw_pairs_test = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                     subset='test', download_if_missing=False)\n    assert_equal(lfw_pairs_test.pairs.shape, (0, 2, 62, 47))  # no test pairs\n\n    # Test with subset='10_folds'\n    lfw_pairs_10_folds = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                         subset='10_folds', download_if_missing=False)\n    assert_equal(lfw_pairs_10_folds.pairs.shape, (0, 2, 62, 47))  # no pairs for 10 folds\n\n    # Test with invalid subset\n    assert_raises(ValueError, fetch_lfw_pairs, data_home=SCIKIT_LEARN_DATA,\n                  subset='invalid_subset', download_if_missing=False)\n", "def test_load_fake_lfw_pairs_too_restrictive():\n    assert_raises(ValueError, fetch_lfw_pairs, subset='invalid_subset',\n                  data_home=SCIKIT_LEARN_DATA, download_if_missing=False)\n", "def test_load_fake_lfw_pairs_subset():\n    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                      subset='train',\n                                      download_if_missing=False)\n\n    assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 62, 47))\n    assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # test that other subsets raise an error for fake data\n    assert_raises(IOError, fetch_lfw_pairs, data_home=SCIKIT_LEARN_DATA,\n                  subset='test', download_if_missing=False)\n    assert_raises(IOError, fetch_lfw_pairs, data_home=SCIKIT_LEARN_DATA,\n                  subset='10_folds', download_if_missing=False)\n", "def test_load_lfw_pairs_subset():\n    for subset in ['train', 'test', '10_folds']:\n        lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                    subset=subset, download_if_missing=False)\n        assert lfw_pairs.pairs.shape[0] > 0\n        assert len(lfw_pairs.target) == lfw_pairs.pairs.shape[0]\n        assert len(lfw_pairs.target_names) == 2\n\n    # test that an invalid subset raises an error\n    assert_raises(ValueError, fetch_lfw_pairs, data_home=SCIKIT_LEARN_DATA,\n                  subset='invalid_subset', download_if_missing=False)\n"], "sample_718": ["def test_check_estimator_with_correct_n_iter():\n    # check that n_iter is correctly checked for estimators\n    # with a parameter max_iter, and is at least 1\n\n    class NonConformantEstimatorNoNIter(object):\n            self.max_iter = 100\n\n            pass\n\n    class NonConformantEstimatorZeroNIter(object):\n            self.max_iter = 100\n            self.n_iter_ = 0\n\n            pass\n\n    assert_raises_regex(AssertionError,\n                        \"Estimator estimator_name should have an attribute \"\n                        \"n_iter_ after fitting.\",\n                        check_non_transformer_estimators_n_iter,\n                        'estimator_name',\n                        NonConformantEstimatorNoNIter())\n    assert_raises_regex(AssertionError,\n                        \"n_iter_ should be greater than or equal to 1.\",\n                        check_non_transformer_estimators_n_iter,\n                        'estimator_name',\n                        NonConformantEstimatorZeroNIter())\n", "def test_check_estimator_sparse_data():\n    # check that estimators raise an exception on providing sparse data\n    X = np.random.sample(10) + 1j * np.random.sample(10)\n    X = X.reshape(-1, 1)\n    y = np.random.sample(10) + 1j * np.random.sample(10)\n    estimator = LinearRegression()\n    assert_raises_regex(ValueError, \"Complex data not supported\",\n                        check_estimator_sparse_data, 'estimator_name', estimator)\n", "def test_check_estimators_empty_data_messages():\n    # check that estimators raise an informative error message when fitted on empty data\n\n    # Estimators are expected to raise a ValueError with an appropriate error message\n    # when fitted on empty data. We're testing this for each estimator using the\n    # check_estimators_empty_data_messages function.\n\n    # We'll only test a subset of estimators here, but in practice you should test\n    # all estimators.\n    estimators = [\n        LinearRegression(),\n        SVC(),\n        RandomForestClassifier(),\n        NMF(),\n    ]\n\n    for estimator in estimators:\n        msg = \"Estimator doesn't raise an informative error message when fitted on empty data\"\n        assert_raises_regex(AssertionError, msg,\n                            check_estimators_empty_data_messages,\n                            type(estimator).__name__, estimator)\n", "def test_check_estimator_sparse_data():\n    # check that estimator can handle sparse data correctly\n\n    class SparseDataErrorEstimator(BaseEstimator):\n            if sp.issparse(X):\n                raise ValueError(\"Sparse data not supported\")\n            return self\n\n            if sp.issparse(X):\n                raise ValueError(\"Sparse data not supported\")\n            return np.ones(X.shape[0])\n\n    msg = (\"Estimator SparseDataErrorEstimator doesn't seem to fail \"\n           \"gracefully on sparse data: it should raise a TypeError when \"\n           \"sparse input is explicitly not supported.\")\n    assert_raises_regex(AssertionError, msg, check_estimator,\n                        SparseDataErrorEstimator())\n", "def test_check_estimator_sparse_data():\n    # check that estimators can handle sparse data correctly\n    from sklearn.datasets import load_iris\n    iris = load_iris()\n\n    for Estimator in [GaussianMixture, LinearRegression,\n                      RandomForestClassifier, NMF, SGDClassifier,\n                      MiniBatchKMeans]:\n        with ignore_warnings(category=FutureWarning):\n            est = Estimator()\n        set_checking_parameters(est)\n        set_random_state(est)\n\n        X_sparse = sp.csr_matrix(iris.data)\n        y = iris.target\n\n        # check if estimator can handle sparse data\n        try:\n            est.fit(X_sparse, y)\n        except TypeError as e:\n            assert \"sparse\" in str(e), (\n                \"Estimator %s does not raise an error when fed sparse data, \"\n                \"but it should.\" % est.__class__.__name__)\n\n        # check if estimator can handle non-sparse data\n        est.fit(iris.data, y)\n"], "sample_719": ["def test_vectorizers_dtype():\n    # Test that CountVectorizer and TfidfVectorizer correctly set the dtype\n    # of their output.\n    for vec in [CountVectorizer(dtype=np.float32), \n                TfidfVectorizer(dtype=np.float32)]:\n        X = vec.fit_transform([\"hello world\", \"goodbye\"])\n        assert_equal(X.dtype, np.float32)\n", "def test_vectorizers_validate_vocabulary():\n    # vectorizers should validate vocabulary on fit and transform methods\n    for vec in [CountVectorizer(), TfidfVectorizer()]:\n        message = \"vocabulary wasn't fitted.\"\n        vec.vocabulary_ = None\n        assert_raise_message(\n            ValueError, message, vec.transform, [\"good news everyone\"])\n\n        vec.vocabulary_ = []\n        message = \"empty vocabulary; perhaps the documents only contain stop words\"\n        assert_raise_message(\n            ValueError, message, vec.transform, [\"good news everyone\"])\n", "def test_countvectorizer_invalid_max_features():\n    # Test that an invalid max_features value raises a ValueError.\n    message = (\"max_features=%r, neither a positive integer nor None\"\n               % 'invalid_value')\n    exception = ValueError\n\n        CountVectorizer(max_features='invalid_value')\n\n    assert_raise_message(exception, message, func)\n", "def test_countvectorizer_sparse_input():\n    # Test that CountVectorizer can handle sparse input.\n    # Create a sparse matrix with the same data as ALL_FOOD_DOCS\n    from scipy.sparse import csr_matrix\n    sparse_data = csr_matrix([\n        [0, 2, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 1, 0, 0, 0, 0, 0, 0],\n        [0, 2, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 1, 0, 0],\n        [0, 0, 0, 0, 0, 1, 2, 0, 0]\n    ])\n\n    # Create a CountVectorizer object and fit it to the sparse data\n    cv = CountVectorizer()\n    cv.fit(sparse_data)\n\n    # Check that the vocabulary is correct\n    expected_vocabulary = {\n        'pizza': 0, 'burger': 1, 'celeri': 2, 'coke': 3,\n        'salad': 4, 'sparkling': 5, 'tomato': 6, 'water': 7\n    }\n    assert_equal(cv.vocabulary_, expected_vocabulary)\n", "def test_tfidf_vectorizer_max_features():\n    # Test the max_features parameter of TfidfVectorizer\n    vectorizer = TfidfVectorizer(max_features=5)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[1], 5)\n\n    # Check that the vocabulary is correctly limited to the top max_features\n    assert_equal(len(vectorizer.vocabulary_), 5)\n    feature_names = vectorizer.get_feature_names()\n    assert_equal(len(feature_names), 5)\n\n    # Check that the most frequent terms are selected\n    count_vectorizer = CountVectorizer()\n    count_X = count_vectorizer.fit_transform(ALL_FOOD_DOCS)\n    counts = np.array(count_X.sum(axis=0)).ravel()\n    indices = np.argsort(counts)[::-1]\n    most_frequent_terms = [count_vectorizer.get_feature_names()[i] for i in indices[:5]]\n    assert_array_equal(most_frequent_terms, feature_names)\n"], "sample_721": ["def test_check_array_dtype_list():\n    # Test that check_array can handle a list of dtypes\n    X = np.array([[1, 2], [3, 4]])\n    X_checked = check_array(X, dtype=[np.float64, np.int64])\n    assert_equal(X_checked.dtype, np.float64)\n\n    X_checked = check_array(X, dtype=['float64', 'int64'])\n    assert_equal(X_checked.dtype, np.float64)\n\n    X_checked = check_array(X.astype(np.int32), dtype=['float64', 'int32'])\n    assert_equal(X_checked.dtype, np.int32)\n", "def test_check_X_y():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    # Test with X and y having different lengths\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1, 2])\n    assert_raises_regex(ValueError, 'inconsistent numbers of samples',\n                        check_X_y, X, y)\n\n    # Test with X not being a 2D array\n    X = np.array([1, 2, 3])\n    y = np.array([0, 1, 2])\n    assert_raises_regex(ValueError, 'Expected 2D array, got 1D array instead',\n                        check_X_y, X, y)\n\n    # Test with y not being a 1D array\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[0, 1], [2, 3]])\n    assert_raises_regex(ValueError, 'bad input shape \\(2, 2\\)',\n                        check_X_y, X, y)\n\n    # Test with multi-output\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[0, 1], [2, 3]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n", "def test_column_or_1d():\n    # Test function for column_or_1d\n    X = np.ones((3, 10))\n    X_1d = column_or_1d(X)\n    assert_equal(X_1d.shape, (10,))\n    X_2d = np.ones((3, 1))\n    X_1d = column_or_1d(X_2d)\n    assert_warns_message(DataConversionWarning, 'A column-vector y was passed',\n                         column_or_1d, X_2d, warn=True)\n    assert_equal(X_1d.shape, (3,))\n    X_2d = np.ones((1, 3))\n    assert_raise_message(ValueError, 'bad input shape', column_or_1d, X_2d)\n", "def test_check_array_dtype_object_with_nan():\n    # Test that np.nan in object arrays is correctly handled.\n    X = np.array([[1, 2], [np.nan, 4]], dtype=object)\n    with pytest.raises(ValueError):\n        check_array(X)\n\n    X_checked = check_array(X, force_all_finite='allow-nan')\n    assert_equal(X_checked.dtype, np.float64)\n    assert_array_equal(X_checked, [[1., 2.], [np.nan, 4.]])\n", "def test_check_X_y_multi_output():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[5, 6], [7, 8]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    # Check that y is not reshaped if it's a 2D array with more than one column\n    y = np.array([[5, 6, 7], [8, 9, 10]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    # Check that an error is raised if y is not a 2D array and multi_output is True\n    y = np.array([5, 6])\n    assert_raises_regex(ValueError, 'bad input shape \\(2,\\)', check_X_y, X, y,\n                        multi_output=True)\n"], "sample_722": ["def test_k_means_init_callable():\n    # This test is used to check that KMeans can be initialized with a callable\n        return np.array([[0.0, 0.0], [5.0, 5.0]])\n\n    km = KMeans(init=init, n_clusters=2, n_init=1)\n    km.fit(X)\n    assert_equal(km.cluster_centers_.shape, (2, 2))\n", "def test_k_means_init_callable():\n    # This test is used to check KMeans with a callable as init parameter.\n        return np.array([[0.0, 0.0], [5.0, 5.0], [-5.0, -5.0]])\n\n    X = np.array([[1.1, 1.1], [-7.5, -7.5], [-1.1, -1.1], [7.5, 7.5]])\n    km = KMeans(init=init, n_clusters=3, n_init=1)\n    km.fit(X)\n\n    assert_equal(km.cluster_centers_.shape, (3, 2))\n", "def test_k_means_n_init_no_convergence():\n    # test case for issue #10794: if n_init is larger than 1 and the first\n    # initialization doesn't converge, then the following initializations\n    # should still be executed.\n    X = np.array([[0, 0], [10, 10], [12, 9], [-1, 1], [2, 0], [8, 10]])\n\n    # with max_iter=1, the first initialization will not converge\n    km = KMeans(n_clusters=3, init='random', n_init=5, max_iter=1,\n                random_state=42).fit(X)\n    assert_equal(km.n_iter_, 1)\n\n    # check that the second run of k-means produces different labels\n    # hence different initialized centroids and the second run converged\n    km_2 = KMeans(n_clusters=3, init='random', n_init=1, max_iter=100,\n                  random_state=np.random.RandomState(42).randint(1))\n    km_2.fit(X)\n    assert_not_equal(km.labels_, km_2.labels_)\n    assert_greater(km.inertia_, km_2.inertia_)\n", "def test_k_means_warns_less_centers_than_unique_points():\n    # check that KMeans warns when there are less centers than unique points\n    X = np.asarray([[0, 0],\n                    [0, 1],\n                    [1, 0],\n                    [1, 0]])  # last point is duplicated\n\n    km = KMeans(n_clusters=3)\n    msg = (\"Number of distinct clusters (2) found smaller than \"\n           \"n_clusters (3). Possibly due to duplicate points in X.\")\n    assert_warns_message(ConvergenceWarning, msg, km.fit, X)\n", "def test_k_means_n_jobs():\n    # Test that KMeans gives the same results with n_jobs=1 and n_jobs>1\n    km = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n    km.fit(X)\n    labels_single = km.labels_\n\n    km = KMeans(n_clusters=n_clusters, n_init=10, random_state=42, n_jobs=-1)\n    km.fit(X)\n    labels_multi = km.labels_\n\n    assert_array_equal(labels_single, labels_multi)\n"], "sample_723": ["def test_imputation_axis1_sparse():\n    # Test imputation with axis=1 on sparse matrices.\n    X = sparse.csr_matrix(np.array([\n        [0, 0, 0, 5],\n        [0, 2, 0, 3],\n        [0, 1, 3, 0],\n        [0, 2, 3, 7],\n    ]))\n\n    X_true = np.array([\n        [2, 0, 5],\n        [2, 3, 3],\n        [1, 3, 3],\n        [2, 3, 7],\n    ])\n\n    imputer = SimpleImputer(missing_values=0, strategy=\"most_frequent\", axis=1)\n    Xt = imputer.fit_transform(X)\n\n    assert_array_almost_equal(Xt, X_true)\n", "def test_imputation_axis1_sparse():\n    # Test imputation with axis=1 on sparse matrix\n    X = sparse.csr_matrix(np.array([\n        [1, 2, np.nan],\n        [np.nan, 3, 4],\n        [5, np.nan, 6]\n    ]))\n\n    imputer = SimpleImputer(strategy=\"mean\", axis=1)\n    Xt = imputer.fit_transform(X)\n\n    assert_array_almost_equal(Xt.toarray(), np.array([\n        [1, 2, (1+2)/2],\n        [(3+4)/2, 3, 4],\n        [5, (5+6)/2, 6]\n    ]))\n", "def test_imputation_dtype_preservation():\n    # Test that the imputer preserves the data type of the input array.\n    X_int = np.array([[1, 2], [np.nan, 3]], dtype='int32')\n    X_float = np.array([[1, 2], [np.nan, 3]], dtype='float64')\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy)\n\n        X_int_imputed = imputer.fit_transform(X_int)\n        assert_equal(X_int_imputed.dtype, X_int.dtype)\n\n        X_float_imputed = imputer.fit_transform(X_float)\n        assert_equal(X_float_imputed.dtype, X_float.dtype)\n", "def test_imputation_sparse_most_frequent():\n    # Test imputation using the most-frequent strategy on sparse data.\n    X = sparse.csc_matrix(np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ]))\n\n    X_true = np.array([\n        [2, 0, 5],\n        [2, 3, 3],\n        [1, 3, 3],\n        [2, 3, 7],\n    ])\n\n    _check_statistics(X, X_true, \"most_frequent\", [np.nan, 2, 3, 3], -1)\n", "def test_imputation_with_nans():\n    # Test imputation with NaNs present in the data.\n    X = np.array([\n        [np.nan, 2, 3],\n        [4, np.nan, 6],\n        [7, 8, np.nan]\n    ])\n\n    imputer = SimpleImputer(strategy=\"mean\")\n    X_imputed = imputer.fit_transform(X)\n\n    assert_array_almost_equal(imputer.statistics_, np.array([5.5, 5., 4.5]))\n    assert_array_almost_equal(X_imputed, np.array([\n        [5.5, 2, 3],\n        [4, 5., 6],\n        [7, 8, 4.5]\n    ]))\n"], "sample_724": ["def test_imputation_axis():\n    # Test imputation with different axis.\n    X = np.array([\n        [np.nan, 2, 5],\n        [1, np.nan, 11],\n        [2, 4, np.nan],\n        [4, 6, 12],\n        [np.nan, 8, 13]\n    ])\n\n    X_imputed_mean_axis0 = np.array([\n        [2, 2, 5],\n        [1, 5, 11],\n        [2, 4, 10],\n        [4, 6, 12],\n        [2, 8, 13]\n    ])\n\n    X_imputed_median_axis0 = np.array([\n        [2, 2, 5],\n        [1, 5, 11],\n        [2, 4, 11.5],\n        [4, 6, 12],\n        [2, 8, 13]\n    ])\n\n    X_imputed_mean_axis1 = np.array([\n        [3.5, 2, 5],\n        [1, 7, 11],\n        [2, 4, 7],\n        [4, 6, 12],\n        [9, 8, 13]\n    ])\n\n    X_imputed_median_axis1 = np.array([\n        [3.5, 2, 5],\n        [1, 7, 11],\n        [2, 4, 5],\n        [4, 6, 12],\n        [9, 8, 13]\n    ])\n\n    _check_statistics(X, X_imputed_mean_axis0, \"mean\", [2, 5, 10], \"NaN\")\n    _check_statistics(X, X_imputed_median_axis0, \"median\", [2, 5, 11.5], \"NaN\")\n    _check_statistics(X, X_imputed_mean_axis1, \"mean\", None, \"NaN\")\n    _check_statistics(X, X_imputed_median_axis1, \"median\", None, \"NaN\")\n", "def test_imputation_invalid_axis():\n    # Test imputation with invalid axis.\n    X = np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ])\n\n    imputer = Imputer(missing_values=-1, strategy=\"mean\", axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_sparse_csr_most_frequent():\n    # Test imputation using the most-frequent strategy on sparse CSR matrices.\n    X = sparse.csr_matrix(np.array([\n        [0, 0, 0, 5],\n        [0, 2, 0, 3],\n        [0, 1, 3, 0],\n        [0, 2, 3, 7],\n    ]))\n\n    X_true = np.array([\n        [2, 0, 5],\n        [2, 3, 3],\n        [1, 3, 3],\n        [2, 3, 7],\n    ])\n\n    imputer = Imputer(missing_values=0, strategy='most_frequent')\n    X_trans = imputer.fit_transform(X)\n    assert_array_almost_equal(X_trans.toarray(), X_true)\n", "def test_imputation_axis_invalid():\n    # Test imputation with invalid axis parameter.\n    X = np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ])\n\n    imputer = Imputer(missing_values=-1, strategy=\"mean\", axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_axis_parameter():\n    # Test imputation with different axis parameters.\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    # Test axis=0\n    imputer = Imputer(strategy=\"mean\", axis=0)\n    imputer.fit(X)\n    assert_array_almost_equal(imputer.statistics_, np.array([np.nan, 2.25, 0., 1.25, 5.25]))\n\n    # Test axis=1\n    imputer = Imputer(strategy=\"mean\", axis=1)\n    imputer.fit(X)\n    assert_array_almost_equal(imputer.statistics_, np.array([1., 1.33333333, 0.4, 6.]))\n\n    # Test axis parameter in transform method\n    imputer = Imputer(strategy=\"mean\", axis=1)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_array_almost_equal(X_trans, np.array([[1., 0., 0., 0., 5.],\n                                                  [1.33333333, 1., 0., 1.33333333, 3.],\n                                                  [0.4, 2., 0., 0., 0.],\n                                                  [6., 6., 0., 5., 13.]]))\n"], "sample_725": ["def test_check_array_dtype_object():\n    # Test that check_array with dtype 'object' raises an error for sparse input\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    msg = \"object dtype is not supported by sparse matrices\"\n    assert_raise_message(ValueError, msg, check_array, X, dtype='object')\n", "def test_indexable():\n    # test that indexable works as expected on various input types\n\n    # list of lists\n    X = [[1, 2], [3, 4]]\n    X_indexable = indexable(X)\n    assert_equal(len(X), len(X_indexable))\n    for x, x_indexable in zip(X, X_indexable):\n        assert_array_equal(x, x_indexable)\n\n    # numpy array\n    X = np.array([[1, 2], [3, 4]])\n    X_indexable = indexable(X)\n    assert_equal(len(X), len(X_indexable))\n    for x, x_indexable in zip(X, X_indexable):\n        assert_array_equal(x, x_indexable)\n\n    # sparse matrix\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    X_indexable = indexable(X)\n    assert_equal(len(X.toarray()), len(X_indexable))\n    for x, x_indexable in zip(X.toarray(), X_indexable):\n        assert_array_equal(x, x_indexable.toarray().flatten())\n\n    # dataframe\n    try:\n        import pandas as pd\n        X = pd.DataFrame(np.array([[1, 2], [3, 4]]))\n        X_indexable = indexable(X)\n        assert_equal(len(X), len(X_indexable))\n        for x, x_indexable in zip(X.values, X_indexable):\n            assert_array_equal(x, x_indexable)\n    except ImportError:\n        raise SkipTest(\"Pandas not found\")\n\n    # non-indexable type (should just pass it through)\n    X = \"not indexable\"\n    X_indexable = indexable(X)\n    assert_equal(X, X_indexable)\n\n    # multiple inputs\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    X_indexable, y_indexable = indexable(X, y)\n    assert_equal(len(X), len(X_indexable))\n    for x, x_indexable in zip(X, X_indexable):\n        assert_array_equal(x, x_indexable)\n    assert_array_equal(y, y_indexable)\n", "def test_column_or_1d():\n    # Test function for column_or_1d\n    X = np.array([[1], [2], [3]])\n    X_checked = column_or_1d(X, warn=True)\n    assert_array_equal(X_checked, X.ravel())\n    X = np.array([1, 2, 3])\n    X_checked = column_or_1d(X, warn=True)\n    assert_array_equal(X_checked, X)\n    X = np.array([[1, 2, 3]])\n    with pytest.raises(ValueError):\n        column_or_1d(X)\n", "def test_check_X_y_length():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2])\n    assert_raises_regex(ValueError, 'Found input variables with inconsistent '\n                        'numbers of samples:', check_X_y, X, y)\n", "def test_check_X_y():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    # Check that X and y are converted to float64\n    X_int = np.array([[1, 2], [3, 4]], dtype=np.int32)\n    y_int = np.array([0, 1], dtype=np.int32)\n    X_checked, y_checked = check_X_y(X_int, y_int)\n    assert_equal(X_checked.dtype, np.float64)\n    assert_equal(y_checked.dtype, np.float64)\n\n    # Check that y is not converted to float64 if it's a string array\n    y_str = np.array(['a', 'b'])\n    X_checked, y_checked = check_X_y(X, y_str)\n    assert_equal(y_checked.dtype, object)\n\n    # Check that y is converted to float64 if it's a pandas DataFrame with\n    # numeric data\n    try:\n        import pandas as pd\n        y_df = pd.DataFrame([0, 1])\n        X_checked, y_checked = check_X_y(X, y_df)\n        assert_equal(y_checked.dtype, np.float64)\n    except ImportError:\n        raise SkipTest(\"Pandas not found\")\n\n    # Check multi-output with default value of multi_output=False\n    y_multi = np.array([[0, 1], [2, 3]])\n    assert_raises_regex(ValueError, 'bad input shape \\(2, 2\\)', check_X_y, X,\n                        y_multi)\n\n    # Check multi-output with multi_output=True\n    X_checked, y_checked = check_X_y(X, y_multi, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y_multi, y_checked)\n"], "sample_726": ["def test_label_binarize_multilabel_empty_class():\n    y = [(1, 2), (1,), ()]\n    classes = [1, 2]\n    expected = np.array([[1, 1], [1, 0], [0, 0]])\n    mlb = LabelBinarizer()\n    got = mlb.fit_transform(y)\n    assert_array_equal(expected, got)\n    assert_array_equal(mlb.classes_, classes)\n    assert_array_equal(mlb.inverse_transform(got), y)\n\n    # Ensure sparse outputs are correct\n    mlb = LabelBinarizer(sparse_output=True)\n    got = mlb.fit_transform(y)\n    assert_true(issparse(got))\n    assert_array_equal(expected, got.toarray())\n    assert_array_equal(mlb.classes_, classes)\n    assert_array_equal(mlb.inverse_transform(got.toarray()), y)\n", "def test_inverse_binarize_thresholding():\n    got = _inverse_binarize_thresholding(csr_matrix([[0, 1, 0],\n                                                     [-1, 0, -1],\n                                                     [0, 0, 0]]),\n                                         output_type=\"multilabel-indicator\",\n                                         classes=np.arange(3),\n                                         threshold=0)\n    assert_array_equal(got, np.array([[0, 1, 0],\n                                      [1, 0, 1],\n                                      [0, 0, 0]]))\n", "def test_label_encoder_with_numpy_array():\n    le = LabelEncoder()\n    y = np.array([1, 2, 2, 6])\n    le.fit(y)\n    assert_array_equal(le.classes_, [1, 2, 6])\n    assert_array_equal(le.transform(y), [0, 1, 1, 2])\n    assert_array_equal(le.inverse_transform([0, 1, 1, 2]), y)\n\n    # check that the fit_transform method works as expected\n    le = LabelEncoder()\n    y_transformed = le.fit_transform(y)\n    assert_array_equal(y_transformed, [0, 1, 1, 2])\n    assert_array_equal(le.classes_, [1, 2, 6])\n\n    # check that the inverse_transform method works as expected\n    y_inverted = le.inverse_transform(y_transformed)\n    assert_array_equal(y_inverted, y)\n", "def test_label_binarize_thresholding_binary():\n    y = np.array([[1, 0], [0, 1], [1, 0]])\n    classes = np.array([0, 1])\n    threshold = 0.5\n\n    # Binary case where threshold is an integer\n    expected = np.array([0, 1, 0])\n    got = _inverse_binarize_thresholding(y, output_type=\"binary\",\n                                         classes=classes, threshold=threshold)\n    assert_array_equal(got, expected)\n\n    # Binary case where threshold is a float\n    expected = np.array([0, 1, 0])\n    got = _inverse_binarize_thresholding(y, output_type=\"binary\",\n                                         classes=classes, threshold=0.7)\n    assert_array_equal(got, expected)\n", "def test_label_binarize_multilabel_indicator_with_unseen_labels():\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1, 2, 3]\n    expected = np.array([[0, 1, 0, 0], [1, 1, 1, 0], [0, 0, 0, 0]])\n\n    lb = LabelBinarizer()\n    got = lb.fit_transform(y_ind)\n    assert_array_equal(got, expected)\n\n    inverse_got = lb.inverse_transform(got)\n    assert_array_equal(inverse_got, y_ind)\n"], "sample_727": ["def test_imputation_sparse_most_frequent():\n    # Test imputation using the most-frequent strategy on sparse data.\n    X = sparse.csr_matrix(np.array([\n        [0, 1, 0],\n        [2, 0, 3],\n        [0, 1, 4],\n        [0, 0, 5]\n    ]))\n\n    X_imputed = np.array([\n        [0, 1, 0],\n        [2, 1, 3],\n        [0, 1, 4],\n        [0, 1, 5]\n    ])\n\n    imputer = Imputer(missing_values=0, strategy='most_frequent')\n    assert_array_almost_equal(imputer.fit_transform(X).toarray(), X_imputed)\n", "def test_imputation_axis_none():\n    # Test imputation with axis=None\n    X = np.array([\n        [np.nan, 0, 0],\n        [1, np.nan, 0],\n        [2, 0, np.nan]\n    ])\n\n    imputer = Imputer(missing_values=np.nan, strategy=\"mean\", axis=None)\n    X_trans = imputer.fit_transform(X)\n\n    assert_array_almost_equal(imputer.statistics_, np.mean(X[~np.isnan(X)]))\n    assert_array_almost_equal(X_trans, np.array([\n        [np.mean(X[~np.isnan(X)]), 0, 0],\n        [1, np.mean(X[~np.isnan(X)]), 0],\n        [2, 0, np.mean(X[~np.isnan(X)])]\n    ]))\n\n    X_sparse = sparse.csc_matrix(X)\n    imputer = Imputer(missing_values=np.nan, strategy=\"mean\", axis=None)\n    X_trans_sparse = imputer.fit_transform(X_sparse)\n\n    assert_array_almost_equal(imputer.statistics_, np.mean(X[~np.isnan(X)]))\n    assert_array_almost_equal(X_trans_sparse.toarray(), np.array([\n        [np.mean(X[~np.isnan(X)]), 0, 0],\n        [1, np.mean(X[~np.isnan(X)]), 0],\n        [2, 0, np.mean(X[~np.isnan(X)])]\n    ]))\n", "def test_imputation_transform_with_invalid_fit_axis():\n    # Test imputation when fit and transform are called with different axes\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n    imputer = Imputer(missing_values=np.nan, axis=0)\n    imputer.fit(X)\n\n    # Try to transform with a different axis than during fit\n    assert_raises(ValueError, imputer.transform, X.transpose())\n", "def test_imputation_sparse_invalid_axis():\n    # Test imputation on sparse matrix with invalid axis.\n    X = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n    imputer = Imputer(missing_values=0, axis=2)\n\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_axis_none():\n    # Test imputation with axis=None.\n    X = np.array([\n        [1, 2],\n        [np.nan, 3],\n        [7, 6]\n    ])\n\n    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n    X_expected = np.array([\n        [1, 2],\n        [4, 3],\n        [7, 6]\n    ])\n\n    assert_array_almost_equal(imputer.fit_transform(X), X_expected)\n\n    imputer = Imputer(missing_values='NaN', strategy='mean', axis=None)\n    assert_array_almost_equal(imputer.fit_transform(X), X_expected)\n"], "sample_728": ["def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2,\n                                   n_classes=3, random_state=0)\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    # Test the cluster centroids are roughly on the vertices of a regular\n    # simplex.\n    for n_classes in range(2, 6):\n        X, y = make_gaussian_quantiles(n_samples=300, n_features=2,\n                                       n_classes=n_classes, random_state=0)\n        means = np.array([X[y == i].mean(axis=0) for i in range(n_classes)])\n        simplex_vertices = np.array([[np.cos(i * np.pi / n_classes),\n                                      np.sin(i * np.pi / n_classes)]\n                                     for i in range(n_classes)])\n        # Translate and rotate to match the two point clouds.\n        means_centered = means - means.mean(axis=0)\n        vertices_centered = simplex_vertices - simplex_vertices.mean(axis=0)\n        R = np.dot(vertices_centered.T, means_centered)\n        U, S, Vt = np.linalg.svd(R)\n        R = np.dot(U, Vt)\n        rotated_vertices = np.dot(R, vertices_centered.T).T\n        assert_array_almost_equal(means_centered, rotated_vertices, decimal=1)\n\n    # Test that the samples are gaussian distributed within each class.\n    X, y = make_gaussian_quantiles(n_samples=10000, n_features=2,\n                                   n_classes=5, random_state=0)\n    for yi in np.unique(y):\n        Xi = X[y == yi]\n        assert_true(np.all(np.abs(np.corrcoef(Xi.T)) > 0.9))\n        assert_true(np.all(np.std(Xi, axis=0) > 0.45))\n        assert_true(np.all(np.std(Xi, axis=0) < 0.55))\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3,\n                                   shuffle=True, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n    assert_array_almost_equal(np.bincount(y) / len(y),\n                              [1/3] * 3, decimal=1,\n                              err_msg=\"Wrong number of samples per class\")\n\n    # Test with non-default values for mean and cov\n    mean = [1.0, 2.0]\n    cov = 2.\n    X, y = make_gaussian_quantiles(mean=mean, cov=cov, n_samples=100,\n                                   n_features=2, n_classes=3,\n                                   shuffle=True, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n    assert_array_almost_equal(np.bincount(y) / len(y),\n                              [1/3] * 3, decimal=1,\n                              err_msg=\"Wrong number of samples per class\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2,\n                                   n_classes=3, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    # Test that clusters are on the vertices of a simplex\n    for i in range(3):\n        assert_almost_equal(np.mean(X[y == i], axis=0),\n                            np.array([[1., 0.], [0., 1.], [0., 0.]])[i],\n                            decimal=1)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3,\n                                   shuffle=True, random_state=None)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n\n    # Ensure that the class labels are in the correct ratio\n    assert_array_almost_equal(np.bincount(y) / len(y), [1/3]*3,\n                              err_msg=\"Wrong number of samples per class\")\n\n    # Ensure that all features are informative\n    for i in range(2):\n        assert_true(np.any(X[y == 0, i] != X[y == 1, i]),\n                    \"Feature {} is not informative\".format(i))\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3,\n                                   shuffle=True, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=1., n_samples=100,\n                                   n_features=2, n_classes=3,\n                                   shuffle=True, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    X, y = make_gaussian_quantiles(mean=None, cov=[[1, .5], [.5, 1]],\n                                   n_samples=100, n_features=2, n_classes=3,\n                                   shuffle=True, random_state=0)\n\n    assert_raises(ValueError, make_gaussian_quantiles, mean=None, cov=[[1, .5], [.5, 1]],\n                  n_samples=100, n_features=3, n_classes=3, shuffle=True,\n                  random_state=0)\n"], "sample_729": ["def test_enet_l1_ratio_warning():\n    # Test that a warning message is raised if an estimator that\n    # uses _alpha_grid is called with l1_ratio < 0.01\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    msg = (\"l1_ratio parameter is less than 0.01. \"\n           \"In current implementation, it is unsafe to set l1_ratio < 0.01.\")\n    with assert_warns_message(Warning, msg):\n        ElasticNetCV(l1_ratio=0.005, random_state=42).fit(X, y)\n    with assert_warns_message(Warning, msg):\n        MultiTaskElasticNetCV(l1_ratio=0.005, random_state=42).fit(X, y[:, None])\n", "def test_enet_multitarget_copy_X_True_check_input_False():\n    X, y, _, _ = build_dataset(n_targets=3)\n    X = X.copy(order='F')\n\n    original_X = X.copy()\n    enet = MultiTaskElasticNet(copy_X=True)\n    enet.fit(X, y, check_input=False)\n\n    assert_array_equal(original_X, X)\n", "def test_multitask_enet_and_lasso_positive_constraint():\n    X, y, _, _ = build_dataset(n_features=10, n_targets=3)\n    for estimator in [MultiTaskElasticNet, MultiTaskLasso]:\n        model_unconstrained = estimator(fit_intercept=True,\n                                        max_iter=1000, tol=1e-7)\n        ignore_warnings(model_unconstrained.fit)(X, y)\n        assert_true(min(model_unconstrained.coef_.ravel()) < 0)\n\n        model_constrained = estimator(positive=True, fit_intercept=True,\n                                      max_iter=1000, tol=1e-7)\n        ignore_warnings(model_constrained.fit)(X, y)\n        assert_true(min(model_constrained.coef_.ravel()) >= 0)\n", "def test_enet_l1_ratio_warning():\n    # Test that a warning message is raised if l1_ratio < 0.01 or > 1\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    msg = (\"l1_ratio must be between 0 and 1; got l1_ratio={}.\")\n    for l1_ratio in [-1, 2]:\n        assert_warns_message(UserWarning, msg.format(l1_ratio),\n                             ElasticNet(l1_ratio=l1_ratio).fit, X, y)\n        assert_warns_message(UserWarning, msg.format(l1_ratio),\n                             ElasticNetCV(l1_ratio=l1_ratio).fit, X, y)\n        assert_warns_message(UserWarning, msg.format(l1_ratio),\n                             MultiTaskElasticNet(l1_ratio=l1_ratio).fit, X,\n                             y[:, None])\n        assert_warns_message(UserWarning, msg.format(l1_ratio),\n                             MultiTaskElasticNetCV(l1_ratio=l1_ratio).fit, X,\n                             y[:, None])\n", "def test_enet_path_max_iter():\n    # Test that enet_path's max_iter parameter is respected\n\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    max_iter = 5\n\n    # Call enet_path with max_iter\n    _, coefs, _ = enet_path(X, y, max_iter=max_iter)\n\n    # Check that the number of iterations is less than or equal to max_iter\n    assert_true(coefs.shape[1] <= max_iter)\n"], "sample_730": ["def test_lasso_path_positive():\n    # Test positive parameter\n\n    X, y, _, _ = build_dataset(n_samples=50, n_features=50)\n\n    # Test that the coefs returned by positive=True in lasso_path are positive\n    pos_path_coef = lasso_path(X, y, positive=True)[1]\n    assert_true(np.all(pos_path_coef >= 0))\n", "def test_multitask_enet_and_lasso_cv_sparse():\n    X, y, _, _ = build_dataset(n_features=50, n_targets=3)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf_sparse = MultiTaskElasticNetCV().fit(X_sparse, y)\n    clf_dense = MultiTaskElasticNetCV().fit(X, y)\n    assert_almost_equal(clf_sparse.alpha_, clf_dense.alpha_, decimal=5)\n    assert_array_almost_equal(clf_sparse.coef_, clf_dense.coef_, decimal=5)\n    assert_array_almost_equal(clf_sparse.intercept_, clf_dense.intercept_, decimal=5)\n\n    clf_sparse = MultiTaskLassoCV().fit(X_sparse, y)\n    clf_dense = MultiTaskLassoCV().fit(X, y)\n    assert_almost_equal(clf_sparse.alpha_, clf_dense.alpha_, decimal=5)\n    assert_array_almost_equal(clf_sparse.coef_, clf_dense.coef_, decimal=5)\n    assert_array_almost_equal(clf_sparse.intercept_, clf_dense.intercept_, decimal=5)\n", "def test_sparse_input_non_fortran():\n    X, y, _, _ = build_dataset(n_features=10)\n    X_sparse = sparse.csr_matrix(X)\n    clf_csr = ElasticNet(selection='cyclic', tol=1e-8)\n    clf_csr.fit(X_sparse, y)\n\n    # Ensure that non-Fortran arrays are converted correctly\n    X_sparse = sparse.coo_matrix(X).tocsc()\n    clf_csc = ElasticNet(selection='cyclic', tol=1e-8)\n    clf_csc.fit(X_sparse, y)\n\n    assert_array_almost_equal(clf_csr.coef_, clf_csc.coef_)\n    assert_almost_equal(clf_csr.intercept_, clf_csc.intercept_)\n", "def test_l1_ratio_param():\n    # Test that l1_ratio is correctly set in ElasticNet and MultiTaskElasticNet\n    X, y, _, _ = build_dataset()\n    clf = ElasticNet(l1_ratio=0.5)\n    clf.fit(X, y)\n    assert_almost_equal(clf.l1_ratio, 0.5)\n\n    clf = MultiTaskElasticNet(l1_ratio=0.5)\n    clf.fit(X, np.hstack((y[:, np.newaxis], y[:, np.newaxis])))\n    assert_almost_equal(clf.l1_ratio, 0.5)\n\n    # Test that an error is raised if l1_ratio is outside [0, 1]\n    clf = ElasticNet(l1_ratio=-0.5)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = ElasticNet(l1_ratio=1.5)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = MultiTaskElasticNet(l1_ratio=-0.5)\n    assert_raises(ValueError, clf.fit, X, np.hstack((y[:, np.newaxis], y[:, np.newaxis])))\n\n    clf = MultiTaskElasticNet(l1_ratio=1.5)\n    assert_raises(ValueError, clf.fit, X, np.hstack((y[:, np.newaxis], y[:, np.newaxis])))\n", "def test_sparse_input_non_fortran():\n    # Test that sparse input in non-Fortran format raises an error\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    csr = sparse.csr_matrix(X)\n    csc = sparse.csc_matrix(X)\n\n    # ElasticNet and Lasso should raise an error for CSR input\n    for est in [ElasticNet(), Lasso()]:\n        assert_raises(ValueError, est.fit, csr, y)\n\n    # ElasticNet and Lasso should work with CSC input\n    for est in [ElasticNet(), Lasso()]:\n        est.fit(csc, y)\n"], "sample_731": ["def test_fetch_california_housing_features():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                     \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert(data.feature_names == feature_names)\n    assert(len(data.feature_names) == data.data.shape[1])\n", "def test_california_housing_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                     \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert(data.feature_names == feature_names)\n", "def test_california_housing_features():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                     \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert(data.feature_names == feature_names)\n    assert(len(data.feature_names) == data.data.shape[1])\n", "def test_fetch_california_housing_features():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                     \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert(data.feature_names == feature_names)\n    assert(data.DESCR == __import__(\"sklearn.datasets.california_housing\").california_housing.__doc__)\n", "def test_california_housing_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                     \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert(data.feature_names == feature_names)\n"], "sample_732": ["def test_random_state():\n    try:\n        dataset1 = fetch_kddcup99(random_state=0, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n        dataset2 = fetch_kddcup99(random_state=0, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n        dataset3 = fetch_kddcup99(random_state=1, subset='SA', shuffle=True,\n                                  percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(dataset1.data.shape, dataset2.data.shape)\n    assert_equal(dataset1.target.shape, dataset2.target.shape)\n    assert_equal(dataset1.data.shape, dataset3.data.shape)\n    assert_equal(dataset1.target.shape, dataset3.target.shape)\n\n    # Check that the data is the same for the same random state\n    assert_array_equal(dataset1.data, dataset2.data)\n    assert_array_equal(dataset1.target, dataset2.target)\n\n    # Check that the data is different for different random states\n    assert_not_array_equal(dataset1.data, dataset3.data)\n    assert_not_array_equal(dataset1.target, dataset3.target)\n", "def test_download_if_missing():\n    try:\n        fetch_kddcup99(download_if_missing=False, percent10=True)\n    except IOError:\n        # dataset is not available, we can test if the download works\n        fetch_kddcup99(download_if_missing=True, percent10=True)\n        # if the download succeeds, the second call should not raise an exception\n        fetch_kddcup99(download_if_missing=False, percent10=True)\n    else:\n        # dataset is already available, we can't test the download\n        raise SkipTest(\"kddcup99 dataset is already downloaded.\")\n", "def test_fetch_kddcup99_return_X_y():\n    try:\n        X, y = fetch_kddcup99(return_X_y=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(X.shape, (494021, 41))\n    assert_equal(y.shape, (494021,))\n", "def test_fetch_kddcup99_return_X_y():\n    try:\n        data = fetch_kddcup99(download_if_missing=False, return_X_y=True)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(len(data), 2)\n    X, y = data\n    assert_equal(X.shape, (494021, 41))\n    assert_equal(y.shape, (494021,))\n", "def test_download_if_missing():\n    try:\n        fetch_kddcup99(download_if_missing=False, percent10=True)\n    except IOError:\n        # the data is not available, let's download it and check again\n        fetch_kddcup99(download_if_missing=True, percent10=True)\n        # now we should be able to load the data without downloading it\n        fetch_kddcup99(download_if_missing=False, percent10=True)\n    else:\n        # if we didn't get an IOError, that means the data was already downloaded\n        # let's delete it and check that we can download it correctly\n        from sklearn.datasets import get_data_home\n        data_home = get_data_home()\n        import os\n        kddcup_dir = os.path.join(data_home, \"kddcup99_10-py3\")\n        import shutil\n        shutil.rmtree(kddcup_dir)\n        # now we should be able to download the data correctly\n        fetch_kddcup99(download_if_missing=True, percent10=True)\n"], "sample_733": ["def test_tfidftransformer_sanity():\n    # sanity check on the idf computation\n    X = [[1, 1, 1],\n         [1, 1, 0],\n         [1, 0, 0]]\n    tfidf = TfidfTransformer().fit(X)\n    assert_array_almost_equal(tfidf.idf_, [1., 1. + np.log(3. / 2.), 1. + np.log(3.)])\n", "def test_vectorizers_dtype():\n    # Test that vectorizers can handle different input dtypes.\n\n    # Create a list of documents with different types.\n    docs = [\"hello world\", b\"goodbye world\", np.array([\"hello\", \"world\"]),\n            np.array([b\"hello\", b\"world\"]), \"hello\"]\n\n    # Initialize the vectorizers with different dtype parameters.\n    vectorizers = [\n        CountVectorizer(dtype='int32'),\n        CountVectorizer(dtype='int64'),\n        TfidfVectorizer(dtype='float32'),\n        TfidfVectorizer(dtype='float64'),\n        HashingVectorizer(dtype='int32', norm=None, non_negative=True),\n        HashingVectorizer(dtype='float64', norm=None, non_negative=True)\n    ]\n\n    # Fit and transform each vectorizer with the documents.\n    for vec in vectorizers:\n        X = vec.fit_transform(docs)\n\n        # Check if the resulting matrix has the correct dtype.\n        assert_equal(X.dtype, vec.dtype)\n\n        # Check if the resulting matrix is not empty.\n        assert_greater(X.nnz, 0)\n", "def test_countvectorizer_max_features_none():\n    # Test that max_features=None does not limit the number of features\n    cv = CountVectorizer(max_features=None)\n    X = cv.fit_transform(JUNK_FOOD_DOCS)\n    assert_equal(X.shape[1], len(cv.vocabulary_))\n    assert_equal(len(cv.stop_words_), 0)\n", "def test_vectorizers_empty_input():\n    # Test that vectorizers can handle empty input\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        X = vec.fit_transform([])\n        assert_equal(X.shape, (0, vec.n_features if hasattr(vec, 'n_features') else 0))\n        assert_array_equal(X.toarray(), np.empty((0, vec.n_features if hasattr(vec, 'n_features') else 0)))\n", "def test_vectorizers_validate_params():\n    # Test that invalid values for parameters raise errors\n    for vec in [HashingVectorizer, CountVectorizer, TfidfVectorizer]:\n        # Check that invalid analyzer raises an error\n        message = (\"Parameter 'analyzer' must be one of: \"\n                   \"'word', 'char', 'char_wb'.\")\n        assert_raise_message(ValueError, message, vec, analyzer='invalid')\n\n        # Check that ngram_range is a tuple\n        message = \"ngram_range must be a tuple of two integers.\"\n        assert_raise_message(TypeError, message, vec, ngram_range='invalid')\n\n        # Check that ngram_range is a tuple of two integers\n        message = \"ngram_range must be a tuple of two integers.\"\n        assert_raise_message(TypeError, message, vec, ngram_range=('a', 2))\n\n        # Check that max_df and min_df are within the valid range\n        message = (\"max_df must be at most 1.0, got 5.\")\n        assert_raise_message(ValueError, message, vec, max_df=5)\n\n        message = (\"min_df must be at least 0.0, got -1.\")\n        assert_raise_message(ValueError, message, vec, min_df=-1)\n\n        # Check that max_features is a positive integer\n        message = (\"max_features must be a positive integer or None, got 0.\")\n        assert_raise_message(ValueError, message, vec, max_features=0)\n"], "sample_734": ["def test_contingency_matrix_eps():\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    C = contingency_matrix(labels_a, labels_b)\n    C_eps = contingency_matrix(labels_a, labels_b, eps=0.5)\n    assert_array_almost_equal(C_eps, C + 0.5)\n", "def test_contingency_matrix_non_integer():\n    # Check that non-integer values in the labels do not cause an error\n    labels_a = np.array([1.0, 1.0, 2.0, 2.0])\n    labels_b = np.array([1.0, 2.0, 1.0, 2.0])\n    C = contingency_matrix(labels_a, labels_b)\n    assert_array_almost_equal(C, [[1, 1], [1, 1]])\n", "def test_comb2_function():\n    # Test the comb2 function for various inputs\n    assert_equal(comb2(0), 0)\n    assert_equal(comb2(1), 0)\n    assert_equal(comb2(2), 1)\n    assert_equal(comb2(3), 3)\n    assert_equal(comb2(10), 45)\n\n    # Test the comb2 function with non-integer input\n    assert_raise_message(TypeError, \"Input must be an integer\",\n                         comb2, 3.5)\n\n    # Test the comb2 function with negative input\n    assert_raise_message(ValueError, \"Input must be a non-negative integer\",\n                         comb2, -1)\n", "def test_contingency_matrix_eps():\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    C = contingency_matrix(labels_a, labels_b, eps=.5)\n    assert_array_almost_equal(C, contingency_matrix(labels_a, labels_b) + .5)\n", "def test_contingency_matrix_eps():\n    # Test the effect of eps on the contingency matrix\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    C = contingency_matrix(labels_a, labels_b)\n    C_eps = contingency_matrix(labels_a, labels_b, eps=0.5)\n    assert_array_almost_equal(C_eps, C + 0.5)\n"], "sample_735": ["def test_get_parameters():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        # Check the parameters getter\n        params = gmm._get_parameters()\n        assert_array_equal(params[0], gmm.weights_)\n        assert_array_equal(params[1], gmm.means_)\n        assert_array_equal(params[2], gmm.covariances_)\n        assert_array_equal(params[3], gmm.precisions_cholesky_)\n", "def test_get_parameters():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        # Check get_parameters returns the correct parameters\n        weights, means, covariances, precisions_cholesky = gmm._get_parameters()\n        assert_array_almost_equal(weights, gmm.weights_)\n        assert_array_almost_equal(means, gmm.means_)\n        assert_array_almost_equal(covariances, gmm.covariances_)\n        assert_array_almost_equal(precisions_cholesky, gmm.precisions_cholesky_)\n", "def test_gaussian_mixture_get_parameters():\n    # We check that the get_parameters returns the correct parameters\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        weights, means, covariances, precisions_chol = gmm._get_parameters()\n        assert_array_almost_equal(weights, gmm.weights_)\n        assert_array_almost_equal(means, gmm.means_)\n        if covar_type == 'full':\n            assert_array_almost_equal(covariances, gmm.covariances_)\n        elif covar_type == 'tied':\n            assert_array_almost_equal(covariances, gmm.covariances_)\n        elif covar_type == 'diag':\n            assert_array_almost_equal(covariances, gmm.covariances_)\n        else:\n            assert_array_almost_equal(covariances, gmm.covariances_)\n        assert_array_almost_equal(precisions_chol, gmm.precisions_cholesky_)\n", "def test_gaussian_mixture_get_parameters():\n    # We check that the correct parameters are returned\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng,\n                              n_init=5)\n        gmm.fit(X)\n\n        weights, means, covariances, precisions_cholesky = gmm._get_parameters()\n        assert_array_almost_equal(weights, gmm.weights_)\n        assert_array_almost_equal(means, gmm.means_)\n        if covar_type == 'full':\n            for cov, precision in zip(covariances, precisions_cholesky):\n                assert_array_almost_equal(np.dot(precision, precision.T),\n                                          linalg.inv(cov))\n        elif covar_type == 'tied':\n            assert_array_almost_equal(np.dot(precisions_cholesky,\n                                             precisions_cholesky.T),\n                                      linalg.inv(covariances))\n        else:\n            assert_array_almost_equal(covariances, 1. / precisions_cholesky ** 2)\n", "def test_get_parameters():\n    # We check that the get_parameters returns the correct number of parameters\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n        weights, means, covariances, precisions_cholesky = gmm._get_parameters()\n        assert_array_equal(gmm.weights_, weights)\n        assert_array_equal(gmm.means_, means)\n        assert_array_equal(gmm.covariances_, covariances)\n        assert_array_equal(gmm.precisions_cholesky_, precisions_cholesky)\n"], "sample_736": ["def test_logistic_regression_warning():\n    # Test that the correct warning is raised when there are only two classes\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0,\n                               n_classes=2)\n\n    # Create an instance of LogisticRegression with multi_class='multinomial'\n    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n\n    # Check that a UserWarning is raised\n    msg = \"The number of classes has to be greater than one.\"\n    assert_warns_message(UserWarning, msg, clf.fit, X, y)\n", "def test_logistic_regression_refit():\n    # Test that refit works as expected for LogisticRegressionCV\n\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n\n    # Test refit=True gives same result as refit=False and then fitting a new\n    # model with the best parameters\n    clf_refit_true = LogisticRegressionCV(Cs=[1.0], cv=5, refit=True)\n    clf_refit_true.fit(X, y)\n\n    clf_refit_false = LogisticRegressionCV(Cs=[1.0], cv=5, refit=False)\n    clf_refit_false.fit(X, y)\n\n    clf_new = LogisticRegression(C=clf_refit_false.C_[0])\n    clf_new.fit(X, y)\n\n    assert_array_almost_equal(clf_refit_true.coef_, clf_new.coef_)\n    assert_array_almost_equal(clf_refit_true.intercept_, clf_new.intercept_)\n\n    # Test refit=True gives same result as refit=False when there is only one C\n    clf_refit_true = LogisticRegressionCV(Cs=[1.0], cv=5, refit=True)\n    clf_refit_true.fit(X, y)\n\n    clf_refit_false = LogisticRegressionCV(Cs=[1.0], cv=5, refit=False)\n    clf_refit_false.fit(X, y)\n\n    assert_array_almost_equal(clf_refit_true.coef_, clf_refit_false.coef_)\n    assert_array_almost_equal(clf_refit_true.intercept_, clf_refit_false.intercept_)\n", "def test_multinomial_hessp_sparse():\n    # Test that _multinomial_grad_hess works with sparse matrices.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 5\n    X = rng.randn(n_samples, n_features)\n    w = rng.rand(n_features)\n    Y = np.zeros((n_samples, 3))\n    ind = np.argmax(np.dot(X, w), axis=0)\n    Y[range(0, n_samples), ind] = 1\n    w = w.ravel()\n    sample_weights = np.ones(X.shape[0])\n    X_sparse = sp.csr_matrix(X)\n\n    grad, hessp = _multinomial_grad_hess(w, X_sparse, Y, alpha=1.,\n                                         sample_weight=sample_weights)\n\n        eps = 1e-6\n        return (grad + eps * v) - grad\n\n    v = np.zeros_like(w)\n    v[0] = 1\n    assert_array_almost_equal(hessp_numerical(v), hessp(v))\n", "def test_predict_proba_ovr():\n    # Test that predict_proba works for ovr classification.\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n\n    clf_multi = LogisticRegression(multi_class=\"ovr\")\n    clf_multi.fit(X, y)\n    y_pred_multi = clf_multi.predict_proba(X)\n\n    clf_bin = LogisticRegression()\n    y_pred_bin = np.zeros((len(y), 3))\n    for i in range(3):\n        mask = (y == i)\n        y_binary = np.zeros_like(y)\n        y_binary[mask] = 1\n        clf_bin.fit(X, y_binary)\n        y_pred_bin[:, i] = clf_bin.predict_proba(X)[:, 1]\n\n    y_pred_bin /= y_pred_bin.sum(axis=1)[:, np.newaxis]\n    assert_array_almost_equal(y_pred_multi, y_pred_bin)\n", "def test_logistic_regression_solvers_convergence():\n    # Test convergence of LogisticRegression with different solvers\n\n    X, y = make_classification(n_samples=100, n_features=10,\n                               n_informative=5, random_state=0)\n\n    for solver in ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, random_state=42)\n        clf.fit(X, y)\n        assert_equal(clf.n_iter_[0] < 100, True)\n\n    # Test convergence with large C and intercept scaling\n    clf = LogisticRegression(solver='liblinear', C=1e6, random_state=42,\n                             intercept_scaling=1e3)\n    clf.fit(X, y)\n    assert_equal(clf.n_iter_[0] < 100, True)\n"], "sample_737": ["def test_vectorizer_input_validation():\n    # Test that vectorizers raise an error when input is not a string or bytes\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        message = (\"Iterable over raw text documents expected, \"\n                   \"{} object received.\")\n        \n        # Test with integer input\n        assert_raise_message(\n            ValueError, message.format(\"int\"), vec.fit_transform, [1, 2, 3])\n        assert_raise_message(\n            ValueError, message.format(\"int\"), vec.fit, [1, 2, 3])\n        assert_raise_message(\n            ValueError, message.format(\"int\"), vec.transform, [1, 2, 3])\n\n        # Test with float input\n        assert_raise_message(\n            ValueError, message.format(\"float\"), vec.fit_transform, [1.0, 2.0, 3.0])\n        assert_raise_message(\n            ValueError, message.format(\"float\"), vec.fit, [1.0, 2.0, 3.0])\n        assert_raise_message(\n            ValueError, message.format(\"float\"), vec.transform, [1.0, 2.0, 3.0])\n\n        # Test with list input (not a string or bytes)\n        assert_raise_message(\n            ValueError, message.format(\"list\"), vec.fit_transform, [[1, 2], [3, 4]])\n        assert_raise_message(\n            ValueError, message.format(\"list\"), vec.fit, [[1, 2], [3, 4]])\n        assert_raise_message(\n            ValueError, message.format(\"list\"), vec.transform, [[1, 2], [3, 4]])\n\n        # Test with None input\n        assert_raise_message(\n            ValueError, message.format(\"NoneType\"), vec.fit_transform, [None, None])\n        assert_raise_message(\n            ValueError, message.format(\"NoneType\"), vec.fit, [None, None])\n        assert_raise_message(\n            ValueError, message.format(\"NoneType\"), vec.transform, [None, None])\n", "def test_vectorizer_preprocessor_callable():\n    # Test that a callable can be passed as a preprocessor\n        return doc.upper()\n\n    vec = CountVectorizer(preprocessor=preprocess)\n    X = vec.fit_transform([\"hello world\", \"hello hello\"])\n    assert_equal(vec.vocabulary_, {\"HELLO\": 0, \"WORLD\": 1})\n    assert_array_equal(X.toarray(), [[1, 1], [2, 0]])\n", "def test_vectorizers_empty_input():\n    # Test that vectorizers can handle empty input\n    vecs = [\n        HashingVectorizer(),\n        CountVectorizer(),\n        TfidfVectorizer()\n    ]\n\n    for vec in vecs:\n        X = vec.fit_transform([])\n        assert_equal(X.shape, (0, vec.n_features))\n        if hasattr(vec, 'vocabulary_'):\n            assert_equal(vec.vocabulary_, {})\n", "def test_vectorizer_empty_input():\n    # Test that vectorizers can handle empty input\n    message = \"Iterable over raw text documents expected, string object received.\"\n\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        assert_raise_message(\n            ValueError, message, vec.fit_transform, \"\")\n        assert_raise_message(\n            ValueError, message, vec.fit, \"\")\n        assert_raise_message(\n            ValueError, message, vec.transform, \"\")\n\n    # Test with empty list\n    X = []\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        try:\n            vec.fit_transform(X)\n        except Exception as e:\n            assert False, \"Unexpected exception: %s\" % str(e)\n\n        try:\n            vec.fit(X)\n        except Exception as e:\n            assert False, \"Unexpected exception: %s\" % str(e)\n\n        try:\n            vec.transform(X)\n        except Exception as e:\n            assert False, \"Unexpected exception: %s\" % str(e)\n", "def test_tfidfvectorizer_invalid_norm():\n    # Non-regression test: TfidfVectorizer should raise a ValueError when\n    # given an invalid norm.\n    v = TfidfVectorizer(norm='invalid')\n    message = (\"'%s' is not a valid norm. Norm must be 'l1', 'l2', or None.\"\n               % v.norm)\n    exception = ValueError\n\n        v.fit(['hello world', 'hello hello'])\n\n    assert_raise_message(exception, message, func)\n"], "sample_738": ["def test_vectorizer_stop_words_set():\n    stop_words = {'the', 'and'}\n    vect = CountVectorizer(stop_words=stop_words)\n    vect.fit(ALL_FOOD_DOCS)\n    assert_equal(vect.stop_words_, stop_words)\n    assert_true(all(w not in vect.vocabulary_ for w in stop_words))\n", "def test_vectorizers_transform_input_type():\n    # Test that transform method of vectorizers accepts different input types\n    vecs = [\n        HashingVectorizer(),\n        CountVectorizer(),\n        TfidfVectorizer()\n    ]\n\n    input_types = [\n        list,\n        tuple,\n        np.array,\n        lambda x: (doc for doc in x)\n    ]\n\n    X = [\"good news everyone\", \"this is a test\"]\n\n    for vec in vecs:\n        vec.fit(X)\n\n        for input_type in input_types:\n            X_transformed = vec.transform(input_type(X))\n            assert_equal(X_transformed.shape, (len(X), vec.n_features))\n", "def test_vectorizer_empty_input():\n    message = (\"Iterable over raw text documents expected, \"\n               \"empty iterable received.\")\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        assert_raise_message(\n            ValueError, message, vec.fit_transform, [])\n        assert_raise_message(\n            ValueError, message, vec.fit, [])\n        assert_raise_message(\n            ValueError, message, vec.transform, [])\n", "def test_countvectorizer_dtype():\n    # Test that the dtype parameter is propagated to the matrix.\n    cv = CountVectorizer(dtype=np.float32)\n    X = cv.fit_transform(JUNK_FOOD_DOCS)\n    assert_equal(X.dtype, np.float32)\n\n    cv = CountVectorizer(dtype=np.int64)\n    X = cv.fit_transform(JUNK_FOOD_DOCS)\n    assert_equal(X.dtype, np.int64)\n", "def test_countvectorizer_dtype():\n    # Test that the dtype parameter is taken into account\n    cv = CountVectorizer(dtype=np.float32)\n    X = cv.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.dtype, np.float32)\n\n    cv = CountVectorizer(dtype=np.int64)\n    X = cv.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.dtype, np.int64)\n"], "sample_739": ["def test_inverse_binarize_thresholding_binary():\n    y = np.array([[0, 1], [1, 0]])\n    classes = np.array([0, 1])\n    threshold = 0.5\n\n    got = _inverse_binarize_thresholding(y, \"binary\", classes, threshold)\n    assert_array_equal(got, np.array([1, 0]))\n", "def test_label_encoder_with_numpy_array():\n    # Test LabelEncoder's fit and transform methods with numpy arrays\n    le = LabelEncoder()\n    y = np.array([1, 1, 4, 5, -1, 0])\n    le.fit(y)\n    assert_array_equal(le.classes_, [-1, 0, 1, 4, 5])\n    assert_array_equal(le.transform(y), [2, 2, 3, 4, 0, 1])\n    assert_array_equal(le.inverse_transform([2, 2, 3, 4, 0, 1]), y)\n", "def test_label_binarize_empty_input():\n    y = []\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n    expected = np.array([])\n\n    binarized = label_binarize(y, classes, neg_label=neg_label,\n                               pos_label=pos_label,\n                               sparse_output=False)\n    assert_array_equal(binarized, expected)\n\n    lb = LabelBinarizer(neg_label=neg_label, pos_label=pos_label,\n                        sparse_output=False)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(binarized, expected)\n", "def test_label_binarizer_with_empty_input():\n    lb = LabelBinarizer()\n    assert_raises(ValueError, lb.fit, [])\n    assert_raises(ValueError, lb.transform, [])\n    assert_raises(ValueError, lb.inverse_transform, [])\n\n    lb.fit([1, 2])\n    assert_array_equal(lb.transform([]), np.array([]))\n    assert_array_equal(lb.inverse_transform(np.array([])), np.array([]))\n", "def test_label_binarize_thresholding():\n    # Test label binarize thresholding with different thresholds\n    y = [[0.1, 0.8, 0.1], [0.7, 0.2, 0.1], [0.5, 0.4, 0.1]]\n    classes = np.arange(3)\n    expected = np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0]])\n\n    for threshold in [0.4, 0.6]:\n        binarized = _inverse_binarize_thresholding(y, type_of_target(y),\n                                                   classes, threshold)\n        assert_array_equal(binarized, expected)\n\n    # Test label binarize thresholding with array-like of thresholds\n    thresholds = [0.4, 0.6, 0.8]\n    binarized = _inverse_binarize_thresholding(y, type_of_target(y), classes,\n                                               thresholds)\n    assert_array_equal(binarized, expected)\n"], "sample_740": ["def test_check_X_y():\n    # Test that check_X_y returns the same arrays when no conversion is needed\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    X_checked, y_checked = check_X_y(X, y)\n    assert X is X_checked\n    assert y is y_checked\n\n    # Test that check_X_y raises an error when X and y have different lengths\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1, 2])\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # Test that check_X_y raises an error when X has less than 2 features\n    X = np.array([[1], [2]])\n    y = np.array([0, 1])\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # Test that check_X_y raises an error when X has non-finite values\n    X = np.array([[1, np.inf], [3, 4]])\n    y = np.array([0, 1])\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # Test that check_X_y raises an error when y has non-finite values\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, np.inf])\n    assert_raises(ValueError, check_X_y, X, y)\n", "def test_check_X_y_warns_on_dtype():\n    # Check that warn_on_dtype also works for check_X_y.\n    X = np.array([[1, 2], [3, 4]], dtype=object)\n    y = np.array([0, 1])\n    assert_warns_message(DataConversionWarning,\n                         \"Data with input dtype object were all converted to \"\n                         \"float64.\",\n                         check_X_y, X, y, dtype=np.float64, warn_on_dtype=True)\n    assert_warns(DataConversionWarning, check_X_y, X, y,\n                 dtype='numeric', warn_on_dtype=True)\n    assert_no_warnings(check_X_y, X, y, dtype='object', warn_on_dtype=True)\n\n    # Also check that it raises a warning for mixed dtypes in X.\n    X_mixed = np.array([['1', 2], ['4', 5]])\n    assert_warns(DataConversionWarning, check_X_y, X_mixed, y,\n                 dtype=np.float64, warn_on_dtype=True)\n    assert_warns(DataConversionWarning, check_X_y, X_mixed, y,\n                 dtype='numeric', warn_on_dtype=True)\n    assert_warns(DataConversionWarning, check_X_y, X_mixed, y,\n                 dtype=object, warn_on_dtype=True)\n\n    # Even with numerical dtypes, a conversion can be made because dtypes are\n    # uniformized throughout the array.\n    X_mixed_numeric = np.array([[1., 2], [4., 5]])\n    assert_warns(DataConversionWarning, check_X_y, X_mixed_numeric, y,\n                 dtype='numeric', warn_on_dtype=True)\n    assert_no_warnings(check_X_y, X_mixed_numeric.astype(int), y,\n                       dtype='numeric', warn_on_dtype=True)\n", "def test_check_X_y():\n    # check that X and y have the same length\n    X = [[1, 2], [3, 4]]\n    y = [0, 1, 2]\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # check that y is not 2D\n    X = [[1, 2], [3, 4]]\n    y = [[0], [1]]\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # check that y is not empty\n    X = [[1, 2], [3, 4]]\n    y = []\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # check that multi-output y is allowed if specified\n    X = [[1, 2], [3, 4]]\n    y = [[0, 1], [2, 3]]\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(y_checked, y)\n", "def test_check_X_y_multilabel():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[1, 0], [0, 1]])\n\n    # multioutput with default multi_output=False should raise an error\n    assert_raises_regex(ValueError, r\"y should be a 1d array, got an array of shape\",\n                        check_X_y, X, y)\n\n    # multioutput with multi_output=True should not raise an error\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n", "def test_column_or_1d():\n    # Test function for column_or_1d\n    X = np.ones((3, 2))\n    assert_raises(ValueError, column_or_1d, X)\n    X = np.ones((2,))\n    X_2d = column_or_1d(X)\n    assert_equal(X_2d.shape, (2,))\n    X = np.ones((2, 1))\n    X_1d = column_or_1d(X, warn=False)\n    assert_equal(X_1d.shape, (2,))\n    X = MockDataFrame(np.array([[1], [2]]))\n    X_1d = column_or_1d(X, warn=False)\n    assert_equal(X_1d.shape, (2,))\n"], "sample_741": ["def test_parameter_sampler_repr():\n    param_distributions = {\"kernel\": [\"rbf\", \"linear\"],\n                           \"C\": uniform(0, 1)}\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=10, random_state=0)\n    repr_ = repr(sampler)\n    assert_true(\"ParameterSampler\" in repr_)\n    assert_true(\"n_iter=10\" in repr_)\n    assert_true(\"random_state=0\" in repr_)\n    assert_true(\"param_distributions={'kernel': ['rbf', 'linear'], 'C': <scipy.stats._distn_infrastructure.rv_continuous object at\" in repr_)\n", "def test_grid_search_cv_results_return_train_score():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    param_grid = {'C': [1, 2, 3]}\n    cv = KFold(n_splits=3)\n    search = GridSearchCV(SVC(gamma='scale'), param_grid, cv=cv,\n                          return_train_score=False)\n    search.fit(X, y)\n\n    assert 'mean_train_score' not in search.cv_results_\n    assert 'std_train_score' not in search.cv_results_\n    for i in range(cv.n_splits):\n        assert f'split{i}_train_score' not in search.cv_results_\n", "def test_grid_search_cv_results_rank_tie_breaking_with_return_train_score_false():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid,\n                               return_train_score=False)\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_score'][0],\n                        cv_results['mean_test_score'][1])\n    assert_almost_equal(cv_results['mean_test_score'][1],\n                        cv_results['mean_test_score'][2])\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [1, 1, 1])\n", "def test_grid_search_cv_results_dtype():\n    # Test that the dtypes of the cv_results_ are correct\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    grid_search = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1, 2]},\n                               cv=3)\n    grid_search.fit(X, y)\n\n    cv_results = grid_search.cv_results_\n    assert_true(all(isinstance(cv_results[param], np.ma.MaskedArray)\n                    for param in ('param_C',)))\n    assert_true(all(cv_results[key].dtype == object\n                    for key in ('param_C',)))\n    assert_false(any(isinstance(cv_results[key], np.ma.MaskedArray)\n                     for key in ('mean_fit_time', 'std_fit_time',\n                                 'mean_score_time', 'std_score_time')))\n    assert_true(all(cv_results[key].dtype == np.float64\n                    for key in ('mean_fit_time', 'std_fit_time',\n                                'mean_score_time', 'std_score_time')))\n    assert_false(any(isinstance(cv_results[key], np.ma.MaskedArray)\n                     for key in ('mean_test_score', 'std_test_score',\n                                 'rank_test_score')))\n    assert_true(all(cv_results[key].dtype == np.float64\n                    for key in ('mean_test_score', 'std_test_score')))\n    assert_true(cv_results['rank_test_score'].dtype == np.int32)\n", "def test_grid_search_with_large_parameter_space():\n    # Test that grid search can handle a large parameter space\n    param_grid = {'C': np.logspace(-10, 1, 1000), 'gamma': np.logspace(-5, 0, 500)}\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n\n    clf = GridSearchCV(SVC(gamma='scale'), param_grid, cv=3)\n    clf.fit(X, y)\n\n    assert_equal(clf.best_params_['C'], clf.cv_results_['param_C'][clf.best_index_])\n    assert_equal(clf.best_params_['gamma'], clf.cv_results_['param_gamma'][clf.best_index_])\n"], "sample_742": ["def test_logreg_predict_proba_ovr():\n    # Predicted probabilities using the ovr method should give a\n    # smaller loss than those using the softmax function.\n    X, y = make_classification(n_samples=10, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n\n    clf_multi = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\")\n    clf_multi.fit(X, y)\n    clf_soft_loss = log_loss(y, clf_multi._predict_proba_lr(X))\n    clf_ovr_loss = log_loss(y, clf_multi.predict_proba(X))\n    assert_greater(clf_soft_loss, clf_ovr_loss)\n", "def test_logistic_regression_multiclass_class_weight_auto():\n    # Test class_weight='auto' for multiclass problems.\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n                             class_weight='auto')\n    assert_warns(DeprecationWarning, clf.fit, X, y)\n    clf = LogisticRegression(multi_class='ovr', solver='liblinear',\n                             class_weight='auto')\n    assert_warns(DeprecationWarning, clf.fit, X, y)\n", "def test_logistic_regression_cv_verbose():\n    # Test that verbose parameter in LogisticRegressionCV prints a message\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    clf = LogisticRegressionCV(verbose=1)\n    import io\n    from contextlib import redirect_stderr\n    f = io.StringIO()\n    with redirect_stderr(f):\n        clf.fit(X, y)\n    assert len(f.getvalue()) > 0\n", "def test_predict_proba_ovr_multiclass():\n    # Test that predict_proba works for ovr and multiclass case\n    X, y = make_classification(n_samples=20, n_features=20, n_informative=10,\n                               n_classes=3, random_state=0)\n\n    clf_multi = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n    clf_multi.fit(X, y)\n    y_pred_multi = clf_multi.predict_proba(X)\n\n    clf_ovr = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\")\n    clf_ovr.fit(X, y)\n    y_pred_ovr = clf_ovr.predict_proba(X)\n\n    assert_array_almost_equal(y_pred_multi, y_pred_ovr)\n", "def test_logistic_regression_refit_param():\n    # Test that the refit parameter works as expected in LogisticRegressionCV\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    clf_refit = LogisticRegressionCV(cv=2, refit=True)\n    clf_no_refit = LogisticRegressionCV(cv=2, refit=False)\n\n    clf_refit.fit(X, y)\n    clf_no_refit.fit(X, y)\n\n    # When refit=True, the best_estimator_ attribute should be set and it\n    # should have a coef_ attribute.\n    assert hasattr(clf_refit, 'best_estimator_')\n    assert hasattr(clf_refit.best_estimator_, 'coef_')\n\n    # When refit=False, the best_estimator_ attribute should not be set.\n    assert not hasattr(clf_no_refit, 'best_estimator_')\n"], "sample_743": ["def test_kneighbors_sparse_output():\n    # Test output of kneighbors when input is sparse\n    X = csr_matrix(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n    nn = neighbors.NearestNeighbors(n_neighbors=2, algorithm='brute').fit(X)\n    dist, ind = nn.kneighbors(X)\n    assert_true(issparse(dist))\n    assert_true(issparse(ind))\n    assert_array_equal(dist.toarray(), np.array([[0., 1.], [1., 0.], [1., 1.]]))\n    assert_array_equal(ind.toarray(), np.array([[0, 1], [1, 0], [2, 0]]))\n", "def test_supervised_integer_mixin_fit_with_sparse_y():\n    # Test that SupervisedIntegerMixin's fit method works with sparse y.\n    X = np.array([[0, 1], [1, 0]])\n    y = csr_matrix(np.array([0, 1]))\n    classifier = neighbors.KNeighborsClassifier()\n    classifier.fit(X, y)\n    assert_array_equal(classifier.classes_, np.array([0, 1]))\n", "def test_kneighbors_n_neighbors_larger_than_population():\n    # Test kneighbors when n_neighbors is larger than population\n\n    X = np.array([[1, 2], [3, 4]])\n\n    for algorithm in ALGORITHMS:\n        nn = neighbors.NearestNeighbors(n_neighbors=3,\n                                        algorithm=algorithm).fit(X)\n        dist, ind = nn.kneighbors()\n        assert_array_equal(dist, [[0., 2.82842712, np.inf],\n                                  [2.82842712, 0., np.inf]])\n        assert_array_equal(ind, [[0, 1, 0],\n                                 [1, 0, 1]])\n\n        dist, ind = nn.kneighbors([[1, 2], [3, 4]])\n        assert_array_equal(dist, [[0., 2.82842712, np.inf],\n                                  [2.82842712, 0., np.inf]])\n        assert_array_equal(ind, [[0, 1, 0],\n                                 [1, 0, 1]])\n\n        graph = nn.kneighbors_graph().A\n        assert_array_equal(graph, [[1., 1., 0.],\n                                   [1., 1., 0.]])\n", "def test_kneighbors_graph_non_euclidean_metric():\n    # Test kneighbors_graph with a non-euclidean metric\n    X = np.array([[0, 1], [1.01, 1.], [2, 0]])\n\n    for metric in ['manhattan', 'chebyshev']:\n        A = neighbors.kneighbors_graph(X, 2, mode='connectivity',\n                                       metric=metric,\n                                       include_self=True)\n        assert_array_equal(\n            A.toarray(),\n            [[1., 1., 0.],\n             [1., 1., 1.],\n             [0., 1., 1.]])\n\n        A = neighbors.kneighbors_graph(X, 2, mode='distance',\n                                       metric=metric,\n                                       include_self=True)\n        if metric == 'manhattan':\n            expected_matrix = np.array([\n                [0., 1.01, 0.],\n                [1.01, 0., 1.01],\n                [0., 1.01, 0.]])\n        else:\n            expected_matrix = np.array([\n                [0., 1.01, 0.],\n                [1.01, 0., 2.],\n                [0., 2., 0.]])\n        assert_array_almost_equal(A.toarray(), expected_matrix)\n", "def test_radius_neighbors_empty_query():\n    # Test radius_neighbors when query is empty.\n    X = np.array([[1.0, 1.0], [2.0, 2.0]])\n    y = np.array([1, 2])\n    radius = 0.1\n\n    for algorithm in ALGORITHMS:\n        for weights in ['uniform', 'distance']:\n            clf = neighbors.RadiusNeighborsClassifier(radius=radius,\n                                                      weights=weights,\n                                                      algorithm=algorithm)\n            clf.fit(X, y)\n\n            # Empty query should return empty arrays\n            dist, ind = clf.radius_neighbors([])\n            assert_array_equal(dist, [])\n            assert_array_equal(ind, [])\n\n            # Predict on empty query should raise an error\n            assert_raises(ValueError, clf.predict, [])\n"], "sample_745": ["def test_function_transformer_sparse_output():\n    # Test that FunctionTransformer works with sparse output\n    from scipy.sparse import csr_matrix\n\n    X = np.array([[1, 2], [3, 4]])\n    transformer = FunctionTransformer(func=lambda x: csr_matrix(x))\n    X_trans = transformer.fit_transform(X)\n    assert sparse.issparse(X_trans)\n    assert_array_equal(X_trans.toarray(), X)\n", "def test_function_transformer_sparse():\n    X_sparse = sparse.csr_matrix(np.random.randn(100, 10))\n    transformer = FunctionTransformer(validate=True, accept_sparse=True)\n    X_sparse_trans = transformer.fit_transform(X_sparse)\n    assert sparse.issparse(X_sparse_trans)\n    assert X_sparse.shape == X_sparse_trans.shape\n\n    # Test with validate=False\n    transformer = FunctionTransformer(validate=False, accept_sparse=True)\n    X_sparse_trans = transformer.fit_transform(X_sparse)\n    assert sparse.issparse(X_sparse_trans)\n    assert X_sparse.shape == X_sparse_trans.shape\n\n    # Test with accept_sparse=False\n    transformer = FunctionTransformer(validate=True, accept_sparse=False)\n    with pytest.raises(TypeError):\n        transformer.fit_transform(X_sparse)\n", "def test_function_transformer_sparse():\n    X_sparse = sparse.csr_matrix(np.array([[1, 0, 3], [0, 2, 0]]))\n    transformer = FunctionTransformer(validate=True, accept_sparse=True)\n    X_trans = transformer.fit_transform(X_sparse)\n    assert sparse.issparse(X_trans)\n\n    # Check that sparse matrix is converted to dense array when accept_sparse=False\n    transformer = FunctionTransformer(validate=True, accept_sparse=False)\n    with pytest.raises(TypeError):\n        transformer.fit_transform(X_sparse)\n", "def test_function_transformer_sparse_output():\n    # Test that FunctionTransformer works with sparse output\n    from scipy.sparse import csr_matrix\n\n    X = np.array([[1, 2], [3, 4]])\n    transformer = FunctionTransformer(func=lambda x: csr_matrix(x))\n    X_trans = transformer.fit_transform(X)\n    assert sparse.issparse(X_trans)\n    assert_array_equal(X_trans.toarray(), X)\n", "def test_function_transformer_sparse_output():\n    X = np.array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n    transformer = FunctionTransformer(func=sparse.csr_matrix, validate=False)\n    X_trans = transformer.fit_transform(X)\n    assert sparse.issparse(X_trans)\n    assert_array_equal(X_trans.toarray(), X)\n"], "sample_746": ["def test_brier_score_loss_multiclass():\n    # Check brier_score_loss function for multiclass case\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.9, 0.05, 0.05], [0.05, 0.9, 0.05],\n                       [0.05, 0.05, 0.9], [0.8, 0.1, 0.1],\n                       [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]])\n    true_score = np.mean(np.sum((y_true[:, np.newaxis] != np.arange(3)) *\n                                (y_pred ** 2), axis=1))\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n", "def test_balanced_accuracy_score():\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 0, 1]\n\n    # Calculate the true balanced accuracy\n    bacc = (recall_score(y_true, y_pred, pos_label=0) +\n            recall_score(y_true, y_pred, pos_label=1)) / 2.\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), bacc)\n\n    # Test with sample weights\n    sample_weight = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n    bacc = (recall_score(y_true, y_pred, pos_label=0,\n                         sample_weight=sample_weight) +\n            recall_score(y_true, y_pred, pos_label=1,\n                         sample_weight=sample_weight)) / 2.\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred,\n                                                sample_weight=sample_weight),\n                        bacc)\n", "def test_brier_score_loss_multiclass():\n    # Check brier_score_loss function for multiclass case\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.9, 0.05, 0.05],\n                       [0.1, 0.8, 0.1],\n                       [0.05, 0.1, 0.85],\n                       [0.8, 0.1, 0.1],\n                       [0.2, 0.7, 0.1],\n                       [0.1, 0.2, 0.7]])\n    true_score = np.mean(np.sum((y_pred - np.eye(3)[y_true]) ** 2, axis=1))\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred[:, :-1])\n    assert_raises(ValueError, brier_score_loss, y_true, np.c_[y_pred, y_pred])\n", "def test_brier_score_loss_multiclass():\n    # Check multiclass brier_score_loss function\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.7, 0.2, 0.1],\n                       [0.1, 0.8, 0.1],\n                       [0.3, 0.4, 0.3],\n                       [0.9, 0.05, 0.05],\n                       [0.05, 0.9, 0.05],\n                       [0.05, 0.05, 0.9]])\n\n    true_score = np.mean(np.sum((y_true[:, np.newaxis] == np.arange(3)) *\n                                (y_pred - 1) ** 2, axis=1))\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n\n    # check when y_true is not consecutive\n    y_true = np.array([0, 1, 3, 0, 1, 3])\n    y_pred = np.array([[0.7, 0.2, 0.1],\n                       [0.1, 0.8, 0.1],\n                       [0.3, 0.4, 0.3],\n                       [0.9, 0.05, 0.05],\n                       [0.05, 0.9, 0.05],\n                       [0.05, 0.05, 0.9]])\n    true_score = np.mean(np.sum((y_true[:, np.newaxis] == np.array([0, 1, 3])) *\n                                (y_pred - 1) ** 2, axis=1))\n    assert_almost_equal(brier_score_loss(y_true, y_pred, labels=[0, 1, 3]),\n                        true_score)\n", "def test_balanced_accuracy_score_binary():\n    # Test that balanced_accuracy_score function returns the average of\n    # sensitivity and specificity\n    y_true = np.array([0, 1, 0, 0, 1, 0])\n    y_pred = np.array([0, 1, 0, 0, 0, 1])\n\n    sensitivity = recall_score(y_true, y_pred)\n    specificity = recall_score(1 - y_true, 1 - y_pred)\n    average = (sensitivity + specificity) / 2.\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), average)\n\n    # Check for a warning if there is only one unique label in y_true\n    y_true = np.array([0, 0, 0, 0, 0, 0])\n    y_pred = np.array([0, 1, 0, 0, 1, 0])\n    with warnings.catch_warnings(record=True):\n        assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.5)\n"], "sample_748": ["def test_grid_search_cv_results_rank_tie_breaking_with_scoring():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid,\n                               scoring='f1_macro')\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    assert_almost_equal(cv_results['mean_test_score'][0],\n                        cv_results['mean_test_score'][1])\n    assert_false(np.allclose(cv_results['mean_test_score'][1],\n                             cv_results['mean_test_score'][2]))\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [1, 1, 3])\n", "def test_grid_search_cv_results_rank_tie_breaking_with_scoring():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    for scoring in ['accuracy', 'f1_macro']:\n        grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid,\n                                   scoring=scoring)\n        grid_search.fit(X, y)\n        cv_results = grid_search.cv_results_\n        assert_almost_equal(cv_results['mean_test_score'][0],\n                            cv_results['mean_test_score'][1])\n        assert_false(np.allclose(cv_results['mean_test_score'][1],\n                                 cv_results['mean_test_score'][2]))\n        assert_almost_equal(cv_results['rank_test_score'], [1, 1, 3])\n", "def test_grid_search_cv_results_rank_tie_breaking_non_uniform_weights():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid,\n                               iid=False, cv=KFold(n_splits=3))\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_score'][0],\n                        cv_results['mean_test_score'][1])\n    assert_almost_equal(cv_results['mean_train_score'][0],\n                        cv_results['mean_train_score'][1])\n    assert_false(np.allclose(cv_results['mean_test_score'][1],\n                             cv_results['mean_test_score'][2]))\n    assert_false(np.allclose(cv_results['mean_train_score'][1],\n                             cv_results['mean_train_score'][2]))\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [1, 1, 3])\n", "def test_grid_search_cv_results_rank_tie_breaking_with_scoring():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid,\n                               scoring='f1_macro')\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_score'][0],\n                        cv_results['mean_test_score'][1])\n    assert_almost_equal(cv_results['mean_train_score'][0],\n                        cv_results['mean_train_score'][1])\n    assert_false(np.allclose(cv_results['mean_test_score'][1],\n                             cv_results['mean_test_score'][2]))\n    assert_false(np.allclose(cv_results['mean_train_score'][1],\n                             cv_results['mean_train_score'][2]))\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [1, 1, 3])\n", "def test_grid_search_cv_results_rank_tie_breaking_with_scoring():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid,\n                               scoring='f1_macro')\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_score'][0],\n                        cv_results['mean_test_score'][1])\n    assert_almost_equal(cv_results['mean_train_score'][0],\n                        cv_results['mean_train_score'][1])\n    assert_false(np.allclose(cv_results['mean_test_score'][1],\n                             cv_results['mean_test_score'][2]))\n    assert_false(np.allclose(cv_results['mean_train_score'][1],\n                             cv_results['mean_train_score'][2]))\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [1, 1, 3])\n"], "sample_749": ["def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert_raise_message(NotImplementedError,\n                         \"get_feature_names is not yet supported when using \"\n                         \"a 'remainder' transformer.\", ct.get_feature_names)\n", "def test_column_transformer_with_n_jobs():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # single column 1D / 2D\n    ct = ColumnTransformer([('trans', Trans(), [0])], n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), np.array([0, 1, 2]).reshape(-1, 1))\n\n    # multiple columns\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])], n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n\n    # remainder passthrough\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder='passthrough', n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n\n    # remainder drop\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder='drop', n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), np.array([0, 1, 2]).reshape(-1, 1))\n", "def test_column_transformer_get_feature_names_remainder_passthrough():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder='passthrough')\n    with pytest.raises(NotImplementedError):\n        ct.fit(X_array).get_feature_names()\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # remainder is passthrough\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder='passthrough')\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), ['trans1__x0', 'remainder__x1'])\n\n    # remainder is drop\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder='drop')\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), ['trans1__x0'])\n\n    # remainder is a transformer\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), ['trans1__x0', 'remainder__x0'])\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).astype('float').T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n\n    ct.fit(X_array)\n    feature_names = ct.get_feature_names()\n    assert feature_names == ['trans1__x0', 'remainder__x1', 'remainder__x2']\n"], "sample_750": ["def test_omp_n_iter():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5, fit_intercept=False,\n                                    normalize=False)\n    omp.fit(X, y[:, 0])\n    assert_equal(omp.n_iter_, 5)\n\n    omp = OrthogonalMatchingPursuit(tol=1.0, fit_intercept=False,\n                                    normalize=False)\n    omp.fit(X, y[:, 0])\n    assert_greater_equal(omp.n_iter_, 1)\n", "def test_omp_cv_with_multiple_targets():\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y)\n    assert_equal(ompcv.n_nonzero_coefs_, n_nonzero_coefs)\n    assert_array_almost_equal(ompcv.coef_, gamma)\n    omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=False,\n                                    n_nonzero_coefs=ompcv.n_nonzero_coefs_)\n    omp.fit(X, y)\n    assert_array_almost_equal(ompcv.coef_, omp.coef_)\n", "def test_omp_cv_multiple_targets():\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y)\n    assert_equal(ompcv.coef_.shape, (n_targets, n_features))\n    for i in range(n_targets):\n        omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=False,\n                                        n_nonzero_coefs=ompcv.n_nonzero_coefs_)\n        omp.fit(X, y[:, i])\n        assert_array_almost_equal(ompcv.coef_[i], omp.coef_)\n", "def test_omp_n_iter():\n    y_ = y[:, 0]\n    omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=False,\n                                    n_nonzero_coefs=5)\n    omp.fit(X, y_)\n    assert_equal(omp.n_iter_, 5)\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y_)\n    assert_array_equal(ompcv.n_iter_, np.repeat(5, n_targets))\n", "def test_omp_n_iter():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5, fit_intercept=False,\n                                    normalize=False)\n    omp.fit(X, y[:, 0])\n    assert_equal(omp.n_iter_, 5)\n    omp.fit(X, y)\n    assert_array_equal(omp.n_iter_, np.repeat(5, n_targets))\n"], "sample_751": ["def test_random_trees_embedding():\n    # Check random trees embedding on iris dataset.\n    from sklearn.ensemble import RandomTreesEmbedding\n    from sklearn.preprocessing import OneHotEncoder\n\n    n_estimators = 5\n    for max_depth in [None, 5]:\n        rt = RandomTreesEmbedding(n_estimators=n_estimators, max_depth=max_depth,\n                                  random_state=0)\n        rt.fit(iris.data)\n        assert_equal(rt.transform(iris.data).shape[1], rt.n_features_)\n        assert_equal(rt.apply(iris.data).shape[1], n_estimators)\n\n        enc = OneHotEncoder()\n        encoded = enc.fit_transform(rt.apply(iris.data))\n        assert_array_equal(encoded.toarray(), rt.transform(iris.data))\n\n        rt_sparse = RandomTreesEmbedding(n_estimators=n_estimators,\n                                         max_depth=max_depth, sparse_output=True,\n                                         random_state=0)\n        rt_sparse.fit(iris.data)\n        assert_equal(rt_sparse.transform(iris.data).shape[1], rt_sparse.n_features_)\n        assert_equal(rt_sparse.apply(iris.data).shape[1], n_estimators)\n\n        enc_sparse = OneHotEncoder(sparse=True)\n        encoded_sparse = enc_sparse.fit_transform(rt_sparse.apply(iris.data))\n        assert_array_equal(encoded_sparse.toarray(), rt_sparse.transform(iris.data).toarray())\n", "def test_sparse_matrix_feature_importances():\n    # Check feature importances with sparse input.\n\n    class CustomSVR(SVR):\n        \"\"\"SVR variant that records the nature of the training set.\"\"\"\n\n            \"\"\"Modification on fit caries data type for later verification.\"\"\"\n            super(CustomSVR, self).fit(X, y, sample_weight=sample_weight)\n            self.data_type_ = type(X)\n            return self\n\n    X, y = datasets.make_regression(n_samples=15, n_features=50, n_targets=1,\n                                    random_state=42)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    for sparse_format in [csc_matrix, csr_matrix, lil_matrix, coo_matrix,\n                          dok_matrix]:\n        X_train_sparse = sparse_format(X_train)\n        X_test_sparse = sparse_format(X_test)\n\n        # Trained on sparse format\n        sparse_classifier = AdaBoostRegressor(\n            base_estimator=CustomSVR(gamma='scale'),\n            random_state=1\n        ).fit(X_train_sparse, y_train)\n\n        # Check feature importances\n        assert_equal(len(sparse_classifier.feature_importances_), 50)\n\n        types = [i.data_type_ for i in sparse_classifier.estimators_]\n\n        assert all([(t == csc_matrix or t == csr_matrix)\n                   for t in types])\n", "def test_feature_importances():\n    # Check feature importances.\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=10,\n                                        n_informative=3,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert_equal(importances.shape[0], 10)\n    assert_equal((importances[:3, np.newaxis] >= importances[3:]).all(),\n                 True)\n\n    reg = RandomForestRegressor(n_estimators=10, random_state=0)\n    reg.fit(X, y)\n    importances = reg.feature_importances_\n\n    assert_equal(importances.shape[0], 10)\n    assert_equal((importances[:3, np.newaxis] >= importances[3:]).all(),\n                 True)\n", "def test_base_estimator_with_sample_weight():\n    # Test that the base estimator's sample weight handling is respected\n\n    class DummyEstimator(BaseEstimator):\n\n            if sample_weight is not None:\n                self.sample_weight_ = sample_weight\n            else:\n                self.sample_weight_ = np.ones_like(y)\n\n            return np.zeros(X.shape[0])\n\n    boost = AdaBoostClassifier(DummyEstimator(), n_estimators=3)\n    sample_weight = np.random.rand(len(X))\n    boost.fit(X, y_class, sample_weight=sample_weight)\n\n    for estimator in boost.estimators_:\n        assert_array_equal(estimator.sample_weight_, sample_weight)\n", "def test_early_stopping():\n    # Check that early stopping is working as expected.\n    clf = AdaBoostClassifier(n_estimators=50, random_state=0)\n    clf.fit(X, y_class)\n\n    # With early stopping, we should have less estimators\n    clf = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=0)\n    clf.fit(X, y_class)\n    assert len(clf.estimators_) < 50\n\n    # Without early stopping, we should have all estimators\n    clf = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=0,\n                             algorithm='SAMME')\n    clf.fit(X, y_class)\n    assert len(clf.estimators_) == 50\n"], "sample_752": ["def test_iforest_fit_with_sample_weight():\n    \"\"\"Test Isolation Forest with sample weights.\"\"\"\n    X = iris.data\n    sample_weight = np.ones(X.shape[0])\n    sample_weight[:10] = 2\n\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X, sample_weight=sample_weight)\n\n    assert_array_equal(clf.score_samples(X), -clf.decision_function(X))\n", "def test_iforest_contamination():\n    # Test that the contamination parameter is handled correctly\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n\n    # Test with float contamination\n    clf = IsolationForest(contamination=0.25, random_state=rng)\n    clf.fit(X)\n    assert_almost_equal(clf.offset_, np.percentile(clf.score_samples(X), 25))\n\n    # Test with 'auto' contamination\n    clf = IsolationForest(contamination='auto', random_state=rng)\n    clf.fit(X)\n    assert_almost_equal(clf.offset_, -0.5)\n\n    # Test with invalid contamination\n    assert_raises(ValueError, IsolationForest, contamination=-1)\n    assert_raises(ValueError, IsolationForest, contamination=1.1)\n    assert_raises(ValueError, IsolationForest, contamination='foo')\n", "def test_iforest_n_features_deprecated():\n    X = iris.data\n    clf = IsolationForest(n_features_=X.shape[1])\n    with pytest.raises(ValueError):\n        clf.fit(X)\n", "def test_iforest_n_features_decreases():\n    # It tests non-regression for #13655 which failed at predict when the\n    # number of features of X is less than max_features.\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n    clf = IsolationForest(max_features=0.8)\n    clf.fit(X_train[:, :-1], y_train)\n    clf.predict(X_test[:, :-1])\n", "def test_iforest_check_is_fitted():\n    X = iris.data\n    clf = IsolationForest()\n    assert_raises(ValueError, clf.decision_function, X)\n    assert_raises(ValueError, clf.predict, X)\n    assert_raises(ValueError, clf.score_samples, X)\n"], "sample_753": ["def test_logistic_regression_predict_proba_ovr():\n    # Test that OvR LogisticRegression predict_proba agree with\n    # predict for a simple case.\n    X = [[1, 0], [1, 1], [0, 1]]\n    y = [1, 0, 1]\n    clf = LogisticRegression(solver='liblinear', multi_class='ovr')\n    clf.fit(X, y)\n    prob = clf.predict_proba(X)\n    assert_array_almost_equal(np.argmax(prob, axis=1), clf.predict(X))\n", "def test_saga_sparse_binary():\n    # Test LogRegCV with solver='saga' works for sparse binary classification\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    clf = LogisticRegression(solver='saga', fit_intercept=False,\n                             max_iter=2000, tol=1e-6)\n    clf.fit(X, y)\n\n    X_sparse = sparse.csr_matrix(X)\n    clf_sparse = LogisticRegression(solver='saga', fit_intercept=False,\n                                    max_iter=2000, tol=1e-6)\n    clf_sparse.fit(X_sparse, y)\n\n    assert_array_almost_equal(clf.coef_, clf_sparse.coef_)\n    assert_array_almost_equal(clf.predict(X), clf_sparse.predict(X_sparse))\n", "def test_multinomial_loss_and_grad():\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0,\n                               n_classes=3, n_informative=5)\n\n    w = np.random.rand(3 * 5)\n    sample_weights = np.ones(X.shape[0])\n\n    loss, grad, _ = _multinomial_loss_grad(w, X, LabelBinarizer().fit_transform(y),\n                                           alpha=1., sample_weight=sample_weights)\n\n    approx_grad = optimize.approx_fprime(\n        w, lambda w: _multinomial_loss(w, X, LabelBinarizer().fit_transform(y),\n                                       alpha=1., sample_weight=sample_weights)[0],\n        1e-3\n    )\n    assert_array_almost_equal(grad, approx_grad, decimal=2)\n", "def test_warm_start_equal_fit():\n    # Test that fitting a model twice is equivalent to fitting once with double\n    # the number of iterations.\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    n_iter = 5\n\n    lr_ws = LogisticRegression(max_iter=n_iter, warm_start=True,\n                               solver='sag', random_state=42)\n    lr_ws.fit(X, y)\n    lr_ws.fit(X, y)\n\n    lr_no_ws = LogisticRegression(max_iter=2 * n_iter, warm_start=False,\n                                  solver='sag', random_state=42)\n    lr_no_ws.fit(X, y)\n\n    assert_array_almost_equal(lr_ws.coef_, lr_no_ws.coef_)\n", "def test_logistic_regression_multiclass_refit():\n    # Test that the refit parameter of LogisticRegressionCV works with multiclass\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n    clf = LogisticRegressionCV(solver='lbfgs', multi_class='multinomial',\n                               max_iter=1000, cv=2, refit=True)\n    clf.fit(X, y)\n    assert_equal(clf.coef_.shape, (3, 20))\n    assert_array_almost_equal(np.unique(y), clf.classes_)\n"], "sample_754": ["def test_sparse_pca_verbose(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng, verbose=10,\n                     normalize_components=norm_comp)\n    U = spca.fit_transform(X)\n    assert_equal(spca.components_.shape, (8, 10))\n    assert_equal(U.shape, (12, 8))\n", "def test_sparse_pca_initialization_with_U_init_V_init_none(norm_comp):\n    rng = np.random.RandomState(0)\n    model = SparsePCA(n_components=3, U_init=None, V_init=None,\n                      random_state=rng, normalize_components=norm_comp)\n    X = rng.randn(5, 4)\n    model.fit(X)\n    assert_equal(model.components_.shape, (3, 4))\n", "def test_sparse_pca_raises(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    with pytest.raises(ValueError):\n        SparsePCA(n_components=0, random_state=rng,\n                  normalize_components=norm_comp).fit(X)\n    with pytest.raises(ValueError):\n        SparsePCA(max_iter=-1, random_state=rng,\n                  normalize_components=norm_comp).fit(X)\n    with pytest.raises(ValueError):\n        SparsePCA(tol=-1, random_state=rng,\n                  normalize_components=norm_comp).fit(X)\n", "def test_sparse_pca_transform_input_validation(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(X)\n\n    # Test that transform method raises error for invalid input\n    with pytest.raises(ValueError):\n        spca.transform(np.random.randn(12))\n\n    with pytest.raises(ValueError):\n        spca.transform(np.random.randn(12, 11))\n\n    with pytest.raises(TypeError):\n        spca.transform('invalid_input')\n", "def test_sparse_pca_error(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(X)\n    assert_equal(len(spca.error_), spca.n_iter_)\n"], "sample_755": ["def test_silhouette_score_non_default_metric():\n    # Test silhouette score with non-default metric\n    from sklearn.metrics.pairwise import pairwise_distances\n\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    # Calculate the distance matrix using the 'manhattan' metric\n    D_manhattan = pairwise_distances(X, metric='manhattan')\n\n    # Calculate the silhouette score using the precomputed 'manhattan' distances\n    score_manhattan_precomputed = silhouette_score(D_manhattan, y, metric='precomputed')\n\n    # Calculate the silhouette score directly using the 'manhattan' metric\n    score_manhattan_direct = silhouette_score(X, y, metric='manhattan')\n\n    # Assert that the scores are equal\n    pytest.approx(score_manhattan_precomputed, score_manhattan_direct)\n", "def test_silhouette_samples_returned_shape():\n    # Assert silhouette_samples returns an array with shape (n_samples,)\n    dataset = datasets.load_iris()\n    X = dataset.data\n    labels = dataset.target\n    sil_samples = silhouette_samples(X, labels)\n    assert sil_samples.shape == (X.shape[0],)\n", "def test_calinski_harabasz_score_non_numpy_arrays():\n    # Test calinski_harabasz_score with non-numpy arrays\n    X = [[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5\n    labels = [0] * 10 + [1] * 10\n    pytest.approx(calinski_harabasz_score(X, labels),\n                  calinski_harabasz_score(np.array(X), np.array(labels)))\n", "def test_calinski_harabasz_score_sparse_input():\n    # Assert the function works with sparse inputs\n    from scipy.sparse import csr_matrix\n    X = csr_matrix([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n                   [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(calinski_harabasz_score(X, labels),\n                  45 * (40 - 4) / (5 * (4 - 1)))\n", "def test_calinski_harabasz_score_sparse_input():\n    # Assert the function works with sparse input\n    X = sp.csr_matrix(np.array([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5))\n    labels = [0] * 10 + [1] * 10\n    pytest.approx(calinski_harabasz_score(X, labels),\n                  45 * (40 - 4) / (5 * (4 - 1)))\n"], "sample_756": ["def test_optics_extract_dbscan_interface():\n    # Test OPTICS extract_dbscan method's interface\n    clust = OPTICS(min_samples=10, algorithm='brute',\n                   metric='euclidean').fit(X)\n\n    with pytest.raises(ValueError, match=\"Specify an epsilon smaller than\"):\n        clust.extract_dbscan(1000.0)\n    \n    with pytest.raises(ValueError, match=\"eps must be a positive float\"):\n        clust.extract_dbscan(-1.0)\n", "def test_optics_extract_dbscan():\n    # Test OPTICS extract_dbscan method returns expected labels\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    # Compute OPTICS\n    clust = OPTICS(min_samples=10).fit(X)\n    labels_dbscan = clust.extract_dbscan(0.3)[1]\n\n    # calculate dbscan labels\n    db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n\n    contingency = contingency_matrix(db.labels_, labels_dbscan)\n    agree = min(np.sum(np.max(contingency, axis=0)),\n                np.sum(np.max(contingency, axis=1)))\n    disagree = X.shape[0] - agree\n\n    # verify core_labels match\n    assert_array_equal(clust.extract_dbscan(0.3)[0], db.core_sample_indices_)\n\n    non_core_count = len(labels_dbscan) - len(db.core_sample_indices_)\n    percent_mismatch = np.round((disagree - 1) / non_core_count, 2)\n\n    # verify label mismatch is <= 5% labels\n    assert percent_mismatch <= 0.05\n", "def test_optics_bad_metric():\n    # Test OPTICS with a bad metric\n    msg = (\"Metric 'foo' not valid for algorithm 'ball_tree'. \"\n           \"Valid metrics are ['l1', 'l2', 'manhattan', 'euclidean', \"\n           \"'minkowski']\")\n\n    X = [[1, 1], [2, 2]]\n    clust = OPTICS(metric='foo')\n    assert_raise_message(ValueError, msg, clust.fit, X)\n", "def test_optics_min_samples():\n    # Test OPTICS with different min_samples values\n    X = [[1, 2], [2, 2], [2, 4], [4, 6], [3, 7], [5, 8]]\n    clust1 = OPTICS(min_samples=2).fit(X)\n    clust2 = OPTICS(min_samples=3).fit(X)\n    assert_array_equal(clust1.labels_, clust2.labels_)\n    assert_array_equal(clust1.core_sample_indices_, clust2.core_sample_indices_)\n    assert_allclose(clust1.reachability_, clust2.reachability_)\n    assert_allclose(clust1.core_distances_, clust2.core_distances_)\n    assert_array_equal(clust1.ordering_, clust2.ordering_)\n    assert_array_equal(clust1.predecessor_, clust2.predecessor_)\n\n    clust3 = OPTICS(min_samples=6).fit(X)\n    assert_array_equal(clust3.labels_, np.array([-1] * len(X)))\n    assert_array_equal(clust3.core_sample_indices_, np.array([]))\n    assert_allclose(clust3.reachability_, np.full(len(X), np.inf))\n    assert_allclose(clust3.core_distances_, np.full(len(X), np.inf))\n    assert_array_equal(clust3.ordering_, np.arange(len(X)))\n    assert_array_equal(clust3.predecessor_, np.full(len(X), -1))\n", "def test_optics_fit_x_none():\n    # Test that the fit method raises a ValueError when X is None\n\n    clust = OPTICS(min_samples=5)\n    msg = \"OPTICS.fit called on an empty array\"\n    with pytest.raises(ValueError, match=msg):\n        clust.fit(None)\n"], "sample_757": ["def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 'b'], ['b', 'a']])\n    enc = OrdinalEncoder(handle_unknown='error')\n    enc.fit(X)\n\n    # test that an error is raised when an unknown category is encountered\n    X2 = np.array([['c', 'b'], ['b', 'a']])\n    msg = \"Found unknown categories\"\n    assert_raises_regex(ValueError, msg, enc.transform, X2)\n\n    # test that handle_unknown='ignore' ignores unknown categories\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2_tr = enc.transform(X2)\n    exp = np.array([[0, 1]], dtype='float64')\n    assert_array_equal(X2_tr, exp)\n", "def test_one_hot_encoder_get_feature_names_unsorted_categories():\n    enc = OneHotEncoder(categories=[['b', 'a']])\n    X = [['b'], ['a']]\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0_b', 'x0_a'], feature_names)\n\n    feature_names2 = enc.get_feature_names(['one'])\n    assert_array_equal(['one_b', 'one_a'], feature_names2)\n", "def test_ordinal_encoder_dtype():\n    X = np.array([[1, 2], [3, 4]], dtype='int64')\n    enc = OrdinalEncoder(dtype='int32')\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == 'int32'\n    enc = OrdinalEncoder(dtype='float64')\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == 'float64'\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 2], ['b', 1]])\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X)\n    \n    # unknown category in new data\n    X2 = np.array([['c', 2]])\n    msg = \"Found unknown categories\"\n    with pytest.raises(ValueError, match=msg):\n        enc.transform(X2)\n\n    # handle unknown by ignoring it\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2_tr = enc.transform(X2)\n    assert_array_equal(X2_tr, [[np.nan, 1.]])\n"], "sample_758": ["def test_check_X_y_multi_output():\n    X = np.ones((2, 2))\n    y = np.array([[1, 2], [3, 4]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    y = sp.csr_matrix(y)\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked.toarray(), y.toarray())\n\n    y = [[1, 2], [3, 4]]\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, np.array(y))\n\n    # check that it raises an error for y with > 2 dimensions\n    y = np.array([[[1, 2]], [[3, 4]]])\n    assert_raise_message(ValueError, \"bad input shape (2, 1, 2)\", check_X_y, X, y, multi_output=True)\n", "def test_check_array_dtype_list():\n    X = np.array([[1, 2], [3, 4]])\n    X_checked = check_array(X, dtype=[np.float32, np.float64])\n    assert_equal(X_checked.dtype, np.float64)\n\n    X_checked = check_array(X, dtype=[np.int32, np.float64])\n    assert_equal(X_checked.dtype, np.int32)\n\n    X_checked = check_array(X, dtype=[np.int32, np.int64])\n    assert_equal(X_checked.dtype, np.int32)\n", "def test_check_X_y_length_mismatch():\n    X = np.ones((3, 2))\n    y = np.ones(4)\n    msg = (\"Found input variables with inconsistent numbers of samples: \"\n           \"[3, 4]\")\n    assert_raise_message(ValueError, msg, check_X_y, X, y)\n", "def test__is_arraylike():\n    # Test that _is_arraylike returns True for array-like objects\n    assert_true(_is_arraylike([1, 2, 3]))\n    assert_true(_is_arraylike(np.array([1, 2, 3])))\n    assert_true(_is_arraylike(sp.csr_matrix([1, 2, 3])))\n\n    # Test that _is_arraylike returns False for non-array-like objects\n    assert_false(_is_arraylike(\"hello\"))\n    assert_false(_is_arraylike(123))\n    assert_false(_is_arraylike(None))\n", "def test_check_X_y_multi_output_with_missing_values():\n    X = np.ones((2, 2))\n    y = np.array([[1, 2], [3, np.nan]])\n    assert_raises_regex(ValueError, \"Multi-output target data is a sparse matrix\",\n                        check_X_y, X, y, multi_output=True)\n    y = np.array([[1, 2], [np.nan, 4]])\n    assert_raises_regex(ValueError, \"Multi-output target data is a sparse matrix\",\n                        check_X_y, X, y, multi_output=True)\n    y = sp.csr_matrix(np.array([[1, 2], [np.nan, 4]]))\n    assert_raises_regex(ValueError, \"Multi-output target data is a sparse matrix\",\n                        check_X_y, X, y, multi_output=True)\n"], "sample_759": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_dtype():\n    # check for correct output dtype\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder(dtype='int64')\n    exp = np.array([[0, 1, 0],\n                    [1, 0, 0]], dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp)\n\n    enc = OrdinalEncoder(dtype='float64')\n    exp = np.array([[0, 1, 0],\n                    [1, 0, 0]], dtype='float64')\n    assert_array_equal(enc.fit_transform(X), exp)\n", "def test_ordinal_encoder_get_params():\n    enc = OrdinalEncoder(categories=[[1, 2], [3, 4]])\n    params = enc.get_params()\n    assert 'categories' in params\n    assert_array_equal(params['categories'], [[1, 2], [3, 4]])\n\n    # Test that setting the categories parameter works as expected\n    enc.set_params(categories=[[5, 6], [7, 8]])\n    params = enc.get_params()\n    assert_array_equal(params['categories'], [[5, 6], [7, 8]])\n", "def test_ordinal_encoder_set_params():\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[[0, 1, 2, 3]])\n    assert enc.get_params()['categories'] == [[0, 1, 2, 3]]\n    assert enc.fit_transform(X).shape == (2, 1)\n    # set params on already fitted object\n    enc.set_params(categories=[[0, 1, 2, 3, 4]])\n    assert enc.fit_transform(X).shape == (2, 1)\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n"], "sample_760": ["def test_make_scorer_with_kwargs():\n    # Test that make_scorer can handle keyword arguments.\n    f = lambda y_true, y_pred, **kwargs: 0\n    scorer = make_scorer(f, greater_is_better=True, needs_proba=False,\n                         needs_threshold=False, some_kwarg='some_value')\n    assert scorer._kwargs == {'some_kwarg': 'some_value'}\n    assert scorer._score_func is f\n", "def test_make_scorer_with_kwargs():\n    # Test that make_scorer can handle keyword arguments.\n    f = lambda y_true, y_pred, **kwargs: 0\n    scorer = make_scorer(f, greater_is_better=True, needs_proba=False,\n                         needs_threshold=False, foo='bar')\n    assert scorer._kwargs == {'foo': 'bar'}\n    assert scorer._score_func is f\n\n    # Test that the scorer uses the provided keyword arguments.\n    f = lambda y_true, y_pred, foo: foo\n    scorer = make_scorer(f, greater_is_better=True, needs_proba=False,\n                         needs_threshold=False, foo='bar')\n    estimator = EstimatorWithFitAndPredict()\n    estimator.fit([[1]], [1])\n    score = scorer(estimator, [[1]], [1])\n    assert score == 'bar'\n", "def test_make_scorer_with_kwargs():\n    # Test that make_scorer can handle kwargs\n    f1_scorer = make_scorer(f1_score, pos_label=1, average='macro')\n    score1 = f1_scorer(LogisticRegression(), [[1], [2]], [1, 0])\n    score2 = f1_score([1, 0], LogisticRegression().fit([[1], [2]], [1, 0]).predict([[1], [2]]), \n                      pos_label=1, average='macro')\n    assert_almost_equal(score1, score2)\n", "def test_make_scorer_kwargs():\n    # Test that make_scorer passes kwargs to the score function.\n        return kwargs['foo']\n\n    scorer = make_scorer(score_func, greater_is_better=True, needs_proba=False,\n                         foo='bar')\n    assert scorer._kwargs == {'foo': 'bar'}\n    assert scorer(None, None, None) == 'bar'\n", "def test_make_scorer_needs_proba():\n    # Test that needs_proba is properly passed to the scorer\n    scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n    assert isinstance(scorer, _ProbaScorer)\n\n    scorer = make_scorer(accuracy_score, needs_proba=False)\n    assert not isinstance(scorer, _ProbaScorer)\n\n    # Test that an error is raised when needs_proba is True but the scorer\n    # does not support proba\n    with pytest.raises(ValueError):\n        make_scorer(mean_squared_error, needs_proba=True)\n"], "sample_761": ["def test_imputation_add_indicator_sparse_matrix_error():\n    # Test that add_indicator raises an error when the sparse matrix has missing values represented as 0.\n    X_sparse = sparse.csc_matrix([\n        [0, 1, 5],\n        [2, 0, 1],\n        [6, 3, 0],\n        [1, 2, 9]\n    ])\n\n    imputer = SimpleImputer(missing_values=0, add_indicator=True)\n\n    with pytest.raises(ValueError, match=\"Imputation not possible when missing_values == 0 and input is sparse\"):\n        imputer.fit_transform(X_sparse)\n", "def test_iterative_imputer_min_value_parameter():\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10,\n                             random_state=rng).toarray()\n\n    imputer = IterativeImputer(missing_values=0,\n                               max_iter=1,\n                               min_value=None,\n                               random_state=rng)\n\n    Xt = imputer.fit_transform(X)\n    assert np.min(Xt) < 0\n\n    imputer = IterativeImputer(missing_values=0,\n                               max_iter=1,\n                               min_value=0,\n                               random_state=rng)\n\n    Xt = imputer.fit_transform(X)\n    assert np.min(Xt) >= 0\n", "def test_iterative_imputer_n_nearest_features():\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n    imputer = IterativeImputer(missing_values=0,\n                               max_iter=1,\n                               n_nearest_features=5,\n                               sample_posterior=False,\n                               min_value=0,\n                               max_value=1,\n                               verbose=1,\n                               imputation_order='ascending',\n                               random_state=rng)\n\n    imputer.fit(X)\n    # check that the number of features used to impute is correct\n    for triplet in imputer.imputation_sequence_:\n        assert len(triplet.neighbor_feat_idx) == imputer.n_nearest_features\n", "def test_iterative_imputer_fit_resample():\n    # Test that IterativeImputer.fit_resample returns the same result as\n    # IterativeImputer.fit_transform when there are no categorical features.\n\n    rng = np.random.RandomState(0)\n\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n    missing_flag = X == 0\n    X[missing_flag] = np.nan\n\n    imputer = IterativeImputer(max_iter=0)\n    X_imputed_fit_transform = imputer.fit_transform(X)\n\n    imputer = IterativeImputer(max_iter=0)\n    X_imputed_fit_resample, _ = imputer.fit_resample(X)\n\n    assert_allclose(X_imputed_fit_transform, X_imputed_fit_resample)\n", "def test_iterative_imputer_n_nearest_features():\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n    missing_flag = X == 0\n    X[missing_flag] = np.nan\n\n    imputer = IterativeImputer(max_iter=1, n_nearest_features=5)\n    X_imputed = imputer.fit_transform(X)\n\n    assert len(imputer.imputation_sequence_) == imputer.n_features_with_missing_\n    for triplet in imputer.imputation_sequence_:\n        assert len(triplet.neighbor_feat_idx) == 5\n"], "sample_762": ["def test_is_regressor():\n    clf = DecisionTreeClassifier()\n    assert_false(is_regressor(clf))\n    reg = DecisionTreeRegressor()\n    assert_true(is_regressor(reg))\n    pipe = Pipeline([('reg', reg)])\n    assert_true(is_regressor(pipe))\n    grid_search = GridSearchCV(reg, {})\n    assert_true(is_regressor(grid_search))\n", "def test_base_estimator_repr_multiline():\n    # Test that the repr of BaseEstimator handles multi-line params correctly\n\n    class DummyEstimator(BaseEstimator):\n            self.param = param\n\n    estimator = DummyEstimator(param=[\"value1\", \"value2\", \"value3\"])\n    expected_repr = (\"DummyEstimator(param=['value1', 'value2', 'value3'])\")\n    assert_equal(repr(estimator), expected_repr)\n\n    estimator = DummyEstimator(param={\"key1\": \"value1\", \"key2\": \"value2\"})\n    expected_repr = (\"DummyEstimator(param={'key1': 'value1', 'key2': 'value2'})\")\n    assert_equal(repr(estimator), expected_repr)\n", "def test_set_params_passes_all_parameters_to_nested_estimators():\n    # Make sure all parameters are passed together to set_params\n    # of nested estimators. Regression test for #9944\n\n    class TestDecisionTree(DecisionTreeClassifier):\n            super(TestDecisionTree, self).set_params(**kwargs)\n            # expected_kwargs is in test scope\n            assert kwargs == expected_kwargs\n            return self\n\n    class NestedEstimator(BaseEstimator):\n            self.estimator = estimator\n\n            if 'estimator__' in kwargs:\n                estimator_kwargs = {k.split('__', 1)[1]: v\n                                    for k, v in kwargs.items()\n                                    if k.startswith('estimator__')}\n                self.estimator.set_params(**estimator_kwargs)\n\n    expected_kwargs = {'max_depth': 5, 'min_samples_leaf': 2}\n    nested_estimator = NestedEstimator(estimator=TestDecisionTree())\n    nested_estimator.set_params(estimator__max_depth=5,\n                                estimator__min_samples_leaf=2)\n", "def test_get_params_with_unexpected_kwargs():\n    class DummyEstimator(BaseEstimator):\n            self.expected_param = expected_param\n\n    estimator = DummyEstimator(expected_param=5)\n    params = estimator.get_params()\n    assert 'expected_param' in params\n    assert len(params) == 1\n\n    # Check that an unexpected kwarg is not added to the params\n    estimator.unexpected_param = 42\n    params = estimator.get_params()\n    assert 'unexpected_param' not in params\n    assert len(params) == 1\n", "def test_set_params_passes_all_parameters_to_nested_estimators():\n    # Make sure all parameters are passed together to set_params\n    # of nested estimators.\n\n    class TestDecisionTree(DecisionTreeClassifier):\n            super(TestDecisionTree, self).set_params(**kwargs)\n            # expected_kwargs is in test scope\n            assert kwargs == expected_kwargs\n            return self\n\n    class NestedEstimator(BaseEstimator):\n            self.estimator = estimator\n\n            if 'estimator__' in kwargs:\n                self.estimator.set_params(**{k.split('__', 1)[1]: v for k, v in kwargs.items() if k.startswith('estimator__')})\n            return self\n\n    expected_kwargs = {'max_depth': 5, 'min_samples_leaf': 2}\n    est = NestedEstimator(estimator=TestDecisionTree())\n    est.set_params(estimator__max_depth=5,\n                   estimator__min_samples_leaf=2)\n"], "sample_763": ["def test_check_X_y_multilabel_sequence():\n    X = [[1, 2], [3, 4]]\n    y = [[0, 1], [1, 0]]\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, np.array(X))\n    assert_array_equal(y_checked, np.array(y))\n\n    X = [[1, 2], [3, 4]]\n    y = [(0, 1), (1, 0)]\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, np.array(X))\n    assert_array_equal(y_checked, np.array(y))\n", "def test_check_X_y_warns_message():\n    # check that X and y validation returns a meaningful message when \n    # there is a mismatch between the lengths of X and y.\n    X = np.ones((10, 2))\n    y = np.ones(9)\n    expected_warn_msg = (\"Input variables with inconsistent numbers of \"\n                         \"samples: [10, 9]\")\n    assert_raise_message(ValueError, expected_warn_msg,\n                         check_X_y, X, y)\n", "def test_check_array_dtype_object_with_floats():\n    # Test that check_array with dtype 'object' does not convert float arrays to object arrays\n    X = np.array([[1.0, 2.0], [3.0, 4.0]])\n    X_checked = check_array(X, dtype='object')\n    assert_equal(X_checked.dtype, np.float64)\n", "def test_check_array_dtype_object():\n    # Test that check_array with dtype='object' doesn't convert the input to\n    # float\n    X = np.array([['a', 'b'], ['c', 'd']], dtype=object)\n    X_checked = check_array(X, dtype='object')\n    assert X_checked.dtype == object\n    assert_array_equal(X_checked, X)\n\n    # Same test but with a pandas DataFrame\n    pd = importorskip(\"pandas\")\n    X_df = pd.DataFrame(X)\n    X_checked = check_array(X_df, dtype='object)\n    assert X_checked.dtype == object\n    assert_array_equal(X_checked, X_df.values)\n", "def test_check_X_y_length_mismatch():\n    X = np.ones((2, 2))\n    y = np.array([1, 2, 3])\n    assert_raise_message(ValueError, \"Found input variables with inconsistent\"\n                         \" numbers of samples: [2, 3]\", check_X_y, X, y)\n"], "sample_764": ["def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n\n    with pytest.raises(NotImplementedError):\n        ct.fit(X_array).get_feature_names()\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n\n    exp = ['trans__x0', 'remainder__x1']\n    assert_equal(ct.get_feature_names(), exp)\n", "def test_column_transformer_transformer_weights_error():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # invalid weight type\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           transformer_weights={'trans': 'string'})\n    with pytest.raises(ValueError,\n                       match=\"transformer_weights should be a dictionary of\"):\n        ct.fit(X_array)\n\n    # not all transformers have weights\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': 10})\n    with pytest.raises(ValueError,\n                       match=\"transformer_weights should have an entry for\"):\n        ct.fit(X_array)\n", "def test_column_transformer_sparse_threshold_zero():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    col_trans = ColumnTransformer([('trans1', Trans(), [0]),\n                                   ('trans2', SparseMatrixTrans(), 1)],\n                                  sparse_threshold=0)\n    col_trans.fit(X_array)\n    X_trans = col_trans.transform(X_array)\n    assert not sparse.issparse(X_trans)\n    assert X_trans.shape == (X_trans.shape[0], X_trans.shape[0] + 1)\n    assert_array_equal(X_trans[:, 1:], np.eye(X_trans.shape[0]))\n", "def test_column_transformer_remainder_with_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_second = np.array([2, 4, 6]).reshape(-1, 1)\n    X_res_both = X_array\n\n    transformer_weights = {'trans1': .1, 'remainder': 10}\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder='passthrough',\n                           transformer_weights=transformer_weights)\n\n    exp = np.hstack([transformer_weights['trans1'] * X_res_first,\n                     transformer_weights['remainder'] * X_res_second])\n    assert_array_equal(ct.fit_transform(X_array), exp)\n    assert_array_equal(ct.fit(X_array).transform(X_array), exp)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans(),\n                           transformer_weights=transformer_weights)\n\n    exp = np.hstack([transformer_weights['trans1'] * X_res_first,\n                     transformer_weights['remainder'] * 2 * X_res_second])\n    assert_array_equal(ct.fit_transform(X_array), exp)\n    assert_array_equal(ct.fit(X_array).transform(X_array), exp)\n"], "sample_765": ["def test_balanced_accuracy_score_single_class():\n    y_true = [1, 1, 1]\n    y_pred = [1, 1, 1]\n    assert balanced_accuracy_score(y_true, y_pred) == 1.0\n    assert balanced_accuracy_score(y_true, y_pred, adjusted=True) == 1.0\n", "def test_multilabel_confusion_matrix_with_no_predicted_samples():\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[0, 0, 0], [0, 0, 0]])\n\n    with warnings.catch_warnings(record=True) as record:\n        warnings.simplefilter('always')\n        cm = multilabel_confusion_matrix(y_true, y_pred)\n        assert_equal(len(record), 3)\n\n    expected_cm = [[[0, 1], [1, 0]], [[0, 1], [0, 0]], [[0, 1], [0, 0]]]\n    assert_array_equal(cm, expected_cm)\n", "def test_balanced_accuracy_score_adjusted():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 1, 1, 0, 2, 2]\n\n    # Calculate the chance accuracy\n    n_classes = len(set(y_true))\n    chance = 1 / n_classes\n\n    # Calculate the adjusted balanced accuracy score\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n\n    # Calculate the adjusted score manually\n    recall = recall_score(y_true, y_pred, average='macro')\n    adjusted_manual = (recall - chance) / (1 - chance)\n\n    assert adjusted == pytest.approx(adjusted_manual)\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 1, 1, 0, 2, 2]\n\n    # we can compute the \"balanced_accuracy\" for this case manually:\n    recall_0 = 2 / 2\n    recall_1 = 1 / 2\n    recall_2 = 1 / 2\n\n    balanced_accuracy = (recall_0 + recall_1 + recall_2) / 3\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred),\n                        balanced_accuracy)\n", "def test_zero_one_loss():\n    # Check zero_one_loss function\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0, 1, 0, 1, 1, 0])\n\n    assert_equal(zero_one_loss(y_true, y_true), 0.0)\n    assert_almost_equal(zero_one_loss(y_true, y_pred), 0.5)\n\n    # check that only integer labels are allowed\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0.4, 0.7, 0.3, 0.8, 0.9, 0.2])\n    assert_raises(ValueError, zero_one_loss, y_true, y_pred)\n\n    # calculate even if only single class in y_true (#6980)\n    assert_equal(zero_one_loss([0], [0]), 0)\n    assert_equal(zero_one_loss([0], [1]), 1)\n"], "sample_766": ["def test_sparse_coder_transform_max_iter():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n\n    X_ = X.copy()\n    X_[:, 0] += 1000\n\n    sc = SparseCoder(dictionary=V, transform_algorithm='lasso_cd',\n                     transform_alpha=0.001, transform_max_iter=1)\n    code = sc.transform(X_)\n    assert not np.all(code == 0)\n\n    sc = SparseCoder(dictionary=V, transform_algorithm='lasso_cd',\n                     transform_alpha=0.001, transform_max_iter=10000)\n    code = sc.transform(X_)\n    assert not np.all(code == 0)\n", "def test_dict_learning_online_n_iter():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    dico = MiniBatchDictionaryLearning(n_components, n_iter=10,\n                                       dict_init=V, random_state=0).fit(X)\n    assert_equal(dico.n_iter_, 10)\n    assert_equal(dico.iter_offset_, 10)\n", "def test_sparse_coder_input():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    Xf = check_array(X, order='F')\n    for algo in ('lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'):\n        a = SparseCoder(dictionary=V, transform_algorithm=algo).transform(X)\n        b = SparseCoder(dictionary=V, transform_algorithm=algo).transform(Xf)\n        assert_array_almost_equal(a, b)\n", "def test_sparse_coder_estimator_positive_code():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    code = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',\n                       positive_code=True).transform(X)\n    assert (code >= 0).all()\n", "def test_sparse_coder_n_jobs():\n    # Test that SparseCoder works with different n_jobs values\n\n    rng = np.random.RandomState(777)\n    n_components, n_features = 40, 64\n    init_dict = rng.rand(n_components, n_features)\n    data = np.random.rand(100, n_features)\n\n    for n_jobs in [1, 2, -1]:\n        sc = SparseCoder(init_dict, transform_algorithm='omp', n_jobs=n_jobs)\n        code = sc.fit_transform(data)\n        assert code.shape == (100, n_components)\n"], "sample_767": ["def test_column_transformer_empty_dataframe():\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(columns=['first', 'second'])\n    ct = ColumnTransformer([('trans', Trans(), ['first'])])\n    assert_array_equal(ct.fit_transform(X_df), np.zeros((0, 1)))\n    assert_array_equal(ct.fit(X_df).transform(X_df), np.zeros((0, 1)))\n", "def test_column_transformer_none_transformer():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    with pytest.raises(TypeError):\n        ColumnTransformer([('trans1', None, [0])]).fit(X_array)\n\n    with pytest.raises(TypeError):\n        ColumnTransformer([('trans1', None, [0])]).fit_transform(X_array)\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n\n    ct.fit(X_array)\n\n    exp = ['trans1__x0', 'remainder__x1', 'remainder__x2']\n\n    assert_equal(ct.get_feature_names(), exp)\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n\n    exp = ['trans__x0', 'remainder__x1']\n    assert_equal(ct.get_feature_names(), exp)\n", "def test_column_transformer_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    transformer_weights = {'trans1': .1, 'trans2': 10}\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights=transformer_weights)\n\n    # check attribute is correctly set\n    assert_dict_equal(ct.transformer_weights, transformer_weights)\n\n    # apply weights during transform\n    res = np.vstack([transformer_weights['trans1'] * X_array[:, 0],\n                     transformer_weights['trans2'] * X_array[:, 1]]).T\n    assert_array_equal(ct.fit_transform(X_array), res)\n    assert_array_equal(ct.fit(X_array).transform(X_array), res)\n\n    # ensure weights are applied to remainder\n    remainder_weight = .2\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder='passthrough',\n                           transformer_weights={'trans': 1,\n                                                'remainder': remainder_weight})\n    res = np.hstack([X_array[:, 0].reshape(-1, 1),\n                     remainder_weight * X_array[:, 1].reshape(-1, 1)])\n    assert_array_equal(ct.fit_transform(X_array), res)\n    assert_array_equal(ct.fit(X_array).transform(X_array), res)\n"], "sample_768": ["def test_validate_shuffle_split_init():\n    # Test _validate_shuffle_split_init for valid and invalid inputs\n\n    # Valid inputs\n    assert_no_warnings(_validate_shuffle_split_init, test_size=0.5)\n    assert_no_warnings(_validate_shuffle_split_init, train_size=0.5)\n    assert_no_warnings(_validate_shuffle_split_init, test_size=10)\n    assert_no_warnings(_validate_shuffle_split_init, train_size=10)\n\n    # Invalid inputs\n    msg = \"test_size should be a float or int\"\n    assert_raise_message(ValueError, msg,\n                         _validate_shuffle_split_init, test_size='a')\n\n    msg = \"train_size should be a float or int\"\n    assert_raise_message(ValueError, msg,\n                         _validate_shuffle_split_init, train_size='a')\n\n    msg = \"test_size should be in the (0, 1) range\"\n    assert_raise_message(ValueError, msg,\n                         _validate_shuffle_split_init, test_size=1.2)\n\n    msg = \"train_size should be in the (0, 1) range\"\n    assert_raise_message(ValueError, msg,\n                         _validate_shuffle_split_init, train_size=1.2)\n\n    msg = \"test_size and train_size cannot both be None\"\n    assert_raise_message(ValueError, msg,\n                         _validate_shuffle_split_init, test_size=None,\n                         train_size=None)\n\n    msg = \"test_size and train_size cannot sum to more than 1.0\"\n    assert_raise_message(ValueError, msg,\n                         _validate_shuffle_split_init, test_size=0.6,\n                         train_size=0.6)\n", "def test_validate_shuffle_split_init():\n    # Test that _validate_shuffle_split_init raises appropriate errors\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=None,\n                  train_size=None)\n\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=1.2)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=-0.5)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=0)\n\n    assert_raises(ValueError, _validate_shuffle_split_init, train_size=1.2)\n    assert_raises(ValueError, _validate_shuffle_split_init, train_size=-0.5)\n    assert_raises(ValueError, _validate_shuffle_split_init, train_size=0)\n\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=0.6,\n                  train_size=0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=10,\n                  train_size=20)\n", "def test_validate_shuffle_split_init():\n    # Test _validate_shuffle_split_init function\n    # Should fail if both test_size and train_size are None\n    assert_raises(ValueError, _validate_shuffle_split_init, None, None)\n    # Should fail if test_size is not a float or int or None\n    assert_raises(TypeError, _validate_shuffle_split_init, 'test', 0.5)\n    # Should fail if test_size is not in range (0, 1) when it's a float\n    assert_raises(ValueError, _validate_shuffle_split_init, 1.2, 0.5)\n    assert_raises(ValueError, _validate_shuffle_split_init, -0.2, 0.5)\n    # Should fail if test_size is not positive when it's an integer\n    assert_raises(ValueError, _validate_shuffle_split_init, 0, 0.5)\n    assert_raises(ValueError, _validate_shuffle_split_init, -3, 0.5)\n    # Should fail if train_size is not a float or int or None\n    assert_raises(TypeError, _validate_shuffle_split_init, 0.5, 'train')\n    # Should fail if train_size is not in range (0, 1) when it's a float\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, 1.2)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, -0.2)\n    # Should fail if train_size is not positive when it's an integer\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, 0)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, -3)\n    # Should fail if test_size + train_size > 1\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.7, 0.6)\n    # Should pass if both test_size and train_size are valid\n    _validate_shuffle_split_init(0.5, 0.3)\n", "def test_splitter_repr():\n    # Test that repr works for all splitters\n    splitters = [\n        KFold(),\n        GroupKFold(),\n        StratifiedKFold(),\n        TimeSeriesSplit(),\n        LeaveOneOut(),\n        LeaveOneGroupOut(),\n        LeavePOut(p=2),\n        LeavePGroupsOut(n_groups=2),\n        ShuffleSplit(random_state=0),\n        GroupShuffleSplit(random_state=0),\n        StratifiedShuffleSplit(random_state=0),\n        PredefinedSplit(test_fold=[1, 1, 2, 2]),\n        RepeatedKFold(),\n        RepeatedStratifiedKFold()\n    ]\n\n    for splitter in splitters:\n        assert isinstance(repr(splitter), str)\n        # Make sure repr can be used to recreate the object\n        if 'random_state' in repr(splitter):\n            # Remove random_state parameter as it may not be recreated exactly\n            # due to numpy's RandomState representation\n            repr_splitter = repr(splitter).split('(')[0] + '(random_state=0)'\n        else:\n            repr_splitter = repr(splitter)\n\n        # Check that we can recreate the object from its repr\n        recreated_splitter = eval(repr_splitter)\n        assert type(splitter) == type(recreated_splitter)\n", "def test_validate_shuffle_split():\n    # Test _validate_shuffle_split function\n    n_samples = 100\n    train_size = 0.8\n    test_size = 0.2\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=-1)\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=1.1)\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size='a')\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, train_size=-1)\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, train_size=1.1)\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, train_size='a')\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=0.5, train_size=0.6)\n\n    train_size, test_size = _validate_shuffle_split(n_samples, test_size=0.2)\n    assert train_size + test_size == n_samples\n\n    train_size, test_size = _validate_shuffle_split(n_samples, train_size=0.8)\n    assert train_size + test_size == n_samples\n"], "sample_769": ["def test_balanced_accuracy_score_equal():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 1, 2, 0, 2, 1]\n    score = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(score, 0.55555556)\n", "def test_balanced_accuracy_score_binary():\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 0, 1, 1]\n    accuracy = accuracy_score(y_true, y_pred)\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n    assert balanced_accuracy == pytest.approx(accuracy)\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    chance = balanced_accuracy_score(y_true, np.full_like(y_true, y_true[0]))\n    assert adjusted == (balanced_accuracy - chance) / (1 - chance)\n", "def test_multilabel_confusion_matrix_sparse_inputs():\n    # Test multilabel confusion matrix with sparse inputs\n    from scipy.sparse import csr_matrix\n\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n\n    y_true_sparse = csr_matrix(y_true)\n    y_pred_sparse = csr_matrix(y_pred)\n\n    cm_dense = multilabel_confusion_matrix(y_true, y_pred)\n    cm_sparse = multilabel_confusion_matrix(y_true_sparse, y_pred_sparse)\n\n    assert_array_equal(cm_dense, cm_sparse)\n", "def test_multilabel_confusion_matrix_sparse_input():\n    # Test multilabel confusion matrix with sparse input\n    from scipy.sparse import csr_matrix\n\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n\n    # convert to sparse matrices\n    y_true_sparse = csr_matrix(y_true)\n    y_pred_sparse = csr_matrix(y_pred)\n\n    cm_dense = multilabel_confusion_matrix(y_true, y_pred)\n    cm_sparse = multilabel_confusion_matrix(y_true_sparse, y_pred_sparse)\n\n    assert_array_equal(cm_dense, cm_sparse)\n", "def test_brier_score_loss_sample_weight():\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    sample_weight = np.array([1., 2., 1., 3., 1., 1.])\n\n    true_score = np.average((y_true - y_pred) ** 2, weights=sample_weight)\n\n    assert_almost_equal(brier_score_loss(y_true, y_true, sample_weight), 0.0)\n    assert_almost_equal(brier_score_loss(y_true, y_pred, sample_weight), true_score)\n"], "sample_770": ["def test_silhouette_score_non_finite_values():\n    # Assert Silhouette Coefficient is nan when there are non-finite values\n    X = [[0.], [1.], [1.], [2.], [3.], [3.]]\n    labels = np.array([0, 1, 1, 1, 2, 2])\n    X[0][0] = np.inf\n    assert np.isnan(silhouette_score(X, labels))\n\n    X[0][0] = np.nan\n    assert np.isnan(silhouette_score(X, labels))\n", "def test_silhouette_samples_overflow():\n    # Check for overflow in silhouette_samples function for large inputs\n    X = np.random.rand(1000, 10)\n    labels = np.random.randint(0, 5, size=1000)\n    sil_samples = silhouette_samples(X, labels)\n    assert not np.isnan(sil_samples).any()\n    assert np.all(sil_samples >= -1)\n    assert np.all(sil_samples <= 1)\n", "def test_silhouette_samples_returned_shape():\n    # Assert silhouette_samples returns an array with shape (n_samples,)\n    dataset = datasets.load_iris()\n    X = dataset.data\n    labels = dataset.target\n    silhouette_values = silhouette_samples(X, labels)\n    assert_equal(silhouette_values.shape, (X.shape[0],))\n", "def test_silhouette_samples_sparse_precomputed():\n    # Test silhouette_samples with sparse precomputed distance matrix\n    from scipy.sparse import csr_matrix\n\n    # Create a random sparse distance matrix\n    rng = np.random.RandomState(0)\n    n_samples = 20\n    distances = rng.rand(n_samples, n_samples)\n    distances += distances.T\n    distances = np.triu(distances, k=1)\n    distances_sparse = csr_matrix(distances)\n\n    # Create some labels\n    labels = rng.randint(0, 3, size=n_samples)\n\n    # Calculate the silhouette scores\n    sil_scores_sparse = silhouette_samples(distances_sparse, labels,\n                                           metric='precomputed')\n\n    # Calculate the silhouette scores using the dense distance matrix\n    sil_scores_dense = silhouette_samples(distances + distances.T, labels,\n                                          metric='precomputed')\n\n    # Assert that the results are the same\n    pytest.approx(sil_scores_sparse, sil_scores_dense)\n", "def test_silhouette_score_with_sparse_input():\n    # Test silhouette score with sparse input\n    from scipy.sparse import csr_matrix\n\n    # Create a random sparse matrix\n    np.random.seed(0)\n    X_sparse = csr_matrix(np.random.rand(100, 10))\n\n    # Create random labels\n    labels = np.random.randint(0, 5, size=100)\n\n    # Calculate silhouette score with sparse input\n    score_sparse = silhouette_score(X_sparse, labels)\n\n    # Convert sparse matrix to dense and calculate silhouette score\n    X_dense = X_sparse.toarray()\n    score_dense = silhouette_score(X_dense, labels)\n\n    # Assert that the scores are equal\n    pytest.approx(score_sparse, score_dense)\n"], "sample_771": ["def test_power_transformer_sparse_input():\n    X_sparse = sparse.csr_matrix(np.abs(X_2d))\n    pt = PowerTransformer(method='yeo-johnson')\n    \n    with pytest.raises(TypeError):\n        pt.fit(X_sparse)\n        \n    with pytest.raises(TypeError):\n        pt.transform(X_sparse)\n        \n    with pytest.raises(TypeError):\n        pt.fit_transform(X_sparse)\n        \n    with pytest.raises(TypeError):\n        pt.inverse_transform(X_sparse)\n", "def test_power_transformer_sparse():\n    X_sparse = sparse.csc_matrix(np.abs(X_2d))\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n    assert pt.fit_transform(X_sparse).shape == X_sparse.shape\n    assert pt.inverse_transform(pt.fit_transform(X_sparse)).shape == X_sparse.shape\n", "def test_power_transformer_sparse_input():\n    # check that PowerTransformer supports sparse input\n    X = sparse.csr_matrix(X_2d)\n    pt = PowerTransformer(method='yeo-johnson')\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n\n    # check that inverse_transform also works with sparse input\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert isinstance(X_inv_trans, sparse.csr_matrix)\n\n    # check that fit works with sparse input when copy=False\n    pt = PowerTransformer(method='yeo-johnson', copy=False)\n    pt.fit(X)\n    assert pt.lambdas_.shape[0] == X.shape[1]\n", "def test_power_transformer_dtype():\n    # Ensure the power transformer supports different dtypes\n    X = np.abs(X_2d)\n    for dtype in [np.float16, np.float32, np.float64]:\n        pt = PowerTransformer()\n        X_trans = pt.fit_transform(X.astype(dtype))\n        assert X_trans.dtype == dtype\n        assert pt.lambdas_.dtype == np.float64\n", "def test_power_transformer_dtype():\n    # check that power_transformer handles different dtypes correctly\n    X = np.abs(X_2d).astype(np.float32)\n    pt = PowerTransformer(method='box-cox', standardize=True)\n    X_trans = pt.fit_transform(X)\n\n    assert X.dtype == X_trans.dtype\n    assert pt.lambdas_.dtype == np.float64\n"], "sample_772": ["def check_min_impurity_split_deprecation(name):\n    # Test if min_impurity_split raises deprecation warning\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n    est = ForestEstimator(min_impurity_split=0.1)\n    assert_warns_message(DeprecationWarning, \"min_impurity_decrease\",\n                         est.fit, X, y)\n\n", "def check_n_features_deprecation(name):\n    # Check that n_features deprecation warning is raised.\n\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator()\n    est.fit([[1, 2], [3, 4]], [0, 1])\n    assert_warns_message(FutureWarning,\n                         \"The `n_features_` attribute is deprecated in version \"\n                         \"1.0 and will be removed in version 1.2. Use \"\n                         \"`n_features_in_` instead.\",\n                         getattr, est, 'n_features_')\n    assert hasattr(est, 'n_features_in_')\n    assert est.n_features_in_ == 2\n\n", "def test_max_samples_bootstrap_samples():\n    # Test that bootstrapping samples work when max_samples is not None.\n\n    X, y = iris.data, iris.target\n\n    # Test for classification\n    clf = RandomForestClassifier(n_estimators=10, max_samples=0.5,\n                                 bootstrap=True, random_state=42)\n    clf.fit(X, y)\n\n    # Test for regression\n    reg = RandomForestRegressor(n_estimators=10, max_samples=0.5,\n                                bootstrap=True, random_state=42)\n    reg.fit(X, y)\n\n    # Test for multi-output regression\n    reg_multi = RandomForestRegressor(n_estimators=10, max_samples=0.5,\n                                      bootstrap=True, random_state=42)\n    reg_multi.fit(X, np.vstack((y, y)).T)\n\n    assert_array_equal(clf.n_features_, X.shape[1])\n    assert_array_equal(reg.n_features_, X.shape[1])\n    assert_array_equal(reg_multi.n_features_, X.shape[1])\n\n    # Check if the right number of samples are used\n    assert_equal(clf.estimators_[0].n_samples_, int(0.5 * len(X)))\n    assert_equal(reg.estimators_[0].n_samples_, int(0.5 * len(X)))\n    assert_equal(reg_multi.estimators_[0].n_samples_, int(0.5 * len(X)))\n\n    # Make sure that max_samples is a float between 0 and 1\n    assert_raises(ValueError,\n                  RandomForestClassifier(n_estimators=10, max_samples=1.5,\n                                         bootstrap=True).fit, X, y)\n    assert_raises(ValueError,\n                  RandomForestRegressor(n_estimators=10, max_samples=1.5,\n                                        bootstrap=True).fit, X, y)\n\n    # Make sure that max_samples is at least one\n    assert_raises(ValueError,\n                  RandomForestClassifier(n_estimators=10, max_samples=0,\n                                         bootstrap=True).fit, X, y)\n    assert_raises(ValueError,\n                  RandomForestRegressor(n_estimators=10, max_samples=0,\n                                        bootstrap=True).fit, X, y)\n", "def check_n_features_deprecated(name):\n    # Check that n_features_ attribute is deprecated.\n\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(random_state=0)\n    est.fit(X, y)\n\n    with pytest.warns(DeprecationWarning):\n        est.n_features_\n", "def check_n_features_deprecated(name):\n    # Check that n_features_ attribute is deprecated.\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator()\n    est.fit(X, y)\n    assert_warns(DeprecationWarning, getattr, est, 'n_features_')\n    assert_equal(est.n_features_in_, X.shape[1])\n\n"], "sample_773": ["def test_logistic_regression_solvers_handle_sparse_data():\n    # Test that all solvers handle sparse data correctly.\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    X_sparse = sp.csr_matrix(X)\n\n    for solver in ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf_dense = LogisticRegression(solver=solver, random_state=0)\n        clf_sparse = LogisticRegression(solver=solver, random_state=0)\n        clf_dense.fit(X, y)\n        clf_sparse.fit(X_sparse, y)\n        assert_array_almost_equal(clf_dense.coef_, clf_sparse.coef_)\n", "def test_check_solver_option_invalid_solver():\n    X, y = iris.data, iris.target\n\n    msg = (\"Logistic Regression supports only solvers in ['liblinear', \"\n           \"'newton-cg', 'lbfgs', 'sag', 'saga'], got wrong_name.\")\n    lr = LogisticRegression(solver=\"invalid_solver\", multi_class=\"ovr\")\n    assert_raise_message(ValueError, msg, lr.fit, X, y)\n", "def test_logistic_regression_path_saga_sparse():\n    # Make sure logistic_regression_path with saga solver works for sparse data\n\n    X, y = make_classification(n_samples=200, n_features=20, random_state=0)\n    X[X < 1] = 0\n    X_sparse = sp.csr_matrix(X)\n\n    Cs = np.logspace(-4, 4, 3)\n    coefs_dense, _, _ = _logistic_regression_path(X, y, Cs=Cs,\n                                                  solver='saga',\n                                                  random_state=0)\n    coefs_sparse, _, _ = _logistic_regression_path(X_sparse, y, Cs=Cs,\n                                                   solver='saga',\n                                                   random_state=0)\n\n    assert_array_almost_equal(coefs_dense, coefs_sparse)\n", "def test_logistic_regression_multinomial_proba():\n    # Test that the predicted probabilities are consistent with the target\n    # class labels in the case of a multinomial logistic regression model.\n    X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n                               n_redundant=0, n_classes=3, random_state=0)\n\n    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n    clf.fit(X, y)\n\n    y_pred_proba = clf.predict_proba(X)\n    assert_array_almost_equal(y_pred_proba.sum(axis=1), np.ones(X.shape[0]))\n\n    # Check that the highest probability is assigned to the true class label\n    y_pred_class_idx = np.argmax(y_pred_proba, axis=1)\n    y_class_idx = LabelEncoder().fit_transform(y)\n    assert_greater(np.mean(y_pred_class_idx == y_class_idx), 0.9)\n", "def test_multinomial_logistic_regression_path_convergence():\n    # Test that logistic_regression_path converges to similar results as\n    # LogisticRegression for the multinomial case.\n\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=3,\n                               random_state=0)\n\n    Cs = np.logspace(-4, 4, 5)\n    multi_class = 'multinomial'\n    solver = 'saga'\n\n    coefs, _, _ = _logistic_regression_path(X, y, Cs=Cs, penalty='l2',\n                                            solver=solver, max_iter=1000,\n                                            tol=1e-6, random_state=0,\n                                            multi_class=multi_class)\n\n    clf = LogisticRegression(solver=solver, multi_class=multi_class, C=Cs[0],\n                             max_iter=1000, tol=1e-6, random_state=0)\n    clf.fit(X, y)\n    assert_array_almost_equal(coefs[:, :, 0], clf.coef_, decimal=3)\n"], "sample_774": ["def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b'], [0, 2]], dtype=object).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[['a', 'b', 'c'], [0, 1, 2]])\n    assert enc.get_params()['categories'] == [['a', 'b', 'c'], [0, 1, 2]]\n    assert enc.fit_transform(X).shape == (2, 2)\n    # set params on already fitted object\n    enc.set_params(categories=[['a', 'b', 'c', 'd'], [0, 1, 2]])\n    assert enc.fit_transform(X).shape == (2, 2)\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b']], dtype=object).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[[0, 1, 2, 3]])\n    assert enc.get_params()['categories'] == [[0, 1, 2, 3]]\n    assert enc.fit_transform(X).shape == (2, 1)\n    # set params on already fitted object\n    enc.set_params(categories=[[0, 1, 2, 3, 4]])\n    assert enc.fit_transform(X).shape == (2, 1)\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b']], dtype=object).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[[0, 1, 2, 3]])\n    assert enc.get_params()['categories'] == [[0, 1, 2, 3]]\n    assert enc.fit_transform(X).shape == (2, 1)\n    # set params on already fitted object\n    enc.set_params(categories=[[0, 1, 2, 3, 4]])\n    assert enc.fit_transform(X).shape == (2, 1)\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b'], [0, 2]], dtype=object).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[['a', 'b', 'c'], [0, 1, 2]])\n    assert enc.get_params()['categories'] == [['a', 'b', 'c'], [0, 1, 2]]\n    assert enc.fit_transform(X).shape == (2, 2)\n    # set params on already fitted object\n    enc.set_params(categories=[['a', 'b', 'c', 'd'], [0, 1, 2, 3]])\n    assert enc.fit_transform(X).shape == (2, 2)\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b'], [0, 2]], dtype=object).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[['a', 'b', 'c'], [0, 1, 2]])\n    assert enc.get_params()['categories'] == [['a', 'b', 'c'], [0, 1, 2]]\n    assert_array_equal(enc.fit_transform(X), np.array([[0.], [1.]]))\n    # set params on already fitted object\n    enc.set_params(categories=[['a', 'b', 'c', 'd'], [0, 1, 2, 3]])\n    assert enc.fit_transform(X).shape == (2, 1)\n"], "sample_775": ["def test__changed_params():\n    # Test that _changed_params correctly filters parameters that were not changed\n\n    class DummyEstimator:\n            self.a = a\n            self.b = b\n\n            return {'a': self.a, 'b': self.b}\n\n    estimator = DummyEstimator(a=2)\n    changed_params = _changed_params(estimator)\n\n    assert changed_params == {'a': 2}\n", "def test_compact_repr():\n    # Test that the compact representation is used when necessary\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    pipeline = Pipeline([\n        ('reduce_dim', PCA(n_components=3)),\n        ('classify', LinearSVC())\n    ])\n\n    expected = \"\"\"", "def test_pprint_estimator_with_empty_params():\n    # Render an estimator with no parameters\n    class DummyEstimator(BaseEstimator):\n            pass\n\n    estimator = DummyEstimator()\n    expected = \"DummyEstimator()\"\n    assert estimator.__repr__() == expected\n", "def test_n_max_elements_to_show_zero():\n    # Test edge case when n_max_elements_to_show is zero.\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=0\n    )\n\n    vocabulary = {i: i for i in range(10)}\n    vectorizer = CountVectorizer(vocabulary=vocabulary)\n\n    expected = r\"\"\"", "def test_prettyprinter_repr_max_length():\n    # Test that the repr of an estimator is not longer than the specified max\n    # length, and that it ends with '...)' if it has been truncated.\n    pp = _EstimatorPrettyPrinter(max_length=100)\n    estimator = LogisticRegression()\n    estimator_repr = pp.pformat(estimator)\n    assert len(estimator_repr) <= 100\n    if len(estimator_repr) == 100:\n        assert estimator_repr.endswith('...)')\n"], "sample_776": ["def test_lasso_lars_cv_copyX_behaviour():\n    \"\"\"\n    Test that user input regarding copy_X is not being overridden in LassoLarsCV\n\n    \"\"\"\n    lasso_lars_cv = LassoLarsCV(copy_X=False, precompute=False)\n    rng = np.random.RandomState(0)\n    X = rng.normal(0, 1, (100, 5))\n    X_copy = X.copy()\n    y = X[:, 2]\n    lasso_lars_cv.fit(X, y)\n    assert np.array_equal(X, X_copy)\n", "def test_lars_path_with_positive_constrained():\n    # Test that the lars_path function works with positive constraint.\n    X = diabetes.data\n    y = diabetes.target\n    alphas, active, coef_path = linear_model.lars_path(X, y, method='lasso',\n                                                       positive=True)\n    assert np.all(coef_path >= 0)\n    assert np.all(np.diff(alphas) <= 0)\n\n    # Also test for sparse X\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix(X)\n    alphas_sparse, active_sparse, coef_path_sparse = linear_model.lars_path(\n        X_sparse, y, method='lasso', positive=True)\n    assert_array_almost_equal(alphas, alphas_sparse)\n    assert_array_almost_equal(coef_path, coef_path_sparse)\n", "def test_lars_path_with_positive_parameter():\n    # Test that lars_path with positive parameter returns the correct result\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6])\n    alphas, active, coefs = linear_model.lars_path(X, y, method='lasso',\n                                                   positive=True)\n    assert np.all(coefs >= 0)\n", "def test_lasso_lars_solver_match():\n    # Test that LassoLarsIC and LassoLarsCV solvers match for simple cases\n    X, y = diabetes.data, diabetes.target\n\n    # Set regularization parameter\n    alpha = 0.1\n\n    # LassoLarsIC solver\n    lars_ic = linear_model.LassoLarsIC(criterion='bic')\n    lars_ic.fit(X, y)\n    lars_ic_coef_ = lars_ic.coef_\n\n    # LassoLarsCV solver\n    lars_cv = linear_model.LassoLarsCV(cv=5)\n    lars_cv.fit(X, y)\n    lars_cv_coef_ = lars_cv.coef_\n\n    # Check coefficients are close\n    assert_array_almost_equal(lars_ic_coef_, lars_cv_coef_, decimal=3)\n", "def test_lars_path_warns_when_max_iter_too_low():\n    # Test that lars_path warns when max_iter is too low\n    X = diabetes.data\n    y = diabetes.target\n\n    with pytest.warns(ConvergenceWarning):\n        linear_model.lars_path(X, y, max_iter=1)\n"], "sample_777": ["def test_gradient_boosting_init_estimator_not_fitted():\n    # Check that an error is raised if the init estimator is not fitted\n\n    X, y = make_classification(n_samples=1000, random_state=0)\n    gbc = GradientBoostingClassifier(init=DummyClassifier())\n    gbc.init.fitted_ = False\n    with pytest.raises(NotFittedError):\n        gbc.fit(X, y)\n\n    gbr = GradientBoostingRegressor(init=DummyRegressor())\n    gbr.init.fitted_ = False\n    with pytest.raises(NotFittedError):\n        gbr.fit(X, y)\n", "def test_gradient_boosting_min_impurity_split():\n    # Check that GradientBoostingRegressor and Classifier work when \n    # min_impurity_split is provided.\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n\n    gbc = GradientBoostingClassifier(min_impurity_split=0.1)\n    gbc.fit(X, y)\n\n    gbr = GradientBoostingRegressor(min_impurity_split=0.1)\n    gbr.fit(X, y)\n\n    for tree in gbc.estimators_.flat:\n        assert_equal(tree.min_impurity_split, 0.1)\n\n    for tree in gbr.estimators_.flat:\n        assert_equal(tree.min_impurity_split, 0.1)\n", "def test_gradient_boosting_with_init_estimator_not_fitted():\n    # Check that GradientBoostingRegressor raises an error if the init\n    # estimator is not fitted.\n\n    X, y = make_classification()\n    init_est = DummyClassifier()\n    gb = GradientBoostingClassifier(init=init_est)\n\n    with pytest.raises(NotFittedError):\n        gb._raw_predict_init(X)\n", "def test_gradient_boosting_init_estimator_with_n_classes():\n    # Check that GradientBoostingClassifier raises an error when the init\n    # estimator is a classifier with n_classes != n_classes_ of the\n    # GradientBoostingClassifier\n\n    X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n    y = [0, 1, 2, 2]\n    init_est = DummyClassifier()\n    init_est.n_classes_ = 2\n    gb = GradientBoostingClassifier(init=init_est)\n    with pytest.raises(\n                ValueError,\n                match='The init estimator must have the same number of'):\n        gb.fit(X, y)\n", "def test_gradient_boosting_init_estimator_with_sample_weights():\n    # Test that the init estimator of GradientBoostingClassifier and \n    # GradientBoostingRegressor can handle sample weights.\n\n    X, y = make_classification(n_samples=100, random_state=0)\n    sample_weight = np.random.RandomState(42).rand(100)\n\n    init_estimator = DummyClassifier()\n    gbc = GradientBoostingClassifier(init=init_estimator, n_estimators=10)\n    gbc.fit(X, y, sample_weight=sample_weight)\n\n    init_estimator = DummyRegressor()\n    gbr = GradientBoostingRegressor(init=init_estimator, n_estimators=10)\n    gbr.fit(X, y, sample_weight=sample_weight)\n"], "sample_778": ["def test_nmf_fit_transform_without_init():\n    # Test that fit_transform works without specifying init\n    A = np.c_[5. - np.arange(1, 6),\n              5. + np.arange(1, 6)]\n    for solver in ('cd', 'mu'):\n        model = NMF(n_components=2, solver=solver)\n        transf = model.fit_transform(A)\n        assert not((model.components_ < 0).any() or\n                   (transf < 0).any())\n", "def test_nmf_convergence_warning():\n    # Test that a ConvergenceWarning is raised when the max_iter is too low\n    n_samples = 20\n    n_features = 15\n    n_components = 10\n    tol = 1e-3\n\n    # initialization\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    np.abs(X, X)\n    W0, H0 = nmf._initialize_nmf(X, n_components, init='random',\n                                 random_state=42)\n\n    for solver in ('cd', 'mu'):\n        for beta_loss in (0, 0.2, 1., 2., 2.5):\n            if solver != 'mu' and beta_loss != 2:\n                # not implemented\n                continue\n\n            msg = \"Maximum number of iteration\"\n            with pytest.warns_message(ConvergenceWarning, message=msg):\n                W, H, _ = non_negative_factorization(\n                    X, W0, H0, beta_loss=beta_loss, init='custom',\n                    n_components=n_components, max_iter=1, tol=tol,\n                    solver=solver, verbose=0, regularization='both',\n                    random_state=0, update_H=True)\n", "def test_nmf_solver_parameter():\n    # Test that an error is raised when the solver parameter is not valid\n    A = np.ones((2, 2))\n    model = NMF(solver='invalid')\n    msg = \"Invalid solver parameter: got 'invalid' instead of one of\"\n    assert_raise_message(ValueError, msg, model.fit, A)\n", "def test_nmf_fit_transform_limit_iterations():\n    # Test that fit_transform with limited iterations equals to\n    # multiple partial_fit calls\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(6, 5))\n    n_components = 4\n\n    # NMF with limited iterations (1 iteration)\n    nmf_limited = NMF(n_components=n_components, max_iter=1,\n                      random_state=0).fit(X)\n\n    # Multiple partial_fit calls\n    nmf_partial = NMF(n_components=n_components, max_iter=1,\n                      random_state=0)\n    W_partial = None\n    H_partial = None\n    for _ in range(10):  # 10 iterations\n        W_partial, H_partial = nmf_partial.partial_fit(X, W=W_partial,\n                                                       H=H_partial)\n\n    assert_array_almost_equal(W_partial, nmf_limited.transform(X), decimal=2)\n", "def test_nmf_shuffle():\n    # Test that shuffling the coordinates in cd solver yields the same result\n    n_samples = 20\n    n_features = 15\n    n_components = 10\n\n    # initialization\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    np.abs(X, X)\n    W0, H0 = nmf._initialize_nmf(X, n_components, init='random',\n                                 random_state=42)\n\n    # without shuffling\n    W1, H1, _ = non_negative_factorization(\n        X, W0, H0, n_components=n_components, init='custom', update_H=True,\n        solver='cd', tol=0., max_iter=1000, alpha=0., l1_ratio=0.,\n        regularization=None, random_state=42, verbose=0, shuffle=False)\n\n    # with shuffling\n    W2, H2, _ = non_negative_factorization(\n        X, W0, H0, n_components=n_components, init='custom', update_H=True,\n        solver='cd', tol=0., max_iter=1000, alpha=0., l1_ratio=0.,\n        regularization=None, random_state=42, verbose=0, shuffle=True)\n\n    assert_array_almost_equal(W1, W2, decimal=10)\n    assert_array_almost_equal(H1, H2, decimal=10)\n"], "sample_779": ["def test_check_estimator_sparse_data():\n    # check that estimators handle sparse data correctly\n    from sklearn.datasets import make_blobs\n    X, y = make_blobs(n_samples=50, random_state=1)\n    X_sparse = sp.csr_matrix(X)\n\n    # check that estimators raise an error on sparse data if they don't\n    # support it\n    msg = (\"Estimator estimator_name doesn't seem to fail gracefully on \"\n           \"sparse data\")\n    assert_raises_regex(AssertionError, msg,\n                        check_estimator_sparse_data,\n                        \"estimator_name\",\n                        NoSparseClassifier())\n\n    # check that estimators that support sparse data work correctly\n    check_estimator_sparse_data(\"estimator_name\", LinearRegression())\n", "def test_check_estimator_sparse_data():\n    # check that estimator can handle sparse data correctly\n\n    class SparseEstimator(BaseEstimator):\n            X = check_array(X, accept_sparse=True)\n            return self\n\n            X = check_array(X, accept_sparse=True)\n            return np.ones(X.shape[0])\n\n    check_estimator(SparseEstimator())\n", "def test_check_estimator_unfitted_attributes():\n    # check that an AttributeError is raised when accessing an attribute\n    # that should be set during fitting\n    class Estimator:\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises_regex(AttributeError,\n                        \"Estimator object has no attribute 'fitted_'\",\n                        lambda: estimator.fitted_)\n    check_estimators_unfitted(\"estimator\", estimator)\n", "def test_check_estimator_sparse_data():\n    # check that check_estimator() works on estimator with sparse data\n\n    class SparseEstimator(BaseEstimator):\n            return self\n\n            return np.ones(X.shape[0])\n\n    est = SparseEstimator()\n    check_estimator(est)\n\n    # check with sparse matrix as input\n    X = sp.csr_matrix(np.random.rand(10, 5))\n    y = np.random.randint(0, 2, size=10)\n    est.fit(X, y)\n    assert_equal(est.predict(X).shape[0], X.shape[0])\n", "def test_check_estimator_sparse_data():\n    # check that estimator works on sparse data\n\n    # test precomputed kernel\n    est = SVC(kernel='precomputed')\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    check_estimator_sparse_data('estimator_name', est)\n\n    # test sparse input with non-precomputed kernel\n    est = SVC()\n    check_estimator_sparse_data('estimator_name', est)\n"], "sample_780": ["def test_lda_get_params():\n    \"\"\"Test get_params method of LatentDirichletAllocation.\"\"\"\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components)\n    params = lda.get_params()\n    assert 'n_components' in params\n    assert 'doc_topic_prior' in params\n    assert 'topic_word_prior' in params\n    assert 'learning_method' in params\n    assert 'learning_decay' in params\n    assert 'learning_offset' in params\n    assert 'max_iter' in params\n    assert 'batch_size' in params\n    assert 'evaluate_every' in params\n    assert 'total_samples' in params\n    assert 'perp_tol' in params\n    assert 'mean_change_tol' in params\n    assert 'max_doc_update_iter' in params\n    assert 'n_jobs' in params\n    assert 'verbose' in params\n    assert 'random_state' in params\n", "def test_lda_get_params():\n    # Test LDA get_params method\n    lda = LatentDirichletAllocation()\n    params = lda.get_params()\n    assert 'n_components' in params\n    assert 'doc_topic_prior' in params\n    assert 'topic_word_prior' in params\n    assert 'learning_method' in params\n    assert 'learning_decay' in params\n    assert 'learning_offset' in params\n    assert 'max_iter' in params\n    assert 'batch_size' in params\n    assert 'evaluate_every' in params\n    assert 'total_samples' in params\n    assert 'perp_tol' in params\n    assert 'mean_change_tol' in params\n    assert 'max_doc_update_iter' in params\n    assert 'n_jobs' in params\n    assert 'verbose' in params\n    assert 'random_state' in params\n", "def test_lda_e_step_jobs():\n    # Test LDA e-step with multiple jobs.\n    n_components, X = _build_sparse_mtx()\n    rng = np.random.RandomState(0)\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=rng)\n\n    # fit the model to initialize the components\n    lda.fit(X)\n\n    # manually call e-step with different number of jobs\n    doc_topic_distr_1 = lda._e_step(X, cal_sstats=False, random_init=False)[0]\n    doc_topic_distr_2 = lda._e_step(X, cal_sstats=False, random_init=False,\n                                    parallel=None)[0]\n    assert_array_almost_equal(doc_topic_distr_1, doc_topic_distr_2)\n", "def test_lda_component_init():\n    # Test that the components_ are initialized correctly\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=0)\n    lda._init_latent_vars(X.shape[1])\n    assert lda.components_.shape == (n_components, X.shape[1])\n    assert np.all(lda.components_ > 0)\n", "def test_lda_n_jobs():\n    \"\"\"Test LDA with different n_jobs parameters.\"\"\"\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, max_iter=1,\n                                    learning_method='batch', random_state=0)\n    lda.fit(X)\n\n    # Test with n_jobs = 1\n    lda_1 = LatentDirichletAllocation(n_components=n_components, max_iter=1,\n                                      learning_method='batch', n_jobs=1,\n                                      random_state=0)\n    lda_1.fit(X)\n    assert_array_almost_equal(lda.components_, lda_1.components_)\n\n    # Test with n_jobs = -1\n    lda_2 = LatentDirichletAllocation(n_components=n_components, max_iter=1,\n                                      learning_method='batch', n_jobs=-1,\n                                      random_state=0)\n    lda_2.fit(X)\n    assert_array_almost_equal(lda.components_, lda_2.components_)\n"], "sample_781": ["def test_forest_predict_unseen_data():\n    # Test that forest can predict unseen data.\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y)\n\n    # Create unseen data with new features\n    X_unseen = np.random.rand(5, 15)\n    X_unseen[:, :10] = X[:5]\n\n    # Check that predict does not raise an error\n    clf.predict(X_unseen)\n", "def test_forest_n_features_in():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    clf = RandomForestClassifier(random_state=42).fit(X, y)\n    assert hasattr(clf, 'n_features_in_')\n    assert clf.n_features_in_ == X.shape[1]\n", "def test_forest_warns_message_n_features_deprecation():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    clf = RandomForestClassifier(random_state=42)\n    msg = (\"The `n_features_` attribute is deprecated in version 1.0 and will \"\n           \"be removed in version 1.2. Use `n_features_in_` instead.\")\n    with pytest.warns(FutureWarning, match=msg):\n        clf.fit(X, y)\n", "def test_forest_estimators_attribute():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                 n_estimators=200).fit(X, y)\n\n    # Check that the estimators_ attribute is populated with DecisionTreeClassifier instances\n    assert all(isinstance(estimator, DecisionTreeClassifier) for estimator in clf.estimators_)\n    assert len(clf.estimators_) == clf.n_estimators\n\n    # Check that the estimators_ attribute is correctly set after cloning\n    from sklearn.base import clone\n    clf_clone = clone(clf)\n    clf_clone.fit(X, y)\n    assert all(isinstance(estimator, DecisionTreeClassifier) for estimator in clf_clone.estimators_)\n    assert len(clf_clone.estimators_) == clf_clone.n_estimators\n", "def test_forest_estimator_params():\n    # Test that estimator params are passed to the underlying trees.\n\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n\n    # Define a custom criterion and pass it as an estimator param.\n    class CustomCriterion:\n            self.foo = foo\n\n    clf = RandomForestClassifier(criterion='gini', random_state=42,\n                                 min_impurity_decrease=0.1)\n    clf.fit(X, y)\n\n    for tree in clf.estimators_:\n        assert tree.criterion == 'gini'\n        assert tree.min_impurity_decrease == 0.1\n\n    clf = RandomForestClassifier(criterion=CustomCriterion('bar'),\n                                 random_state=42)\n    clf.fit(X, y)\n\n    for tree in clf.estimators_:\n        assert isinstance(tree.criterion, CustomCriterion)\n        assert tree.criterion.foo == 'bar'\n"], "sample_782": ["def test_column_transformer_get_feature_names_with_remainder_passthrough():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder='passthrough')\n\n    exp = ['trans1__x0', 'x1']\n\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), exp)\n\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n    ct = ColumnTransformer([('trans1', StandardScaler(), ['first'])],\n                           remainder='passthrough')\n\n    exp = ['trans1__first', 'second']\n\n    ct.fit(X_df)\n    assert_equal(ct.get_feature_names(), exp)\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', OneHotEncoder(), [1])],\n                           remainder='passthrough')\n    with pytest.raises(NotImplementedError,\n                       match='get_feature_names is not yet supported'):\n        ct.fit(X_array).get_feature_names()\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=OneHotEncoder())\n    assert ct.fit(X_array).get_feature_names() == ['trans1__x0', 'remainder__x0_2']\n", "def test_column_transformer_remainder_transformer_weights():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    transformer_weights = {'trans1': 0.5, 'remainder': 2}\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans(),\n                           transformer_weights=transformer_weights)\n\n    X_res_both = X_array.copy()\n    X_res_both[:, 0] *= transformer_weights['trans1']\n    X_res_both[:, 1:3] *= transformer_weights['remainder']\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), ['trans1__x0', 'remainder__x0',\n                                          'remainder__x1'])\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n\n    ct.fit(X_array)\n\n    exp = ['trans1__x0', 'remainder__x1']\n\n    assert_equal(ct.get_feature_names(), exp)\n"], "sample_783": ["def test_imputation_statistics_output_shape(strategy):\n    # Test the output shape of the statistics_ attribute\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n\n    imputer = SimpleImputer(strategy=strategy)\n    imputer.fit(X)\n\n    assert imputer.statistics_.shape == (X.shape[1],)\n", "def test_simple_imputer_get_params():\n    # Test SimpleImputer.get_params() return expected dict.\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=99, missing_values=-1)\n    params = imputer.get_params()\n    assert 'strategy' in params\n    assert 'fill_value' in params\n    assert 'missing_values' in params\n    assert params['strategy'] == \"constant\"\n    assert params['fill_value'] == 99\n    assert params['missing_values'] == -1\n", "def test_missing_indicator_sparse_csr():\n    # Test MissingIndicator with sparse csr input\n    X = sparse.csr_matrix(np.array([[0, 1, np.nan],\n                                    [np.nan, 2, 3]]))\n    indicator = MissingIndicator(missing_values=np.nan)\n    X_trans = indicator.fit_transform(X)\n    assert_array_equal(indicator.features_, np.array([0, 2]))\n    assert_allclose(X_trans.toarray(), np.array([[False, True], [True, False]]))\n", "def test_imputation_sparse_no_missing_values(strategy):\n    # Test imputation on sparse data with no missing values\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    imputer = SimpleImputer(strategy=strategy)\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X.toarray(), X_trans.toarray())\n", "def test_imputation_statistics_dtype(strategy):\n    # Test that the statistics_ attribute has the correct dtype\n    X = np.array([[1, 2], [np.nan, 3], [4, 5]])\n    imputer = SimpleImputer(strategy=strategy)\n    imputer.fit(X)\n\n    if strategy == \"mean\":\n        assert imputer.statistics_.dtype == np.float64\n    elif strategy == \"median\":\n        assert imputer.statistics_.dtype == np.float64\n    else:\n        assert imputer.statistics_.dtype == X.dtype\n"], "sample_784": ["def test_calibration_curve_no_data_points():\n    \"\"\"Check calibration_curve function when there are no data points in a bin\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=10)\n    assert len(prob_true) == len(prob_pred)\n    assert len(prob_true) < 10  # Not all bins have data points\n    assert np.all(np.isfinite(prob_true))\n    assert np.all(np.isfinite(prob_pred))\n", "def test_calibration_curve_pos_label():\n    \"\"\"Check calibration_curve function with pos_label argument\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2,\n                                              pos_label=1)\n    assert_almost_equal(prob_true, [0, 1])\n    assert_almost_equal(prob_pred, [0.1, 0.9])\n\n    # Check that changing pos_label gives the correct results\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2,\n                                              pos_label=0)\n    assert_almost_equal(prob_true, [1, 0])\n    assert_almost_equal(prob_pred, [0.9, 0.1])\n", "def test_calibration_with_two_classes():\n    # Test calibration with two classes\n    X, y = make_classification(n_samples=100, n_features=5,\n                               n_informative=3, n_redundant=0,\n                               random_state=42, n_classes=2)\n    clf = LinearSVC(C=1.0)\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n    cal_clf.fit(X, y)\n\n    assert len(cal_clf.classes_) == 2\n\n    proba = cal_clf.predict_proba(X)\n    assert proba.shape[1] == 2\n\n    # Check that predict returns one of the two classes\n    predictions = cal_clf.predict(X)\n    assert np.all(np.isin(predictions, cal_clf.classes_))\n", "def test_calibration_curve_no_data_in_bins():\n    \"\"\"Check calibration_curve function when no data falls in a bin\"\"\"\n    y_true = np.array([0, 1])\n    y_pred = np.array([0., 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 2)\n    assert_almost_equal(prob_true, [0, 1])\n    assert_almost_equal(prob_pred, [0, 1])\n", "def test_calibration_curve_edges():\n    \"\"\"Check calibration_curve function handles edge cases\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0., 0., 1., 1., 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 2)\n    assert_almost_equal(prob_true, [0, 1])\n    assert_almost_equal(prob_pred, [0, 1])\n\n    # check that empty bins are correctly handled\n    y_pred = np.array([0., 0., 0., 0.5, 0.5, 0.5])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 3)\n    assert_almost_equal(prob_true, [0, 0.5, 1])\n    assert_almost_equal(prob_pred, [0, 0.5, np.nan])\n"], "sample_785": ["def test_validate_shuffle_split_init():\n    # Test validation in _validate_shuffle_split_init function\n    assert_raises(ValueError, _validate_shuffle_split_init, None, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, 'wrong_type', None)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.6, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, 8, 2)\n    assert_raises(TypeError, _validate_shuffle_split_init, 1j)\n    assert_raises(ValueError, _validate_shuffle_split_init, 1.1)\n    assert_raises(ValueError, _validate_shuffle_split_init, -0.1)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0)\n", "def test_validate_shuffle_split_init():\n    # Test that _validate_shuffle_split_init raises errors for invalid input\n    assert_raises(ValueError, _validate_shuffle_split_init, None, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, 10, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, 11)\n    assert_raises(TypeError, _validate_shuffle_split_init, 'test', 0.5)\n    assert_raises(TypeError, _validate_shuffle_split_init, 0.5, 'train')\n", "def test_validate_shuffle_split():\n    # Test _validate_shuffle_split function.\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=1.1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, train_size=1.1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=0.0)\n    assert_raises(ValueError, _validate_shuffle_split, 10, train_size=0.0)\n\n    # When neither train nor test size is specified, the default value of\n    # test_size (0.25) must be used\n    train_size, test_size = _validate_shuffle_split(100)\n    assert_almost_equal(train_size / 100., 0.75)\n    assert_almost_equal(test_size / 100., 0.25)\n\n    # When only train size is specified, test size should be calculated\n    # based on it\n    train_size, test_size = _validate_shuffle_split(100, train_size=0.5)\n    assert_almost_equal(train_size / 100., 0.5)\n    assert_almost_equal(test_size / 100., 0.5)\n\n    # When only test size is specified, train size should be calculated\n    # based on it\n    train_size, test_size = _validate_shuffle_split(100, test_size=0.33)\n    assert_almost_equal(train_size / 100., 0.67)\n    assert_almost_equal(test_size / 100., 0.33)\n\n    # When both test and train sizes are specified, they should add up to 1\n    assert_raises(ValueError, _validate_shuffle_split, 100, test_size=0.5,\n                  train_size=0.5)\n\n    # When test size is specified as an integer, it should be used directly\n    train_size, test_size = _validate_shuffle_split(100, test_size=20)\n    assert_equal(train_size, 80)\n    assert_equal(test_size, 20)\n\n    # When train size is specified as an integer, it should be used directly\n    train_size, test_size = _validate_shuffle_split(100, train_size=30)\n    assert_equal(train_size, 30)\n    assert_equal(test_size, 70)\n", "def test_validate_shuffle_split_init():\n    # Test that _validate_shuffle_split_init raises errors for invalid input\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=None,\n                  train_size=None)\n    assert_raises(TypeError, _validate_shuffle_split_init, test_size=1.5,\n                  train_size=None)\n    assert_raises(TypeError, _validate_shuffle_split_init, test_size='a',\n                  train_size=None)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=0.6,\n                  train_size=0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=-0.2,\n                  train_size=None)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=1.2,\n                  train_size=None)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=None,\n                  train_size=1.2)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=None,\n                  train_size=-0.2)\n", "def test_split_with_dataframe():\n    # Test that the split function works with pandas DataFrame\n    import pandas as pd\n\n    X = pd.DataFrame(np.arange(10).reshape(5, 2))\n    y = np.array([0, 1, 0, 1, 0])\n\n    kf = KFold(n_splits=2)\n    for train_index, test_index in kf.split(X):\n        assert_array_equal(train_index, [0, 1, 4] if len(train_index) == 3 else [2, 3])\n        assert_array_equal(test_index, [2, 3] if len(test_index) == 2 else [0, 1, 4])\n"], "sample_786": ["def test_fit_transform_constant_feature():\n    X = np.array([[1, 2], [1, 3], [1, 4]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n    assert len(est.bin_edges_[0]) == 2\n", "def test_constant_features():\n    X = np.array([[1, 2, 3], [1, 4, 5], [1, 6, 7]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n    assert est.bin_edges_[0].shape == (2,)\n", "def test_constant_feature(strategy):\n    X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n    est = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n    assert len(est.bin_edges_[0]) == 2\n", "def test_kmeans_strategy_constant_feature():\n    X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n    est = KBinsDiscretizer(n_bins=3, strategy='kmeans', encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert_array_equal(est.n_bins_, [1, 3])\n    assert_array_equal(est.bin_edges_[0], [-np.inf, np.inf])\n", "def test_fit_transform_constant_feature():\n    X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n"], "sample_787": ["def test_multilabel_confusion_matrix_with_sparse_inputs():\n    from scipy.sparse import csr_matrix\n\n    # Test with sparse arrays\n    y_true = csr_matrix(np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]]))\n    y_pred = csr_matrix(np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]]))\n\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, [[[1, 0], [1, 1]],\n                            [[1, 0], [1, 1]],\n                            [[0, 2], [1, 0]]])\n\n    # Test with sparse and dense arrays\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = csr_matrix(np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]]))\n\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, [[[1, 0], [1, 1]],\n                            [[1, 0], [1, 1]],\n                            [[0, 2], [1, 0]]])\n", "def test_multilabel_confusion_matrix_with_binary_labels():\n    # Test multilabel_confusion_matrix with binary labels\n    y_true = np.array([[0, 1], [1, 0]])\n    y_pred = np.array([[0, 1], [1, 1]])\n\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    expected_cm = np.array([[[1, 0], [0, 1]], [[1, 1], [0, 0]]])\n    assert_array_equal(cm, expected_cm)\n", "def test_balanced_accuracy_score_multiclass():\n    # Test balanced accuracy score for multiclass case\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred),\n                        (1. / 3 + 0.5 / 3 + 0. / 3))\n", "def test_balanced_accuracy_score_binary():\n    # Test balanced accuracy on binary data.\n    y_true = [0, 1, 0, 1, 0]\n    y_pred = [0, 0, 0, 1, 1]\n\n    # Calculate balanced accuracy manually.\n    recall_0 = 2 / 3\n    recall_1 = 1 / 2\n    balanced_accuracy_manual = (recall_0 + recall_1) / 2\n\n    # Calculate balanced accuracy using the function.\n    with ignore_warnings():\n        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n\n    assert balanced_accuracy == pytest.approx(balanced_accuracy_manual)\n", "def test_balanced_accuracy_score_adjusted():\n    y_true = [0, 1, 1, 1]\n    y_pred = [0, 0, 1, 1]\n\n    # The chance level is the proportion of the majority class in y_true\n    chance = 3/4\n\n    # The adjusted balanced accuracy is (balanced_accuracy - chance) / (1 - chance)\n    with ignore_warnings():\n        # Warnings are tested in test_balanced_accuracy_score_unseen\n        balanced = balanced_accuracy_score(y_true, y_pred)\n    adjusted = (balanced - chance) / (1 - chance)\n\n    assert adjusted == pytest.approx(balanced_accuracy_score(y_true, y_pred, adjusted=True))\n"], "sample_788": ["def test_inverse_transform_constant_feature():\n    X = np.array([[0, 1], [0, 2], [0, 3]])\n    kbd = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv[:, 1:], X[:, 1:])\n    assert_array_almost_equal(Xinv[:, 0], np.zeros(X.shape[0]))\n", "def test_inverse_transform_constant_feature():\n    X = np.array([[1, 2], [1, 3], [1, 4]])\n    kbd = KBinsDiscretizer(n_bins=2, encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv[:, 1], X[:, 1])\n    assert_array_almost_equal(Xinv[:, 0], np.mean(kbd.bin_edges_[0][[0, -1]]))\n", "def test_inverse_transform_unchanged_n_bins():\n    # Test that inverse transform does not change n_bins_\n    X = np.array([[1, 2], [3, 4]])\n    kbd = KBinsDiscretizer(n_bins=2, encode='ordinal')\n    kbd.fit(X)\n    n_bins_before = kbd.n_bins_.copy()\n    Xt = kbd.transform(X)\n    kbd.inverse_transform(Xt)\n    assert_array_equal(n_bins_before, kbd.n_bins_)\n", "def test_fit_transform_constant_feature():\n    X = np.array([[1, 2, 3], [1, 4, 5], [1, 6, 7]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n    assert len(est.bin_edges_[0]) == 2\n", "def test_constant_feature():\n    X = np.array([[1, 2], [1, 3], [1, 4]])\n    est = KBinsDiscretizer(n_bins=3)\n    msg = (\"Feature 0 is constant and will be replaced with 0.\")\n    assert_warns_message(UserWarning, msg, est.fit, X)\n    assert_array_equal(est.n_bins_, [1, 3])\n    assert_array_almost_equal(est.bin_edges_[0], [-np.inf, np.inf])\n    assert_array_almost_equal(est.transform(X)[:, 0], [0, 0, 0])\n"], "sample_789": ["def test_adaboost_regressor_with_constant_target():\n    \"\"\"\n    Check that the AdaBoostRegressor can work with constant target variable\n    \"\"\"\n\n    X = np.random.randn(50, 3)\n    y = np.ones(50)\n\n    boost = AdaBoostRegressor(n_estimators=10, random_state=0)\n    boost.fit(X, y)\n    assert_equal(len(boost.estimators_), boost.n_estimators)\n    assert_array_almost_equal(boost.predict(X), y)\n", "def test_feature_importances_(self):\n    # Check feature importances calculation.\n\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=10,\n                                        n_informative=3,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    clf = AdaBoostClassifier()\n    clf.fit(X, y)\n\n    # Ensure feature importances are in the correct range\n    assert_array_less(0, clf.feature_importances_)\n    assert_array_less(clf.feature_importances_, 1)\n\n    # Check if feature_importances_ are normalized\n    assert_almost_equal(np.sum(clf.feature_importances_), 1)\n", "def test_adaboost_regressor_loss():\n    \"\"\"\n    Test that the loss parameter of AdaBoostRegressor is correctly handled.\n    \"\"\"\n    # Define a simple dataset\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    # Define the possible loss functions\n    losses = ['linear', 'square', 'exponential']\n\n    for loss in losses:\n        # Initialize the regressor with the current loss function\n        boost = AdaBoostRegressor(loss=loss, random_state=0)\n\n        # Fit the regressor to the data\n        boost.fit(X, y)\n\n        # Check that the fitted estimator is not empty\n        assert len(boost.estimators_) > 0\n", "def test_base_estimator_none():\n    \"\"\"\n    Check that the AdaBoost estimators can work with base_estimator=None\n    \"\"\"\n\n    boost = AdaBoostClassifier(n_estimators=3)\n    boost.fit(X, y_class)\n    assert_equal(len(boost.estimator_weights_), len(boost.estimator_errors_))\n\n    boost = AdaBoostRegressor(n_estimators=3, random_state=0)\n    boost.fit(X, y_regr)\n    assert_equal(len(boost.estimator_weights_), len(boost.estimator_errors_))\n", "def test_adaboost_regressor_with_loss():\n    # Check AdaBoostRegressor with different loss functions.\n\n    X, y = datasets.make_regression(n_samples=50, n_features=5,\n                                    random_state=rng)\n    est = DecisionTreeRegressor(max_depth=3)\n\n    for loss in ['linear', 'square', 'exponential']:\n        clf = AdaBoostRegressor(est, n_estimators=10, loss=loss,\n                                random_state=rng)\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.score(X, y), 1.0, decimal=2)\n"], "sample_790": ["def test_kernel_pca_inverse_transform_precomputed():\n    # Test if inverse transform works correctly with precomputed kernel.\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    kpca = KernelPCA(kernel=\"precomputed\", n_components=2,\n                     fit_inverse_transform=True)\n    X_fit_kernel = np.dot(X_fit, X_fit.T)\n    X_pred_kernel = np.dot(X_pred, X_fit.T)\n\n    X_kpca = kpca.fit_transform(X_fit_kernel)\n    X_pred_kpca = kpca.transform(X_pred_kernel)\n\n    X_inv = kpca.inverse_transform(X_pred_kpca)\n    assert_array_almost_equal(np.abs(X_inv), np.abs(X_pred_kernel))\n", "def test_kernel_pca_remove_zero_eig_with_inverse_transform():\n    X = np.array([[1 - 1e-30, 1], [1, 1], [1, 1 - 1e-20]])\n\n    # remove_zero_eig is True by default, fit_inverse_transform=True\n    kpca = KernelPCA(fit_inverse_transform=True)\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape, (3, 0))\n\n    kpca = KernelPCA(n_components=2, remove_zero_eig=True,\n                     fit_inverse_transform=True)\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape, (3, 0))\n", "def test_kernel_pca_duplicate_eigenvalues():\n    # Test if KernelPCA can handle duplicate eigenvalues.\n    X = np.array([[1, 0], [0, 1], [1, 0], [0, 1]])\n\n    # With n_components=None, all components should be kept.\n    kpca = KernelPCA(n_components=None)\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape, (4, 2))\n\n    # With n_components=2, both components should be kept.\n    kpca = KernelPCA(n_components=2)\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape, (4, 2))\n\n    # With n_components=1, only the first component should be kept.\n    kpca = KernelPCA(n_components=1)\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape, (4, 1))\n", "def test_kernel_pca_inverse_transform_precomputed():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    # precomputed kernel\n    K_fit = np.dot(X_fit, X_fit.T)\n    K_pred = np.dot(X_pred, X_fit.T)\n\n    kpca = KernelPCA(4, kernel='precomputed', fit_inverse_transform=False)\n    assert_raises(ValueError, kpca.inverse_transform, kpca.fit_transform(K_fit))\n\n    kpca = KernelPCA(4, kernel='precomputed', fit_inverse_transform=True)\n    assert_raises(NotImplementedError, kpca.fit, K_fit)\n\n    # transform fit data\n    kpca = KernelPCA(4, kernel='rbf', fit_inverse_transform=True)\n    X_fit_transformed = kpca.fit_transform(X_fit)\n\n    # inverse transform\n    X_fit_original = kpca.inverse_transform(X_fit_transformed)\n    assert_equal(X_fit_original.shape, X_fit.shape)\n\n    # transform new data\n    X_pred_transformed = kpca.transform(X_pred)\n\n    # inverse transform\n    X_pred_original = kpca.inverse_transform(X_pred_transformed)\n    assert_equal(X_pred_original.shape, X_pred.shape)\n", "def test_kernel_pca_clone():\n    # Test if KernelPCA instance can be cloned\n    from sklearn.base import clone\n\n    X = np.array([[1, 2], [3, 4]])\n\n    kpca = KernelPCA(n_components=1)\n    kpca_cloned = clone(kpca)\n\n    assert_equal(kpca.get_params(), kpca_cloned.get_params())\n    assert_not_equal(id(kpca), id(kpca_cloned))\n\n    kpca.fit(X)\n    assert_raises(NotFittedError, kpca_cloned.transform, X)\n"], "sample_791": ["def test_ordinal_encoder_dtype():\n    X = np.array([[1, 2], [3, 4]], dtype='int64')\n    enc = OrdinalEncoder(dtype='int32')\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == 'float64'\n    enc = OrdinalEncoder(dtype=np.float32)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == 'float32'\n\n    X = np.array([['a', 'b'], ['c', 'd']], dtype=object)\n    enc = OrdinalEncoder(dtype='int32')\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == 'float64'\n    enc = OrdinalEncoder(dtype=np.float32)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == 'float32'\n", "def test_one_hot_encoder_drop_equals_n_values():\n    # Test that when drop equals n_values, an empty array is returned\n    X = [['Male', 1], ['Female', 2]]\n    enc = OneHotEncoder(drop=['Male', 2])\n    trans = enc.fit_transform(X).toarray()\n    exp = np.array([[0., 0.], [0., 0.]])\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['Male', 2])\n", "def test_ordinal_encoder_dtype():\n    # check that OrdinalEncoder works with different dtypes\n    X = np.array([[1, 2], [3, 4]], dtype='int64')\n    enc = OrdinalEncoder(dtype='int32')\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == 'int32'\n\n    X = np.array([[1, 2], [3, 4]], dtype='float64')\n    enc = OrdinalEncoder(dtype='float32')\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == 'float32'\n", "def test_ordinal_encoder_get_feature_names_out():\n    enc = OrdinalEncoder()\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc.fit(X)\n\n    with pytest.raises(AttributeError):\n        enc.get_feature_names_out()\n", "def test_one_hot_encoder_drop_first():\n    # Test drop='first' with multiple features\n    X = [['Male', 1, 'a'],\n         ['Female', 2, 'b']]\n    enc = OneHotEncoder(drop='first')\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0., 0., 0.],\n           [1., 1., 1.]]\n    assert_array_equal(trans, exp)\n    assert_array_equal(enc.drop_idx_, 0)\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n"], "sample_792": ["def test_cnb_partial_fit():\n    # Test ComplementNB when partial_fit is used.\n    X = np.array([[1, 0], [1, 1]])\n    y = np.array([0, 1])\n\n    clf = ComplementNB()\n    clf.partial_fit(X, y, classes=[0, 1])\n    prob = np.array([[0.5, 0.5], [0.25, 0.75]])\n    assert_array_almost_equal(clf.predict_proba(X), prob)\n\n    # Check that a second call to partial fit does not override the\n    # counts previously seen.\n    clf.partial_fit(X, y)\n    prob = np.array([[0.2, 0.8], [0.125, 0.875]])\n    assert_array_almost_equal(clf.predict_proba(X), prob)\n\n    # Check that providing the same data in one go gives the same result\n    clf2 = ComplementNB()\n    clf2.fit(np.vstack([X, X]), np.hstack([y, y]))\n    assert_array_almost_equal(clf.feature_log_prob_, clf2.feature_log_prob_)\n    assert_array_almost_equal(clf.class_count_, clf2.class_count_)\n    assert_array_almost_equal(clf.feature_all_, clf2.feature_all_)\n", "def test_cnb_sparse_input():\n    # Test ComplementNB with sparse input\n    X = scipy.sparse.csr_matrix(np.array([[1, 0], [1, 1]]))\n    y = np.array([0, 1])\n\n    clf = ComplementNB(alpha=1.0)\n    clf.fit(X, y)\n\n    # Check that counts/weights are correct.\n    feature_count = np.array([[1, 0], [0, 1]])\n    assert_array_equal(clf.feature_count_, feature_count)\n    class_count = np.array([1, 1])\n    assert_array_equal(clf.class_count_, class_count)\n    feature_all = np.array([2, 1])\n    assert_array_equal(clf.feature_all_, feature_all)\n", "def test_complementnb_sample_weight():\n    # Test ComplementNB with sample weights\n    X = np.array([[1, 0], [1, 1]])\n    y = np.array([0, 1])\n    sample_weight = np.array([0.5, 1.5])\n\n    clf = ComplementNB()\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    feature_count = np.array([[0.5, 0.5], [0, 1.5]])\n    assert_array_equal(clf.feature_count_, feature_count)\n    class_count = np.array([0.5, 1.5])\n    assert_array_equal(clf.class_count_, class_count)\n    feature_all = np.array([0.5, 2])\n    assert_array_equal(clf.feature_all_, feature_all)\n", "def test_predict_proba_unseen_class():\n    # Test that predict_proba handles classes that were not seen during training\n    X_train = np.array([[0, 1], [1, 0]])\n    y_train = np.array([0, 1])\n    X_test = np.array([[0, 0]])\n\n    gnb = GaussianNB()\n    gnb.fit(X_train, y_train)\n\n    # Create a test label that is not seen during training\n    unseen_label = np.array([2])\n\n    assert_raises(ValueError, gnb.predict_proba, unseen_label)\n", "def test_gnb_partial_fit_classes_param():\n    # Test that using the classes parameter in partial_fit results in correct\n    # setting of classes_ attribute.\n    clf = GaussianNB()\n    X1, y1 = X[:3], y[:3]\n    X2, y2 = X[3:], y[3:]\n\n    clf.partial_fit(X1, y1, classes=np.unique(y))\n    assert_array_equal(clf.classes_, np.unique(y))\n\n    clf.partial_fit(X2, y2)\n    assert_array_equal(clf.classes_, np.unique(y))\n\n    # Test that passing different classes to second call to partial_fit raises\n    # an error.\n    clf = GaussianNB()\n    clf.partial_fit(X1, y1, classes=np.unique(y))\n    assert_raises(ValueError, clf.partial_fit, X2, y2,\n                  classes=np.array([1, 2, 3]))\n"], "sample_793": ["def test_offset_():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    clf2 = IsolationForest().fit(X_train)\n    assert isinstance(clf1.offset_, float)\n    assert isinstance(clf2.offset_, float)\n    assert clf1.offset_ != clf2.offset_\n", "def test_offset_():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    clf2 = IsolationForest().fit(X_train)\n    assert_array_equal(clf1.offset_, -0.5)\n    assert_array_equal(clf2.offset_, -0.5)\n    clf3 = IsolationForest(behaviour='new', contamination='auto').fit(X_train)\n    assert_array_equal(clf3.offset_, -0.5)\n    clf4 = IsolationForest(behaviour='new', contamination=0.2).fit(X_train)\n    assert_array_almost_equal(clf4.offset_, np.percentile(clf4.score_samples(X_train), 20))\n", "def test_offset_():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    clf2 = IsolationForest().fit(X_train)\n    assert isinstance(clf1.offset_, float)\n    assert isinstance(clf2.offset_, float)\n    assert clf1.offset_ != clf2.offset_\n    assert clf1.offset_ < 0\n    assert clf2.offset_ < 0\n", "def test_iforest_estimators_samples():\n    # Test that estimators_samples_ has the correct shape and values.\n    X = iris.data\n    clf = IsolationForest(n_estimators=10, random_state=42).fit(X)\n    assert clf.estimators_samples_.shape == (10, X.shape[0])\n    for samples in clf.estimators_samples_:\n        assert np.unique(samples).size == len(samples)\n", "def test_offset_():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    clf2 = IsolationForest(contamination='auto').fit(X_train)\n    assert_array_equal(clf1.offset_, -0.5)\n    assert clf2.offset_ < 0\n"], "sample_794": ["def test_ridgecv_floating_point_alphas():\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = [1, 1, 1, -1, -1]\n\n    # Floating point values\n    ridge = RidgeCV(alphas=(0.1, 1.0, 10.0))\n    ridge.fit(X, y)\n", "def test_ridge_regression_dtype_stability_sample_weights():\n    random_state = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = random_state.randn(n_samples, n_features)\n    coef = random_state.randn(n_features)\n    y = np.dot(X, coef) + 0.01 * rng.randn(n_samples)\n    alpha = 1.0\n    sample_weight = random_state.rand(n_samples)\n\n    results = dict()\n    for current_dtype in (np.float32, np.float64):\n        results[current_dtype] = ridge_regression(X.astype(current_dtype),\n                                                  y.astype(current_dtype),\n                                                  alpha=alpha,\n                                                  solver='cholesky',\n                                                  sample_weight=sample_weight.astype(current_dtype))\n\n    assert results[np.float32].dtype == np.float32\n    assert results[np.float64].dtype == np.float64\n    assert_allclose(results[np.float32], results[np.float64], rtol=1e-5)\n", "def test_ridgecv_check_cv_values():\n    # Test that cv_values has correct shape and that best alpha is in alphas.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 50, 100\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alphas = [0.1, 1.0, 10.0]\n    ridgecv = RidgeCV(alphas=alphas, store_cv_values=True, cv=None)\n    ridgecv.fit(X, y)\n\n    assert len(ridgecv.cv_values_.shape) == 2\n    assert ridgecv.cv_values_.shape[0] == n_samples\n    assert ridgecv.cv_values_.shape[1] == len(alphas)\n\n    assert ridgecv.alpha_ in alphas\n", "def test_ridge_classifier_multiclass():\n    # Test multiclass classification with RidgeClassifier\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, n_classes=3,\n                               random_state=42)\n\n    clf = RidgeClassifier()\n    clf.fit(X, y)\n    assert_equal(clf.coef_.shape, (3, 10))\n    assert_array_equal(np.unique(clf.predict(X)), np.array([0, 1, 2]))\n\n    # Check decision_function\n    dec_func = clf.decision_function(X)\n    assert_equal(dec_func.shape, (100, 3))\n\n    # Check predict_proba\n    proba = clf.predict_proba(X)\n    assert_equal(proba.shape, (100, 3))\n    assert_array_almost_equal(proba.sum(axis=1), np.ones(100))\n", "def test_ridge_classifier_multiclass():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=3,\n                               random_state=42)\n\n    ridge = RidgeClassifier()\n    ridge.fit(X, y)\n    assert_array_equal(np.unique(y), ridge.classes_)\n    assert_equal(len(ridge.coef_), 3)\n    assert_equal(len(ridge.intercept_), 3)\n\n    y_pred = ridge.predict(X)\n    assert_array_equal(np.unique(y_pred), np.array([0, 1, 2]))\n\n    # check the shape of the decision function\n    dec = ridge.decision_function(X)\n    assert_equal(dec.shape, (X.shape[0], len(ridge.classes_)))\n"], "sample_795": ["def test_check_estimators_overwrite_params():\n    # check that estimators only overwrite params during fit when they are\n    # actually set\n\n    class OverwritingEstimator(BaseEstimator):\n            self.param = param\n\n            self.param = \"overwritten\"\n            return self\n\n    assert_raises_regex(AssertionError,\n                        \"Estimator OverwritingEstimator should not change or \"\n                        \"mutate  the parameter param from None to \"\n                        \"'overwritten' during fit.\",\n                        check_estimator, OverwritingEstimator())\n\n    class NonOverwritingEstimator(BaseEstimator):\n            self.param = param\n\n            if self.param is None:\n                self.param = \"set\"\n            return self\n\n    check_estimator(NonOverwritingEstimator())\n", "def test_check_fit_idempotent():\n    # check that est.fit(X) is the same as est.fit(X).fit(X)\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    for Estimator in [LinearRegression, SGDClassifier]:\n        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n            est = Estimator()\n            set_checking_parameters(est)\n            set_random_state(est)\n\n            est.fit(X, y)\n            result1 = {method: getattr(est, method)(X)\n                       for method in [\"predict\", \"transform\"]\n                       if hasattr(est, method)}\n\n            est.fit(X, y).fit(X, y)\n            result2 = {method: getattr(est, method)(X)\n                       for method in [\"predict\", \"transform\"]\n                       if hasattr(est, method)}\n\n            for method in result1:\n                assert_allclose_dense_sparse(result1[method], result2[method])\n", "def test_check_estimator_sparsify():\n    # check that sparsify works on estimators with a sparsify method\n    from sklearn.linear_model import SGDClassifier\n    est = SGDClassifier()\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    est.fit(X, y)\n    assert not sp.issparse(est.coef_)\n    est.sparsify()\n    assert sp.issparse(est.coef_)\n    check_estimator(est)\n", "def test_check_estimator_sparse_data():\n    # check that estimator can handle sparse data correctly\n\n    class SparseDataEstimator(BaseEstimator):\n            X = check_array(X, accept_sparse='csr')\n            return self\n\n            X = check_array(X, accept_sparse='csr')\n            return np.ones(X.shape[0])\n\n    check_estimator(SparseDataEstimator())\n", "def test_check_estimator_sparse_data():\n    # check that check_estimator() works on estimator with sparse data\n\n    class SparseEstimator(BaseEstimator):\n            return self\n\n            return X\n\n    est = SparseEstimator()\n    check_estimator(est)\n\n    # check that check_estimator() raises an error when estimator does not\n    # support sparse data\n\n    class NonSparseEstimator(BaseEstimator):\n            if sp.issparse(X):\n                raise ValueError(\"Estimator does not support sparse data\")\n            return self\n\n            return X\n\n    msg = (\"Estimator NonSparseEstimator doesn't seem to fail gracefully \"\n           \"on sparse data\")\n    assert_raises_regex(AssertionError, msg, check_estimator,\n                        NonSparseEstimator())\n"], "sample_796": ["def test_huber_epsilon_value_error():\n    # Test that epsilon should be greater than or equal to 1.0.\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True, epsilon=0.9)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n", "def test_huber_raise_error_for_low_epsilon():\n    # Test that HuberRegressor raises an error for low epsilon\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(epsilon=0.5)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n", "def test_huber_sparse_sample_weights_error():\n    # Test that sparse data with sample weights raises an error when\n    # check_input=False.\n    X, y = make_regression_with_outliers()\n    X_csr = sparse.csr_matrix(X)\n    huber = HuberRegressor(fit_intercept=True, alpha=0.1)\n    sample_weight = np.ones(y.shape[0])\n    with pytest.raises(ValueError):\n        huber.fit(X_csr, y, sample_weight=sample_weight, check_input=False)\n", "def test_huber_raise_epsilon():\n    # Test that HuberRegressor raises an error for epsilon < 1.0\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(epsilon=0.5)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n", "def test_huber_edge_cases():\n    # Test edge cases for HuberRegressor\n\n    # Test with X having a single feature\n    X, y = make_regression_with_outliers(n_features=1)\n    huber = HuberRegressor(fit_intercept=True)\n    huber.fit(X, y)\n    assert huber.coef_.shape == (1,)\n    assert huber.intercept_.shape == ()\n\n    # Test with X having a single sample\n    X, y = make_regression_with_outliers(n_samples=1)\n    huber = HuberRegressor(fit_intercept=True)\n    huber.fit(X, y)\n    assert huber.coef_.shape == (X.shape[1],)\n    assert huber.intercept_.shape == ()\n\n    # Test with all samples being outliers\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True, epsilon=0.01)\n    huber.fit(X, y)\n    assert np.all(huber.outliers_)\n"], "sample_797": ["def test_power_transformer_dtype():\n    # check that power_transformer supports float32 and float64 dtypes\n    X = np.abs(X_2d).astype(np.float32)\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    assert X.dtype == X_trans.dtype\n\n    X = np.abs(X_2d).astype(np.float64)\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    assert X.dtype == X_trans.dtype\n", "def test_power_transformer_sparse_matrices():\n    # Check that PowerTransformer works with sparse matrices\n\n    X_sparse = sparse.csr_matrix(np.abs(X_2d))\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n\n    assert_array_almost_equal(pt.fit_transform(X_sparse).toarray(),\n                              pt.fit_transform(X_sparse.toarray()))\n\n    X_sparse_trans = pt.transform(X_sparse)\n    assert sparse.issparse(X_sparse_trans)\n    assert_array_almost_equal(X_sparse_trans.toarray(),\n                              pt.transform(X_sparse.toarray()))\n", "def test_power_transformer_sparse_matrix():\n    # check that PowerTransformer works with sparse matrices\n\n    X = sparse.csr_matrix(np.abs(X_2d))\n    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X.toarray(), X_inv_trans.toarray())\n", "def test_power_transformer_sparse_inputs():\n    # Test that PowerTransformer handles sparse inputs correctly\n    X = sparse.csr_matrix(X_2d)\n    pt = PowerTransformer(method='yeo-johnson')\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n\n    X_inv = pt.inverse_transform(X_trans)\n    assert isinstance(X_inv, sparse.csr_matrix)\n    assert_array_almost_equal(X.toarray(), X_inv.toarray())\n", "def test_power_transformer_sparse_input():\n    # check that PowerTransformer supports sparse input and returns a dense array\n    X_sparse = sparse.csr_matrix(X_2d)\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X_sparse)\n    assert not sparse.issparse(X_trans)\n    assert_array_almost_equal(pt.transform(X_sparse), pt.transform(X_2d))\n    assert_array_almost_equal(pt.inverse_transform(X_trans), X_2d)\n"], "sample_798": ["def test_ridge_classifier_multiclass():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, n_classes=3,\n                               random_state=0)\n\n    ridge = RidgeClassifier()\n    ridge.fit(X, y)\n    assert_array_equal(np.unique(y), ridge.classes_)\n    assert_equal(len(ridge.coef_), len(np.unique(y)))\n", "def test_ridge_saga_solver_convergence():\n    # Test that the SAGA solver converges to the same solution as the 'cholesky'\n    # solver for a wide range of alpha values.\n    X, y = make_regression(n_samples=200, n_features=10, random_state=0)\n    alphas = np.logspace(-5, 3, 10)\n\n    for alpha in alphas:\n        saga = Ridge(alpha=alpha, solver='saga', tol=1e-10, max_iter=10000,\n                     random_state=0)\n        cholesky = Ridge(alpha=alpha, solver='cholesky')\n        saga.fit(X, y)\n        cholesky.fit(X, y)\n        assert_array_almost_equal(saga.coef_, cholesky.coef_)\n", "def test_ridgecv_scorer():\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = [1, 1, 1, -1, -1]\n\n    # Test with scorer as string\n    ridge = RidgeCV(scorer='neg_mean_squared_error')\n    ridge.fit(X, y)\n\n    # Test with scorer as callable\n    scorer = get_scorer('neg_mean_squared_error')\n    ridge = RidgeCV(scorer=scorer)\n    ridge.fit(X, y)\n\n    # Test with scorer as None (should default to 'neg_mean_squared_error')\n    ridge = RidgeCV()\n    ridge.fit(X, y)\n\n    # Test with a custom scorer\n        return np.mean((estimator.predict(X) - y) ** 2)\n\n    ridge = RidgeCV(scorer=custom_scorer)\n    ridge.fit(X, y)\n", "def test_ridge_multitarget():\n    # Test that Ridge can handle multi-target regression\n    X, y = make_regression(n_samples=10, n_features=5, n_targets=3,\n                           random_state=0)\n    ridge = Ridge()\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (3, 5))\n    assert_equal(ridge.intercept_.shape, (3,))\n    y_pred = ridge.predict(X)\n    assert_equal(y_pred.shape, (10, 3))\n", "def test_ridge_multitarget_dtype_match():\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features, n_targets = 6, 5, 3\n    X_64 = rng.randn(n_samples, n_features)\n    y_64 = rng.randn(n_samples, n_targets)\n    X_32 = X_64.astype(np.float32)\n    y_32 = y_64.astype(np.float32)\n\n    solvers = [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\"]\n    for solver in solvers:\n\n        # Check type consistency 32bits\n        ridge_32 = Ridge(alpha=alpha, solver=solver)\n        ridge_32.fit(X_32, y_32)\n        coef_32 = ridge_32.coef_\n\n        # Check type consistency 64 bits\n        ridge_64 = Ridge(alpha=alpha, solver=solver)\n        ridge_64.fit(X_64, y_64)\n        coef_64 = ridge_64.coef_\n\n        # Do the actual checks at once for easier debug\n        assert coef_32.dtype == X_32.dtype\n        assert coef_64.dtype == X_64.dtype\n        assert ridge_32.predict(X_32).dtype == X_32.dtype\n        assert ridge_64.predict(X_64).dtype == X_64.dtype\n        assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)\n"], "sample_799": ["def test_cross_validate_return_estimator():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    cv = KFold(n_splits=3)\n    scores = cross_validate(clf, X, y, cv=cv, return_estimator=True)\n    assert len(scores['estimator']) == 3\n    for est in scores['estimator']:\n        assert isinstance(est, SVC)\n", "def test_fit_and_score_multimetric():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n    multimetric_scorer = {'accuracy': 'accuracy', 'precision': 'precision'}\n    fit_and_score_args = [clf, X, y, multimetric_scorer, train, test, 0]\n    result = _fit_and_score(*fit_and_score_args)\n    assert isinstance(result[0], dict)\n    assert 'accuracy' in result[0]\n    assert 'precision' in result[0]\n", "def test_cross_val_score_with_fit_params_none():\n    # Test that cross_val_score works with fit_params=None\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    scores = cross_val_score(clf, X, y, fit_params=None)\n    assert_array_equal(scores, np.ones(5))\n", "def test_cross_val_score_with_ndarraylike():\n    # test that cross_val_score works with ndarray-like inputs (like pandas\n    # DataFrames) when using non-array like fit_params\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    df_X = MockDataFrame(X)\n    df_y = MockDataFrame(y)\n    clf = CheckingClassifier(check_X=lambda x: isinstance(x, MockDataFrame),\n                             check_y=lambda x: isinstance(x, MockDataFrame))\n    cv = KFold(n_splits=2)\n    fit_params = {'sample_weight': np.array([1, 2])}\n    cross_val_score(clf, df_X, df_y, cv=cv, fit_params=fit_params)\n", "def test_cross_validate_fit_params():\n    # Test that cross_validate can handle fit_params\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    # create a mock fit function that checks for fit_params\n    class MockSVC(SVC):\n            assert 'sample_weight' in fit_params\n            return super().fit(X, y, **fit_params)\n\n    mock_clf = MockSVC(kernel=\"linear\", random_state=0)\n    sample_weight = np.ones(len(y))\n    scores = cross_validate(mock_clf, X, y, cv=5, fit_params={'sample_weight': sample_weight})\n    assert_array_equal(scores['test_score'], cross_validate(clf, X, y, cv=5)['test_score'])\n"], "sample_800": ["def test_check_estimator_tags():\n    # check that estimator tags are correctly set and checked\n\n    class MyEstimator(BaseEstimator):\n            return {\"binary_only\": True}\n\n    class MyEstimator2(BaseEstimator):\n            return {\"multioutput_only\": True}\n\n    assert_raises_regex(AssertionError,\n                        \"Estimator MyEstimator should not have the tag \"\n                        \"_skip_test to be checked by check_estimator().\",\n                        check_estimator, MyEstimator())\n\n    assert_raises_regex(AssertionError,\n                        \"Estimator MyEstimator2 should not have the tag \"\n                        \"_skip_test to be checked by check_estimator().\",\n                        check_estimator, MyEstimator2())\n", "def test_check_estimator_uses_fit_parameter():\n    # check that check_estimator() catches estimators that have a fit method\n    # but do not use it, i.e. they don't have 'X' and 'y' as parameters.\n    class MyEstimator(BaseEstimator):\n            pass\n\n            pass\n\n    assert_raises_regex(AssertionError,\n                        r\"fit\\(\\) method does not use the X and y parameters.\",\n                        check_estimator, MyEstimator())\n", "def test_check_estimator_with_tags():\n    class MyEstimator(BaseEstimator):\n            return self\n\n            return np.ones(X.shape[0])\n\n            return {'binary_only': True}\n\n    check_estimator(MyEstimator())\n", "def test_check_estimator_check_fit_score_takes_y():\n    # check that fit takes y and raises an error if y is not passed.\n    class EstimatorWithoutYInFit(BaseEstimator):\n            return self\n\n            return np.zeros(X.shape[0])\n\n    msg = 'fit() missing 1 required positional argument: \\'y\\''\n    assert_raises_regex(TypeError, msg, check_fit_score_takes_y,\n                        \"test\", EstimatorWithoutYInFit())\n\n    # check that methods with a y=None are OK\n    class EstimatorWithYNONEInFit(BaseEstimator):\n            return self\n\n            return np.zeros(X.shape[0])\n\n    check_fit_score_takes_y(\"test\", EstimatorWithYNONEInFit())\n", "def test_check_estimator_pickle():\n    # check that check_estimator() verifies estimator can be pickled\n    class NonPicklableEstimator(BaseEstimator):\n            self unpicklable_attribute = lambda x: x\n\n            return self\n\n            return np.zeros(X.shape[0])\n\n    assert_raises_regex(AssertionError,\n                        \"Estimator estimator_name cannot be pickled.\",\n                        check_estimator,\n                        'estimator_name',\n                        NonPicklableEstimator())\n"], "sample_801": ["def test_clone():\n    # Test clone function for various estimators\n    from sklearn.feature_selection import SelectKBest, f_classif\n\n    # Test that clone raises an error on invalid type\n    invalid_type = \"I am not an estimator\"\n    assert_raises(TypeError, clone, invalid_type)\n\n    # Test cloning for a variety of estimators\n    estimators = [\n        RFE(estimator=SVR(kernel=\"rbf\"), n_features_to_select=2, step=0.5),\n        SelectKBest(score_func=f_classif, k=3),\n        GridSearchCV(SVC(), {\"C\": [1, 2]}),\n        Pipeline([(\"svc\", SVC())]),\n    ]\n\n    for estimator in estimators:\n        cloned_estimator = clone(estimator)\n        assert estimator is not cloned_estimator\n        assert isinstance(cloned_estimator, type(estimator))\n\n    # Test that clone does not copy data\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    estimator = SVC().fit(X, y)\n    assert estimator.X_fit_.shape == (150, 4)\n\n    cloned_estimator = clone(estimator)\n    assert cloned_estimator.X_fit_ is None\n\n    # Test clone with safe=False\n    estimator = SVC(gamma=float('nan'))\n    cloned_estimator = clone(estimator, safe=False)\n    assert np.isnan(cloned_estimator.gamma)\n", "def test_is_classifier():\n    # Test that is_classifier identifies classifiers and non-classifiers correctly\n    assert is_classifier(LogisticRegression()) is True\n    assert is_classifier(StandardScaler()) is False\n", "def test_is_classifier():\n    # Test that is_classifier works as expected\n    assert is_classifier(LogisticRegression())\n    assert not is_classifier(PCA())\n    assert not is_classifier(SVC(probability=False))\n    assert is_classifier(SVC(probability=True))\n", "def test_clone():\n    # Tests that clone creates a deep copy of estimators\n    from sklearn.feature_selection import SelectKBest, f_classif\n\n    selector = SelectKBest(score_func=f_classif, k=5)\n    new_selector = clone(selector)\n\n    assert selector.score_func.__name__ == new_selector.score_func.__name__\n    assert selector.k == new_selector.k\n\n    # Modify the original selector\n    selector.k = 10\n    assert selector.k != new_selector.k\n", "def test_clone():\n    # Check clone for various estimators.\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.tree import DecisionTreeRegressor\n    from sklearn.utils.estimator_checks import parametrize_with_checks\n\n    @parametrize_with_checks([LogisticRegression, DecisionTreeRegressor])\n        from sklearn.base import clone\n        new_estimator = clone(estimator())\n        assert repr(estimator()) == repr(new_estimator)\n\n    test_clone_estimator()\n"], "sample_802": ["def test_pipeline_inverse_transform():\n    X = np.array([[1]])\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n    pipeline = Pipeline([('m2', mult2), ('m3', mult3)])\n    Xt = pipeline.fit_transform(X)\n    assert_array_equal([[6]], Xt)\n    X_inv = pipeline.inverse_transform(Xt)\n    assert_array_equal(X, X_inv)\n", "def test_pipeline_get_params():\n    # Test the get_params method of the pipeline.\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    scaler = StandardScaler()\n    pca = PCA(svd_solver='full', n_components=2)\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n\n    pipe = Pipeline([('scaler', scaler), ('pca', pca), ('svc', clf)])\n\n    params = pipe.get_params(deep=True)\n\n    # Check that we have all the parameters\n    assert 'scaler' in params\n    assert 'pca' in params\n    assert 'svc' in params\n    assert 'scaler__with_mean' in params\n    assert 'pca__n_components' in params\n    assert 'svc__C' in params\n\n    # Check that the parameters are correctly set\n    assert params['scaler'] is scaler\n    assert params['pca'] is pca\n    assert params['svc'] is clf\n    assert params['scaler__with_mean'] is scaler.with_mean\n    assert params['pca__n_components'] == pca.n_components\n    assert params['svc__C'] == clf.C\n\n    # Check that the parameters are not modified by the pipeline\n    params['scaler__with_mean'] = False\n    assert scaler.with_mean\n\n    # Check that the get_params method returns a dictionary\n    assert isinstance(params, dict)\n", "def test_pipeline_score_samples():\n    # Test that score_samples works on a pipeline.\n    X = np.array([[1, 2]])\n    y = np.array([1])\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n\n    pipeline = Pipeline([\n        ('m2', mult2),\n        ('m3', mult3)\n    ])\n\n    pipeline.fit(X, y)\n    assert_array_equal(pipeline.score_samples(X), [6])\n", "def test_pipeline_with_passthrough_and_steps():\n    X = np.array([[1]])\n    y = np.array([1])\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n    pipeline = Pipeline([('m2', mult2), ('passthrough', 'passthrough'), \n                         ('m3', mult3)])\n    pipeline.fit(X, y)\n    assert_array_equal([[6]], pipeline.transform(X))\n    assert_array_equal([6], pipeline.predict(X))\n    assert_array_equal(X, pipeline.inverse_transform([[6]]))\n\n    # check that 'passthrough' is an identity transformation\n    assert_array_equal(X, pipeline.named_steps['passthrough'].transform(X))\n", "def test_pipeline_memory_with_fit_params():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y)\n        pipe.fit(X, y)\n        # Get the time stamp of the transformer in the cached pipeline\n        ts = cached_pipe.named_steps['transf'].timestamp_\n\n        # Fit again with different fit params\n        cached_pipe.fit(X, y, svc__C=10)\n        pipe.set_params(svc__C=10).fit(X, y)\n\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe.named_steps['transf'].means_)\n        assert not ts == cached_pipe.named_steps['transf'].timestamp_\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_803": ["def test_label_ranking_average_precision_score_empty():\n    # Check label_ranking_average_precision_score with empty arrays.\n    assert_equal(label_ranking_average_precision_score([], []), 0.0)\n    assert_equal(label_ranking_average_precision_score(np.array([]), np.array([])), 0.0)\n    assert_equal(label_ranking_average_precision_score(np.empty((0, 0)), np.empty((0, 0))), 0.0)\n", "def test_label_ranking_average_precision_score_zero_or_all_relevant_labels():\n    # Test that the function handles cases where all or none of the labels are relevant.\n    y_true = np.array([[1, 1, 1], [0, 0, 0]])\n    y_score = np.array([[0.75, 0.5, 0.25], [0.25, 0.5, 0.75]])\n\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), 1.)\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function returns an error when the average\n    # parameter is not one of the allowed values.\n    rng = check_random_state(404)\n    y_pred = rng.rand(10)\n    y_true = np.zeros(10, dtype=\"int\")\n    y_true[::2] = 1\n\n    # valid average parameters\n    for average in [\"macro\", \"weighted\", \"samples\", \"micro\"]:\n        try:\n            roc_auc_score(y_true, y_pred, average=average)\n        except ValueError:\n            assert False, f\"Unexpected error for average='{average}'\"\n\n    # invalid average parameters\n    for average in [\"\", None, \"unknown\"]:\n        with pytest.raises(ValueError):\n            roc_auc_score(y_true, y_pred, average=average)\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function returns an error when the 'average'\n    # parameter is not one of [None, 'micro', 'macro', 'weighted', 'samples']\n    rng = check_random_state(404)\n    y_pred = rng.rand(10)\n    y_true = np.array([0] * 5 + [1] * 5)\n\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred, average=\"f1\")\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred, average=\"score\")\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred, average=1)\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred, average=0.5)\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function returns an error when the average\n    # parameter is not one of the allowed values.\n    rng = check_random_state(404)\n    y_pred = rng.rand(10)\n    y_true = np.array([0] * 5 + [1] * 5)\n\n    assert_raise_message(ValueError, \"average must be one of\",\n                         roc_auc_score, y_true, y_pred, average=\"invalid\")\n    assert_raise_message(ValueError, \"average must be one of\",\n                         roc_auc_score, y_true, y_pred, average=1)\n    assert_raise_message(ValueError, \"average must be one of\",\n                         roc_auc_score, y_true, y_pred, average=1.0)\n"], "sample_804": ["def test_ordinal_encoder_dtype():\n    X = [['a', 1], ['b', 2]]\n    enc = OrdinalEncoder(dtype=np.float32)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == np.float32\n    X_inv = enc.inverse_transform(X_tr)\n    assert_array_equal(X_inv, X)\n\n    X = [['a', 1], ['b', 2]]\n    enc = OrdinalEncoder(dtype=np.int64)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == np.int64\n    X_inv = enc.inverse_transform(X_tr)\n    assert_array_equal(X_inv, X)\n", "def test_one_hot_encoder_get_feature_names_with_drop():\n    # Test get_feature_names with dropped features\n    X = [['Male', 1, 'girl', 2, 3],\n         ['Female', 41, 'girl', 1, 10],\n         ['Male', 51, 'boy', 12, 3],\n         ['Male', 91, 'girl', 21, 30]]\n\n    enc = OneHotEncoder(drop='first')\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert isinstance(feature_names, np.ndarray)\n\n    exp = ['x0_Female', 'x1_41', 'x1_51', 'x1_91', 'x2_girl', 'x3_1', \n           'x3_12', 'x3_21', 'x4_10', 'x4_30']\n    assert_array_equal(exp, feature_names)\n\n    feature_names2 = enc.get_feature_names(['one', 'two', 'three', 'four', 'five'])\n    exp = ['one_Female', 'two_41', 'two_51', 'two_91', 'three_girl', 'four_1', \n           'four_12', 'four_21', 'five_10', 'five_30']\n    assert_array_equal(exp, feature_names2)\n\n    enc = OneHotEncoder(drop=['Male', 1, 'boy', 2, 3])\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    exp = ['x0_Female', 'x1_41', 'x1_51', 'x1_91', 'x2_girl', 'x3_1', \n           'x3_12', 'x3_21', 'x4_10', 'x4_30']\n    assert_array_equal(exp, feature_names)\n", "def test_one_hot_encoder_get_feature_names_with_drop():\n    X = [['Male', 1, 'girl'], ['Female', 2, 'girl']]\n    enc = OneHotEncoder(drop='first')\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0_Female', 'x1_2', 'x2_girl'], feature_names)\n\n    feature_names = enc.get_feature_names(['one', 'two', 'three'])\n    assert_array_equal(['one_Female', 'two_2', 'three_girl'], feature_names)\n", "def test_ordinal_encoder_get_feature_names():\n    X = [['Male', 1], ['Female', 3]]\n    enc = OrdinalEncoder()\n    enc.fit(X)\n\n    feature_names = enc.get_feature_names()\n    assert isinstance(feature_names, np.ndarray)\n    assert_array_equal(['x0', 'x1'], feature_names)\n\n    feature_names2 = enc.get_feature_names(['one', 'two'])\n    assert_array_equal(['one', 'two'], feature_names2)\n\n    with pytest.raises(ValueError, match=\"input_features should have length\"):\n        enc.get_feature_names(['one'])\n", "def test_one_hot_encoder_get_feature_names_drop():\n    enc = OneHotEncoder(drop='first')\n    X = [['Male', 1], ['Female', 3]]\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0_Female', 'x1_3'], feature_names)\n\n    feature_names2 = enc.get_feature_names(['one', 'two'])\n    assert_array_equal(['one_Female', 'two_3'], feature_names2)\n\n    enc = OneHotEncoder(drop=['Male', 1])\n    X = [['Male', 1], ['Female', 3]]\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0_Female', 'x1_3'], feature_names)\n"], "sample_805": ["def test_tweedie_deviance_power_params():\n    y_true = np.array([1.0, 2.0, 3.0])\n    y_pred = np.array([1.5, 2.5, 3.5])\n\n    # check that power=0 is equivalent to mean squared error\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=0),\n                        mean_squared_error(y_true, y_pred))\n\n    # check that power=1 is equivalent to mean Poisson deviance\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=1),\n                        mean_poisson_deviance(y_true, y_pred))\n\n    # check that power=2 is equivalent to mean Gamma deviance\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=2),\n                        mean_gamma_deviance(y_true, y_pred))\n", "def test_mean_tweedie_deviance_sample_weight():\n    y_true = [1.0, 2.0, 3.0]\n    y_pred = [1.5, 2.5, 3.5]\n    sample_weight = [0.2, 0.4, 0.4]\n\n    score = mean_tweedie_deviance(y_true, y_pred, p=1, sample_weight=sample_weight)\n    assert score >= 0\n\n    # test that zero sample weight leads to zero error\n    sample_weight = [0.0, 0.0, 0.0]\n    score = mean_tweedie_deviance(y_true, y_pred, p=1, sample_weight=sample_weight)\n    assert_almost_equal(score, 0.0)\n", "def test_mean_tweedie_deviance_sample_weight():\n    y_true = [1, 2, 3]\n    y_pred = [1.1, 1.9, 3.2]\n    sample_weight = [0.5, 0.5, 1]\n    p = 1\n\n    deviance_weighted = mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight, p=p)\n    deviance_unweighted = mean_tweedie_deviance(y_true, y_pred, p=p)\n\n    assert deviance_weighted != deviance_unweighted\n    assert isinstance(deviance_weighted, float)\n", "def test_mean_squared_error_with_zero_weight():\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([1, 2, 3, 5])\n    sample_weight = np.array([1, 1, 1, 0])\n\n    # With zero weight, the last sample should be ignored in the computation.\n    mse = mean_squared_error(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(mse, 0.00, 2)\n\n    # The squared parameter should not affect the result when all errors are zero.\n    mse = mean_squared_error(y_true, y_pred, sample_weight=sample_weight, squared=False)\n    assert_almost_equal(mse, 0.00, 2)\n", "def test_mean_tweedie_deviance_power_zero():\n    # test that mean_tweedie_deviance with power=0 equals mean_squared_error\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.0])\n\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=0),\n                        mean_squared_error(y_true, y_pred))\n"], "sample_806": ["def test_gradient_boosting_with_init_estimator_with_n_iter_no_change():\n    # Check that GradientBoosting works when init is an estimator and\n    # n_iter_no_change is specified.\n\n    X, y = make_classification(n_samples=1000, random_state=0)\n    gbc = GradientBoostingClassifier(n_estimators=100,\n                                     n_iter_no_change=10,\n                                     learning_rate=0.1, max_depth=3,\n                                     init=DummyClassifier(),\n                                     random_state=42)\n\n    gbc.fit(X, y)\n    assert gbc.n_estimators_ <= 100\n", "def test_gradient_boosting_n_iter_no_change_none():\n    # Test that n_iter_no_change=None does not trigger early stopping\n\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=100,\n                                     n_iter_no_change=None,\n                                     learning_rate=0.1, max_depth=3,\n                                     random_state=42)\n    gbr = GradientBoostingRegressor(n_estimators=100, n_iter_no_change=None,\n                                    learning_rate=0.1, max_depth=3,\n                                    random_state=42)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    gbc.fit(X_train, y_train)\n    gbr.fit(X_train, y_train)\n\n    assert gbc.n_estimators_ == 100\n    assert gbr.n_estimators_ == 100\n", "def test_gradient_boosting_min_impurity_split_deprecation():\n    # Check that min_impurity_split is deprecated.\n    X, y = make_classification(random_state=0)\n\n    with pytest.warns_message(DeprecationWarning,\n                              message=\"The parameter 'min_impurity_split' \"\n                                      \"is deprecated\"):\n        GradientBoostingClassifier(min_impurity_split=1e-7).fit(X, y)\n", "def test_gradient_boosting_with_init_string():\n    # Check that GradientBoostingRegressor works when init is a string.\n\n    X, y = make_regression(random_state=0)\n\n    gb = GradientBoostingRegressor(init='zero')\n    gb.fit(X, y)\n\n    assert_array_almost_equal(gb.init_.estimators_[0, 0].value,\n                              np.zeros((1, 1, 1)))\n", "def test_gradient_boosting_early_stopping_multiclass():\n    # Test early stopping for multiclass classification\n\n    X, y = make_classification(n_samples=1000, n_classes=3, n_features=10,\n                               random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=1000, n_iter_no_change=10,\n                                     learning_rate=0.1, max_depth=3,\n                                     random_state=42)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    gbc.fit(X_train, y_train)\n\n    assert gbc.n_estimators_ < 1000\n    assert gbc.score(X_test, y_test) > 0.7\n"], "sample_807": ["def test_calibration_curve_strategies():\n    \"\"\"Check calibration_curve function with different strategies\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n\n    # Check 'uniform' strategy\n    prob_true_uniform, prob_pred_uniform = calibration_curve(\n        y_true, y_pred, n_bins=3, strategy='uniform')\n    assert len(prob_true_uniform) == len(prob_pred_uniform)\n    assert len(prob_true_uniform) == 3\n\n    # Check 'quantile' strategy\n    prob_true_quantile, prob_pred_quantile = calibration_curve(\n        y_true, y_pred, n_bins=3, strategy='quantile')\n    assert len(prob_true_quantile) == len(prob_pred_quantile)\n    assert len(prob_true_quantile) == 3\n\n    # Check that 'quantile' and 'uniform' strategies give different results\n    assert not np.array_equal(prob_true_uniform, prob_true_quantile)\n    assert not np.array_equal(prob_pred_uniform, prob_pred_quantile)\n", "def test_calibration_curve_error():\n    \"\"\"Check calibration_curve function raises error for invalid input\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n\n    # test that y_true and y_pred have the same length\n    assert_raises(ValueError, calibration_curve, y_true[:5], y_pred)\n\n    # test that y_true is a binary array\n    assert_raises(ValueError, calibration_curve, np.array([0, 1, 2]), y_pred)\n\n    # test that y_pred has at least two distinct values\n    assert_raises(ValueError, calibration_curve, y_true, np.array([0.] * 6))\n", "def test_calibration_curve_fewer_bins_than_unique_predictions():\n    \"\"\"Check calibration_curve function when there are fewer bins than unique predictions\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=1)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n    assert_almost_equal(prob_true, [0.5])\n    assert_almost_equal(prob_pred, [0.5])\n", "def test_calibration_curve_warning():\n    \"\"\"Check calibration_curve function raises a warning when n_bins is too high\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n\n    with pytest.warns(UserWarning):\n        calibration_curve(y_true, y_pred, n_bins=10)\n", "def test_calibration_curve_few_bins():\n    \"\"\"Check calibration_curve function with fewer bins than unique values\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=1)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n    assert_almost_equal(prob_true, [0.5])\n    assert_almost_equal(prob_pred, [0.5])\n\n    # test with more than one bin but less than unique values\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 3)\n    assert_almost_equal(prob_true[0], 0)\n    assert_almost_equal(prob_true[-1], 1)\n    assert_almost_equal(prob_pred[0], 0.1)\n    assert_almost_equal(prob_pred[-1], 0.95)\n"], "sample_808": ["def test_iforest_fit_time_increases_with_n_estimators():\n    \"\"\"Test that fit time of IsolationForest increases with n_estimators\"\"\"\n    X_train = np.array([[0, 1], [1, 2]])\n\n    start_time = time.time()\n    IsolationForest(n_estimators=10).fit(X_train)\n    end_time_10_estimators = time.time()\n\n    IsolationForest(n_estimators=100).fit(X_train)\n    end_time_100_estimators = time.time()\n\n    assert end_time_100_estimators - start_time > end_time_10_estimators - start_time\n", "def test_iforest_fit_predict_sparse():\n    \"\"\"Test fit and predict with sparse data\"\"\"\n    X_train = csr_matrix(np.array([[0, 1], [1, 2]]))\n    X_test = csr_matrix(np.array([[2, 1], [1, 1]]))\n\n    clf = IsolationForest()\n    clf.fit(X_train)\n    assert clf.predict(X_test).shape == (X_test.shape[0],)\n", "def test_iforest_feature_importances_():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest(n_estimators=10).fit(X_train)\n    assert_array_equal(clf.feature_importances_.shape, (X_train[0].shape[0],))\n    assert np.allclose(np.sum(clf.feature_importances_), 1)\n", "def test_iforest_parallel_predict():\n    \"\"\"Check parallel predict.\"\"\"\n    rng = check_random_state(0)\n\n    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n                                                        boston.target,\n                                                        random_state=rng)\n\n    ensemble = IsolationForest(n_jobs=3,\n                               random_state=0).fit(X_train)\n\n    ensemble.set_params(n_jobs=1)\n    y1 = ensemble.predict(X_test)\n    ensemble.set_params(n_jobs=2)\n    y2 = ensemble.predict(X_test)\n    assert_array_almost_equal(y1, y2)\n\n    ensemble = IsolationForest(n_jobs=1,\n                               random_state=0).fit(X_train)\n\n    y3 = ensemble.predict(X_test)\n    assert_array_almost_equal(y1, y3)\n", "def test_iforest_set_params():\n    \"\"\"Test set_params method of IsolationForest\"\"\"\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest(n_estimators=10, max_samples=20,\n                          random_state=0).fit(X_train)\n    \n    # Test setting n_estimators to a lower value\n    clf.set_params(n_estimators=5)\n    assert len(clf.estimators_) == 5\n    \n    # Test setting n_estimators to the same value\n    clf.set_params(n_estimators=5)\n    assert len(clf.estimators_) == 5\n    \n    # Test setting n_estimators to a higher value\n    clf.set_params(n_estimators=10, warm_start=True)\n    assert len(clf.estimators_) == 10\n    \n    # Test setting other parameters\n    clf.set_params(max_samples=10, contamination=0.2)\n    assert clf.max_samples == 10\n    assert clf.contamination == 0.2\n"], "sample_809": ["def test_mutual_info_classif_sparse():\n    # Test that sparse and dense inputs give the same results for\n    # mutual_info_classif.\n    X = np.array([[0, 0, 1],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n\n    X_sparse = csr_matrix(X)\n\n    mi_dense = mutual_info_classif(X, y, discrete_features=True)\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features=True)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Test that sparse and dense inputs give the same results for mutual_info_classif\n    X = np.array([[0, 0, 0],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n    X_sparse = csr_matrix(X)\n\n    mi_dense = mutual_info_classif(X, y, discrete_features='auto', random_state=0)\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features='auto', random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Test that mutual_info_classif works with sparse matrices\n    rng = check_random_state(0)\n    X = rng.rand(100, 5)\n    y = (X[:, 0] > 0.5).astype(int)\n\n    # Create a sparse matrix with the same data as X\n    X_sparse = csr_matrix(X)\n\n    mi_dense = mutual_info_classif(X, y, random_state=0)\n    mi_sparse = mutual_info_classif(X_sparse, y, random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Test that sparse and dense inputs give the same results.\n    rng = check_random_state(0)\n    X = rng.randint(0, 2, size=(100, 5))\n    y = rng.randint(0, 2, size=100)\n\n    X_sparse = csr_matrix(X)\n\n    mi_dense = mutual_info_classif(X, y, discrete_features='auto')\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features='auto')\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Test that sparse and dense inputs give the same results for\n    # mutual_info_classif.\n    rng = check_random_state(0)\n    X = rng.randint(0, 2, size=(100, 5))\n    y = rng.randint(0, 2, size=100)\n\n    X_sparse = csr_matrix(X)\n    mi_dense = mutual_info_classif(X, y, discrete_features='auto', random_state=0)\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features='auto', random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n"], "sample_810": ["def test_pipeline_get_params():\n    # Test that get_params returns all the parameters of the pipeline\n    # and its components\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n    pipeline = Pipeline([('m2', mult2), ('m3', mult3)])\n    params = pipeline.get_params()\n    assert 'm2' in params\n    assert 'm3' in params\n    assert 'm2__mult' in params\n    assert 'm3__mult' in params\n    assert params['m2'] is mult2\n    assert params['m3'] is mult3\n    assert params['m2__mult'] == 2\n    assert params['m3__mult'] == 3\n", "def test_pipeline_score_samples():\n    X = np.array([[1, 2]])\n    y = np.array([1])\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())])\n    pipe.fit(X, y)\n    assert_array_equal(pipe.score_samples(X), pipe.named_steps['clf'].score_samples(X))\n", "def test_pipeline_get_params():\n    # Test that get_params returns all the parameters of the pipeline\n    # and that set_params correctly sets these parameters.\n    pipe = Pipeline([('svc', SVC()), ('logistic', LogisticRegression())])\n    params = pipe.get_params()\n    assert 'svc' in params\n    assert 'logistic' in params\n\n    # Check that we can set the parameters using the given dictionary:\n    params['svc__C'] = 10\n    params['logistic__max_iter'] = 1000\n    pipe.set_params(**params)\n    assert pipe.named_steps['svc'].C == 10\n    assert pipe.named_steps['logistic'].max_iter == 1000\n", "def test_pipeline_memory_cache_dir():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        pipe = Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                        memory=memory)\n        pipe.fit(X, y)\n        # Check that the cache directory was created\n        assert os.path.exists(cachedir)\n        # Check that the cache directory is not empty\n        assert os.listdir(cachedir)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_fit_params_none():\n    # Test that None fit_params are handled correctly\n    X = np.array([[1]])\n    y = np.array([1])\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n\n    pipeline = Pipeline([('m2', mult2), ('m3', mult3)])\n    pipeline.fit(X, y, m2__mult=None, m3__mult=4)\n    assert pipeline.named_steps['m2'].mult == 2\n    assert pipeline.named_steps['m3'].mult == 4\n\n    pipeline.set_params(m2__mult=None, m3__mult=5)\n    assert pipeline.named_steps['m2'].mult == 2\n    assert pipeline.named_steps['m3'].mult == 5\n"], "sample_811": ["def test_pairwise_distances_dtype_preservation():\n    # Ensure that pairwise_distances preserves dtype of input arrays\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4)).astype(np.float32)\n    Y = rng.random_sample((3, 4)).astype(np.float32)\n\n    dist = pairwise_distances(X, Y, metric='euclidean')\n    assert dist.dtype == np.float32\n\n    dist = pairwise_distances(X, Y, metric='manhattan')\n    assert dist.dtype == np.float32\n\n    dist = pairwise_distances(X, Y, metric='cosine')\n    assert dist.dtype == np.float32\n", "def test_pairwise_distances_data_derived_params_callable_metric():\n    # check that pairwise_distances give the same result when metric is a\n    # callable with data-derived parameters.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 10))\n    Y = rng.random_sample((50, 10))\n\n    # Define a custom metric with data-derived parameters\n        if V is None:\n            V = np.var(np.vstack([x, y]), axis=0, ddof=1)\n        return np.sum((x - y) ** 2 / V)\n\n    expected_dist_default_params = cdist(X, Y, metric=custom_metric)\n    params = {'V': np.var(np.vstack([X, Y]), axis=0, ddof=1)}\n    expected_dist_explicit_params = cdist(X, Y, metric=custom_metric, **params)\n    dist = pairwise_distances(X, Y, metric=custom_metric)\n\n    assert_allclose(dist, expected_dist_explicit_params)\n    assert_allclose(dist, expected_dist_default_params)\n", "def test_pairwise_distances_argmin_min_large():\n    # Check pairwise minimum distances computation for any metric\n    # on a large input.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((500, 4))\n    Y = rng.random_sample((200, 4))\n\n    idx1, vals1 = pairwise_distances_argmin_min(X, Y, metric=\"euclidean\")\n    idx2 = pairwise_distances_argmin(X, Y, metric=\"euclidean\")\n\n    assert_array_almost_equal(idx1, idx2)\n\n    # Compare with naive implementation\n    dist = pairwise_distances(X, Y, metric=\"manhattan\")\n    dist_orig_ind = dist.argmin(axis=1)\n    dist_orig_val = dist[np.arange(len(dist)), dist_orig_ind]\n\n    dist_chunked_ind, dist_chunked_val = pairwise_distances_argmin_min(\n        X, Y, axis=1, metric=\"manhattan\")\n    np.testing.assert_almost_equal(dist_orig_ind, dist_chunked_ind, decimal=7)\n    np.testing.assert_almost_equal(dist_orig_val, dist_chunked_val, decimal=7)\n", "def test_pairwise_distances_argmin_min_with_sparse_input():\n    # Check pairwise minimum distances computation for any metric with sparse input\n    X = csr_matrix([[0], [1]])\n    Y = csr_matrix([[-2], [3]])\n\n    idx, vals = pairwise_distances_argmin_min(X, Y, metric=\"euclidean\")\n    assert_array_almost_equal(idx, [0, 1])\n    assert_array_almost_equal(vals, [2, 2])\n\n    # Test with sparse matrix and dense matrix as input\n    X = csr_matrix([[0], [1]])\n    Y = np.array([[-2], [3]])\n\n    idx, vals = pairwise_distances_argmin_min(X, Y, metric=\"euclidean\")\n    assert_array_almost_equal(idx, [0, 1])\n    assert_array_almost_equal(vals, [2, 2])\n", "def test_pairwise_distances_reduce_func_callable():\n    # Test that the reduce_func parameter of pairwise_distances_chunked\n    # can be a callable.\n    X = np.random.rand(100, 5)\n    Y = np.random.rand(100, 5)\n\n        return D_chunk.sum(axis=1)\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func)\n    result = np.vstack(list(gen))\n    expected_result = pairwise_distances(X, Y).sum(axis=1)\n    assert_array_almost_equal(result, expected_result)\n"], "sample_812": ["def test_pprint_estimator_with_numpy_array():\n    # Test that numpy arrays are correctly handled in the repr of an estimator.\n    lr = LogisticRegression()\n    lr.coef_ = np.array([[1, 2], [3, 4]])\n    expected = \"\"\"", "def test_repr_with_numpy_array():\n    # Test that the repr of an estimator with a numpy array parameter\n    # does not throw an error and displays the array correctly.\n\n    class EstimatorWithArray(BaseEstimator):\n            self.array = array\n\n    estimator = EstimatorWithArray()\n    repr_ = estimator.__repr__()\n    assert 'array=[1 2 3]' in repr_\n", "def test_estimator_with_nan_parameter():\n    # Test that estimator with NaN parameter is correctly handled\n    estimator = LogisticRegression(C=float('nan'))\n    expected = \"\"\"LogisticRegression(C=nan)\"\"\"\n    assert estimator.__repr__() == expected\n\n    set_config(print_changed_only=False)\n    expected = \"\"\"LogisticRegression(C=nan, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)\"\"\"\n    assert estimator.__repr__() == expected\n", "def test_estimator_with_array_params():\n    # Test that estimator with array parameters can be pretty printed\n    estimator = LogisticRegressionCV(Cs=np.array([0.1, 1, 10]))\n    expected = \"\"\"", "def test_invalid_n_max_elements_to_show():\n    # Test that a ValueError is raised when n_max_elements_to_show is not a positive integer\n    for n in [0, -1, 'a', None]:\n        with pytest.raises(ValueError):\n            _EstimatorPrettyPrinter(n_max_elements_to_show=n)\n"], "sample_813": ["def test_bayesian_ridge_solver():\n    # Test BayesianRidge with different solver for the posterior covariance\n    X, y = diabetes.data, diabetes.target\n\n    # n_samples > n_features\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n    clf1 = BayesianRidge(solver='svd')\n    clf1.fit(X_train, y_train)\n    clf2 = BayesianRidge(solver='eigen')\n    clf2.fit(X_train, y_train)\n    assert_array_almost_equal(clf1.coef_, clf2.coef_)\n\n    # n_samples < n_features\n    X_train, X_test, y_train, y_test = train_test_split(X.T, y, test_size=0.1)\n    clf1 = BayesianRidge(solver='svd')\n    clf1.fit(X_train, y_train)\n    clf2 = BayesianRidge(solver='eigen')\n    clf2.fit(X_train, y_train)\n    assert_array_almost_equal(clf1.coef_, clf2.coef_)\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with and without intercept\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    # With intercept\n    br_with_intercept = BayesianRidge(fit_intercept=True)\n    br_with_intercept.fit(X, y)\n\n    # Without intercept\n    X_mean = np.mean(X, axis=0)\n    X_centered = X - X_mean\n    y_mean = np.mean(y)\n    y_centered = y - y_mean\n    br_without_intercept = BayesianRidge(fit_intercept=False)\n    br_without_intercept.fit(X_centered, y_centered)\n\n    # Check that the coefficients are equal\n    assert_array_almost_equal(br_with_intercept.coef_, br_without_intercept.coef_)\n\n    # Check that the intercept is correctly calculated\n    assert_almost_equal(br_with_intercept.intercept_, y_mean - np.dot(X_mean, br_with_intercept.coef_))\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with and without intercept\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(fit_intercept=True)\n    clf.fit(X, Y)\n\n    clf_no_intercept = BayesianRidge(fit_intercept=False)\n    clf_no_intercept.fit(X, Y - 5)\n\n    assert_array_almost_equal(clf.coef_, clf_no_intercept.coef_)\n    assert_almost_equal(clf.intercept_, 5)\n", "def test_bayesian_ridge_normalize():\n    # Test BayesianRidge with and without normalization\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    clf_normalized = BayesianRidge(normalize=True)\n    clf_not_normalized = BayesianRidge(normalize=False)\n\n    clf_normalized.fit(X, y)\n    clf_not_normalized.fit(X, y)\n\n    assert_array_almost_equal(clf_normalized.coef_, clf_not_normalized.coef_)\n    assert_almost_equal(clf_normalized.intercept_, clf_not_normalized.intercept_)\n", "def test_bayesian_ridge_alpha_lambda_initialization():\n    # Test BayesianRidge with initial values (alpha_init, lambda_init) equal to None\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(alpha_init=None, lambda_init=None)\n    clf.fit(X, y)\n\n    assert clf.alpha_ is not None\n    assert clf.lambda_ is not None\n\n    # Check that the model could approximately learn the identity function\n    test = [[1], [3], [4]]\n    assert_array_almost_equal(clf.predict(test), [1, 3, 4], 2)\n"], "sample_814": ["def test_gradient_boosting_init_estimator_with_sample_weights():\n    # Check that the init estimator is fit with sample weights if provided.\n\n    X, y = make_classification(n_samples=100, random_state=0)\n    sample_weight = np.random.RandomState(42).rand(100)\n\n    # With sample weights\n    gbc = GradientBoostingClassifier(init=DummyClassifier())\n    gbc.fit(X, y, sample_weight=sample_weight)\n    assert hasattr(gbc.init_, 'sample_weight')\n\n    # Without sample weights\n    gbc = GradientBoostingClassifier(init=DummyClassifier())\n    gbc.fit(X, y)\n    assert not hasattr(gbc.init_, 'sample_weight')\n", "def test_gradient_boosting_init_estimator_not_fitted():\n    # Check that an error is raised if trying to use an initial estimator\n    # that has not been fitted\n\n    X, y = make_classification()\n    init_est = DummyClassifier()\n    gb = GradientBoostingClassifier(init=init_est)\n\n    with pytest.raises(NotFittedError):\n        gb.fit(X, y)\n", "def test_gbr_feature_importances_with_constant_features():\n    # Make sure that constant features have an importance of zero.\n    X, y = make_regression(n_samples=10, n_features=3, random_state=42)\n    X[:, 0] = 1.0  # make the first feature constant\n    gbr = GradientBoostingRegressor(random_state=42).fit(X, y)\n    assert_almost_equal(gbr.feature_importances_[0], 0.0)\n", "def test_gradient_boosting_with_init_estimator_not_fitted():\n    # Check that an error is raised if the init estimator has not been fitted\n\n    X, y = make_regression(random_state=0)\n    init_est = LinearRegression()\n    gb = GradientBoostingRegressor(init=init_est)\n\n    with pytest.raises(NotFittedError,\n                       match=\"Estimator needs to be fit before predict\"):\n        gb.fit(X, y)\n", "def test_gbr_degenerate_oob_improvement():\n    # Test if oob_improvement_ is populated with NaN when subsample=1.0\n    X = np.zeros((10, 10))\n    y = np.ones((10,))\n    gbr = GradientBoostingRegressor(subsample=1.0).fit(X, y)\n    assert np.isnan(gbr.oob_improvement_).all()\n"], "sample_815": ["def test_confusion_matrix_sparse_input():\n    # Test confusion matrix with sparse input\n    from scipy.sparse import csr_matrix\n\n    y_true = [1, 2, 3]\n    y_pred = [2, 3, 4]\n    expected_cm = np.array([[0, 0, 0, 0],\n                            [0, 0, 1, 0],\n                            [0, 0, 0, 1],\n                            [0, 0, 0, 0]])\n\n    # Convert to sparse matrices\n    y_true_sparse = csr_matrix((np.ones(len(y_true)), (np.arange(len(y_true)),\n                                                      y_true)))\n    y_pred_sparse = csr_matrix((np.ones(len(y_pred)), (np.arange(len(y_pred)),\n                                                      y_pred)))\n\n    cm = confusion_matrix(y_true_sparse, y_pred_sparse)\n    assert_array_equal(cm, expected_cm)\n\n    # Test with non-integer labels\n    y_true = ['a', 'b', 'c']\n    y_pred = ['b', 'c', 'd']\n    expected_cm = np.array([[0, 0, 0, 0],\n                            [0, 0, 1, 0],\n                            [0, 0, 0, 1],\n                            [0, 0, 0, 0]])\n\n    # Convert to sparse matrices\n    y_true_sparse = csr_matrix((np.ones(len(y_true)), (np.arange(len(y_true)),\n                                                      y_true)))\n    y_pred_sparse = csr_matrix((np.ones(len(y_pred)), (np.arange(len(y_pred)),\n                                                      y_pred)))\n\n    cm = confusion_matrix(y_true_sparse, y_pred_sparse)\n    assert_array_equal(cm, expected_cm)\n", "def test_multilabel_confusion_matrix_with_object_dtype():\n    y_true = np.array([[1, 0, 1], [0, 1, 0]], dtype='object')\n    y_pred = np.array([[1, 0, 0], [0, 1, 1]], dtype='object')\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    expected_cm = np.array([[[1, 0], [1, 0]], [[1, 0], [0, 1]], [[0, 1], [1, 0]]])\n    assert_array_equal(cm, expected_cm)\n", "def test_check_set_wise_labels():\n    y_true = [1, 1, 2, 3]\n    y_pred = [1, 3, 3, 2]\n    average_options = (None, 'micro', 'macro', 'weighted', 'samples')\n\n    # labels=None and pos_label=1 should return all unique labels in y_true and y_pred\n    assert_array_equal(_check_set_wise_labels(y_true, y_pred, None),\n                       np.array([1, 2, 3]))\n\n    # labels=None and pos_label not in y_true or y_pred should raise an error\n    assert_raises(ValueError, _check_set_wise_labels, y_true, y_pred,\n                  None, pos_label=4)\n\n    # if pos_label is in labels it should be ignored when average is not 'binary'\n    for average in average_options[1:]:\n        assert_warns_message(UserWarning,\n                             \"Note that pos_label (set to 2) is \"\n                             \"ignored when average != 'binary' (got '%s').\"\n                             % average,\n                             _check_set_wise_labels, y_true, y_pred,\n                             average, labels=[1, 2, 3], pos_label=2)\n\n    # labels should override pos_label when average='binary'\n    assert_array_equal(_check_set_wise_labels(y_true, y_pred, 'binary',\n                                              labels=[1, 2], pos_label=3),\n                       np.array([1]))\n\n    # average='binary' with multiclass/multilabel data should raise an error\n    assert_raises(ValueError, _check_set_wise_labels, y_true, y_pred,\n                  'binary')\n", "def test_zero_one_loss():\n    # Test zero_one_loss function\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 2, 1, 0, 0, 1])\n\n    assert_equal(zero_one_loss(y_true, y_pred), 4 / 6)\n    assert_equal(zero_one_loss(y_true, y_true), 0)\n    assert_equal(zero_one_loss(y_true, y_true, normalize=False), 0)\n\n    assert_equal(zero_one_loss(y_true, y_pred, normalize=False), 4)\n    assert_equal(zero_one_loss(y_true, y_pred, normalize=True), 4 / 6)\n", "def test_fbeta_score_multiclass():\n    # Test fbeta_score for multiclass classification task\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute scores with default labels introspection\n    f2 = fbeta_score(y_true, y_pred, beta=2, average=None)\n    assert_array_almost_equal(f2, [0.81, 0.16, 0.57], 2)\n\n    # averaging tests\n    fs = fbeta_score(y_true, y_pred, beta=2, pos_label=1, average='micro')\n    assert_array_almost_equal(fs, 0.53, 2)\n\n    fs = fbeta_score(y_true, y_pred, beta=2, average='macro')\n    assert_array_almost_equal(fs, 0.51, 2)\n\n    fs = fbeta_score(y_true, y_pred, beta=2, average='weighted')\n    assert_array_almost_equal(fs, 0.48, 2)\n\n    assert_raises(ValueError, fbeta_score, y_true, y_pred,\n                  beta=2, average=\"samples\")\n"], "sample_816": ["def test_tfidfvectorizer_custom_preprocessor():\n    # check that custom preprocessors are not ignored\n    preprocessor = lambda x: x.upper()\n    vec = TfidfVectorizer(preprocessor=preprocessor)\n    X = vec.fit_transform([\"this is a test\"])\n    assert \"THIS\" in vec.vocabulary_\n", "def test_vectorizer_inverse_transform_empty_doc(Vectorizer):\n    # Test that inverse_transform works correctly with empty documents.\n    data = ['hello world', '', 'another document']\n    vectorizer = Vectorizer()\n    transformed_data = vectorizer.fit_transform(data)\n    inversed_data = vectorizer.inverse_transform(transformed_data)\n    analyze = vectorizer.build_analyzer()\n    for doc, inversed_terms in zip(data, inversed_data):\n        terms = np.sort(np.unique(analyze(doc)))\n        inversed_terms = np.sort(np.unique(inversed_terms))\n        assert_array_equal(terms, inversed_terms)\n", "def test_countvectorizer_empty_string():\n    cv = CountVectorizer()\n    X = cv.fit_transform(['', ''])\n    assert_equal(X.shape, (2, 0))\n    assert_equal(len(cv.vocabulary_), 0)\n", "def test_countvectorizer_sort_features_empty_matrix():\n    \"\"\"\n    Check that CountVectorizer._sort_features handles empty matrices correctly.\n    \"\"\"\n    X = sparse.csr_matrix((5, 0), dtype=np.int64)\n    vocabulary = {\n            \"scikit-learn\": 0,\n            \"is\": 1,\n            \"great!\": 2\n            }\n\n    Xs = CountVectorizer()._sort_features(X, vocabulary)\n\n    assert Xs.shape == (5, 0)\n", "def test_vectorizer_dtype():\n    # Tests that the dtype parameter is propagated to the matrix.\n    vectorizer = CountVectorizer(dtype=np.float32)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert X.dtype == np.float32\n\n    vectorizer = TfidfVectorizer(dtype=np.float64)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert X.dtype == np.float64\n\n    vectorizer = HashingVectorizer(dtype=np.int32)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert X.dtype == np.int32\n"], "sample_817": ["def test_variance_threshold_sparse_min_max_axis():\n    # Test VarianceThreshold on sparse data with min_max_axis.\n    data = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n    X = csr_matrix(data)\n    sel = VarianceThreshold().fit(X)\n    assert_array_equal([1, 2], sel.get_support(indices=True))\n", "def test_variance_threshold_sparse_matrix():\n    # Test VarianceThreshold with sparse matrix and custom variance.\n\n    X = csr_matrix([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4]])\n    sel = VarianceThreshold(threshold=.4).fit(X)\n    assert_array_equal([3], sel.get_support(indices=True))\n\n    X = csc_matrix([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4]])\n    sel = VarianceThreshold(threshold=.4).fit(X)\n    assert_array_equal([3], sel.get_support(indices=True))\n", "def test_variance_threshold_sparse_peak_to_peak():\n    # Test VarianceThreshold with sparse matrix and default threshold=0.\n    # Ensure that peak-to-peak is used instead of variance.\n\n    data = [[0, 2, 0, 3], [0, 2, 4, 3], [0, 2, 1, 3]]\n    X = csr_matrix(data)\n\n    selector = VarianceThreshold(threshold=0)\n    selector.fit(X)\n    assert_array_equal([1, 2], selector.get_support(indices=True))\n", "def test_get_support_mask():\n    # Test _get_support_mask with fitted VarianceThreshold.\n    X = np.array([[0, 1, 2, 3, 4],\n                  [0, 2, 2, 3, 5],\n                  [1, 1, 2, 4, 0]])\n    sel = VarianceThreshold().fit(X)\n    mask = sel._get_support_mask()\n    assert_array_equal([False, True, False, True, True], mask)\n", "def test_get_support_mask():\n    # Test VarianceThreshold._get_support_mask with fitted and non-fitted estimator\n\n    X = np.array(data)\n    sel = VarianceThreshold().fit(X)\n    assert_array_equal([0, 1, 3, 4], sel._get_support_mask())\n\n    # Test _get_support_mask raises an error if not fitted\n    sel = VarianceThreshold()\n    assert_raises(ValueError, sel._get_support_mask)\n"], "sample_818": ["def test_spectral_clustering_with_eigen_tol():\n    # Test that spectral_clustering works with eigen_tol parameter\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    sp = SpectralClustering(n_clusters=2, eigen_solver='arpack',\n                            eigen_tol=1e-5, random_state=0)\n    labels = sp.fit(X).labels_\n    assert adjusted_rand_score(y, labels) == 1\n\n    # Test that eigen_tol affects result\n    labels_diff_tol = SpectralClustering(n_clusters=2, eigen_solver='arpack',\n                                         eigen_tol=1e-3, random_state=0).fit(X).labels_\n    assert not np.array_equal(labels, labels_diff_tol)\n", "def test_spectral_clustering_random_state():\n    # Test that different random states yield different results\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    sp1 = SpectralClustering(n_clusters=2, random_state=0)\n    sp2 = SpectralClustering(n_clusters=2, random_state=42)\n    labels1 = sp1.fit(X).labels_\n    labels2 = sp2.fit(X).labels_\n    assert not np.array_equal(labels1, labels2)\n", "def test_spectral_clustering_n_init():\n    # Test that n_init parameter is used correctly in SpectralClustering\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    sp = SpectralClustering(n_clusters=2, n_init=1, random_state=0)\n    labels_single_init = sp.fit(X).labels_\n    \n    sp = SpectralClustering(n_clusters=2, n_init=10, random_state=0)\n    labels_multiple_init = sp.fit(X).labels_\n    \n    assert adjusted_rand_score(labels_single_init, labels_multiple_init) == 1\n", "def test_spectral_clustering_n_init():\n    # Test that different initializations lead to the same result\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n\n    S = rbf_kernel(X, gamma=1)\n    S = np.maximum(S - 1e-4, 0)\n\n    labels_10_init = spectral_clustering(S, n_clusters=2, random_state=0, n_init=10)\n    labels_1_init = spectral_clustering(S, n_clusters=2, random_state=0, n_init=1)\n\n    assert_array_equal(labels_10_init, labels_1_init)\n", "def test_spectral_clustering_with_different_kernels():\n    # Test spectral clustering with different kernels\n    X, y = make_blobs(n_samples=200, random_state=1, centers=2)\n    # Test with rbf kernel\n    sp_rbf = SpectralClustering(n_clusters=2, affinity='rbf', random_state=0)\n    labels_rbf = sp_rbf.fit(X).labels_\n    assert adjusted_rand_score(y, labels_rbf) > 0.9\n\n    # Test with nearest_neighbors kernel\n    sp_nn = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n                               random_state=0)\n    labels_nn = sp_nn.fit(X).labels_\n    assert adjusted_rand_score(y, labels_nn) > 0.9\n\n    # Test with precomputed kernel\n    D = pairwise_distances(X)  # Distance matrix\n    S = np.max(D) - D  # Similarity matrix\n    sp_precomputed = SpectralClustering(n_clusters=2, affinity='precomputed',\n                                        random_state=0)\n    labels_precomputed = sp_precomputed.fit(S).labels_\n    assert adjusted_rand_score(y, labels_precomputed) > 0.9\n\n    # Test with callable kernel\n        return np.dot(x, y.T)\n    sp_callable = SpectralClustering(n_clusters=2, affinity=custom_kernel,\n                                     random_state=0)\n    labels_callable = sp_callable.fit(X).labels_\n    assert adjusted_rand_score(y, labels_callable) > 0.9\n"], "sample_819": ["def test_voting_classifier_get_params():\n    \"\"\"Test that VotingClassifier.get_params() returns all estimator parameters\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                voting='soft')\n    params = eclf.get_params()\n    assert 'lr__C' in params\n    assert 'rf__n_estimators' in params\n    assert 'voting' in params\n    assert 'weights' in params\n    assert 'estimators' in params\n    assert len(params) == 15\n", "def test_voting_regressor_get_params():\n    \"\"\"Test VotingRegressor get_params works correctly\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor()\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2)])\n\n    params = ereg.get_params()\n    assert 'lr' in params\n    assert 'rf' in params\n    assert 'estimators' in params\n\n    params = ereg.get_params(deep=True)\n    assert 'lr__n_jobs' in params\n    assert 'rf__n_estimators' in params\n", "def test_voting_regressor_get_params():\n    \"\"\"Check VotingRegressor.get_params return estimators parameters\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor()\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2)])\n\n    params = ereg.get_params(deep=True)\n    assert 'lr__n_jobs' in params\n    assert 'rf__n_estimators' in params\n\n    params = ereg.get_params(deep=False)\n    assert 'lr__n_jobs' not in params\n    assert 'rf__n_estimators' not in params\n", "def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=5)\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)],\n        weights=[1, 1]).fit(X, y)\n\n    assert_array_equal(ereg.transform(X).shape, (2, 2))\n    assert_array_almost_equal(ereg.transform(X),\n                              np.column_stack((reg1.predict(X), reg2.predict(X))))\n", "def test_get_params():\n    \"\"\"Test get_params method of VotingClassifier.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                voting='soft')\n\n    params = eclf.get_params()\n    assert 'lr' in params\n    assert 'rf' in params\n    assert 'voting' in params\n\n    params = eclf.get_params(deep=True)\n    assert 'lr__C' in params\n    assert 'rf__n_estimators' in params\n"], "sample_820": ["def test_get_params():\n    \"\"\"Check that get_params return the hyperparameters and their values.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf1 = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                weights=[1, 2],\n                voting='soft')\n\n    params_dict = {'estimators': [('lr', clf1), ('rf', clf2)],\n                   'weights': [1, 2], 'voting': 'soft', 'n_jobs': None}\n\n    assert_equal(eclf1.get_params(), params_dict)\n    assert type(eclf1.get_params()) is dict\n\n    eclf2 = VotingRegressor(estimators=[\n                ('lr', LinearRegression()), ('rf', RandomForestRegressor())],\n                            weights=[1, 2])\n\n    params_dict = {'estimators': [('lr', LinearRegression()),\n                                  ('rf', RandomForestRegressor())],\n                   'weights': [1, 2], 'n_jobs': None}\n\n    assert_equal(eclf2.get_params(), params_dict)\n    assert type(eclf2.get_params()) is dict\n", "def test_voting_classifier_get_params():\n    \"\"\"Check VotingClassifier get_params method.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                voting='soft',\n                weights=[1, 2],\n                n_jobs=2,\n                flatten_transform=True)\n\n    params = eclf.get_params()\n    assert 'lr' in params\n    assert 'rf' in params\n    assert params['lr'] is clf1\n    assert params['rf'] is clf2\n    assert params['voting'] == 'soft'\n    assert params['weights'] == [1, 2]\n    assert params['n_jobs'] == 2\n    assert params['flatten_transform'] is True\n", "def test_voting_regressor_predict_shape(weights):\n    \"\"\"Check VotingRegressor predict shape.\"\"\"\n    X_r_train, X_r_test, y_r_train, _ = train_test_split(X_r, y_r, test_size=.5)\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = DummyRegressor(strategy='median')\n    ereg = VotingRegressor([('mean', reg1), ('median', reg2)], weights=weights)\n    ereg.fit(X_r_train, y_r_train)\n    y_pred = ereg.predict(X_r_test)\n    assert y_pred.shape == (X_r_test.shape[0],)\n", "def test_voting_classifier_get_params():\n    \"\"\"Check that get_params works properly for VotingClassifier\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                voting='soft')\n    params = eclf.get_params()\n    assert 'lr' in params\n    assert 'rf' in params\n    assert 'voting' in params\n    assert params['lr'] is clf1\n    assert params['rf'] is clf2\n    assert params['voting'] == 'soft'\n", "def test_get_params():\n    \"\"\"Check get_params returns correct shapes and values.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                weights=[1, 2],\n                voting='soft')\n\n    params = eclf.get_params()\n    assert 'estimators' in params\n    assert len(params['estimators']) == 2\n    assert 'weights' in params\n    assert len(params['weights']) == 2\n    assert 'voting' in params\n    assert params['voting'] == 'soft'\n"], "sample_821": ["def test_affinity_propagation_return_n_iter():\n    # Test that return_n_iter returns the correct number of iterations\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    cluster_centers_indices, labels, n_iter = affinity_propagation(\n        S, preference=preference, return_n_iter=True)\n\n    assert isinstance(n_iter, int)\n    assert n_iter > 0\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    af.fit(S)\n\n    assert hasattr(af, 'n_iter_')\n    assert isinstance(af.n_iter_, int)\n    assert af.n_iter_ > 0\n", "def test_affinity_propagation_copy():\n    # Test that the affinity matrix is not modified when copy=True\n    S = -euclidean_distances(X, squared=True)\n    S_copy = S.copy()\n    preference = np.median(S) * 10\n    affinity_propagation(S, preference=preference, copy=True)\n    assert_array_equal(S, S_copy)\n\n    # Test that the affinity matrix is modified when copy=False\n    S = -euclidean_distances(X, squared=True)\n    S_copy = S.copy()\n    preference = np.median(S) * 10\n    affinity_propagation(S, preference=preference, copy=False)\n    assert not np.array_equal(S, S_copy)\n", "def test_affinity_propagation_fit_return_n_iter():\n    # Test that the number of iterations is returned when return_n_iter=True\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n    cluster_centers_indices, labels, n_iter = affinity_propagation(\n        S, preference=preference, return_n_iter=True)\n\n    assert isinstance(n_iter, int)\n    assert n_iter > 0\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    af.fit(S)\n    assert hasattr(af, 'n_iter_')\n    assert af.n_iter_ == n_iter\n", "def test_affinity_propagation_return_n_iter():\n    # Test that affinity_propagation returns the correct number of iterations\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n    cluster_centers_indices, labels, n_iter = affinity_propagation(\n        S, preference=preference, return_n_iter=True)\n\n    assert isinstance(n_iter, int)\n    assert n_iter > 0\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    af.fit(S)\n    assert not hasattr(af, 'n_iter_')\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\",\n                             verbose=True)\n    af.fit(S)\n    assert not hasattr(af, 'n_iter_')\n", "def test_affinity_propagation_return_n_iter():\n    # Test that affinity_propagation returns the correct number of iterations\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n    cluster_centers_indices, labels, n_iter = affinity_propagation(\n        S, preference=preference, return_n_iter=True)\n\n    assert isinstance(n_iter, int)\n    assert n_iter > 0\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    af.fit(S)\n    assert hasattr(af, 'n_iter_')\n    assert af.n_iter_ == n_iter\n"], "sample_822": ["def test_pairwise_distances_argmin_min_axis():\n    # Test that pairwise_distances_argmin_min works correctly with axis parameter\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[0, 0], [5, 5]])\n\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y, axis=0)\n    assert_array_equal(argmin, [0, 1])\n    assert_array_almost_equal(min_dist, [np.sqrt(5), np.sqrt(2)])\n\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y, axis=1)\n    assert_array_equal(argmin, [0, 1])\n    assert_array_almost_equal(min_dist, [np.sqrt(5), np.sqrt(2)])\n", "def test_pairwise_distances_chunked_callable_metric():\n    # Test that pairwise_distances_chunked works with a callable metric.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 10))\n    Y = rng.random_sample((50, 10))\n\n        return np.sum(np.abs(x - y))\n\n    dist = np.vstack(tuple(pairwise_distances_chunked(X, Y, metric=custom_metric)))\n    expected_dist = pairwise_distances(X, Y, metric=custom_metric)\n\n    assert_allclose(dist, expected_dist)\n", "def test_pairwise_distances_argmin_min_axis():\n    # Check pairwise minimum distances computation for any metric along axis=0\n    X = [[0], [1]]\n    Y = [[-2], [3]]\n\n    expected_idx = [0, 1]\n    expected_vals = [2, 2]\n\n    idx, vals = pairwise_distances_argmin_min(X, Y, axis=0, metric=\"euclidean\")\n    assert_array_almost_equal(idx, expected_idx)\n    assert_array_almost_equal(vals, expected_vals)\n\n    idx2 = pairwise_distances_argmin(X, Y, axis=0, metric=\"euclidean\")\n    assert_array_almost_equal(idx2, expected_idx)\n", "def test_pairwise_distances_chunked_reduce_too_many_values():\n    # Test that a ValueError is raised when the reduce_func returns too many values.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    \n        return (D_chunk, D_chunk, D_chunk)\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    with pytest.raises(ValueError, match=\"Expected sequence(s) of length\"):\n        next(gen)\n", "def test_pairwise_distances_argmin_min_equal_X_Y():\n    # Test that pairwise_distances_argmin_min returns the correct result when X=Y\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = X.copy()\n    distances = pairwise_distances(X, Y)\n    min_indices, min_values = pairwise_distances_argmin_min(X, Y)\n    assert_array_almost_equal(min_values, np.zeros(len(X)))\n    assert_array_almost_equal(min_indices, np.arange(len(X)))\n"], "sample_823": ["def test_pairwise_distances_reduce_func_not_callable():\n    # Test that an error is raised when reduce_func is not callable.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((2, 4))\n    reduce_func = \"not callable\"\n    assert_raises(TypeError, pairwise_distances_chunked, X, Y,\n                  reduce_func=reduce_func)\n", "def test_check_paired_arrays_different_length():\n    # Ensure an error is raised if the lengths of X and Y are different.\n    XA = np.resize(np.arange(40), (5, 8))\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, XB)\n", "def test_check_pairwise_arrays_dtype():\n    # Test that check_pairwise_arrays returns the correct dtype\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n\n    X_checked, Y_checked = check_pairwise_arrays(X, Y)\n    assert_equal(X_checked.dtype, np.float64)\n    assert_equal(Y_checked.dtype, np.float64)\n\n    X_checked, Y_checked = check_pairwise_arrays(X, None)\n    assert_equal(X_checked.dtype, np.float32)\n    assert_equal(Y_checked.dtype, np.float32)\n\n    X_checked, Y_checked = check_pairwise_arrays(X.astype(np.float64), Y)\n    assert_equal(X_checked.dtype, np.float64)\n    assert_equal(Y_checked.dtype, np.float64)\n", "def test_pairwise_distances_reduce_func_invalid_output():\n    # Test that an error is raised when reduce_func returns an array with\n    # incorrect shape.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 5))\n    Y = rng.random_sample((10, 5))\n\n        return D_chunk[:3]\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func,\n                                     working_memory=1)\n\n    with pytest.raises(ValueError, match=\"Expected sequence of length\"):\n        next(gen)\n", "def test_pairwise_distances_chunked_reduce_callable():\n    # Test pairwise_distances_chunked with a callable reduce_func\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    Y = rng.random_sample((200, 4))\n\n        return (D_chunk.sum(axis=1), D_chunk.max(axis=1))\n\n    S_chunks = list(pairwise_distances_chunked(X, Y, reduce_func=reduce_func,\n                                              working_memory=2 ** -16))\n\n    S_sum, S_max = zip(*S_chunks)\n\n    expected_S_sum = pairwise_distances(X, Y).sum(axis=1)\n    expected_S_max = pairwise_distances(X, Y).max(axis=1)\n\n    assert_allclose(np.concatenate(S_sum), expected_S_sum)\n    assert_allclose(np.concatenate(S_max), expected_S_max)\n"], "sample_824": ["def test_pairwise_distances_chunked_reduce_memory_error():\n    # Test that pairwise_distances_chunked raises a MemoryError if the input is\n    # too large to fit in memory.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10000, 4))\n    Y = rng.random_sample((10000, 4))\n\n        return D\n\n    with pytest.raises(MemoryError):\n        next(pairwise_distances_chunked(X, Y, reduce_func=reduce_func,\n                                        working_memory=1e-6))\n", "def test_check_paired_arrays():\n    # Ensure that paired array check works.\n    XA = np.resize(np.arange(40), (5, 8))\n    XB = np.resize(np.arange(40), (5, 8))\n\n    # Test that the returned arrays are equal to the original arrays.\n    XA_checked, XB_checked = check_paired_arrays(XA, XB)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n\n    # Check that non-ndarray inputs are converted to ndarrays.\n    XA_checked, XB_checked = check_paired_arrays(XA.tolist(), XB.tolist())\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n\n    # Check that invalid inputs raise an error.\n    assert_raises(ValueError, check_paired_arrays, XA, XB.T)\n", "def test_pairwise_distances_argmin_min_dtype():\n    # Check pairwise minimum distances computation for any metric\n    X = [[0], [1]]\n    Y = [[-2], [3]]\n\n    Xsp = dok_matrix(X)\n    Ysp = csr_matrix(Y, dtype=np.float32)\n\n    idx, vals = pairwise_distances_argmin_min(X, Y, metric=\"euclidean\")\n    assert idx.dtype == np.int64\n    assert vals.dtype == np.float64\n\n    idxsp, valssp = pairwise_distances_argmin_min(Xsp, Ysp, metric=\"euclidean\")\n    assert idxsp.dtype == np.int64\n    assert valssp.dtype == np.float64\n", "def test_pairwise_distances_upcast_dtype():\n    # Test that pairwise distances returns a float64 array when X and Y are\n    # float32 and metric is 'euclidean'\n    X = np.random.rand(5, 4).astype(np.float32)\n    Y = np.random.rand(3, 4).astype(np.float32)\n    D = euclidean_distances(X, Y)\n    assert_equal(D.dtype, np.float64)\n", "def test_pairwise_distances_argmin_min_dtype():\n    # Check pairwise minimum distances computation for any metric\n    X = [[0], [1]]\n    Y = [[-2], [3]]\n\n    Xsp = dok_matrix(X)\n    Ysp = csr_matrix(Y, dtype=np.float32)\n\n    expected_idx = [0, 1]\n    expected_vals = [2, 2]\n\n    idx, vals = pairwise_distances_argmin_min(X, Y, metric=\"euclidean\")\n    assert_array_almost_equal(idx, expected_idx)\n    assert_array_almost_equal(vals, expected_vals)\n    assert_equal(vals.dtype, np.float64)\n\n    idxsp, valssp = pairwise_distances_argmin_min(Xsp, Ysp, metric=\"euclidean\")\n    assert_array_almost_equal(idxsp, expected_idx)\n    assert_array_almost_equal(valssp, expected_vals)\n    assert_equal(valssp.dtype, np.float32)\n"], "sample_825": ["def test_pls_predict_shape():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression()]:\n        clf.fit(X, Y)\n        y_pred = clf.predict(X)\n        assert y_pred.shape == Y.shape\n        # single sample\n        y_pred = clf.predict(X[0].reshape(1, -1))\n        assert y_pred.shape == (1, Y.shape[1])\n", "def test_pls_transform_predict_no_fit():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        assert_raise_message(ValueError, \"fit before calling transform or predict\",\n                             clf.transform, X)\n        assert_raise_message(ValueError, \"fit before calling transform or predict\",\n                             clf.predict, X)\n", "def test_pls_predict_shape():\n    # Check the shape of the prediction output\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred.shape, Y.shape)\n", "def test_pls_predict_shape():\n    # Test that predict output shape is correct\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for n_components in range(1, X.shape[1]):\n        pls = pls_.PLSRegression(n_components=n_components)\n        pls.fit(X, Y)\n        assert_equal(pls.predict(X).shape, (X.shape[0], Y.shape[1]))\n\n    for n_components in range(1, X.shape[1]):\n        pls = pls_.PLSCanonical(n_components=n_components)\n        pls.fit(X, Y)\n        assert_equal(pls.transform(X).shape, (X.shape[0], n_components))\n\n    for n_components in range(1, X.shape[1]):\n        pls = pls_.PLSSVD(n_components=n_components)\n        pls.fit(X, Y)\n        assert_equal(pls.transform(X).shape, (X.shape[0], n_components))\n", "def test_pls_weights_loadings_orthogonality():\n    # Test orthogonality of weights and loadings for PLSRegression and PLSCanonical\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf in [pls_.PLSRegression(), pls_.PLSCanonical()]:\n        clf.fit(X, Y)\n        assert_array_almost_equal(np.eye(clf.n_components),\n                                  np.dot(clf.x_weights_.T, clf.x_weights_))\n        assert_array_almost_equal(np.eye(clf.n_components),\n                                  np.dot(clf.y_weights_.T, clf.y_weights_))\n        assert_array_almost_equal(np.eye(clf.n_components),\n                                  np.dot(clf.x_loadings_.T, clf.x_loadings_))\n        assert_array_almost_equal(np.eye(clf.n_components),\n                                  np.dot(clf.y_loadings_.T, clf.y_loadings_))\n"], "sample_826": ["def test_ordinal_encoder_get_params():\n    enc = OrdinalEncoder(categories=[['a', 'b'], [1, 2]])\n    params = enc.get_params()\n    assert 'categories' in params\n    assert_array_equal(params['categories'], [['a', 'b'], [1, 2]])\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b']], dtype=object).T\n    enc = OrdinalEncoder()\n    enc.set_params(categories=[['a', 'b', 'c']])\n    assert enc.get_params()['categories'] == [['a', 'b', 'c']]\n    enc.fit(X)\n    assert_array_equal(enc.transform(X), [[0.], [1.]])\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b']], dtype=object).T\n    enc = OrdinalEncoder()\n    exp = np.array([[0.], [1.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    # set params on already fitted object\n    enc.set_params(categories=[['c', 'a', 'b']])\n    assert enc.get_params()['categories'] == [['c', 'a', 'b']]\n    assert_array_equal(enc.fit_transform(X), np.array([[1.], [2.]]))\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b']], dtype=object).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[['a', 'b', 'c']])\n    assert enc.get_params()['categories'] == [['a', 'b', 'c']]\n    assert enc.fit_transform(X).shape == (2, 1)\n    # set params on already fitted object\n    enc.set_params(categories=[['a', 'b', 'c', 'd']])\n    assert enc.fit_transform(X).shape == (2, 1)\n", "def test_ordinal_encoder_feature_names():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3]]\n    enc.fit(X)\n    feature_names = enc.get_feature_names(['feature1', 'feature2'])\n    assert_array_equal(['feature1', 'feature2'], feature_names)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0', 'x1'], feature_names)\n"], "sample_827": ["def test_csc_median_axis_0_empty():\n    # Test csc_median_axis_0 on an empty matrix.\n    X = sp.csc_matrix((0, 10))\n    median = csc_median_axis_0(X)\n    assert_array_equal(median, np.zeros(10) * np.nan)\n", "def test_inplace_csr_column_scale_edge_cases():\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    scale = np.array([0.5, 1.0, 2.0])\n\n    # Test with zero-valued scale\n    scale[1] = 0\n    inplace_csr_column_scale(X, scale)\n    assert_array_equal(X.toarray(), np.array([[1, 0, 6], [4, 0, 12]]))\n\n    # Test with NaN-valued scale\n    scale[1] = np.nan\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    with pytest.raises(ValueError):\n        inplace_csr_column_scale(X, scale)\n\n    # Test with infinity-valued scale\n    scale[1] = np.inf\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    with pytest.raises(ValueError):\n        inplace_csr_column_scale(X, scale)\n", "def test_inplace_csr_row_normalize_l1_zeros():\n    X = sp.csr_matrix(np.array([[0, 3, 0],\n                                [2, -1, 0],\n                                [0, 0, 0],\n                                [9, 8, 7],\n                                [4, 0, 5]], dtype=np.float64))\n    inplace_csr_row_normalize_l1(X)\n    assert_array_almost_equal(np.abs(X).sum(axis=1), np.ones((X.shape[0], 1)))\n\n    X = sp.csr_matrix(np.array([[0, 3, 0],\n                                [2, -1, 0],\n                                [0, 0, 0],\n                                [9, 8, 7],\n                                [4, 0, 5]], dtype=np.float32))\n    inplace_csr_row_normalize_l1(X)\n    assert_array_almost_equal(np.abs(X).sum(axis=1), np.ones((X.shape[0], 1)))\n", "def test_inplace_csr_column_scale_int32_overflow():\n    # Test that inplace_csr_column_scale does not overflow for large values\n    X = sp.csr_matrix(np.random.rand(100, 5))\n    scale = np.array([2**30], dtype=np.float64)\n    inplace_csr_column_scale(X, scale)\n    assert_array_almost_equal(X.toarray(), X.toarray() * scale)\n", "def test_inplace_csr_row_normalize_l1_zero_row():\n    # Test that inplace_csr_row_normalize_l1 handles zero rows correctly.\n    X = np.array([[0, 0, 0], [1, 2, 3]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n\n    inplace_csr_row_normalize_l1(X_csr)\n\n    expected_X_csr = sp.csr_matrix(np.array([[0, 0, 0], [1/6, 2/6, 3/6]]))\n    assert_array_almost_equal(X_csr.toarray(), expected_X_csr.toarray())\n"], "sample_828": ["def test_pairwise_distances_chunked_y_none():\n    # Test pairwise_distances_chunked with Y=None\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 10))\n    D = pairwise_distances(X)\n    gen = pairwise_distances_chunked(X, working_memory=1)\n    chunked_D = np.vstack(list(gen))\n    assert_array_almost_equal(D, chunked_D)\n", "def test_pairwise_distances_argmin_min_non_euclidean():\n    # Test pairwise_distances_argmin_min with non-euclidean metric\n    X = np.array([[0, 1], [1, 1], [2, 2]])\n    Y = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n\n    min_indices, min_values = pairwise_distances_argmin_min(X, Y, metric='manhattan')\n    expected_indices = [0, 1, 2]\n    expected_values = [1, 0, 1]\n    assert_array_equal(min_indices, expected_indices)\n    assert_array_equal(min_values, expected_values)\n\n    min_indices, min_values = pairwise_distances_argmin_min(X, Y, metric='cosine')\n    expected_indices = [0, 1, 2]\n    expected_values = [0, 0, 0]\n    assert_array_equal(min_indices, expected_indices)\n    assert_array_almost_equal(min_values, expected_values)\n", "def test_check_paired_arrays_different_lengths():\n    # Ensure an error is raised if the lengths are different.\n    XA = np.resize(np.arange(40), (5, 8))\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, XB)\n", "def test_check_paired_arrays_shape_mismatch():\n    # Test that an error is raised when the input arrays have different shapes.\n    X = np.random.rand(5, 4)\n    Y = np.random.rand(3, 4)\n    with pytest.raises(ValueError):\n        check_paired_arrays(X, Y)\n\n    X = np.random.rand(5, 4)\n    Y = np.random.rand(5, 3)\n    with pytest.raises(ValueError):\n        check_paired_arrays(X, Y)\n", "def test_pairwise_distances_dtype_preservation():\n    # Test that pairwise_distances preserves the dtype of the input arrays\n    X = np.random.rand(10, 5).astype(np.float32)\n    Y = np.random.rand(10, 5).astype(np.float32)\n\n    dist = pairwise_distances(X, Y, metric='euclidean')\n    assert dist.dtype == np.float32\n\n    dist = pairwise_distances(X, Y, metric='manhattan')\n    assert dist.dtype == np.float32\n\n    X = np.random.rand(10, 5).astype(np.float64)\n    Y = np.random.rand(10, 5).astype(np.float64)\n\n    dist = pairwise_distances(X, Y, metric='euclidean')\n    assert dist.dtype == np.float64\n\n    dist = pairwise_distances(X, Y, metric='manhattan')\n    assert dist.dtype == np.float64\n"], "sample_829": ["def test_incremental_pca_n_components_changes():\n    # Test that changing n_components raises an error when components_ is set.\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2)\n    ipca.fit(X)\n\n    # Set n_components to a different value\n    ipca.n_components = 3\n\n    with pytest.raises(\n            ValueError,\n            match=\"Number of input features has changed from 2 \"\n            \"to 3 between calls to partial_fit! Try \"\n            \"setting n_components to a fixed value.\"):\n        ipca.partial_fit(X)\n", "def test_incremental_pca_partial_fit_batch_size_change():\n    # Test that changing batch size between partial_fit calls works correctly.\n\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 10)\n\n    ipca = IncrementalPCA(n_components=5, batch_size=20)\n    ipca.partial_fit(X[:20])\n\n    # Change batch size to 30 for the next partial_fit call\n    ipca.batch_size = 30\n    ipca.partial_fit(X[20:50])\n\n    # Change batch size to 25 for the next partial_fit call\n    ipca.batch_size = 25\n    ipca.partial_fit(X[50:75])\n\n    # Finish with the remaining data\n    ipca.partial_fit(X[75:])\n\n    # Check that the singular values are correct\n    pca = PCA(n_components=5).fit(X)\n    assert_array_almost_equal(ipca.singular_values_, pca.singular_values_, 2)\n", "def test_incremental_pca_get_covariance():\n    # Test that get_covariance works for IncrementalPCA.\n    X = iris.data\n    batch_size = X.shape[0] // 3\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n    pca = PCA(n_components=2)\n    pca.fit_transform(X)\n\n    ipca.fit(X)\n    cov = ipca.get_covariance()\n    precision = ipca.get_precision()\n    np.testing.assert_allclose(np.dot(cov, precision),\n                               np.eye(X.shape[1]), atol=1e-13)\n", "def test_incremental_pca_transform_float_input():\n    # Test that transform method accepts float32 input\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 3).astype(np.float32)\n    ipca = IncrementalPCA(n_components=2)\n    ipca.fit(X)\n    Xt = ipca.transform(X)\n    assert Xt.dtype == np.float32\n", "def test_incremental_pca_fit_transform_n_components_equal_n_features():\n    # Test that fit_transform works when n_components equals n_features.\n    X = iris.data\n    n_samples, n_features = X.shape\n    ipca = IncrementalPCA(n_components=n_features, batch_size=25)\n    Xt = ipca.fit_transform(X)\n    assert Xt.shape == (n_samples, n_features)\n    assert np.allclose(Xt, X)\n"], "sample_830": ["def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n"], "sample_831": ["def test_plot_tree_plot_options(pyplot):\n    # mostly smoke tests\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test filled option\n    nodes = plot_tree(clf, filled=True)\n    assert len(nodes) == 3\n\n    # Test impurity option\n    nodes = plot_tree(clf, impurity=False)\n    assert len(nodes) == 3\n\n    # Test node_ids option\n    nodes = plot_tree(clf, node_ids=True)\n    assert len(nodes) == 3\n\n    # Test proportion option\n    nodes = plot_tree(clf, proportion=True)\n    assert len(nodes) == 3\n\n    # Test rounded option\n    nodes = plot_tree(clf, rounded=True)\n    assert len(nodes) == 3\n\n    # Test precision option\n    nodes = plot_tree(clf, precision=4)\n    assert len(nodes) == 3\n\n    # Test fontsize option\n    nodes = plot_tree(clf, fontsize=10)\n    assert len(nodes) == 3\n", "def test_plot_tree_max_depth(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for max_depth parameter\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=1)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 6\\nvalue = [3, 3]\")\n    assert nodes[1].get_text() == \"(...)\"\n    assert nodes[2].get_text() == \"(...)\"\n\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=0)\n    assert len(nodes) == 1\n    assert nodes[0].get_text() == (\"(...)\")\n", "def test_plot_tree_filled(pyplot):\n    # Test filled option in plot_tree\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names, filled=True)\n    assert len(nodes) == 3\n\n    # check if filled box is drawn for each node\n    for node in nodes:\n        assert node.get_bbox_patch() is not None\n", "def test_plot_tree_filled(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for filled option\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names, filled=True)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 6\\nvalue = [3, 3]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n\n    # Check that the boxes are filled\n    for node in nodes:\n        assert node.get_bbox_patch().get_facecolor() != (1, 1, 1, 1)\n", "def test_plot_tree_mse(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for criterion = mse\n    clf = DecisionTreeRegressor(max_depth=3,\n                                min_samples_split=2,\n                                criterion=\"mse\",\n                                random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\nmse = 1.0\\n\"\n                                   \"samples = 6\\nvalue = [3.0]\")\n    assert nodes[1].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = [3.0]\"\n    assert nodes[2].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = [0.0]\"\n"], "sample_832": ["def test_bayesian_ridge_fit_intercept():\n    # Test BayesianRidge with and without intercept\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    \n    clf_with_intercept = BayesianRidge(fit_intercept=True)\n    clf_with_intercept.fit(X, y)\n    \n    clf_without_intercept = BayesianRidge(fit_intercept=False)\n    clf_without_intercept.fit(X, y)\n    \n    assert clf_with_intercept.intercept_ != 0\n    assert clf_without_intercept.intercept_ == 0\n", "def test_bayesian_ridge_attribute_types():\n    \"\"\"Check types of attributes after fitting\"\"\"\n    X, y = diabetes.data, diabetes.target\n\n    clf = BayesianRidge(compute_score=True)\n    clf.fit(X, y)\n\n    assert isinstance(clf.coef_, np.ndarray)\n    assert isinstance(clf.alpha_, float)\n    assert isinstance(clf.lambda_, float)\n    assert isinstance(clf.sigma_, np.ndarray)\n    assert isinstance(clf.scores_, np.ndarray)\n    assert isinstance(clf.X_offset_, np.ndarray)\n    assert isinstance(clf.X_scale_, np.ndarray)\n    assert isinstance(clf.n_iter_, int)\n", "def test_bayesian_ridge_edge_cases():\n    # Test BayesianRidge with edge cases (zero or negative n_iter, tol)\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([1, 2, 6, 8, 10])\n\n    # Test negative n_iter\n    clf = BayesianRidge(n_iter=-1)\n    msg = \"n_iter should be greater than or equal to 1.\"\n    assert_raise_message(ValueError, msg, clf.fit, X, y)\n\n    # Test zero tol\n    clf = BayesianRidge(tol=0)\n    msg = \"tol must be strictly positive.\"\n    assert_raise_message(ValueError, msg, clf.fit, X, y)\n", "def test_bayesian_ridge_init_parameters():\n    # Test BayesianRidge with different initial values (alpha_init, lambda_init)\n    X = np.vander(np.linspace(0, 4, 5), 4)\n    y = np.array([0., 1., 0., -1., 0.])    # y = (x^3 - 6x^2 + 8x) / 3\n\n    # In this case, starting from the default initial values will increase\n    # the bias of the fitted curve. So, lambda_init should be small.\n    reg1 = BayesianRidge(alpha_init=1., lambda_init=1e-3)\n    reg2 = BayesianRidge(alpha_init=1., lambda_init=1.)\n    reg3 = BayesianRidge(alpha_init=0.1, lambda_init=1.)\n\n    # Check the R2 score nearly equals to one for all initializations.\n    r2_1 = reg1.fit(X, y).score(X, y)\n    r2_2 = reg2.fit(X, y).score(X, y)\n    r2_3 = reg3.fit(X, y).score(X, y)\n\n    assert_almost_equal(r2_1, 1.)\n    assert_almost_equal(r2_2, 1.)\n    assert_almost_equal(r2_3, 1.)\n", "def test_bayesian_ridge_compute_score():\n    # Test BayesianRidge with compute_score=False\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(compute_score=False)\n    clf.fit(X, Y)\n    assert not hasattr(clf, 'scores_')\n"], "sample_833": ["def test_logistic_regression_penalty_none_predict_proba():\n    # Test that predict_proba works when penalty='none'\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    lr_none = LogisticRegression(penalty='none', random_state=0)\n    lr_none.fit(X, y)\n\n    pred_proba = lr_none.predict_proba(X)\n    assert_array_almost_equal(pred_proba.sum(axis=1), np.ones(X.shape[0]))\n", "def test_logistic_regression_auto_multiclass_solver():\n    # Test that 'auto' multiclass solver choice gives the same result as the\n    # explicitly chosen solver for different data.\n\n    # Create binary and multi-class problems.\n    X_bin, y_bin = make_classification(n_samples=100, n_classes=2,\n                                       random_state=0)\n    X_multi, y_multi = make_classification(n_samples=100, n_classes=3,\n                                           random_state=0)\n\n    solvers = ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']\n\n    # Check binary problem.\n    for solver in solvers:\n        lr_auto = LogisticRegression(solver=solver, multi_class='auto',\n                                     random_state=0)\n        lr_ovr = LogisticRegression(solver=solver, multi_class='ovr',\n                                    random_state=0)\n        lr_auto.fit(X_bin, y_bin)\n        lr_ovr.fit(X_bin, y_bin)\n        assert_array_almost_equal(lr_auto.coef_, lr_ovr.coef_)\n\n    # Check multi-class problem.\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        lr_auto = LogisticRegression(solver=solver, multi_class='auto',\n                                     random_state=0)\n        lr_multi = LogisticRegression(solver=solver, multi_class='multinomial',\n                                      random_state=0)\n        lr_auto.fit(X_multi, y_multi)\n        lr_multi.fit(X_multi, y_multi)\n        assert_array_almost_equal(lr_auto.coef_, lr_multi.coef_)\n\n    # Check liblinear with multi-class problem.\n    lr_auto = LogisticRegression(solver='liblinear', multi_class='auto',\n                                 random_state=0)\n    lr_ovr = LogisticRegression(solver='liblinear', multi_class='ovr',\n                                random_state=0)\n    lr_auto.fit(X_multi, y_multi)\n    lr_ovr.fit(X_multi, y_multi)\n    assert_array_almost_equal(lr_auto.coef_, lr_ovr.coef_)\n", "def test_logistic_regression_sparsify_multiclass():\n    # Test sparsify and densify members with multiclass case.\n\n    n_samples, n_features = iris.data.shape\n    target = iris.target_names[iris.target]\n\n    clf = LogisticRegression(random_state=0).fit(iris.data, target)\n\n    pred_d_d = clf.decision_function(iris.data)\n\n    clf.sparsify()\n    assert sp.issparse(clf.coef_)\n    pred_s_d = clf.decision_function(iris.data)\n\n    sp_data = sp.coo_matrix(iris.data)\n    pred_s_s = clf.decision_function(sp_data)\n\n    clf.densify()\n    pred_d_s = clf.decision_function(sp_data)\n\n    assert_array_almost_equal(pred_d_d, pred_s_d)\n    assert_array_almost_equal(pred_d_d, pred_s_s)\n    assert_array_almost_equal(pred_d_d, pred_d_s)\n", "def test_penalty_none_multiclass():\n    # Make sure setting penalty='none' is equivalent to setting C=np.inf with\n    # l2 penalty in the multiclass case.\n    X, y = make_classification(n_samples=1000, n_classes=3, random_state=0)\n\n    lr_none = LogisticRegression(penalty='none', solver='lbfgs',\n                                 multi_class='multinomial', random_state=0)\n    lr_l2_C_inf = LogisticRegression(penalty='l2', C=np.inf, solver='lbfgs',\n                                     multi_class='multinomial', random_state=0)\n    pred_none = lr_none.fit(X, y).predict(X)\n    pred_l2_C_inf = lr_l2_C_inf.fit(X, y).predict(X)\n    assert_array_equal(pred_none, pred_l2_C_inf)\n", "def test_logistic_regressionCV_multiclass_auto():\n    # Test that multi_class='auto' works as expected with LogisticRegressionCV\n\n    # Create a binary and a multiclass dataset\n    X, y_binary = make_classification(n_samples=100, n_features=5,\n                                      n_informative=3, n_redundant=0,\n                                      n_classes=2, random_state=0)\n    X, y_multiclass = make_classification(n_samples=100, n_features=5,\n                                          n_informative=3, n_redundant=0,\n                                          n_classes=3, random_state=0)\n\n    # Test that 'auto' gives the same result as 'ovr' for binary data\n    lr_ovr_binary = LogisticRegressionCV(multi_class='ovr', cv=2)\n    lr_auto_binary = LogisticRegressionCV(multi_class='auto', cv=2)\n    lr_ovr_binary.fit(X, y_binary)\n    lr_auto_binary.fit(X, y_binary)\n    assert_array_almost_equal(lr_ovr_binary.coef_, lr_auto_binary.coef_)\n\n    # Test that 'auto' gives the same result as 'multinomial' for multiclass\n    # data\n    lr_multiclass = LogisticRegressionCV(multi_class='multinomial', cv=2)\n    lr_auto_multiclass = LogisticRegressionCV(multi_class='auto', cv=2)\n    lr_multiclass.fit(X, y_multiclass)\n    lr_auto_multiclass.fit(X, y_multiclass)\n    assert_array_almost_equal(lr_multiclass.coef_, lr_auto_multiclass.coef_)\n"], "sample_834": ["def test_labels_validation():\n    # Test that invalid labels raise value error\n    X = np.arange(12).reshape(4, 3)\n    y = [1, 2, 3, 'a']\n    NCA = NeighborhoodComponentsAnalysis\n\n    # ValueError\n    assert_raises(ValueError, NCA().fit, X, y)\n\n    # check_classification_targets\n    y = [1, 1, 1, 1]\n    assert_warns_message(UserWarning,\n                         'Only one class present in y. NCA is not needed.',\n                         NCA().fit, X, y)\n", "def test_labels_validation():\n    # Test that invalid labels raise value error\n    X = np.arange(12).reshape(4, 3)\n    y = [1, 2, 3, 'a']\n\n    nca = NeighborhoodComponentsAnalysis()\n    assert_raises(ValueError, nca.fit, X, y)\n", "def test_components_read_only():\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis()\n    nca.fit(X, y)\n\n    with pytest.raises(AttributeError):\n        nca.components_ = np.random.rand(X.shape[1], X.shape[1])\n", "def test_labels_encoding():\n    X = iris_data\n    y = iris_target\n\n    # encode labels as strings\n    y_str = np.array([str(label) for label in y])\n\n    nca = NeighborhoodComponentsAnalysis()\n    nca.fit(X, y_str)\n\n    # check that labels are encoded correctly\n    assert_array_equal(nca.classes_, np.unique(y))\n", "def test_labels_not_0_based():\n    # Test that labels not 0-based are handled correctly.\n    X, y = iris_data, iris_target\n    y += 10  # Make labels not 0-based\n\n    nca = NeighborhoodComponentsAnalysis()\n    nca.fit(X, y)\n\n    # Check that the transformation is learned correctly\n    X_transformed = nca.transform(X)\n    assert_array_equal(X_transformed.shape, (X.shape[0], X.shape[1]))\n"], "sample_835": ["def test_adaboost_classifier_with_string_labels():\n    # Check that AdaBoostClassifier can handle string labels\n    X, y = datasets.load_iris(return_X_y=True)\n    y = np.array(['a' if label == 0 else 'b' if label == 1 else 'c' for label in y])\n\n    clf = AdaBoostClassifier()\n    clf.fit(X, y)\n\n    assert_array_equal(clf.classes_, np.unique(y))\n    assert clf.predict(X).shape == (X.shape[0],)\n", "def test_adaboost_regressor_with_constant_target():\n    # Check that AdaBoostRegressor works with constant target values.\n    # This is a regression test for:\n    # https://github.com/scikit-learn/scikit-learn/issues/15193\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([5, 5, 5])\n\n    model = AdaBoostRegressor(random_state=0)\n    model.fit(X, y)\n\n    assert_array_almost_equal(model.predict(X), y)\n", "def test_adaboost_with_missing_values():\n    # Check that AdaBoost estimators can handle missing values in the data.\n    from sklearn.impute import SimpleImputer\n    from sklearn.pipeline import Pipeline\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 0, 1])\n\n    # Introduce missing values\n    X[0, 0] = np.nan\n\n    for Estimator in [AdaBoostClassifier, AdaBoostRegressor]:\n        estimator = Estimator()\n        pipeline = Pipeline([\n            ('imputer', SimpleImputer()),\n            ('estimator', estimator)\n        ])\n\n        # Fit and predict should work without errors\n        pipeline.fit(X, y)\n        pipeline.predict(X)\n", "def test_adaboost_regressor_with_constant_target():\n    # check that AdaBoostRegressor works with constant target values\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([5, 5, 5])\n\n    model = AdaBoostRegressor()\n    model.fit(X, y)\n\n    assert_array_almost_equal(model.predict(X), y)\n", "def test_adaboost_regressor_with_constant_target():\n    # Check that AdaBoostRegressor works with constant target values.\n    # This is a regression test for issue #15869.\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 5])\n\n    model = AdaBoostRegressor()\n    model.fit(X, y)\n    assert_array_almost_equal(model.predict(X), y)\n"], "sample_836": ["def test_check_partial_fit_first_call():\n    # Test that _check_partial_fit_first_call raises an error when classes is None\n    clf = SVC()\n    assert_raises(ValueError, _check_partial_fit_first_call, clf)\n\n    # Test that _check_partial_fit_first_call sets classes_ when classes is not None\n    classes = [0, 1, 2]\n    _check_partial_fit_first_call(clf, classes=classes)\n    assert_array_equal(clf.classes_, classes)\n\n    # Test that _check_partial_fit_first_call raises an error when classes is different from classes_\n    classes_2 = [0, 1]\n    assert_raises(ValueError, _check_partial_fit_first_call, clf, classes=classes_2)\n\n    # Test that _check_partial_fit_first_call does not raise an error when classes is the same as classes_\n    _check_partial_fit_first_call(clf, classes=classes)\n", "def test_ovr_decision_function_ties():\n    # test tie breaking in ovr decision function\n\n    predictions = np.array([[0, 1, 1],\n                            [0, 1, 0],\n                            [0, 1, 1],\n                            [0, 1, 1]])\n\n    confidences = np.array([[-1e16, 0, -1e16],\n                            [1., 2., -3.],\n                            [-5., 2., 5.],\n                            [-0.5, 0.2, 0.5]])\n\n    n_classes = 3\n\n    dec_values = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # check that ties are broken correctly\n    tied_samples = np.where(np.ptp(dec_values, axis=1) == 0)[0]\n    if len(tied_samples) > 0:\n        tied_confidences = confidences[tied_samples]\n        tied_votes = np.array([np.sum(confidence == np.max(confidence)) for confidence in tied_confidences])\n        assert_array_equal(np.argmax(dec_values[tied_samples], axis=1), np.argmax(tied_votes))\n", "def test_ovr_decision_function_tie_breaking():\n    # test tie breaking properties for ovr decision function\n\n    predictions = np.array([[0, 1, 1],\n                            [0, 1, 0],\n                            [0, 1, 1],\n                            [0, 1, 1]])\n\n    confidences = np.array([[-1e16, 0, -1e16],\n                            [1., 2., -3.],\n                            [-5., 2., 5.],\n                            [-0.5, 0.2, 0.5]])\n\n    n_classes = 3\n\n    dec_values = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # check that ties are broken using confidence values\n    tied_samples = np.where(np.sum(dec_values == np.max(dec_values, axis=1)[:, np.newaxis], axis=1) > 1)[0]\n\n    if len(tied_samples) > 0:\n        winning_classes = np.argmax(dec_values[tied_samples], axis=1)\n        tied_confidences = confidences[tied_samples]\n        winning_confidences = np.max(tied_confidences, axis=1)\n        assert_array_equal(winning_classes, np.argmax(tied_confidences, axis=1))\n        assert_array_equal(winning_confidences, np.max(tied_confidences, axis=1))\n\n    # check that when all classes have the same confidence, we get a uniform decision value\n    uniform_confidences = np.array([[1., 1., 1.]])\n    uniform_predictions = np.array([[0, 1, 1]])\n    uniform_dec_values = _ovr_decision_function(uniform_predictions, uniform_confidences, n_classes)\n    assert_array_almost_equal(uniform_dec_values[0], np.array([1/3, 1/3, 1/3]))\n", "def test_type_of_target_confidence_scores():\n    # Test type_of_target with confidence scores\n    y = np.array([[0.7, 0.3], [0.4, 0.6]])\n    assert type_of_target(y) == 'continuous-multioutput'\n", "def test_ovr_decision_function_ties():\n    # test ties are broken correctly\n\n    predictions = np.array([[0, 1, 1],\n                            [0, 1, 0],\n                            [0, 1, 1],\n                            [0, 1, 1]])\n\n    confidences = np.array([[-1e16, 0, -1e16],\n                            [1., 2., -3.],\n                            [-5., 2., 5.],\n                            [-0.5, 0.2, 0.5]])\n\n    n_classes = 3\n\n    dec_values = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # check that ties are broken by confidence\n    confidences_tied = np.array([[-1e16, 0, -1e16],\n                                [1., 2., -3.],\n                                [-5., 2., 5.],\n                                [-0.5, 0.5, 0.5]])  # tie in confidence\n\n    dec_values_tied = _ovr_decision_function(predictions, confidences_tied, n_classes)\n\n    # tie should be won by class 1 (due to order of classes)\n    expected_prediction_tied = np.array([2, 1, 2, 1])\n    assert_array_equal(np.argmax(dec_values_tied, axis=1), expected_prediction_tied)\n\n    # check that the decision values reflect the tie breaking\n    assert (dec_values_tied[3, 1] > dec_values_tied[3, 2])\n"], "sample_837": ["def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n"], "sample_838": ["def test_column_transformer_remainder_with_sparse_threshold():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.5)\n\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n    assert_array_equal(X_trans, np.hstack((X_array[:, 0].reshape(-1, 1),\n                                           np.eye(X_array.shape[0]))))\n\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.1)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert_array_equal(X_trans.toarray(), np.hstack((X_array[:, 0].reshape(-1, 1),\n                                                     np.eye(X_array.shape[0]))))\n", "def test_column_transformer_remainder_with_no_features():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0, 1, 2])],\n                           remainder='passthrough')\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] != 'remainder'\n\n    ct = ColumnTransformer([('trans', Trans(), [0, 1, 2])],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert ct.get_feature_names() == ['trans__x0', 'remainder__x0', 'remainder__x1']\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    feature_names = ct.get_feature_names()\n    assert len(feature_names) == X_array.shape[1]\n    assert all(name.startswith('trans__') or name.startswith('remainder__')\n               for name in feature_names)\n", "def test_column_transformer_get_feature_names_with_empty_columns():\n    # Test that get_feature_names works correctly when some columns are empty\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', DictVectorizer(), [0]),\n                            ('trans2', 'drop', [1])])\n    ct.fit(X_array)\n    assert ct.get_feature_names() == ['trans1__']\n"], "sample_839": ["def test_vectorizer_analyzer_callable():\n    # analyzer can be a callable that returns a callable\n            return [w for w in doc.split() if len(w) > 2]\n        return inner\n\n    cv = CountVectorizer(analyzer=analyzer_callable())\n    X = cv.fit_transform([\"This is a test document.\"])\n    assert \"test\" in cv.vocabulary_\n    assert \"document\" in cv.vocabulary_\n\n    cv = CountVectorizer(analyzer=analyzer_callable)\n    X = cv.fit_transform([\"This is a test document.\"])\n    assert \"test\" in cv.vocabulary_\n    assert \"document\" in cv.vocabulary_\n", "def test_countvectorizer_empty_input():\n    # Test CountVectorizer with empty input\n    cv = CountVectorizer()\n    X = cv.fit_transform([])\n    assert X.shape == (0, 0)\n    assert cv.vocabulary_ == {}\n", "def test_vectorizer_stop_words_validation_with_custom_tokenizer():\n    class CustomTokenizer:\n            return [t for t in re.findall(r'\\w{2,}', text) if t not in ['and']]\n\n    message = ('Your stop_words may be inconsistent with your '\n               'preprocessing. Tokenizing the stop words generated '\n               'tokens %s not in stop_words.' % str(['you\\'ve']))\n\n    for vec in [CountVectorizer(), TfidfVectorizer()]:\n        vec.set_params(tokenizer=CustomTokenizer(),\n                       stop_words=[\"you've\", \"you\", \"you'll\", 'AND'])\n        assert_warns_message(UserWarning, message, vec.fit_transform,\n                             ['hello world you\\'ve'])\n\n    # Only one warning per stop list\n    assert_no_warnings(vec.fit_transform, ['hello world'])\n", "def test_tfidfvectorizer_dtype():\n    X = ['This is a sample document.', 'Another example document.']\n    vectorizer = TfidfVectorizer(dtype=np.float32)\n    X_tfidf = vectorizer.fit_transform(X)\n    assert X_tfidf.dtype == np.float32\n\n    # Test that providing an invalid dtype raises an error\n    vectorizer = TfidfVectorizer(dtype='invalid')\n    with pytest.raises(ValueError):\n        vectorizer.fit_transform(X)\n", "def test_countvectorizer_inverse_transform_with_stop_words():\n    stop_words = ['the', 'and']\n    cv = CountVectorizer(stop_words=stop_words)\n    X = cv.fit_transform(ALL_FOOD_DOCS)\n    inversed_data = cv.inverse_transform(X)\n    for doc, inversed_terms in zip(ALL_FOOD_DOCS, inversed_data):\n        terms = [term for term in cv.build_analyzer()(doc) if term not in stop_words]\n        assert_array_equal(np.sort(np.unique(terms)), np.sort(np.unique(inversed_terms)))\n"], "sample_840": ["def test_pls_predict_shape():\n    # Test that the shape of the predictions is correct\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        y_pred = clf.predict(X)\n        assert y_pred.shape == Y.shape\n", "def test_pls_predict_shape():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        y_pred = clf.predict(X)\n\n        # check shape of predictions\n        assert y_pred.shape == Y.shape\n", "def test_pls_invalid_input():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    # Invalid n_components\n    pls = pls_.PLSCanonical(n_components=0)\n    assert_raise_message(ValueError, \"Invalid number of components\",\n                         pls.fit, X, Y)\n\n    pls = pls_.PLSRegression(n_components=-1)\n    assert_raise_message(ValueError, \"Invalid number of components\",\n                         pls.fit, X, Y)\n\n    pls = pls_.PLSSVD(n_components=\"a\")\n    assert_raise_message(ValueError, \"Invalid number of components\",\n                         pls.fit, X, Y)\n\n    # Invalid algorithm\n    pls = pls_.PLSCanonical(algorithm=\"a\")\n    assert_raise_message(ValueError, \"Got algorithm a when only 'svd' \"\n                         \"and 'nipals' are known\", pls.fit, X, Y)\n\n    # Invalid deflation mode\n    pls = pls_.PLSCanonical(deflation_mode=\"a\")\n    assert_raise_message(ValueError, \"The deflation mode is unknown\",\n                         pls.fit, X, Y)\n\n    # Invalid mode\n    pls = pls_.PLSCanonical(mode=\"a\")\n    assert_raise_message(ValueError, \"Invalid mode\",\n                         pls.fit, X, Y)\n", "def test_pls_algorithm():\n    # Check that algorithm='svd' and algorithm='nipals' give the same results\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls_nipals = pls_.PLSCanonical(n_components=X.shape[1], algorithm='nipals')\n    pls_svd = pls_.PLSCanonical(n_components=X.shape[1], algorithm='svd')\n\n    pls_nipals.fit(X, Y)\n    pls_svd.fit(X, Y)\n\n    assert_array_almost_equal(pls_nipals.x_weights_, pls_svd.x_weights_)\n    assert_array_almost_equal(pls_nipals.y_weights_, pls_svd.y_weights_)\n    assert_array_almost_equal(pls_nipals.x_loadings_, pls_svd.x_loadings_)\n    assert_array_almost_equal(pls_nipals.y_loadings_, pls_svd.y_loadings_)\n", "def test_pls_scores_shape():\n    # check shape of scores\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        assert clf.x_scores_.shape == (X.shape[0], clf.n_components)\n        assert clf.y_scores_.shape == (Y.shape[0], clf.n_components)\n"], "sample_841": ["def test_ridge_regression_dtype_stability_sample_weights(solver):\n    random_state = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = random_state.randn(n_samples, n_features)\n    coef = random_state.randn(n_features)\n    y = np.dot(X, coef) + 0.01 * random_state.randn(n_samples)\n    sample_weight = random_state.rand(n_samples)\n    alpha = 1.0\n    results = dict()\n    atol = 1e-3 if solver == \"sparse_cg\" else 1e-5\n    for current_dtype in (np.float32, np.float64):\n        results[current_dtype] = ridge_regression(\n            X.astype(current_dtype), y.astype(current_dtype),\n            alpha=alpha, solver=solver, sample_weight=sample_weight,\n            max_iter=500, tol=1e-10, return_n_iter=False,\n            return_intercept=False)\n\n    assert results[np.float32].dtype == np.float32\n    assert results[np.float64].dtype == np.float64\n    assert_allclose(results[np.float32], results[np.float64], atol=atol)\n", "def test_ridgecv_with_multitarget_and_sample_weights():\n    # Test RidgeCV with multi-target regression and sample weights.\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([[1, 2], [1, 3], [1, 1], [-1, -2], [-1, -3]])\n\n    sample_weights = np.array([1, 1, 1, 1, 1])\n\n    ridgecv = RidgeCV(alphas=[1e-1, 1e0, 1e1], cv=None)\n    ridgecv.fit(X, y, sample_weight=sample_weights)\n\n    assert hasattr(ridgecv, 'coef_')\n    assert hasattr(ridgecv, 'intercept_')\n    assert hasattr(ridgecv, 'alpha_')\n\n    assert isinstance(ridgecv.coef_, np.ndarray)\n    assert isinstance(ridgecv.intercept_, np.ndarray)\n    assert isinstance(ridgecv.alpha_, float)\n\n    assert ridgecv.coef_.shape == (y.shape[1], X.shape[1])\n    assert ridgecv.intercept_.shape == (y.shape[1], )\n", "def test_ridge_cv_with_saga_solver():\n    # Check that RidgeCV works with 'saga' solver\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n    alphas = [0.1, 1.0, 10.0]\n    ridge_cv = RidgeCV(alphas=alphas, solver='saga', cv=5)\n    ridge_cv.fit(X, y)\n    assert ridge_cv.coef_.shape == (X.shape[1],)\n    assert ridge_cv.alpha_ in alphas\n", "def test_ridge_saga_solver_convergence():\n    # Test that saga solver converges to the same solution as svd solver\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n    ridge_svd = Ridge(solver='svd', alpha=1.0)\n    ridge_saga = Ridge(solver='saga', alpha=1.0, max_iter=10000, tol=1e-6)\n    ridge_svd.fit(X, y)\n    ridge_saga.fit(X, y)\n    assert_allclose(ridge_svd.coef_, ridge_saga.coef_)\n    assert_allclose(ridge_svd.intercept_, ridge_saga.intercept_)\n", "def test_ridge_saga_solver():\n    # Test that the SAGA solver works as expected\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n    ridge_saga = Ridge(solver='saga', max_iter=1000, tol=1e-10)\n    ridge_saga.fit(X, y)\n\n    ridge_sv = Ridge(solver='svd')\n    ridge_sv.fit(X, y)\n\n    assert_allclose(ridge_saga.coef_, ridge_sv.coef_, atol=1e-3)\n    assert_allclose(ridge_saga.intercept_, ridge_sv.intercept_, atol=1e-3)\n"], "sample_842": ["def test_kernel_str(kernel):\n    # Smoke-test for str in kernels.\n\n    str(kernel)\n", "def test_kernel_operator_repr(kernel):\n    # Smoke-test for repr in kernel operators.\n\n    kernel_operator = 2.0 * kernel + 3.0 * kernel\n    repr(kernel_operator)\n", "def test_kernel_str(kernel):\n    # Smoke-test for str representation of kernels.\n\n    assert isinstance(str(kernel), str)\n", "def test_clone_with_non_default_hyperparameters(kernel):\n    # Test that clone works correctly when kernel has non-default hyperparameters\n\n    # Set non-default hyperparameters for the kernel\n    params = kernel.get_params()\n    for param_name, value in params.items():\n        if isinstance(value, (int, float)):\n            params[param_name] *= 2\n        elif isinstance(value, (list, tuple)):\n            params[param_name] = [v * 2 for v in value]\n\n    kernel.set_params(**params)\n\n    # Clone the kernel\n    kernel_cloned = clone(kernel)\n\n    # Check that all constructor parameters are equal.\n    assert kernel.get_params() == kernel_cloned.get_params()\n\n    # Check that all hyperparameters are equal.\n    check_hyperparameters_equal(kernel, kernel_cloned)\n", "def test_kernel_str(kernel):\n    # Smoke-test for str in kernels.\n\n    str(kernel)\n"], "sample_843": ["def test_kernel_operator_compatibility():\n    # Test that kernel operators can handle different types of kernels.\n    k1 = RBF(length_scale=1.0)\n    k2 = ConstantKernel(constant_value=5.0)\n\n    # Check addition\n    assert isinstance(k1 + k2, Sum)\n    assert isinstance(k2 + k1, Sum)\n\n    # Check multiplication\n    assert isinstance(k1 * k2, Product)\n    assert isinstance(k2 * k1, Product)\n\n    # Check exponentiation\n    assert isinstance(k1 ** 2, Exponentiation)\n", "def test_kernel_operator_bounds():\n    # Check that bounds of kernel operators are correctly determined.\n    kernel1 = RBF(length_scale=1.0, length_scale_bounds=(1e-5, 1e5))\n    kernel2 = RBF(length_scale=2.0, length_scale_bounds=(1e-3, 1e3))\n\n    # Addition\n    kernel_sum = kernel1 + kernel2\n    assert_array_almost_equal(kernel_sum.bounds,\n                              np.log(np.array([[1e-5, 1e5], [1e-3, 1e3]])))\n\n    # Multiplication\n    kernel_product = kernel1 * kernel2\n    assert_array_almost_equal(kernel_product.bounds,\n                              np.log(np.array([[1e-5, 1e5], [1e-3, 1e3]])))\n\n    # Exponentiation\n    kernel_exp = kernel1 ** 2\n    assert_array_almost_equal(kernel_exp.bounds,\n                              np.log(np.array([[1e-5, 1e5]])))\n", "def test_kernel_operator_type_error():\n    # Test that kernel operator raises a TypeError when inputs are not kernels.\n\n    with pytest.raises(TypeError):\n        Sum(1.0, RBF())\n    with pytest.raises(TypeError):\n        Product(1.0, RBF())\n    with pytest.raises(TypeError):\n        Exponentiation(1.0, 2.0)\n", "def test_kernel_operator_equality():\n    # Test equality of kernel operators.\n    k1 = RBF(length_scale=2.0)\n    k2 = 3.0 * RBF(length_scale=2.0)\n    k3 = ConstantKernel(constant_value=3.0) * RBF(length_scale=2.0)\n    k4 = ConstantKernel(constant_value=3.0) * RBF(length_scale=2.0)\n    assert k2 == k3\n    assert k3 == k4\n    assert k2 != k1\n    assert k3 != k1\n    assert k1 != k4\n", "def test_kernel_equality():\n    # Test equality of kernels.\n    kernel1 = RBF(length_scale=2.0)\n    kernel2 = RBF(length_scale=2.0)\n    assert kernel1 == kernel2\n\n    kernel3 = RBF(length_scale=3.0)\n    assert kernel1 != kernel3\n\n    kernel4 = ConstantKernel(constant_value=2.0)\n    assert kernel1 != kernel4\n\n    kernel5 = RBF(length_scale=[2.0, 2.0])\n    kernel6 = RBF(length_scale=[2.0, 2.0])\n    assert kernel5 == kernel6\n\n    kernel7 = RBF(length_scale=[2.0, 3.0])\n    assert kernel5 != kernel7\n"], "sample_844": ["def test_optics_fit_predict():\n    # Test that fit_predict works and gives the same result as fit + predict\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 10\n    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2))\n\n    clust1 = OPTICS(min_samples=3, xi=.05)\n    labels1 = clust1.fit_predict(X)\n\n    clust2 = OPTICS(min_samples=3, xi=.05)\n    clust2.fit(X)\n    labels2 = clust2.labels_\n\n    assert_array_equal(labels1, labels2)\n", "def test_optics_with_n_jobs():\n    # testing that using n_jobs in OPTICS does not change the results\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 20\n    C1 = [-5, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .2 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, 2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3, C4))\n\n    clust1 = OPTICS(cluster_method='dbscan', eps=.5, n_jobs=1).fit(X)\n    clust2 = OPTICS(cluster_method='dbscan', eps=.5, n_jobs=-1).fit(X)\n\n    assert_array_equal(clust1.labels_, clust2.labels_)\n    assert_allclose(clust1.reachability_, clust2.reachability_)\n", "def test_optics_fit_predict():\n    # Test that fit_predict works as expected\n    clust = OPTICS(min_samples=10, algorithm='brute')\n    labels = clust.fit_predict(X)\n    assert_array_equal(labels, clust.labels_)\n", "def test_optics_input_validation():\n    # Test OPTICS input validation\n\n    # Check that an error is raised when the input is not an array\n    msg = \"Input should be a square array\"\n    with pytest.raises(ValueError, match=msg):\n        OPTICS().fit(\"not_an_array\")\n\n    # Check that an error is raised when the input array is not square\n    msg = \"Input should be a square array\"\n    with pytest.raises(ValueError, match=msg):\n        OPTICS().fit(np.array([[1, 2], [3, 4], [5, 6]]))\n\n    # Check that an error is raised when the metric is not valid\n    msg = \"Invalid metric 'not_a_metric'\"\n    with pytest.raises(ValueError, match=msg):\n        OPTICS(metric=\"not_a_metric\").fit(X)\n\n    # Check that an error is raised when the algorithm is not valid\n    msg = \"algorithm must be one of 'auto', 'ball_tree', 'kd_tree', 'brute'\"\n    with pytest.raises(ValueError, match=msg):\n        OPTICS(algorithm=\"not_an_algorithm\").fit(X)\n", "def test_optics_fit_with_n_jobs():\n    # test that the fit method works with n_jobs > 1\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 20\n    C1 = [-5, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2))\n\n    clust = OPTICS(min_samples=10, algorithm='ball_tree',\n                   metric='euclidean', n_jobs=2).fit(X)\n\n    assert_array_equal(np.sort(np.unique(clust.labels_)), [-1, 0, 1])\n"], "sample_845": ["def test_vectorizer_dtype():\n    # Test that vectorizers return matrices with the correct dtype.\n    X = [\"This is a sample document.\", \"Another document for testing.\"]\n\n    vectorizers = [\n        CountVectorizer(),\n        TfidfVectorizer(),\n        HashingVectorizer(dtype=np.float32)\n    ]\n\n    expected_dtypes = [np.int64, np.float64, np.float32]\n\n    for vect, expected_dtype in zip(vectorizers, expected_dtypes):\n        result = vect.fit_transform(X)\n        assert result.dtype == expected_dtype\n", "def test_countvectorizer_vocabulary_attribute():\n    # Test that the vocabulary_ attribute is correctly set after fitting.\n    cv = CountVectorizer()\n    cv.fit(ALL_FOOD_DOCS)\n    assert isinstance(cv.vocabulary_, dict)\n    assert len(cv.vocabulary_) == len(cv.get_feature_names())\n    for feature in cv.get_feature_names():\n        assert feature in cv.vocabulary_\n", "def test_tfidfvectorizer_dtype():\n    # Test that TfidfVectorizer produces the correct dtype\n    X = ['hello world', 'hello hello']\n    vect = TfidfVectorizer(dtype=np.float32)\n    X_trans = vect.fit_transform(X)\n    assert X_trans.dtype == np.float32\n\n    vect = TfidfVectorizer(dtype=np.float64)\n    X_trans = vect.fit_transform(X)\n    assert X_trans.dtype == np.float64\n", "def test_vectorizer_analyzer_custom_tokenization():\n    # Test that a custom analyzer with tokenization works as expected\n\n    class CustomAnalyzer:\n            # Custom tokenization: split on spaces and convert to lowercase\n            return [t.lower() for t in doc.split()]\n\n    data = [\"This is a test document\", \"Another test document\"]\n    vectorizer = CountVectorizer(analyzer=CustomAnalyzer())\n    X = vectorizer.fit_transform(data)\n\n    # Check that the vocabulary is correct\n    vocab = vectorizer.vocabulary_\n    assert set(vocab.keys()) == {\"this\", \"is\", \"a\", \"test\", \"document\", \"another\"}\n\n    # Check that the transformed data is correct\n    expected_X = sparse.csr_matrix([[1, 1, 1, 1, 1, 0],\n                                    [0, 0, 0, 1, 1, 1]])\n    assert_array_equal(X.toarray(), expected_X.toarray())\n", "def test_vectorizer_stop_words_custom_tokenizer():\n    # check that the warning about inconsistent stop words is raised\n    # when a custom tokenizer is used\n    class CustomTokenizer:\n            return [t for t in doc.split() if t != 'and']\n\n    vec = CountVectorizer(tokenizer=CustomTokenizer(), stop_words=['and'])\n    message = ('Your stop_words may be inconsistent with your '\n               'preprocessing. Tokenizing the stop words generated '\n               'tokens %s not in stop_words.' % \"[]\")\n    assert_warns_message(UserWarning, message, vec.fit_transform,\n                         ['hello and world'])\n"], "sample_846": ["def test_column_transformer_feature_names_out():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Single transformer with feature names\n    ct = ColumnTransformer([('trans', StandardScaler(), [0, 1])])\n    ct.fit(X)\n    assert ct.get_feature_names_out() == ['trans__x0', 'trans__x1']\n\n    # Multiple transformers with feature names\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(), [1, 2])])\n    ct.fit(X)\n    assert ct.get_feature_names_out() == ['trans1__x0', 'trans2__x1', 'trans2__x2']\n\n    # 'drop' transformer\n    ct = ColumnTransformer([('trans', 'drop', [0]),\n                            ('trans2', StandardScaler(), [1, 2])])\n    ct.fit(X)\n    assert ct.get_feature_names_out() == ['trans2__x1', 'trans2__x2']\n\n    # 'passthrough' transformer\n    ct = ColumnTransformer([('trans', 'passthrough', [0]),\n                            ('trans2', StandardScaler(), [1, 2])])\n    ct.fit(X)\n    assert ct.get_feature_names_out() == ['trans__x0', 'trans2__x1', 'trans2__x2']\n", "def test_column_transformer_feature_names_with_drop():\n    # Test that feature names are correctly generated when using 'drop'\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', 'drop', [1]),\n                            ('trans3', StandardScaler(), [2])])\n    ct.fit(X)\n    feature_names = ct.get_feature_names()\n    assert feature_names == ['trans1__x0', 'trans3__x2']\n", "def test_column_transformer_get_feature_names_with_empty_columns():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', Trans(), [])],\n                           remainder='passthrough')\n    ct.fit(X_array)\n    assert ct.get_feature_names() == ['remainder__x0', 'remainder__x1']\n", "def test_column_transformer_remainder_with_negative_indexing():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    X_res_both = X_array\n\n    # specify passthrough with negative indexing\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder='passthrough')\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [1])\n\n    ct = ColumnTransformer([('trans', Trans(), [-3])],\n                           remainder='passthrough')\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [0, 1])\n", "def test_column_transformer_supports_dataframe_with_missing_values():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n    X_df.iloc[1, 0] = np.nan\n\n    ct = ColumnTransformer([('trans', Trans(), ['first'])],\n                           remainder='drop')\n\n    # this shouldn't raise any error and should give the expected result\n    exp = np.array([0, np.nan, 2]).reshape(-1, 1)\n    assert_array_equal(ct.fit_transform(X_df), exp)\n    assert_array_equal(ct.fit(X_df).transform(X_df), exp)\n\n    # same test but with passthrough to ensure it doesn't impact\n    ct = ColumnTransformer([('trans', Trans(), ['first'])],\n                           remainder='passthrough')\n    exp = X_df\n    assert_array_equal(ct.fit_transform(X_df), exp)\n    assert_array_equal(ct.fit(X_df).transform(X_df), exp)\n"], "sample_847": ["def test_enet_coordinate_descent_multithreading():\n    \"\"\"Test that multithreading in enet_coordinate_descent gives the same result\n    as single-threading\"\"\"\n    X, y, _, _ = build_dataset(n_samples=1000, n_features=500)\n    clf_single_threading = ElasticNet(max_iter=1000, n_jobs=1)\n    clf_multithreading = ElasticNet(max_iter=1000, n_jobs=-1)\n    clf_single_threading.fit(X, y)\n    clf_multithreading.fit(X, y)\n    assert_array_almost_equal(clf_single_threading.coef_, clf_multithreading.coef_)\n", "def test_enet_coordinate_descent_coef_init():\n    \"\"\"Test that ElasticNet accepts a custom initialization for the coefficients.\"\"\"\n    X, y, _, _ = build_dataset()\n    clf = ElasticNet(max_iter=1000)\n    clf.fit(X, y)\n\n    # Introduce a small perturbation in the coefficients\n    coef_init = clf.coef_ + np.random.randn(*clf.coef_.shape) * 1e-2\n\n    clf_custom_init = ElasticNet(max_iter=1000, coef_init=coef_init)\n    clf_custom_init.fit(X, y)\n\n    assert_array_almost_equal(clf.coef_, clf_custom_init.coef_)\n", "def test_lassoCV_positive_parameter():\n    # Test that LassoCV can handle positive parameter correctly\n    X, y, _, _ = build_dataset()\n    clf = LassoCV(positive=True, cv=2)\n    clf.fit(X, y)\n    assert np.all(clf.coef_ >= 0)\n\n    clf = LassoCV(positive=False, cv=2)\n    clf.fit(X, y)\n    assert not np.all(clf.coef_ >= 0)\n", "def test_enet_coordinate_descent_sparse_input_dtype():\n    \"\"\"Test ElasticNet with sparse input and different dtypes\"\"\"\n    X, y, _, _ = build_dataset(n_samples=50, n_features=100)\n    clf = ElasticNet()\n    X_sparse = sparse.csr_matrix(X)\n\n    # float64\n    clf.fit(X_sparse, y)\n    coef_64 = clf.coef_\n    # float32\n    clf.fit(X_sparse.astype(np.float32), y)\n    coef_32 = clf.coef_\n    assert_array_almost_equal(coef_64, coef_32, decimal=5)\n", "def test_multitask_enet_coordinate_descent():\n    \"\"\"Test that a warning is issued if model does not converge\"\"\"\n    clf = MultiTaskElasticNet(max_iter=2)\n    n_samples = 5\n    n_features = 2\n    X = np.ones((n_samples, n_features)) * 1e50\n    y = np.ones((n_samples, 2))\n    assert_warns(ConvergenceWarning, clf.fit, X, y)\n"], "sample_848": ["def test_multi_output_estimator_tags():\n    # Test that the multioutput estimator has the correct tags\n    sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)\n    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)\n\n    assert not multi_target_linear._get_tags()['multioutput']\n    assert multi_target_linear._get_tags()['multioutput_only']\n\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    assert not rgr._get_tags()['multioutput']\n    assert rgr._get_tags()['multioutput_only']\n", "def test_multi_output_regressor_with_cross_val_predict():\n    # Test multi output regressor with cross_val_predict\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test = X[50:]\n\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    rgr.fit(X_train, y_train)\n\n    y_pred = cross_val_predict(rgr, X_train, y_train, cv=3)\n    assert y_pred.shape == y_train.shape\n", "def test_regressor_chain_crossval_fit_and_predict_sparse_data():\n    # Fit regressor chain with cross_val_predict and verify predict\n    # performance using sparse data\n    X, Y = generate_multilabel_dataset_with_correlations()\n    X_sparse = sp.csr_matrix(X)\n\n    chain = RegressorChain(Ridge(), cv=3)\n    chain.fit(X_sparse, Y)\n    Y_pred = chain.predict(X_sparse)\n\n    assert Y_pred.shape == Y.shape\n    assert mean_squared_error(Y, Y_pred) < .25\n", "def test_regressor_chain_crossval_fit_and_predict_with_sparse_data():\n    # Fit regressor chain with cross_val_predict and verify predict\n    # performance using sparse data\n    X, Y = generate_multilabel_dataset_with_correlations()\n    X_sparse = sp.csr_matrix(X)\n\n    chain = RegressorChain(Ridge(), cv=3)\n    chain.fit(X_sparse, Y)\n    Y_pred_cv_sparse = chain.predict(X_sparse)\n\n    chain = RegressorChain(Ridge(), cv=3)\n    chain.fit(X, Y)\n    Y_pred_cv_dense = chain.predict(X)\n\n    assert_array_almost_equal(Y_pred_cv_sparse, Y_pred_cv_dense)\n", "def test_multi_output_regressor_score():\n    # Test multi output regressor score method\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    regr_mult = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    regr_mult.fit(X_train, y_train)\n\n    score_mult = regr_mult.score(X_test, y_test)\n    assert isinstance(score_mult, float)\n\n    # compare with single target regressors\n    scores_single = []\n    for n in range(3):\n        regr = GradientBoostingRegressor(random_state=0)\n        regr.fit(X_train, y_train[:, n])\n        scores_single.append(regr.score(X_test, y_test[:, n]))\n\n    score_single = np.mean(scores_single)\n    assert_almost_equal(score_mult, score_single)\n"], "sample_849": ["def test_validate_shuffle_split():\n    # Test if _validate_shuffle_split returns the correct train and test sizes\n    n_samples = 10\n\n    # Test when both train_size and test_size are specified as fractions\n    train_size, test_size = 0.7, 0.3\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == int(train_size * n_samples)\n    assert n_test == int(test_size * n_samples)\n\n    # Test when only train_size is specified as a fraction\n    train_size, test_size = 0.7, None\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == int(train_size * n_samples)\n    assert n_test == n_samples - n_train\n\n    # Test when only test_size is specified as a fraction\n    train_size, test_size = None, 0.3\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_test == int(test_size * n_samples)\n    assert n_train == n_samples - n_test\n\n    # Test when both train_size and test_size are specified as integers\n    train_size, test_size = 7, 3\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == train_size\n    assert n_test == test_size\n\n    # Test when only train_size is specified as an integer\n    train_size, test_size = 7, None\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == train_size\n    assert n_test == n_samples - n_train\n\n    # Test when only test_size is specified as an integer\n    train_size, test_size = None, 3\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_test == test_size\n    assert n_train == n_samples - n_test\n\n    # Test when train_size and test_size do not add up to n_samples\n    train_size, test_size = 8, 3\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size, train_size)\n", "def test_validate_shuffle_split():\n    # Test if _validate_shuffle_split returns the correct train and test sizes\n    n_samples = 10\n    for test_size, train_size in [(2, 8), (0.2, 0.8), (None, 8), (2, None)]:\n        n_train, n_test = _validate_shuffle_split(\n            n_samples, test_size, train_size)\n        assert n_train + n_test == n_samples\n\n    # Test if _validate_shuffle_split raises an error when test_size or\n    # train_size is invalid\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, -1, None)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, 1.1, None)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, None, -1)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, None, 1.1)\n\n    # Test if _validate_shuffle_split returns the same result for different\n    # types of input (int, float, np.int64, np.float64)\n    test_sizes = [2, 2.0, np.int64(2), np.float64(2)]\n    train_sizes = [8, 8.0, np.int64(8), np.float64(8)]\n\n    for test_size in test_sizes:\n        for train_size in train_sizes:\n            n_train, n_test = _validate_shuffle_split(\n                n_samples, test_size, train_size)\n            assert n_train == 8\n            assert n_test == 2\n", "def test_split_with_dataframe():\n    import pandas as pd\n\n    # create a dataframe\n    df = pd.DataFrame({\n        'A': [1, 2, 3, 4, 5],\n        'B': [11, 12, 13, 14, 15]\n    })\n\n    X = df[['A']]\n    y = df['B']\n\n    # test train_test_split with dataframe\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    assert isinstance(X_train, pd.DataFrame)\n    assert isinstance(X_test, pd.DataFrame)\n    assert isinstance(y_train, pd.Series)\n    assert isinstance(y_test, pd.Series)\n\n    # test KFold with dataframe\n    kf = KFold(n_splits=2)\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n        assert isinstance(X_train, pd.DataFrame)\n        assert isinstance(X_test, pd.DataFrame)\n        assert isinstance(y_train, pd.Series)\n        assert isinstance(y_test, pd.Series)\n", "def test_validate_shuffle_split():\n    # Test the validation of test_size and train_size\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.6, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 8, 3)\n    assert_raises(TypeError, _validate_shuffle_split, 10, 1.5, None)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 11, None)\n    assert_raises(ValueError, _validate_shuffle_split, 10, -1, None)\n    assert_raises(ValueError, _validate_shuffle_split, 10, -.6, None)\n\n    # Test that it doesn't raise an error when test_size and train_size are\n    # valid.\n    _validate_shuffle_split(10, 0.2, None)\n    _validate_shuffle_split(10, None, 0.8)\n    _validate_shuffle_split(10, 0.2, 0.8)\n    _validate_shuffle_split(10, 2, None)\n    _validate_shuffle_split(10, None, 8)\n    _validate_shuffle_split(10, 2, 8)\n", "def test_validate_shuffle_split():\n    # Test if _validate_shuffle_split returns the correct train and test sizes\n    n_samples = 20\n\n    # Test when both train_size and test_size are specified as int\n    train_size, test_size = 10, 5\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == train_size\n    assert n_test == test_size\n\n    # Test when both train_size and test_size are specified as float\n    train_size, test_size = 0.5, 0.25\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == int(train_size * n_samples)\n    assert n_test == int(test_size * n_samples)\n\n    # Test when only train_size is specified as int\n    train_size = 15\n    n_train, n_test = _validate_shuffle_split(n_samples, None, train_size)\n    assert n_train == train_size\n    assert n_test == n_samples - n_train\n\n    # Test when only train_size is specified as float\n    train_size = 0.75\n    n_train, n_test = _validate_shuffle_split(n_samples, None, train_size)\n    assert n_train == int(train_size * n_samples)\n    assert n_test == n_samples - n_train\n\n    # Test when only test_size is specified as int\n    test_size = 5\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, None)\n    assert n_test == test_size\n    assert n_train == n_samples - n_test\n\n    # Test when only test_size is specified as float\n    test_size = 0.25\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, None)\n    assert n_test == int(test_size * n_samples)\n    assert n_train == n_samples - n_test\n\n    # Test when neither train_size nor test_size is specified\n    n_train, n_test = _validate_shuffle_split(n_samples, None, None,\n                                              default_test_size=0.25)\n    assert n_test == int(0.25 * n_samples)\n    assert n_train == n_samples - n_test\n"], "sample_850": ["def test_nystroem_fit_transform_sparse():\n    # Test Nystroem fit and transform on sparse data.\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n    X_sparse = csr_matrix(X)\n\n    nystroem = Nystroem(n_components=5)\n    X_transformed_sparse = nystroem.fit_transform(X_sparse)\n    X_transformed_dense = nystroem.fit_transform(X)\n\n    assert_array_almost_equal(X_transformed_sparse, X_transformed_dense)\n", "def test_nystroem_invalid_kernel():\n    # Test that invalid kernels raise a ValueError\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    nystroem = Nystroem(kernel=' invalid_kernel', n_components=X.shape[0])\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n\n    nystroem = Nystroem(kernel=None, n_components=X.shape[0])\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n\n    nystroem = Nystroem(kernel=123, n_components=X.shape[0])\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n", "def test_nystroem_kernel_params():\n    # Test that kernel parameters are passed correctly to the pairwise kernel.\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    # test gamma parameter for rbf kernel\n    nystroem = Nystroem(kernel=\"rbf\", n_components=X.shape[0], gamma=2)\n    X_transformed = nystroem.fit_transform(X)\n    K = rbf_kernel(X, gamma=2)\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n\n    # test degree and coef0 parameters for polynomial kernel\n    nystroem = Nystroem(kernel=\"polynomial\", n_components=X.shape[0],\n                        degree=3, coef0=2)\n    X_transformed = nystroem.fit_transform(X)\n    K = polynomial_kernel(X, degree=3, coef0=2)\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n", "def test_nystroem_sparse_input():\n    # Test Nystroem on sparse input.\n    rnd = np.random.RandomState(42)\n    n_samples = 10\n    X = csr_matrix(rnd.uniform(size=(n_samples, 4)))\n\n    nystroem = Nystroem(n_components=5)\n    X_transformed = nystroem.fit_transform(X)\n    assert X_transformed.shape == (n_samples, 5)\n\n    K = rbf_kernel(X.toarray())\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n", "def test_nystroem_sparse_input():\n    # Test Nystroem on sparse input.\n    from scipy.sparse import csr_matrix\n    rnd = np.random.RandomState(42)\n    n_samples = 10\n    X = csr_matrix(rnd.uniform(size=(n_samples, 4)))\n\n    nystroem = Nystroem(n_components=n_samples)\n    X_transformed = nystroem.fit_transform(X)\n\n    K = rbf_kernel(X.toarray())\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n"], "sample_851": ["def test_mean_tweedie_deviance_power_parameter():\n    y_true = np.array([1.0, 2.0, 3.0])\n    y_pred = np.array([1.1, 1.9, 3.2])\n\n    # check that power parameter is validated\n    with pytest.raises(ValueError,\n                       match=\"is only defined for power<=0 and power>=1\"):\n        mean_tweedie_deviance(y_true, y_pred, power=0.5)\n\n    # check that power parameter is validated in multioutput case\n    y_true = np.array([[1.0, 2.0], [3.0, 4.0]])\n    y_pred = np.array([[1.1, 1.9], [3.2, 4.1]])\n    with pytest.raises(ValueError,\n                       match=\"is only defined for power<=0 and power>=1\"):\n        mean_tweedie_deviance(y_true, y_pred, power=0.5)\n", "def test_mean_tweedie_deviance_with_sample_weight():\n    y_true = np.array([1.0, 2.0, 3.0, 4.0])\n    y_pred = np.array([1.5, 2.5, 3.5, 4.5])\n    sample_weight = np.array([0.5, 0.5, 0.5, 0.5])\n\n    # Test that sample weights are properly applied\n    assert_almost_equal(\n        mean_tweedie_deviance(y_true, y_pred, power=0, sample_weight=sample_weight),\n        mean_squared_error(y_true, y_pred, sample_weight=sample_weight)\n    )\n\n    # Test with different powers\n    for power in [1, 2]:\n        assert_allclose(\n            mean_tweedie_deviance(y_true, y_pred, power=power, sample_weight=sample_weight),\n            mean_tweedie_deviance(y_true, y_pred, power=power)\n        )\n", "def test_regression_metrics_with_zero_weights():\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([1, 2, 3, 5])\n    sample_weight = np.array([1, 1, 1, 0])\n\n    assert_almost_equal(mean_squared_error(y_true, y_pred, sample_weight=sample_weight), 0.00, 2)\n    assert_almost_equal(mean_absolute_error(y_true, y_pred, sample_weight=sample_weight), 0.00, 2)\n    assert_almost_equal(r2_score(y_true, y_pred, sample_weight=sample_weight), 1.00, 2)\n    assert_almost_equal(explained_variance_score(y_true, y_pred, sample_weight=sample_weight), 1.00, 2)\n\n    # Test with zero total weight\n    sample_weight = np.array([0, 0, 0, 0])\n    msg = \"Sample weights sum to zero.\"\n    with pytest.raises(ValueError, match=msg):\n        mean_squared_error(y_true, y_pred, sample_weight=sample_weight)\n    with pytest.raises(ValueError, match=msg):\n        mean_absolute_error(y_true, y_pred, sample_weight=sample_weight)\n    with pytest.raises(ValueError, match=msg):\n        r2_score(y_true, y_pred, sample_weight=sample_weight)\n    with pytest.raises(ValueError, match=msg):\n        explained_variance_score(y_true, y_pred, sample_weight=sample_weight)\n", "def test_mean_poisson_deviance():\n    # Poisson deviance is equivalent to the Tweedie deviance with power=1.\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred),\n                        mean_tweedie_deviance(y_true, y_pred, power=1))\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred),\n                        1.4260346668531746)\n\n    # Test edge case where y_pred has zero values\n    y_pred = np.array([0.0, 0.5, 2., 2.])\n    err_msg = (\"Mean Poisson deviance can only be used on non-negative y_true \"\n               \"and strictly positive y_pred.\")\n    with pytest.raises(ValueError, match=err_msg):\n        mean_poisson_deviance(y_true, y_pred)\n", "def test_max_error():\n    y_true = [3, 2, 7, 1]\n    y_pred = [4, 2, 7, 1]\n\n    assert_almost_equal(max_error(y_true, y_pred), 1.)\n\n    y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    y_pred = [[0, 2], [-1, 2], [8, -5]]\n\n    with pytest.raises(ValueError):\n        max_error(y_true, y_pred)\n"], "sample_852": ["def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2,\n                                   n_classes=3, random_state=0)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    # Test that clusters are on vertices of the hypercube\n    for i in range(3):\n        assert_array_almost_equal(np.mean(X[y == i], axis=0),\n                                  [-1, -1] + i * [2, 2],\n                                  decimal=1)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=0)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=0)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n", "def test_make_classification_n_features_greater_than_30():\n    X, y = make_classification(n_samples=2000, n_features=31, n_informative=31,\n                               hypercube=True, scale=0.5, random_state=0)\n\n    assert X.shape == (2000, 31), \"X shape mismatch\"\n    assert y.shape == (2000,), \"y shape mismatch\"\n\n    # Ensure that the points are on the vertices of a hypercube\n    for i in range(31):\n        assert np.any(X[:, i] == 0.5), \"Not all points are on the vertices\"\n        assert np.any(X[:, i] == -0.5), \"Not all points are on the vertices\"\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=None)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=None)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n", "def test_make_classification_unbalanced_classes():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=1, n_repeated=1, n_classes=3,\n                               n_clusters_per_class=1, hypercube=False,\n                               shift=None, scale=None, weights=[0.8, 0.1, 0.1],\n                               random_state=0)\n\n    assert X.shape == (100, 20), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n    assert sum(y == 0) > sum(y == 1), \"Class #0 should have more samples than class #1\"\n    assert sum(y == 0) > sum(y == 2), \"Class #0 should have more samples than class #2\"\n"], "sample_853": ["def test_transform_target_regressor_check_inverse_false():\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                      func=np.sqrt, inverse_func=lambda x: x**2,\n                                      check_inverse=False)\n    assert_no_warnings(regr.fit, X, y)\n    y_pred = regr.predict(X)\n    assert_allclose(y_pred, regr.inverse_func(regr.regressor_.predict(X)))\n", "def test_transform_target_regressor_predict_shape():\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                      func=np.log, inverse_func=np.exp)\n    regr.fit(X, y)\n\n    # check shape when y is 1D\n    y_pred = regr.predict(X)\n    assert y.shape == y_pred.shape\n\n    # check shape when y is 2D\n    y_2d = np.vstack((y, y ** 2 + 1)).T\n    regr.fit(X, y_2d)\n    y_pred = regr.predict(X)\n    assert y_2d.shape == y_pred.shape\n", "def test_transform_target_regressor_regressor_is_none():\n    X, y = friedman\n    regr = TransformedTargetRegressor()\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n    y_pred = regr.predict(X)\n    assert y.shape == y_pred.shape\n", "def test_transform_target_regressor_check_inverse_with_arrays():\n    X, y = friedman\n    # Define a transformer with inverse that does not lead to the original data\n    class InvertibleButNotIdentityTransformer(TransformerMixin, BaseEstimator):\n            return self\n\n            return X * 2\n\n            return X / 3\n\n    regr = TransformedTargetRegressor(\n        regressor=LinearRegression(),\n        transformer=InvertibleButNotIdentityTransformer(),\n        check_inverse=True\n    )\n\n    assert_warns_message(UserWarning, \"The provided functions or transformer\"\n                         \" are not strictly inverse of each other.\",\n                         regr.fit, X, y)\n", "def test_transform_target_regressor_check_inverse_using_function():\n    # check that the target ``y`` passed to the transformer will be checked for\n    # inverse using function if check_inverse is True.\n    X, y = friedman\n    tt = TransformedTargetRegressor(func=np.sqrt, inverse_func=np.square,\n                                    check_inverse=True)\n    tt.fit(X, y)\n\n    # check that the warning is raised when the inverse function is not correct\n    tt = TransformedTargetRegressor(func=np.sqrt, inverse_func=lambda x: x,\n                                    check_inverse=True)\n    with pytest.warns(UserWarning, match=\"The provided functions or transformer\"\n                      \" are not strictly inverse of each other\"):\n        tt.fit(X, y)\n"], "sample_854": ["def test_sparse_predict_with_csr_input():\n    # Test that the predict method can handle sparse csr input\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf = svm.SVC(kernel='linear', random_state=0)\n    clf.fit(X_sparse, y)\n\n    y_pred_sparse = clf.predict(X_sparse)\n    y_pred_dense = clf.predict(X)\n\n    assert_array_equal(y_pred_sparse, y_pred_dense)\n", "def test_sparse_libsvm():\n    # Test libsvm with sparse data\n    clf = svm.SVC(kernel='linear')\n    X_sparse = sparse.csr_matrix(X)\n    clf.fit(X_sparse, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # same as before, but using a callable function instead of the kernel\n    # matrix. kernel is just a linear kernel\n\n    kfunc = lambda x, y: np.dot(x, y.T)\n    clf = svm.SVC(kernel=kfunc)\n    clf.fit(X_sparse, Y)\n    assert_array_equal(clf.predict(T), true_result)\n", "def test_svc_multi_class_predict_proba_ties():\n    # Test predict_proba breaks ties appropriately\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([0, 1, 2, 3])\n\n    clf = svm.SVC(kernel=\"linear\", probability=True,\n                  decision_function_shape='ovr', random_state=42)\n    clf.fit(X, y)\n\n    # Create a sample that will result in a tie between two classes\n    # when predict is called\n    x = np.array([0.5, 0.5]).reshape(1, -1)\n    assert_array_equal(clf.predict(x), np.array([0]))\n\n    # But predict_proba should return the probability for both classes\n    proba = clf.predict_proba(x).ravel()\n    assert np.allclose(proba[[0, 1]], [0.5, 0.5])\n    assert np.allclose(proba[[2, 3]], [0., 0.])\n", "def test_base_libsvm_sparse_input():\n    # Test that sparse input to BaseLibSVM is handled correctly\n    X = sparse.csr_matrix(np.array([[0, 1], [1, 0]]))\n    y = np.array([0, 1])\n    clf = svm.SVC(kernel='linear')\n    clf.fit(X, y)\n    assert clf.score(X, y) == 1.0\n", "def test_svm_n_support_equal_support_vectors():\n    # Test that n_support is equal to the number of support vectors\n    X, y = make_blobs(random_state=42)\n    for clf in (svm.SVC(), svm.NuSVC(), svm.SVR(), svm.NuSVR()):\n        clf.fit(X, y)\n        assert clf.n_support_.sum() == clf.support_vectors_.shape[0]\n"], "sample_855": ["def test_dummy_classifier_sparse_input():\n    X = sp.csr_matrix(np.array([[0, 1], [1, 0]]))\n    y = np.array([0, 1])\n\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    clf.fit(X, y)\n\n    assert_array_equal(clf.predict(X), np.array([0, 0]))\n", "def test_dummy_classifier_constant_strategy_with_sparse_targets():\n    X = [[0]] * 5\n    y = sp.csr_matrix(np.array([1, 2, 1, 2, 1]))\n    clf = DummyClassifier(strategy='constant', constant=2)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), np.array([2, 2, 2, 2, 2]))\n", "def test_dummy_classifier_predict_log_proba():\n    X = [[0]] * 5  # ignored\n    y = [1, 2, 1, 1, 2]\n    clf = DummyClassifier(strategy=\"prior\", random_state=0)\n    clf.fit(X, y)\n\n    log_proba = clf.predict_log_proba(X)\n    assert_array_almost_equal(np.log(clf.predict_proba(X)), log_proba)\n", "def test_dummy_regressor_sparse_target():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[1], [2], [1], [1], [2]]))\n\n    reg = DummyRegressor(strategy=\"mean\")\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), np.ones((len(X), 1)) * 1.4)\n", "def test_dummy_classifier_sparse_output():\n    X = np.array([[0], [0], [0], [0]])\n    y = np.array([0, 1, 1, 0])\n\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    clf.fit(X, y)\n\n    y_sparse = sp.csr_matrix(y)\n    clf_sparse = DummyClassifier(strategy=\"most_frequent\")\n    clf_sparse.fit(X, y_sparse)\n\n    assert_array_equal(clf.predict(X), clf_sparse.predict(X).toarray().ravel())\n"], "sample_856": ["def test_predefined_split_too_many_folds():\n    # Check that PredefinedSplit raises an error when there are too many folds.\n    groups = np.array([1, 2, 3, 4, 5])\n    ps = PredefinedSplit(groups)\n    X = y = np.ones(len(groups))\n    with pytest.raises(\n            ValueError,\n            match=\"The test_fold should be of same length as the input \"\n                  \"data\"):\n        next(ps.split(X[:3], y[:3]))\n", "def test_validate_shuffle_split():\n    # Test the validation of test_size and train_size\n    assert_raises(ValueError, _validate_shuffle_split, 10, 1.1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.0)\n    assert_raises(ValueError, _validate_shuffle_split, 10, -0.1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 11)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0)\n\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 1.1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 0.0)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, -0.1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 11)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 0)\n\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.6, 0.6)\n\n    # Check integer test_size and train_size\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=7,\n                  train_size=5)\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=4,\n                  train_size=7)\n\n    # Check float test_size and train_size\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=0.7,\n                  train_size=0.5)\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=0.4,\n                  train_size=0.7)\n", "def test_validate_shuffle_split():\n    # Test that _validate_shuffle_split raises an error if test_size and\n    # train_size are not None and do not add up to n_samples\n    n_samples = 10\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=5, train_size=7)\n", "def test_split_numpy_array_with_object_dtype():\n    # Check that arrays with object dtype are handled correctly\n    X = np.array([[1, 2], [3, 4], [5, 6]], dtype=object)\n    y = np.array([1, 2, 3])\n\n    for cv in [KFold(), StratifiedKFold(), ShuffleSplit(), StratifiedShuffleSplit()]:\n        for train_index, test_index in cv.split(X, y):\n            assert isinstance(train_index, np.ndarray)\n            assert isinstance(test_index, np.ndarray)\n\n    # Check that the train and test indices are correct\n    kf = KFold(n_splits=2)\n    train_index, test_index = next(kf.split(X, y))\n    assert_array_equal(train_index, [0, 1])\n    assert_array_equal(test_index, [2])\n\n    # Check that the split works with a numpy array with object dtype\n    X_obj = np.array([[1, 2], [3, 4], [5, 6]], dtype=object)\n    y_obj = np.array([1, 2, 3], dtype=object)\n\n    for train_index, test_index in kf.split(X_obj, y_obj):\n        assert isinstance(train_index, np.ndarray)\n        assert isinstance(test_index, np.ndarray)\n", "def test_validate_shuffle_split():\n    # Test the _validate_shuffle_split function\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.6, 0.5)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 8, 3)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, None)\n\n    train_size, test_size = _validate_shuffle_split(10, 0.2, None)\n    assert train_size == 8\n    assert test_size == 2\n\n    train_size, test_size = _validate_shuffle_split(10, None, 0.2)\n    assert train_size == 8\n    assert test_size == 2\n\n    train_size, test_size = _validate_shuffle_split(10, 0.2, 0.8)\n    assert train_size == 8\n    assert test_size == 2\n"], "sample_857": ["def test_get_depth():\n    # Test the get_depth method.\n    X = np.array([[0, 1], [0, 2]])\n    y = np.array([0, 1])\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    assert clf.get_depth() == 1\n\n    clf = DecisionTreeRegressor(random_state=0)\n    clf.fit(X, y)\n\n    assert clf.get_depth() == 1\n", "def test_n_features_deprecated():\n    X = [[0, 0], [2, 2], [4, 6], [10, 11]]\n    y = [0.5, 2.5, 3.5, 5.5]\n    clf = DecisionTreeRegressor()\n    clf = clf.fit(X, y)\n\n    match = (\"attribute is to be deprecated from version \"\n             \"0.22 and will be removed in 0.24.\")\n\n    with pytest.warns(DeprecationWarning, match=match):\n        n = clf.n_features_\n        assert n == clf.tree_.n_features\n", "def test_prune_tree_with_sample_weights():\n    # Test that pruning works with sample weights.\n    X = [[0, 0], [2, 2], [4, 6], [10, 11]]\n    y = [0, 1, 1, 0]\n    sample_weight = [1.0, 1.0, 0.5, 1.0]\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    clf_pruned = DecisionTreeClassifier(random_state=0, ccp_alpha=0.1)\n    clf_pruned.fit(X, y, sample_weight=sample_weight)\n\n    assert_is_subtree(clf.tree_, clf_pruned.tree_)\n", "def test_prune_tree_ccp_alpha_not_affecting_fitting():\n    X, y = iris.data, iris.target\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    ccp_alphas = [0.1, 0.3]\n    for ccp_alpha in ccp_alphas:\n        clf_ccp = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n        clf_ccp.fit(X, y)\n\n        assert_tree_equal(clf.tree_, clf_ccp.tree_,\n                          \"Trees are not equal with ccp_alpha={}\".format(ccp_alpha))\n", "def test_ccp_alpha_not_used_with_max_leaf_nodes():\n    clf = DecisionTreeClassifier(max_leaf_nodes=5, ccp_alpha=10)\n    with pytest.warns(UserWarning, match=\"ccp_alpha is not used\"):\n        clf.fit(X, y)\n\n    reg = DecisionTreeRegressor(max_leaf_nodes=5, ccp_alpha=10)\n    with pytest.warns(UserWarning, match=\"ccp_alpha is not used\"):\n        reg.fit(X, y)\n"], "sample_858": ["def test_voting_classifier_get_params():\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n                            voting='soft')\n    params = eclf.get_params()\n    assert 'estimators' in params\n    assert len(params['estimators']) == 2\n    assert 'voting' in params\n    assert params['voting'] == 'soft'\n    assert 'weights' in params\n    assert params['weights'] is None\n    assert 'n_jobs' in params\n    assert params['n_jobs'] is None\n    assert 'flatten_transform' in params\n    assert params['flatten_transform'] is True\n", "def test_voting_regressor_shape():\n    \"\"\"Check VotingRegressor's output shape\"\"\"\n    X = np.array([[1], [2]])\n    y = np.array([1, 2])\n\n    reg1 = LinearRegression()\n    reg2 = DecisionTreeRegressor()\n\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('tree', reg2)])\n    ereg.fit(X, y)\n\n    assert_array_equal(ereg.predict(X).shape, (2,))\n    assert_array_equal(ereg.transform(X).shape, (2, 2))\n", "def test_check_voting_classifier_get_params():\n    # Check if VotingClassifier get_params return the correct estimators\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(n_estimators=10, random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)])\n    \n    params = eclf.get_params()\n    assert params['estimators'] == [('lr', clf1), ('rf', clf2)]\n    assert params['lr'] is clf1\n    assert params['rf'] is clf2\n", "def test_validate_estimator_names():\n    # Check that an error is raised when estimator names are not unique or\n    # contain double underscore\n    clf1 = LogisticRegression()\n    clf2 = RandomForestClassifier()\n\n    msg = \"Names provided are not unique: ['lr', 'lr']\"\n    with pytest.raises(ValueError, match=msg):\n        VotingClassifier(estimators=[('lr', clf1), ('lr', clf2)])\n\n    msg = \"Estimator names must not contain __: got ['lr__rf']\"\n    with pytest.raises(ValueError, match=msg):\n        VotingClassifier(estimators=[('lr__rf', clf1), ('nb', clf2)])\n\n    msg = (\"Estimator names conflict with constructor arguments: ['estimators']\")\n    with pytest.raises(ValueError, match=msg):\n        VotingClassifier(estimators=[('estimators', clf1), ('nb', clf2)])\n", "def test_voting_classifier_get_params():\n    \"\"\"Test VotingClassifier.get_params()\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n                            voting='soft', weights=[1, 2])\n    params = eclf.get_params()\n    assert 'estimators' in params\n    assert 'voting' in params\n    assert 'weights' in params\n    assert params['voting'] == 'soft'\n    assert params['weights'] == [1, 2]\n    assert params['estimators'] == [('lr', clf1), ('rf', clf2)]\n    assert all(isinstance(est, tuple) for est in params['estimators'])\n    assert all(len(est) == 2 for est in params['estimators'])\n    assert all(isinstance(est[0], str) for est in params['estimators'])\n    assert all(isinstance(est[1], BaseEstimator) for est in params['estimators'])\n"], "sample_859": ["def test_coordinate_descent_convergence():\n    # Test convergence of coordinate descent on a non-trivial problem.\n    X, y, _, _ = build_dataset(n_samples=100, n_features=50)\n    model = ElasticNet(max_iter=10000, tol=1e-10)\n    model.fit(X, y)\n    assert model.n_iter_ < 10000  # check that we converged before max_iter\n    # Check that the solution is stable over multiple calls to fit\n    prev_coef = model.coef_.copy()\n    prev_intercept = model.intercept_\n    model.fit(X, y)\n    assert_array_almost_equal(model.coef_, prev_coef)\n    assert_almost_equal(model.intercept_, prev_intercept)\n", "def test_coordinate_descent_sparse_input_dtype():\n    # Generate sparse data with two columns\n    X = sparse.csr_matrix(np.array([[0, 1], [1, 0]]))\n    y = np.array([1, -1])\n\n    for dtype in [np.float32, np.float64]:\n        X = X.astype(dtype)\n        y = y.astype(dtype)\n\n        clf = Lasso()\n        clf.fit(X, y)\n\n        assert clf.coef_.dtype == dtype\n", "def test_enet_coordinate_descent_sparse_input_dtype():\n    \"\"\"Test that sparse input with different dtypes gives same results\"\"\"\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    clf_dt64 = ElasticNet(selection='cyclic', tol=1e-8)\n    clf_dt32 = ElasticNet(selection='cyclic', tol=1e-8)\n\n    X_dt64 = sparse.csr_matrix(X, dtype=np.float64)\n    X_dt32 = sparse.csr_matrix(X, dtype=np.float32)\n\n    clf_dt64.fit(X_dt64, y)\n    clf_dt32.fit(X_dt32, y)\n\n    assert_array_almost_equal(clf_dt64.coef_, clf_dt32.coef_)\n    assert_almost_equal(clf_dt64.intercept_, clf_dt32.intercept_)\n", "def test_lasso_path_dtype():\n    # Test that lasso_path returns the correct type for different input types\n    X, y, _, _ = build_dataset()\n    for dtype in [np.float32, np.float64]:\n        X = X.astype(dtype)\n        y = y.astype(dtype)\n        alphas, coefs, _ = lasso_path(X, y)\n        assert coefs.dtype == dtype\n        assert alphas.dtype == dtype\n", "def test_lasso_cv_sparse_input_dtype():\n    n_samples, n_features = 10, 3\n    rng = np.random.RandomState(42)\n    X = rng.binomial(1, .5, size=(n_samples, n_features))\n    X_sparse = sparse.csr_matrix(X)\n    y = X[:, [0, 0]].copy()\n    est = LassoCV(n_alphas=5, fit_intercept=True).fit(X_sparse, y)\n    assert_array_almost_equal(est.coef_, [[1, 0, 0]] * 2, decimal=3)\n\n    X_sparse = X_sparse.astype(np.float32)\n    est = LassoCV(n_alphas=5, fit_intercept=True).fit(X_sparse, y)\n    assert_array_almost_equal(est.coef_, [[1, 0, 0]] * 2, decimal=3)\n"], "sample_860": ["def test_check_array_dtype_object():\n    X = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.object)\n    X_checked = check_array(X, dtype='numeric')\n    assert X_checked.dtype.kind == 'f'\n", "def test_check_X_y_length_mismatch():\n    X = np.ones((2, 2))\n    y = [1, 2, 3]\n    msg = (\"Found input variables with inconsistent numbers of\"\n           \" samples: [2, 3]\")\n    assert_raise_message(ValueError, msg, check_X_y, X, y)\n", "def test_check_array_dtype_numeric_object():\n    # Test that object dtype is not treated as numeric\n    X = np.array([['1', '2'], ['3', '4']], dtype=object)\n    with pytest.raises(ValueError, match=\"could not convert string to float\"):\n        check_array(X, dtype='numeric')\n", "def test_check_array_dtype_object_conversion_with_inf():\n    # Test that data-frame like objects with dtype object and inf values\n    # get converted to float64 and raise an error if force_all_finite=True\n    X = np.array([[0.2, np.inf], [0.6, 0.5]], dtype=object)\n    X_df = MockDataFrame(X)\n\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity\"):\n        check_array(X_df, force_all_finite=True)\n\n    X_checked = check_array(X_df, force_all_finite=False)\n    assert X_checked.dtype == np.float64\n", "def test_check_X_y_multilabel_sequence():\n    # Test that check_X_y works for multilabel sequence input.\n    X = np.random.rand(10, 5)\n    y = [[0, 1], [1, 2], [3], [4, 5], [6], [7, 8], [9], [0], [1], [2]]\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n"], "sample_861": ["def test_grid_search_cv_results_with_return_train_score_equals_false():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    grid_search = GridSearchCV(SVC(), cv=n_splits,\n                               param_grid=params, return_train_score=False)\n    grid_search.fit(X, y)\n\n    cv_results = grid_search.cv_results_\n    assert not any(key.startswith('split') and key.endswith('_train_score')\n                   for key in cv_results.keys())\n    assert 'mean_train_score' not in cv_results.keys()\n    assert 'std_train_score' not in cv_results.keys()\n", "def test_parameter_sampler_n_iter():\n    # Test that n_iter is honored by ParameterSampler\n    param_distributions = {\"kernel\": [\"rbf\", \"linear\"],\n                           \"C\": uniform(0, 1)}\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=10, random_state=0)\n    samples = [x for x in sampler]\n    assert len(samples) == 10\n\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=5, random_state=0)\n    samples = [x for x in sampler]\n    assert len(samples) == 5\n", "def test_grid_search_cv_results_rank_tie_breaking_with_nan():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid,\n                               return_train_score=True)\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n\n    # Set one of the scores to NaN\n    cv_results['mean_test_score'][0] = np.nan\n\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_score'][1],\n                        cv_results['mean_test_score'][2])\n    assert not np.allclose(cv_results['mean_test_score'][1],\n                           cv_results['mean_test_score'][0])\n    assert not np.allclose(cv_results['mean_train_score'][1],\n                           cv_results['mean_train_score'][0])\n\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [3, 1, 1])\n", "def test_grid_search_cv_results_rank_tie_breaking_multiple_scores():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid,\n                               scoring=('accuracy', 'precision'),\n                               refit='accuracy', return_train_score=True)\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_accuracy'][0],\n                        cv_results['mean_test_accuracy'][1])\n    assert_almost_equal(cv_results['mean_train_accuracy'][0],\n                        cv_results['mean_train_accuracy'][1])\n    assert not np.allclose(cv_results['mean_test_accuracy'][1],\n                           cv_results['mean_test_accuracy'][2])\n    assert not np.allclose(cv_results['mean_train_accuracy'][1],\n                           cv_results['mean_train_accuracy'][2])\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_accuracy'], [1, 1, 3])\n", "def test_grid_search_cv_results_rank_tie_breaking_with_return_train_score():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid,\n                               return_train_score=True)\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_score'][0],\n                        cv_results['mean_test_score'][1])\n    assert_almost_equal(cv_results['mean_train_score'][0],\n                        cv_results['mean_train_score'][1])\n    assert not np.allclose(cv_results['mean_test_score'][1],\n                           cv_results['mean_test_score'][2])\n    assert not np.allclose(cv_results['mean_train_score'][1],\n                           cv_results['mean_train_score'][2])\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [1, 1, 3])\n"], "sample_862": ["def test_vectorizer_build_preprocessor(Vectorizer):\n    # Test that the preprocessor is correctly built and set on the vectorizer\n    vectorizer = Vectorizer()\n    preprocessor = vectorizer.build_preprocessor()\n    assert callable(preprocessor)\n    assert vectorizer.preprocessor is None\n\n    # Test that the preprocessor is correctly set when passing a custom one\n    custom_preprocessor = lambda x: x.lower()\n    vectorizer = Vectorizer(preprocessor=custom_preprocessor)\n    assert vectorizer.preprocessor is custom_preprocessor\n    preprocessor = vectorizer.build_preprocessor()\n    assert preprocessor is custom_preprocessor\n", "def test_countvectorizer_inverse_transform_with_empty_vocabulary():\n    # raw documents\n    data = ALL_FOOD_DOCS\n\n    vectorizer = CountVectorizer(vocabulary=[])\n    X = vectorizer.fit_transform(data)\n\n    with pytest.raises(ValueError):\n        vectorizer.inverse_transform(X)\n", "def test_vectorizer_validate_params():\n    # Test that CountVectorizer validates parameters at fit time\n\n    # invalid ngram_range\n    cv = CountVectorizer(ngram_range=(2, 1))\n    with pytest.raises(ValueError):\n        cv.fit(JUNK_FOOD_DOCS)\n\n    # invalid max_df\n    cv = CountVectorizer(max_df=-1)\n    with pytest.raises(ValueError):\n        cv.fit(JUNK_FOOD_DOCS)\n\n    # invalid min_df\n    cv = CountVectorizer(min_df=-1)\n    with pytest.raises(ValueError):\n        cv.fit(JUNK_FOOD_DOCS)\n\n    # invalid max_features\n    cv = CountVectorizer(max_features=-1)\n    with pytest.raises(ValueError):\n        cv.fit(JUNK_FOOD_DOCS)\n\n    # duplicate vocabulary entries\n    vocab = {'hello': 0, 'world': 1, 'hello': 2}\n    cv = CountVectorizer(vocabulary=vocab)\n    with pytest.raises(ValueError):\n        cv.fit(JUNK_FOOD_DOCS)\n\n    # invalid vocabulary type\n    vocab = 123\n    cv = CountVectorizer(vocabulary=vocab)\n    with pytest.raises(ValueError):\n        cv.fit(JUNK_FOOD_DOCS)\n", "def test_vectorizer_input_validation():\n    # test that invalid input types are caught\n    message = (\"Iterable over raw text documents expected, \"\n               \"string object received.\")\n    assert_raise_message(\n        ValueError, message, CountVectorizer().fit_transform, \"hello world!\"\n    )\n\n    message = (\"Iterable over raw text documents expected, \"\n               \"string object received.\")\n    assert_raise_message(\n        ValueError, message, TfidfVectorizer().fit_transform, \"hello world!\"\n    )\n\n    message = (\"Iterable over raw text documents expected, \"\n               \"string object received.\")\n    assert_raise_message(\n        ValueError, message, HashingVectorizer().fit_transform, \"hello world!\"\n    )\n", "def test_vectorizer_inverse_transform_with_empty_vocabulary():\n    vect = CountVectorizer(vocabulary=[])\n    X = sparse.csr_matrix(np.array([[1, 0], [0, 1]]))\n    with pytest.raises(ValueError):\n        vect.inverse_transform(X)\n"], "sample_863": ["def test_pipeline_get_params_returns_parameter_names():\n    # Test that get_params returns parameter names for all steps in the pipeline.\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    params = pipe.get_params()\n    assert 'transf__a' in params\n    assert 'transf__b' in params\n    assert 'clf__should_succeed' in params\n", "def test_pipeline_fit_params_empty():\n    # Test that pipeline fit method raises an error when given empty fit params\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    X = [[1], [2]]\n    y = [1, 2]\n\n    with pytest.raises(ValueError,\n                       match=\"Pipeline.fit does not accept the {} parameter. \"\n                             \"You can pass parameters to specific steps of \"\n                             \"your pipeline using the stepname__parameter \"\n                             \"format, e.g. `Pipeline.fit(X, y, \"\n                             \"logisticregression__sample_weight=sample_weight)`.\"\n                             .format('')):\n        pipe.fit(X, y, **{})\n", "def test_pipeline_get_feature_names():\n    # Test that pipeline get_feature_names works correctly\n\n    X = np.array([[1, 2]])\n    mult2 = Mult(2)\n    mult2.get_feature_names = lambda: ['x2']\n    mult3 = Mult(3)\n    mult3.get_feature_names = lambda: ['x3']\n\n    pipeline = Pipeline([('m2', mult2), ('m3', mult3)])\n    assert_array_equal([[6]], pipeline.fit_transform(X))\n    assert ['x2'] == pipeline.named_steps['m2'].get_feature_names()\n    assert ['x3'] == pipeline.named_steps['m3'].get_feature_names()\n\n    pipeline.set_params(m2='passthrough')\n    assert_array_equal([[6]], pipeline.fit_transform(X))\n    assert_raise_message(AttributeError,\n                         \"'str' object has no attribute 'get_feature_names'\",\n                         getattr, pipeline.named_steps['m2'], 'get_feature_names')\n\n    pipeline.set_params(m2=mult2)\n    pipeline.set_params(m3='passthrough')\n    assert_array_equal([[2]], pipeline.fit_transform(X))\n    assert ['x2'] == pipeline.named_steps['m2'].get_feature_names()\n    assert_raise_message(AttributeError,\n                         \"'str' object has no attribute 'get_feature_names'\",\n                         getattr, pipeline.named_steps['m3'], 'get_feature_names')\n", "def test_pipeline_get_params():\n    # Test that get_params returns all parameters of the estimators in the\n    # pipeline.\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n    pipeline = Pipeline([('mult2', mult2), ('mult3', mult3)])\n\n    params = pipeline.get_params()\n    assert 'mult2__mult' in params\n    assert 'mult3__mult' in params\n\n    assert params['mult2__mult'] == 2\n    assert params['mult3__mult'] == 3\n", "def test_pipeline_get_feature_names():\n    # Test that pipeline get_feature_names works as expected\n    pipe = make_pipeline(SelectKBest(k=1), LogisticRegression())\n    pipe.fit(iris.data, iris.target)\n    assert_raises_regex(AttributeError,\n                        \"Transformer at index 0 does not provide \"\n                        \"get_feature_names\", pipe.get_feature_names)\n\n    pipe = make_pipeline(PCA(n_components=2), LogisticRegression())\n    pipe.fit(iris.data, iris.target)\n    assert len(pipe.get_feature_names()) == 2\n\n    pipe = make_pipeline(PCA(n_components=2), SelectKBest(k=1))\n    pipe.fit(iris.data, iris.target)\n    assert len(pipe.get_feature_names()) == 1\n"], "sample_864": ["def test_mean_shift_n_jobs():\n    # Test MeanShift with different n_jobs values\n    ms1 = MeanShift(n_jobs=1)\n    ms1.fit(X)\n\n    ms2 = MeanShift(n_jobs=2)\n    ms2.fit(X)\n\n    ms3 = MeanShift(n_jobs=-1)\n    ms3.fit(X)\n\n    assert_array_almost_equal(ms1.cluster_centers_, ms2.cluster_centers_)\n    assert_array_almost_equal(ms1.cluster_centers_, ms3.cluster_centers_)\n    assert_array_equal(ms1.labels_, ms2.labels_)\n    assert_array_equal(ms1.labels_, ms3.labels_)\n", "def test_mean_shift_large_bandwidth():\n    # Test MeanShift with a large bandwidth to check if all points are assigned\n    # to the same cluster\n    ms = MeanShift(bandwidth=10)\n    labels = ms.fit(X).labels_\n    assert len(np.unique(labels)) == 1\n", "def test_mean_shift_seeds():\n    # Test MeanShift with custom seeds\n    ms = MeanShift(seeds=[[10, 10], [12, 10]])\n    labels = ms.fit(X).labels_\n    labels_unique = np.unique(labels)\n    n_clusters_ = len(labels_unique)\n    assert n_clusters_ == 2\n\n    cluster_centers, labels_mean_shift = mean_shift(X, seeds=[[10, 10], [12, 10]])\n    labels_mean_shift_unique = np.unique(labels_mean_shift)\n    n_clusters_mean_shift = len(labels_mean_shift_unique)\n    assert n_clusters_mean_shift == 2\n", "def test_meanshift_empty_input():\n    # Test MeanShift with empty input\n    ms = MeanShift()\n    X = np.array([])\n    msg = \"Input contains no data\"\n    with pytest.raises(ValueError, match=msg):\n        ms.fit(X)\n", "def test_mean_shift_with_n_jobs():\n    # Test MeanShift with different n_jobs values\n    ms1 = MeanShift(n_jobs=1)\n    ms1.fit(X)\n\n    ms2 = MeanShift(n_jobs=-1)\n    ms2.fit(X)\n\n    assert_array_almost_equal(ms1.cluster_centers_, ms2.cluster_centers_)\n    assert_array_equal(ms1.labels_, ms2.labels_)\n\n    ms3 = MeanShift(n_jobs=2)\n    ms3.fit(X)\n\n    assert_array_almost_equal(ms1.cluster_centers_, ms3.cluster_centers_)\n    assert_array_equal(ms1.labels_, ms3.labels_)\n"], "sample_865": ["def test_n_features_deprecated():\n    X = [[0, 0], [2, 2], [4, 6], [10, 11]]\n    y = [0.5, 2.5, 3.5, 5.5]\n    clf = DecisionTreeRegressor()\n    clf = clf.fit(X, y)\n\n    match = (\"attribute is to be deprecated from version \"\n             \"0.22 and will be removed in 0.24.\")\n\n    with pytest.warns(FutureWarning, match=match):\n        n = clf.n_features_\n        assert n == clf.tree_.n_features\n\n    with pytest.warns(FutureWarning, match=match):\n        assert clf.n_features_ == X.shape[1]\n", "def test_prune_tree_with_sample_weights():\n    # Test pruning with sample weights.\n    X = [[0, 0], [2, 2], [4, 6], [10, 11]]\n    y = [0.5, 2.5, 3.5, 5.5]\n    sample_weight = [1.0, 1.0, 0.5, 1.0]\n\n    clf = DecisionTreeRegressor(random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    path = clf.cost_complexity_pruning_path(X, y, sample_weight=sample_weight)\n    ccp_alphas = path.ccp_alphas\n    impurities = path.impurities\n\n    assert np.all(np.diff(ccp_alphas) >= 0)\n    assert np.all(np.diff(impurities) >= 0)\n\n    # A pruned tree must be a subtree of the previous tree (which had a\n    # smaller ccp_alpha)\n    prev_alpha = -np.inf\n    for alpha in ccp_alphas:\n        assert alpha > prev_alpha\n        clf.set_params(ccp_alpha=alpha)\n        clf._prune_tree()\n        assert_is_subtree(clf.tree_, clf.tree_)\n        prev_alpha = alpha\n", "def test_prune_tree_ccp_alpha_not_affecting_fitting():\n    clf = DecisionTreeClassifier(random_state=0)\n    X, y = iris.data, iris.target\n\n    # Set a high ccp_alpha and assert that the tree is pruned to have only one node.\n    clf.set_params(ccp_alpha=100.0)\n    clf.fit(X, y)\n    assert clf.tree_.node_count == 1\n\n    # Set a low ccp_alpha and assert that the tree is not pruned.\n    clf.set_params(ccp_alpha=0.0)\n    clf.fit(X, y)\n    assert clf.tree_.node_count > 1\n\n    # Assert that ccp_alpha does not affect the fitting of the tree when\n    # max_leaf_nodes is specified.\n    clf.set_params(ccp_alpha=100.0, max_leaf_nodes=10)\n    clf.fit(X, y)\n    assert clf.tree_.node_count > 1\n", "def test_prune_tree_is_idempotent():\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    # Pruning twice with the same alpha should result in the same tree\n    clf.prune_tree(clf.ccp_alpha)\n    pruned_tree_1 = copy.deepcopy(clf.tree_)\n    clf.prune_tree(clf.ccp_alpha)\n    pruned_tree_2 = clf.tree_\n\n    assert_tree_equal(pruned_tree_1, pruned_tree_2,\n                      \"Pruning with the same alpha twice resulted in \"\n                      \"different trees\")\n", "def test_tree_value_dtype():\n    # Check the dtype of tree_.value for classification and regression.\n    X = iris.data\n    y = iris.target\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n    assert clf.tree_.value.dtype == np.float64\n\n    reg = DecisionTreeRegressor(random_state=0)\n    reg.fit(X, y)\n    assert reg.tree_.value.dtype == np.float64\n"], "sample_866": ["def test_affinity_propagation_fit_predict():\n    # Test that fit_predict works and returns the same labels as fit followed\n    # by predict.\n    af = AffinityPropagation()\n    labels_fit_predict = af.fit_predict(X)\n    labels_fit = af.labels_\n\n    af = AffinityPropagation()\n    af.fit(X)\n    labels_predict = af.predict(X)\n\n    assert_array_equal(labels_fit_predict, labels_fit)\n    assert_array_equal(labels_fit_predict, labels_predict)\n", "def test_affinity_propagation_verbose():\n    # Test AffinityPropagation with verbose flag\n    ap = AffinityPropagation(verbose=True)\n    with pytest.warns(None) as record:\n        ap.fit(X)\n    assert len(record) == 1\n    assert \"Converged after\" in str(record[0].message)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from scipy.sparse import csr_matrix\n\n    af = AffinityPropagation(affinity=\"euclidean\")\n    X_sparse = csr_matrix(X)\n    labels_sparse = af.fit_predict(X_sparse)\n\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_dense = af.fit_predict(X)\n\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_fit_predict():\n    # Test AffinityPropagation.fit_predict\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X)\n    assert_array_equal(labels, af.labels_)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_sparse = af.fit_predict(X_sparse)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_dense = af.fit_predict(X)\n    assert_array_equal(labels_sparse, labels_dense)\n"], "sample_867": ["def test_search_cv__estimator_type_property_delegated_to_base_estimator():\n    \"\"\"\n    Test implementation of BaseSearchCV has the _estimator_type property\n    which matches the _estimator_type property of its estimator.\n    This test make sure _estimator_type is delegated to the base estimator.\n\n    Non-regression test for issue #13920.\n    \"\"\"\n    est = BaseEstimator()\n    attr_message = \"BaseSearchCV _estimator_type property must match estimator\"\n\n    for _estimator_type_setting in ['classifier', 'regressor']:\n        setattr(est, '_estimator_type', _estimator_type_setting)\n        cv = GridSearchCV(est, {'n_neighbors': [10]})\n        assert _estimator_type_setting == cv._estimator_type, attr_message\n", "def test_search_cv__estimator_type_property_delegated_to_base_estimator():\n    \"\"\"\n    Test implementation of BaseSearchCV has the _estimator_type property\n    which matches the _estimator_type property of its estimator.\n    This test make sure _estimator_type is delegated to the base estimator.\n\n    Non-regression test for issue #13920.\n    \"\"\"\n    est = BaseEstimator()\n    attr_message = \"BaseSearchCV _estimator_type property must match estimator\"\n\n    for _estimator_type_setting in ['classifier', 'regressor']:\n        setattr(est, '_estimator_type', _estimator_type_setting)\n        cv = GridSearchCV(est, {'n_neighbors': [10]})\n        assert _estimator_type_setting == cv._estimator_type, attr_message\n", "def test_search_cv_results_rank_tie_breaking_with_multiple_metrics():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid,\n                               scoring=['accuracy', 'precision'],\n                               refit='accuracy',\n                               return_train_score=True)\n    random_search = RandomizedSearchCV(SVC(), n_iter=3,\n                                       param_distributions=param_grid,\n                                       scoring=['accuracy', 'precision'],\n                                       refit='accuracy',\n                                       return_train_score=True)\n\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        cv_results = search.cv_results_\n        # Check tie breaking strategy -\n        # Check that there is a tie in the mean scores between\n        # candidates 1 and 2 alone\n        assert_almost_equal(cv_results['mean_test_accuracy'][0],\n                            cv_results['mean_test_accuracy'][1])\n        assert_almost_equal(cv_results['mean_train_accuracy'][0],\n                            cv_results['mean_train_accuracy'][1])\n        assert not np.allclose(cv_results['mean_test_accuracy'][1],\n                               cv_results['mean_test_accuracy'][2])\n        assert not np.allclose(cv_results['mean_train_accuracy'][1],\n                               cv_results['mean_train_accuracy'][2])\n        # 'min' rank should be assigned to the tied candidates\n        assert_almost_equal(search.cv_results_['rank_test_accuracy'], [1, 1, 3])\n", "def test_grid_search_cv_results_with_return_train_score_equals_false():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    for iid in (False, True):\n        grid_search = GridSearchCV(SVC(), cv=n_splits,\n                                   iid=iid, param_grid=params,\n                                   return_train_score=False)\n        grid_search.fit(X, y)\n        cv_results = grid_search.cv_results_\n        \n        # Check that 'split*_train_score' and 'mean_train_score' and \n        # 'std_train_score' are not present in the results\n        assert all('split%d_train_score' % s not in cv_results \n                   for s in range(n_splits))\n        assert 'mean_train_score' not in cv_results\n        assert 'std_train_score' not in cv_results\n", "def test_search_cv_results_rank_tie_breaking_non_finite_scores():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid,\n                               return_train_score=True)\n    random_search = RandomizedSearchCV(SVC(), n_iter=3,\n                                       param_distributions=param_grid,\n                                       return_train_score=True)\n\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        cv_results = search.cv_results_\n        # Check tie breaking strategy -\n        # Check that there is a tie in the mean scores between\n        # candidates 1 and 2 alone\n        cv_results['mean_test_score'][0] = np.nan\n        cv_results['mean_test_score'][1] = np.inf\n        cv_results['mean_test_score'][2] = -np.inf\n\n        ranks = []\n        for i in range(len(cv_results['params'])):\n            rank = 1 + np.sum(\n                np.ma.masked_invalid(cv_results['mean_test_score']) > \n                cv_results['mean_test_score'][i])\n            ranks.append(rank)\n\n        assert_array_almost_equal(search.cv_results_['rank_test_score'], ranks)\n"], "sample_868": ["def test_empty_input(metric_name, metric_func):\n    # Test that empty input arrays return the correct score for each metric\n    assert metric_func([], []) == 1.0 if metric_name in [\"adjusted_rand_score\", \"v_measure_score\"] else 0.0\n", "def test_empty_input(metric_name, metric_func):\n    # Test that metrics raise an error on empty input\n    with pytest.raises(ValueError, match='true labels must be 1D'):\n        metric_func([], [])\n\n    # Test that metrics work with empty-like input (i.e., a single cluster)\n    assert metric_func([0], [0]) == pytest.approx(1.0)\n", "def test_empty_input(metric_name, metric_func):\n    # Test that supervised metrics handle empty inputs correctly\n    with pytest.raises(ValueError, match='true and predicted labels must be 1D arrays'):\n        metric_func([], [])\n", "def test_empty_input(metric_name):\n    # All supervised clustering metrics should be able to handle empty input\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0\n", "def test_check_clusterings(metric_name):\n    # Test that check_clusterings is called in supervised metrics\n    metric = SUPERVISED_METRICS[metric_name]\n    y_true = np.array([0, 0, 1, 1])\n    y_pred = np.array([0, 0, 1, 2])\n\n    # Check that no exception is raised when labels are valid\n    metric(y_true, y_pred)\n\n    # Check that an exception is raised when labels have different lengths\n    with pytest.raises(ValueError):\n        metric(y_true, y_pred[:-1])\n\n    # Check that an exception is raised when labels are not 1D arrays\n    with pytest.raises(ValueError):\n        metric(y_true, y_pred.reshape(-1, 1))\n\n    # Check that an exception is raised when labels contain non-integer values\n    with pytest.raises(ValueError):\n        metric(y_true, y_pred.astype(float))\n"], "sample_870": ["def test_gpr_predict_multioutput_with_n_targets():\n    \"\"\"\n    Check that the output shape of `predict` is consistent when n_targets is\n    specified for multi-output data.\n    \"\"\"\n    rng = np.random.RandomState(1024)\n\n    n_samples, n_features, n_targets = 10, 3, 2\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n\n    model = GaussianProcessRegressor(n_targets=n_targets)\n    model.fit(X, y)\n\n    X_test = rng.randn(5, n_features)\n    y_mean, y_cov = model.predict(X_test, return_cov=True)\n    y_mean, y_std = model.predict(X_test, return_std=True)\n\n    assert y_mean.shape == (5, n_targets)\n    assert y_cov.shape == (5, 5, n_targets)\n    assert y_std.shape == (5, n_targets)\n", "def test_gpr_predict_multioutput_with_n_targets():\n    \"\"\"\n    Check that the output shape of `predict` is consistent when n_targets is\n    specified for multi-output data.\n    \"\"\"\n    rng = np.random.RandomState(1024)\n\n    n_samples, n_features, n_targets = 10, 3, 2\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n\n    model = GaussianProcessRegressor(n_targets=n_targets)\n    model.fit(X, y)\n\n    X_test = rng.randn(5, n_features)\n    y_mean, y_cov = model.predict(X_test, return_cov=True)\n    y_mean, y_std = model.predict(X_test, return_std=True)\n\n    assert y_mean.shape == (5, n_targets)\n    assert y_cov.shape == (5, 5, n_targets)\n    assert y_std.shape == (5, n_targets)\n", "def test_gpr_predict_multioutput_std_cov():\n    \"\"\"\n    Check that the `return_std` and `return_cov` parameters work correctly for\n    multi-output Gaussian Processes.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/22174\n    \"\"\"\n    rng = np.random.RandomState(1234)\n\n    n_features, n_samples_train, n_samples_test = 6, 9, 7\n    n_targets = 3\n\n    X_train = rng.randn(n_samples_train, n_features)\n    X_test = rng.randn(n_samples_test, n_features)\n    y_train = rng.randn(n_samples_train, n_targets)\n\n    model = GaussianProcessRegressor()\n    model.fit(X_train, y_train)\n\n    y_mean, y_std = model.predict(X_test, return_std=True)\n    _, y_cov = model.predict(X_test, return_cov=True)\n\n    assert y_mean.shape == (n_samples_test, n_targets)\n    assert y_std.shape == (n_samples_test, n_targets)\n    assert y_cov.shape == (n_samples_test, n_samples_test, n_targets)\n\n    # Check that std is the square root of the diagonal of cov\n    assert_allclose(y_std**2, np.diagonal(y_cov, axis1=0, axis2=1))\n", "def test_gpr_sample_y_random_state():\n    \"\"\"\n    Check that the random_state parameter in sample_y method generates the same\n    results for a given seed.\n    \"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n\n    model = GaussianProcessRegressor()\n    model.fit(X, y)\n\n    y_samples_1 = model.sample_y(X, n_samples=5, random_state=42)\n    y_samples_2 = model.sample_y(X, n_samples=5, random_state=42)\n\n    assert_array_almost_equal(y_samples_1, y_samples_2)\n", "def test_gpr_predict_multioutput_std():\n    \"\"\"Check the shape of y_std for multi-output GPR when return_std=True.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10, 2)\n\n    model = GaussianProcessRegressor()\n    model.fit(X, y)\n\n    _, std = model.predict(X, return_std=True)\n    assert std.shape == (10, 2)\n"], "sample_871": ["def test_calinski_harabasz_score_sparse():\n    # Assert the function works with sparse matrices\n    from scipy.sparse import csr_matrix\n\n    X = csr_matrix(\n        (\n            [[0, 0], [1, 1]] * 5\n            + [[3, 3], [4, 4]] * 5\n            + [[0, 4], [1, 3]] * 5\n            + [[3, 1], [4, 0]] * 5\n        )\n    )\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(calinski_harabasz_score(X, labels), 45 * (40 - 4) / (5 * (4 - 1)))\n", "def test_silhouette_score_sparse_input_with_sampling():\n    \"\"\"Check that silhouette_score works with sparse input and sampling.\"\"\"\n    rng = np.random.RandomState(0)\n    X = csr_matrix(rng.rand(100, 10))\n    labels = np.array([0] * 50 + [1] * 50)\n    score = silhouette_score(X, labels, sample_size=50, random_state=rng)\n    assert score >= -1.0 and score <= 1.0\n", "def test_calinski_harabasz_score_sparse_input():\n    \"\"\"Check that calinski_harabasz_score works for sparse inputs.\"\"\"\n    from scipy.sparse import csr_matrix\n\n    X = csr_matrix(np.array([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5))\n    labels = [0] * 10 + [1] * 10\n    score = calinski_harabasz_score(X, labels)\n    pytest.approx(score, 45 * (40 - 4) / (5 * (4 - 1)))\n", "def test_calinski_harabasz_score_sparse_input():\n    # Assert the function works with sparse input\n    from scipy.sparse import csr_matrix\n\n    X = csr_matrix(np.array([[1, 0, 2], [0, 0, 3], [4, 5, 0]]))\n    labels = np.array([0, 0, 1])\n    score = calinski_harabasz_score(X, labels)\n    assert score > 0\n", "def test_silhouette_score_for_dense_input_with_sample_size():\n    \"\"\"Check that silhouette_score works for dense input with sample_size.\"\"\"\n    X = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]\n    labels = [0, 0, 1, 1, 1]\n    score_all_samples = silhouette_score(X, labels)\n    score_sample_size_3 = silhouette_score(X, labels, sample_size=3, random_state=42)\n    assert score_sample_size_3 != score_all_samples\n    score_sample_size_5 = silhouette_score(X, labels, sample_size=5, random_state=42)\n    pytest.approx(score_sample_size_5, score_all_samples)\n"], "sample_872": ["def test_roc_auc_score_multi_class_labels_not_present_in_y_true():\n    # Test that roc_auc_score function raises an error when labels are not present in y_true\n    y_true = np.array([0, 1, 2, 2])\n    y_scores = np.array(\n        [[0.1, 0.8, 0.1], [0.3, 0.4, 0.3], [0.35, 0.5, 0.15], [0, 0.2, 0.8]]\n    )\n    labels = [0, 1, 2, 3]  # 3 is not present in y_true\n\n    err_msg = \"'y_true' contains labels not in parameter 'labels'\"\n    with pytest.raises(ValueError, match=err_msg):\n        roc_auc_score(y_true, y_scores, labels=labels, multi_class=\"ovr\")\n", "def test_roc_auc_score_average_options():\n    # Check that multiclass roc_auc raises an error for average options other than\n    # 'macro', 'weighted' and None.\n    rng = check_random_state(404)\n    y_true = rng.randint(0, 3, size=20)\n    y_pred = rng.rand(20, 3)\n\n    msg = (\n        r\"average must be one of \\('micro', 'macro', 'weighted', None\\) for \"\n        r\"multiclass problems\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        roc_auc_score(y_true, y_pred, multi_class=\"ovr\", average=\"samples\")\n\n    with pytest.raises(ValueError, match=msg):\n        roc_auc_score(y_true, y_pred, multi_class=\"ovo\", average=\"samples\")\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function raises an error when trying\n    # to compute multiclass AUC for parameters where an output\n    # is not defined.\n    rng = check_random_state(404)\n    y_score = rng.rand(20, 3)\n    y_prob = softmax(y_score)\n    y_true = rng.randint(0, 3, size=20)\n\n    msg = (\n        r\"average must be one of \\('macro', 'weighted', None\\) for \"\n        r\"multiclass problems\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=\"samples\")\n\n    msg = (\n        r\"average must be one of \\('micro', 'macro', 'weighted', None\\) for \"\n        r\"multiclass problems\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        roc_auc_score(y_true, y_prob, multi_class=\"ovo\", average=\"micro\")\n", "def test_ndcg_score_equal_to_zero():\n    # Test that ndcg_score returns 0 when y_true and y_score are unrelated.\n    rng = np.random.RandomState(42)\n    n_samples, n_classes = 10, 5\n    y_true = rng.randint(0, 2, size=(n_samples, n_classes))\n    y_score = rng.rand(n_samples, n_classes)\n    score = ndcg_score(y_true, y_score)\n    assert score == pytest.approx(0.0)\n", "def test_roc_auc_score_average_options():\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array(\n        [\n            [0.5, 0.2, 0.3],\n            [0.4, 0.5, 0.1],\n            [0.1, 0.2, 0.7],\n            [0.3, 0.4, 0.3],\n            [0.2, 0.6, 0.2],\n            [0.1, 0.2, 0.7],\n        ]\n    )\n\n    # Test 'ovr' average option\n    expected_ovr = (\n        roc_auc_score(y_true == 0, y_pred[:, 0])\n        + roc_auc_score(y_true == 1, y_pred[:, 1])\n        + roc_auc_score(y_true == 2, y_pred[:, 2])\n    ) / 3\n\n    assert_almost_equal(\n        roc_auc_score(y_true, y_pred, multi_class=\"ovr\", average=\"macro\"),\n        expected_ovr,\n    )\n\n    # Test 'ovo' average option\n    expected_ovo = (\n        roc_auc_score(y_true == 0, y_pred[:, 0] - y_pred[:, 1])\n        + roc_auc_score(y_true == 0, y_pred[:, 0] - y_pred[:, 2])\n        + roc_auc_score(y_true == 1, y_pred[:, 1] - y_pred[:, 2])\n    ) / 3\n\n    assert_almost_equal(\n        roc_auc_score(y_true, y_pred, multi_class=\"ovo\", average=\"macro\"),\n        expected_ovo,\n    )\n"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with pandas dataframe input\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame(X, columns=feature_names)\n    sel.fit(X_df, y)\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with None input_features\n    feature_names_out_actual = sel.get_feature_names_out(None)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n", "def test_get_support_mask_abstract_method():\n    class AbstractSelector(SelectorMixin, BaseEstimator):\n            return self\n\n    sel = AbstractSelector()\n    with pytest.raises(NotImplementedError):\n        sel._get_support_mask()\n", "def test_get_support_empty():\n    sel = StepSelector(step=0)\n    sel.fit(X, y)\n    assert_array_equal(np.zeros(X.shape[1], dtype='bool), sel.get_support())\n    assert_array_equal(np.array([]), sel.get_support(indices=True))\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with default input features\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with None input features\n    sel = StepSelector()\n    sel.fit(np.array([[1, 2], [3, 4]]))\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal([\"x0\"], feature_names_out_actual)\n"], "sample_874": ["def test_validate_params():\n    class DummyEstimator(BaseEstimator):\n        _parameter_constraints = {\n            \"param1\": [\"array-like\", \"sparse matrix\"],\n            \"param2\": [str, callable],\n        }\n\n            self.param1 = param1\n            self.param2 = param2\n\n    estimator = DummyEstimator(param1=np.array([1, 2, 3]), param2=\"hello\")\n    estimator._validate_params()\n\n    with pytest.raises(ValueError):\n        estimator = DummyEstimator(param1=\"not array-like\", param2=\"hello\")\n        estimator._validate_params()\n\n    with pytest.raises(ValueError):\n        estimator = DummyEstimator(param1=np.array([1, 2, 3]), param2=123)\n        estimator._validate_params()\n", "def test_clone():\n    sel = StepSelector(step=3)\n    sel_cloned = clone(sel)\n\n    assert sel.get_params() == sel_cloned.get_params()\n    assert sel_cloned.step == 3\n\n    # Check that clone does not copy the fitted state of an estimator\n    sel.fit(X, y)\n    sel_cloned = clone(sel)\n    assert not hasattr(sel_cloned, \"n_features_in_\")\n", "def test_clone_with_n_features_in():\n    # Test that clone works with estimators that have n_features_in_\n    from sklearn.linear_model import LogisticRegression\n\n    estimator = LogisticRegression()\n    estimator.n_features_in_ = 10\n\n    cloned_estimator = clone(estimator)\n\n    assert not hasattr(cloned_estimator, \"n_features_in_\")\n", "def test_clone_with_safe_false():\n    class Estimator(BaseEstimator):\n            self.param = param\n\n    estimator = Estimator()\n    clone_estimator = clone(estimator, safe=False)\n\n    assert_array_equal(estimator.param, clone_estimator.param)\n", "def test_get_support_mask():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(support, sel._get_support_mask())\n"], "sample_876": ["def test_mlp_regressor_multilabel():\n    # Test that multi-output regression works as expected\n    X, y = make_regression(n_samples=200, n_targets=3)\n    mlp = MLPRegressor(solver=\"lbfgs\", hidden_layer_sizes=50, max_iter=200, random_state=1)\n    mlp.fit(X, y)\n    assert mlp.score(X, y) > 0.9\n    y_pred = mlp.predict(X)\n    assert y_pred.shape == (200, 3)\n", "def test_mlp_classifier_predict_proba_binary_single_output():\n    # Test that predict_proba for binary classification returns an array with\n    # shape (n_samples, 2) when there is a single output.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    clf = MLPClassifier(hidden_layer_sizes=5, activation=\"logistic\", random_state=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n    y_proba = clf.predict_proba(X)\n\n    assert y_proba.shape == (len(X), 2)\n", "def test_mlp_feature_names_out():\n    \"\"\"Check that feature names out are correctly computed.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    rng = np.random.RandomState(0)\n\n    X = pd.DataFrame(data=rng.randn(10, 2), columns=[\"colname_a\", \"colname_b\"])\n    y = pd.Series(data=np.full(10, 1), name=\"colname_y\")\n\n    model = MLPClassifier()\n    model.fit(X, y)\n    assert model.feature_names_in_ == [\"colname_a\", \"colname_b\"]\n    assert model.n_features_in_ == 2\n", "def test_mlp_early_stopping_n_iter_no_change():\n    \"\"\"Check that early stopping occurs after n_iter_no_change.\"\"\"\n    mlp = MLPClassifier(\n        max_iter=1000, random_state=0, early_stopping=True, n_iter_no_change=5\n    )\n    with pytest.warns(ConvergenceWarning):\n        mlp.fit(X_digits, y_digits)\n    assert len(mlp.validation_scores_) - mlp.best_validation_score_idx_ == 5\n", "def test_mlp_fit_partial_fit_equivalence():\n    # Test that fit and partial_fit return the same results for classification\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n\n    mlp_fit = MLPClassifier(\n        hidden_layer_sizes=50, solver=\"sgd\", max_iter=1000, random_state=1\n    )\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp_fit.fit(X, y)\n\n    mlp_partial_fit = MLPClassifier(\n        hidden_layer_sizes=50, solver=\"sgd\", max_iter=1000, random_state=1\n    )\n    for _ in range(1000):\n        mlp_partial_fit.partial_fit(X, y, classes=np.unique(y))\n\n    assert_array_equal(mlp_fit.predict(X), mlp_partial_fit.predict(X))\n"], "sample_877": ["def test_isotonic_regression_transform_before_fit():\n    \"\"\"Check that calling `transform` before `fit` raises an error.\"\"\"\n    X = np.arange(10)\n    y = np.arange(10)\n    iso_reg = IsotonicRegression()\n\n    msg = \"IsotonicRegression is not fitted\"\n    with pytest.raises(NotFittedError, match=msg):\n        iso_reg.transform(X)\n\n    with pytest.raises(NotFittedError, match=msg):\n        iso_reg.predict(X)\n", "def test_isotonic_regression_out_of_bounds_below_min():\n    \"\"\"Check that `predict` raises an error when input is below min.\"\"\"\n    X = np.arange(10)\n    y = np.arange(10)\n\n    iso_reg = IsotonicRegression(out_of_bounds=\"raise\")\n    iso_reg.fit(X, y)\n\n    msg = \"below the interpolation range\"\n    with pytest.raises(ValueError, match=msg):\n        iso_reg.predict([np.min(X) - 1])\n", "def test_isotonic_regression_out_of_bounds_clip_warning():\n    \"\"\"Check that a warning is raised when out_of_bounds='clip' and X has values\n    outside of the training range.\"\"\"\n    X_train = np.array([1, 2, 3])\n    y_train = np.array([4, 5, 6])\n    X_test = np.array([0, 1, 2, 3, 4])\n\n    ir = IsotonicRegression(out_of_bounds='clip')\n    ir.fit(X_train, y_train)\n\n    msg = \"X has values outside of the training range\"\n    with pytest.warns(UserWarning, match=msg):\n        ir.predict(X_test)\n", "def test_isotonic_regression_increasing_parameter_validation():\n    # Test that the 'increasing' parameter of IsotonicRegression is validated\n    # correctly.\n    X = np.arange(10)\n    y = np.arange(10)\n\n    # Test that a boolean value is accepted\n    iso_reg = IsotonicRegression(increasing=True).fit(X, y)\n    assert iso_reg.increasing_ == True\n\n    iso_reg = IsotonicRegression(increasing=False).fit(X, y)\n    assert iso_reg.increasing_ == False\n\n    # Test that a string value 'auto' is accepted\n    iso_reg = IsotonicRegression(increasing='auto').fit(X, y)\n    assert isinstance(iso_reg.increasing_, bool)\n\n    # Test that other string values are not accepted\n    msg = r\"Invalid value for parameter 'increasing'. It should be 'auto', `True` or `False`.\"\n    with pytest.raises(ValueError, match=msg):\n        IsotonicRegression(increasing='invalid')\n\n    # Test that non-string and non-boolean values are not accepted\n    with pytest.raises(ValueError, match=msg):\n        IsotonicRegression(increasing=1)\n\n    with pytest.raises(ValueError, match=msg):\n        IsotonicRegression(increasing=None)\n", "def test_isotonic_regression_X_thresholds_dtype():\n    \"\"\"Check that the dtype of `X_thresholds_` is consistent with `X`.\"\"\"\n    X = np.arange(10, dtype=np.float32)\n    y = np.arange(10)\n\n    iso_reg = IsotonicRegression().fit(X, y)\n    assert iso_reg.X_thresholds_.dtype == X.dtype\n\n    X = np.arange(10, dtype=np.float64)\n    y = np.arange(10)\n\n    iso_reg = IsotonicRegression().fit(X, y)\n    assert iso_reg.X_thresholds_.dtype == X.dtype\n"], "sample_878": ["def test_column_transformer_feature_names_out_dtype():\n    \"\"\"Check the dtype of feature names out.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n    ct = ColumnTransformer(\n        [(\"trans\", TransWithNames([\"a\", \"b\"]), [\"A\", \"B\"])],\n        verbose_feature_names_out=True,\n    )\n    ct.fit(X_df)\n    names = ct.get_feature_names_out()\n    assert names.dtype == object\n\n    ct = ColumnTransformer(\n        [(\"trans\", TransWithNames([\"a\", \"b\"]), [\"A\", \"B\"])],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X_df)\n    names = ct.get_feature_names_out()\n    assert names.dtype == object\n\n    # No feature names\n    ct = ColumnTransformer([(\"trans\", \"drop\", [\"A\", \"B\"])]).fit(X_df)\n    names = ct.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n    assert len(names) == 0\n", "def test_column_transformer_get_feature_names_with_numpy_array():\n    \"\"\"Check get_feature_names_out with numpy array.\"\"\"\n    X = np.array([[1.0, 2.2], [3.0, 1.0]])\n    ct = ColumnTransformer(\n        [\n            (\"scale\", StandardScaler(), [0]),\n            (\"passthrough\", \"passthrough\", [1]),\n        ],\n        verbose_feature_names_out=True,\n    )\n    ct.fit(X)\n    names = ct.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n    assert_array_equal(names, [\"scale__x0\", \"passthrough__x1\"])\n\n    # with verbose_feature_names_out=False\n    ct.set_params(verbose_feature_names_out=False)\n    names = ct.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n    assert_array_equal(names, [\"x0\", \"x1\"])\n", "def test_column_transformer_set_output_after_fitting_multiple_times():\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=False, dtype=\"int16\"),\n                [\"pet\"],\n            ),\n            (\"age\", StandardScaler(), [\"age\"]),\n        ],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    )\n\n    # fit without calling set_output\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, np.ndarray)\n    assert X_trans.dtype == \"float64\"\n\n    ct.set_output(transform=\"pandas\")\n    X_trans_df = ct.transform(df)\n    expected_dtypes = {\n        \"pet_cat\": \"int16\",\n        \"pet_dog\": \"int16\",\n        \"pet_snake\": \"int16\",\n        \"height\": \"int64\",\n        \"age\": \"float64\",\n    }\n    for col, dtype in X_trans_df.dtypes.items():\n        assert dtype == expected_dtypes[col]\n\n    # Call set_output again with a different configuration\n    ct.set_output(transform=\"default\")\n    X_trans_np = ct.transform(df)\n    assert isinstance(X_trans_np, np.ndarray)\n    assert X_trans_np.dtype == \"float64\"\n", "def test_column_transformer_feature_names_out_dtype():\n    \"\"\"Check that get_feature_names_out returns an array of dtype object.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(\n        {\n            \"col_int\": np.array([0, 1, 2], dtype=int),\n            \"col_float\": np.array([0.0, 1.0, 2.0], dtype=float),\n            \"col_str\": [\"one\", \"two\", \"three\"],\n        },\n        columns=[\"col_int\", \"col_float\", \"col_str\"],\n    )\n\n    ct = ColumnTransformer(\n        [\n            (\"trans1\", StandardScaler(), [0]),\n            (\"trans2\", OneHotEncoder(sparse_output=False), slice(1, 3)),\n        ],\n        remainder=\"passthrough\",\n    )\n    ct.fit(X_df)\n    names = ct.get_feature_names_out()\n    assert names.dtype == object\n    assert all(isinstance(name, str) for name in names)\n", "def test_column_transformer_set_params_with_remainder_estimator():\n    \"\"\"Check set_params works with remainder estimator.\"\"\"\n    ct = ColumnTransformer([(\"trans1\", StandardScaler(), [0])], remainder=StandardScaler())\n    new_scaler = StandardScaler(with_mean=False)\n    ct.set_params(remainder=new_scaler)\n    assert ct.remainder is new_scaler\n    ct.set_params(remainder__with_std=False)\n    assert not ct.remainder.with_std\n"], "sample_879": ["def test_one_hot_encoder_get_feature_names_out_numpy_array():\n    \"\"\"Check get_feature_names_out with numpy array.\"\"\"\n    X = np.array([[\"a\", \"b\"], [\"c\", \"d\"]])\n    ohe = OneHotEncoder()\n    ohe.fit(X)\n    feature_names = ohe.get_feature_names_out()\n    assert_array_equal(feature_names, [\"x0_a\", \"x0_c\", \"x1_b\", \"x1_d\"])\n", "def test_one_hot_encoder_handle_unknown_infrequent_if_exist():\n    X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3]).T\n    ohe = OneHotEncoder(\n        handle_unknown=\"infrequent_if_exist\",\n        sparse_output=False,\n        max_categories=2,\n    ).fit(X)\n\n    # 'e' is an unknown category and will be mapped to the infrequent category\n    X_test = [[\"a\"], [\"e\"], [\"b\"]]\n    expected = np.array([[1, 0], [0, 1], [0, 0]])\n\n    X_trans = ohe.transform(X_test)\n    assert_allclose(expected, X_trans)\n", "def test_one_hot_encoder_handle_unknown_if_exist_with_infrequent():\n    \"\"\"Check handle_unknown='infrequent_if_exist' when infrequent categories exist.\"\"\"\n\n    X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3]).T\n    ohe = OneHotEncoder(\n        handle_unknown=\"infrequent_if_exist\",\n        sparse_output=False,\n        max_categories=2,\n    ).fit(X)\n\n    # \"e\" is an unknown category, it will be mapped to the infrequent category\n    X_test = [[\"b\"], [\"a\"], [\"c\"], [\"d\"], [\"e\"]]\n    expected = np.array([[1, 0], [0, 1], [0, 1], [0, 1], [0, 1]])\n\n    X_trans = ohe.transform(X_test)\n    assert_allclose(expected, X_trans)\n", "def test_ohe_get_feature_names_out_drop_if_binary():\n    \"\"\"Check get_feature_names_out with drop='if_binary'.\"\"\"\n    X = np.array([[\"Male\", 1], [\"Female\", 3], [\"Female\", 2]], dtype=object)\n    ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False).fit(X)\n\n    feature_names = ohe.get_feature_names_out()\n    assert_array_equal(feature_names, [\"x0_Female\", \"x1_2\", \"x1_3\"])\n", "def test_ordinal_encoder_drop_idx():\n    \"\"\"Check that drop_idx is defined correctly for OrdinalEncoder.\"\"\"\n    X = np.array([[\"a\"] * 2 + [\"b\"] * 4 + [\"c\"] * 4 + [\"d\"] * 4 + [\"e\"] * 4], dtype=object).T\n    oe = OrdinalEncoder().fit(X)\n\n    oe.set_params(encoded_missing_value=-1)\n    assert oe.drop_idx_ is None\n\n    oe.set_params(encoded_missing_value=\"a\")\n    assert_array_equal(oe.drop_idx_, [0])\n\n    oe.set_params(encoded_missing_value=\"b\")\n    assert_array_equal(oe.drop_idx_, [1])\n\n    oe.set_params(encoded_missing_value=\"f\")\n    msg = \"encoded_missing_value should be one of the categories\"\n    with pytest.raises(ValueError, match=msg):\n        oe.transform(X)\n"], "sample_880": ["def test_class_distribution_sparse_explicit_zeros():\n    # Define the sparse matrix with explicit zeros\n    data = np.array([1, 2, 0, 4, 2, 1, 0, 2, 3, 2, 3, 1, 1, 1, 1, 1, 1])\n    indices = np.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 5, 0, 1, 2, 3, 4, 5])\n    indptr = np.array([0, 6, 11, 11, 17])\n    y_sp = sp.csc_matrix((data, indices, indptr), shape=(6, 4))\n\n    classes, n_classes, class_prior = class_distribution(y_sp)\n\n    classes_expected = [[0, 1, 2, 4], [0, 2, 3], [0], [1]]\n    n_classes_expected = [4, 3, 1, 1]\n    class_prior_expected = [\n        [1 / 6, 2 / 6, 1 / 6, 2 / 6],\n        [1 / 3, 1 / 3, 1 / 3],\n        [1.0],\n        [1.0],\n    ]\n\n    for k in range(y_sp.shape[1]):\n        assert_array_almost_equal(classes[k], classes_expected[k])\n        assert_array_almost_equal(n_classes[k], n_classes_expected[k])\n        assert_array_almost_equal(class_prior[k], class_prior_expected[k])\n", "def test_type_of_target_with_none_values():\n    # Test that type_of_target correctly handles arrays with None values\n    y = np.array([0, 1, None])\n    assert type_of_target(y) == \"unknown\"\n\n    y = np.array([0, 1, np.nan])\n    assert type_of_target(y) == \"continuous\"\n\n    y = np.array([\"a\", \"b\", None], dtype=object)\n    assert type_of_target(y) == \"unknown\"\n", "def test_check_partial_fit_first_call():\n    clf = SVC()\n    classes = [0, 1, 2]\n    assert _check_partial_fit_first_call(clf, classes=classes) is True\n    assert clf.classes_ == classes\n\n    # Test that subsequent calls with the same classes do not raise an error\n    assert _check_partial_fit_first_call(clf, classes=classes) is False\n\n    # Test that a call with different classes raises an error\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(clf, classes=[0, 1])\n\n    # Test that a call with no classes specified raises an error on first call\n    clf2 = SVC()\n    with pytest.raises(ValueError):\n        _check_partial_fit_first_call(clf2)\n", "def test_ovr_decision_function_tie_breaking():\n    # Test tie breaking in ovr decision function\n\n    predictions = np.array([[0, 1, 1], [0, 1, 0], [0, 1, 1], [0, 1, 1]])\n\n    confidences = np.array(\n        [[-1e16, 0, -1e16], [1.0, 2.0, -3.0], [-5.0, 2.0, 5.0], [-0.5, 0.2, 0.5]]\n    )\n\n    n_classes = 3\n\n    dec_values = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # Create a tie in the votes\n    confidences_tied = confidences.copy()\n    confidences_tied[0, 1] = confidences_tied[0, 2]\n\n    dec_values_tied = _ovr_decision_function(predictions, confidences_tied, n_classes)\n\n    # Check that the tie is broken by the confidence values\n    assert dec_values_tied[0, 1] > dec_values_tied[0, 2]\n", "def test_type_of_target_sparse():\n    # Test type_of_target with sparse matrices\n    from scipy.sparse import csr_matrix\n\n    # Binary classification\n    y = csr_matrix(np.array([0, 1, 1, 0]))\n    assert type_of_target(y) == \"binary\"\n\n    # Multiclass classification\n    y = csr_matrix(np.array([0, 1, 2, 0]))\n    assert type_of_target(y) == \"multiclass\"\n\n    # Multilabel-indicator classification\n    y = csr_matrix(np.array([[0, 1], [1, 0]]))\n    assert type_of_target(y) == \"multilabel-indicator\"\n\n    # Unknown type\n    y = csr_matrix(np.array([[0, 1], [1, 2]]))\n    assert type_of_target(y) == \"unknown\"\n"], "sample_881": ["def test_roc_auc_score_binary_averaging():\n    # Test that roc_auc_score with multi-class data and\n    # binary averaging strategy gives the same result as binary data.\n    X, y = make_classification(n_samples=100, n_informative=5, n_redundant=0,\n                               n_classes=3, random_state=42)\n    y_binary = np.array([y == c for c in np.unique(y)]).T\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    y_binary_train, y_binary_test = [x.T for x in train_test_split(y_binary, random_state=42)]\n\n    clf = LogisticRegression(random_state=42)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n\n    scores = []\n    for i, (y_test_i, y_pred_i) in enumerate(zip(y_binary_test.T, y_pred.T)):\n        score = roc_auc_score(y_test_i, y_pred_i)\n        scores.append(score)\n\n    assert_almost_equal(roc_auc_score(y_test, y_pred, average='macro'),\n                        np.mean(scores))\n    assert_almost_equal(roc_auc_score(y_test, y_pred, average='weighted'),\n                        np.average(scores, weights=[np.sum(y_test == i) for i in np.unique(y)]))\n", "def test_roc_auc_score_with_non_finite_values():\n    y_true = [0, 1, 0, 1]\n    y_pred = [np.nan, 1.0, 0.0, np.inf]\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity\"):\n        roc_auc_score(y_true, y_pred)\n    y_pred = [np.nan, 1.0, 0.0, -np.inf]\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity\"):\n        roc_auc_score(y_true, y_pred)\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function returns an error when the 'average'\n    # parameter is not one of 'micro', 'macro', 'weighted', 'samples', None.\n    rng = check_random_state(404)\n    y_true = rng.randint(0, 2, size=(10,))\n    y_pred = rng.rand(10)\n\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"invalid\")\n", "def test_roc_auc_score_average_options_binary():\n    y_true = [0, 1]\n    y_pred = [0, 1]\n\n    # Test average is None\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=None)\n\n    # Test average is binary\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average=\"binary\"), 1.0)\n\n    # Test average is micro\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average=\"micro\"), 1.0)\n\n    # Test average is macro\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average=\"macro\"), 1.0)\n\n    # Test average is weighted\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average=\"weighted\"), 1.0)\n\n    # Test average is samples\n    assert_almost_equal(roc_auc_score(y_true, y_pred, average=\"samples\"), 1.0)\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function returns an error when the 'average'\n    # parameter is not one of None, \"macro\", \"weighted\", \"samples\", \"micro\".\n    rng = check_random_state(404)\n    y_true = rng.randint(0, 2, size=(10,))\n    y_pred = rng.rand(10)\n\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"somethingelse\")\n"], "sample_882": ["def test_mlp_n_iter_no_change_default():\n    # Test that the default value of n_iter_no_change is used correctly.\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n    clf = MLPClassifier(tol=0.5, max_iter=3000, solver=\"sgd\")\n    clf.fit(X, y)\n    assert clf._no_improvement_count == clf.n_iter_no_change + 1\n", "def test_mlp_sparse_input_with_early_stopping():\n    \"\"\"Check that sparse input is handled correctly with early stopping.\"\"\"\n    X_sparse = csr_matrix(X_digits_binary[:100])\n    y = y_digits_binary[:100]\n\n    mlp = MLPClassifier(early_stopping=True, random_state=0)\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp.fit(X_sparse, y)\n\n    assert mlp.score(X_sparse, y) > 0.9\n", "def test_mlp_regressor_early_stopping_stratified():\n    # Make sure data splitting for early stopping is stratified in regression\n    X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n    y = [0, 0, 0, 1]\n\n    mlp = MLPRegressor(early_stopping=True)\n    with pytest.raises(\n        ValueError, match=\"The least populated class in y has only 1 member\"\n    ):\n        mlp.fit(X, y)\n", "def test_mlp_feature_names_out():\n    \"\"\"Check that transformed data has feature names.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    rng = np.random.RandomState(0)\n\n    X = pd.DataFrame(data=rng.randn(10, 2), columns=[\"colname_a\", \"colname_b\"])\n    y = pd.Series(data=np.full(10, 1), name=\"colname_y\")\n\n    model = MLPRegressor()\n    Xt = model.fit_transform(X, y)\n\n    assert Xt.columns.tolist() == [f\"MLPRegressor{model.n_outputs_}\"]\n", "def test_mlp_classifier_predict_proba_output_shape():\n    # Test that the output shape of predict_proba is correct.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    clf = MLPClassifier(hidden_layer_sizes=5, activation=\"logistic\", random_state=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n\n    y_proba = clf.predict_proba(X)\n    assert y_proba.shape == (X.shape[0], np.unique(y).size)\n"], "sample_883": ["def test_bayesian_ridge_fit_intercept_false():\n    # Test BayesianRidge with fit_intercept=False\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    clf = BayesianRidge(fit_intercept=False)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.intercept_, 0.0)\n    assert_array_almost_equal(clf.predict(X), clf._decision_function(X))\n", "def test_bayesian_ridge_parameter_initialization():\n    # Test BayesianRidge with initial values (alpha_init, lambda_init)\n    X = np.vander(np.linspace(0, 4, 5), 4)\n    y = np.array([0.0, 1.0, 0.0, -1.0, 0.0])  # y = (x^3 - 6x^2 + 8x) / 3\n\n    # Test that the initial values of alpha_ and lambda_ are set correctly\n    reg = BayesianRidge(alpha_init=1.0, lambda_init=1e-3)\n    reg.fit(X, y)\n\n    assert reg.alpha_ == pytest.approx(1.0)\n    assert reg.lambda_ == pytest.approx(1e-3)\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with and without intercept.\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n\n    # With intercept\n    model = BayesianRidge(fit_intercept=True)\n    model.fit(X, y)\n    assert model.intercept_ is not None\n\n    # Without intercept\n    model = BayesianRidge(fit_intercept=False)\n    model.fit(X, y)\n    assert model.intercept_ == 0.0\n", "def test_bayesian_ridge_ard_with_zero_variance_targets():\n    # Test BayesianRidge and ARDRegression with zero variance targets\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([0, 0, 0, 0, 0, 0, 0]).T\n\n    for clf in [BayesianRidge(), ARDRegression()]:\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.predict(X), y)\n", "def test_bayesian_ridge_ard_init_parameters():\n    # Test BayesianRidge and ARDRegression with different initial values\n    X, y = diabetes.data, diabetes.target\n\n    # Test with alpha_init and lambda_init\n    br_model = BayesianRidge(alpha_init=1.0, lambda_init=1.0)\n    br_model.fit(X, y)\n\n    ard_model = ARDRegression(alpha_1=1e-5, alpha_2=1e-5, lambda_1=1e-5, lambda_2=1e-5)\n    ard_model.fit(X, y)\n\n    assert br_model.alpha_ > 0\n    assert br_model.lambda_ > 0\n    assert ard_model.alpha_ > 0\n    assert np.all(ard_model.lambda_ > 0)\n\n    # Test with default initial values\n    br_model = BayesianRidge()\n    br_model.fit(X, y)\n\n    ard_model = ARDRegression()\n    ard_model.fit(X, y)\n\n    assert br_model.alpha_ > 0\n    assert br_model.lambda_ > 0\n    assert ard_model.alpha_ > 0\n    assert np.all(ard_model.lambda_ > 0)\n"], "sample_884": ["def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n    assert MockClass2().n_features_ == 10\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n    assert MockClass2().n_features_ == 10\n", "def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n    assert MockClass2().n_features_ == 10\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n    assert MockClass2().n_features_ == 10\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n    assert MockClass2().n_features_ == 10\n"], "sample_885": ["def test_interval_type_real_not_int():\n    \"\"\"Check that the Interval constraint raises a ValueError for invalid type.\"\"\"\n    with pytest.raises(ValueError, match=\"type must be either numbers.Integral\"):\n        Interval(\"invalid_type\", 0, 1, closed=\"both\")\n", "def test_interval_real_not_int_generate_valid_param():\n    \"\"\"Check that generate_valid_param works with 'real_not_int' interval.\"\"\"\n    constraint = Interval(\"real_not_int\", 0, 1, closed=\"both\")\n    value = generate_valid_param(constraint)\n    assert constraint.is_satisfied_by(value)\n", "def test_interval_real_not_int_edge_cases():\n    \"\"\"Check edge cases for the type \"real_not_int\" in the Interval constraint.\"\"\"\n    constraint = Interval(\"real_not_int\", 0, 1, closed=\"both\")\n    assert constraint.is_satisfied_by(0.0)\n    assert not constraint.is_satisfied_by(0)\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n\n    constraint = Interval(\"real_not_int\", None, 1, closed=\"left\")\n    assert constraint.is_satisfied_by(-1.0)\n    assert not constraint.is_satisfied_by(-1)\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n\n    constraint = Interval(\"real_not_int\", 0, None, closed=\"right\")\n    assert constraint.is_satisfied_by(0.0)\n    assert not constraint.is_satisfied_by(0)\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n\n    constraint = Interval(\"real_not_int\", None, None, closed=\"neither\")\n    assert constraint.is_satisfied_by(0.0)\n    assert not constraint.is_satisfied_by(0)\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n", "def test_interval_real_not_int_edge_cases():\n    \"\"\"Check for the type \"real_not_int\" in the Interval constraint with edge cases.\"\"\"\n    constraint = Interval(\"real_not_int\", 0, 1, closed=\"both\")\n    assert constraint.is_satisfied_by(0.0)\n    assert not constraint.is_satisfied_by(0)\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n\n    constraint = Interval(\"real_not_int\", 0, 1, closed=\"left\")\n    assert constraint.is_satisfied_by(0.0)\n    assert not constraint.is_satisfied_by(0)\n    assert not constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n\n    constraint = Interval(\"real_not_int\", 0, 1, closed=\"right\")\n    assert not constraint.is_satisfied_by(0.0)\n    assert not constraint.is_satisfied_by(0)\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n\n    constraint = Interval(\"real_not_int\", 0, 1, closed=\"neither\")\n    assert not constraint.is_satisfied_by(0.0)\n    assert not constraint.is_satisfied_by(0)\n    assert not constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n", "def test_make_constraint_list_of_constraints():\n    \"\"\"Check that make_constraint works with a list of constraints.\"\"\"\n    constraints = [Interval(Real, 0, 1, closed=\"both\"), StrOptions({\"option1\", \"option2\"})]\n    constraint = make_constraint(constraints)\n    assert isinstance(constraint, list)\n    assert len(constraint) == 2\n    assert isinstance(constraint[0], Interval)\n    assert isinstance(constraint[1], StrOptions)\n"], "sample_886": ["def test__wrap_in_pandas_container_no_columns():\n    \"\"\"Check that _wrap_in_pandas_container works when columns=None.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame({\"feat1\": [1, 2, 3], \"feat2\": [3, 4, 5]})\n\n    X_wrapped = _wrap_in_pandas_container(X_df, columns=None)\n    assert_array_equal(X_wrapped.columns, X_df.columns)\n\n    X_np = np.asarray([[1, 3], [2, 4], [3, 5]])\n    X_wrapped = _wrap_in_pandas_container(X_np, columns=None)\n    assert_array_equal(X_wrapped.columns, range(X_np.shape[1]))\n", "def test_set_output_with_fit_transform():\n    \"\"\"Check that set_output works with fit_transform.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().set_output(transform=\"pandas\")\n    X_trans = est.fit_transform(X)\n\n    assert isinstance(X_trans, pd.DataFrame)\n", "def test_set_output_mixin_with_fit_transform():\n    \"\"\"Check that set_output mixin works with fit_transform.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().set_output(transform=\"pandas\")\n\n    X_trans_pd = est.fit_transform(X)\n    assert isinstance(X_trans_pd, pd.DataFrame)\n", "def test_wrap_data_with_container_method():\n    \"\"\"Check that _wrap_data_with_container works as expected.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X)\n\n    # default output config\n    wrapped_X = _wrap_data_with_container(\"transform\", X, X, est)\n    assert isinstance(wrapped_X, np.ndarray)\n\n    # pandas output config\n    est.set_output(transform=\"pandas\")\n    wrapped_X = _wrap_data_with_container(\"transform\", X, X, est)\n    assert isinstance(wrapped_X, pd.DataFrame)\n\n    # if estimator is not configured for wrapping, return data_to_wrap unchanged\n    est._sklearn_auto_wrap_output_keys = set()\n    wrapped_X = _wrap_data_with_container(\"transform\", X, X, est)\n    assert wrapped_X is X\n", "def test__wrap_data_with_container_no_auto_wrap():\n    \"\"\"Check that _wrap_data_with_container does not wrap when auto-wrap is disabled.\"\"\"\n    est = EstimatorWithSetOutputNoAutoWrap()\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    assert X is _wrap_data_with_container(\"transform\", X, X, est)\n"], "sample_887": ["def test_calibrated_classifier_cv_feature_names_out(data):\n    \"\"\"Check that CalibratedClassifierCV propagates the `feature_names_out` attribute.\"\"\"\n    X, y = data\n    estimator = LogisticRegression()\n    calibrated_clf = CalibratedClassifierCV(estimator)\n    calibrated_clf.fit(X, y)\n\n    assert hasattr(calibrated_clf, \"feature_names_out_\")\n    assert calibrated_clf.feature_names_out_ is None\n\n    # Check that we can set feature names and they propagate\n    X_with_names = np.asarray(X)\n    X_with_names = _convert_container(X_with_names, \"dataframe\")\n    calibrated_clf_with_names = CalibratedClassifierCV(estimator)\n    calibrated_clf_with_names.fit(X_with_names, y)\n\n    assert hasattr(calibrated_clf_with_names, \"feature_names_out_\")\n    assert len(calibrated_clf_with_names.feature_names_out_) == X.shape[1]\n", "def test_calibration_with_n_features_in():\n    \"\"\"Check that `CalibratedClassifierCV` handles `n_features_in_` correctly.\"\"\"\n    X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)\n    clf = LinearSVC()\n    cal_clf = CalibratedClassifierCV(clf)\n\n    # Check that `n_features_in_` is not set before fitting\n    assert not hasattr(cal_clf, \"n_features_in_\")\n\n    cal_clf.fit(X, y)\n\n    # Check that `n_features_in_` is set after fitting\n    assert hasattr(cal_clf, \"n_features_in_\")\n    assert cal_clf.n_features_in_ == X.shape[1]\n", "def test_calibration_curve_label_encoder():\n    # Check that `calibration_curve` works with `LabelEncoder`\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0.0, 0.1, 0.2, 0.8, 0.9, 1.0])\n\n    le = LabelEncoder()\n    y_true_encoded = le.fit_transform(y_true)\n\n    prob_true, prob_pred = calibration_curve(y_true, y_pred)\n    prob_true_encoded, prob_pred_encoded = calibration_curve(y_true_encoded, y_pred)\n\n    assert_array_almost_equal(prob_true, prob_true_encoded)\n    assert_array_almost_equal(prob_pred, prob_pred_encoded)\n", "def test_calibration_curve_with_one_bin():\n    \"\"\"Check calibration_curve function with one bin\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0.0, 0.1, 0.2, 0.8, 0.9, 1.0])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=1)\n    assert len(prob_true) == len(prob_pred)\n    assert len(prob_true) == 1\n    assert_almost_equal(prob_true, [0.5])\n    assert_almost_equal(prob_pred, [0.5])\n", "def test_calibration_curve_pos_label_ignored_if_y_binary():\n    \"\"\"Check that `pos_label` is ignored if `y_true` is binary.\"\"\"\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([0.2, 0.8, 0.3, 0.7])\n\n    # default case\n    prob_true, _ = calibration_curve(y_true, y_pred, n_bins=2)\n    assert_allclose(prob_true, [0, 1])\n\n    # using pos_label\n    prob_true, _ = calibration_curve(y_true, y_pred, n_bins=2, pos_label=1)\n    assert_allclose(prob_true, [0, 1])\n\n    # check that using the other label does not change the result\n    prob_true, _ = calibration_curve(y_true, y_pred, n_bins=2, pos_label=0)\n    assert_allclose(prob_true, [0, 1])\n"], "sample_888": ["def test_iforest_fit_time():\n    \"\"\"Check that fitting time is not excessive\"\"\"\n    X, _ = make_classification(n_samples=1000, n_features=10, random_state=0)\n    start_time = time.time()\n    IsolationForest(n_estimators=10).fit(X)\n    end_time = time.time()\n    assert (end_time - start_time) < 2\n", "def test_iforest_estimator_property():\n    X = np.array([[1, 2], [3, 4]])\n    model = IsolationForest()\n    model.fit(X)\n\n    assert isinstance(model.estimator_, ExtraTreeRegressor)\n    assert model.estimator_.max_features == 1\n    assert model.estimator_.splitter == \"random\"\n", "def test_iforest_check_feature_names():\n    \"\"\"Check that feature names are checked during scoring when available.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    rng = np.random.RandomState(0)\n\n    X_train = pd.DataFrame(data=rng.randn(10, 2), columns=[\"a\", \"b\"])\n    model = IsolationForest(random_state=0).fit(X_train)\n\n    X_test = pd.DataFrame(data=rng.randn(5, 2), columns=[\"a\", \"c\"])\n    with pytest.raises(ValueError, match=\"Feature names must be identical\"):\n        model.predict(X_test)\n", "def test_iforest_fit_time_decreases_with_n_jobs():\n    \"\"\"Check that fitting time decreases when using multiple jobs\"\"\"\n    from sklearn.datasets import make_blobs\n    import time\n\n    X, _ = make_blobs(n_samples=1000, n_features=10, random_state=0)\n\n    model_single_job = IsolationForest(n_estimators=100, n_jobs=1, random_state=0)\n    start_single_job = time.time()\n    model_single_job.fit(X)\n    fit_time_single_job = time.time() - start_single_job\n\n    model_multiple_jobs = IsolationForest(n_estimators=100, n_jobs=-1, random_state=0)\n    start_multiple_jobs = time.time()\n    model_multiple_jobs.fit(X)\n    fit_time_multiple_jobs = time.time() - start_multiple_jobs\n\n    assert fit_time_multiple_jobs < fit_time_single_job\n", "def test_iforest_with_non_integer_max_samples():\n    \"\"\"Test whether iforest handles non-integer max_samples correctly\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    iforest = IsolationForest(max_samples=0.75)\n    iforest.fit(X)\n    assert iforest.max_samples_ == int(0.75 * X.shape[0])\n"], "sample_889": ["def test_calibration_curve_multiclass_error():\n    \"\"\"Check that calibration_curve raises an error for multiclass problems.\"\"\"\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([0.0, 0.5, 1.0])\n\n    err_msg = \"Only binary classification is supported.\"\n    with pytest.raises(ValueError, match=err_msg):\n        calibration_curve(y_true, y_pred)\n", "def test_calibration_with_string_labels(data):\n    \"\"\"Check that calibration works with string labels.\"\"\"\n    X, y = data\n    y_str = np.array([\"class_0\" if label == 0 else \"class_1\" for label in y])\n\n    estimator = LogisticRegression()\n    calibrated_clf = CalibratedClassifierCV(estimator)\n    calibrated_clf.fit(X, y_str)\n\n    assert hasattr(calibrated_clf, \"classes_\")\n    assert_array_equal(calibrated_clf.classes_, np.unique(y_str))\n", "def test_calibration_curve_with_invalid_pos_label():\n    \"\"\"Check calibration_curve function raises error with invalid pos_label\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0.0, 0.1, 0.2, 0.8, 0.9, 1.0])\n\n    err_msg = \"pos_label=2 is not a valid label: It should be one of\"\n    with pytest.raises(ValueError, match=err_msg):\n        calibration_curve(y_true, y_pred, pos_label=2)\n", "def test_calibration_display_pos_label_invalid():\n    \"\"\"Check the error message when passing an invalid `pos_label`.\"\"\"\n    y_true = np.array([0, 1, 1, 0])\n    y_pred = np.array([0.2, 0.8, 0.7, 0.3])\n\n    err_msg = (\n        \"y_true takes value in {0, 1} and pos_label is not specified: \"\n        \"either make y_true take value in {0, 1} or {-1, 1} or pass \"\n        \"pos_label explicitly\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        CalibrationDisplay.from_predictions(y_true, y_pred)\n\n    err_msg = \"Invalid pos_label for the target variable. Got 5 but targets are [0 1]\"\n    with pytest.raises(ValueError, match=err_msg):\n        CalibrationDisplay.from_predictions(y_true, y_pred, pos_label=5)\n", "def test_calibrated_classifier_cv_base_estimator_is_pipeline():\n    \"\"\"Check that CalibratedClassifierCV works when the base estimator is a pipeline.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10)\n    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression())])\n    calibrated_clf = CalibratedClassifierCV(pipe)\n    calibrated_clf.fit(X, y)\n    assert hasattr(calibrated_clf, \"calibrated_classifiers_\")\n"], "sample_890": ["def test_n_features_to_select_with_cross_validation():\n    \"\"\"Check that the number of features to select is correctly determined when\n    using cross-validation.\"\"\"\n\n    X, y = make_regression(n_features=10)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        cv=5,\n    )\n    sfs.fit(X, y)\n\n    assert sfs.n_features_to_select_ > 0\n    assert sfs.n_features_to_select_ <= X.shape[1]\n", "def test_sfs_with_custom_scoring():\n    # Make sure that a custom scoring function is accepted\n\n    X, y = make_regression(n_features=10)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        cv=2,\n        scoring=\"neg_mean_squared_error\",\n    )\n    sfs.fit(X, y)\n    sfs.transform(X)\n\n        return estimator.score(X, y)\n\n    sfs = SequentialFeatureSelector(\n        LinearRegression(), n_features_to_select=\"auto\", cv=2, scoring=custom_scoring\n    )\n    sfs.fit(X, y)\n    sfs.transform(X)\n", "def test_sfs_with_scoring():\n    \"\"\"Check that SequentialFeatureSelector works with a specified scoring function\"\"\"\n\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        direction=\"forward\",\n        scoring=\"neg_mean_squared_error\",\n    )\n    sfs.fit(X, y)\n\n    assert sfs.get_support().sum() > 0\n    assert sfs.transform(X).shape[1] == sfs.get_support().sum()\n", "def test_n_features_to_select_with_custom_scorer(n_features_to_select):\n    # Test SequentialFeatureSelector with a custom scorer\n\n    X, y = make_regression(n_features=10)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=n_features_to_select,\n        scoring=\"neg_mean_squared_error\",\n        cv=2,\n    )\n    sfs.fit(X, y)\n    assert sfs.n_features_to_select_ == n_features_to_select\n", "def test_validate_params():\n    # Make sure that parameters are validated correctly\n\n    X, y = make_regression(n_features=10)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        cv=2,\n        direction=\"invalid_direction\",\n    )\n\n    with pytest.raises(ValueError, match=\"direction must be either 'forward' or 'backward'\"):\n        sfs.fit(X, y)\n\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"invalid_n_features_to_select\",\n        cv=2,\n        direction=\"forward\",\n    )\n\n    with pytest.raises(ValueError, match=\"n_features_to_select must be either 'auto', an integer in\"):\n        sfs.fit(X, y)\n\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        cv=\"invalid_cv\",\n        direction=\"forward\",\n    )\n\n    with pytest.raises(TypeError, match=\"cv should be an integer, cross-validator or an iterable yielding\"):\n        sfs.fit(X, y)\n"], "sample_891": ["def test_auc_x_equals_y():\n    # Test Area Under Curve (AUC) computation when x equals y\n    x = [0, 1]\n    y = [0, 1]\n    assert_array_almost_equal(auc(x, y), 0.5)\n", "def test_roc_auc_score_average_binary():\n    y_true = np.array([0, 1])\n    y_score = np.array([0.25, 0.75])\n\n    # 'macro' and 'weighted' average methods should return the same as 'binary'\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, average=\"macro\"),\n        roc_auc_score(y_true, y_score, average=\"binary\"),\n    )\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, average=\"weighted\"),\n        roc_auc_score(y_true, y_score, average=\"binary\"),\n    )\n\n    # 'micro' average method should also return the same as 'binary'\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, average=\"micro\"),\n        roc_auc_score(y_true, y_score, average=\"binary\"),\n    )\n\n    # 'samples' average method is not applicable for binary classification\n    err_msg = (\n        \"Parameter 'average' must be one of ('binary', 'micro', \"\n        \"'macro', 'weighted') for binary classification. Got 'samples'.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        roc_auc_score(y_true, y_score, average=\"samples\")\n\n    # None average method is not applicable for binary classification\n    err_msg = (\n        \"Parameter 'average' must be one of ('binary', 'micro', \"\n        \"'macro', 'weighted') for binary classification. Got None.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        roc_auc_score(y_true, y_score, average=None)\n", "def test_ndcg_score_constant_y_true():\n    \"\"\"Check that ndcg_score is well-defined when y_true is constant.\"\"\"\n    y_true = np.array([[1, 1, 1], [1, 1, 1]])\n    y_score = np.array([[3, 2, 1], [3, 2, 1]])\n    score = ndcg_score(y_true, y_score)\n    assert score == pytest.approx(1.0)\n\n    y_true = np.array([[0, 0, 0], [0, 0, 0]])\n    y_score = np.array([[3, 2, 1], [3, 2, 1]])\n    score = ndcg_score(y_true, y_score)\n    assert score == pytest.approx(1.0)\n", "def test_label_ranking_avg_precision_score_pos_label():\n    # Test that label_ranking_avg_precision_score with binary y_true\n    # raises an informative error when pos_label is not specified.\n    y_true = np.array([[0, 1], [1, 0]])\n    y_score = np.array([[0.5, 0.9], [0.1, 0.8]])\n\n    err_msg = (\n        \"Parameter 'pos_label' is fixed to 1 for multilabel-indicator y_true. \"\n        \"Do not set 'pos_label' or set 'pos_label' to 1.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        label_ranking_average_precision_score(y_true, y_score, pos_label=2)\n", "def test_roc_auc_score_pos_label_errors():\n    # Raise an error when pos_label is not in binary y_true\n    y_true = np.array([0, 1])\n    y_pred = np.array([0, 1])\n    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\"\n    with pytest.raises(ValueError, match=err_msg):\n        roc_auc_score(y_true, y_pred, pos_label=2)\n    # Raise an error for multilabel-indicator y_true with\n    # pos_label other than 1\n    y_true = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])\n    y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2], [0.2, 0.8]])\n    err_msg = (\n        \"Parameter pos_label is fixed to 1 for multilabel-indicator y_true. \"\n        \"Do not set pos_label or set pos_label to 1.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        roc_auc_score(y_true, y_pred, pos_label=0)\n"], "sample_892": ["def test_adaboost_with_tree_stumps():\n    # Check that AdaBoost works with tree stumps.\n    X, y = datasets.make_classification(\n        n_samples=2000, n_features=10, n_informative=3, n_redundant=0, random_state=1\n    )\n    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=5)\n    clf.fit(X, y)\n    assert_array_equal(clf.score(X, y), 1.0)\n", "def test_adaboost_regressor_with_constant_target():\n    # Check that AdaBoostRegressor works with constant target.\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 5])\n    model = AdaBoostRegressor()\n    model.fit(X, y)\n    assert_array_almost_equal(model.predict(X), np.array([5, 5]))\n", "def test_adaboostclassifier_with_custom_estimator():\n    # Test that AdaBoostClassifier works with a custom estimator.\n    class CustomEstimator:\n            self.classes_ = None\n\n            self.classes_ = np.unique(y)\n            return self\n\n            return np.zeros(X.shape[0])\n\n            return np.ones((X.shape[0], len(self.classes_))) / len(self.classes_)\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    clf = AdaBoostClassifier(estimator=CustomEstimator())\n    clf.fit(X, y)\n    assert clf.predict(X).shape == (2,)\n    assert clf.predict_proba(X).shape == (2, 2)\n", "def test_adaboost_classifier_attributes():\n    # Check that the attributes of AdaBoostClassifier are set correctly.\n    X, y = iris.data, iris.target\n    clf = AdaBoostClassifier(n_estimators=10)\n    clf.fit(X, y)\n\n    assert hasattr(clf, \"estimators_\")\n    assert len(clf.estimators_) == 10\n    assert hasattr(clf, \"estimator_weights_\")\n    assert len(clf.estimator_weights_) == 10\n    assert hasattr(clf, \"estimator_errors_\")\n    assert len(clf.estimator_errors_) == 10\n    assert hasattr(clf, \"feature_importances_\")\n    assert clf.feature_importances_.shape == (X.shape[1],)\n", "def test_adaboost_estimator_params():\n    # Check that estimator parameters are passed correctly.\n    X, y = iris.data, iris.target\n    clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1))\n    clf.fit(X, y)\n\n    assert clf.estimators_[0].max_depth == 1\n\n    clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2))\n    clf.fit(X, y)\n\n    assert clf.estimators_[0].max_depth == 2\n"], "sample_893": ["def test_export_text_max_depth():\n    clf = DecisionTreeClassifier(max_depth=5, random_state=0)\n    clf.fit(X, y)\n\n    # testing that max_depth parameter works as expected\n    export_text(clf, max_depth=3)\n    export_text(clf, max_depth=1)\n\n    # testing that max_depth parameter raises an error when negative\n    err_msg = \"'max_depth' should be greater or equal to 0.\"\n    with pytest.raises(ValueError, match=err_msg):\n        export_text(clf, max_depth=-1)\n", "def test_plot_tree_friedman_mse(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for criterion = friedman_mse\n    clf = DecisionTreeRegressor(\n        max_depth=3, min_samples_split=2, criterion=\"friedman_mse\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\nfriedman_mse = 1.0\\nsamples = 6\\nvalue = 0.0\"\n    )\n    assert nodes[1].get_text() == \"friedman_mse = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"friedman_mse = 0.0\\nsamples = 3\\nvalue = 1.0\"\n", "def test_plot_tree_ax_parameter(pyplot):\n    # Test that the ax parameter is correctly handled\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    fig, ax = pyplot.subplots()\n    nodes = plot_tree(clf, ax=ax)\n    assert len(nodes) == 3\n    assert ax.get_title() == \"\"\n\n    # Test that an error is raised if ax is not a matplotlib Axes instance\n    with pytest.raises(TypeError):\n        plot_tree(clf, ax=\"not an Axes instance\")\n", "def test_plot_tree_label_truncation(pyplot):\n    # Test if labels are properly truncated when they are too long\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = [\"a very long feature name\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=1)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"a very long featu\u2026 <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n", "def test_plot_tree_filled(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for filled option\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names, filled=True)\n    assert len(nodes) == 3\n\n    # check that boxes are filled and colors are correct\n    assert nodes[0].get_bbox_patch().get_facecolor() == (1.0, 1.0, 1.0, 1.0)\n    assert nodes[1].get_bbox_patch().get_facecolor() == pytest.approx((0.9569, 0.5098, 0.2196, 1.0))\n    assert nodes[2].get_bbox_patch().get_facecolor() == pytest.approx((0.2392, 0.9856, 0.9102, 1.0))\n"], "sample_894": ["def test_max_features_as_integer():\n    # Test that an integer value for `max_features` is handled correctly.\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([0, 1])\n\n    clf = RandomForestClassifier(max_features=2, random_state=0)\n    clf.fit(X, y)\n\n    assert clf.n_features_in_ == 3\n    assert clf.max_features_ == 2\n", "def test_n_features_in_attribute(ForestClass):\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n\n    # Test that n_features_in_ attribute is set after fitting the model\n    model = ForestClass()\n    assert not hasattr(model, \"n_features_in_\")\n\n    model.fit(X, y)\n    assert hasattr(model, \"n_features_in_\")\n    assert model.n_features_in_ == X.shape[1]\n\n    # Check that n_features_in_ attribute is also set when using fit_predict,\n    # predict_proba, and decision_path methods\n    model = ForestClass()\n    assert not hasattr(model, \"n_features_in_\")\n\n    model.fit_predict(X, y)\n    assert hasattr(model, \"n_features_in_\")\n    assert model.n_features_in_ == X.shape[1]\n\n    if ForestClass == RandomForestClassifier:\n        model = ForestClass()\n        assert not hasattr(model, \"n_features_in_\")\n\n        model.predict_proba(X)\n        assert not hasattr(model, \"n_features_in_\")\n\n        model.fit(X, y)\n        model.predict_proba(X)\n        assert hasattr(model, \"n_features_in_\")\n        assert model.n_features_in_ == X.shape[1]\n\n        model = ForestClass()\n        assert not hasattr(model, \"n_features_in_\")\n\n        model.decision_path(X)\n        assert not hasattr(model, \"n_features_in_\")\n\n        model.fit(X, y)\n        model.decision_path(X)\n        assert hasattr(model, \"n_features_in_\")\n        assert model.n_features_in_ == X.shape[1]\n", "def test_n_features_in_attribute():\n    \"\"\"Check that the `n_features_in_` attribute is correctly set.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10)\n    est = RandomForestClassifier()\n    est.fit(X, y)\n    assert hasattr(est, \"n_features_in_\")\n    assert est.n_features_in_ == 10\n\n    # Check that it's still correct after a second fit\n    est.fit(X[:, :5], y)\n    assert est.n_features_in_ == 5\n", "def test_parallel_fit_with_threads():\n    # Test that parallel fit with threads works correctly.\n    X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n\n    forest = RandomForestClassifier(n_estimators=10, n_jobs=4, random_state=42)\n    forest.fit(X, y)\n\n    predictions = forest.predict(X)\n    assert_array_equal(predictions.shape, (X.shape[0],))\n", "def test_random_forest_attribute_estimator():\n    \"\"\"Check that the estimator attribute of a RandomForestClassifier is set.\"\"\"\n    X, y = make_classification(n_samples=10, n_features=4, random_state=0)\n    clf = RandomForestClassifier(n_estimators=5, random_state=0)\n    clf.fit(X, y)\n\n    assert clf.estimator_.__class__.__name__ == \"DecisionTreeClassifier\"\n"], "sample_895": ["def test_column_transformer_set_output_multiple_times():\n    \"\"\"Check that setting output multiple times does not raise an error.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [True, False, True], \"b\": [1, 2, 3]})\n\n    ct = make_column_transformer(\n        (VarianceThreshold(), make_column_selector(dtype_include=bool)),\n        remainder=VarianceThreshold(),\n        verbose_feature_names_out=False,\n    )\n    ct.set_output(transform=\"pandas\")\n    ct.set_output(transform=\"default\")\n    ct.set_output(transform=\"pandas\")\n\n    out = ct.fit_transform(df)\n    assert isinstance(out, pd.DataFrame)\n", "def test_column_transformer_feature_names_out_with_nested_pipeline():\n    \"\"\"Check that get_feature_names_out works with nested pipelines.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4], \"C\": [5, 6]})\n\n    pipeline = make_pipeline(StandardScaler(), OneHotEncoder())\n    ct = ColumnTransformer([(\"pipe\", pipeline, [\"A\", \"B\"]), (\"pass\", \"passthrough\", [\"C\"])])\n\n    ct.fit(df)\n    feature_names = ct.get_feature_names_out()\n    assert_array_equal(feature_names, [\"pipe__x0_1.0\", \"pipe__x0_2.0\", \"pipe__x1_3.0\", \"pipe__x1_4.0\", \"pass__C\"])\n", "def test_column_transformer_get_feature_names_out_with_transformer_weights():\n    \"\"\"Check feature names with transformer weights.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(\n        {\n            \"col_int\": np.array([0, 1, 2], dtype=int),\n            \"col_float\": np.array([0.0, 1.0, 2.0], dtype=float),\n            \"col_str\": [\"one\", \"two\", \"three\"],\n        },\n        columns=[\"col_int\", \"col_float\", \"col_str\"],\n    )\n\n    ct = ColumnTransformer(\n        [\n            (\"num\", StandardScaler(), [\"col_int\", \"col_float\"]),\n            (\"cat\", OneHotEncoder(), [\"col_str\"]),\n        ],\n        transformer_weights={\"num\": 0.5, \"cat\": 2},\n    )\n    ct.fit(X_df)\n    assert_array_equal(\n        ct.get_feature_names_out(),\n        [\n            \"num__col_int\",\n            \"num__col_float\",\n            \"cat__col_str_one\",\n            \"cat__col_str_three\",\n            \"cat__col_str_two\",\n        ],\n    )\n", "def test_column_transformer_with_feature_names():\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=False, dtype=\"int16\"),\n                [\"pet\"],\n            ),\n            (\"age\", StandardScaler(), [\"age\"]),\n        ],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    )\n\n    X_trans_df = ct.fit_transform(X_df)\n    expected_dtypes = {\n        \"pet_cat\": \"int16\",\n        \"pet_dog\": \"int16\",\n        \"pet_snake\": \"int16\",\n        \"height\": \"int64\",\n        \"age\": \"float64\",\n    }\n    for col, dtype in X_trans_df.dtypes.items():\n        assert dtype == expected_dtypes[col]\n\n    # Manually specify the input_features to get_feature_names_out.\n    feature_names_out = ct.get_feature_names_out(input_features=[\"a\", \"b\", \"c\"])\n    assert_array_equal(feature_names_out, [\"pet_cat\", \"pet_dog\", \"pet_snake\", \"b\", \"a\"])\n\n    # Use get_feature_names_out without manually specifying input_features.\n    # Since feature_names_in_ is defined, it will be used as the input_features.\n    assert hasattr(ct, \"feature_names_in_\")\n    feature_names_out_default = ct.get_feature_names_out()\n    assert_array_equal(\n        feature_names_out_default, [\"pet_cat\", \"pet_dog\", \"pet_snake\", \"height\", \"age\"]\n    )\n", "def test_column_transformer_feature_names_out_non_fitted_transformer():\n    \"\"\"Check feature names out for non-fitted transformers.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        [\n            (\"bycol1\", TransWithNames(), [\"b\"]),\n            (\"bycol2\", \"passthrough\", [\"b\"]),\n        ],\n        remainder=\"drop\",\n        verbose_feature_names_out=False,\n    )\n\n    msg = re.escape(\n        \"The transformer 'bycol2' does not provide get_feature_names_out. \"\n        \"Either a transformer is not fitted yet (only when using feature \"\n        \"names) or the transformer itself does not provide \"\n        \"get_feature_names_out.\"\n    )\n    with pytest.raises(AttributeError, match=msg):\n        ct.get_feature_names_out()\n"], "sample_896": ["def test_nmf_w_h_warning_update_H_false():\n    # Check that warnings are raised if user provided W is not used when update_H=False\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    H_init = rng.random_sample((2, 5))\n    W_init = rng.random_sample((6, 2))\n\n    with pytest.warns(\n        RuntimeWarning, match=\"When update_H=False, the provided initial W is not used.\"\n    ):\n        non_negative_factorization(X, H=H_init, W=W_init, update_H=False)\n\n    with pytest.warns(\n        RuntimeWarning, match=\"When update_H=False, the provided initial W is not used.\"\n    ):\n        NMF().fit_transform(X, H=H_init, W=W_init, update_H=False)\n", "def test_nmf_alpha_H_same():\n    # Check that alpha_H='same' sets alpha_H=alpha_W\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    nmf = NMF(alpha_W=0.5, alpha_H=\"same\")\n    assert nmf.alpha_H == nmf.alpha_W\n", "def test_nmf_get_n_components():\n    # Check that the number of components is correctly inferred from the data.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    nmf = NMF(n_components=\"auto\", init=\"random\", random_state=0)\n    assert nmf._get_n_components(X) == X.shape[1]\n", "def test_nmf_minibatchnmf_equivalence_with_regularization():\n    # Test that MiniBatchNMF is equivalent to NMF when batch_size = n_samples and\n    # forget_factor 0.0 (stopping criterion put aside) with regularization\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(48, 5))\n\n    nmf = NMF(\n        n_components=5,\n        beta_loss=2,\n        solver=\"mu\",\n        random_state=0,\n        tol=0,\n        alpha_W=0.1,\n        alpha_H=0.1,\n        l1_ratio=0.5,\n    )\n    mbnmf = MiniBatchNMF(\n        n_components=5,\n        beta_loss=2,\n        random_state=0,\n        tol=0,\n        max_no_improvement=None,\n        batch_size=X.shape[0],\n        forget_factor=0.0,\n        alpha_W=0.1,\n        alpha_H=0.1,\n        l1_ratio=0.5,\n    )\n    W = nmf.fit_transform(X)\n    mbW = mbnmf.fit_transform(X)\n    assert_allclose(W, mbW)\n", "def test_nmf_w_h_init_type_error():\n    # Check that an informative error is raised when custom initialization has the\n    # wrong type\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    H = rng.random_sample((2, 5))\n    nmf = NMF(n_components=2, init=\"custom\", random_state=0)\n\n    with pytest.raises(TypeError, match=\"W should be of type array-like\"):\n        nmf.fit(X, H=H, W=\"not_array_like\")\n\n    with pytest.raises(TypeError, match=\"H should be of type array-like\"):\n        nmf.fit(X, H=\"not_array_like\", W=rng.random_sample((6, 2)))\n"], "sample_897": ["def test_partial_dependence_display_feature_names_none(pyplot, clf_diabetes, diabetes):\n    \"\"\"Check that feature names are generated when not provided.\"\"\"\n    disp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 2],\n        grid_resolution=20,\n    )\n\n    assert disp.feature_names == [f\"feature {i}\" for i in range(diabetes.data.shape[1])]\n", "def test_partial_dependence_display_with_sample_weight(pyplot, clf_diabetes, diabetes):\n    \"\"\"Check that the utilization of sample weights has an effect on the plot.\"\"\"\n    # define a custom sample weight such that the first half of the data has more\n    # importance than the second half.\n    sample_weight = np.ones_like(diabetes.target)\n    sample_weight[: len(sample_weight) // 2] *= 2\n\n    disp_sw = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 1],\n        sample_weight=sample_weight,\n        kind=\"average\",\n        method=\"brute\",\n    )\n\n    disp_no_sw = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 1],\n        kind=\"average\",\n        method=\"brute\",\n    )\n\n    # make sure that using a sample weight has an effect on the partial dependence\n    assert not np.array_equal(\n        disp_sw.pd_results[0][\"average\"], disp_no_sw.pd_results[0][\"average\"]\n    )\n", "def test_partial_dependence_display_with_non_constant_sample_weight(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_categorical_feature_with_single_category(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_with_sample_weight(pyplot, clf_diabetes, diabetes):\n    \"\"\"Check that the utilization of sample weights will lead to a different\n    plot.\n    \"\"\"\n    disp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 1],\n        kind=\"average\",\n        method=\"brute\",\n    )\n\n    sample_weight = np.random.rand(len(diabetes.target))\n    disp_sw = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 1],\n        sample_weight=sample_weight,\n        kind=\"average\",\n        method=\"brute\",\n    )\n\n    assert not np.array_equal(\n        disp.pd_results[0][\"average\"], disp_sw.pd_results[0][\"average\"]\n    )\n"], "sample_898": ["def test_auc_score_at_all_thresholds():\n    # Test that auc is the same for all threshold values of a perfect\n    # predictor.\n    y_true = [0, 0, 0, 1, 1, 1]\n    y_score = [0., 0.5, 0.9, 1., 1.1, 1.2]\n\n    auc_full = roc_auc_score(y_true, y_score)\n    auc_values = []\n    for t in y_score:\n        y_pred = (y_score >= t).astype(int)\n        auc_values.append(roc_auc_score(y_true, y_pred))\n    assert_array_almost_equal(auc_values, [auc_full]*len(y_score))\n", "def test_multiclass_input_average_precision_score():\n    # Test multiclass input to average precision score function\n\n    # Generate some data\n    y_true = np.array([0, 1, 2, 3, 4])\n    y_score = np.array([[0.9, 0.1, 0, 0, 0],\n                        [0, 0.8, 0.2, 0, 0],\n                        [0, 0, 0.7, 0.3, 0],\n                        [0, 0, 0, 0.6, 0.4],\n                        [0, 0, 0, 0, 1]])\n\n    # Check that the function runs without errors\n    average_precision_score(y_true, y_score, average='macro')\n    average_precision_score(y_true, y_score, average='weighted')\n\n    # Check that the function returns the correct shape\n    assert_equal(average_precision_score(y_true, y_score,\n                                         average=None).shape[0],\n                 np.unique(y_true).shape[0])\n\n    # Check for invalid average argument\n    assert_raises(ValueError, average_precision_score,\n                  y_true, y_score, average='invalid')\n", "def test_thresholded_metrics_with_invalid_inputs():\n    y_true = np.array([0, 1, 1, 0])\n    y_score = np.array([0.5, 0.6, 0.7, 0.8])\n\n    for name in THRESHOLDED_METRICS:\n        metric = ALL_METRICS[name]\n\n        # Test with NaN and inf values\n        y_score_invalid = np.array([np.nan, np.inf, 0.7, 0.8])\n        assert_raise_message(ValueError,\n                             \"contains NaN, infinity\",\n                             metric, y_true, y_score_invalid)\n\n        # Test with non-array inputs\n        assert_raise_message(TypeError,\n                             \"Expected sequence or array-like\",\n                             metric, y_true, \"invalid\")\n\n        # Test with different lengths\n        y_score_invalid = np.array([0.5, 0.6, 0.7])\n        assert_raise_message(ValueError,\n                             \"Found input variables with inconsistent numbers of samples\",\n                             metric, y_true, y_score_invalid)\n", "def test_multiclass_input_validation():\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([0, 2, 3])\n\n    for name in METRICS_WITH_AVERAGING:\n        metric = ALL_METRICS[name]\n\n        # Test that labels are checked for consistency with y_true and y_pred\n        assert_raises(ValueError, metric, y_true, y_pred, labels=[4, 5, 6])\n\n        # Test that an error is raised when the labels argument contains \n        # classes not present in y_true or y_pred\n        assert_raises(ValueError, metric, y_true, y_pred, labels=[0, 1, 4])\n\n        # Test that labels=None is handled correctly\n        try:\n            metric(y_true, y_pred, labels=None)\n        except ValueError as e:\n            if \"labels\" in str(e):\n                raise AssertionError(\"Metric raised an exception when \"\n                                     \"labels=None\")\n", "def test_auc_score_non_binary_class_labels():\n    # Test that roc_auc_score function raises an error when given\n    # non-binary class labels.\n    y_true = np.array([0, 0, 1, 2])\n    y_pred = np.array([0.1, 0.4, 0.35, 0.8])\n\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred)\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred, multi_class='ovr')\n    assert_raises(ValueError, roc_auc_score, y_true, y_pred, multi_class='ovo')\n"], "sample_899": ["def test_check_estimator_transformer_returns_float64():\n    # check that transformers return float64 arrays\n    X = np.random.RandomState(0).randn(10, 5)\n    estimator = SparseTransformer()\n    estimator.fit(X)\n    X_trans = estimator.transform(X)\n    assert X_trans.dtype == np.float64\n", "def test_check_estimator_no_base_estimator():\n    # check that estimator without BaseEstimator as base class raises an error\n\n    class NonConformantEstimator:\n            return self\n\n            return np.ones(X.shape[0])\n\n    msg = \"estimator should be an instance of BaseEstimator\"\n    assert_raises_regex(TypeError, msg, check_estimator, NonConformantEstimator())\n", "def test_check_estimator_check_fit_idempotent():\n    # check that estimators do not change the state when calling fit multiple times\n\n    class EstimatorWithFitIdempotent(BaseEstimator):\n            self.n_features = n_features\n\n            self.coef_ = np.random.rand(self.n_features)\n            return self\n\n    class EstimatorWithoutFitIdempotent(BaseEstimator):\n            self.n_features = n_features\n            self.coef_ = None\n\n            if self.coef_ is not None:\n                self.coef_ += np.random.rand(self.n_features)\n            else:\n                self.coef_ = np.random.rand(self.n_features)\n            return self\n\n    check_estimator(EstimatorWithFitIdempotent())\n\n    msg = \"Estimator's fit method is not idempotent\"\n    assert_raises_regex(AssertionError, msg,\n                        check_estimator, EstimatorWithoutFitIdempotent())\n", "def test_check_estimators_empty_data_messages():\n    # check that estimators raise a ValueError on empty data and\n    # informative error messages\n\n    class Raiser(BaseEstimator):\n            if len(X) == 0:\n                raise ValueError(\"Empty data\")\n            return self\n\n    check_estimator(Raiser())\n\n    class NoRaiser(BaseEstimator):\n            return self\n\n    msg = (\"The estimator {} does not raise an error when an empty data is \"\n           \"used to train. Perhaps use check_array in train.\".format(\n               NoRaiser.__name__))\n    assert_raises_regex(AssertionError, msg,\n                        check_estimator, NoRaiser())\n", "def test_check_estimator_warns_message():\n    # check that the proper warning is raised when the estimator doesn't\n    # have a parameter in get_params\n    class NonConformantEstimator(object):\n            self.param = param\n\n            return {}\n\n    msg = (\"Estimator's parameters changed after set_params but params \"\n           \"are not set\")\n    with assert_warns_message(UserWarning, msg):\n        check_estimator(NonConformantEstimator())\n"], "sample_900": ["def test_mlp_classifier_output_dtype():\n    # Test that the output of predict is correct dtype.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    clf = MLPClassifier(hidden_layer_sizes=5, random_state=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n\n    y_pred = clf.predict(X)\n    assert y_pred.dtype.kind == y.dtype.kind\n\n    y_pred_proba = clf.predict_proba(X)\n    assert y_pred_proba.dtype == np.float64\n\n    y_pred_log_proba = clf.predict_log_proba(X)\n    assert y_pred_log_proba.dtype == np.float64\n", "def test_early_stopping_split():\n    # Test that early stopping uses a validation set with the correct size.\n    X = np.random.rand(100, 10)\n    y = np.random.randint(2, size=100)\n\n    mlp = MLPClassifier(validation_fraction=0.2, early_stopping=True,\n                        max_iter=1)\n    mlp.fit(X, y)\n\n    # check that the validation set is 20% of the training set\n    assert mlp.validation_scores_.shape[0] == 1\n    assert mlp.best_validation_score_ > 0\n    assert mlp.X_val_.shape[0] == int(0.2 * X.shape[0])\n    assert mlp.y_val_.shape[0] == int(0.2 * y.shape[0])\n", "def test_sparse_input_with_partial_fit():\n    # Test that sparse input works with partial_fit.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n    X_sparse = csr_matrix(X)\n    mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=15, random_state=1)\n\n    for _ in range(5):\n        mlp.partial_fit(X_sparse, y, classes=np.unique(y))\n\n    assert_array_equal(mlp.predict(X), mlp.predict(X_sparse))\n", "def test_early_stopping_split():\n    # Make sure that early stopping split is stratified and does not use the\n    # validation set for training.\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n    y = np.array([0, 0, 0, 1, 1, 1])\n\n    mlp = MLPClassifier(early_stopping=True, validation_fraction=1/3,\n                        max_iter=1)\n    mlp.fit(X, y)\n\n    assert mlp.validation_scores_.shape == (1,)\n    assert mlp.best_validation_score_ is not None\n    assert mlp.X_val_.shape == (2, 2)\n    assert mlp.y_val_.shape == (2,)\n    assert_array_equal(mlp.classes_, np.unique(y))\n", "def test_mlpclassifier_partial_fit_incremental():\n    # Test that MLPClassifier.partial_fit() works as expected with incremental\n    # fitting (i.e., when X and y are provided in batches).\n    n_samples = 100\n    n_features = 10\n    n_batches = 5\n    batch_size = n_samples // n_batches\n\n    # Generate some random data\n    rng = np.random.RandomState(0)\n    X = rng.rand(n_samples, n_features)\n    y = rng.randint(0, 2, size=n_samples)\n\n    # Create an instance of MLPClassifier\n    clf = MLPClassifier(solver='sgd', max_iter=100, random_state=1,\n                        learning_rate_init=0.01, batch_size=batch_size)\n\n    # Fit the classifier incrementally using partial_fit()\n    for i in range(n_batches):\n        start = i * batch_size\n        end = (i + 1) * batch_size\n        clf.partial_fit(X[start:end], y[start:end], classes=[0, 1])\n\n    # Check that the classifier has been fitted correctly\n    assert clf.score(X, y) > 0.7\n"], "sample_901": ["def test_kmeans_predict_with_single_sample():\n    # Check if predict method works with single sample input\n    km = KMeans(n_clusters=3, random_state=0)\n    km.fit(X)\n    single_sample = X[0].reshape(1, -1)\n    assert_array_equal(km.predict(single_sample).shape, (1,))\n", "def test_k_means_init_centers_dtype():\n    # This test is used to check KMeans won't mutate the user provided input\n    # array silently even if input data and init centers have the same type\n    X_small = np.array([[1.1, 1.1], [-7.5, -7.5], [-1.1, -1.1], [7.5, 7.5]])\n    init_centers = np.array([[0.0, 0.0], [5.0, 5.0], [-5.0, -5.0]], dtype=np.float32)\n\n    km = KMeans(init=init_centers, n_clusters=3, n_init=1)\n    km.fit(X_small.astype(np.float64))\n    assert np.may_share_memory(km.cluster_centers_, init_centers) is False\n    assert km.cluster_centers_.dtype == X_small.dtype\n", "def test_k_means_init_fitted_centers_sparse():\n    # Get a local optimum\n    centers = KMeans(n_clusters=3).fit(X_csr).cluster_centers_\n\n    # Fit starting from a local optimum shouldn't change the solution\n    new_centers = KMeans(n_clusters=3, init=centers,\n                         n_init=1).fit(X_csr).cluster_centers_\n    assert_array_almost_equal(centers, new_centers)\n", "def test_k_means_n_clusters_attribute():\n    # Test that the n_clusters attribute is set correctly\n    km = KMeans(n_clusters=5)\n    assert km.n_clusters == 5\n    km.fit(X)\n    assert km.n_clusters == 5\n", "def test_k_means_init_random_state():\n    # Check that the initialization is deterministic when random_state is given\n    X = np.random.rand(10, 3)\n    km1 = KMeans(n_clusters=3, init=\"random\", random_state=42, n_init=1)\n    km2 = KMeans(n_clusters=3, init=\"random\", random_state=42, n_init=1)\n    km1.fit(X)\n    km2.fit(X)\n    assert_array_equal(km1.cluster_centers_, km2.cluster_centers_)\n"], "sample_902": ["def test_pipeline_get_params():\n    # Test that get_params returns all parameters of the estimators\n    mult2 = Mult(2)\n    mult3 = Mult(3)\n    pipeline = Pipeline([('m1', mult2), ('m2', mult3)])\n\n    params = pipeline.get_params(deep=True)\n    assert 'm1__mult' in params\n    assert 'm2__mult' in params\n    assert params['m1__mult'] == 2\n    assert params['m2__mult'] == 3\n\n    # Check that parameter values are updated correctly\n    pipeline.set_params(m1__mult=10, m2__mult=20)\n    params = pipeline.get_params(deep=True)\n    assert params['m1__mult'] == 10\n    assert params['m2__mult'] == 20\n", "def test_pipeline_with_none_estimator():\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', None)])\n    assert_raise_message(TypeError,\n                         \"Last step of Pipeline should implement fit. \"\n                         \"'NoneType' (type <class 'NoneType'>) doesn't\",\n                         pipe.fit, X)\n", "def test_pipeline_transformer_get_params():\n    # Test that get_params() on a pipeline with a transformer returns the\n    # correct parameters for that transformer.\n    mult = Mult(mult=5)\n    pipeline = make_pipeline(mult)\n    params = pipeline.get_params(deep=True)\n    assert 'mult__mult' in params\n    assert params['mult__mult'] == 5\n", "def test_pipeline_memory_temporary_directory():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    memory = Memory(cachedir=cachedir, verbose=10)\n    pipeline = Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                        memory=memory)\n    try:\n        pipeline.fit(X, y)\n    finally:\n        shutil.rmtree(cachedir)\n\n    # Check that the cache directory is removed after deletion\n    assert_false(os.path.exists(cachedir))\n", "def test_pipeline_get_params():\n    # Test that get_params returns all the parameters of a pipeline\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    clf = SVC(probability=True, random_state=0)\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', transf), ('svc', clf)])\n\n    params = pipe.get_params(deep=True)\n    assert 'transf__means_' not in params\n    assert 'svc__probability' in params\n    assert 'svc__random_state' in params\n\n    pipe.fit(X, y)\n\n    params = pipe.get_params(deep=True)\n    assert 'transf__means_' in params\n    assert 'svc__probability' in params\n    assert 'svc__random_state' in params\n"], "sample_903": ["def test_tsne_with_n_iter_without_progress_less_than_one():\n    \"\"\"Make sure that TSNE raises an error when n_iter_without_progress is less than one\"\"\"\n    random_state = check_random_state(0)\n    X = random_state.randn(50, 2).astype(np.float32)\n    tsne = TSNE(n_components=2, n_iter_without_progress=0)\n    assert_raises_regexp(ValueError, \"n_iter_without_progress must be a positive integer\", \n                         tsne.fit_transform, X)\n", "def test_tsne_with_n_components_1():\n    \"\"\"Make sure that TSNE works for n_components=1\"\"\"\n    random_state = check_random_state(0)\n    n_components_original = 3\n    n_components_embedding = 1\n    X = random_state.randn(50, n_components_original).astype(np.float32)\n    tsne = TSNE(n_components=n_components_embedding, random_state=0)\n    X_transformed_tsne = tsne.fit_transform(X)\n    assert X_transformed_tsne.shape == (X.shape[0], n_components_embedding)\n", "def test_tsne_with_custom_distance_metric():\n    \"\"\"Make sure that TSNE works for custom distance metrics\"\"\"\n    random_state = check_random_state(0)\n    n_components_original = 3\n    n_components_embedding = 2\n    X = random_state.randn(50, n_components_original).astype(np.float32)\n\n    # Define a custom distance metric\n        return np.sum(np.abs(x - y), axis=1)\n\n    # Create a TSNE object with the custom distance metric\n    tsne_custom = TSNE(metric=custom_distance, n_components=n_components_embedding,\n                       random_state=0)\n\n    # Fit and transform the data using the custom distance metric\n    X_transformed_custom = tsne_custom.fit_transform(X)\n\n    # Compare the result with the precomputed distance matrix\n    dist_custom = pairwise_distances(X, metric=custom_distance)\n    tsne_precomputed = TSNE(metric='precomputed', n_components=n_components_embedding,\n                            random_state=0)\n    X_transformed_precomputed = tsne_precomputed.fit_transform(dist_custom)\n\n    assert_array_equal(X_transformed_custom, X_transformed_precomputed)\n", "def test_tsne_with_sparse_input_and_exact_method():\n    \"\"\"Make sure that TSNE works with sparse input and exact method\"\"\"\n    random_state = check_random_state(0)\n    n_components_original = 3\n    n_components_embedding = 2\n    X = random_state.randn(50, n_components_original).astype(np.float32)\n    X_sparse = sp.csr_matrix(X)\n    tsne = TSNE(n_components=n_components_embedding, method='exact',\n                random_state=0)\n    X_transformed_tsne_sparse = tsne.fit_transform(X_sparse)\n    X_transformed_tsne_dense = tsne.fit_transform(X)\n    assert_array_almost_equal(X_transformed_tsne_sparse, X_transformed_tsne_dense,\n                              decimal=3)\n", "def test_tsne_with_sparse_input():\n    \"\"\"Make sure that TSNE works with sparse input\"\"\"\n    random_state = check_random_state(0)\n    n_components_original = 3\n    n_components_embedding = 2\n    X = random_state.randn(50, n_components_original).astype(np.float32)\n    X_sparse = sp.csr_matrix(X)\n\n    X_transformed_tsne_dense = TSNE(\n        n_components=n_components_embedding, random_state=0,\n        method='exact').fit_transform(X)\n    X_transformed_tsne_sparse = TSNE(\n        n_components=n_components_embedding, random_state=0,\n        method='exact').fit_transform(X_sparse)\n\n    assert_array_almost_equal(X_transformed_tsne_dense, X_transformed_tsne_sparse)\n"], "sample_904": ["def test_build_reference_node_with_title():\n    env = mock.Mock()\n    builder = mock.Mock()\n    builder.get_relative_uri.return_value = 'relative-uri'\n    fromdocname = 'from-doc-name'\n    docname = 'doc-name'\n    labelid = 'label-id'\n    sectname = 'Section Name'\n    title = 'Reference Title'\n\n    node = StandardDomain(env).build_reference_node(fromdocname, builder,\n                                                     docname, labelid,\n                                                     sectname, 'ref',\n                                                     title=title)\n\n    assert_node(node, nodes.reference, internal=True, title=title)\n    assert_node(node[0], nodes.inline, sectname)\n    assert node['refuri'] == 'relative-uri#label-id'\n", "def test_numfig_disabled(app):\n    app.config.numfig = False\n\n    text = (\".. figure:: foo.jpg\\n\"\n            \"   :name: foo\\n\"\n            \"\\n\"\n            \":numref:`foo`\\n\")\n\n    doctree = restructuredtext.parse(app, text)\n    fig_ref = doctree[1]\n    assert isinstance(fig_ref, nodes.reference)\n    assert fig_ref.astext() == ''\n", "def test_resolve_xref_numref_with_title(app):\n    text = (\".. _foo:\\n\"\n            \"\\n\"\n            \"Title\\n\"\n            \"======\\n\")\n    restructuredtext.parse(app, text)\n\n    domain = app.env.get_domain(\"std\")\n    refnode = domain.resolve_xref(app.env, 'index', app.builder, 'numref', 'foo',\n                                  pending_xref(), nodes.paragraph())\n    assert_node(refnode, nodes.reference, title=\"Title\")\n", "def test_is_enumerable_node():\n    env = mock.Mock(domaindata={})\n    env.app.registry.enumerable_nodes = {}\n    domain = StandardDomain(env)\n\n    # Test default enumerable nodes\n    for node in [nodes.figure, nodes.table, nodes.container]:\n        assert domain.is_enumerable_node(node())\n\n    # Test custom enumerable nodes\n    class CustomNode(nodes.Element):\n        pass\n\n    env.app.registry.enumerable_nodes = {CustomNode: ('custom', None)}\n    assert domain.is_enumerable_node(CustomNode())\n", "def test_get_fignumber(env, app):\n    env.toc_secnumbers = {'docname': {'': (1, 2, 3)}}\n    env.toc_fignumbers = {'docname': {'figure': {'id1': (4, 5, 6)}}}\n    domain = StandardDomain(env)\n\n    node = nodes.figure()\n    node['ids'] = ['id1']\n\n    result = domain.get_fignumber(env, app.builder, 'section', 'docname', node)\n    assert result == (1, 2, 3)\n\n    result = domain.get_fignumber(env, app.builder, 'figure', 'docname', node)\n    assert result == (4, 5, 6)\n\n    with pytest.raises(ValueError):\n        domain.get_fignumber(env, app.builder, 'table', 'docname', node)\n"], "sample_905": ["def test_getorigbases():\n    class Foo:\n        pass\n\n    class Bar(Foo):\n        __orig_bases__ = (Foo,)\n\n    assert inspect.getorigbases(Bar) == (Foo,)\n    assert inspect.getorigbases(Foo) is None\n    assert inspect.getorigbases(object()) is None\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(normal_func) is False\n", "def test_getorigbases():\n    class Base:\n        pass\n\n    class Child(Base):\n        pass\n\n    assert inspect.getorigbases(Child) == (Base,)\n    assert inspect.getorigbases(Base) is None\n    assert inspect.getorigbases(object) is None\n    assert inspect.getorigbases(int) is None\n\n    try:\n        inspect.getorigbases(\"not a class\")\n    except TypeError:\n        pass\n    else:\n        pytest.fail(\"TypeError not raised\")\n\n    try:\n        inspect.getorigbases(123)\n    except TypeError:\n        pass\n    else:\n        pytest.fail(\"TypeError not raised\")\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(normal_func) is False\n\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(normal_function) is False\n"], "sample_908": ["def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse_arguments(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse_arguments(module.body[0].args) == expected\n"], "sample_909": ["def test_usage_section(self):\n    docstring = \"\"\"", "def test_custom_generic_sections_with_admonition(self):\n\n    docstrings = ((\"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_custom_generic_sections_with_multiple_lines(self):\n    docstring = \"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"\\"], "sample_910": ["def test_get_node_location():\n    node = nodes.Node()\n    node.source, node.line = ('index.txt', 10)\n    assert logging.get_node_location(node) == 'index.txt:10'\n\n    node.source, node.line = ('index.txt', None)\n    assert logging.get_node_location(node) == 'index.txt:'\n\n    node.source, node.line = (None, 10)\n    assert logging.get_node_location(node) == '<unknown>:10'\n\n    node.source, node.line = (None, None)\n    assert logging.get_node_location(node) is None\n", "def test_get_node_location():\n    node = nodes.Node()\n    node.source, node.line = ('index.txt', 10)\n    assert logging.get_node_location(node) == 'index.txt:10'\n\n    node.source, node.line = ('index.txt', None)\n    assert logging.get_node_location(node) == 'index.txt:'\n\n    node.source, node.line = (None, 10)\n    assert logging.get_node_location(node) == '<unknown>:10'\n\n    node.source, node.line = (None, None)\n    assert logging.get_node_location(node) is None\n", "def test_SphinxLogRecordTranslator(app, status, warning):\n    translator = SphinxLogRecordTranslator(app)\n    record = logging.LogRecord(__name__, logging.INFO, __file__, 1, 'message', None, None)\n\n    # location is None\n    translated_record = translator.filter(record)\n    assert translated_record.location is None\n\n    # location is tuple (docname, lineno)\n    record.location = ('index', 10)\n    translated_record = translator.filter(record)\n    assert translated_record.location == 'index.txt:10'\n\n    # location is docutils node\n    node = nodes.Node()\n    node.source, node.line = ('index.txt', 10)\n    record.location = node\n    translated_record = translator.filter(record)\n    assert translated_record.location == 'index.txt:10'\n\n    # location is string\n    record.location = 'index'\n    translated_record = translator.filter(record)\n    assert translated_record.location == 'index.txt'\n\n    # location is string with lineno\n    record.location = 'index:10'\n    translated_record = translator.filter(record)\n    assert translated_record.location == 'index.txt:10'\n", "def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    memhandler = logging.MemoryHandler()\n    logger.addHandler(memhandler)\n\n    logger.info('message1')\n    logger.warning('message2')\n\n    assert len(memhandler.buffer) == 2\n\n    memhandler.flushTo(logger)\n\n    assert 'message1' in status.getvalue()\n    assert 'message2' in warning.getvalue()\n\n    memhandler.clear()\n\n    assert len(memhandler.buffer) == 0\n", "def test_log_collector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    log_collector = logging.LogCollector()\n    with log_collector.collect():\n        logger.info('message1')\n        logger.warning('message2')\n\n    assert len(log_collector.logs) == 2\n    assert log_collector.logs[0].getMessage() == 'message1'\n    assert log_collector.logs[1].getMessage() == 'WARNING: message2'\n"], "sample_238": ["def test_aggregation_on_filtered_subquery_annotation(self):\n    \"\"\"Aggregations work on filtered subquery annotations.\"\"\"\n    books_with_multiple_authors = Book.objects.filter(authors__isnull=False).annotate(\n        num_authors=Count('authors')\n    ).filter(num_authors__gt=1)\n    publisher_qs = Publisher.objects.annotate(\n        book_with_multiple_authors=Exists(books_with_multiple_authors.filter(publisher=OuterRef('pk'))),\n    ).values_list('book_with_multiple_authors').annotate(total=Count('*'))\n    self.assertEqual(dict(publisher_qs), {True: 2, False: 3})\n", "def test_aggregation_empty_subquery(self):\n    \"\"\"Empty subqueries in aggregations are correctly handled.\"\"\"\n    no_books_qs = Book.objects.filter(pk__in=[])\n    authors = Author.objects.annotate(has_books=Exists(no_books_qs)).values_list('has_books', flat=True)\n    self.assertQuerysetEqual(authors, [False] * Author.objects.count())\n", "def test_aggregation_subquery_annotation_with_filter(self):\n    \"\"\"Subquery annotations work with filters.\"\"\"\n    publisher = Publisher.objects.create(name='Test Publisher', num_awards=2)\n    book = Book.objects.create(\n        isbn='159059998', name='Test book.', pages=819, rating=2.5,\n        price=Decimal('14.44'), contact=self.a9, publisher=publisher,\n        pubdate=datetime.date(2019, 12, 6),\n    )\n    book.authors.add(self.a5, self.a6, self.a7)\n    books_qs = Book.objects.annotate(\n        contact_publisher=Subquery(\n            Publisher.objects.filter(\n                pk=OuterRef('publisher'),\n                name=OuterRef('contact__name'),\n            ).values('name')[:1],\n        )\n    ).filter(\n        contact_publisher__isnull=False,\n        pages__gt=800,\n    ).annotate(count=Count('authors'))\n    self.assertSequenceEqual(books_qs, [book])\n", "def test_aggregation_subquery_annotation_outerref(self):\n    subquery_qs = Publisher.objects.filter(\n        pk=OuterRef('publisher'),\n    ).values('num_awards')\n    books_qs = Book.objects.annotate(\n        publisher_num_awards=Subquery(subquery_qs),\n    ).annotate(count=Count('authors'))\n    self.assertEqual(books_qs.count(), 6)\n", "def test_aggregation_subquery_annotation_with_values_list(self):\n    \"\"\"Subquery annotations can be used with values_list().\"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).values_list('name', 'latest_book_pubdate')\n    self.assertEqual(list(publisher_qs), [\n        ('Apress', datetime.date(2008, 6, 23)),\n        ('Sams', datetime.date(2008, 3, 3)),\n        ('Prentice Hall', datetime.date(2008, 11, 3)),\n        ('Morgan Kaufmann', datetime.date(1991, 10, 15)),\n        (\"Jonno's House of Books\", None),\n    ])\n"], "sample_912": ["def test_py_module_index_ignores_modules_without_docname(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    app.env.domains['py'].modules['module_without_docname'] = ('', '', '', '', '')\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        True\n    )\n", "def test_pyclass_signature(app):\n    text = \".. py:class:: MyClass(a, b, c)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"MyClass\"],\n                                                    desc_parameterlist)],\n                                  desc_content)]))\n    assert_node(doctree[1][0][1], [desc_parameterlist, ([desc_parameter, \"a\"],\n                                                        [desc_parameter, \"b\"],\n                                                        [desc_parameter, \"c\"])])\n\n    text = \".. py:class:: MyClass(a: int, b: str, c: bool)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"MyClass\"],\n                                                    desc_parameterlist)],\n                                  desc_content)]))\n    assert_node(doctree[1][0][1], [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                                          [desc_sig_punctuation, \":\"],\n                                                                          \" \",\n                                                                          [pending_xref, \"int\"])],\n                                                        [desc_parameter, ([desc_sig_name, \"b\"],\n                                                                          [desc_sig_punctuation, \":\"],\n                                                                          \" \",\n                                                                          [pending_xref, \"str\"])],\n                                                        [desc_parameter, ([desc_sig_name, \"c\"],\n                                                                          [desc_sig_punctuation, \":\"],\n                                                                          \" \",\n                                                                          [pending_xref, \"bool\"])])])\n", "def test_pyattribute_old(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :annotation: = ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \" = ''\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'class.attr', 'attribute')\n", "def test_pyattribute_old_syntax(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :annotation: = ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \" = ''\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'class.attr', 'attribute')\n", "def test_pyfunction_with_abstractmethod_option(app):\n    text = (\".. py:function:: func1\\n\"\n            \"   :abstractmethod:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"abstract \"],\n                                                    [desc_name, \"func1\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert 'func1' in domain.objects\n    assert domain.objects['func1'] == ('index', 'func1', 'function')\n"], "sample_914": ["def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    function_def = module.body[0]\n    assert ast.unparse_arguments(function_def.args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    function_def = module.body[0]\n    assert ast.unparse_arguments(function_def.args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse_arguments(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    function_def = module.body[0]\n    assert ast.unparse_arguments(function_def.args) == expected\n"], "sample_915": ["def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(normal_fun) is False\n", "def test_getdoc_builtin_function():\n    assert inspect.getdoc(len) is not None\n    assert inspect.getdoc(type) is not None\n", "def test_getdoc_partial_function():\n        \"\"\"docstring.\"\"\"\n        pass\n\n    partial_func = functools.partial(func, 1)\n    assert inspect.getdoc(partial_func) == \"docstring.\"\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(not_singledispatch_fun) is False\n\n", "def test_getdoc_partial_function():\n        \"\"\"docstring.\"\"\"\n        pass\n\n    partial_func = functools.partial(func, 1)\n    assert inspect.getdoc(partial_func) == \"docstring.\"\n"], "sample_916": ["def test_symbol_removal():\n    # create a symbol and add some children and siblings\n    root = Symbol(None, None, None, None)\n    child1 = root.add_name(ASTIdentifier(\"Child1\"))\n    child2 = root.add_name(ASTIdentifier(\"Child2\"))\n    sibling1 = Symbol(root, ASTIdentifier(\"Sibling1\"), None, None)\n    sibling2 = Symbol(root, ASTIdentifier(\"Sibling2\"), None, None)\n    child1.siblingAbove = sibling1\n    child1.siblingBelow = sibling2\n    sibling1.siblingBelow = child1\n    sibling2.siblingAbove = child1\n\n    # check that everything is as expected\n    assert len(root._children) == 3\n    assert root._children == [sibling1, child1, sibling2]\n    assert child1.parent == root\n    assert child1.siblingAbove == sibling1\n    assert child1.siblingBelow == sibling2\n    assert sibling1.parent == root\n    assert sibling1.siblingBelow == child1\n    assert sibling2.parent == root\n    assert sibling2.siblingAbove == child1\n\n    # remove the child\n    child1.remove()\n\n    # check that everything is as expected\n    assert len(root._children) == 2\n    assert root._children == [sibling1, sibling2]\n    assert child1.parent is None\n    assert child1.siblingAbove is None\n    assert child1.siblingBelow is None\n    assert sibling1.parent == root\n    assert sibling1.siblingBelow == sibling2\n    assert sibling2.parent == root\n    assert sibling2.siblingAbove == sibling1\n", "def test_symbol_redeclare():\n    root = Symbol(None, None, None, None)\n    parent = root.add_name(ASTNestedName([ASTIdentifier(\"Parent\")], rooted=False))\n    s1 = parent.add_declaration(ASTDeclaration('type', 'type', ASTTypeWithInit(\n        ASTType(ASTDeclSpecs(\"type\", ASTDeclSpecsSimple(\n            None, None, False, False, False, []), None, None),\n                ASTDeclaratorNameParam(declId=ASTNestedName([ASTIdentifier(\"S1\")],\n                                                            rooted=False), arrayOps=[], param=None)),\n        None)), docname=\"TestDoc\")\n    with pytest.raises(_DuplicateSymbolError):\n        parent.add_declaration(ASTDeclaration('type', 'type', ASTTypeWithInit(\n            ASTType(ASTDeclSpecs(\"type\", ASTDeclSpecsSimple(\n                None, None, False, False, False, []), None, None),\n                    ASTDeclaratorNameParam(declId=ASTNestedName([ASTIdentifier(\"S1\")],\n                                                                rooted=False), arrayOps=[], param=None)),\n            None)), docname=\"TestDoc\")\n\n    assert len(parent._children) == 1\n", "def test_macro_definitions():\n    check('macro', 'MACRO(a, b)', {1: \"MACRO\", 2: \"8MACRO\"})\n    check('macro', 'MACRO(a, ...)', {1: \"MACRO\", 2: \"8MACRO\"})\n    check('macro', 'MACRO(...)', {1: \"MACRO\", 2: \"8MACRO\"})\n", "def test_name_mangling():\n    # Check that name mangling is done correctly for C++ symbols.\n    check('function', 'void foo()', {2: '3fooEv'})\n    check('function', 'void foo(int)', {2: '3fooi'})\n    check('function', 'void foo(int, double)', {2: '3fooidE'})\n    check('function', 'void foo(double, int)', {2: '3foodiE'})\n    check('class', 'class Foo', {2: '3Foo'})\n    check('class', 'class Foo<>', {2: 'I0E3Foo'})\n    check('class', 'class Foo<int>', {2: 'IiE3Foo'})\n    check('class', 'class Foo<int, double>', {2: 'IidE3Foo'})\n", "def test_attribute_parsing():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(\"void f [[nodiscard]] ()\", location=None, config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    assert str(ast) == \"void f()\"\n    assert len(ast.declaration.attributes) == 1\n\n    parser = DefinitionParser(\"void f __attribute__((nodiscard)) ()\", location=None, config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    assert str(ast) == \"void f()\"\n    assert len(ast.declaration.attributes) == 1\n\n    parser = DefinitionParser(\"void f id_attr ()\", location=None, config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    assert str(ast) == \"void f()\"\n    assert len(ast.declaration.attributes) == 1\n\n    parser = DefinitionParser(\"void f paren_attr() ()\", location=None, config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    assert str(ast) == \"void f()\"\n    assert len(ast.declaration.attributes) == 1\n\n    parser = DefinitionParser(\"void f paren_attr(abc) ()\", location=None, config=Config())\n    ast = parser.parse_declaration(\"function\", \"function\")\n    assert str(ast) == \"void f()\"\n    assert len(ast.declaration.attributes) == 1\n"], "sample_918": ["def test_get_index_text():\n    domain = PythonDomain(Mock())\n    assert domain.get_index_text('module', ('Class', 'class')) == 'Class (class in module)'\n    assert domain.get_index_text(None, ('Function', 'function')) == 'Function() (built-in function)'\n    assert domain.get_index_text('module', ('Function', 'function')) == 'Function() (in module module)'\n    assert domain.get_index_text('module', ('Variable', 'data')) == 'Variable (in module module)'\n    assert domain.get_index_text(None, ('Variable', 'data')) == 'Variable (built-in variable)'\n", "def test_get_full_qualified_name_for_module():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    node = nodes.reference(reftarget='func', py__module='module1')\n    assert domain.get_full_qualified_name(node) == 'module1.func'\n\n    node = nodes.reference(reftarget='Class', py__module='module1')\n    assert domain.get_full_qualified_name(node) == 'module1.Class'\n", "def test_pytype_to_xref():\n    doctree = type_to_xref(\"int\")\n    assert_node(doctree, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n\n    doctree = type_to_xref(\"None\")\n    assert_node(doctree, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n", "def test_pyattribute_with_type_and_value(app):\n    text = (\".. py:attribute:: attr\\n\"\n            \"   :type: str\\n\"\n            \"   :value: 'default value'\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"str\"])],\n                                                    [desc_annotation, \" = 'default value'\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"attribute\",\n                domain=\"py\", objtype=\"attribute\", noindex=False)\n", "def test_pyexception(app):\n    text = (\".. py:exception:: Exception\\n\"\n            \"   :module: exceptions\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"Exception\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n\n    assert 'exceptions.Exception' in domain.objects\n    assert domain.objects['exceptions.Exception'] == ('index', 'exceptions.Exception', 'exception')\n"], "sample_920": ["def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"\\"], "sample_921": ["def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'e'\n    assert argspec.varkw == 'f'\n    assert argspec.defaults == (1, 2)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwonlydefaults is None\n    assert argspec.annotations == {}\n\n    # Test with a bound method\n    class Foo:\n            pass\n\n    argspec = inspect.getargspec(Foo().meth)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'e'\n    assert argspec.varkw == 'f'\n    assert argspec.defaults == (1, 2)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwonlydefaults is None\n    assert argspec.annotations == {}\n", "def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'e'\n    assert argspec.varkw == 'f'\n    assert argspec.defaults == (1, 2)\n", "def test_getdoc_with_partial():\n        \"\"\"func1 docstring.\"\"\"\n        pass\n\n    func2 = functools.partial(func1, 1)\n    func2.__doc__ = \"func2\"\n\n    assert inspect.getdoc(func2) == \"func2\"\n    assert inspect.getdoc(func2, allow_inherited=True) == \"func1 docstring.\"\n", "def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'e'\n    assert argspec.varkw == 'f'\n    assert argspec.defaults == (1, 2)\n\n    # Test with a bound method\n    class Foo:\n            pass\n\n    foo = Foo()\n    argspec = inspect.getargspec(foo.meth)\n    assert argspec.args == ['arg1']\n    assert argspec.varargs is None\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults is None\n", "def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'e'\n    assert argspec.varkw == 'f'\n    assert argspec.defaults == (1, 2)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwonlydefaults == {}\n    assert argspec.annotations == {}\n"], "sample_922": ["def test_pytypehintedfunction_signature(app):\n    text = \".. py:function:: hello(name: str, age: int) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, \n                 [desc_parameter, ([desc_sig_name, \"name\"],\n                                   [desc_sig_punctuation, \":\"],\n                                   \" \",\n                                   [pending_xref, \"str\"])],\n                 [desc_parameter, ([desc_sig_name, \"age\"],\n                                   [desc_sig_punctuation, \":\"],\n                                   \" \",\n                                   [pending_xref, \"int\"])]])\n", "def test_get_full_qualified_name_for_module():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    node = nodes.reference(reftarget='module1', **{'py:module': 'module1'})\n    assert domain.get_full_qualified_name(node) == 'module1'\n", "def test_pytype_output(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1][2], desc_returns,\n                [pending_xref, \"str\"])\n    assert_node(doctree[1][0][1][2][0],\n                pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n", "def test_pytype_annotation(app):\n    text = (\".. py:function:: hello(name: str) -> str\\n\"\n            \"   :return: Greeting.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [desc_sig_name, pending_xref, \"str\"])])\n\n    # :return: should be converted to :rtype:\n    assert_node(doctree[1][1], desc_content, ('return', 'Greeting.'))\n    assert_node(doctree[1][0][2],\n                [desc_returns, pending_xref, \"str\"])\n", "def test_get_full_qualified_name_with_module_and_class_context():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # with both py:module and py:class context\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n\n    # with same py:module and py:class context, but with sub module\n    kwargs = {'py:module': 'module1.module2', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.module2.Class.func'\n"], "sample_925": ["def test_MockFinder():\n    modname = 'sphinx.unknown'\n    submodule = modname + '.submodule'\n    finder = MockFinder([modname])\n\n    spec = finder.find_spec(modname, None)\n    assert spec is not None\n    assert spec.name == modname\n    assert spec.loader is finder.loader\n\n    spec = finder.find_spec(submodule, None)\n    assert spec is not None\n    assert spec.name == submodule\n    assert spec.loader is finder.loader\n\n    spec = finder.find_spec('other.module', None)\n    assert spec is None\n\n    finder.invalidate_caches()\n    assert modname not in sys.modules\n", "def test_MockObject_repr_with_name():\n    mock = _MockObject()\n    assert repr(mock) == '_MockObject'\n\n    class SubClass(mock.SomeClass):\n        pass\n\n    obj = SubClass()\n    assert repr(obj) == 'SubClass'\n", "def test_MockObject_repr():\n    mock = _MockObject()\n    mock.__display_name__ = 'MockedObject'\n    assert repr(mock) == 'MockedObject'\n\n    subclass = _make_subclass('SubClass', 'module', superclass=_MockObject)\n    obj = subclass()\n    assert repr(obj) == 'module.SubClass'\n", "def test_MockFinder():\n    modname = 'sphinx.unknown'\n    submodule = modname + '.submodule'\n    finder = MockFinder([modname])\n\n    spec = finder.find_spec(modname, None)\n    assert spec is not None\n    assert spec.name == modname\n    assert spec.loader is finder.loader\n\n    spec = finder.find_spec(submodule, None)\n    assert spec is not None\n    assert spec.name == submodule\n    assert spec.loader is finder.loader\n\n    spec = finder.find_spec('other.module', None)\n    assert spec is None\n\n    finder.invalidate_caches()\n    assert modname not in sys.modules\n    assert submodule not in sys.modules\n", "def test_MockObject_mro_entries():\n    mock = _MockObject()\n    subclass = type('SubClass', (mock.SomeClass,), {})\n    assert subclass.__mro__ == [subclass, _MockObject]\n"], "sample_928": ["def test_default_role(app):\n    docname = 'dummy'\n    name = 'strong'\n\n    with default_role(docname, name) as _:\n        assert docutils.roles._roles[''] == roles.role(name, english, 0, Reporter('', 4, 4))[0]\n    assert '' not in docutils.roles._roles\n\n    # test without name\n    with default_role(docname, '') as _:\n        pass\n    assert '' not in docutils.roles._roles\n\n    # test with non-existent role\n    logger.warning = lambda *args, **kwargs: None  # suppress warning\n    with default_role(docname, 'nonexistent') as _:\n        pass\n    assert '' not in docutils.roles._roles\n", "def test_default_role():\n    docname = 'dummy'\n    name = 'ref'\n\n    with default_role(docname, name):\n        assert docutils.roles._roles[''] is not None\n\n    assert docutils.roles._roles.get('') is None\n\n    # Test with unknown role\n    name = 'unknown'\n    logger.warning = lambda msg, *args, **kwargs: None  # suppress warning\n    with default_role(docname, name):\n        assert docutils.roles._roles.get('') is None\n", "def test_default_role(app):\n    docname = 'dummy'\n    name = 'default_role_test'\n\n    # Register a default role\n    with default_role(docname, name):\n        # Check if the role is registered\n        assert docutils.roles._roles.get('') is not None\n\n    # Check if the role is unregistered after the context manager\n    assert docutils.roles._roles.get('') is None\n", "def test_default_role():\n    docname = 'dummy'\n    name = 'ref'\n    with default_role(docname, name) as _:\n        assert docutils.registered_roles[''] is not None\n    assert docutils.registered_roles.get('') is None\n\n    # test default role not found\n    logger.warning.reset_mock()\n    name = 'invalid'\n    with default_role(docname, name) as _:\n        pass\n    logger.warning.assert_called_once_with(__('default role %s not found') % name, location=docname)\n", "def test_default_role():\n    docname = 'dummy'\n    name = 'strong'\n\n    with default_role(docname, name):\n        assert docutils.get_role('') is not None\n\n    assert docutils.get_role('') is None\n\n    with default_role(docname, 'nonexistent'):\n        assert docutils.get_role('') is None\n"], "sample_929": ["def test_pyexception(app):\n    text = (\".. py:exception:: Exception\\n\"\n            \"   :module: exceptions\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"Exception\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pydata_with_module(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:data:: var\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"example.\"],\n                                                    [desc_name, \"var\"])],\n                                  [desc_content, ()])]))\n    assert 'example.var' in domain.objects\n    assert domain.objects['example.var'] == ('index', 'example.var', 'data')\n", "def test_pydecorator_function(app):\n    text = (\".. py:decorator:: my_decorator\\n\"\n            \"   :module: my_module\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"my_decorator\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'my_module.my_decorator' in domain.objects\n    assert domain.objects['my_module.my_decorator'] == ('index', 'my_module.my_decorator', 'function')\n", "def test_pyfunction_with_annotation(app):\n    text = (\".. py:function:: func1(a: int) -> str\\n\"\n            \".. py:function:: func2(b: float) -> None\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func1\"],\n                                                    [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                                                            [desc_sig_punctuation, \":\"],\n                                                                                            \" \",\n                                                                                            [pending_xref, \"int\"])])],\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func2\"],\n                                                    [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"b\"],\n                                                                                            [desc_sig_punctuation, \":\"],\n                                                                                            \" \",\n                                                                                            [pending_xref, \"float\"])])],\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                  desc_content)]))\n    assert 'func1' in domain.objects\n    assert domain.objects['func1'] == ('index', 'func1', 'function')\n    assert 'func2' in domain.objects\n    assert domain.objects['func2'] == ('index', 'func2', 'function')\n", "def test_pymodule(app):\n    text = (\".. py:module:: example\\n\"\n            \"   :platform: Unix, Windows\\n\"\n            \"   :synopsis: Sample\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index))\n    assert_node(doctree[0], nodes.target,\n                ids=['module-example'])\n    assert_node(doctree[1], addnodes.index,\n                entries=[('pair', 'module; example', 'module-example', '', None)])\n\n    assert 'example' in domain.modules\n    module = domain.modules['example']\n    assert module.docname == 'index'\n    assert module.synopsis == 'Sample'\n    assert module.platform == 'Unix, Windows'\n    assert module.deprecated is True\n"], "sample_930": ["def test_create_index_with_group_entries(app):\n    text = (\".. index:: docutils\\n\"\n            \".. index:: Python\\n\"\n            \".. index:: pip; install\\n\"\n            \".. index:: pip; upgrade\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')])], None]),\n                              ('pip', [[], [('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Python; documentation tool\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=True)\n    assert len(index) == 2\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('Python', [[],\n                                     [('documentation tool', [('', '#index-2')]),\n                                      ('interpreter', [('', '#index-1')])],\n                                     None])])\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: docutils\\n\"\n            \".. index:: Python\\n\"\n            \".. index:: pip; install\\n\"\n            \".. index:: pip; upgrade\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 5\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')]),\n                                            ('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-4')], [], None])])\n\n    # check that subitems are not grouped\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=True)\n    assert len(index) == 5\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')]),\n                                            ('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-4')], [], None])])\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: docutils\\n\"\n            \".. index:: Python\\n\"\n            \".. index:: pip; install\\n\"\n            \".. index:: pip; upgrade\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')])], None]),\n                              ('pip', [[], [('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n", "def test_create_index_group_entries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: single: Python\\n\"\n            \".. index:: pip; install\\n\"\n            \".. index:: pip; upgrade\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')])], None]),\n                              ('pip', [[], [('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n"], "sample_931": ["def test_role_for_objtype():\n    domain = PythonDomain(Mock(env=Mock()))\n    assert domain.role_for_objtype('function') == 'func'\n    assert domain.role_for_objtype('data') == 'data'\n    assert domain.role_for_objtype('class') == 'class'\n    assert domain.role_for_objtype('exception') == 'exc'\n    assert domain.role_for_objtype('method') == 'meth'\n    assert domain.role_for_objtype('classmethod') == 'meth'\n    assert domain.role_for_objtype('staticmethod') == 'meth'\n    assert domain.role_for_objtype('attribute') == 'attr'\n    assert domain.role_for_objtype('module') == 'mod'\n    with pytest.raises(KeyError):\n        domain.role_for_objtype('unknown')\n", "def test_pyattribute_with_type_and_no_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str]\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"Optional\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \"]\"])])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][3], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_pyattribute_with_default_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str]\\n\"\n            \"      :value: 'default'\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"Optional\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \"]\"])],\n                                                     [desc_annotation, \" = 'default'\"])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][3], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_module_index_with_deprecated(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \"   :deprecated:\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', 'Deprecated', '')])],\n        False\n    )\n", "def test_pyattribute_with_no_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n"], "sample_933": ["def test_should_write(app):\n    app.builder.build_all()\n    pofn = app.outdir / 'extapi.pot'\n    with open(pofn, 'w', encoding='utf-8') as pofile:\n        pofile.write('msgid \"\"\\nmsgstr \"\"\\n')\n    assert should_write(str(pofn), POHEADER % {'copyright': '2020', 'project': 'Test Project', 'version': '1.0'})\n    with open(pofn, 'w', encoding='utf-8') as pofile:\n        pofile.write(POHEADER % {'copyright': '2020', 'project': 'Test Project', 'version': '1.0'})\n    assert not should_write(str(pofn), POHEADER % {'copyright': '2020', 'project': 'Test Project', 'version': '1.0'})\n", "def test_gettext_location_disabled(app):\n    app.config.gettext_location = False\n    app.builder.build_all()\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert '#: ' not in pot\n", "def test_gettext_location(app):\n    app.builder.build_all()\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert '#: ../gettext/index_entries.rst:3' in pot\n    assert '#: ../gettext/index_entries.rst:5' in pot\n", "def test_gettext_location(app):\n    app.builder.build_all()\n    pot = (app.outdir / 'index_entries.pot').read_text()\n\n    # By default, location comments are included in the pot file.\n    assert \"#: index_entries.rst:\" in pot\n\n    # Rebuild with gettext_location set to False.\n    app.config.gettext_location = False\n    app.builder.build_all()\n    pot = (app.outdir / 'index_entries.pot').read_text()\n\n    # Location comments should now be excluded from the pot file.\n    assert \"#: index_entries.rst:\" not in pot\n", "def test_gettext_disable_location_enable_uuid(app):\n    app.builder.build_all()\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert '#: ' not in pot  # location is disabled\n    assert '# uuid:' in pot  # uuid is enabled\n"], "sample_936": ["def test_stringify_type_hints_forward_ref():\n    assert stringify(\"List[int]\") == \"List[int]\"\n    assert stringify(\"Dict[str, 'int']\") == \"Dict[str, int]\"\n    assert stringify(\"Tuple['str', ...]\") == \"Tuple[str, ...]\"\n    assert stringify(\"Union['int', str]\") == \"Union[int, str]\"\n", "def test_stringify_forward_ref():\n    ForwardRefType = ForwardRef('MyClass1')\n    assert stringify(ForwardRefType) == 'MyClass1'\n", "def test_stringify_type_hints_forward_ref():\n    ForwardRefType = typing.ForwardRef('MyClass1')\n    assert stringify(ForwardRefType) == \"test_util_typing.MyClass1\"\n", "def test_stringify_type_hints_forward_ref():\n    ForwardRefType = typing.ForwardRef('MyClass1')\n    assert stringify(ForwardRefType) == \"test_util_typing.MyClass1\"\n", "def test_stringify_forward_ref():\n    class MyClass:\n        pass\n\n    ForwardRefType = typing.ForwardRef('MyClass')\n    assert stringify(ForwardRefType) == \"MyClass\"\n\n    if sys.version_info >= (3, 7):\n        from typing import _GenericAlias\n        ForwardRefType = _GenericAlias(List, [ForwardRef('MyClass')])\n        assert stringify(ForwardRefType) == \"List[MyClass]\"\n"], "sample_937": ["def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_generic_visit():\n    class UnknownNode(ast.AST):\n        pass\n\n    with pytest.raises(NotImplementedError) as excinfo:\n        ast.unparse(UnknownNode())\n\n    assert str(excinfo.value) == 'Unable to parse UnknownNode object'\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n"], "sample_10": ["def test_table_attribute_with_default():\n    class MyTable(Table):\n        baz = TableAttribute(default=1)\n\n    t = MyTable()\n    assert '__attributes__' not in t.meta\n    assert t.baz == 1\n    assert '__attributes__' in t.meta\n", "def test_set_column_attribute():\n    dat = [[1, 2, 3],\n           [4, 5, 6],\n           [7, 8, 9]]\n    t = table.Table(dat, names=('a', 'b', 'c'), units=('m', 's', 'kg'))\n    t.set_column_attribute('b', 'description', 'time')\n    assert t['b'].info.description == 'time'\n\n    t.set_column_attribute('c', 'unit', 'g')\n    assert t['c'].info.unit == u.g\n\n    t.set_column_attribute('c', 'unit', 'kg')\n    assert t['c'].info.unit == u.kg\n\n    with pytest.raises(TypeError):\n        t.set_column_attribute('c', 'unit', (1, 2))\n\n    with pytest.raises(TypeError):\n        t.set_column_attribute('c', 'unit', [1, 2])\n\n    with pytest.raises(ValueError):\n        t.set_column_attribute('d', 'unit', 'kg')\n", "def test_setitem_inplace():\n    \"\"\"Test the _setitem_inplace attribute with a mixin column\"\"\"\n    class MixinColumn:\n            raise Exception(\"Should not get here\")\n\n    class MyTable(Table):\n        _setitem_inplace = True\n\n    t = MyTable()\n    t['a'] = MixinColumn()\n    with pytest.raises(Exception, match=\"Should not get here\"):\n        t['a'] = [1, 2, 3]\n", "def test_table_attribute_conflict():\n    \"\"\"\n    Test that if a custom `TableAttribute` is defined on a table class with the same name as\n    a built-in attribute or method, a `RuntimeError` is raised.\n    \"\"\"\n    with pytest.raises(RuntimeError) as err:\n        class MyTable(Table):\n            ppformat = TableAttribute()\n    assert \"Error calling __set_name__\" in str(err.value)\n", "def test_pprint_exclude_names():\n    t = table.Table([[1, 2], [3, 4]], names=('a', 'b'))\n    assert t.pprint_exclude_names is None\n    assert t.pprint_exclude_names() is None\n\n    t.pprint_exclude_names = ['a']\n    assert t.pprint_exclude_names == ['a']\n\n    t.pprint_exclude_names.add('b')\n    assert t.pprint_exclude_names == ['a', 'b']\n\n    t.pprint_exclude_names.remove('a')\n    assert t.pprint_exclude_names == ['b']\n\n    t.pprint_exclude_names.remove('b')\n    assert t.pprint_exclude_names == []\n\n    t.pprint_exclude_names = None\n    assert t.pprint_exclude_names is None\n"], "sample_19": ["def test_preserve_shape_sub():\n    w = wcs.WCS(naxis=3)\n\n    x = np.random.random((2, 3, 4))\n    y = np.random.random((2, 3, 4))\n    z = np.random.random((2, 3, 4))\n\n    w_sub = w.sub([1, 2])\n    xw, yw = w_sub.wcs_pix2world(x, y, 1)\n\n    assert xw.shape == (2, 3, 4)\n    assert yw.shape == (2, 3, 4)\n\n    xp, yp = w_sub.wcs_world2pix(x, y, 1)\n\n    assert xp.shape == (2, 3, 4)\n    assert yp.shape == (2, 3, 4)\n", "def test_fits_fixed_warning():\n    header = get_pkg_data_contents(\"data/nonstandard_units.hdr\", encoding=\"binary\")\n    with pytest.warns(wcs.FITSFixedWarning):\n        wcs.WCS(header, fix=True, translate_units=\"dhs\")\n    with pytest.warns(wcs.FITSFixedWarning):\n        wcs.WCS(header, fix=True, translate_units=\"shd\")\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"TIME\"]\n    w.wcs.set()\n    assert w.axis_type_names == [\"Right Ascension\", \"Declination\", \"Time\"]\n\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.set()\n    assert w.axis_type_names == [\"Right Ascension\", \"Declination\", \"Frequency\"]\n", "def test_pixel_bounds():\n    \"\"\"Test that WCS.pixel_bounds returns the correct bounds.\"\"\"\n    w = wcs.WCS(naxis=2)\n    w._naxis = [100, 200]\n    assert w.pixel_bounds == ((1, 100), (1, 200))\n    w._pixel_bounds = ((5, 95), (10, 190))\n    assert w.pixel_bounds == ((5, 95), (10, 190))\n    w.pixel_bounds = None\n    assert w.pixel_bounds is None\n    w._naxis = [100, 200]\n    assert w.pixel_bounds == ((1, 100), (1, 200))\n", "def test_wcs_get_axis_types():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.crpix = [32.5, 16.5, 1.0]\n    w.wcs.crval = [5.63, -72.05, 1.0]\n    w.wcs.pc = [[5.9e-06, 1.3e-05, 0.0], [-1.2e-05, 5.0e-06, 0.0], [0.0, 0.0, 1.0]]\n    w.wcs.cdelt = [1.0, 1.0, 1.0]\n    w.wcs.set()\n    expected_result = [\n        {\n            \"coordinate_type\": \"celestial\",\n            \"scale\": \"non-linear celestial\",\n            \"group\": 0,\n            \"number\": 0,\n        },\n        {\n            \"coordinate_type\": \"celestial\",\n            \"scale\": \"non-linear celestial\",\n            \"group\": 0,\n            \"number\": 1,\n        },\n        {\n            \"coordinate_type\": \"spectral\",\n            \"scale\": \"linear\",\n            \"group\": 0,\n            \"number\": 0,\n        },\n    ]\n    assert w.get_axis_types() == expected_result\n"], "sample_55": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.user1 = User.objects.create_user(username='test1', email='test1@example.com')\n        cls.user2 = User.objects.create_user(username='test2', email='test2@example.com')\n        cls.user3 = User.objects.create_user(username='test3', email='test3@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.joepublicuser = User.objects.create_user(username='joepublic', password='secret')\n"], "sample_66": ["    def test_create_from_iterable(self):\n        q = QueryDict.fromkeys(['a', 'b', 'c'], 'value')\n        self.assertEqual(q, {'a': ['value'], 'b': ['value'], 'c': ['value']})\n", "def test_http_headers_parse_header_name(self):\n    # The HttpHeaders.parse_header_name method should ignore headers\n    # that do not begin with HTTP_ or are not CONTENT_TYPE or CONTENT_LENGTH\n    self.assertIsNone(HttpHeaders.parse_header_name('PATH_INFO'))\n    self.assertEqual(HttpHeaders.parse_header_name('HTTP_ACCEPT'), 'Accept')\n    self.assertEqual(HttpHeaders.parse_header_name('HTTP_USER_AGENT'), 'User-Agent')\n    self.assertEqual(HttpHeaders.parse_header_name('HTTP_X_FORWARDED_PROTO'), 'X-Forwarded-Proto')\n    self.assertEqual(HttpHeaders.parse_header_name('CONTENT_TYPE'), 'Content-Type')\n    self.assertEqual(HttpHeaders.parse_header_name('CONTENT_LENGTH'), 'Content-Length')\n", "    def test_querydict_init(self):\n        q = QueryDict('a=1&a=2&b=3', mutable=True)\n        self.assertEqual(list(q.items()), [('a', '1'), ('a', '2'), ('b', '3')])\n", "def test_validate_host(self):\n    allowed_hosts = ['example.com', 'example.net', '.example.org']\n    self.assertTrue(validate_host('example.com', allowed_hosts))\n    self.assertTrue(validate_host('test.example.org', allowed_hosts))\n    self.assertFalse(validate_host('example.io', allowed_hosts))\n    self.assertFalse(validate_host('example', allowed_hosts))\n", "def test_build_absolute_uri_with_query_string(self):\n    request = HttpRequest()\n    request.get_host = lambda: 'www.example.com'\n    request.path = '/path'\n    request.META['QUERY_STRING'] = 'query=value'\n    self.assertEqual(\n        request.build_absolute_uri(location='/path2'),\n        'http://www.example.com/path2'\n    )\n    self.assertEqual(\n        request.build_absolute_uri(),\n        'http://www.example.com/path?query=value'\n    )\n"], "sample_95": ["    def test_adds_headers(self):\n        response = HttpResponse()\n        patch_response_headers(response, cache_timeout=123)\n        self.assertEqual(response['Expires'], http_date(time.time() + 123))\n        self.assertEqual(response['Cache-Control'], 'max-age=123')\n", "    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Language', 'Accept-Encoding'])\n        self.assertEqual(response['Vary'], 'Accept-Language, Accept-Encoding')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600')\n", "    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Language', 'Cookie'])\n        self.assertEqual(response['Vary'], 'Accept-Language, Cookie')\n", "    def test_learn_cache_key(self):\n        request = HttpRequest()\n        request.method = 'GET'\n        request.path = '/path/to/page'\n        response = HttpResponse()\n        response['Vary'] = 'Accept-Language'\n\n        # Simulate that we haven't learned any cache keys yet\n        cache_key = get_cache_key(request)\n        self.assertIsNone(cache_key)\n\n        # Learn cache key from response\n        learned_key = learn_cache_key(request, response)\n\n        # Check that we learned a cache key\n        self.assertIsNotNone(learned_key)\n\n        # Check that we get the same cache key on subsequent requests\n        cache_key = get_cache_key(request)\n        self.assertEqual(cache_key, learned_key)\n"], "sample_124": ["def test_disabled_fields_with_initial(self):\n    class MyForm(Form):\n        disabled_field = CharField(initial='Disabled value', disabled=True)\n        enabled_field = CharField(initial='Enabled value')\n\n    form = MyForm()\n    self.assertEqual(form.fields['disabled_field'].initial, 'Disabled value')\n    self.assertEqual(form.fields['enabled_field'].initial, 'Enabled value')\n    self.assertEqual(form['disabled_field'].value(), 'Disabled value')\n    self.assertEqual(form['enabled_field'].value(), 'Enabled value')\n\n    form = MyForm(data={'disabled_field': 'New value', 'enabled_field': 'New enabled value'})\n    self.assertEqual(form['disabled_field'].value(), 'Disabled value')\n    self.assertEqual(form['enabled_field'].value(), 'New enabled value')\n", "def test_field_widget_override(self):\n    # A field's widget can be overridden in the form's __init__ method\n    class MyForm(Form):\n        field = CharField()\n\n    form = MyForm()\n    self.assertIsInstance(form.fields['field'].widget, TextInput)\n\n    form = MyForm()\n    form.fields['field'].widget = Textarea()\n    self.assertIsInstance(form.fields['field'].widget, Textarea)\n", "def test_email_field(self):\n    class EmailForm(Form):\n        email = EmailField()\n\n    # A blank value is invalid.\n    f = EmailForm({'email': ''})\n    self.assertFalse(f.is_valid())\n    self.assertEqual(f.errors['email'], ['This field is required.'])\n\n    # Invalid email\n    f = EmailForm({'email': 'Invalid'})\n    self.assertFalse(f.is_valid())\n    self.assertEqual(f.errors['email'], ['Enter a valid email address.'])\n\n    # Valid email\n    f = EmailForm({'email': 'valid@example.com'})\n    self.assertTrue(f.is_valid())\n    self.assertEqual(f.cleaned_data['email'], 'valid@example.com')\n", "def test_field_required_with_custom_widget(self):\n    class CustomWidget(TextInput):\n            return False\n\n    class MyForm(Form):\n        use_required_attribute = True\n        f1 = CharField(max_length=30, widget=CustomWidget)\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\"></p>',\n    )\n", "def test_disabled_fields_multipart(self):\n    class MyForm(forms.Form):\n        file1 = forms.FileField(required=True)\n        file2 = forms.FileField(required=True, disabled=True)\n\n    form = MyForm(files={'file1': SimpleUploadedFile('name', b'file1')})\n    self.assertTrue(form.is_valid())\n\n    form = MyForm(files={'file1': SimpleUploadedFile('name', b'file1'), 'file2': SimpleUploadedFile('name', b'file2')})\n    self.assertTrue(form.is_valid())\n\n    form = MyForm(files={'file2': SimpleUploadedFile('name', b'file2')})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {'file1': ['This field is required.']})\n"], "sample_297": ["    def test_ticket_25370(self):\n        teacher = Teacher.objects.create()\n        student1 = Student.objects.create()\n        student2 = Student.objects.create()\n        Classroom.objects.create(teacher=teacher, student=student1)\n        Classroom.objects.create(teacher=teacher, student=student2)\n        self.assertSequenceEqual(\n            Teacher.objects.filter(classroom__student__in=[student1]).values_list('pk', flat=True),\n            [teacher.pk]\n        )\n        self.assertSequenceEqual(\n            Teacher.objects.filter(classroom__student=student1).values_list('pk', flat=True),\n            [teacher.pk]\n        )\n", "    def test_ticket_25367(self):\n        school = School.objects.create()\n        classroom = Classroom.objects.create(school=school)\n        teacher = Teacher.objects.create()\n        teacher.classrooms.add(classroom)\n        self.assertSequenceEqual(\n            Teacher.objects.filter(classrooms__school=school),\n            [teacher]\n        )\n", "    def test_ticket_24863(self):\n        \"\"\"\n        Test that a subquery in a .values() call is evaluated only once.\n        \"\"\"\n        # This is a simplified version of the original test in the ticket.\n        # It still hits the relevant code path.\n        Tag.objects.create(name='foo')\n        qs = Tag.objects.values('name')\n        self.assertEqual(qs.query.subq_aliases, set())\n        subquery = Tag.objects.order_by('name').values_list('name', flat=True)\n        qs = qs.annotate(name_count=Subquery(subquery, output_field=IntegerField()))\n        self.assertEqual(qs.query.subq_aliases, {'U'})\n        self.assertEqual(list(qs), [{'name': 'foo', 'name_count': 'foo'}])\n", "    def test_ticket_25086(self):\n        \"\"\"\n        Ensure join promotion is applied to nested conditions.\n        \"\"\"\n        f1 = Food.objects.create(name='apples')\n        f2 = Food.objects.create(name='oranges')\n        Eaten.objects.create(food=f1, meal='dinner')\n        Eaten.objects.create(food=f2, meal='dinner')\n\n        self.assertEqual(\n            set(Food.objects.filter(Q(eaten__meal='dinner') | Q(eaten=None))),\n            {f1, f2}\n        )\n", "    def test_ticket_24900(self):\n        school = School.objects.create(name='school')\n        student1 = Student.objects.create(school=school)\n        student2 = Student.objects.create(school=school)\n        classroom = Classroom.objects.create(school=school)\n        classroom.students.add(student1, student2)\n        self.assertSequenceEqual(\n            Student.objects.filter(classroom__students=student1),\n            [student1, student2]\n        )\n        self.assertSequenceEqual(\n            Student.objects.filter(classroom__students=student2),\n            [student1, student2]\n        )\n"], "sample_136": ["def test_split_domain_port_ipv6(self):\n    domain, port = split_domain_port('[2001:19f0:feee::dead:beef:cafe]:8080')\n    self.assertEqual(domain, '[2001:19f0:feee::dead:beef:cafe]')\n    self.assertEqual(port, '8080')\n", "    def test_create(self):\n        q = QueryDict('a=1&a=2&b=3')\n        self.assertEqual(list(q.items()), [('a', '1'), ('a', '2'), ('b', '3')])\n        self.assertEqual(list(q), ['a', 'a', 'b'])\n", "    def test_create_querydict_from_string(self):\n        q = QueryDict('a=1&a=2&b=3', encoding='utf-8')\n        self.assertEqual(q['a'], ['1', '2'])\n        self.assertEqual(q['b'], ['3'])\n", "def test_split_domain_port_invalid(self):\n    tests = (\n        # invalid hostnames\n        ('', ('', '')),\n        ('example', ('', '')),\n        ('example:', ('', '')),\n        (':80', ('', '')),\n        # invalid IP addresses\n        ('256.1.1.1', ('', '')),\n        ('2001:19f0:feee::dead:beef:cafe:fail', ('', '')),\n    )\n    for host, expected in tests:\n        with self.subTest(host=host):\n            domain, port = split_domain_port(host)\n            self.assertEqual((domain, port), expected)\n", "def test_split_domain_port_ipv6(self):\n    domain, port = split_domain_port('[2001:19f0:feee::dead:beef:cafe]:8080')\n    self.assertEqual(domain, '2001:19f0:feee::dead:beef:cafe')\n    self.assertEqual(port, '8080')\n"], "sample_311": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "compilation error", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.r1 = Region.objects.create(name='Region 1')\n        cls.r2 = Region.objects.create(name='Region 2')\n        cls.d1 = District.objects.create(name='District 1')\n        cls.d2 = District.objects.create(name='District 2')\n        cls.r1.districts.add(cls.d1, cls.d2)\n        cls.r2.districts.add(cls.d1)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.adduser = User.objects.create_user(username='adduser', password='secret', is_staff=True)\n        cls.model_admin = ModelAdmin(CustomArticle, site)\n"], "sample_319": ["def test_alter_enum_adding_new_value(self):\n    # Test adding a new value to an Enum field.\n\n    class Color(models.TextChoices):\n        RED = 'red'\n        GREEN = 'green'\n\n    class NewColor(models.TextChoices):\n        RED = 'red'\n        GREEN = 'green'\n        BLUE = 'blue'\n\n    model_state = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"color\", models.CharField(max_length=5, choices=Color.choices)),\n        ],\n    )\n    new_model_state = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"color\", models.CharField(max_length=5, choices=NewColor.choices)),\n        ],\n    )\n    changes = self.get_changes([model_state], [new_model_state])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n", "def test_add_unique_constraint(self):\n    author_with_unique_name = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        options={\n            \"constraints\": [\n                models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\"),\n            ]\n        },\n    )\n    changes = self.get_changes([self.author_empty], [author_with_unique_name])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertEqual(\n        changes[\"testapp\"][0].operations[0].options[\"constraints\"][0].name,\n        \"unique_name\",\n    )\n", "def test_create_model_with_custom_primary_key(self):\n    \"\"\"Test creation of new model with custom primary key.\"\"\"\n    aardvark = ModelState(\n        \"testapp\",\n        \"Aardvark\",\n        [\n            (\"id_value\", models.IntegerField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n    )\n    changes = self.get_changes([], [aardvark])\n    # Right number of migrations?\n    self.assertEqual(len(changes[\"testapp\"]), 1)\n    # Right number of actions?\n    migration = changes[\"testapp\"][0]\n    self.assertEqual(len(migration.operations), 1)\n    # Right actions order?\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Aardvark\")\n", "def test_alter_model_table_with_custom_database_table_name(self):\n    \"\"\"\n    Tests when model changes but db_table stays as-is, autodetector must not\n    create more than one operation.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_db_table_options],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n    # Ensure no AlterModelTable operation is created\n    for operation in changes[\"testapp\"][0].operations:\n        self.assertNotIsInstance(operation, operations.AlterModelTable)\n", "def test_alter_field_and_index_together_rename(self):\n    \"\"\"Fields are altered after deleting some index_together, then renaming.\"\"\"\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField(db_index=True)),\n        ],\n        {\n            \"index_together\": {(\"name\",)},\n        },\n    )\n    author_reversed_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"names\", models.CharField(max_length=200, unique=True)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"index_together\": {(\"age\",)},\n        },\n    )\n    changes = self.get_changes(\n        [initial_author],\n        [author_reversed_constraints],\n        MigrationQuestioner({\"ask_rename\": True}),\n    )\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\n            \"AlterIndexTogether\",\n            \"AlterField\",\n            \"AlterField\",\n            \"AlterIndexTogether\",\n            \"RenameField\",\n        ],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        index_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        model_name=\"author\",\n        name=\"age\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        2,\n        model_name=\"author\",\n        name=\"name\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        3,\n        name=\"author\",\n        index_together={(\"age\",)},\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        4,\n        model_name=\"author\",\n        old_name=\"name\",\n        new_name=\"names\",\n    )\n"], "sample_320": ["def test_rename_index_in_separate_database_and_state(self):\n    \"\"\"\n    Tests that RenameIndex works in SeparateDatabaseAndState.\n    \"\"\"\n    app_label = \"test_rename_index_separate_db_state\"\n    project_state = self.set_up_test_model(app_label, index=True)\n\n    database_operations = [\n        migrations.RenameIndex(\"Pony\", new_name=\"new_pony_index\", old_name=\"pony_pink_idx\")\n    ]\n\n    state_operations = [migrations.RenameIndex(\"Pony\", new_name=\"new_pony_index\", old_name=\"pony_pink_idx\")]\n\n    operation = migrations.SeparateDatabaseAndState(\n        state_operations=state_operations,\n        database_operations=database_operations,\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n\n    # Rename index.\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n\n    self.assertIndexNameNotExists(f\"{app_label}_pony\", \"pony_pink_idx\")\n    self.assertIndexNameExists(f\"{app_label}_pony\", \"new_pony_index\")\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n\n    self.assertIndexNameExists(f\"{app_label}_pony\", \"pony_pink_idx\")\n    self.assertIndexNameNotExists(f\"{app_label}_pony\", \"new_pony_index\")\n    # Deconstruction.\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"SeparateDatabaseAndState\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(\n        sorted(definition[2]), [\"database_operations\", \"state_operations\"]\n    )\n", "def test_references_field_by_order_with_respect_to(self):\n    operation = FieldOperation(\n        \"Model\", \"_order\", models.IntegerField()\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"whatever\", \"migrations\"), True\n    )\n    operation.order_with_respect_to = \"whatever\"\n    self.assertIs(\n        operation.references_field(\"Model\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"other\", \"migrations\"), False\n    )\n", "compilation error", "def test_can_reduce_through(self):\n    operation = CreateModel(\"Pony\", [(\"id\", models.AutoField(primary_key=True))])\n    other = CreateModel(\"Pony\", [(\"id\", models.AutoField(primary_key=True))])\n    self.assertIs(operation.can_reduce(other, \"migrations\"), True)\n    other = CreateModel(\"Pony\", [(\"name\", models.CharField(max_length=20))])\n    self.assertIs(operation.can_reduce(other, \"migrations\"), False)\n", "def test_remove_field_state_forwards(self):\n    project_state = self.set_up_test_model(\"test_rmflsf\")\n    operation = migrations.RemoveField(\"Pony\", \"pink\")\n    old_model = project_state.apps.get_model(\"test_rmflsf\", \"Pony\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_rmflsf\", new_state)\n    new_model = new_state.apps.get_model(\"test_rmflsf\", \"Pony\")\n    self.assertIsNot(old_model, new_model)\n"], "sample_387": ["    def test_widget_delegates_value_omitted_from_data(self):\n        class CustomWidget(forms.Select):\n                return False\n\n        rel = Album._meta.get_field(\"band\").remote_field\n        widget = CustomWidget()\n        wrapper = widgets.RelatedFieldWidgetWrapper(widget, rel, admin.site)\n        self.assertIs(wrapper.value_omitted_from_data({}, {}, \"band\"), False)\n", "    def test_action_checkbox(self):\n        band = Band.objects.create(name=\"Linkin Park\")\n        ma = admin.ModelAdmin(Band, admin.site)\n        self.assertHTMLEqual(\n            ma.action_checkbox(band),\n            '<input type=\"checkbox\" name=\"_selected_action\" value=\"{}\">'.format(band.pk),\n        )\n", "    def test_many_to_many(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_band_add\")\n        )\n        main_window = self.selenium.current_window_handle\n\n        # No value has been selected yet\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_members\").get_attribute(\"value\"), \"\"\n        )\n\n        # Open the popup window and click on a member\n        self.selenium.find_element(By.ID, \"lookup_id_members\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Add member\")\n        link.click()\n\n        # Fill the member form\n        self.selenium.find_element(By.ID, \"id_name\").send_keys(\"John Doe\")\n        self.selenium.find_element(By.ID, \"id_email\").send_keys(\"john@example.com\")\n        self.selenium.find_element(By.ID, \"id_birthdate_0\").send_keys(\"1990-01-01\")\n        self.selenium.find_element(By.ID, \"id_birthdate_1\").send_keys(\"12:00:00\")\n        self.selenium.find_element(By.ID, \"id_gender\").send_keys(\"M\")\n\n        # Save the member\n        self.selenium.find_element(By.NAME, \"_save\").click()\n        self.selenium.switch_to.window(main_window)\n\n        # The field now contains the selected member's id\n        self.wait_for_value(\"#id_members\", \"1\")\n\n        # Reopen the popup window and click on another member\n        self.selenium.find_element(By.ID, \"lookup_id_members\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Add member\")\n        link.click()\n\n        # Fill the member form\n        self.selenium.find_element(By.ID, \"id_name\").send_keys(\"Jane Doe\")\n        self.selenium.find_element(By.ID, \"id_email\").send_keys(\"jane@example.com\")\n        self.selenium.find_element(By.ID, \"id_birthdate_0\").send_keys(\"1991-01-01\")\n        self.selenium.find_element(By.ID, \"id_birthdate_1\").send_keys(\"12:00:00\")\n        self.selenium.find_element(By.ID, \"id_gender\").send_keys(\"F\")\n\n        # Save the member\n        self.selenium.find", "    def test_inline_admin_formset(self):\n        # Create an empty inline formset\n        class TestInline(admin.TabularInline):\n            model = Album\n\n        class TestAdmin(admin.ModelAdmin):\n            inlines = [TestInline]\n\n        admin = TestAdmin(Band, admin.site)\n        request = RequestFactory().get(\"/\")\n\n        inline_formset = inlineformset_factory(\n            Band,\n            Album,\n            formset=admin.get_inline_formsets(\n                request, obj=None, fields=None\n            )[0].formset,\n        )\n        formset = inline_formset(instance=Band())\n\n        self.assertEqual(len(formset.forms), 3)\n", "    def test_get_content_type(self):\n        model_admin = ModelAdmin(Band, admin.site)\n        self.assertEqual(\n            model_admin.get_content_type_for_model(Band),\n            ContentType.objects.get_for_model(Band, for_concrete_model=False),\n        )\n"], "sample_389": ["    def test_querydict(self):\n        # Create a QueryDict from a string\n        qd = QueryDict(\"a=1&b=2&c=3&d=&e=f\")\n        self.assertEqual(qd.get(\"a\"), \"1\")\n        self.assertEqual(qd.get(\"b\"), \"2\")\n        self.assertEqual(qd.get(\"c\"), \"3\")\n        self.assertEqual(qd.get(\"d\"), \"\")\n        self.assertEqual(qd.get(\"e\"), \"f\")\n", "    def test_accepted_types(self):\n        request = HttpRequest()\n        request.META[\"HTTP_ACCEPT\"] = \"text/plain; q=0.5, text/html, text/x-dvi; q=0.8, text/x-c\"\n        accepted_types = request.accepted_types\n        self.assertEqual(len(accepted_types), 4)\n\n        self.assertEqual(str(accepted_types[0]), \"text/html\")\n        self.assertEqual(str(accepted_types[1]), \"text/x-dvi; q=0.8\")\n        self.assertEqual(str(accepted_types[2]), \"text/x-c\")\n        self.assertEqual(str(accepted_types[3]), \"text/plain; q=0.5\")\n", "    def test_httprequest_full_path_with_script_name(self):\n        request = HttpRequest()\n        request.path = \"/path/to/object/\"\n        request.path_info = \"/path/to/object/\"\n        request.META = {\n            \"HTTP_HOST\": \"example.com\",\n            \"SERVER_NAME\": \"example.com\",\n            \"SERVER_PORT\": \"80\",\n        }\n        set_script_prefix(\"/my/site/\")\n        try:\n            self.assertEqual(request.get_full_path(), \"/my/site/path/to/object/\")\n            self.assertEqual(request.get_full_path_info(), \"/path/to/object/\")\n        finally:\n            clear_script_prefix()\n", "    def test_creation(self):\n        data = QueryDict(\"a=1&a=2&b=3\")\n        self.assertEqual(data.get(\"a\"), \"2\")\n        self.assertEqual(data.getlist(\"a\"), [\"1\", \"2\"])\n        self.assertEqual(data.get(\"b\"), \"3\")\n        self.assertEqual(data.get(\"c\"), None)\n", "    def test_querydict(self):\n        # Test the basic QueryDict methods\n        q = QueryDict(\"a=1&b=2\")\n        self.assertEqual(q[\"a\"], \"1\")\n        self.assertEqual(q.get(\"b\"), \"2\")\n        self.assertEqual(q.get(\"c\"), None)\n        self.assertEqual(q.get(\"c\", \"default\"), \"default\")\n        self.assertIn(\"a\", q)\n        self.assertNotIn(\"c\", q)\n"], "sample_396": ["    def test_ticket_25831(self):\n        \"\"\"\n        Test that query filter correctly handles subquery with Exists in filter()\n        \"\"\"\n        o1 = Order.objects.create()\n        o2 = Order.objects.create()\n        oi1 = OrderItem.objects.create(order=o1, status=0)\n        oi2 = OrderItem.objects.create(order=o2, status=1)\n\n        subquery = OrderItem.objects.filter(status=0)\n        self.assertSequenceEqual(\n            Order.objects.filter(items__in=subquery).order_by(\"pk\"),\n            [o1],\n        )\n\n        subquery = OrderItem.objects.filter(\n            Exists(OrderItem.objects.filter(status=0, order=OuterRef(\"order\")))\n        )\n        self.assertSequenceEqual(\n            Order.objects.filter(items__in=subquery).order_by(\"pk\"),\n            [o1],\n        )\n", "    def test_ticket_24863(self):\n        qs = Number.objects.filter(num__in=[1, 2])\n        q = Q(num=3)\n        combined_qs = qs | Number.objects.filter(q)\n        self.assertEqual(combined_qs.query.subq_aliases, {\"T\", \"U\"})\n", "    def test_ticket_24911(self):\n        \"\"\"\n        Make sure that filtering for a relation when using annotations on both\n        sides of the relation works correctly.\n        \"\"\"\n        Tag.objects.create(name=\"tag1\")\n        Tag.objects.create(name=\"tag2\")\n        Annotation.objects.create(name=\"ann1\", tag_id=1)\n        Annotation.objects.create(name=\"ann2\", tag_id=2)\n        qs = Tag.objects.annotate(ann_name=F(\"annotation__name\")).filter(\n            ann_name=F(\"annotation__name\"),\n        )\n        self.assertEqual(len(qs), 2)\n        qs = Tag.objects.annotate(ann_name=F(\"annotation__name\")).filter(\n            ann_name=F(\"name\"),\n        )\n        self.assertEqual(len(qs), 0)\n", "    def test_ticket_24863(self):\n        generic = NamedCategory.objects.create(name=\"Generic\")\n        t1 = Tag.objects.create(name=\"t1\", category=generic)\n        Annotation.objects.create(name=\"a1\", tag=t1)\n        Annotation.objects.create(name=\"a2\", tag=t1)\n        Annotation.objects.create(name=\"a3\", tag=t1)\n        Annotation.objects.create(name=\"a4\", tag=t1)\n        qs = Annotation.objects.all()[:2].values_list(\"tag__name\")\n        self.assertEqual(set(qs), {(\"t1\",)})\n", "    def test_filter_on_subclass_with_parent_link(self):\n        p = SingleObject.objects.create(name=\"parent\")\n        c = Child.objects.create(parent=p, name=\"child\", value=1)\n        rel = RelatedObject.objects.create(single=p)\n        self.assertSequenceEqual(\n            SingleObject.objects.filter(child__isnull=True),\n            [p],\n        )\n        self.assertSequenceEqual(\n            SingleObject.objects.filter(child__isnull=False),\n            [p],\n        )\n        self.assertSequenceEqual(\n            RelatedObject.objects.filter(single__child__isnull=True),\n            [rel],\n        )\n        self.assertSequenceEqual(\n            RelatedObject.objects.filter(single__child__isnull=False),\n            [rel],\n        )\n        self.assertSequenceEqual(\n            SingleObject.objects.filter(child__value=1),\n            [p],\n        )\n        self.assertSequenceEqual(\n            RelatedObject.objects.filter(single__child__value=1),\n            [rel],\n        )\n"], "sample_394": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.s1 = State.objects.create(name=\"New York\")\n        cls.s2 = State.objects.create(name=\"Illinois\")\n        cls.s3 = State.objects.create(name=\"California\")\n        cls.c1 = City.objects.create(state=cls.s1, name=\"New York\")\n        cls.c2 = City.objects.create(state=cls.s2, name=\"Chicago\")\n        cls.c3 = City.objects.create(state=cls.s3, name=\"San Francisco\")\n        cls.r1 = Restaurant.objects.create(city=cls.c1, name=\"Italian Pizza\")\n        cls.r2 = Restaurant.objects.create(city=cls.c1, name=\"Boulevard\")\n        cls.r3 = Restaurant.objects.create(city=cls.c2, name=\"Chinese Dinner\")\n        cls.r4 = Restaurant.objects.create(city=cls.c2, name=\"Angels\")\n        cls.r5 = Restaurant.objects.create(city=cls.c2, name=\"Take Away\")\n        cls.r6 = Restaurant.objects.create(city=cls.c3, name=\"The Unknown Restaurant\")\n        cls.w1 = Worker.objects.create(work_at=cls.r1, name=\"Mario\", surname=\"Rossi\")\n        cls.w2 = Worker.objects.create(\n            work_at=cls.r1, name=\"Antonio\", surname=\"Bianchi\"\n        )\n        cls.w3 = Worker.objects.create(work_at=cls.r1, name=\"John\", surname=\"Doe\")\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.user = User.objects.create_user(\n            username=\"user\", password=\"secret\", email=\"user@example.com\", is_staff=True\n        )\n        cls.group1 = Group.objects.create(name=\"group1\")\n        cls.group2 = Group.objects.create(name=\"group2\")\n        cls.group3 = Group.objects.create(name=\"group3\")\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n"], "sample_400": ["def test_alter_field_with_unique_true(self):\n    \"\"\"#23957 - Tests autodetection of unique field.\"\"\"\n    changes = self.get_changes([self.author_name], [self.author_name_unique])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True\n    )\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, unique=True)\n", "def test_alter_model_with_foo_together(self):\n    \"\"\"\n    AlterModelTable is generated first when changing a model's table name and\n    foo_together constraints.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_db_table_options, self.book_foo_together],\n        [\n            self.author_with_new_db_table_options,\n            self.book_foo_together_2,\n        ],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\"\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"AlterUniqueTogether\", \"AlterIndexTogether\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\", unique_together={(\"title\", \"author\")}\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, name=\"book\", index_together={(\"title\", \"author\")}\n    )\n", "def test_alter_model_options_with_multiple_options(self):\n    author_with_options = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        {\n            \"permissions\": [(\"can_hire\", \"Can hire\")],\n            \"verbose_name\": \"Authi\",\n            \"verbose_name_plural\": \"Authis\",\n        },\n    )\n    changes = self.get_changes([self.author_empty], [author_with_options])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        options={\n            \"permissions\": [(\"can_hire\", \"Can hire\")],\n            \"verbose_name\": \"Authi\",\n            \"verbose_name_plural\": \"Authis\",\n        },\n    )\n", "def test_generate_altered_unique_together_for_model_with_m2m_through(self):\n    model_state_with_m2m_through = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"publishers\",\n                models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Contract\"),\n            ),\n        ],\n        {\n            \"unique_together\": {(\"id\", \"publishers\")},\n        },\n    )\n    model_state_without_m2m_through = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"publishers\", models.ManyToManyField(\"testapp.Publisher\")),\n        ],\n        {\n            \"unique_together\": {(\"id\",)},\n        },\n    )\n    changes = self.get_changes(\n        [model_state_with_m2m_through],\n        [model_state_without_m2m_through],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\", \"AlterUniqueTogether\"])\n", "def test_alter_field_type_to_fk(self):\n    \"\"\"\n    Changing a field type to ForeignKey should work.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty], \n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"publisher\", models.ForeignKey(\"testapp.Publisher\", models.CASCADE)),\n                ]\n            ),\n            self.publisher\n        ]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"CreateModel\", \"AddField\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"Publisher\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, model_name=\"author\", name=\"publisher\"\n    )\n"], "sample_405": ["compilation error", "def test_references_field_by_m2m_through_model(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\"Other\", through=\"migrations.Through\"),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"migrations.Through\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Whatever\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_order_with_respect_to(self):\n    operation = FieldOperation(\n        \"Model\", \"_order\", models.IntegerField(default=0), model_options={\"order_with_respect_to\": \"field\"}\n    )\n    self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"whatever\", \"migrations\"), False)\n", "compilation error", "def test_references_field_by_order_with_respect_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"_order\",\n        models.IntegerField(),\n        model_options={\"order_with_respect_to\": \"field\"},\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"field\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"other_field\", \"migrations\"), False\n    )\n"], "sample_408": ["def test_alter_model_table_with_fks(self):\n    \"\"\"\n    Test altering the table name of a model with foreign keys.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.book],\n        [self.author_with_db_table_options, self.book],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table=\"author_one\"\n    )\n    # Now, test the other model with the FK.\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, model_name=\"book\", name=\"author\"\n    )\n", "def test_alter_model_table_with_fks(self):\n    \"\"\"\n    AlterModelTable should also alter foreign keys referencing the model.\n    \"\"\"\n    before = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"Bar\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"foo\", models.ForeignKey(\"app.Foo\", models.CASCADE)),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\"db_table\": \"app_foo_table\"},\n        ),\n        ModelState(\n            \"app\",\n            \"Bar\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"foo\", models.ForeignKey(\"app.Foo\", models.CASCADE)),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"AlterModelTable\", \"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, name=\"foo\", table=\"app_foo_table\"\n    )\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 1, model_name=\"bar\", name=\"foo\"\n    )\n", "def test_alter_model_table_circular_dependency(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_options, self.book_with_author],\n        [\n            self.author_with_new_db_table_options,\n            self.book_with_author,\n        ],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\"\n    )\n    # otherapp should depend on testapp.\n    self.assertMigrationDependencies(\n        changes, \"otherapp\", 0, [(\"testapp\", \"auto_1\")]\n    )\n", "def test_optimize_migrations(self):\n    class Migration(migrations.Migration):\n        dependencies = [(\"testapp\", \"0001_initial\")]\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.DeleteModel(\"Person\"),\n        ]\n\n    autodetector = MigrationAutodetector(None, None)\n    optimized_operations = MigrationOptimizer().optimize(\n        Migration(\"0002_initial\", \"test_app\").operations, \"test_app\"\n    )\n    self.assertEqual(optimized_operations, [])\n\n    class Migration(migrations.Migration):\n        dependencies = [(\"testapp\", \"0001_initial\")]\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.RenameModel(\"Person\", \"Author\"),\n            migrations.DeleteModel(\"Author\"),\n        ]\n\n    optimized_operations = MigrationOptimizer().optimize(\n        Migration(\"0002_initial\", \"test_app\").operations, \"test_app\"\n    )\n    self.assertEqual(optimized_operations, [])\n", "def test_m2m_renamed_through_model(self):\n    \"\"\"\n    Renaming the through model of a ManyToManyField results in\n    AlterField for the ManyToManyField and RenameModel for the\n    through model.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_m2m_through, self.publisher, self.contract],\n        [\n            self.author_with_renamed_m2m_through,\n            self.publisher,\n            self.contract_renamed,\n        ],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Contract\", new_name=\"Deal\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, model_name=\"author\", name=\"publishers\"\n    )\n"], "sample_423": ["def test_alter_constraints_name(self):\n    \"\"\"Tests detection for renaming a constraint.\"\"\"\n    before = ModelState(\n        \"testapp\",\n        \"Model\",\n        [],\n        {\n            \"constraints\": [\n                models.UniqueConstraint(\n                    fields=(\"field\",), name=\"unique_field\"\n                ),\n            ],\n        },\n    )\n    after = ModelState(\n        \"testapp\",\n        \"Model\",\n        [],\n        {\n            \"constraints\": [\n                models.UniqueConstraint(\n                    fields=(\"field\",), name=\"new_unique_field\"\n                ),\n            ],\n        },\n    )\n    changes = self.get_changes([before], [after])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameConstraint\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        constraint_name=\"new_unique_field\",\n        old_name=\"unique_field\",\n        new_name=\"new_unique_field\",\n    )\n", "def test_nested_m2m_dependency(self):\n    \"\"\"\n    Adding a nested M2M relation to an existing M2M relation does not create\n    the nested M2M relation before its related model.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_m2m, self.publisher],\n        [\n            self.author_with_m2m,\n            self.publisher,\n            ModelState(\n                \"testapp\",\n                \"Nested\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"publishers\",\n                        models.ManyToManyField(\n                            \"testapp.Publisher\", through=\"testapp.Contract\"\n                        ),\n                    ),\n                ],\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"AddField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Nested\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"publishers\")\n", "def test_migrations_with_same_app_label_and_prefix(self):\n    class Migration1(migrations.Migration):\n        initial = True\n        operations = [migrations.CreateModel(\"Model1\", fields=[])]\n\n    class Migration2(migrations.Migration):\n        initial = True\n        operations = [migrations.CreateModel(\"Model2\", fields=[])]\n\n    migration1 = Migration1(\"0001_initial\", \"test_app\")\n    migration2 = Migration2(\"0001_initial\", \"test_app\")\n    loader = MigrationLoader(connection)\n    loader.disk_migrations = {\"test_app\": [migration1, migration2]}\n    autodetector = MigrationAutodetector(loader.project_state(), loader.project_state())\n    changes = autodetector.arrange_for_graph(autodetector._detect_changes(), loader.graph)\n    self.assertEqual(\n        changes[\"test_app\"][0].name,\n        \"0001_initial\",\n    )\n    self.assertEqual(\n        changes[\"test_app\"][1].name,\n        \"0002_initial\",\n    )\n", "def test_check_dependency_custom_through_model(self):\n    app_label = \"app\"\n    model_name = \"Foo\"\n    from_app_label = \"app\"\n    from_model_name = \"Bar\"\n    field_name = \"foo\"\n    from_field_name = \"bar\"\n    custom_through_model = f\"{from_app_label}.{from_model_name}CustomThrough\"\n\n    # Create a custom through model.\n    project_state = self.make_project_state(\n        [\n            ModelState(\n                from_app_label,\n                from_model_name,\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            ModelState(\n                from_app_label,\n                f\"{from_model_name}CustomThrough\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        from_field_name,\n                        models.ForeignKey(\n                            f\"{from_app_label}.{from_model_name}\", models.CASCADE\n                        ),\n                    ),\n                    (\n                        field_name,\n                        models.ForeignKey(f\"{app_label}.{model_name}\", models.CASCADE),\n                    ),\n                ],\n            ),\n            ModelState(\n                app_label,\n                model_name,\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        field_name,\n                        models.ManyToManyField(\n                            f\"{from_app_label}.{from_model_name}\",\n                            through=custom_through_model,\n                        ),\n                    ),\n                ],\n            ),\n        ]\n    )\n\n    # Check dependency.\n    detector = MigrationAutodetector(project_state, project_state)\n    self.assertTrue(\n        detector.check_dependency(\n            operations.RemoveField(model_name=model_name, name=field_name),\n            (app_label, model_name, field_name, False),\n        )\n    )\n    self.assertTrue(\n        detector.check_dependency(\n            operations.AlterField(\n                model_name=model_name,\n                name=field_name,\n                field=models.ManyToManyField(\n                    f\"{from_app_label}.{from_model_name}\",\n                    through=custom_through_model,\n                ),\n            ),\n            (app_label, model_name, field_name, \"alter\"),\n        )\n    )\n", "def test_alter_index_together_with_m2m(self):\n    changes = self.get_changes(\n        [self.book_with_multiple_authors],\n        [self.book_with_multiple_authors_through_attribution],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\n            \"AlterField\",\n            \"AlterIndexTogether\",\n        ],\n    )\n"], "sample_424": ["def test_references_field_by_m2m_target_model(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ManyToManyField(\"Other\")\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_alter_field_db_collation(self):\n    \"\"\"\n    AlterField operation changes the db_collation of the column.\n    \"\"\"\n    app_label = \"test_alter_field_db_collation\"\n    collation1 = connection.features.test_collations.get(\"non_default\")\n    collation2 = connection.features.test_collations.get(\"swedish_ci\")\n    if not collation1 or not collation2:\n        self.skipTest(\"Language collations are not supported.\")\n\n    operations = [\n        migrations.CreateModel(\n            \"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=20, db_collation=collation1)),\n            ],\n        ),\n    ]\n    project_state = self.apply_operations(app_label, ProjectState(), operations)\n    new_state = project_state.clone()\n    operation = migrations.AlterField(\n        \"Pony\", \"name\", models.CharField(max_length=20, db_collation=collation2)\n    )\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertColumnCollation(\"%s_pony\" % app_label, \"name\", collation2)\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    self.assertColumnCollation(\"%s_pony\" % app_label, \"name\", collation1)\n    # Deconstruction.\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"AlterField\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(\n        sorted(definition[2]),\n        [\"field\", \"model_name\", \"name\"],\n    )\n    # Ensure db_collation isn't mutated by deconstruct.\n    self.assertEqual(\n        definition[2][\"field\"].db_collation,\n        collation2,\n    )\n", "def test_references_field_by_expression(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.Expression(\"F('other__field')\"),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"field\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_rename_model_in_separate_database_and_state(self):\n    \"\"\"\n    RenameModel operation should also rename the database table\n    when used inside SeparateDatabaseAndState.\n    \"\"\"\n    app_label = \"test_rename_model_in_separate_database_and_state\"\n\n    database_operations = [\n        migrations.CreateModel(\n            \"Pony\",\n            [(\"id\", models.AutoField(primary_key=True))],\n        ),\n    ]\n    state_operations = [\n        migrations.CreateModel(\n            \"Pony\",\n            [(\"id\", models.AutoField(primary_key=True))],\n        ),\n    ]\n    operation = migrations.SeparateDatabaseAndState(\n        state_operations=state_operations, database_operations=database_operations\n    )\n\n    new_state = self.apply_operations(app_label, ProjectState(), operations=[operation])\n\n    rename_operation = migrations.SeparateDatabaseAndState(\n        state_operations=[migrations.RenameModel(\"Pony\", \"NewPony\")],\n        database_operations=[migrations.RenameModel(\"Pony\", \"NewPony\")],\n    )\n    self.apply_operations(app_label, new_state, operations=[rename_operation])\n\n    self.assertTableExists(\"%s_newpony\" % app_label)\n    self.assertTableNotExists(\"%s_pony\" % app_label)\n", "def test_reduce_model_option_operations(self):\n    operation = migrations.AlterModelOptions(\n        name=\"model\", options={\"managed\": False, \"verbose_name\": \"My model\"}\n    )\n    operation2 = migrations.AlterModelOptions(\n        name=\"model\", options={\"managed\": True, \"verbose_name\": \"My model\"}\n    )\n    reduced_operation = operation.reduce(operation2, app_label=None)\n    self.assertEqual(len(reduced_operation), 1)\n    self.assertIsInstance(reduced_operation[0], migrations.AlterModelOptions)\n\n    operation = migrations.AlterModelOptions(\n        name=\"model\", options={\"managed\": False, \"verbose_name\": \"My model\"}\n    )\n    operation2 = migrations.AlterModelOptions(name=\"model\", options={\"managed\": True})\n    reduced_operation = operation.reduce(operation2, app_label=None)\n    self.assertEqual(len(reduced_operation), 1)\n    self.assertIsInstance(reduced_operation[0], migrations.AlterModelOptions)\n\n    operation = migrations.AlterModelOptions(\n        name=\"model\", options={\"managed\": False, \"verbose_name\": \"My model\"}\n    )\n    operation2 = migrations.AlterModelOptions(name=\"model\", options={})\n    reduced_operation = operation.reduce(operation2, app_label=None)\n    self.assertEqual(len(reduced_operation), 1)\n    self.assertIsInstance(reduced_operation[0], migrations.AlterModelOptions)\n"], "sample_430": ["def test_renamed_field_with_f_expression_default(self):\n    class MyModel(models.Model):\n        a = models.IntegerField(unique=True)\n\n    model_state = ModelState(\n        \"testapp\",\n        \"MyModel\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"b\", models.IntegerField(unique=True)),\n        ],\n    )\n    model_state.options[\"default_permissions\"] = ()\n    changes = self.get_changes(\n        [ModelState.from_model(MyModel)], [model_state], MigrationQuestioner({\"ask_rename\": True})\n    )\n    self.assertEqual(len(changes), 1)\n    self.assertEqual(len(changes[\"testapp\"]), 1)\n    self.assertEqual(len(changes[\"testapp\"][0].operations), 1)\n    self.assertIsInstance(changes[\"testapp\"][0].operations[0], RenameField)\n", "def test_swappable_circular_mti(self):\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        project_state = self.make_project_state(\n            ModelState(\n                \"a\",\n                \"Base\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            ModelState(\n                \"a\",\n                \"User\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                bases=(\"a.Base\",),\n            ),\n        )\n        project_state.real_apps = {\"a\"}\n        with override_settings(AUTH_USER_MODEL=\"a.User\"):\n            changes = self.get_changes([project_state], [project_state.clone()])\n        self.assertEqual(len(changes), 0)\n", "def test_alter_model_table_with_m2m_through(self):\n    \"\"\"\n    Tests when a model with m2m through model changes its db_table,\n    autodetector must create two operations.\n    \"\"\"\n    contract = ModelState(\n        \"testapp\",\n        \"Contract\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"publisher\", models.ForeignKey(\"testapp.Publisher\", models.CASCADE)),\n        ],\n    )\n    author_with_m2m_through = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"publishers\",\n                models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Contract\"),\n            ),\n        ],\n    )\n    author_renamed_with_m2m_through = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"publishers\",\n                models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Contract\"),\n            ),\n        ],\n        options={\"db_table\": \"new_author\"},\n    )\n    changes = self.get_changes(\n        [contract, author_with_m2m_through],\n        [contract, author_renamed_with_m2m_through],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"AlterModelTable\", \"AlterField\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table=\"new_author\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, model_name=\"author\", name=\"publishers\"\n    )\n", "def test_alter_field_with_choices(self):\n    \"\"\"Tests detection of changed choices in fields.\"\"\"\n    changes = self.get_changes(\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\n                        \"name\",\n                        models.CharField(\n                            max_length=200, choices=[(\"A\", \"A\"), (\"B\", \"B\")]\n                        ),\n                    ),\n                ],\n            )\n        ],\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\n                        \"name\",\n                        models.CharField(\n                            max_length=200, choices=[(\"A\", \"A\"), (\"C\", \"C\")]\n                        ),\n                    ),\n                ],\n            )\n        ],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True\n    )\n", "def test_alter_model_table_comments_digits(self):\n    # The model table comment should be truncated to 2048 characters and end with a valid comment.\n    long_comment = 'a' * 2045 + '--Comment.'\n    book = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\"db_table_comment\": long_comment},\n    )\n\n    changes = self.get_changes([book], [book])\n\n    # No changes should be detected.\n    self.assertEqual(len(changes), 0)\n\n    # The comment should be shortened and end with a valid comment.\n    long_comment += 'a'\n    book_with_too_long_comment = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\"db_table_comment\": long_comment},\n    )\n\n    changes = self.get_changes([book], [book_with_too_long_comment])\n\n    self.assertEqual(len(changes), 1)\n    self.assertEqual(changes[\"otherapp\"][0].operations[0].table_comment, 'a' * 2045 + '-')\n"], "sample_439": ["def test_template_name_div(self):\n    class FrameworkForm(Form):\n        name = CharField()\n        language = ChoiceField(\n            choices=[(\"P\", \"Python\"), (\"J\", \"Java\")],\n            widget=RadioSelect,\n        )\n\n    f = FrameworkForm()\n    self.assertHTMLEqual(\n        f.render(f.template_name_div),\n        '<div><label for=\"id_name\">Name:</label><input type=\"text\" name=\"name\" '\n        'required id=\"id_name\"></div><div><fieldset><legend>Language:</legend>'\n        '<div id=\"id_language\"><div><label for=\"id_language_0\"><input type=\"radio\" '\n        'name=\"language\" value=\"P\" required id=\"id_language_0\">Python</label></div>'\n        '<div><label for=\"id_language_1\"><input type=\"radio\" name=\"language\" value=\"J\" '\n        'required id=\"id_language_1\">Java</label></div></div></fieldset></div>',\n    )\n    self.assertHTMLEqual(\n        f.render(f.template_name_div, label_suffix=\"\"),\n        '<div><label for=\"id_name\">Name</label><input type=\"text\" name=\"name\" '\n        'required id=\"id_name\"></div><div><fieldset><legend>Language</legend>'\n        '<div id=\"id_language\"><div><label for=\"id_language_0\"><input type=\"radio\" '\n        'name=\"language\" value=\"P\" required id=\"id_language_0\">Python</label></div>'\n        '<div><label for=\"id_language_1\"><input type=\"radio\" name=\"language\" value=\"J\" '\n        'required id=\"id_language_1\">Java</label></div></div></fieldset></div>',\n    )\n", "def test_get_context_with_renderer(self):\n    # Ensure that the get_context method returns a dictionary with 'form' and 'fields' keys\n    class TestForm(Form):\n        name = CharField()\n\n    form = TestForm()\n    context = form.get_context()\n    self.assertIn('form', context)\n    self.assertIn('fields', context)\n    self.assertEqual(context['form'], form)\n    self.assertEqual(len(context['fields']), 1)\n    self.assertEqual(context['fields'][0][0].name, 'name')\n", "    def test_template_rendering(self):\n        class CustomFrameworkForm(FrameworkForm):\n            template_name = \"forms_tests/form_snippet.html\"\n\n        f = CustomFrameworkForm()\n        self.assertHTMLEqual(\n            str(f),\n            \"\"\"\n            <div class=\"fieldWrapper\"><label for=\"id_name\">Name:</label>\n            <input type=\"text\" name=\"name\" required id=\"id_name\"></div>\n            <div class=\"fieldWrapper\"><label for=\"id_language\">Language:</label>\n            <input type=\"text\" name=\"language\" required id=\"id_language\"></div>\n            \"\"\",\n        )\n", "def test_custom_error_class(self):\n    class CustomErrorList(ErrorList):\n            return self.as_div()\n\n            if not self:\n                return \"\"\n            return '<div class=\"errorlist\">%s</div>' % (\n                \"\".join(\n                    '<div class=\"error\">%s</div>' % conditional_escape(e) for e in self\n                )\n            )\n\n    class CommentForm(Form):\n        name = CharField(max_length=50, required=False)\n        email = EmailField()\n        comment = CharField()\n\n    data = {\"email\": \"invalid\"}\n    f = CommentForm(data, auto_id=False, error_class=CustomErrorList)\n    self.assertHTMLEqual(\n        f.as_p(),\n        '<p>Name: <input type=\"text\" name=\"name\" maxlength=\"50\"></p>'\n        '<div class=\"errorlist\">'\n        '<div class=\"error\">Enter a valid email address.</div>'\n        \"</div><p>Email: \"\n        '<input type=\"email\" name=\"email\" value=\"invalid\" required></p>'\n        '<div class=\"errorlist\"><div class=\"error\">This field is required.</div></div>'\n        '<p>Comment: <input type=\"text\" name=\"comment\" required></p>',\n    )\n", "def test_get_context(self):\n    class FirstNameForm(Form):\n        first_name = CharField()\n\n    f = FirstNameForm()\n    context = f.get_context()\n    self.assertEqual(context[\"form\"], f)\n    self.assertEqual(context[\"fields\"], list(f))\n    self.assertEqual(context[\"hidden_fields\"], [])\n    self.assertEqual(context[\"errors\"], [])\n"], "sample_452": ["def test_alter_model_table_with_database_table(self):\n    \"\"\"\n    Tests the AlterModelTable operation when the model has a database table\n    specified.\n    \"\"\"\n    app_label = \"test_almtdbt\"\n    project_state = self.set_up_test_model(app_label, db_table=\"test_almtdbt_p\")\n    # Test the state alteration\n    operation = migrations.AlterModelTable(\"Pony\", None)\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    self.assertEqual(\n        new_state.models[app_label, \"pony\"].options[\"db_table\"],\n        f\"{app_label}_pony\",\n    )\n    # Test the database alteration\n    self.assertTableExists(\"test_almtdbt_p\")\n    self.assertTableNotExists(f\"{app_label}_pony\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertTableNotExists(\"test_almtdbt_p\")\n    self.assertTableExists(f\"{app_label}_pony\")\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    self.assertTableExists(\"test_almtdbt_p\")\n    self.assertTableNotExists(f\"{app_label}_pony\")\n", "def test_references_field_by_through_fields_with_self(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\n            \"self\", through=\"Through\", through_fields=(\"first\", \"second\"), symmetrical=False\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"first\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"second\", \"migrations\"), True\n    )\n", "def test_references_field_by_expression(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.CheckConstraint(check=models.Q(field__gt=0), name=\"check_field\"),\n    )\n    self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"other\", \"migrations\"), False)\n", "def test_reduce_model_operation(self):\n    \"\"\"FieldOperation.reduce() works with ModelOperation.\"\"\"\n    operation = migrations.AddField(\"Model\", \"field\", models.BooleanField(default=False))\n    model_operation = migrations.CreateModel(\"Model\", [(\"id\", models.AutoField(primary_key=True))])\n    result = operation.reduce([model_operation], app_label=\"migrations\")\n    self.assertEqual(result, [model_operation, operation])\n    self.assertModelEqual(result[0].model_name, \"Model\")\n\n    operation = migrations.AddField(\"Model\", \"field\", models.BooleanField(default=False))\n    model_operation = migrations.CreateModel(\"Model\", [(\"id\", models.AutoField(primary_key=True)), (\"field\", models.BooleanField(default=False))])\n    result = operation.reduce([model_operation], app_label=\"migrations\")\n    self.assertEqual(result, [model_operation])\n    self.assertModelEqual(result[0].model_name, \"Model\")\n\n    operation = migrations.RemoveField(\"Model\", \"field\")\n    model_operation = migrations.CreateModel(\"Model\", [(\"id\", models.AutoField(primary_key=True))])\n    result = operation.reduce([model_operation], app_label=\"migrations\")\n    self.assertEqual(result, [model_operation, operation])\n    self.assertModelEqual(result[0].model_name, \"Model\")\n\n    operation = migrations.RemoveField(\"Model\", \"field\")\n    model_operation = migrations.CreateModel(\"Model\", [(\"id\", models.AutoField(primary_key=True)), (\"field\", models.BooleanField(default=False))])\n    result = operation.reduce([model_operation], app_label=\"migrations\")\n    self.assertEqual(len(result), 1)\n    self.assertEqual(len(result[0].fields), 1)\n    self.assertModelEqual(result[0].model_name, \"Model\")\n\n    operation = migrations.RemoveField(\"Model\", \"field\")\n    model_operation = migrations.DeleteModel(\"Model\")\n    result = operation.reduce([model_operation], app_label=\"migrations\")\n    self.assertEqual(result, [model_operation])\n    self.assertModelEqual(result[0].model_name, \"Model\")\n", "def test_rename_index_with_condition(self):\n    app_label = \"test_rename_index_with_condition\"\n    operation = migrations.CreateModel(\n        \"Pony\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"pink\", models.IntegerField(default=3)),\n            (\"weight\", models.FloatField()),\n        ],\n        options={\"constraints\": [models.CheckConstraint(check=models.Q(pink__gt=2), name=\"pony_pink_gt_2\")]},\n    )\n    project_state = self.apply_operations(app_label, ProjectState(), operations=[operation])\n    # Rename index.\n    new_state = project_state.clone()\n    operation = migrations.RenameIndex(\n        \"Pony\",\n        new_name=\"pony_pink_idx\",\n        old_name=\"pony_pink_gt_2\",\n    )\n    operation.state_forwards(app_label, new_state)\n    # Add table.\n    self.assertTableNotExists(\"%s_pony\" % app_label)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertTableExists(\"%s_pony\" % app_label)\n    Pony = new_state.apps.get_model(app_label, \"Pony\")\n    Pony.objects.create(pink=1, weight=4.0)\n    Pony.objects.create(pink=3, weight=5.0)\n    Pony.objects.create(pink=3, weight=7.0)\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    Pony.objects.create(pink=2, weight=6.0)\n"], "sample_454": ["    def test_eq(self):\n        self.assertEqual(\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n            ),\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n            ),\n        )\n        self.assertEqual(\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n            ),\n            mock.ANY,\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n            ),\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"SPGiST\",\n            ),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n            ),\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"baz\", \"<\")],\n                index_type=\"GIST\",\n            ),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n            ),\n            1,\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n            ),\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n                deferrable=models.Deferrable.DEFERRED,\n            ),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n            ),\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\", \"<\")],\n                index_type=\"GIST\",\n                include=[\"baz\"],\n            ),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(\n                name=\"unique\",\n                expressions=[(\"foo\", \"=\"), (\"bar\",", "    def setUpTestData(cls):\n        cls.p1 = Product.objects.create(price=10, discounted_price=5)\n", "    def test_eq(self):\n        constraint1 = ExclusionConstraint(\n            name=\"exclusion\",\n            expressions=[(\"name\", \"=\"), (\"color\", \"=\")],\n        )\n        constraint2 = ExclusionConstraint(\n            name=\"exclusion\",\n            expressions=[(\"name\", \"=\"), (\"color\", \"=\")],\n        )\n        self.assertEqual(constraint1, constraint2)\n", "    def setUpTestData(cls):\n        cls.p1 = Product.objects.create(price=10, discounted_price=5)\n", "    def setUpTestData(cls):\n        cls.p1 = Product.objects.create(price=10, discounted_price=5)\n        cls.p2 = Product.objects.create(price=15, discounted_price=10)\n"], "sample_460": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.joepublicuser = User.objects.create_user(\n            username=\"joepublic\", password=\"secret\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n"], "sample_463": ["def test_check_constraint_rename(self):\n    model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        options={\n            \"constraints\": [\n                models.CheckConstraint(\n                    check=models.Q(name__contains=\"Bob\"), name=\"constraint_name\"\n                )\n            ]\n        },\n    )\n    model_state_renamed = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        options={\n            \"constraints\": [\n                models.CheckConstraint(\n                    check=models.Q(name__contains=\"Bob\"), name=\"renamed_constraint\"\n                )\n            ]\n        },\n    )\n    changes = self.get_changes([model_state], [model_state_renamed])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameConstraint\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        old_name=\"constraint_name\",\n        new_name=\"renamed_constraint\",\n    )\n", "def test_swappable_circular_dependency_with_intermediate_model(self):\n    \"\"\"\n    Swappable models that have circular dependencies with intermediate\n    models are correctly resolved.\n    \"\"\"\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        person = ModelState(\n            \"a\",\n            \"Person\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"parent1\",\n                    models.ForeignKey(\n                        settings.AUTH_USER_MODEL,\n                        models.CASCADE,\n                        related_name=\"children\",\n                    ),\n                ),\n            ],\n        )\n        parent = ModelState(\n            \"a\",\n            \"Parent\",\n            [(\"user\", models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE))],\n        )\n        user = ModelState(\n            \"a\",\n            \"User\",\n            [],\n            bases=(AbstractBaseUser, \"a.Parent\"),\n        )\n        changes = self.get_changes([], [person, parent, user])\n\n    self.assertNumberMigrations(changes, \"a\", 1)\n    self.assertOperationTypes(\n        changes, \"a\", 0, [\"CreateModel\", \"CreateModel\", \"CreateModel\", \"AddField\"]\n    )\n", "def test_suggest_name_truncation(self):\n    max_length = 52  # as set by suggest_name\n\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.AddConstraint(\n                model_name=\"a\" * (max_length // 2),\n                constraint=models.CheckConstraint(\n                    check=models.Q(x=1), name=\"some_name\" * (max_length // 2)\n                ),\n            )\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    self.assertEqual(len(migration.suggest_name()), max_length)\n", "def test_alter_model_table_with_db_table_and_model_change(self):\n    \"\"\"\n    Tests when model and db_table changes, and the original model had a custom\n    db_table, autodetector must create two operations.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_new_db_table_options],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"RenameModel\", \"AlterModelTable\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, name=\"newauthor\", table=\"author_three\"\n    )\n", "def test_alter_model_managers_with_nested_manager(self):\n    \"\"\"\n    Changing the model managers with a nested manager works correctly.\n    \"\"\"\n    changes = self.get_changes(\n        [self.other_pony],\n        [\n            ModelState(\n                \"otherapp\",\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                managers=[\n                    (\n                        \"food_qs\",\n                        FoodQuerySet.as_manager(),\n                    ),\n                    (\n                        \"food_mgr\",\n                        FoodManager(\"a\", \"b\"),\n                    ),\n                    (\n                        \"nested_food_mgr\",\n                        FoodManager(\n                            \"x\", \"y\", managers=[(\"inner\", FoodQuerySet.as_manager())]\n                        ),\n                    ),\n                ],\n            )\n        ],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterModelManagers\"])\n    self.assertEqual(\n        len(changes[\"otherapp\"][0].operations[0].managers), 3\n    )\n    self.assertEqual(\n        changes[\"otherapp\"][0].operations[0].managers[2][0], \"nested_food_mgr\"\n    )\n    self.assertIsInstance(\n        changes[\"otherapp\"][0].operations[0].managers[2][1], FoodManager\n    )\n    self.assertEqual(\n        len(changes[\"otherapp\"][0].operations[0].managers[2][1].managers), 1\n    )\n    self.assertEqual(\n        changes[\"otherapp\"][0].operations[0].managers[2][1].managers[0][\n            0\n        ],\n        \"inner\",\n    )\n    self.assertIsInstance(\n        changes[\"otherapp\"][0].operations[0].managers[2][1].managers[0][\n            1\n        ],\n        FoodQuerySet,\n    )\n"], "sample_491": ["def test_use_required_attribute_with_disabled_field(self):\n    class MyForm(Form):\n        use_required_attribute = True\n        f1 = CharField(max_length=30, disabled=True)\n        f2 = CharField(max_length=30, required=False, disabled=True)\n        f3 = CharField(widget=Textarea, disabled=True)\n        f4 = ChoiceField(choices=[(\"P\", \"Python\"), (\"J\", \"Java\")], disabled=True)\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_f1\">F1:</label>'\n        '<input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\" disabled></p>'\n        '<p><label for=\"id_f2\">F2:</label>'\n        '<input id=\"id_f2\" maxlength=\"30\" name=\"f2\" type=\"text\" disabled></p>'\n        '<p><label for=\"id_f3\">F3:</label>'\n        '<textarea cols=\"40\" disabled id=\"id_f3\" name=\"f3\" rows=\"10\"></textarea></p>'\n        '<p><label for=\"id_f4\">F4:</label> <select disabled id=\"id_f4\" name=\"f4\">'\n        '<option value=\"P\">Python</option>'\n        '<option value=\"J\">Java</option>'\n        \"</select></p>\",\n    )\n", "def test_radio_input_has_required_attribute(self):\n    class MyForm(Form):\n        use_required_attribute = True\n        f = ChoiceField(widget=RadioSelect, choices=[(1, \"one\"), (2, \"two\")])\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_f_0\">F:</label><div id=\"id_f\">'\n        '<div><label for=\"id_f_0\">'\n        '<input checked id=\"id_f_0\" name=\"f\" required type=\"radio\" value=\"1\">'\n        \"one</label></div><div><label for=\"id_f_1\">\"\n        '<input id=\"id_f_1\" name=\"f\" required type=\"radio\" value=\"2\">'\n        \"two</label></div></div></p>\",\n    )\n", "def test_field_with_widget_required(self):\n    class TestForm(Form):\n        field = CharField(widget=TextInput(attrs={\"required\": True}))\n\n    form = TestForm()\n    self.assertHTMLEqual(\n        str(form[\"field\"]), '<input type=\"text\" name=\"field\" required>'\n    )\n\n    class TestForm(Form):\n        field = CharField(required=False, widget=TextInput(attrs={\"required\": True}))\n\n    form = TestForm()\n    self.assertHTMLEqual(\n        str(form[\"field\"]), '<input type=\"text\" name=\"field\" required>'\n    )\n\n    class TestForm(Form):\n        field = CharField(required=False, widget=TextInput())\n\n    form = TestForm()\n    self.assertHTMLEqual(str(form[\"field\"]), '<input type=\"text\" name=\"field\">')\n", "def test_boundfield_use_fieldset(self):\n    class MyForm(Form):\n        field1 = CharField(widget=TextInput(attrs={\"use_fieldset\": True}))\n        field2 = CharField(widget=TextInput(attrs={\"use_fieldset\": False}))\n        field3 = CharField()\n\n    f = MyForm()\n    self.assertTrue(f[\"field1\"].use_fieldset)\n    self.assertFalse(f[\"field2\"].use_fieldset)\n    self.assertFalse(f[\"field3\"].use_fieldset)\n", "def test_boundfield_uses_template_from_renderer(self):\n    class CustomRenderer(DjangoTemplates):\n        field_template_name = \"forms_tests/custom_boundfield.html\"\n\n    class TestForm(Form):\n        field = CharField()\n\n    form = TestForm(renderer=CustomRenderer())\n    self.assertHTMLEqual(\n        str(form[\"field\"]),\n        '<label for=\"id_field\">Field:</label><p>Custom Field<p>'\n        '<input type=\"text\" name=\"field\" required id=\"id_field\">',\n    )\n\n    class CustomBoundFieldRenderer(DjangoTemplates):\n        field_template_name = \"forms_tests/custom_boundfield_render.html\"\n\n    class TestForm(Form):\n        field = CharField()\n\n    form = TestForm(renderer=CustomBoundFieldRenderer())\n    self.assertHTMLEqual(\n        form[\"field\"].render(),\n        '<label for=\"id_field\">Field:</label><p>Custom Field<p>'\n        '<input type=\"text\" name=\"field\" required id=\"id_field\">',\n    )\n"], "sample_536": ["def test_MultiCursor_scroll(horizOn, vertOn):\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    # useblit=false to avoid having to draw the figure to cache the renderer\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=horizOn, vertOn=vertOn\n    )\n\n    # mock a scroll event\n    event = mock_event(ax1, xdata=.5, ydata=.25, button='up')\n    multi.on_scroll(event)\n\n    # the lines in the first two ax should not move\n    for l in multi.vlines:\n        assert l.get_xdata() == (0, 0)\n    for l in multi.hlines:\n        assert l.get_ydata() == (0, 0)\n    # The relevant lines should still be invisible after scroll.\n    assert len([line for line in multi.vlines if line.get_visible()]) == 0\n    assert len([line for line in multi.hlines if line.get_visible()]) == 0\n", "def test_MultiCursor_clear():\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=True, vertOn=True\n    )\n\n    # mock a motion_notify_event\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n    \n    # clear MultiCursor\n    multi.clear()\n\n    # check if all lines are cleared (invisible)\n    for l in multi.vlines:\n        assert not l.get_visible()\n    for l in multi.hlines:\n        assert not l.get_visible()\n", "def test_lock_draw():\n    ax = get_ax()\n    lock = widgets.LockDraw()\n    assert lock.available(None)\n    assert not lock.locked()\n\n    lock(ax)\n    assert not lock.available(None)\n    assert lock.locked()\n\n    lock.release(ax)\n    assert lock.available(None)\n    assert not lock.locked()\n\n    with pytest.raises(ValueError):\n        lock.release(ax)\n", "def test_LassoSelector_empty(ax):\n    tool = widgets.LassoSelector(ax, onselect=noop)\n    tool.onrelease(mock_event(ax, xdata=100, ydata=100, button=1))\n    assert tool.verts is None\n", "def test_ellipse_selector_ax_removal():\n    fig, ax = plt.subplots()\n    tool = widgets.EllipseSelector(ax, onselect=noop)\n    assert len(fig.axes) == 1\n    ax.remove()\n    assert len(fig.axes) == 0\n    # Should be able to remove the tool without any exceptions\n    tool.remove()\n"], "sample_539": ["def test_ellipse_handle(ax):\n    tool = widgets.EllipseSelector(ax, onselect=noop, interactive=True,\n                                   grab_range=10)\n    tool.extents = (100, 150, 100, 150)\n\n    assert_allclose(tool.corners, ((100, 150, 150, 100), (100, 100, 150, 150)))\n    assert tool.extents == (100, 150, 100, 150)\n    assert_allclose(tool.edge_centers,\n                    ((100, 125.0, 150, 125.0), (125.0, 100, 125.0, 150)))\n    assert tool.extents == (100, 150, 100, 150)\n\n    # grab a corner and move it\n    click_and_drag(tool, start=(100, 100), end=(120, 120))\n    assert tool.extents == (120, 150, 120, 150)\n\n    # grab the center and move it\n    click_and_drag(tool, start=(132, 132), end=(120, 120))\n    assert tool.extents == (108, 138, 108, 138)\n\n    # create a new ellipse\n    click_and_drag(tool, start=(10, 10), end=(100, 100))\n    assert tool.extents == (10, 100, 10, 100)\n", "def test_MultiCursor_blit(ax):\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    multi = widgets.MultiCursor(None, (ax1, ax2), useblit=True)\n\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n\n    # make sure we have cached the background\n    assert multi._background is not None\n\n    # make sure we clear the background\n    multi.clear(event)\n    assert multi._background is None\n\n    # disconnect to clean up\n    multi.disconnect()\n", "def test_lasso_selector_empty(ax):\n    tool = widgets.LassoSelector(ax, onselect=noop)\n    assert tool.verts is None\n\n    do_event(tool, 'press', xdata=100, ydata=100, button=1)\n    assert tool.verts == [(100, 100)]\n\n    do_event(tool, 'release', xdata=100, ydata=100, button=1)\n    assert tool.verts is None\n", "def test_LassoSelector_empty(ax):\n    tool = widgets.LassoSelector(ax, onselect=noop)\n    do_event(tool, 'press', xdata=100, ydata=100, button=1)\n    do_event(tool, 'release', xdata=100, ydata=100, button=1)\n    assert tool.verts is None\n", "def test_MultiCursor_connect_disconnect(ax):\n    multi = widgets.MultiCursor(None, (ax,))\n    for cid in multi._cids.values():\n        assert cid\n    multi.disconnect()\n    for cid in multi._cids.values():\n        assert cid is None\n"], "sample_541": ["def test_check_buttons_frame_props(fig_test, fig_ref):\n    label_props = {'color': ['red'], 'fontsize': [24]}\n    frame_props = {'facecolor': 'green', 'edgecolor': 'blue', 'linewidth': 2}\n\n    widgets.CheckButtons(fig_ref.subplots(), ['tea', 'coffee'], [True, True],\n                         label_props=label_props, frame_props=frame_props)\n\n    cb = widgets.CheckButtons(fig_test.subplots(), ['tea', 'coffee'],\n                              [True, True])\n    cb.set_label_props(label_props)\n    # Setting the label size automatically increases default marker size, so we\n    # need to do that here as well.\n    cb.set_frame_props({**frame_props, 'sizes': (24 / 2)**2})\n", "def test_MultiCursor_with_toolmanager():\n    # test that MultiCursor works with toolmanager\n    fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True)\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=True, vertOn=True\n    )\n\n    # simulating pressing 't' to toggle MultiCursor with ToolManager\n    fig.canvas.manager.toolmanager.add_tool('MultiCursor', widgets.MultiCursor)\n    fig.canvas.manager.toolmanager.toggle_tool('MultiCursor')\n\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n    # force a draw + draw event to exercise clear\n    ax1.figure.canvas.draw()\n\n    # the lines in the first two ax should both move\n    for l in multi.vlines:\n        assert l.get_xdata() == (.5, .5)\n    for l in multi.hlines:\n        assert l.get_ydata() == (.25, .25)\n    # The relevant lines get turned on after move.\n    assert len([line for line in multi.vlines if line.get_visible()]) == 2\n    assert len([line for line in multi.hlines if line.get_visible()]) == 2\n", "def test_MultiCursor_with_toolmanager():\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    # useblit=false to avoid having to draw the figure to cache the renderer\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False\n    )\n\n    # Only two of the axes should have a line drawn on them.\n    assert len(multi.vlines) == 2\n    assert len(multi.hlines) == 2\n\n    # mock a motion_notify_event\n    # Can't use `do_event` as that helper requires the widget\n    # to have a single .ax attribute.\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n    # force a draw + draw event to exercise clear\n    ax1.figure.canvas.draw()\n\n    # the lines in the first two ax should both move\n    for l in multi.vlines:\n        assert l.get_xdata() == (.5, .5)\n    for l in multi.hlines:\n        assert l.get_ydata() == (.25, .25)\n\n    # test keypress\n    event = mock_event(ax1, key='control')\n    multi._on_key_press(event)\n    event = mock_event(ax1, key='shift')\n    multi._on_key_press(event)\n\n    # test key release\n    event = mock_event(ax1, key='control')\n    multi._on_key_release(event)\n    event = mock_event(ax1, key='shift')\n    multi._on_key_release(event)\n", "def test_LassoSelector_deprecation(ax):\n    with pytest.warns(\n        MatplotlibDeprecationWarning,\n            match=\"was deprecated in Matplotlib 3.6\"):\n        widgets.LassoSelector(ax, lambda *args: None)\n", "def test_polygon_selector_remove_all(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        # Polygon completed, now remove all verts.\n        *polygon_remove_vertex(*verts[0]),\n        *polygon_remove_vertex(*verts[1]),\n        *polygon_remove_vertex(*verts[2]),\n    ]\n    check_polygon_selector(event_sequence, [], 2,\n                           draw_bounding_box=draw_bounding_box)\n"], "sample_543": ["def test_LassoSelector_not_callable_onselect(ax):\n    with pytest.raises(TypeError):\n        widgets.LassoSelector(ax, None)\n", "def test_rectangle_selector_angle(ax):\n    tool = widgets.RectangleSelector(ax, onselect=noop, interactive=True)\n    click_and_drag(tool, start=(100, 100), end=(130, 140))\n    assert tool.extents == (100.0, 130.0, 100.0, 140.0)\n    assert len(tool._state) == 0\n\n    # Rotate anticlockwise using top-right corner\n    do_event(tool, 'on_key_press', key='r')\n    assert tool._state == {'rotate'}\n    assert len(tool._state) == 1\n    click_and_drag(tool, start=(130, 140), end=(120, 145))\n    do_event(tool, 'on_key_press', key='r')\n    assert len(tool._state) == 0\n    # Extents shouldn't change (as shape of rectangle hasn't changed)\n    assert tool.extents == (100.0, 130.0, 100.0, 140.0)\n    assert_allclose(tool.rotation, 25.56, atol=0.01)\n    tool.rotation = 45\n    assert tool.rotation == 45\n    # Corners should move\n    assert_allclose(tool.corners,\n                    np.array([[118.53, 139.75, 111.46, 90.25],\n                              [95.25, 116.46, 144.75, 123.54]]), atol=0.01)\n", "def test_LassoSelector_deprecation(ax):\n    with pytest.warns(\n        MatplotlibDeprecationWarning,\n            match=\"was deprecated in Matplotlib 3.6\"):\n        widgets.LassoSelector(ax, onselect=None)\n    with pytest.warns(\n        MatplotlibDeprecationWarning,\n            match=\"was deprecated in Matplotlib 3.6\"):\n        widgets.Lasso(ax, (0, 0), lambda *args: None)\n", "def test_cursor(ax):\n    cursor = widgets.Cursor(ax)\n    event = mock_event(ax, xdata=.5, ydata=.25)\n    cursor.onmove(event)\n    assert cursor.lineh.get_ydata() == (.25, .25)\n    assert cursor.linev.get_xdata() == (.5, .5)\n    assert len(cursor.lineh.get_xdata()) == 2\n    assert len(cursor.linev.get_ydata()) == 2\n", "def test_LassoSelector_update_background(ax):\n    tool = widgets.LassoSelector(ax, onselect=noop, useblit=True)\n    tool.update_background(None)\n    assert tool.background is not None\n"], "sample_559": ["def test_axes_divider_append_axes_with_autoadjust():\n    fig, ax = plt.subplots()\n    divider = make_axes_locatable(ax)\n    axs = {\n        \"main\": ax,\n        \"top\": divider.append_axes(\"top\", 1.2, pad=0.1, sharex=ax),\n        \"bottom\": divider.append_axes(\"bottom\", 1.2, pad=0.1, sharex=ax),\n        \"left\": divider.append_axes(\"left\", 1.2, pad=0.1, sharey=ax),\n        \"right\": divider.append_axes(\"right\", 1.2, pad=0.1, sharey=ax),\n    }\n    fig.canvas.draw()\n    make_axes_area_auto_adjustable(axs[\"right\"], pad=0.1)\n    fig.canvas.draw()\n", "def test_axesgrid_colorbar_log_smoketest_with_one_element_array():\n    fig = plt.figure()\n    grid = AxesGrid(fig, 111,  # modified to be only subplot\n                    nrows_ncols=(1, 1),\n                    ngrids=1,\n                    label_mode=\"L\",\n                    cbar_location=\"top\",\n                    cbar_mode=\"single\",\n                    )\n\n    Z = 10000 * np.random.rand(10, 10)\n    im = grid[0].imshow(Z, interpolation=\"nearest\", norm=LogNorm())\n\n    grid.cbar_axes[0].colorbar(im)\n", "def test_auto_adjustable_with_tight_layout():\n    fig = plt.figure()\n    ax = fig.add_axes([0, 0, 1, 1])\n    pad = 0.1\n    make_axes_area_auto_adjustable(ax, pad=pad)\n    fig.canvas.draw()\n    tbb = ax.get_tightbbox()\n    assert tbb.x0 == pytest.approx(pad * fig.dpi)\n    assert tbb.x1 == pytest.approx(fig.bbox.width - pad * fig.dpi)\n    assert tbb.y0 == pytest.approx(pad * fig.dpi)\n    assert tbb.y1 == pytest.approx(fig.bbox.height - pad * fig.dpi)\n\n    fig.tight_layout()\n    fig.canvas.draw()\n    tbb = ax.get_tightbbox()\n    assert tbb.x0 == pytest.approx(pad * fig.dpi)\n    assert tbb.x1 == pytest.approx(fig.bbox.width - pad * fig.dpi)\n    assert tbb.y0 == pytest.approx(pad * fig.dpi)\n    assert tbb.y1 == pytest.approx(fig.bbox.height - pad * fig.dpi)\n", "def test_axes_rgb():\n    fig = plt.figure()\n    ax = RGBAxes(fig, (0.1, 0.1, 0.8, 0.8), pad=0.1)\n    ax.imshow_rgb(np.arange(25).reshape(5, 5),\n                  np.arange(25).reshape(5, 5),\n                  np.arange(25).reshape(5, 5),\n                  interpolation='none')\n    fig.canvas.draw()\n    ax.remove()\n", "def test_axes_grid1_floating_point_row_ratio():\n    fig = plt.figure(figsize=(4, 4))\n    grid = AxesGrid(fig, 111, nrows_ncols=(3, 3),\n                    axes_pad=0.05, cbar_location=\"right\",\n                    cbar_mode=\"single\", cbar_size=\"7%\",\n                    cbar_pad=\"2%\")\n    Z = np.random.rand(11, 11)\n    for i in range(9):\n        im = grid[i].imshow(Z, interpolation=\"nearest\")\n    grid.cbar_axes[0].colorbar(im)\n"], "sample_564": ["def test_stem_change_markerline(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_ref = fig_ref.add_subplot(projection='3d')\n\n    theta = np.linspace(0, 2*np.pi)\n    x = np.cos(theta - np.pi/2)\n    y = np.sin(theta - np.pi/2)\n    z = theta\n\n    st = ax_test.stem(x, y, z, label='Stem 3D', linefmt='C0--',\n                      markerfmt='C0o', basefmt='C0.', use_line_collection=True)\n\n    st = ax_ref.stem(x, y, z, label='Stem 3D', linefmt='C0--',\n                     markerfmt='C0o', basefmt='C0.', use_line_collection=True)\n\n    st.markerline.set_marker('D')\n    st.markerline.set_markersize(5)\n\n    st_ref = ax_ref.stem(x, y, z, label='Stem 3D', linefmt='C0--',\n                         markerfmt='C0D', basefmt='C0.',\n                         use_line_collection=True, markersize=5)\n", "def test_zaxis_pane_color():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_zticks([])\n    ax.w_zaxis.set_pane_color('r')\n    ax.w_zaxis.set_pane_color((0, 1, 0, 0.5))\n", "def test_contourf_zdir_is_list_of_str(fig_test, fig_ref):\n    # Test contourf with zdir is a list of str.\n    x = np.linspace(-3.0, 3.0, 100)\n    y = np.linspace(-3.0, 3.0, 100)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-(X**2 + Y**2) / 2) / (2 * np.pi)\n    Z2 = (np.exp(-(((X - 1) / 1.5)**2 + ((Y - 1) / 0.5)**2) / 2) /\n          (2 * np.pi * 0.5 * 1.5))\n    Z = Z2 - Z1\n\n    ax_ref = fig_ref.add_subplot(1, 1, 1, projection=\"3d\")\n    ax_ref.contourf(X, Y, Z, zdir='z', offset=-1, cmap='viridis')\n    ax_ref.contourf(X, Y, Z, zdir='x', offset=-3, cmap='plasma')\n    ax_ref.contourf(X, Y, Z, zdir='y', offset=3, cmap='inferno')\n\n    ax_test = fig_test.add_subplot(1, 1, 1, projection=\"3d\")\n    ax_test.contourf(X, Y, Z, zdir=['z', 'x', 'y'], offset=[-1, -3, 3],\n                     cmap=['viridis', 'plasma', 'inferno'])\n", "def test_patch_collection_zorder(fig_test, fig_ref):\n    # Test that PatchCollection zorder is respected\n    fig_test = plt.figure()\n    ax_test = fig_test.add_subplot(projection='3d')\n    fig_ref = plt.figure()\n    ax_ref = fig_ref.add_subplot(projection='3d')\n\n    # Create patches\n    patches = [Circle((0, 0), 0.1), Circle((0.1, 0.1), 0.1)]\n\n    collection = art3d.Patch3DCollection(patches, zorder=1)\n    ax_test.add_collection3d(collection)\n\n    # Create individual patches to control the drawing order\n    for patch in patches:\n        patch = art3d.pathpatch_2d_to_3d(patch)\n        ax_ref.add_artist(patch)\n\n    # Set the view to ensure the circles overlap\n    for ax in (ax_test, ax_ref):\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n        ax.set_zlim(-1, 1)\n        ax.view_init(elev=0, azim=0)\n", "def test_stem_create_3D_lines(fig_test, fig_ref):\n    # Ensure stem3D creates 3D lines\n    fig, ax = fig_test.subplots(subplot_kw={'projection': '3d'})\n    ax.stem([1, 2, 3], [1, 2, 3], [1, 2, 3])\n\n    for line in ax.lines:\n        assert isinstance(line, art3d.Line3D)\n"], "sample_591": ["def test_merge_no_conflicts_broadcast(self):\n    ds1 = xr.Dataset({\"x\": (\"y\", [0])})\n    ds2 = xr.Dataset({\"x\": np.nan})\n    actual = ds1.merge(ds2, compat=\"no_conflicts\")\n    expected = xr.Dataset({\"x\": (\"y\", [0])})\n    assert expected.identical(actual)\n\n    ds1 = xr.Dataset({\"x\": (\"y\", [np.nan])})\n    ds2 = xr.Dataset({\"x\": 0})\n    actual = ds1.merge(ds2, compat=\"no_conflicts\")\n    assert expected.identical(actual)\n", "def test_merge_coordinates(self):\n    data = create_test_data()\n    ds1 = data[[\"var1\"]]\n    ds2 = data[[\"var3\"]]\n\n    expected = data[[\"var1\", \"var3\"]]\n    actual = ds1.merge(ds2, combine_attrs=\"override\")\n    assert actual.identical(expected)\n", "def test_merge_update_dataset(self):\n    data = create_test_data()\n    ds1 = data[[\"var1\"]]\n    ds2 = data[[\"var3\"]]\n    expected = data[[\"var1\", \"var3\"]]\n    ds1 = ds1.merge(ds2)\n    ds1.update(ds2)\n    assert expected.identical(ds1)\n", "def test_merge_no_conflicts_multi_var(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [np.nan, 3]), \"b\": (\"x\", [1, 2])})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 3]), \"b\": (\"x\", [np.nan, 2])}, {\"x\": [0, 1]})\n\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\"))\n", "def test_merge_no_conflicts_multidim(self):\n    ds1 = xr.Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]])})\n    ds2 = xr.Dataset({\"a\": ((\"x\", \"y\"), [[3, 4], [5, 6]])})\n    expected = xr.Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4], [5, 6]])})\n\n    with pytest.raises(xr.MergeError):\n        ds1.merge(ds2, compat=\"no_conflicts\")\n\n    ds2 = xr.Dataset({\"a\": ((\"x\", \"y\"), [[3, 4], [np.nan, 6]])})\n    with pytest.raises(xr.MergeError):\n        ds1.merge(ds2, compat=\"no_conflicts\")\n"], "sample_617": ["def test_cross_array_not_broadcastable() -> None:\n    a = xr.DataArray(np.array([1, 2, 3]), dims=[\"x\"])\n    b = xr.DataArray(np.array([4, 5]), dims=[\"y\"])\n    with pytest.raises(ValueError, match=r\"operands cannot be broadcast together\"):\n        xr.cross(a, b, dim=\"x\")\n", "def test_unify_chunks() -> None:\n    # Test with DataArrays\n    da1 = xr.DataArray(\n        np.random.rand(3, 4), dims=(\"x\", \"y\"), chunks={\"x\": 1, \"y\": 2}\n    )\n    da2 = xr.DataArray(\n        np.random.rand(3, 4), dims=(\"x\", \"y\"), chunks={\"x\": 2, \"y\": 2}\n    )\n    da3 = xr.DataArray(\n        np.random.rand(3, 4), dims=(\"x\", \"y\"), chunks={\"x\": 1, \"y\": 4}\n    )\n\n    da1_unified, da2_unified, da3_unified = unify_chunks(da1, da2, da3)\n\n    assert da1_unified.chunks == da2_unified.chunks == da3_unified.chunks\n    assert da1_unified.chunks == ((1, 1, 1), (2, 2))\n\n    # Test with Datasets\n    ds1 = xr.Dataset()\n    ds1[\"var1\"] = da1\n    ds1[\"var2\"] = da2\n\n    ds2 = xr.Dataset()\n    ds2[\"var1\"] = da2\n    ds2[\"var2\"] = da3\n\n    ds1_unified, ds2_unified = unify_chunks(ds1, ds2)\n\n    assert ds1_unified[\"var1\"].chunks == ds1_unified[\"var2\"].chunks\n    assert ds1_unified[\"var1\"].chunks == ds2_unified[\"var1\"].chunks\n    assert ds1_unified[\"var1\"].chunks == ds2_unified[\"var2\"].chunks\n    assert ds1_unified[\"var1\"].chunks == ((1, 1, 1), (2, 2))\n", "def test_cross_type_error() -> None:\n    # Test that cross errors out when inputs are not DataArray or Variable\n    with pytest.raises(TypeError):\n        xr.cross(1, 2, dim=\"x\")\n    with pytest.raises(TypeError):\n        xr.cross(xr.DataArray([1, 2]), 2, dim=\"x\")\n    with pytest.raises(TypeError):\n        xr.cross(1, xr.DataArray([1, 2]), dim=\"x\")\n    with pytest.raises(TypeError):\n        xr.cross(xr.Dataset(), xr.Dataset(), dim=\"x\")\n", "def test_cross_error() -> None:\n    # Error handling\n    da_a = xr.DataArray([1, 2, 3], dims=(\"x\"))\n    da_b = xr.DataArray([4, 5, 6], dims=(\"y\"))\n\n    # Dimension not on a\n    with pytest.raises(ValueError):\n        xr.cross(da_a, da_b, dim=\"y\")\n\n    # Dimension not on b\n    with pytest.raises(ValueError):\n        xr.cross(da_a, da_b, dim=\"x\")\n\n    # Dimension size > 3\n    da_a = xr.DataArray(np.arange(4), dims=(\"x\"))\n    da_b = xr.DataArray(np.arange(4), dims=(\"x\"))\n    with pytest.raises(ValueError):\n        xr.cross(da_a, da_b, dim=\"x\")\n\n    # Dimension size > 3 (dataset)\n    ds_a = xr.Dataset()\n    ds_a[\"x\"] = da_a\n    ds_b = xr.Dataset()\n    ds_b[\"x\"] = da_b\n    with pytest.raises(ValueError):\n        xr.cross(ds_a, ds_b, dim=\"x\")\n", "def test_cross_error_handling():\n    # Test error handling for cross function\n\n    # Test invalid dimension\n    a = xr.DataArray([1, 2, 3])\n    b = xr.DataArray([4, 5, 6])\n    with pytest.raises(ValueError, match=r\"Dimension 'y' not on a\"):\n        xr.cross(a, b, dim=\"y\")\n\n    # Test invalid size of dimension\n    a = xr.DataArray([1, 2])\n    b = xr.DataArray([4, 5, 6])\n    with pytest.raises(ValueError, match=r\"size of 'dim_0' on a must be\"):\n        xr.cross(a, b, dim=\"dim_0\")\n\n    # Test invalid size of dimension (array size 1 is allowed)\n    a = xr.DataArray([1])\n    b = xr.DataArray([4, 5, 6])\n    xr.cross(a, b, dim=\"dim_0\")  # This should not raise an error\n\n    # Test coordinates do not match\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"y\")\n    with pytest.raises(ValueError, match=r\"Dimension 'x' not on b\"):\n        xr.cross(a, b, dim=\"x\")\n\n    # Test broadcasting arrays with different sizes\n    a = xr.DataArray([1, 2], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n    with pytest.raises(ValueError, match=r\"The size of 'x' on a must be\"):\n        xr.cross(a, b, dim=\"x\")\n"], "sample_606": ["def test_cross_broadcasting_coords() -> None:\n    a = xr.DataArray(\n        [[1, 2], [3, 4]],\n        dims=[\"w\", \"cartesian\"],\n        coords=dict(\n            w=([\"w\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\"]),\n        ),\n    )\n    b = xr.DataArray(\n        [4, 5, 6],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    expected = np.cross(\n        np.array([[1, 2, 0], [3, 4, 0]]),\n        np.array([4, 5, 6]),\n        axis=-1,\n    )\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_crossRaiseMismatchDimCoords() -> None:\n    da = xr.DataArray(\n        np.random.random((3, 4)),\n        dims=[\"a\", \"cartesian\"],\n        coords=dict(\n            a=([\"a\"], [0, 1, 2]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\", \"w\"]),\n        ),\n    )\n    with pytest.raises(ValueError):\n        xr.cross(da, da, dim=\"cartesian\")\n", "def test_cross_broadcasting() -> None:\n    a = xr.DataArray(\n        np.array([[1, 2], [3, 4]]),\n        dims=(\"x\", \"cartesian\"),\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    b = xr.DataArray(\n        np.array([4, 5, 6]),\n        dims=(\"cartesian\",),\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n\n    expected = np.cross(\n        np.array([[[1, 2, 0], [3, 4, 0]]]),\n        np.array([[[4, 5, 6], [4, 5, 6]]]),\n        axis=-1,\n    )\n\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert actual.dims == (\"x\", \"cartesian\")\n    assert actual.shape == (2, 3)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_broadcasting() -> None:\n    a = xr.DataArray([1, 2, 3], dims=(\"x\",))\n    b = xr.DataArray([4, 5, 6], dims=(\"y\",))\n\n    expected = np.cross(a.values[:, np.newaxis], b.values[np.newaxis, :])\n    actual = xr.cross(a, b, dim=None)\n\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_errors() -> None:\n    a = xr.DataArray([1, 2, 3])\n    b = xr.DataArray([4, 5, 6, 7])\n\n    with pytest.raises(ValueError, match=r\"dimension 'x' not on a\"):\n        xr.cross(a, b, dim=\"x\")\n    with pytest.raises(ValueError, match=r\"dimension 'x' not on b\"):\n        xr.cross(xr.DataArray([1, 2, 3], dims=[\"x\"]), b, dim=\"x\")\n    with pytest.raises(ValueError, match=r\"The size of 'x' on a must be\"):\n        xr.cross(xr.DataArray([1, 2, 3, 4, 5], dims=[\"x\"]), b, dim=\"x\")\n    with pytest.raises(ValueError, match=r\"The size of 'x' on b must be\"):\n        xr.cross(a, b, dim=\"x\")\n"], "sample_720": ["def test_power_transformer_function_dtype():\n    X_int = np.abs(X_2d.astype(np.int64))\n    X_float32 = np.abs(X_2d.astype(np.float32))\n    X_float64 = np.abs(X_2d.astype(np.float64))\n\n    for X in [X_int, X_float32, X_float64]:\n        X_trans = power_transform(X)\n        assert X_trans.dtype == np.float64\n", "def test_power_transformer_function_equivalence():\n    X = np.abs(X_2d)\n\n    pt = PowerTransformer(method='box-cox', standardize=True)\n    X_trans_class = pt.fit_transform(X)\n    X_trans_func = power_transform(X, standardize=True)\n\n    assert_array_almost_equal(X_trans_class, X_trans_func)\n\n    X_inv_class = pt.inverse_transform(X_trans_class)\n    X_inv_func = power_transform(X_trans_func, standardize=True, inverse=True)\n\n    assert_array_almost_equal(X_inv_class, X_inv_func)\n", "def test_power_transformer_output_shape():\n    X = np.abs(X_2d)\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X_trans = pt.fit_transform(X)\n    assert X.shape == X_trans.shape\n    X_inv = pt.inverse_transform(X_trans)\n    assert X.shape == X_inv.shape\n", "def test_power_transformer_inverse_transform_ignores_lambdas():\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X = np.abs(X_2d)[:, 0:1]\n\n    pt.fit(X)\n    X_trans = pt.transform(X)\n\n    # Set lambdas to an arbitrary value\n    pt.lambdas_ = np.array([10])\n\n    # Check that inverse transform ignores the lambdas attribute\n    assert_array_almost_equal(pt.inverse_transform(X_trans), X)\n", "def test_power_transformer_function_invalid_input():\n    X = np.abs(X_2d)\n    # Test invalid method parameter\n    assert_raises_regex(ValueError, \"'method' must be one of\",\n                        power_transform, X, method='monty-python')\n    # Test invalid standardize parameter\n    assert_raises_regex(ValueError, \"'standardize' parameter only takes \"\n                        \"boolean values\", power_transform, X, standardize='42')\n"], "sample_744": ["def test_power_transformer_lambda_one():\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X = np.abs(X_2d)[:, 0:1]\n\n    # Test the lambda = 1 case\n    pt.lambdas_ = np.array([1])\n    X_trans = pt.transform(X)\n    assert_array_almost_equal(pt.inverse_transform(X_trans), X)\n", "def test_power_transformer_1d_warm_start():\n    X = np.abs(X_1col)\n\n    pt = PowerTransformer(method='box-cox', warm_start=True,\n                          standardize=True)\n\n    pt.fit(X[:10])\n    X_trans_1 = pt.transform(X)\n    lambda_1 = pt.lambdas_\n\n    pt.fit(X)\n    X_trans_2 = pt.transform(X)\n    lambda_2 = pt.lambdas_\n\n    assert_not_equal(lambda_1, lambda_2)\n    assert_not_equal(X_trans_1, X_trans_2)\n", "def test_power_transformer_constant_features():\n    X = np.abs(X_2d)\n    X[:, 0] = 1.0  # make first feature constant\n\n    pt = PowerTransformer(method='box-cox')\n\n    X_trans = pt.fit_transform(X)\n    assert_array_almost_equal(X_trans[:, 0], X[:, 0])\n\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n", "def test_power_transformer_incorrect_dtype():\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X = np.abs(X_2d).astype('object')\n\n    # Test that an exception is raised for incorrect dtype\n    assert_raises_regex(ValueError, \"The input data must be of type\",\n                        pt.fit, X)\n", "def test_power_transformer_clip():\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X = np.abs(X_2d)[:, 0:1]\n\n    # Test the clipping behavior\n    pt.lambdas_ = np.array([0])\n    X_trans_unclipped = pt.transform(X)\n\n    pt.clip = True\n    X_trans_clipped = pt.transform(X)\n\n    assert_array_less(X_trans_clipped, X_trans_unclipped)\n"], "sample_747": ["def test_power_transformer_sparse_input():\n    pt = PowerTransformer(method='box-cox', standardize=True)\n\n    # Positive sparse matrix\n    X_sparse_positive = sparse.csr_matrix(np.abs(X_2d))\n    X_trans_sparse = pt.fit_transform(X_sparse_positive)\n\n    X_trans_dense = pt.fit_transform(np.abs(X_2d))\n    assert_array_almost_equal(X_trans_sparse, X_trans_dense)\n\n    # Sparse matrix with negative values\n    X_sparse_negative = sparse.csr_matrix(X_2d)\n    assert_raises(ValueError, pt.fit, X_sparse_negative)\n", "def test_power_transformer_warning():\n    X = np.abs(X_2d)\n    pt = PowerTransformer(method='box-cox')\n\n    # An warning should be raised when standardize=True and a constant feature\n    # is found\n    X[:, 0] = 1.0\n    assert_warns_message(DataConversionWarning,\n                         \"No standardization will be performed for features\",\n                         pt.fit, X)\n", "def test_power_transformer_constant_feature():\n    pt = PowerTransformer(method='box-cox', standardize=True)\n    X = np.abs(X_2d)\n    X[:, 0] = 1.0  # make first feature constant\n\n    X_trans = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n", "def test_power_transformer_constant_feature():\n    # Test that PowerTransformer returns a constant when the input is constant\n    X = np.ones((100, 1))\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n        X_trans = pt.fit_transform(X)\n\n        if standardize:\n            assert_array_almost_equal(X_trans, np.zeros_like(X_trans))\n        else:\n            assert_array_almost_equal(X_trans, X)\n", "def test_power_transformer_sparse_data():\n    X_sparse = sparse.csr_matrix(np.abs(X_2d))\n    pt = PowerTransformer(method='box-cox')\n    assert_raises(TypeError, pt.fit, X_sparse)\n"], "sample_869": ["def test__check_set_wise_labels():\n    # Validate that _check_set_wise_labels correctly merges labels and finds\n    # the set of unique labels.\n    # The function is used by precision_recall_fscore_support,\n    # fbeta_score, and f1_score\n    y_true = [1, 2, 3]\n    y_pred = [3, 2, 1]\n    labels, present_labels = None, [1, 2, 3]\n\n    ret_labels = _check_set_wise_labels(y_true, y_pred, 'macro', labels, 1)\n    assert_array_equal(ret_labels, present_labels)\n\n    ret_labels = _check_set_wise_labels(y_true, y_pred, 'weighted', labels, 1)\n    assert_array_equal(ret_labels, present_labels)\n\n    ret_labels = _check_set_wise_labels(y_true, y_pred, 'micro', labels, 1)\n    assert_array_equal(ret_labels, present_labels)\n\n    # If labels and present_labels are identical, labels should still be\n    # returned\n    ret_labels = _check_set_wise_labels(y_true, y_pred, 'macro', present_labels, 1)\n    assert_array_equal(ret_labels, present_labels)\n\n    # If labels and present_labels have only one label in common, return it\n    # even if multiple labels are provided\n    labels = [1, 2, 3, 4, 5]\n    y_true = [1, 1, 1]\n    y_pred = [1, 1, 1]\n    ret_labels = _check_set_wise_labels(y_true, y_pred, 'macro', labels, 1)\n    assert_array_equal(ret_labels, [1])\n\n    # If labels are explicitly provided, return labels as is, even if it is\n    # a subset of present_labels\n    labels = [1]\n    y_true = [1, 2, 3]\n    y_pred = [3, 2, 1]\n    ret_labels = _check_set_wise_labels(y_true, y_pred, 'macro', labels, 1)\n    assert_array_equal(ret_labels, [1])\n\n    # If labels are explicitly provided and are empty, return the labels\n    # without adding the present labels\n    labels = []\n    y_true = [1, 2, 3]\n    y_pred = [3, 2, 1]\n    ret_labels =", "def test_jaccard_score_average_options():\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[0, 0, 1], [1, 0, 1]])\n\n    # 'macro' average\n    score = jaccard_score(y_true, y_pred, average='macro')\n    assert_almost_equal(score, 2/3)\n\n    # 'weighted' average\n    score = jaccard_score(y_true, y_pred, average='weighted')\n    assert_almost_equal(score, 5/6)\n\n    # 'samples' average\n    score = jaccard_score(y_true, y_pred, average='samples')\n    assert_almost_equal(score, 3/4)\n\n    # 'micro' average\n    score = jaccard_score(y_true, y_pred, average='micro')\n    assert_almost_equal(score, 3/5)\n\n    # list of labels\n    score = jaccard_score(y_true, y_pred, average=None)\n    assert_array_almost_equal(score, [1/2, 1, 1/2])\n\n    with pytest.raises(ValueError):\n        jaccard_score(y_true, y_pred, average='unknown')\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = [1, 0, 1, 0, 1, 0]\n    y_pred = [1, 1, 1, 0, 0, 1]\n    sample_weight = [1, 1, 2, 1, 1, 2]\n    macro_recall = recall_score(y_true, y_pred, average='macro',\n                                sample_weight=sample_weight)\n    with ignore_warnings():\n        balanced = balanced_accuracy_score(y_true, y_pred,\n                                           sample_weight=sample_weight)\n    assert balanced == pytest.approx(macro_recall)\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True,\n                                       sample_weight=sample_weight)\n    chance = balanced_accuracy_score(y_true, np.full_like(y_true, y_true[0]),\n                                     sample_weight=sample_weight)\n    assert adjusted == (balanced - chance) / (1 - chance)\n", "def test_brier_score_loss_sample_weight():\n    # Check brier_score_loss function with sample weights\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    sample_weight = np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n    true_score = np.average((y_true - y_pred) ** 2, weights=sample_weight)\n\n    assert_almost_equal(brier_score_loss(y_true, y_true, sample_weight=sample_weight), 0.0)\n    assert_almost_equal(brier_score_loss(y_true, y_pred, sample_weight=sample_weight), true_score)\n    assert_almost_equal(brier_score_loss(1. + y_true, y_pred, sample_weight=sample_weight), true_score)\n    assert_almost_equal(brier_score_loss(2 * y_true - 1, y_pred, sample_weight=sample_weight), true_score)\n", "def test_jaccard_score_binary_averaged():\n    y_true = np.array([0, 1, 1, 0, 1])\n    y_pred = np.array([1, 1, 1, 1, 0])\n    assert_almost_equal(jaccard_score(y_true, y_pred,\n                                      average='binary'), 2. / 5)\n    assert_almost_equal(jaccard_score(y_true, y_pred,\n                                      average='binary',\n                                      pos_label=0), 1. / 6)\n"], "sample_875": ["def test_balanced_accuracy_score_equal():\n    y_true = [0, 1, 0, 1, 0, 1]\n    y_pred = [0, 1, 0, 1, 0, 1]\n    assert balanced_accuracy_score(y_true, y_pred) == 1.0\n    assert balanced_accuracy_score(y_true, y_pred, adjusted=True) == 1.0\n", "def test_balanced_accuracy_score_average_parameter():\n    y_true, y_pred = [\"a\", \"b\", \"a\", \"b\"], [\"a\", \"a\", \"a\", \"b\"]\n    with pytest.raises(ValueError, match=\"average has to be one of\"):\n        balanced_accuracy_score(y_true, y_pred, average=\"weighted\")\n", "def test_balanced_accuracy_scoreSample_weight():\n    y_true = [1, 0, 1, 0]\n    y_pred = [1, 1, 0, 0]\n    sample_weight = [2, 1, 1, 2]\n\n    # we want to assert that sample_weight is doing what it should, i.e. it\n    # should be the same as repeating the true and predicted labels with\n    # the weights.\n    weighted = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    y_true_weighted = [y for y, w in zip(y_true, sample_weight) for _ in range(w)]\n    y_pred_weighted = [y for y, w in zip(y_pred, sample_weight) for _ in range(w)]\n    unweighted = balanced_accuracy_score(y_true_weighted, y_pred_weighted)\n\n    assert weighted == unweighted\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n    y_pred = [0, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n    sample_weight = [1, 1, 1, 1, 1, 1, 1, 1, 1, 10]\n    with ignore_warnings():\n        # Warnings are tested in test_balanced_accuracy_score_unseen\n        balanced = balanced_accuracy_score(\n            y_true, y_pred, sample_weight=sample_weight, adjusted=False\n        )\n        recall = recall_score(\n            y_true, y_pred, sample_weight=sample_weight, average=\"macro\"\n        )\n    assert balanced == pytest.approx(recall)\n", "def test_brier_score_loss_with_sample_weight():\n    # Check brier_score_loss function with sample weights\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1.0, 0.95])\n    sample_weight = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.5])\n    true_score = np.average((y_true - y_pred) ** 2, weights=sample_weight)\n\n    assert_almost_equal(brier_score_loss(y_true, y_true, sample_weight=sample_weight), 0.0)\n    assert_almost_equal(brier_score_loss(y_true, y_pred, sample_weight=sample_weight), true_score)\n"], "sample_906": ["def test_domain_cpp_ast_template_specialization():\n    check('class', 'template<> {key}A<int>', {2: 'IE1AIiE'})\n    check('function', 'template<> void A<int>(', {2: 'IE1AIiv'})\n    check('type', 'template<> {key}using A<int> = B', {2: 'IE1AIiE'}, key='using')\n    check('class', 'template<> {key}A<int, double>', {2: 'IE1AIidE'})\n    check('function', 'template<> void A<int, double>(', {2: 'IE1AIidv'})\n    check('type', 'template<> {key}using A<int, double> = B', {2: 'IE1AIidE'}, key='using')\n    check('class', 'template<> {key}A<0>', {2: 'IE1AL0E'})\n    check('function', 'template<> void A<0>(', {2: 'IE1AL0v'})\n    check('type', 'template<> {key}using A<0> = B', {2: 'IE1AL0E'}, key='using')\n\n    check('class', 'template<auto> {key}A<42>', {4: 'I_Ji42EEE1A'})\n    check('function', 'template<auto> void A<42>(', {4: 'I_Ji42EEv'})\n    check('type', 'template<auto> {key}using A<42> = B', {4: 'I_Ji42EEE1A'}, key='using')\n\n    check('class', 'template<auto...> {key}A<1, 2, 3>', {4: 'I_Ji1DpEEE1A'})\n    check('function', 'template<auto...> void A<1, 2, 3>(', {4: 'I_Ji1DpEEv'})\n    check('type', 'template<auto...> {key}using A<1, 2, 3> = B', {4: 'I_Ji1DpEEE1A'}, key='using')\n\n    check('class', 'template<int N> {key}A<{N}>;', {2: 'I_iE1AIXE'}),\n    check('class', 'template<int N> {key", "def test_domain_cpp_parse_template_introduction():\n    check('class', 'abc::ns::{id_0, id_1, id_2}', {2: 'I000EXN3abc2nsEI4id_04id_14id_2EE'})\n    check('class', 'abc::ns::{id_0, id_1, ...id_2}', {2: 'I00DpEXN3abc2nsEI4id_04id_1sp4id_2EE'})\n    check('class', 'abc::ns::{id_0, id_1, id_2} A<int, int>', {2: 'I000EXN3abc2nsEI4id_04id_14id_2EE1AIiiEE'})\n    check('class', 'abc::ns::{id_0, id_1, ...id_2} A<int, int...>', {2: 'I00DpEXN3abc2nsEI4id_04id_1sp4id_2EE1AIiDpiEE'})\n    check('class', 'abc::ns::{id_0, id_1, id_2} A<id_0, id_1, id_2>', {2: 'I000EXN3abc2nsEI4id_04id_14id_2EE1AI4id_04id_14id_2EE'})\n    check('class', 'abc::ns::{id_0, id_1, ...id_2} A<id_0, id_1, id_2...>', {2: 'I00DpEXN3abc2nsEI4id_04id_1sp4id_2EE1AI4id_04id_1Dp4id_2EE'})\n", "def test_domain_cpp_ast_identifier():\n    check('function', 'void f(int A::B::C)', {1: 'f__A::B::C', 2: '1fN1A1B1CE'})\n    check('function', 'void f(int ::A::B::C)', {1: 'f__A::B::C', 2: '1fN1A1B1CE'},\n          output='void f(int A::B::C)')\n    check('function', 'void f(int A::B)', {1: 'f__A::B', 2: '1fN1A1BE'})\n    check('function', 'void f(int A)', {1: 'f__A', 2: '1f1A'})\n    # anonymous union\n    check('function', 'void f(int @1::B)', {3: '1fv'}, asTextOutput='void f(int [anonymous]::B)')\n    check('function', 'void f(int @[A]::B)', {3: '1fv'}, asTextOutput='void f(int [anonymous]::B)')\n    # anonymous namespace\n    check('function', 'void f(int ::@1::B)', {3: '1fv'}, asTextOutput='void f(int ::[anonymous]::B)')\n    check('function', 'void f(int ::@[A]::B)', {3: '1fv'}, asTextOutput='void f(int ::[anonymous]::B)')\n    # anonymous global scope\n    check('function', 'void f(int ::@1)', {3: '1fv'}, asTextOutput='void f(int ::[anonymous])')\n    check('function', 'void f(int ::@[A])', {3: '1fv'}, asTextOutput='void f(int ::[anonymous])')\n    # anonymous local scope\n    check('function', 'void f(int @1)', {3: '1fv'}, asTextOutput='void f(int [anonymous])')\n    check('function', 'void f(int @[A])', {3: '1fv'}, asTextOutput='void f(int [anonymous])')\n", "def test_domain_cpp_ast_type_with_init():\n    check(\"type\", \"public bool b = 42\", {1: \"b\", 2: \"1b\"}, \"{key}bool b = 42\", key='typedef')\n    check(\"type\", \"{key}bool A::b = 42\", {1: \"A::b\", 2: \"N1A1bE\"}, key='typedef')\n    check(\"type\", \"{key}bool *b = 42\", {1: \"b\", 2: \"1b\"}, key='typedef')\n    check(\"type\", \"{key}bool *const b = 42\", {1: \"b\", 2: \"1b\"}, key='typedef')\n    check(\"type\", \"{key}bool *volatile const b = 42\", {1: \"b\", 2: \"1b\"}, key='typedef')\n    check(\"type\", \"{key}bool *volatile const *b = 42\", {1: \"b\", 2: \"1b\"}, key='typedef')\n    check(\"type\", \"{key}bool &b = 42\", {1: \"b\", 2: \"1b\"}, key='typedef')\n    check(\"type\", \"{key}bool b[] = 42\", {1: \"b\", 2: \"1b\"}, key='typedef')\n    check(\"type\", \"{key}std::pair<int, int> coord = 42\", {1: \"coord\", 2: \"5coord\"}, key='typedef')\n    check(\"type\", \"{key}long long int foo = 42\", {1: \"foo\", 2: \"3foo\"}, key='typedef')\n    check(\"type\", \"{key}std::vector<std::pair<std::string, long long>> module::blah = 42\",\n          {1: \"module::blah\", 2: \"N6module4blahE\"}, key='typedef')\n    check(\"type\", \"{key}std::function<void()> F = 42\", {1: \"F\", 2: \"1F\"}, key='typedef')\n    check(\"type\", \"{key}std::function<R(A1, A2)> F = 42\", {1: \"F\", 2: \"1F\"}, key='typedef')\n    check(\"type\", \"{key}std::function<R(A1, A2, A3)> F = 42\", {", "def test_domain_cpp_parse_mix_decl_order_matters(app, warning):\n    # Issue 8270\n    text = (\".. cpp:function:: void A()\\n\"\n            \".. cpp:struct:: A\\n\"\n            \".. cpp:function:: void A()\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[0]\n    assert \"Declaration is '.. cpp:function:: void A()'.\" in ws[1]\n    assert \"index.rst:2: WARNING: Duplicate C++ declaration, also defined at index:1.\" not in ws[2]\n    assert \"Declaration is '.. cpp:struct:: A'.\" not in ws[3]\n    assert ws[4] == \"\"\n"], "sample_907": ["def test_domain_cpp_build_template_introduction(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"template-introduction\")\n    assert len(ws) == 0\n", "def test_domain_cpp_build_std_prefix(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"index\")\n    assert len(ws) == 0\n\n", "def test_domain_cpp_parse_template_introduction():\n    text = (\".. cpp:class:: A\\n\"\n            \"   .. cpp:template-introduction:: C<T>\\n\"\n            \"   .. cpp:member:: a\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n", "def test_domain_cpp_parse_template_parameters_in_names(app, warning):\n    text = (\".. cpp:class:: A<X, Y>\\n\"\n            \".. cpp:class:: B<X, Y>::C\\n\"\n            \".. cpp:function:: void B<X, Y>::f()\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 1\n    assert ws[0] == \"\"\n", "def test_domain_cpp_parse_mix_decl_template(app, warning):\n    # Issue 8270\n    text = (\".. cpp:struct:: A<T>\\n\"\n            \".. cpp:function:: void A()\\n\"\n            \".. cpp:struct:: A<int>\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 1\n    assert ws[0] == \"\"\n"], "sample_911": ["def test_type_using():\n    check(\"type\", \"using T = int\", {2: \"1T\"}, output=\"typedef int T\")\n    check(\"type\", \"using namespace std\", {2: None}, output=\"using namespace std\")\n    check(\"type\", \"using std::vector\", {2: None}, output=\"using std::vector\")\n    check(\"type\", \"using std::vector<int>\", {2: None}, output=\"using std::vector<int>\")\n    check(\"type\", \"using std::vector<T>\", {2: None}, output=\"using std::vector<T>\")\n    check(\"type\", \"using T = std::vector<int>\", {2: \"1T\"}, output=\"typedef std::vector<int> T\")\n    check(\"type\", \"using T = std::vector<T>\", {2: \"1T\"}, output=\"typedef std::vector<T> T\")\n    check(\"type\", \"using T = std::vector<using T = int>\", {2: \"1T\"}, output=\"typedef std::vector<typedef int T> T\")\n\n    # check for error handling\n    with pytest.raises(DefinitionError):\n        parse(\"type\", \"using T\")\n    with pytest.raises(DefinitionError):\n        parse(\"type\", \"using T =\")\n    with pytest.raises(DefinitionError):\n        parse(\"type\", \"using T = ;\")\n", "def test_concept_parameter_list():\n    check(\"concept\", \"template<typename T1, typename T2> A::B::Concept\",\n          {2: \"I00EN1A1B7ConceptE\"},\n          output=\"template<typename T1, typename T2> concept A::B::Concept\")\n    check(\"concept\", \"template<typename A, typename B, typename ...C> Foo\",\n          {2: \"I00DpEN3FooE\"},\n          output=\"template<typename A, typename B, typename... C> concept Foo\")\n    check(\"concept\", \"template<template<typename> typename T> A::B::Concept\",\n          {2: \"II0E0EN1A1B7ConceptE\"},\n          output=\"template<template<typename> typename T> concept A::B::Concept\")\n    check(\"concept\", \"template<template<typename> typename ...T> A::B::Concept\",\n          {2: \"II0EDpEN1A1B7ConceptE\"},\n          output=\"template<template<typename> typename... T> concept A::B::Concept\")\n    check(\"concept\", \"template<auto... T> A::B::Concept\",\n          {2: \"I_AiDpEN1A1B7ConceptE\"},\n          output=\"template<auto... T> concept A::B::Concept\")\n    check(\"concept\", \"template<template<auto...> typename T> A::B::Concept\",\n          {2: \"II_AiDpE0EN1A1B7ConceptE\"},\n          output=\"template<template<auto...> typename T> concept A::B::Concept\")\n    check(\"concept\", \"template<template<auto...> typename ...T> A::B::Concept\",\n          {2: \"II_AiDpEDpEN1A1B7ConceptE\"},\n          output=\"template<template<auto...> typename... T> concept A::B::Concept\")\n", "def test_nested_declarators():\n    check('function', '(A *(*f)(C::*d))', {2: 'I1AIPFivEPM1CKivEEE'})\n    check('function', '(void (*f)(A::*d))', {2: 'IPFivEM1AivEEE'})\n    check('type', '(A *(*f)[10])', {2: 'PA10_IAIPFivEE'})\n    check('type', '(A *(*f)[10][10])', {2: 'PA10_A10_IAIPFivEEE'})\n", "def test_template_parameter_pack():\n    check('function', 'template<typename... T> void f(T... t)',\n          {2: 'IDpE1fvDp1T'})\n    check('function', 'template<typename... T> void f(T... t, int i)',\n          {2: 'IDpE1fvDp1Ti'})\n    check('function', 'template<typename... T> void f(int i, T... t)',\n          {2: 'IDpE1fviDp1T'})\n    check('function', 'template<typename... T> void f(T (&...t)[42])',\n          {2: 'IDpE1fvRA42_1T'})\n", "def test_template_parameter_pack():\n    check('class', 'template<typename... Ts> A', {2: 'IDpE1A'})\n    check('function', 'template<typename... Ts> void f(Ts... ts)',\n          {2: 'IDpEv1fI2TsEE'})\n    check('function', 'template<typename... Ts> void f(Ts... ts, int i)',\n          {2: 'IDpEv1fI2TsEi'})\n    check('function', 'template<typename... Ts> void f(int i, Ts... ts)',\n          {2: 'IDpEv1fiI2TsEE'})\n"], "sample_913": ["def test_pyattributedeprecated(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :deprecated:\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', 'deprecated')])\n", "def test_resolve_xref_for_data(app):\n    text = (\".. py:data:: MY_DATA\\n\"\n            \"   This is data.\\n\"\n            \"\\n\"\n            \"My data is here: :py:data:`MY_DATA`.\")\n    doctree = restructuredtext.parse(app, text)\n    refnodes = list(doctree.traverse(addnodes.pending_xref))\n    assert len(refnodes) == 1\n    assert_node(refnodes[0], addnodes.pending_xref,\n                refdomain='py', reftype='data', reftarget='MY_DATA',\n                refwarn=False, refexplicit=True)\n", "def test_desc_signature_class_name_with_prefix(app):\n    text = (\".. py:class:: mymodule.MyClass\\n\"\n            \"\\n\"\n            \"   .. py:method:: mymethod\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"mymodule.\"],\n                                                    [desc_name, \"MyClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][1],\n                ([desc_signature, ([desc_name, \"mymethod\"],\n                                   [desc_parameterlist, ()])],\n                 [desc_content, ()]))\n", "def test_get_objects():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n    domain.objects['mod'] = ('docname', 'node_id', 'module')\n    domain.objects['func'] = ('docname', 'node_id', 'function')\n    domain.modules['mod'] = ('docname', 'node_id', 'synopsis', 'platform', False)\n\n    objects = list(domain.get_objects())\n\n    assert len(objects) == 2\n    assert objects[0] == ('mod', 'mod', 'module', 'docname', 'node_id', 0)\n    assert objects[1] == ('func', 'func', 'function', 'docname', 'node_id', 1)\n", "def test_pyattribute_with_module(app):\n    text = (\".. py:attribute:: module_a.submodule.ModTopLevel.prop\\n\"\n            \"   :type: str\\n\"\n            \"   :value: ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, 'module_a.submodule.ModTopLevel.'],\n                                                    [desc_name, 'prop'],\n                                                    [desc_annotation, ': str'],\n                                                    [desc_annotation, \" = ''\"])],\n                                  [desc_content, ()])]))\n    assert 'module_a.submodule.ModTopLevel.prop' in domain.objects\n    assert domain.objects['module_a.submodule.ModTopLevel.prop'] == ('index', 'module_a.submodule.ModTopLevel.prop', 'attribute')\n"], "sample_917": ["def test_namespace_definitions():\n    check('namespace', 'A', {2: 'N1A'})\n    check('namespace', 'A::B', {2: 'N1A1B'})\n    check('namespace', 'A::B::C', {2: 'N1A1B1C'})\n    check('namespace', '::A', {2: 'N1A'})\n    check('namespace', '::A::B', {2: 'N1A1B'})\n    check('namespace', '::A::B::C', {2: 'N1A1B1C'})\n    check('namespace', 'std', {2: 'NSt3'})\n    check('namespace', 'std::string', {2: 'NSt7stringE'})\n    check('namespace', '::std::string', {2: 'NSt7stringE'})\n", "def test_template_param_pack_expansion_simple():\n    check('class', 'template<typename... T> A<T...>', {2: 'IDpE1AI2TE'})\n    check('class', 'template<typename... T> A<T &, ...>', {2: 'IDpE1ARI2TE'})\n    check('class', 'template<typename... T> A<T const..., T volatile &...>',\n          {2: 'IDpE1AKI2TEORI2TEE'})\n", "def test_type_using():\n    check('type', 'using T = A::B', {2: '1T'})\n    check('type', 'T = A::B', {2: '1T'})\n    check('type', 'using T = A::B::operator()', {2: '1T'})\n    check('type', 'T = A::B::operator()', {2: '1T'})\n    check('type', 'using T = A::B::operator bool', {2: '1T'})\n    check('type', 'T = A::B::operator bool', {2: '1T'})\n    check('type', 'T = A::operator()` would not be valid', None)\n    with pytest.raises(DefinitionError):\n        parse('type', 'T = A::operator()')\n", "def test_type_using():\n    check('type', 'A = B', {1: None, 2: '1A'}, output='A = B')\n    check('type', 'A = B::C', {1: None, 2: 'N1B1CE'}, output='A = B::C')\n    check('type', 'T = A::template B<int>::template C<double>', {1: None, 2: '1T'}, output='T = A::B<int>::C<double>')\n    check('type', 'T = Q<A::operator()>', {1: None, 2: '1T'}, output='T = Q<A::operator()>')\n    check('type', 'T = Q<A::operator()<int>>', {1: None, 2: '1T'}, output='T = Q<A::operator()<int>>')\n    check('type', 'T = Q<A::operator bool>', {1: None, 2: '1T'}, output='T = Q<A::operator bool>')\n", "def test_type_template_parameter_pack():\n    check('function', 'template<typename... T> void f()', {2: 'IDpEv1fv'})\n    check('function', 'template<typename T, typename... U> void f()', {2: 'I0DpEv1fv'})\n    check('function', 'template<typename... U, typename T> void f()', {2: 'IDp0Ev1fv'})\n    check('function', 'template<typename...> void f()', {2: 'IDpEv1fv'})\n    check('function', 'template<typename> void f()', {2: 'I0Ev1fv'})\n    check('class', 'template<typename... T> A', {2: 'IDpE1A'})\n    check('class', 'template<typename T, typename... U> A', {2: 'I0DpE1A'})\n    check('class', 'template<typename... U, typename T> A', {2: 'IDp0E1A'})\n    check('class', 'template<typename...> A', {2: 'IDpE1A'})\n    check('class', 'template<typename> A', {2: 'I0E1A'})\n"], "sample_923": ["def test_validate_expression():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(expr, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_expression()\n        parser.assert_end()\n        # validate_expression would usually be called by the ObjectDescription\n        return cppDomain.validate_expression(ast)\n\n    # valid expressions\n    assert parse(\"1 + 1\") is True\n    assert parse(\"1 + 1.5\") is True\n    assert parse(\"1 + 1.5f\") is True\n    assert parse(\"1 + 1.5L\") is True\n    assert parse(\"(int)1\") is True\n    assert parse(\"(int*)1\") is True\n    assert parse(\"(1 + 1) * 2\") is True\n    assert parse(\"a + b\") is True\n    assert parse(\"a + b * c\") is True\n    assert parse(\"a ? b : c\") is True\n\n    # invalid expressions\n    assert parse(\"\") is False\n    assert parse(\"1 1\") is False\n    assert parse(\"1 +\") is False\n    assert parse(\"+ 1\") is False\n    assert parse(\"(\") is False\n    assert parse(\")\") is False\n    assert parse(\"(1\") is False\n    assert parse(\"1)\") is False\n    assert parse(\"(1 + 1\") is False\n    assert parse(\"1 + 1)\") is False\n", "def test_macro_definitions():\n    check('macro', 'MY_MACRO(x, y) ( (x) + (y) )', {1: 'MY_MACRO', 2: '1MY_MACRO'})\n    check('macro', 'MY_MACRO(x, y) ( (x) + (y) )', {1: 'MY_MACRO', 2: '1MY_MACRO'},\n          output='{key}MY_MACRO(x, y) ((x) + (y))')\n\n    check('macro', 'MY_MACRO(x, ...) ( (x) + (__VA_ARGS__) )', {1: 'MY_MACRO', 2: '1MY_MACRO'})\n    check('macro', 'MY_MACRO(x, ...) ( (x) + (__VA_ARGS__) )', {1: 'MY_MACRO', 2: '1MY_MACRO'},\n          output='{key}MY_MACRO(x, ...) ((x) + (__VA_ARGS__))')\n\n    check('macro', 'MY_MACRO(x, y) ( (x) + (y) )', {1: 'MY_MACRO', 2: '1MY_MACRO'})\n    check('macro', 'MY_MACRO(x, y) ( (x) + (y) )', {1: 'MY_MACRO', 2: '1MY_MACRO'},\n          output='{key}MY_MACRO(x, y) ((x) + (y))')\n\n    check('macro', 'MY_MACRO (x, y) ( (x) + (y) )', {1: 'MY_MACRO', 2: '1MY_MACRO'})\n    check('macro', 'MY_MACRO (x, y) ( (x) + (y) )', {1: 'MY_MACRO', 2: '1MY_MACRO'},\n          output='{key}MY_MACRO (x, y) ((x) + (y))')\n", "def test_lookup():\n    # Create a C domain environment\n    env = BuildEnvironment()\n    env.domaindata['c'] = {}\n    env.domaindata['c']['root_symbol'] = Symbol(None, None, None, None)\n\n    # Set up parser\n    parser = DefinitionParser(\"content\", location=None, config=None)\n\n    # Parse \"namespace::myclass myclass\" and add to the root symbol\n    ast = parser.parse_declaration(\"class\", \"class\")\n    rootSymbol = env.domaindata['c']['root_symbol']\n    parent = rootSymbol.add_declaration(ast, \"docname\")\n\n    # Check that lookup finds the symbol\n    lookupResult = parent._symbol_lookup(ASTNestedName([ASTIdentifier(\"myclass\")], False),\n                                         lambda p, i: None,\n                                         ancestorLookupType=None,\n                                         matchSelf=True,\n                                         recurseInAnon=True,\n                                         searchInSiblings=True)\n    assert lookupResult.parentSymbol == parent\n    assert len(list(lookupResult.symbols)) == 1\n\n    # Check that we can find it using the prefix\n    lookupResult2 = parent._symbol_lookup(ASTNestedName([ASTIdentifier(\"myclass\")], True),\n                                          lambda p, i: None,\n                                          ancestorLookupType=None,\n                                          matchSelf=True,\n                                          recurseInAnon=True,\n                                          searchInSiblings=True)\n    assert lookupResult2.parentSymbol == parent\n    assert len(list(lookupResult2.symbols)) == 1\n", "def test_build_domain_cpp_use_parentheses_in_titles(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"use-parentheses-in-titles\")\n    assert len(ws) == 0\n", "def test_function_template_param_parsing():\n    check('function', 'template<template<typename> typename T> void f()',\n          {2: \"II0E0E1fv\", 4: \"II0E0E1fvv\"})\n    check('function', 'template<template<typename> typename T = A> void f()',\n          {2: \"II0E0E1fv\", 4: \"II0E0E1fvv\"})\n    check('function', 'template<template<typename> typename... T> void f()',\n          {2: \"II0EDpE1fv\", 4: \"II0EDpE1fvv\"})\n    check('function', 'template<template<typename> typename... T = A> void f()',\n          {2: \"II0EDpE1fv\", 4: \"II0EDpE1fvv\"})\n    check('function', 'template<template<typename> typename...> void f()',\n          {2: \"II0EDpE1fv\", 4: \"II0EDpE1fvv\"})\n\n    check('function', 'template<template<template<typename> typename> typename T> void f()',\n          {2: \"III0E0E0E1fv\", 4: \"III0E0E0E1fvv\"})\n    check('function', 'template<template<template<typename> typename> typename... T> void f()',\n          {2: \"III0E0EDpE1fv\", 4: \"III0E0EDpE1fvv\"})\n\n    check('function', 'template<template<int> typename T> void f()',\n          {2: \"II_iE0E1fv\", 4: \"II_iE0E1fvv\"})\n    check('function', 'template<template<int> typename... T> void f()',\n          {2: \"II_DpiE0E1fv\", 4: \"II_DpiE0E1fvv\"})\n\n    # Check that parsing works with multiple template parameter lists\n    check('function', 'template<template<typename, typename> typename T> void f()',\n          {2: \"II0E0E1fv\", 4: \"II0E0E1fvv\"})\n    check('function', 'template<template<typename, typename...> typename T> void"], "sample_919": ["def test_lookup_key():\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    # void ns1::ns2::func(int arg)\n    ns1Symbol = rootSymbol.add_name(\"ns1\", None)\n    ns2Symbol = ns1Symbol.add_name(\"ns2\", None)\n    funcSymbol = ns2Symbol.add_declaration(parse(\"function\", \"void func(int arg)\"), docname=\"TestDoc\")\n    # void ns1::ns2::ns3::func2(int arg)\n    ns3Symbol = ns2Symbol.add_name(\"ns3\", None)\n    func2Symbol = ns3Symbol.add_declaration(parse(\"function\", \"void func2(int arg)\"), docname=\"TestDoc\")\n\n    lookupKey = funcSymbol.get_lookup_key()\n    assert rootSymbol.direct_lookup(lookupKey) == funcSymbol\n    lookupKey.data[1] = (None, None, None)  # remove ns2\n    assert rootSymbol.direct_lookup(lookupKey) is None\n    lookupKey.data[1] = (ASTNestedNameElement(ASTIdentifier(\"ns2\"), None), None, None)  # restore ns2, but remove its id\n    assert rootSymbol.direct_lookup(lookupKey) == funcSymbol\n\n    lookupKey = func2Symbol.get_lookup_key()\n    assert rootSymbol.direct_lookup(lookupKey) == func2Symbol\n    lookupKey.data[2] = (None, None, None)  # remove ns3\n    assert rootSymbol.direct_lookup(lookupKey) is None\n    lookupKey.data[2] = (ASTNestedNameElement(ASTIdentifier(\"ns3\"), None), None, None)  # restore ns3, but remove its id\n    assert rootSymbol.direct_lookup(lookupKey) == func2Symbol\n\n    with pytest.raises(KeyError):\n        lookupKey.data[0] = (None, None, None)  # remove ns1\n        rootSymbol.direct_lookup(lookupKey)\n", "def test_xref_with_template_introduction():\n    # check that template introductions are correctly handled in cross-references\n    check('type', 'Concept{{T}} A<int>::B', {2: 'I0EX7ConceptI1TEEN1AIiE1BE'})\n    check('type', 'Concept{{T, U}} A<int, long>::B',\n          {2: 'I00EX7ConceptI1T1UEEN1AIiElE1BE'})\n", "def test_xref_fallbacks():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n        parser2 = DefinitionParser(target, location=None,\n                                   config=Config())\n        parser2.allowFallbackExpressionParsing = True\n        ast2, isShorthand2 = parser2.parse_xref_object()\n        parser2.assert_end()\n        assert isShorthand == isShorthand2\n        if ast is not None:\n            assert str(ast) == str(ast2)\n    check('f')\n    check('f()')\n    check('void f()')\n    check('T f()')\n    check('T<arg> f()')\n    check('T<arg, arg> f()')\n    check('T<arg, arg, arg> f()')\n    check('T<arg> f(arg)')\n    check('T<arg, arg> f(arg)')\n    check('T<arg, arg, arg> f(arg)')\n    check('T<arg> f(arg, arg)')\n    check('T<arg, arg> f(arg, arg)')\n    check('T<arg, arg, arg> f(arg, arg)')\n    check('T<arg> f(arg, arg, arg)')\n    check('T<arg, arg> f(arg, arg, arg)')\n    check('T<arg, arg, arg> f(arg, arg, arg)')\n", "def test_template_partial_specialization():\n    check('class', 'template<> {key}A<>', {2: 'IE1AIvE'})\n    check('class', 'template<> {key}A<int>', {2: 'IE1AIiE'})\n    check('class', 'template<> {key}A<int, double>', {2: 'IE1AIidE'})\n    check('class', 'template<> {key}A<int, double, void>', {2: 'IE1AIidvE'})\n    check('class', 'template<int I> {key}A<I>', {2: 'I_iE1AIvE'})\n    check('class', 'template<int I, int J> {key}A<I, J>', {2: 'I__iE1AIvE'})\n    check('class', 'template<int I, int J, int K> {key}A<I, J, K>', {2: 'I___iE1AIvE'})\n", "def test_template_introduction():\n    check('class', 'abc::ns::foo{{id_0, id_1, id_2}} {key}xyz::bar',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz3barE'})\n    check('class', 'abc::ns::foo{{id_0, id_1, ...id_2}} {key}xyz::bar',\n          {2: 'I00DpEXN3abc2ns3fooEI4id_04id_1sp4id_2EEN3xyz3barE'})\n    check('class', 'abc::ns::foo{{id_0, id_1, id_2}} {key}xyz::bar<id_0, id_1, id_2>',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz3barI4id_04id_14id_2EE'})\n    check('class', 'abc::ns::foo{{id_0, id_1, ...id_2}} {key}xyz::bar<id_0, id_1, id_2...>',\n          {2: 'I00DpEXN3abc2ns3fooEI4id_04id_1sp4id_2EEN3xyz3barI4id_04id_1Dp4id_2EE'})\n\n    check('type', 'abc::ns::foo{{id_0, id_1, id_2}} {key}xyz::bar = ghi::qux',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz3barE'}, key='using')\n    check('type', 'abc::ns::foo{{id_0, id_1, ...id_2}} {key}xyz::bar = ghi::qux',\n          {2: 'I00DpEXN3abc2ns3fooEI4id_04id_1sp4id_2EEN3xyz3barE'}, key='using')\n    check('function', 'abc::ns::foo{id_0, id_1, id_2} void xyz::bar()',\n         "], "sample_924": ["def test_template_parameter_pack():\n    check('class', 'template<typename... Ts> {key}C', {2: 'IDpE1C'})\n    check('function', 'template<typename... Ts> void f()', {2: 'IDpE1fv', 4: 'IDpE1fvv'})\n    check('member', 'template<typename... Ts> A a', {2: 'IDpE1a'})\n    check('type', \"template<typename... Ts> {key}a = A\", {2: 'IDpE1a'}, key='using')\n", "def test_partial_builds():\n    # Partial builds will create C++ objects without declarations.\n    # This test ensures that they can be properly merged with later declarations.\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    s1 = rootSymbol.add_name(\"S1\")\n    assert not s1.declaration\n\n    ast = parse(\"function\", \"void S1()\")\n    assert ast\n    s2 = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n    assert s2 == s1\n    assert s1.declaration\n", "def test_template_parameter_lookup(app, status, warning):\n    app.builder.build_all()\n\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    test = 'template-parameter-lookup.html'\n    output = (app.outdir / test).read_text()\n\n    # check links are generated\n    check(r'>A<', output, test)\n    check(r'>B<', output, test)\n    check(r'>T<', output, test)\n\n    # check there are no warnings about failed lookups\n    ws = filter_warnings(warning, test)\n    assert len(ws) == 0\n", "def test_template_parameter_lists():\n    check('class', 'template<typename T> {key}A', {2: 'I0E1A'})\n    check('class', 'template<class T> {key}A', {2: 'I0E1A'})\n    check('class', 'template<typename T, typename U> {key}A', {2: 'I00E1A'})\n    check('class', 'template<typename T, class U> {key}A', {2: 'I00E1A'})\n    check('class', 'template<typename... T> {key}A', {2: 'IDpE1A'})\n    check('class', 'template<class... T> {key}A', {2: 'IDpE1A'})\n    check('class', 'template<typename T = int> {key}A', {2: 'I0E1A'})\n    check('class', 'template<class T = int> {key}A', {2: 'I0E1A'})\n    check('class', 'template<typename T, typename U = int> {key}A', {2: 'I00E1A'})\n    check('class', 'template<typename T, class U = int> {key}A', {2: 'I00E1A'})\n\n    check('class', 'template<template<typename> typename T> {key}A', {2: 'II0E0E1A'})\n    check('class', 'template<template<typename> class T> {key}A', {2: 'II0E0E1A'})\n    check('class', 'template<template<typename, typename> typename T> {key}A', {2: 'II00E0E1A'})\n    check('class', 'template<template<typename, typename> class T> {key}A', {2: 'II00E0E1A'})\n    check('class', 'template<template<typename...> typename T> {key}A', {2: 'II0EDpE0E1A'})\n    check('class', 'template<template<typename...> class T> {key}A', {2: 'II0EDpE0E1A'})\n\n    check('class', 'template<int T> {key}A', {2: 'I_iE1A'})\n    check('class', 'template<int T,", "def test_fallback_parsing():\n        # call .format() on the expr to unescape double curly braces\n        idDict = {2: 'IE1CIA%s_1aE' % expr.format(), 3: 'IE1CIA%s_1aE' % id}\n        if id4 is not None:\n            idDict[4] = 'IE1CIA%s_1aE' % id4\n        check('class', 'template<> {key}C<a[%s]>' % expr, idDict)\n\n    # arithmetic\n    check('a + b * c', 'ml1b1cpl1a')\n    check('a / b + c * d', 'ml1c1dpl1a1b')\n    check('a / b + c / d', 'dvl1a1bpl1c1d')\n    check('a + b / c', 'pl1a1b1c')\n    check('a / (b + c)', 'dvl1a1bpl1cE')\n\n    # comparison\n    check('a < b < c', 'lt1a1blt1b1c')\n    check('(a < b) < c', 'ltlt1a1b1cE')\n\n    # logical\n    check('a || b && c', 'aa1aoo1b1c')\n    check('(a || b) && c', 'aaoo1a1b1cE')\n    check('a || (b && c)', 'oo1a1b1c')\n"], "sample_927": ["def test_template_param_pack_expansion():\n    check('function', 'template<typename... Ts> void f(Ts... ts, Ts...)',\n          {2: 'IDpE1fvDp1Ts'})\n", "def test_template_parameter_pack():\n    check('function', 'template<typename... Ts> void f(Ts... ts)',\n          {2: 'IDpE1fDp1Ts'})\n    check('function', 'template<typename T, typename... Ts> void f(T t, Ts... ts)',\n          {2: 'I0DpE1f1TTs'})\n", "def test_type_id_with_template_params():\n    check('function', 'void f(std::vector<int> v)', {2: \"1fNSt6vectorIiEE\"})\n    check('function', 'void f(std::vector<std::vector<int>> v)', {2: \"1fNSt6vectorINS_6vectorIiEEE\"})\n", "def test_all_perform_id_generation():\n    # Test that all object types can generate ids\n    for directiveName, objType in cppDomain.directives.items():\n        if directiveName in ('namespace', 'namespace-push', 'namespace-pop'):\n            continue\n        obj = objType(None)\n        obj.objtype = directiveName\n        ast = parse(obj.objtype, \"template<typename T> {key}A\")\n        id = ast.get_id(version=2)\n        assert id\n", "def test_template_introduction():\n    check('class', 'std::index_sequence<I...> {key}mySequence',\n          {2: 'I000ENSt13index_sequenceIDpE10mySequenceE'})\n    check('type', 'typename T {key}MyType = int',\n          {2: 'I_1TE6MyType'}, key='using')\n    check('class', 'typename T::type {key}MyType', {2: 'I_1TS_4typeE6MyType'})\n    check('function', 'typename T::type my_function()', {2: 'I_1TS_4typeE12my_functionEv',\n                                                         4: 'I_1TS_4typeE12my_functionEvv'})\n    check('member', 'typename T::type my_member', {2: 'I_1TS_4typeE9my_member'})\n"], "sample_926": ["def test_lookup_key():\n    root = Symbol(None, None, None, None)\n    s = root.add_name(ASTNestedName([ASTIdentifier(\"S\")], rooted=False))\n    a = s.add_name(ASTNestedName([ASTIdentifier(\"A\")], rooted=False))\n    b = s.add_name(ASTNestedName([ASTIdentifier(\"B\")], rooted=False))\n\n    # check that a lookup key for \"A\" in the global scope doesn't find anything\n    key = root._make_key([ASTIdentifier(\"A\")])\n    assert root.direct_lookup(key) is None\n\n    # check that a lookup key for \"A\" in the scope of S works\n    key = s._make_key([ASTIdentifier(\"A\")])\n    assert s.direct_lookup(key) == a\n\n    # check that we can use a lookup key across scopes\n    key = root._make_key([ASTIdentifier(\"S\"), ASTIdentifier(\"A\")])\n    assert root.direct_lookup(key) == a\n\n    # check that the reverse of a lookup key is equal to the scope path\n    # of the found symbol\n    assert key.reverse() == [root, s, a]\n\n    # check that we use the full scope name when equal identifiers are in the list\n    key = root._make_key([ASTIdentifier(\"S\"), ASTIdentifier(\"S\")])\n    assert root.direct_lookup(key) is None\n\n    # check that an empty lookup key finds the root\n    key = root._make_key([])\n    assert root.direct_lookup(key) == root\n", "def test_macro_definitions():\n    check(\"macro\", \"MACRO\", {1: \"MACRO\"}, output='{key}MACRO')\n    check(\"macro\", \"MACRO(arg)\", {1: \"MACRO\"})\n    check(\"macro\", \"MACRO(arg1, arg2)\", {1: \"MACRO\"})\n    check(\"macro\", \"MACRO(arg1, arg2, ...)\", {1: \"MACRO\"})\n", "def test_find_identifier():\n    root = Symbol(None, None, None, None)\n    s1 = root.add_name('A')\n    s2 = s1.add_name('b')\n\n    assert s2.find_identifier('A', matchSelf=False, recurseInAnon=True, searchInSiblings=False) is s1\n    assert s2.find_identifier('b', matchSelf=True, recurseInAnon=True, searchInSiblings=False) is s2\n    assert s2.find_identifier('b', matchSelf=False, recurseInAnon=True, searchInSiblings=False) is None\n\n    s3 = s2.add_name('A')\n\n    assert s3.find_identifier('A', matchSelf=True, recurseInAnon=True, searchInSiblings=False) is s3\n    assert s3.find_identifier('A', matchSelf=False, recurseInAnon=True, searchInSiblings=False) is s1\n\n    # create anonymous symbol\n    anon = s3.add_name('@anon')\n    s4 = anon.add_name('A')\n\n    assert s4.find_identifier('A', matchSelf=False, recurseInAnon=True, searchInSiblings=False) is s3\n    assert s4.find_identifier('A', matchSelf=False, recurseInAnon=False, searchInSiblings=False) is None\n", "def test_misuse_of_roles_with_fallback():\n    # Test that even with fallback parsing disabled, the misuse of role warnings are generated.\n\n        datadir = path.join(path.dirname(__file__), 'domain-cpp')\n        with open(path.join(datadir, file + '.rst'), encoding='utf-8') as f:\n            nodes = list(restructuredtext.parse(f.read()))\n        app = SphinxTester()\n        app.build_fixrefs = True\n        doctree = nodes[0]\n        app.env.prepare_settings('')\n        app.env.docnames.add(file)\n        app.env.prepare_document(file, doctree)\n        # warnings are now emitted in the second parse\n        doctree = app.env.get_and_resolve_doctree(file, doctree)\n        return doctree.warnings\n\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n        allowFallbackExpressionParsing = False\n\n    origConfig = cppDomain.cppConfig\n    cppDomain.cppConfig = Config\n\n    try:\n        warnings = find_warnings('roles-targets-warn')\n        # use a list so that we get an error message on unexpected count\n        warnMessage = [\"\"] * len(warnings)\n        for i in range(len(warnings)):\n            warnMessage[i] = str(warnings[i])\n        expectMessage = [\n            'cpp:class targets a function (test r\"function role targets function\"), perhaps use cpp:func instead?',\n            'cpp:class targets a member (test r\"class role targets member\"), perhaps use cpp:member instead?',\n            'cpp:class targets a variable (test r\"class role targets variable\"), perhaps use cpp:var instead?',\n            'cpp:class targets a type (test r\"class role targets type\"), perhaps use cpp:type instead?',\n            'cpp:class targets a concept (test r\"class role targets concept\"), perhaps use cpp:concept instead?',\n            'cpp:class targets an enum (test r\"class role targets enum\"), perhaps use cpp:enum instead?',\n            'cpp:class targets an enumerator (test r\"class role targets enumerator\"), perhaps use cpp:enumerator instead?',\n            'cpp:class targets a type parameter (test r\"class role targets type parameter\"), perhaps use cpp:classParam instead?',\n            'cpp:class targets a function parameter (test r\"class role targets function parameter\"), perhaps use cpp:functionParam instead?',\n\n            'cpp:struct targets a function (test r\"function role targets function\"), perhaps use cpp:", "def test_build_domain_c(app, status, warning):\n    app.builder.build_all()\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n"], "sample_934": ["def test_init_declarations():\n    check('class', '{key}A {1, 2}', {2: '1A'})\n    check('class', '{key}A {{{1, 2}}}', {2: '1A'})\n    check('class', '{key}A {1, 2,}', {2: '1A'})\n    check('class', '{key}A {{{1, 2,}}}', {2: '1A'})\n    check('class', '{key}A {{1, 2}} = {}', {2: '1A'})\n    check('class', '{key}A {{1, 2}} = {{}}', {2: '1A'})\n    check('class', '{key}A {{1, 2}} = {{1, 2}}', {2: '1A'})\n    check('class', '{key}A {{1, 2}} = {{1, 2,}}', {2: '1A'})\n", "def test_build_domain_c(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"domain-c\")\n    assert len(ws) == 0\n", "def test_inline_namespaces():\n    check('class', 'inline {key}A', {2: '1A'})\n    check('enum', 'inline {key}A', {2: '1A'})\n    check('function', 'inline {key}A f()', {2: '1AfEv'})\n", "def test_build_domain_c(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"c_domain\")\n    assert len(ws) == 0\n", "def test_id_prefixes():\n    assert len(_id_prefix) == _max_id + 1\n    for i in range(1, _max_id + 1):\n        assert _id_prefix[i] is not None\n"], "sample_938": ["def test_manpage_unknown_document(app, status, warning):\n    # test that a warning is emitted for an unknown document\n    app.config.man_pages = [('unknown', 'unknown', 'Unknown manual page', [], 1)]\n    app.build()\n    assert 'man_pages config value references unknown document' in warning.getvalue()\n", "def test_no_man_pages_config(app, status, warning):\n    app.build()\n    warnings = warning.getvalue().strip()\n    assert \"no \\\"man_pages\\\" config value found; no manual pages will be written\" in warnings\n", "def test_man_pages_config_empty(app, status, warning, monkeypatch):\n    monkeypatch.setattr(app.config, 'man_pages', [])\n    app.builder.init()\n    assert (\"no 'man_pages' config value found; no manual pages will be written\"\n            in warning.getvalue())\n", "def test_empty_authors(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert '.TH python 1' in content\n    assert '.SH NAME' in content\n    assert '.SH DESCRIPTION' in content\n", "def test_empty_authors(app, status, warning):\n    app.build()\n    assert (app.outdir / 'python.1').exists()\n    content = (app.outdir / 'python.1').read_text()\n    assert '.TH python 1' in content\n"], "sample_932": ["def test_self_substitution():\n    # regression test for #7044\n    check('function', \"template<typename T> void f(T*)\",\n          {2: \"I0E1fPT_E\", 3: \"I0E1fPT_E\", 4: \"I0E1fvPT_E\"})\n\n", "def test_nested_name_parsing(app, status, warning):\n    text = \"\"\"", "def test_anon_template_specializations():\n    check('function', 'template<> void A<int>::foo()', {2: 'IEN1AIiE3fooEv'})\n    check('function', 'template<int> void foo(A<>)', {2: 'I_iE3fooI1AEv'})\n    check('function', 'template<int> void foo(A<>, B<>)', {2: 'I_iE3fooI1AEI1BEv'})\n    check('function', 'void foo(A<>)', {2: '3fooI1AEv'})\n    check('function', 'void foo(A<>, B<>)', {2: '3fooI1AEI1BEv'})\n", "def test_warning_for_misused_qualified_names():\n    # Test that the parser raises a warning for qualified names used as template parameter names.\n    # This should be tested with different types of qualified names.\n        parser = DefinitionParser(input, location=None, config=None)\n        try:\n            parser.parse_declaration('templateParam', 'templateParam')\n        except DefinitionError as e:\n            assert False, str(e)\n        ws = filter_warnings(warning.getvalue(), \"warn-template-param-qualified-name\")\n        assert len(ws) == 1\n        assert \"WARNING: Using a qualified name as a template parameter name\" in ws[0]\n\n    warning = StringIO()\n    check('template<template<typename> class std::vector> class A;', warning)\n    check('template<template<typename> class A::vector> class A;', warning)\n    check('template<template<typename> class ::A::vector> class A;', warning)\n    check('template<template<typename> class NS::A::vector> class A;', warning)\n", "def test_template_parameter_packs():\n    check('function', 'void f(int...) {}', {2: '1fDpi'})\n    check('function', 'void f(int...) const {}', {2: 'NK1fDpiEv'})\n    check('function', 'void f(int...) & {}', {2: 'NR1fDpiEv'})\n    check('function', 'void f(int...) && {}', {2: 'NO1fDpiEv'})\n    check('function', 'void f(int...) noexcept {}', {2: '1fDpie'})\n    check('function', 'void f(int...) throw(std::runtime_error) {}', {2: '1fDpitNSt15runtime_errorEE'})\n\n    check('function', 'template<int... Ts> void f(Ts... ts) {}', {2: 'I_DpiE1fDp1TsEE'})\n    check('function', 'template<int... Ts> void f(Ts... ts) const {}', {2: 'NKI_DpiE1fDp1TsEEv'})\n    check('function', 'template<int... Ts> void f(Ts... ts) & {}', {2: 'NRI_DpiE1fDp1TsEEv'})\n    check('function', 'template<int... Ts> void f(Ts... ts) && {}', {2: 'NOI_DpiE1fDp1TsEEv'})\n    check('function', 'template<int... Ts> void f(Ts... ts) noexcept {}', {2: 'I_DpiE1fDp1TsEEe'})\n    check('function', 'template<int... Ts> void f(Ts... ts) throw(std::runtime_error) {}', {2: 'I_DpiE1fDp1TsEEtNSt15runtime_errorEE'})\n"], "sample_935": ["def test_resolve_xref(app, status, warning):\n    app.builder.build_all()\n\n        assert node.get('reftype') == 'any'\n        assert node.get('reftarget') == expected_reftarget\n        assert node.get('refdomain') == expected_refdomain\n\n    doctree = app.env.get_doctree('resolve_xref.rst')\n    for node in doctree.traverse(addnodes.pending_xref):\n        if node['reftype'] == 'any':\n            if node.get('cpp:parent_key') == 'MyClass':\n                check_xref(node, 'MyClass::member', 'cpp')\n            elif node.get('cpp:parent_key') == 'MyClass::MyNestedClass':\n                check_xref(node, 'MyClass::MyNestedClass::member', 'cpp')\n            elif node.get('cpp:parent_key') == '::MyClass':\n                check_xref(node, '::MyClass::member', 'cpp')\n            elif node.get('cpp:parent_key') == 'MyFunction':\n                check_xref(node, 'MyFunction::local', 'cpp')\n", "def test_parse_template_argument_list():\n    parser = DefinitionParser(\"A<B, C>\", location=None, config=None)\n    args = parser._parse_template_argument_list()\n    assert str(args) == \"<B, C>\"\n    assert len(args.args) == 2\n    assert args.args[0].name == \"B\"\n    assert args.args[1].name == \"C\"\n\n    parser = DefinitionParser(\"A<B, C, D>\", location=None, config=None)\n    args = parser._parse_template_argument_list()\n    assert str(args) == \"<B, C, D>\"\n    assert len(args.args) == 3\n    assert args.args[0].name == \"B\"\n    assert args.args[1].name == \"C\"\n    assert args.args[2].name == \"D\"\n\n    parser = DefinitionParser(\"A<\", location=None, config=None)\n    with pytest.raises(DefinitionError):\n        parser._parse_template_argument_list()\n\n    parser = DefinitionParser(\"A<B\", location=None, config=None)\n    with pytest.raises(DefinitionError):\n        parser._parse_template_argument_list()\n\n    parser = DefinitionParser(\"A<B, C\", location=None, config=None)\n    with pytest.raises(DefinitionError):\n        parser._parse_template_argument_list()\n", "def test_cross_references_with_anon_types(app, status, warning):\n    app.builder.build_all()\n\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    f = 'anon_class.html'\n    t = (app.outdir / f).read_text()\n    check(r'href=\"\\[^::]+::\\(anon_class\\.html#cpp:class::.+\\)\">', t, f)\n    check(r'href=\"\\[^::]+::\\(anon_class\\.html#cpp:class::.+\\)\">', t, f)\n    check(r'href=\"\\(.+\\.html#cpp:class::.+\\)\">', t, f)\n    check(r'href=\"\\(.+\\.html#cpp:class::.+\\)\">', t, f)\n    check(r'href=\"\\(.+\\.html#cpp:struct::.+\\)\">', t, f)\n    check(r'href=\"\\(.+\\.html#cpp:union::.+\\)\">', t, f)\n\n    f = 'anon_union.html'\n    t = (app.outdir / f).read_text()\n    check(r'href=\"\\[^::]+::\\(anon_union\\.html#cpp:union::.+\\)\">', t, f)\n    check(r'href=\"\\(.+\\.html#cpp:class::.+\\)\">', t, f)\n    check(r'href=\"\\(.+\\.html#cpp:struct::.+\\)\">', t, f)\n    check(r'href=\"\\(.+\\.html#cpp:union::.+\\)\">', t, f)\n\n    f = 'normal_class.html'\n    t = (app.outdir / f).read_text()\n    check(r'href=\"\\[^::]+::\\(normal_class\\.html#cpp:class::.+\\)\">', t, f)\n", "def test_template_parameter_packs():\n    check('class', \"template<typename... Ts> {key}A\", {2: \"IDpE1A\"})\n    check('class', \"template<int... Is> {key}A\", {2: \"I_DpiE1A\"})\n    check('class', \"template<int... Is> {key}A<Is...>\", {2: \"I_DpiE1AIE\"})\n    check('class', \"template<int... Is> {key}A<Is...>\",\n          {2: \"I_DpiE1AIE\", 3: \"I_DpiE1AIJX2IsEEE\"})\n    check('class', \"template<int... Is, typename... Ts> {key}A<Is...>\",\n          {2: \"I_Dp_DpiE1AIE\", 3: \"I_Dp_DpiE1AIJX2IsEEE\"})\n    check('class', \"template<int... Is, typename... Ts> {key}A<Ts...>\",\n          {2: \"I_Dp_DpiE1AIE\", 3: \"I_Dp_DpiE1AIJX2TsEEE\"})\n    check('class', \"template<int... Is, typename... Ts> {key}A<Is..., Ts...>\",\n          {2: \"I_Dp_DpiE1AIE\", 3: \"I_Dp_DpiE1AIJX2IsJX2TsEEE\"})\n    check('class', \"template<int... Is, typename... Ts> {key}A<Is..., Ts...>\",\n          {2: \"I_Dp_DpiE1AIE\", 3: \"I_Dp_DpiE1AIJX2IsJX2TsEEE\"})\n", "def test_xref_target_name_mangling():\n    class TestNode(nodes.Element):\n        tagname = 'cpp'\n\n    env = BuildEnvironment(app=None)\n    env.domains['cpp'] = CPPDomain(env)\n    env.temp_data['cpp:parent_symbol'] = None\n\n        node = TestNode()\n        xref = pending_xref(text, refdomain='cpp', reftype=role)\n        node.append(xref)\n        env.get_domain('cpp').resolve_xref(env, fromdocname='test', builder=None, typ=role,\n                                           target=text, node=xref, contnode=xref)\n        return xref\n\n        assert xref.attributes['refid'] == expected\n\n    # Plain name reference\n    xref = build_xref('MyType', 'type')\n    assert_xref_target(xref, '_CPPv51MyTypeE')\n\n    # Explicit scope resolution operator\n    xref = build_xref('std::vector<int>', 'type')\n    assert_xref_target(xref, '_CPPvNSt6vectorIiE')\n\n    # Template class parameter\n    xref = build_xref('std::__1::vector<int>', 'type')\n    assert_xref_target(xref, '_CPPvNSt3__16vectorIiE')\n\n    # Explicit template parameters\n    xref = build_xref('my_function<int>(int)', 'func')\n    assert_xref_target(xref, '_CPPv10my_functionIiEi')\n\n    # Template class with template parameter\n    xref = build_xref('std::vector<std::vector<int> >', 'type')\n    assert_xref_target(xref, '_CPPvNSt6vectorINS_6vectorIiEEE')\n\n    # Symbol enclosed in parentheses\n    xref = build_xref('(MyType)', 'type')\n    assert_xref_target(xref, '_CPPv51MyTypeE')\n\n    # Symbol enclosed in angle brackets\n    xref = build_xref('<MyType>', 'type')\n    assert_xref_target(xref, '_CPPv51MyTypeE')\n\n    # Multiple template parameter packs\n    xref = build_xref('my_function<int, std::vector<int>, ...>', 'func')\n    assert_xref_target(xref, '_CPPv10my_functionIJiNSt6vectorIiEEDpEE"], "sample_940": ["def test_unwrap_all_stop():\n            return func(*args, **kwargs)\n        return inner\n\n    @wrapper\n        pass\n\n        return isinstance(obj, types.FunctionType) and obj.__name__ == 'wrapper'\n\n    assert inspect.unwrap_all(func, stop=stop) is wrapper\n", "def test_getdoc_cls_for_staticmethod():\n    class Foo:\n        @staticmethod\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n        @staticmethod\n            pass\n\n    assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") is None\n    assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") == \"docstring.\"\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(func) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(not_singledispatch_func) is False\n\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(fun) is True\n    assert inspect.is_singledispatch_function(lambda x: x) is False\n", "def test_getdoc_partial_function():\n        \"\"\"docstring.\"\"\"\n\n    partial_func = functools.partial(func)\n\n    assert inspect.getdoc(partial_func) == \"docstring.\"\n    assert inspect.getdoc(partial_func, getattr, False) == \"docstring.\"\n    assert inspect.getdoc(partial_func, getattr, True) == \"docstring.\"\n"], "sample_941": ["def test_get_type_hints():\n        return True\n\n    type_hints = get_type_hints(my_function)\n    assert type_hints == {'x': int, 'y': str, 'return': bool}\n\n    class MyClass:\n            self.x = x\n            self.y = y\n\n    type_hints = get_type_hints(MyClass.__init__)\n    assert type_hints == {'x': int, 'y': str}\n\n        \"\"\"My function with annotations.\"\"\"\n        return True\n    my_function_with_annotations.__annotations__ = {'x': int, 'y': str, 'return': bool}\n\n    type_hints = get_type_hints(my_function_with_annotations)\n    assert type_hints == {'x': int, 'y': str, 'return': bool}\n\n    type_hints = get_type_hints(object)\n    assert type_hints == {}\n\n    type_hints = get_type_hints(BrokenType)\n    assert type_hints == {}\n", "def test_get_type_hints():\n        return 1.0\n\n    hints = get_type_hints(function_with_type_hints)\n    assert hints['a'] == int\n    assert hints['b'] == str\n    assert hints['return'] == float\n\n    class ClassWithMethod:\n            return 'hello'\n\n    hints = get_type_hints(ClassWithMethod.method)\n    assert hints['x'] == int\n    assert hints['return'] == str\n", "def test_restify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    forward_ref = ForwardRef(\"MyForwardRef\")\n    assert restify(forward_ref) == \":class:`MyForwardRef`\"\n", "def test_stringify_type_hints_Ellipsis():\n    assert stringify(Ellipsis) == \"...\"\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n"], "sample_939": ["def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_generic_visit():\n    class UnknownNode(ast.AST):\n        pass\n\n    with pytest.raises(NotImplementedError) as excinfo:\n        ast.unparse(UnknownNode())\n    assert 'Unable to parse UnknownNode object' in str(excinfo.value)\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n"], "sample_944": ["def test_is_system_TypeVar():\n    T = TypeVar('T')\n    assert is_system_TypeVar(T) is False\n    assert is_system_TypeVar(TypeVar) is True\n", "def test_restify_type_hints_Ellipsis():\n    assert restify(Ellipsis) == '...'\n    assert restify(List[Ellipsis]) == \":class:`List`\\\\ [...]\"\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(foo) == {'a': int, 'b': str, 'return': None}\n\n    class Bar:\n            pass\n\n    assert get_type_hints(Bar.__init__) == {'a': int, 'b': str, 'return': None}\n\n    # test with __annotations__ attribute\n        pass\n    baz.__annotations__ = {'a': str, 'b': int, 'return': str}\n\n    assert get_type_hints(baz) == {'a': str, 'b': int, 'return': str}\n\n    # test with broken class\n    class Broken:\n        __annotations__ = 'broken'\n\n    assert get_type_hints(Broken) == {}\n\n    # test with ForwardRef\n    from typing import ForwardRef\n    forward_ref = ForwardRef('int')\n    assert get_type_hints(forward_ref) == {}\n", "def test_stringify_type_hints_ForwardRef():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"myint\")) == \"myint\"\n    else:\n        from sphinx.util.typing import ForwardRef\n        assert stringify(ForwardRef(\"myint\", is_argument=True)) == \"myint\"\n", "def test_get_type_hints():\n        pass\n\n    hints = get_type_hints(func)\n    assert hints == {'x': int, 'y': str, 'return': float}\n\n    class MyClass:\n            pass\n\n    hints = get_type_hints(MyClass.method)\n    assert hints == {'x': int, 'y': str, 'return': float}\n\n    # Test that get_type_hints doesn't raise an error on runtime\n        pass\n\n    hints = get_type_hints(func_with_unknown_type)\n    assert hints == {'x': 'unknown'}\n\n    # Test that get_type_hints returns an empty dictionary for an object without __annotations__\n    obj = object()\n    hints = get_type_hints(obj)\n    assert hints == {}\n"], "sample_943": ["def test_file_suffix(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'example.py').write_text('')\n    apidoc_main(['-o', tempdir, '--suffix', '.txt', tempdir])\n    assert (outdir / 'example.txt').exists()\n\n    content = (outdir / 'example.txt').read_text()\n    assert content == (\"example module\\n\"\n                       \"==============\\n\"\n                       \"\\n\"\n                       \".. automodule:: example\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n\")\n", "def test_create_modules_toc_file(tempdir):\n    outdir = path(tempdir)\n    modules = ['module1', 'module2', 'module3']\n    opts = namedtuple('opts', 'header maxdepth docnames suffix destdir dryrun force quiet')(None, 4, [], 'rst', outdir, False, False, False)\n\n    create_modules_toc_file(modules, opts, 'tocfile')\n    assert (outdir / 'tocfile.rst').exists()\n\n    content = (outdir / 'tocfile.rst').read_text()\n    assert content == (\"Modules\\n\"\n                       \"=======\\n\"\n                       \"\\n\"\n                       \".. toctree::\\n\"\n                       \"   :maxdepth: 4\\n\"\n                       \"\\n\"\n                       \"   module1\\n\"\n                       \"   module2\\n\"\n                       \"   module3\\n\")\n", "def test_namespace_package_file_without_submodules(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'testpkg').makedirs()\n    apidoc_main(['--implicit-namespaces', '-o', tempdir, tempdir / 'testpkg'])\n    assert not (outdir / 'testpkg.rst').exists()\n", "def test_is_excluded(tempdir):\n    excludes = [tempdir / 'exclude_me']\n    (tempdir / 'exclude_me' / 'file.py').write_text('')\n    (tempdir / 'do_not_exclude_me' / 'file.py').write_text('')\n\n    assert is_excluded(tempdir / 'exclude_me' / 'file.py', excludes)\n    assert not is_excluded(tempdir / 'do_not_exclude_me' / 'file.py', excludes)\n", "def test_is_excluded(tempdir):\n    exclude_pattern = str(tempdir / 'exclude')\n    root = str(tempdir / 'root')\n\n    # Create directories\n    (tempdir / 'exclude').mkdir()\n    (tempdir / 'root').mkdir()\n\n    # Test excluded pattern\n    assert is_excluded(exclude_pattern, [exclude_pattern])\n    assert not is_excluded(root, [exclude_pattern])\n\n    # Test excluded pattern with trailing slash\n    assert is_excluded(exclude_pattern + '/', [exclude_pattern + '/'])\n    assert not is_excluded(root, [exclude_pattern + '/'])\n\n    # Test excluded pattern with glob\n    assert is_excluded(exclude_pattern, [str(tempdir / 'ex*')])\n    assert not is_excluded(root, [str(tempdir / 'ex*')])\n"], "sample_942": ["def test_python_domain_get_objects(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: method\\n\")\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    assert list(domain.get_objects()) == [\n        ('Class', 'Class', 'class', 'index', 'class-Class', 1),\n        ('example', 'example', 'module', 'index', 'module-example', 1),\n        ('Class.method', 'Class.method', 'method', 'index', 'method-Class.method', 1),\n    ]\n", "def test_info_field_list_return(app):\n    text = (\".. py:function:: func\\n\"\n            \"\\n\"\n            \"   :return: blah blah\\n\"\n            \"   :rtype: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, (desc_signature,\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Returns\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :return: + :rtype:\n    assert_node(doctree[1][1][0][0][1][0],\n                (\" -- \",\n                 \"blah blah\",\n                 \" \",\n                 [addnodes.literal_strong, \"Return type\"],\n                 \": \",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"]))\n    assert_node(doctree[1][1][0][0][1][0][5], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n", "def test_pyattribute_options(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str]\\n\"\n            \"      :value: ''\\n\"\n            \"      :annotation: =imals\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"Optional\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \"]\"])],\n                                                     [desc_annotation, \" = '' =imals\"])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][3], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n", "def test_resolve_xref_for_data(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'module.html').read_text()\n    assert ('Link to <a class=\"reference internal\" href=\"#module_a.submodule.ModTopLevel.prop\"'\n            ' title=\"module_a.submodule.ModTopLevel.prop\">'\n            '<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">'\n            'prop</span></code></a>' in content)\n", "def test_builtin_resolver(app):\n    text = (\".. py:function:: built_in\\n\"\n            \"   This function uses :py:class:`None` and :py:class:`list` and \"\n            \":py:class:`int` and also references non-existent :py:class:`Unknown` and \"\n            \"has a pending xref that is :any:`unknown <Unknown>`\")\n\n    doctree = restructuredtext.parse(app, text)\n    from sphinx.domains.python import builtin_resolver\n    for node in doctree.traverse(pending_xref):\n        result = builtin_resolver(app, app.env, node, node)\n        if node['reftarget'] in ['None', 'list', 'int']:\n            assert result is node[0]\n        else:\n            assert result is None\n"], "sample_945": ["def test_module_synopsis(app):\n    text = (\".. py:module:: example\\n\"\n            \"   :synopsis: Example module.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index))\n    assert doctree[1]['entries'] == [('pair', 'module; example', 'module-example', '', None)]\n    domain = app.env.get_domain('py')\n    assert 'example' in domain.modules\n    assert domain.modules['example'] == ('index', 'module-example', 'Example module.', '', False)\n", "def test_pyattribute_with_union_type_operator(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int | str\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][1][1][0][1],\n                ([desc_name, \"attr\"],\n                 [desc_annotation, (\": \",\n                                    [pending_xref, \"int\"],\n                                    \" \",\n                                    [desc_sig_punctuation, \"|\"],\n                                    \" \",\n                                    [pending_xref, \"str\"])]))\n", "def test_resolve_xref_for_variables(app):\n    text = (\".. py:variable:: version\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 1\\n\"\n            \"\\n\"\n            \".. py:variable:: other_version\\n\"\n            \"   :type: float\\n\"\n            \"   :value: 1.0\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    # Test resolving the cross reference for a variable\n    refnodes = list(doctree.traverse(pending_xref))\n    assert len(refnodes) == 2\n\n    assert_refnode(refnodes[0], None, None, 'version', 'data')\n    assert_refnode(refnodes[1], None, None, 'other_version', 'data')\n", "def test_get_index_text_with_canonical_name(app, status, warning):\n    text = (\".. py:class:: io.StringIO\\n\"\n            \"   :canonical: _io.StringIO\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    class_obj = domain.objects['io.StringIO']\n    assert domain.get_index_text('io', ('io.StringIO',)) is None\n    assert domain.get_index_text('io', class_obj) == 'StringIO (class in io)'\n", "def test_resolve_xref_for_deprecated_objects(app):\n    text = (\".. py:function:: foo()\\n\"\n            \"   :deprecated:\\n\"\n            \"\\n\"\n            \".. py:class:: Bar\\n\"\n            \"   :deprecated:\\n\"\n            \"\\n\"\n            \"   .. py:method:: baz()\\n\"\n            \"      :deprecated:\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    # foo()\n    refnode = pending_xref()\n    refnode['reftarget'] = 'foo'\n    refnode['refdomain'] = 'py'\n    refnode['reftype'] = 'func'\n    resolved = app.env.get_domain('py').resolve_xref(app.env, 'index', app.builder, 'func', 'foo', refnode, None)\n    assert_node(resolved[0], nodes.inline, classes=['deprecated'])\n\n    # Bar\n    refnode = pending_xref()\n    refnode['reftarget'] = 'Bar'\n    refnode['refdomain'] = 'py'\n    refnode['reftype'] = 'class'\n    resolved = app.env.get_domain('py').resolve_xref(app.env, 'index', app.builder, 'class', 'Bar', refnode, None)\n    assert_node(resolved[0], nodes.inline, classes=['deprecated'])\n\n    # Bar.baz()\n    refnode = pending_xref()\n    refnode['reftarget'] = 'Bar.baz'\n    refnode['refdomain'] = 'py'\n    refnode['reftype'] = 'meth'\n    resolved = app.env.get_domain('py').resolve_xref(app.env, 'index', app.builder, 'meth', 'Bar.baz', refnode, None)\n    assert_node(resolved[0], nodes.inline, classes=['deprecated'])\n"], "sample_947": ["def test_build_c_expression_role(app, warning):\n    text = \"\"\"", "def test_c_xref_role(app):\n    text = \"Some text :c:expr:`int i`.\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[0], nodes.inline,\n                classes=['xref', 'c', 'c-texpr'])\n    assert len(doctree[0]) == 1\n    assert_node(doctree[0][0], nodes.inline,\n                classes=['desc', 'desc_inline', 'inline'])\n", "def test_cexpr_role(app):\n    text = \"The expression :c:expr:`a + b` is a C expression.\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[0], desc_inline, expr=\"a + b\",\n                domain=\"c\", objtype=\"expr\")\n", "def test_printing_types():\n    # Test that types are printed correctly.\n    parser = DefinitionParser(\"int *i\", location=None, config=None)\n    ast = parser.parse_declaration(\"member\", \"member\")\n    symbol = Symbol(None, None, None, None, None)\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(\"\", '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    assert parentNode.astext() == \"int *i\"\n    # Test that the 'const' keyword is printed correctly.\n    parser = DefinitionParser(\"const int *i\", location=None, config=None)\n    ast = parser.parse_declaration(\"member\", \"member\")\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(\"\", '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    assert parentNode.astext() == \"const int *i\"\n    # Test that the 'volatile' keyword is printed correctly.\n    parser = DefinitionParser(\"volatile int *i\", location=None, config=None)\n    ast = parser.parse_declaration(\"member\", \"member\")\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(\"\", '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    assert parentNode.astext() == \"volatile int *i\"\n    # Test that the 'restrict' keyword is printed correctly.\n    parser = DefinitionParser(\"restrict int *i\", location=None, config=None)\n    ast = parser.parse_declaration(\"member\", \"member\")\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(\"\", '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    assert parentNode.astext() == \"restrict int *i\"\n    # Test that multiple keywords are printed correctly.\n    parser = DefinitionParser(\"const volatile int *i\", location=None, config=None)\n    ast = parser.parse_declaration(\"member\", \"member\")\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(\"\", '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    assert parentNode.astext() == \"const volatile int *i\"\n", "def test_cenum(app):\n    text = \"\"\""], "sample_946": ["def test_resolve_xref_for_data(app):\n    text = \".. py:data:: MY_DATA\"\n    doctree = restructuredtext.parse(app, text)\n    data_ref = find_pending_xref_condition(doctree.traverse(pending_xref)[0], 'resolved')\n    assert_node(data_ref, pending_xref, refdomain='py', reftype='data', reftarget='MY_DATA')\n\n    text = \".. py:data:: MY_DATA\\n   :type: int\"\n    doctree = restructuredtext.parse(app, text)\n    data_ref = find_pending_xref_condition(doctree.traverse(pending_xref)[0], 'resolved')\n    assert_node(data_ref, pending_xref, refdomain='py', reftype='data', reftarget='MY_DATA')\n    type_ref = find_pending_xref_condition(doctree.traverse(pending_xref)[1], 'resolved')\n    assert_node(type_ref, pending_xref, refdomain='py', reftype='class', reftarget='int')\n", "def test_info_field_list_raises(app):\n    text = (\".. py:function:: func\\n\"\n            \"\\n\"\n            \"   :raises ValueError: blah blah\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"function \"],\n                                                    [desc_name, \"func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0],\n                ([nodes.field_name, \"Raises\"],\n                 [nodes.field_body, nodes.paragraph]))\n    # :raises ValueError:\n    assert_node(doctree[1][1][0][0][1][0],\n                ([pending_xref, addnodes.literal_emphasis, \"ValueError\"],\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[1][1][0][0][1][0][0], pending_xref,\n                refdomain=\"py\", reftype=\"exc\", reftarget=\"ValueError\")\n", "def test_module_index_entry(app, status, warning):\n    text = (\".. py:module:: example\\n\"\n            \"   :platform: Unix, Windows\\n\"\n            \"   :synopsis: This is a test module.\\n\"\n            \"   :deprecated:\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    entries = index.generate()[0][0][1]\n    assert len(entries) == 1\n    entry = entries[0]\n    assert entry.name == 'example'\n    assert entry.docname == 'index'\n    assert entry.qualname == 'module-example'\n    assert entry.anchor == ''\n    assert entry.extra == 'Deprecated'\n    assert entry.subtype == 0\n    assert entry.description == 'This is a test module. (Unix, Windows)'\n", "def test_pyattribute_with_union_type_operator(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int | str\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][1][1],\n                ([desc_name, \"attr\"],\n                 [desc_annotation, (\": \",\n                                    [pending_xref, \"int\"],\n                                    \" \",\n                                    [desc_sig_punctuation, \"|\"],\n                                    \" \",\n                                    [pending_xref, \"str\"])]))\n", "def test_info_field_list_with_property(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :property str name: blah blah\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, (desc_signature,\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Properties\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :property str name:\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"name\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[1][1][0][0][1][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\", **{\"py:class\": \"Class\"})\n"], "sample_949": ["def test_custom_man_page_settings(app, status, warning):\n    app.build()\n    assert (app.outdir / 'custom.1').exists()\n    content = (app.outdir / 'custom.1').read_text()\n    assert 'Custom Manual Page' in content\n    assert 'Custom Author' in content\n", "def test_no_man_pages_configured(app, status, warning):\n    app.build()\n    warnings = warning.getvalue().strip()\n    assert \"no \\\"man_pages\\\" config value found; no manual pages will be written\" in warnings\n", "def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'URL: ' in content\n", "def test_custom_man_page(app, status, warning):\n    app.build()\n    assert (app.outdir / 'test.2').exists()\n    content = (app.outdir / 'test.2').read_text()\n    assert 'Test man page' in content\n    assert '2' in content\n", "def test_man_pages_config(app, status, warning):\n    app.build()\n    assert (app.outdir / 'foo.2').exists()\n    content = (app.outdir / 'foo.2').read_text()\n    assert '.TH FOO 2' in content\n"], "sample_950": ["def test_module_full_qualified_name():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # non-module references\n    node = nodes.reference()\n    assert domain.get_full_qualified_name(node) is None\n\n    # simple module reference\n    node = nodes.reference(reftarget='module')\n    assert domain.get_full_qualified_name(node) == 'module'\n\n    # with py:module context\n    kwargs = {'py:module': 'package'}\n    node = nodes.reference(reftarget='module', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'package.module'\n", "def test_get_full_qualified_name_with_module_and_class():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    node = nodes.reference(reftarget='func', py_module='module1', py_class='Class')\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n", "def test_pydecorator_function(app):\n    text = (\".. py:decorator:: my_decorator\\n\"\n            \"   :module: my_module\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_addname, \"my_module.\"],\n                                                    [desc_name, \"my_decorator\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'my_module.my_decorator' in domain.objects\n    assert domain.objects['my_module.my_decorator'] == ('index', 'my_module.my_decorator', 'function', False)\n", "def test_handle_signature_with_class_prefix(app):\n    text = \".. py:method:: foo.bar()\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"foo.bar\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1][0], addnodes.desc_signature, names=[\"foo.bar\"])\n\n    text = \".. py:method:: .foo.bar()\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"foo.bar\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1][0], addnodes.desc_signature, names=[\"foo.bar\"])\n\n    text = \".. py:method:: ~foo.bar()\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"bar\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1][0], addnodes.desc_signature, names=[\"foo.bar\"])\n", "def test_get_full_qualified_name_with_module_and_class():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    node = nodes.reference(reftarget='func', py_module='module1', py_class='Class')\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n"], "sample_948": ["def test_build_domain_cpp_template_parameter_list(app, status, warning):\n    text = (\".. cpp:class:: template <typename T, typename U> Test\\n\"\n            \"   :tparam T: Template type parameter T.\\n\"\n            \"   :tparam U: Template type parameter U.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, addnodes.index, desc, addnodes.field_list))\n    assert_node(doctree[2], desc, objtype=\"class\", desctype=\"class\",\n                declname=\"Test\", fullname=\"Test\")\n    assert_node(doctree[2][0], desc_signature, first=False)\n    assert_node(doctree[2][1], addnodes.desc_content)\n    assert_node(doctree[2][1][0], addnodes.field_list, fields=[\n        (\"tparam\", [\"Template type parameter T.\"], \"T\"),\n        (\"tparam\", [\"Template type parameter U.\"], \"U\"),\n    ])\n    # check that the template parameter list is there and has the expected elements\n    assert_node(doctree[2][0][0], addnodes.desc_signature_line, text='template <typename T, typename U>')\n    assert_node(doctree[2][0][1], addnodes.desc_signature_item, text='Test')\n", "def test_template_lists():\n    # Test explicit template lists.\n    check('class', 'template<int T1, int T2> {key}A<T1, T2>',\n          {2: 'I_iI_iE1AIiE'})\n    check('class', 'template<int T1, int T2> {key}A<T2, T1>',\n          {2: 'I_iI_iE1AIiE'})\n    # Test implicit template lists.\n    check('class', 'template<int T1, int T2> {key}A',\n          {2: 'I_iI_iE1A'})\n    # Test mixed template lists.\n    check('class', 'template<int T1, int T2> {key}A<T2>',\n          {2: 'I_iI_iE1AIiE'})\n    # Test missing template parameters.\n    with pytest.raises(DefinitionError):\n        parse('class', 'template<int T1, int T2> {key}A<T1, T2, T3>')\n    # Test extra template parameters.\n    check('class', 'template<int T1, int T2> {key}A<T1>', {2: 'I0I_iI_iE1AIiE'})\n    check('class', 'template<int T1, int T2> {key}A<>', {2: 'I0I_iI_iE1A'})\n    check('class', 'template<int T1, int T2> {key}A<>{}', {2: 'I0I_iI_iE1A'})\n", "def test_build_domain_cpp_template_parameter_pack(app, status, warning):\n    text = (\".. cpp:class:: A\\n\"\n            \"   .. cpp:member:: template<typename... Ts> void f(Ts... ts)\\n\"\n            \".. cpp:class:: B\\n\"\n            \"   .. cpp:member:: template<typename... Ts> void f(Ts... ts)\\n\")\n    restructuredtext.parse(app, text)\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"index\")\n    assert len(ws) == 0\n", "def test_template_parameter_pack_in_function_parameter(app, status, warning):\n    text = \".. cpp:function:: void f(std::vector<Ts...> v)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, addnodes.index, desc, addnodes.index))\n    assert_node(doctree[0], desc, objtype='function', desctype='function')\n    assert_node(doctree[0][0], addnodes.desc_signature, first=False)\n", "def test_build_domain_cpp_templateParamInQualifiedLookups(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"templateParamInQualified\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:class reference target not found: B<T>::foo\" in ws[0]\n    assert \"WARNING: cpp:function reference target not found: B<T>::foo::bar\" in ws[1]\n"], "sample_951": ["def test_getall():\n    class Foo:\n        __all__ = ['a', 'b']\n\n    class Bar:\n        __all__ = ('c', 'd')\n\n    class Baz:\n        __all__ = ['invalid', 123, None]\n\n    class Qux:\n        pass\n\n    assert inspect.getall(Foo) == ['a', 'b']\n    assert inspect.getall(Bar) == ['c', 'd']\n    with pytest.raises(ValueError):\n        inspect.getall(Baz)\n    assert inspect.getall(Qux) is None\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    fun.register(int, lambda x: x)\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(normal_function) is False\n\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert inspect.getmro(A) == (A, object)\n    assert inspect.getmro(B) == (B, A, object)\n    assert inspect.getmro(C) == (C, B, A, object)\n\n    # Check that getmro doesn't crash on non-class objects\n    assert inspect.getmro(object()) == tuple()\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    class TestClass:\n        @singledispatch\n            return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n    assert inspect.is_singledispatch_function(TestClass().method) is False\n", "def test_getmro():\n    class Base:\n        pass\n\n    class Derived(Base):\n        pass\n\n    class MultiDerived(Base, object):\n        pass\n\n    assert inspect.getmro(Base) == (Base, object)\n    assert inspect.getmro(Derived) == (Derived, Base, object)\n    assert inspect.getmro(MultiDerived) == (MultiDerived, Base, object)\n\n    with pytest.raises(TypeError):\n        inspect.getmro(123)\n"], "sample_952": ["def test_getmro():\n    class A: pass\n    class B(A): pass\n    class C(B): pass\n\n    assert inspect.getmro(A) == (A, object)\n    assert inspect.getmro(B) == (B, A, object)\n    assert inspect.getmro(C) == (C, B, A, object)\n", "def test_getdoc_partial():\n        \"\"\"\n        docstring\n            indented text\n        \"\"\"\n\n    func2 = functools.partial(func1, 1)\n    func2.__doc__ = \"partial docstring\"\n\n    assert inspect.getdoc(func2) == \"partial docstring\"\n\n    # Test getdoc with allow_inherited=True\n    func2.__doc__ = None\n    assert inspect.getdoc(func2, allow_inherited=True) == func1.__doc__\n\n    # Test getdoc with allow_inherited=False\n    assert inspect.getdoc(func2, allow_inherited=False) is None\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig, {'int': int, 'str': str})\n    assert evaluated_sig.parameters['a'].annotation == int\n    assert evaluated_sig.parameters['b'].annotation == int\n    assert evaluated_sig.return_annotation == str\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    mro = inspect.getmro(C)\n    assert mro == (C, B, A, object)\n\n    mro = inspect.getmro(object)\n    assert mro == (object,)\n\n    mro = inspect.getmro(int)\n    assert mro == (int, object)\n\n    with pytest.raises(TypeError):\n        inspect.getmro(\"not a class\")\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(not_singledispatch_fun) is False\n\n"], "sample_953": ["def test_valid_dir(tempdir):\n    d = {'path': tempdir}\n    assert qs.valid_dir(d) is True\n\n    with open(tempdir / 'Makefile', 'w') as f:\n        f.write('dummy content')\n    assert qs.valid_dir(d) is False\n\n    d = {'path': tempdir, 'sep': True}\n    assert qs.valid_dir(d) is True\n\n    (tempdir / 'source').mkdir()\n    with open(tempdir / 'source' / 'conf.py', 'w') as f:\n        f.write('dummy content')\n    assert qs.valid_dir(d) is False\n", "def test_valid_dir(tempdir):\n    d = {'path': tempdir, 'sep': False, 'dot': '_', 'master': 'index', 'suffix': '.rst'}\n    assert qs.valid_dir(d)\n\n    # Directory does not exist\n    d['path'] = tempdir / 'nonexistent'\n    assert qs.valid_dir(d)\n\n    # Directory exists but is a file\n    (tempdir / 'file_dir').touch()\n    d['path'] = tempdir / 'file_dir'\n    assert not qs.valid_dir(d)\n\n    # Directory contains Makefile\n    (tempdir / 'Makefile').touch()\n    d['path'] = tempdir\n    assert not qs.valid_dir(d)\n\n    # Directory contains make.bat\n    (tempdir / 'make.bat').touch()\n    d['path'] = tempdir\n    assert not qs.valid_dir(d)\n\n    # Directory contains conf.py\n    (tempdir / 'conf.py').touch()\n    d['path'] = tempdir\n    assert not qs.valid_dir(d)\n\n    # Directory contains master document\n    (tempdir / 'index.rst').touch()\n    d['path'] = tempdir\n    assert not qs.valid_dir(d)\n\n    # Directory contains _static and _templates\n    (tempdir / '_static').mkdir()\n    (tempdir / '_templates').mkdir()\n    d['path'] = tempdir\n    assert not qs.valid_dir(d)\n", "def test_valid_dir(tempdir):\n    d = {'path': tempdir}\n    assert qs.valid_dir(d) is True\n\n    # An existing conf.py should return False\n    (tempdir / 'conf.py').touch()\n    assert qs.valid_dir(d) is False\n\n    # An existing Makefile or make.bat should return False\n    d['path'] = tempdir / 'newdir'\n    d['path'].mkdir()\n    (d['path'] / 'Makefile').touch()\n    assert qs.valid_dir(d) is False\n    (d['path'] / 'Makefile').unlink()\n    (d['path'] / 'make.bat').touch()\n    assert qs.valid_dir(d) is False\n\n    # A non-empty 'dot' directory should return False\n    d['dot'] = 'dotdir'\n    d['path'] = tempdir / 'dotdir'\n    d['path'].mkdir()\n    (d['path'] / 'dotdir' / 'static').mkdir(parents=True)\n    assert qs.valid_dir(d) is False\n\n    # A non-empty 'master' file should return False\n    d['master'] = 'master'\n    d['suffix'] = '.rst'\n    d['path'] = tempdir / 'masterdir'\n    d['path'].mkdir()\n    (d['path'] / 'master.rst').touch()\n    assert qs.valid_dir(d) is False\n", "def test_valid_dir(tempdir):\n    d = {'path': tempdir, 'sep': True, 'dot': '_', 'master': 'index', 'suffix': '.rst'}\n    assert qs.valid_dir(d) is True\n\n    (tempdir / 'Makefile').touch()\n    assert qs.valid_dir(d) is False\n\n    d['sep'] = False\n    (tempdir / 'conf.py').touch()\n    assert qs.valid_dir(d) is False\n\n    d['sep'] = True\n    (tempdir / 'source' / 'conf.py').touch()\n    assert qs.valid_dir(d) is False\n\n    d['sep'] = False\n    (tempdir / '_templates').mkdir()\n    assert qs.valid_dir(d) is False\n\n    d['dot'] = '.'\n    assert qs.valid_dir(d) is True\n", "def test_valid_dir(tempdir):\n    d = {'path': tempdir, 'sep': False, 'dot': '_', 'master': 'index', 'suffix': '.rst'}\n    assert qs.valid_dir(d) is True\n\n    (tempdir / 'conf.py').write_text('')\n    assert qs.valid_dir(d) is False\n\n    d['sep'] = True\n    assert qs.valid_dir(d) is True\n\n    (tempdir / 'source' / 'conf.py').write_text('')\n    assert qs.valid_dir(d) is False\n\n    d['sep'] = False\n    d['dot'] = '.'\n    (tempdir / '.templates').mkdir()\n    assert qs.valid_dir(d) is False\n\n    d['sep'] = True\n    assert qs.valid_dir(d) is True\n\n    (tempdir / 'source' / '.templates').mkdir()\n    assert qs.valid_dir(d) is False\n\n    (tempdir / 'Makefile').write_text('')\n    assert qs.valid_dir(d) is False\n"], "sample_954": ["def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'http://www.python.org/' in content\n", "def test_build_man(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'index.1').exists()\n\n    content = (app.outdir / 'index.1').read_text()\n    assert r'\\fBHeader Level 1\\fP' in content\n    assert r'\\fBHeader Level 2\\fP' in content\n\n    # test productionlist\n    assert '.nf\\n' in content\n    assert 'foo  ::=  bar\\n' in content\n    assert '.fi\\n' in content\n", "def test_manpage_node(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'manpage.1').read_text()\n    assert r'\\fBmanpage\\en\\fP' in content\n    assert '.SH SYNOPSIS' in content\n    assert '.SH DESCRIPTION' in content\n    assert '.SH OPTIONS' in content\n    assert '.SH EXIT STATUS' in content\n    assert '.SH RETURN VALUE' in content\n    assert '.SH ERRORS' in content\n    assert '.SH ENVIRONMENT' in content\n    assert '.SH FILES' in content\n    assert '.SH VERSIONS' in content\n    assert '.SH CONFORMING TO' in content\n    assert '.SH NOTES' in content\n    assert '.SH BUGS' in content\n    assert '.SH EXAMPLE' in content\n    assert '.SH AUTHORS' in content\n    assert '.SH COPYRIGHT' in content\n", "def test_manpage_builder(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'sphinxtests.1').exists()\n\n    # test ManualPageTranslator.header method\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert '.TH SPHINXTESTS \"1\" \"Sphinx Test Documentation\" \"\"' in content\n\n    # test ManualPageTranslator.visit_start_of_file method\n    assert '.ad l' not in content\n\n    # test ManualPageTranslator.visit_manpage method\n    assert '.BR manpage (1)' in content\n\n    # test ManualPageTranslator.visit_centered method\n    assert '.sp\\n.ce\\nCentered text\\n.ce 0\\n' in content\n\n    # test ManualPageTranslator.visit_comment method\n    assert 'This is a comment' not in content\n\n    # test ManualPageTranslator.visit_footnote method\n    assert '.sp\\n.FS 1\\nFootnote text\\n.FE 1\\n' in content\n", "def test_visit_raw(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert '.TH SPHINXTESTS 1 \"2022-01-01\" \"1.0\" \"SphinxTests\"\\n' in content\n    assert '.SH NAME\\n' in content\n    assert 'SphinxTests \\\\- Sphinx test suite\\n' in content\n"], "sample_955": ["def test_unparse_exception():\n    with pytest.raises(NotImplementedError):\n        class UnknownASTNode(ast.AST):\n            pass\n        ast.unparse(UnknownASTNode())\n", "def test_unparse_arguments():\n    source = \"def func(a: int, b, c=1, *args, d, e=2, **kwargs): pass\"\n    module = ast.parse(source)\n    expected = \"a: int, b, c=1, *args, d, e=2, **kwargs\"\n    assert ast.unparse(module.body[0].args, source) == expected\n\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == expected\n", "def test_unparse_generic_visit():\n    with pytest.raises(NotImplementedError):\n        class UnknownNode(ast.AST):\n            pass\n\n        ast.unparse(UnknownNode())\n"], "sample_957": ["def test_get_type_hints():\n        pass\n\n    type_hints = get_type_hints(my_function)\n    assert type_hints == {'a': int, 'b': str, 'return': NoneType}\n\n    # Test with a class\n    class MyClass:\n            pass\n\n    type_hints = get_type_hints(MyClass.__init__)\n    assert type_hints == {'a': int, 'b': str, 'return': NoneType, 'self': MyClass}\n\n    # Test with a module (using this module as an example)\n    import sys\n    type_hints = get_type_hints(sys)\n    assert type_hints == {}\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(foo) == {'a': int, 'b': str, 'return': float}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass.method) == {'a': int, 'b': str, 'return': float}\n    assert get_type_hints(MyClass) == {}\n\n        pass\n\n    assert get_type_hints(bar) == {}\n\n    # Test ForwardRef\n    from typing import ForwardRef\n\n    class MyForwardRef:\n        __forward_arg__ = 'MyForwardRef'\n\n    assert get_type_hints(MyForwardRef) == {}\n\n    # Test with broken class\n    class BrokenClass:\n        pass\n\n    broken_class = BrokenClass()\n    broken_class.__annotations__ = 'broken'\n    assert get_type_hints(broken_class) == {}\n\n    # Test with KeyError\n    class KeyErrorClass:\n            raise KeyError\n\n    key_error_class = KeyErrorClass()\n    assert get_type_hints(key_error_class) == {}\n\n    # Test with NameError\n        a: MyNameError  # noqa: F821\n\n    assert get_type_hints(name_error_func) == {'a': 'MyNameError'}\n\n    # Test with TypeError\n    class TypeErrorClass:\n            raise TypeError\n\n    type_error_class = TypeErrorClass()\n    assert get_type_hints(type_error_class) == {}\n", "def test_stringify_type_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_stringify_type_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_restify_type_hints_Ellipsis():\n    assert restify(Ellipsis) == \"...\"\n"], "sample_956": ["def test_normalize_intersphinx_mapping(tempdir, app, status, warning):\n    \"\"\"Test for normalize_intersphinx_mapping()\"\"\"\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n\n    # Old format with string value\n    app.config.intersphinx_mapping = {\n        'https://docs.python.org/': inv_file,\n    }\n    normalize_intersphinx_mapping(app, app.config)\n    assert app.config.intersphinx_mapping == {\n        'https://docs.python.org/': (None, (inv_file,)),\n    }\n\n    # Old format with tuple value\n    app.config.intersphinx_mapping = {\n        'https://docs.python.org/': (inv_file, inv_file),\n    }\n    normalize_intersphinx_mapping(app, app.config)\n    assert app.config.intersphinx_mapping == {\n        'https://docs.python.org/': (None, (inv_file, inv_file)),\n    }\n\n    # New format with string key\n    app.config.intersphinx_mapping = {\n        'python': ('https://docs.python.org/', inv_file),\n    }\n    normalize_intersphinx_mapping(app, app.config)\n    assert app.config.intersphinx_mapping == {\n        'python': ('https://docs.python.org/', (inv_file,)),\n    }\n\n    # New format with None key\n    app.config.intersphinx_mapping = {\n        None: ('https://docs.python.org/', inv_file),\n    }\n    normalize_intersphinx_mapping(app, app.config)\n    assert app.config.intersphinx_mapping == {\n        None: ('https://docs.python.org/', (inv_file,)),\n    }\n\n    # Test for invalid key type\n    app.config.intersphinx_mapping = {\n        12345: ('https://docs.python.org/', inv_file),\n    }\n    normalize_intersphinx_mapping(app, app.config)\n    assert app.config.intersphinx_mapping == {}\n    assert \"intersphinx identifier 12345 is not string. Ignored\" in warning.getvalue()\n", "def test_resolve_reference_in_domain_by_target(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    domain = app.env.get_domain('py')\n    inventory = InventoryAdapter(app.builder.env).main_inventory\n\n    # check resolution when a target is found\n    node, contnode = fake_node('py', 'func', 'module1.func', 'foo')\n    rn = _resolve_reference_in_domain_by_target(None, inventory, domain,\n                                                ['py:func'], node['reftarget'],\n                                                node, contnode)\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n    assert rn['reftitle'] == '(in foo v2.0)'\n    assert rn[0].astext() == 'foo'\n\n    # check case insensitive match for std:term\n    node, contnode = fake_node('std', 'term', 'A TERM', 'A TERM')\n    rn = _resolve_reference_in_domain_by_target(None, inventory, app.env.get_domain('std'),\n                                                ['std:term'], node['reftarget'],\n                                                node, contnode)\n    assert rn.astext() == 'A TERM'\n\n    # check case sensitive match for other types\n    node, contnode = fake_node('py', 'func', 'MODULE1.FUNC', 'foo')\n    rn = _resolve_reference_in_domain_by_target(None, inventory, domain,\n                                                ['py:func'], node['reftarget'],\n                                                node, contnode)\n    assert rn is None\n", "def test_resolve_reference_detect_inventory(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'inv': ('https://docs.python.org/', inv_file),\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # resolve reference in any inventory\n    node, contnode = fake_node('py', 'func', 'module1.func', 'func()')\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn.astext() == 'func()'\n\n    # resolve reference in specific inventory\n    node, contnode = fake_node('py', 'func', 'inv:module1.func', 'func()')\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn.astext() == 'func()'\n\n    # unresolvable reference\n    node, contnode = fake_node('py', 'func', 'unknown', 'unknown')\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn is None\n", "def test_inventory_adapter_clear(tempdir, app, status, warning):\n    \"\"\"Test InventoryAdapter.clear()\"\"\"\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # check if inventory is not empty\n    assert app.env.intersphinx_inventory\n    assert app.env.intersphinx_named_inventory\n\n    # clear the inventory\n    InventoryAdapter(app.builder.env).clear()\n\n    # check if inventory is empty\n    assert not app.env.intersphinx_inventory\n    assert not app.env.intersphinx_named_inventory\n", "def test_resolve_reference_detect_inventory_no_split(tempdir, app, status, warning):\n    \"\"\"Test resolve_reference_detect_inventory without splitting the target.\"\"\"\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n        'py3k': ('https://docs.python.org/py3k/', inv_file),\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    node, contnode = fake_node('py', 'mod', 'module1', 'module1')\n    rn = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert rn is not None\n\n    # Check that the target was not split\n    assert node['reftarget'] == 'module1'\n"], "sample_958": ["def test_domain_cpp_ast_anon_union_definitions():\n    check('member', '@a', {1: 'a__i', 2: '1a'}, output='int [anonymous]')\n    check('member', '@1', {1: 'a__i', 2: '1a'}, output='int [anonymous]')\n    check('member', '@A::a', {1: 'A::a__i', 2: 'N1A1aE'},\n          output='int A::[anonymous]')\n\n    check('member', 'static @a', {1: 'a__i', 2: '1a'},\n          output='static int [anonymous]')\n    check('member', 'static @1', {1: 'a__i', 2: '1a'},\n          output='static int [anonymous]')\n    check('member', 'static @A::a', {1: 'A::a__i', 2: 'N1A1aE'},\n          output='static int A::[anonymous]')\n\n    check('member', 'extern @a', {1: 'a__i', 2: '5a'},\n          output='extern int [anonymous]')\n    check('member', 'extern @1', {1: 'a__i', 2: '5a'},\n          output='extern int [anonymous]')\n    check('member', 'extern @A::a', {1: 'A::a__i', 2: 'N1A1aE'},\n          output='extern int A::[anonymous]')\n\n    check('member', 'thread_local @a', {1: 'a__i', 2: '1a'},\n          output='thread_local int [anonymous]')\n    check('member', 'thread_local @1', {1: 'a__i', 2: '1a'},\n          output='thread_local int [anonymous]')\n    check('member', 'thread_local @A::a', {1: 'A::a__i', 2: 'N1A1aE'},\n          output='thread_local int A::[anonymous]')\n\n    check('member', 'extern thread_local @a', {1: 'a__i', 2: '1a'},\n          output='extern thread_local int [anonymous]')\n    check('member', 'extern thread_local @1', {1: 'a__i', 2: '1a'},\n", "def test_domain_c_build_misuse_of_roles(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"roles-targets-ok\")\n    assert len(ws) == 0\n\n    ws = filter_warnings(warning, \"roles-targets-warn\")\n    # the roles that should be able to generate warnings:\n    allRoles = ['member', 'var', 'func', 'macro', 'struct', 'union', 'enum', 'enumerator', 'type']\n    ok = [  # targetType, okRoles\n        ('member', ['member', 'var']),\n        ('func', ['func', 'type']),\n        ('macro', ['macro']),\n        ('struct', ['type', 'struct']),\n        ('union', ['type', 'union']),\n        ('enum', ['type', 'enum']),\n        ('enumerator', ['enumerator']),\n        ('type', ['type']),\n        ('functionParam', ['member', 'var']),\n    ]\n    warn = []\n    for targetType, roles in ok:\n        txtTargetType = \"function\" if targetType == \"func\" else targetType\n        for r in allRoles:\n            if r not in roles:\n                warn.append(\"WARNING: c:{} targets a {} (\".format(r, txtTargetType))\n                if targetType == 'functionParam':\n                    warn.append(\"WARNING: c:{} targets a {} (\".format(r, txtTargetType))\n                    warn.append(\"WARNING: c:{} targets a {} (\".format(r, txtTargetType))\n    warn = list(sorted(warn))\n    for w in ws:\n        assert \"targets a\" in w\n    ws = [w[w.index(\"WARNING:\"):] for w in ws]\n    ws = list(sorted(ws))\n    print(\"Expected warnings:\")\n    for w in warn:\n        print(w)\n    print(\"Actual warnings:\")\n    for w in ws:\n        print(w)\n\n    for i in range(min(len(warn), len(ws))):\n        assert ws[i].startswith(warn[i])\n\n    assert len(ws) == len(warn)\n", "def test_domain_cpp_parse_template_shorthand():\n    check('class', 'abc::ns::foo{id_0, id_1, id_2} xyz::bar',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz3barE'})\n    check('class', 'abc::ns::foo{id_0, id_1, ...id_2} xyz::bar',\n          {2: 'I00DpEXN3abc2ns3fooEI4id_04id_1sp4id_2EEN3xyz3barE'})\n    check('class', 'abc::ns::foo{id_0, id_1, id_2} xyz::bar<id_0, id_1, id_2>',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz3barI4id_04id_14id_2EE'})\n    check('class', 'abc::ns::foo{id_0, id_1, ...id_2} xyz::bar<id_0, id_1, id_2...>',\n          {2: 'I00DpEXN3abc2ns3fooEI4id_04id_1sp4id_2EEN3xyz3barI4id_04id_1Dp4id_2EE'})\n\n    check('type', 'abc::ns::foo{id_0, id_1, id_2} xyz::bar = ghi::qux',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz3barE'})\n    check('type', 'abc::ns::foo{id_0, id_1, ...id_2} xyz::bar = ghi::qux',\n          {2: 'I00DpEXN3abc2ns3fooEI4id_04id_1sp4id_2EEN3xyz3barE'})\n    check('function', 'abc::ns::foo{id_0, id_1, id_2} void xyz::bar()',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz", "def test_domain_cpp_ast_macro_definitions():\n    check('macro', '{key}MACRO', {2: \"IE1MACRO\"})\n    check('macro', '{key}MACRO(', {2: \"IE1MACRO\"})\n    check('macro', '{key}MACRO()', {2: \"IE1MACROv\"})\n    check('macro', '{key}MACRO(a)', {2: \"IE1MACRO1a\"})\n    check('macro', '{key}MACRO(a, b)', {2: \"IE1MACRO1a1b\"})\n    check('macro', '{key}MACRO(a, b, ...)', {2: \"IE1MACRO1a1b_zE\"})\n    check('macro', '{key}MACRO(a = 42)', {2: \"IE1MACRO1aL42E\"})\n    check('macro', '{key}MACRO(a = 42, b)', {2: \"IE1MACRO1aL42E1b\"})\n    check('macro', '{key}MACRO(a, b = 42)', {2: \"IE1MACRO1a1bL42E\"})\n    check('macro', '{key}MACRO(a, b = 42, ...)', {2: \"IE1MACRO1a1bL42E_zE\"})\n", "def test_domain_cpp_parse_type_declaration(app):\n    # Issue 10005\n    text = (\".. cpp:type:: std::vector<int>\\n\"\n            \"   Some type.\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, addnodes.index))\n    indexnode = doctree[1]\n    assert_node(indexnode, addnodes.index, entries=[('single', 'std::vector<int> (C++ type)', '_CPPvNSt6vectorIiEE5typeE', '', None)])\n"], "sample_960": ["def test_type_to_xref_unqualified_type_names(app):\n    env = app.env\n    env.config.python_use_unqualified_type_names = True\n    env.ref_context['py:module'] = 'module_a'\n    env.ref_context['py:class'] = 'ClassA'\n\n    xref = type_to_xref('module_a.ClassA', env)\n    assert_node(xref, (pending_xref, ([pending_xref_condition, \"ClassA\"],\n                                      [pending_xref_condition, \"module_a.ClassA\"])))\n\n    xref = type_to_xref('module_b.ClassB', env)\n    assert_node(xref, pending_xref, **{\"py:module\": \"module_a\", \"py:class\": \"ClassA\"})\n\n    env.config.python_use_unqualified_type_names = False\n    xref = type_to_xref('module_a.ClassA', env)\n    assert_node(xref, pending_xref)\n", "def test_resolve_xref_with_module_and_class_contexts(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: method\\n\"\n            \"      See :py:func:`function`.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    fromdocname = 'index'\n    builder = Mock()\n    builder.env = app.env\n    env = app.env\n    node = doctree.traverse(pending_xref)[0]\n    node['refdomain'] = 'py'\n    node['reftype'] = 'func'\n    node['reftarget'] = 'function'\n    node['py:module'] = 'example'\n    node['py:class'] = 'Class'\n    result = app.env.domains['py'].resolve_xref(env, fromdocname, builder, 'func', 'function', node, None)\n    assert result\n    assert_node(result[0], nodes.reference,\n                refid='example.function', **{'py:module': 'example', 'py:class': 'Class'})\n", "def test_pyannotation(app):\n    text = (\".. py:function:: func1\\n\"\n            \"   :return: A tuple of (int, str)\\n\"\n            \"   :rtype: Tuple[int, str]\\n\"\n            \"\\n\"\n            \".. py:function:: func2\\n\"\n            \"   :return: A value of type: List[int]\\n\"\n            \"   :rtype: List[int]\\n\"\n            \"\\n\"\n            \".. py:function:: func3\\n\"\n            \"   :return: A dictionary with keys of type str and values of type int\\n\"\n            \"   :rtype: Dict[str, int]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func1\"],\n                                                    [desc_parameterlist, ()],\n                                                    [desc_returns, pending_xref, \"Tuple\"])],\n                                  desc_content)],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func2\"],\n                                                    [desc_parameterlist, ()],\n                                                    [desc_returns, pending_xref, \"List\"])],\n                                  desc_content)],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func3\"],\n                                                    [desc_parameterlist, ()],\n                                                    [desc_returns, pending_xref, \"Dict\"])],\n                                  desc_content)]))\n    assert_node(doctree[1][0][2], ([desc_returns, pending_xref, \"Tuple\"],\n                                   [desc_sig_punctuation, \"[\"],\n                                   [pending_xref, \"int\"],\n                                   [desc_sig_punctuation, \", \"],\n                                   [pending_xref, \"str\"],\n                                   [desc_sig_punctuation, \"]\"]))\n    assert_node(doctree[3][0][2], ([desc_returns, pending_xref, \"List\"],\n                                   [desc_sig_punctuation, \"[\"],\n                                   [pending_xref, \"int\"],\n                                   [desc_sig_punctuation, \"]\"]))\n    assert_node(doctree[5][0][2], ([desc_returns, pending_xref, \"Dict\"],\n                                   [desc_sig_punctuation, \"[\"],\n                                   [pending_xref, \"str\"],\n                                   [desc_sig_punctuation, \", \"],\n                                   [pending_xref, \"int\"],\n                                   [desc_sig_punctuation, \"]\"]))\n", "def test_get_full_qualified_name_with_pyclass(app):\n    text = (\".. py:class:: Class\\n\"\n            \"   .. py:method:: my_method()\\n\"\n            \"      :noindex:\\n\")\n\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    node = doctree[1][1][0][0][2][0]\n\n    assert domain.get_full_qualified_name(node) == 'Class.my_method'\n", "def test_parse_annotation_dict_with_empty_string(app):\n    doctree = _parse_annotation(\"{'key': ''}\", app.env)\n    assert_node(doctree, ([pending_xref, \"dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"]\"]))\n"], "sample_959": ["def test_domain_cpp_parse_mix_decl_template(app, warning):\n    # Issue 8270\n    text = (\".. cpp:struct:: A\\n\"\n            \".. cpp:function:: void A<>()\\n\"\n            \".. cpp:struct:: A\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"index.rst:2: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[0]\n    assert \"Declaration is '.. cpp:function:: void A()'.\" in ws[1]\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[2]\n    assert \"Declaration is '.. cpp:struct:: A'.\" in ws[3]\n    assert ws[4] == \"\"\n", "def test_domain_cpp_parse_template_introduction(app, warning):\n    # Issue 8986\n    text = (\".. cpp:class:: template<int T = 42> MyClass\\n\"\n            \".. cpp:class:: template<typename... Ts> MyClass<(Ts)...>\\n\"\n            \".. cpp:class:: template<int... Is> MyClass<(Is)...>\\n\"\n            \".. cpp:class:: template<int T> requires C<T> MyClass\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 1\n    assert ws[0] == \"\"\n", "def test_domain_cpp_parse_mix_templ_decl_non_type():\n        parser = DefinitionParser(string, location=None, config=None)\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_declaration(\"type\", \"type\")\n        parser.assert_end()\n        return ast\n\n    # just the non-type part\n    string = \"typename T\"\n    ast = parse_type(string)\n    assert isinstance(ast.declaration, ASTTemplateParamNonType)\n    assert isinstance(ast.declaration.param, ASTTemplateKeyParamPackIdDefault)\n\n    # type without default\n    string = \"typename T = int\"\n    ast = parse_type(string)\n    assert isinstance(ast.declaration, ASTTemplateParamConstrainedTypeWithInit)\n    assert isinstance(ast.declaration.type, ASTTemplateParamNonType)\n\n    # type with default\n    string = \"typename T\"\n    ast = parse_type(string)\n    assert isinstance(ast.declaration, ASTTemplateParamConstrainedTypeWithInit)\n    assert isinstance(ast.declaration.type, ASTTemplateParamNonType)\n", "def test_domain_cpp_name_spanning():\n    assert Symbol.debug_indent_string * 0 == \"\"\n    assert Symbol.debug_indent_string * 1 == \"  \"\n    assert Symbol.debug_indent_string * 2 == \"    \"\n    assert Symbol.debug_indent_string * 3 == \"      \"\n\n    rootSymbol = Symbol(None, None, None, None, None, None, None)\n    s1 = rootSymbol.add_name(\"ns1\", None)\n    s2 = s1.add_name(\"ns2\", None)\n    s3 = s2.add_name(\"ns3\", None)\n    s4 = s3.add_name(\"ns4\", None)\n    s5 = s4.add_name(\"ns5\", None)\n\n    for i, (symbol, name) in enumerate(((rootSymbol, \"ns1\"), (s1, \"ns2\"), (s2, \"ns3\"), (s3, \"ns4\"), (s4, \"ns5\"))):\n        sChild = symbol._add_symbols(ASTNestedName([ASTNestedNameElement(ASTIdentifier(name), None)], [False], rooted=False), [], None, None, None)\n        assert sChild is not symbol\n        assert sChild.parent == symbol\n        assert sChild.parent is not None\n        assert sChild.get_full_nested_name() == ASTNestedName(\n            [ASTNestedNameElement(ASTIdentifier(\"ns1\"), None),\n             ASTNestedNameElement(ASTIdentifier(\"ns2\"), None),\n             ASTNestedNameElement(ASTIdentifier(\"ns3\"), None),\n             ASTNestedNameElement(ASTIdentifier(\"ns4\"), None),\n             ASTNestedNameElement(ASTIdentifier(\"ns5\"), None)],\n            [False, False, False, False, False], rooted=False)[:i+1]\n", "def test_domain_cpp_ast_ptr_to_member_function():\n    check('function', 'void (C::* f)()', {2: 'FivEM1C'})\n    check('function', 'void (C::* f)() const', {2: 'KFivEM1C'})\n    check('function', 'void (C::* f)() volatile', {2: 'VFivEM1C'})\n    check('function', 'void (C::* f)() const volatile', {2: 'VKFivEM1C'})\n    check('function', 'void (C::* f)() &', {2: 'NFivEM1C'})\n    check('function', 'void (C::* f)() &&', {2: 'OFivEM1C'})\n    check('function', 'void (C::* f)() const &', {2: 'NKFivEM1C'})\n    check('function', 'void (C::* f)() const &&', {2: 'OKFivEM1C'})\n    check('function', 'void (C::* f)(int) = 0', {2: 'FivEiM1C'})\n    check('function', 'void (C::* f)(int) const = 0', {2: 'KFivEiM1C'})\n    check('function', 'void (C::* f)(int) volatile = 0', {2: 'VFivEiM1C'})\n    check('function', 'void (C::* f)(int) const volatile = 0', {2: 'VKFivEiM1C'})\n    check('function', 'void (C::* f)(int) & = 0', {2: 'NFivEiM1C'})\n    check('function', 'void (C::* f)(int) && = 0', {2: 'OFivEiM1C'})\n    check('function', 'void (C::* f)(int) const & = 0', {2: 'NKFivEiM1C'})\n    check('function', 'void (C::* f)(int) const && = 0', {2: 'OKFivEiM1C'})\n"], "sample_963": ["def test_stringify_type_hints_ForwardRef():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"myint\")) == \"myint\"\n    else:\n        from typing import _ForwardRef  # type: ignore\n        assert stringify(_ForwardRef(\"myint\")) == \"myint\"\n", "def test_stringify_type_hints_newtype():\n    MyNewInt = NewType('MyNewInt', int)\n    assert stringify(MyNewInt) == \"MyNewInt\"\n\n    if sys.version_info >= (3, 10):\n        assert stringify(MyNewInt) == \"tests.test_util_typing.MyNewInt\"\n", "def test_stringify_type_forward_ref():\n    from typing import ForwardRef\n    forward_ref = ForwardRef('MyClass')\n    assert stringify(forward_ref) == 'MyClass'\n", "def test_restify_type_hints_Ellipsis():\n    assert restify(Ellipsis) == '...'\n    assert restify(type(Ellipsis)) == ':py:class:`ellipsis`'\n", "def test_stringify_type_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n"], "sample_962": ["def test_ismock():\n    with mock(['unknown']):\n        import unknown\n        assert ismock(unknown)\n        assert ismock(unknown.secret)\n        assert ismock(unknown.secret.Class)\n        assert not ismock(int)\n        assert not ismock(str)\n        assert not ismock(MyClass1)\n", "def test_ismock():\n    assert not ismock(int)\n    assert not ismock(str)\n    assert not ismock(None)\n    assert not ismock(Any)\n\n    with mock(['unknown']):\n        import unknown\n        assert ismock(unknown)\n        assert ismock(unknown.secret)\n        assert ismock(unknown.secret.Class)\n        assert ismock(unknown.secret.Class())\n", "def test_ismock():\n    with mock(['unknown']):\n        import unknown\n        assert ismock(unknown)\n        assert ismock(unknown.secret)\n        assert ismock(unknown.secret.Class)\n        assert not ismock(int)\n        assert not ismock(str)\n", "def test_ismock():\n    with mock(['unknown']):\n        import unknown\n        assert ismock(unknown)\n        assert ismock(unknown.secret)\n        assert ismock(unknown.secret.Class)\n        assert not ismock(int)\n        assert not ismock(str)\n", "def test_undecorate():\n    with mock(['unknown']):\n        import unknown\n        obj = unknown.secret.Class()\n        decorated_obj = unknown.secret.Class(obj)\n        assert undecorate(decorated_obj) == obj\n"], "sample_961": ["def test_resolve_xref_with_module_and_class_in_options(app):\n    text = (\".. py:module:: module_a.submodule\\n\"\n            \".. py:class:: ModTopLevel\\n\"\n            \"\\n\"\n            \"   .. py:method:: module_a.submodule.ModTopLevel.mod_child_1\\n\"\n            \"   .. py:method:: ModTopLevel.mod_child_2\")\n    doctree = restructuredtext.parse(app, text)\n    refnodes = list(doctree.traverse(pending_xref))\n    assert len(refnodes) == 0\n", "def test_pydecorator(app):\n    text = (\".. py:decorator:: my_decorator\\n\"\n            \"\\n\"\n            \"   .. py:method:: my_method\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"my_decorator\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'my_method() (my_decorator method)', 'my_decorator.my_method', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"my_method\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n    assert 'my_decorator.my_method' in domain.objects\n    assert domain.objects['my_decorator.my_method'] == ('index', 'my_decorator.my_method', 'method', False)\n", "def test_python_domain_add_canonical(app, warning):\n    text = (\".. py:class:: foo.Foo\\n\"\n            \"   :canonical: _foo.Foo\")\n    restructuredtext.parse(app, text)\n    assert warning.getvalue() == \"\"\n\n    domain = app.env.get_domain('py')\n    assert domain.objects['foo.Foo'] == ('index', 'foo.Foo', 'class', False)\n    assert domain.objects['_foo.Foo'] == ('index', 'foo.Foo', 'class', True)\n", "def test_pyattribute_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"\"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"Optional\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \"]\"])])],\n                                   [desc_content, ()]))\n", "def test_resolve_xref_canonical(app):\n    doctree = app.env.get_doctree('canonical')\n    refnodes = list(doctree.traverse(pending_xref))\n    assert len(refnodes) == 1\n    assert refnodes[0]['reftarget'] == 'canonical.Foo'\n    assert refnodes[0]['refid'] == 'canonical.Foo'\n    assert refnodes[0]['refdoc'] == 'canonical'\n"], "sample_965": ["def test_getorigbases():\n    class Base:\n        pass\n\n    class Child(Base):\n        pass\n\n    assert inspect.getorigbases(Child) == (Base,)\n    assert inspect.getorigbases(Base) is None\n    assert inspect.getorigbases(object) is None\n    assert inspect.getorigbases(1) is None\n", "def test_isabstractmethod():\n    class AbstractBaseClass:\n        @abstractmethod\n            pass\n\n    class ConcreteClass:\n            pass\n\n    abstract_base_instance = AbstractBaseClass()\n    concrete_instance = ConcreteClass()\n\n    assert inspect.isabstractmethod(AbstractBaseClass.abstract_method) is True\n    assert inspect.isabstractmethod(ConcreteClass.concrete_method) is False\n    assert inspect.isabstractmethod(abstract_base_instance.abstract_method) is True\n    assert inspect.isabstractmethod(concrete_instance.concrete_method) is False\n", "def test_getorigbases():\n    class Foo:\n        pass\n\n    class Bar(Foo):\n        __orig_bases__ = (Foo,)\n\n    class Baz(Foo):\n        pass\n\n    assert inspect.getorigbases(Foo) is None\n    assert inspect.getorigbases(Bar) == (Foo,)\n    assert inspect.getorigbases(Baz) is None\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(normal_func) is False\n\n", "def test_getall():\n    class Module:\n        __all__ = ['a', 'b']\n\n    class InvalidModule1:\n        __all__ = 'string'\n\n    class InvalidModule2:\n        __all__ = ['a', 1]\n\n    class InvalidModule3:\n        __all__ = ('a', 'b', 'c')\n\n    class InvalidModule4:\n        pass\n\n    assert inspect.getall(Module()) == ['a', 'b']\n\n    with pytest.raises(ValueError):\n        inspect.getall(InvalidModule1())\n\n    with pytest.raises(ValueError):\n        inspect.getall(InvalidModule2())\n\n    assert inspect.getall(InvalidModule3()) is None\n\n    assert inspect.getall(InvalidModule4()) is None\n"], "sample_966": ["def test_get_full_qualified_name_nested(app):\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # nested references\n    node = nodes.reference(reftarget='nested_func', **{'py:module': 'module1', 'py:class': 'Class1'})\n    assert domain.get_full_qualified_name(node) == 'module1.Class1.nested_func'\n\n    node = nodes.reference(reftarget='nested_func', **{'py:module': 'module1', 'py:class': 'module1.Class1'})\n    assert domain.get_full_qualified_name(node) == 'module1.Class1.nested_func'\n\n    node = nodes.reference(reftarget='nested_func', **{'py:module': 'module1', 'py:class': 'Class1.Class2'})\n    assert domain.get_full_qualified_name(node) == 'module1.Class1.Class2.nested_func'\n", "def test_pytype_xref(app, status, warning):\n    text = \".. py:function:: foo(arg: typing.List[str])\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1][0][2][2][0][2],\n                pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"typing.List\")\n    assert_node(doctree[1][0][1][0][2][2][0][4],\n                pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n", "def test_get_full_qualified_name_with_context(app):\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # simple reference\n    node = nodes.reference(reftarget='func')\n    node['py:module'] = 'module1'\n    node['py:class'] = 'Class1'\n    assert domain.get_full_qualified_name(node) == 'module1.Class1.func'\n\n    # with context\n    env.ref_context['py:module'] = 'module2'\n    env.ref_context['py:class'] = 'Class2'\n    assert domain.get_full_qualified_name(node) == 'module1.Class1.func'\n", "def test_get_objects(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: method\\n\"\n            \"   .. py:attribute:: attr\\n\")\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    objects = list(domain.get_objects())\n    assert len(objects) == 4\n    assert ('example', 'example', 'module', 'index', 'module-example', 0) in objects\n    assert ('Class', 'example.Class', 'class', 'index', 'example.Class', 1) in objects\n    assert ('method', 'example.Class.method', 'method', 'index', 'example.Class.method', 1) in objects\n    assert ('attr', 'example.Class.attr', 'attribute', 'index', 'example.Class.attr', 1) in objects\n", "def test_python_domain_module_index_title(app):\n    text = \".. py:module:: sphinx.config\\n\"\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate()[0] == [\n        ('s', [IndexEntry('sphinx', 1, '', '', '', '', ''),\n               IndexEntry('sphinx.config', 2, 'index', 'module-sphinx.config', '', '', '')])\n    ]\n"], "sample_964": ["def test_parse_annotation_subscript(app):\n    doctree = _parse_annotation(\"List[str][int]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_python_domain_module_index_common_prefix_wildcard(app):\n    app.config.modindex_common_prefix = ['sphinx.*']\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.config\\n\"\n            \".. py:module:: sphinx.builders\\n\"\n            \".. py:module:: sphinx.builders.html\\n\"\n            \".. py:module:: sphinx_intl\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('b', [IndexEntry('builders', 1, 'index', 'module-sphinx.builders', '', '', ''),\n                IndexEntry('builders.html', 2, 'index', 'module-sphinx.builders.html', '', '', '')]),\n         ('c', [IndexEntry('config', 0, 'index', 'module-sphinx.config', '', '', '')]),\n         ('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx_intl', 0, 'index', 'module-sphinx_intl', '', '', '')])],\n        True\n    )\n", "def test_imported_members_are_not_documented(app):\n    text = (\".. py:module:: example\\n\"\n            \"\\n\"\n            \"   .. py:currentmodule:: example.submodule\\n\"\n            \"\\n\"\n            \"   .. class:: Class\\n\"\n            \"      :members: method\\n\"\n            \"\\n\"\n            \"   .. class:: ImportedClass\\n\"\n            \"      :members: method\\n\"\n            \"\\n\"\n            \"   .. class:: SubmoduleClass\\n\"\n            \"      :members: method\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree,\n                (nodes.target,\n                 addnodes.index,\n                 addnodes.index,\n                 desc,\n                 addnodes.index,\n                 desc,\n                 addnodes.index,\n                 desc))\n    assert_node(doctree[3][0][0],\n                ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                   [desc_addname, \"example.submodule.\"],\n                                   [desc_name, \"Class\"])],\n                 desc_content, ([nodes.paragraph, ()],\n                                [desc, ([desc_signature, ([desc_addname, \"method\"],\n                                                           [desc_name, \"Class.method\"])],\n                                          desc_content)])))\n    assert_node(doctree[5][0][0],\n                ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                   addnodes.desc_addname,\n                                   \"example.submodule.\",\n                                   [desc_name, \"ImportedClass\"])],\n                 desc_content))\n    assert_node(doctree[7][0][0],\n                ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                   [desc_addname, \"example.submodule.\"],\n                                   [desc_name, \"SubmoduleClass\"])],\n                 desc_content, ([nodes.paragraph, ()],\n                                [desc, ([desc_signature, ([desc_addname, \"method\"],\n                                                           [desc_name, \"SubmoduleClass.method\"])],\n                                          desc_content)])))\n", "def test_get_full_qualified_name_class_attribute():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    node = nodes.reference(reftarget='attr', pyclass='Class')\n    assert domain.get_full_qualified_name(node) == 'Class.attr'\n\n    node = nodes.reference(reftarget='attr', pymodule='module', pyclass='Class')\n    assert domain.get_full_qualified_name(node) == 'module.Class.attr'\n", "def test_get_full_qualified_name_for_module(app):\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # module reference\n    node = nodes.reference(reftarget='module')\n    assert domain.get_full_qualified_name(node) == 'module'\n\n    # module reference with py:module context\n    kwargs = {'py:module': 'package'}\n    node = nodes.reference(reftarget='module', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'package.module'\n"], "sample_967": ["def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://example.com/mathjax.js' in content\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://example.com/MathJax.js' in content\n", "def test_mathjax_custom_path_and_options(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<script async=\"async\" '\n            'src=\"https://example.com/mathjax.js\"></script>' in content)\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://cdn.example.com/mathjax.js' in content\n", "def test_mathjax_inline_custom(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = ('<span class=\"math notranslate nohighlight\">$$E=mc^2$$</span>')\n    assert html in content\n"], "sample_970": ["def test_getorigbases():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        __orig_bases__ = (B,)\n\n    assert inspect.getorigbases(A) is None\n    assert inspect.getorigbases(B) is None\n    assert inspect.getorigbases(C) == (B,)\n    assert inspect.getorigbases(int) is None\n    assert inspect.getorigbases(1) is None\n", "def test_getorigbases():\n    class Foo:\n        pass\n\n    class Bar(Foo):\n        pass\n\n    assert inspect.getorigbases(Foo) is None\n    assert inspect.getorigbases(Bar) == (Foo,)\n\n    class Baz(typing.Generic[Foo]):\n        pass\n\n    assert inspect.getorigbases(Baz) == (typing.Generic[Foo],)\n", "def test_isasyncgenfunction():\n    async def asyncgenfunc():\n        yield\n\n        yield\n\n    async def asyncfunc():\n        pass\n\n    assert inspect.isasyncgenfunction(asyncgenfunc) is True\n    assert inspect.isasyncgenfunction(syncgenfunc) is False\n    assert inspect.isasyncgenfunction(asyncfunc) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(normal_fun) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(func2) is False\n"], "sample_969": ["def test_restify_type_hints_ForwardRef_py36():\n    class ForwardRef:\n        \"\"\"A pseudo ForwardRef class for py36.\"\"\"\n            self.arg = arg\n\n    assert restify(ForwardRef(\"myint\")) == \":py:class:`myint`\"\n", "def test_get_type_hints():\n        pass\n\n    hints = get_type_hints(func)\n    assert hints['a'] == int\n    assert hints['b'] == str\n    assert hints['return'] == float\n\n    # test on a class\n    class MyClass:\n            pass\n\n            pass\n\n    hints = get_type_hints(MyClass)\n    assert hints == {}\n\n    hints = get_type_hints(MyClass.__init__)\n    assert hints['a'] == int\n    assert hints['b'] == str\n    assert hints['return'] == type(None)\n\n    hints = get_type_hints(MyClass.method)\n    assert hints['c'] == float\n    assert hints['d'] == bool\n    assert hints['return'] == str\n", "def test_stringify_type_hints_invalid_object():\n    class InvalidObject:\n            raise TypeError('Invalid object')\n\n    assert stringify(InvalidObject(), False) == 'InvalidObject'\n    assert stringify(InvalidObject(), True) == '~InvalidObject'\n", "def test_get_type_hints():\n        pass\n\n    hints = get_type_hints(func)\n    assert hints == {'a': int, 'b': str, 'return': NoneType}\n", "def test_get_type_hints():\n        return 1.0\n\n        pass\n\n        return 1.0\n\n    assert get_type_hints(func1) == {'a': int, 'b': str, 'return': float}\n    assert get_type_hints(func2) == {'a': int, 'b': str, 'return': NoneType}\n    assert get_type_hints(func3) == {'a': int, 'b': str}\n\n    class MyClass:\n            return 1.0\n\n            pass\n\n            return 1.0\n\n    assert get_type_hints(MyClass.method1) == {'a': int, 'b': str, 'return': float}\n    assert get_type_hints(MyClass.method2) == {'a': int, 'b': str, 'return': NoneType}\n    assert get_type_hints(MyClass.method3) == {'a': int, 'b': str}\n"], "sample_971": ["def test_log_level_names():\n    # Test predefined log level names\n    assert logging.LEVEL_NAMES['CRITICAL'] == logging.CRITICAL\n    assert logging.LEVEL_NAMES['SEVERE'] == logging.CRITICAL\n    assert logging.LEVEL_NAMES['ERROR'] == logging.ERROR\n    assert logging.LEVEL_NAMES['WARNING'] == logging.WARNING\n    assert logging.LEVEL_NAMES['INFO'] == logging.INFO\n    assert logging.LEVEL_NAMES['VERBOSE'] == logging.VERBOSE\n    assert logging.LEVEL_NAMES['DEBUG'] == logging.DEBUG\n\n    # Test undefined log level names default to WARNING\n    assert logging.LEVEL_NAMES['UNDEFINED'] == logging.WARNING\n", "def test_pending_logging(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.info('message1')\n    with logging.pending_logging():\n        # not logged yet (buffered) in here\n        logger.info('message2')\n        logger.info('message3')\n        assert 'message1' in status.getvalue()\n        assert 'message2' not in status.getvalue()\n        assert 'message3' not in status.getvalue()\n\n    # actually logged as ordered\n    assert 'message2\\nmessage3' in status.getvalue()\n", "def test_colored_logs_with_location(app, status, warning):\n    app.verbosity = 2\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    # default colors with location\n    logger.debug('message1', location='index')\n    logger.verbose('message2', location=('index', 10))\n    logger.info('message3', location='index')\n    logger.warning('message4', location=('index', 10))\n    logger.critical('message5', location='index')\n    logger.error('message6', location=('index', 10))\n\n    assert colorize('darkgray', 'index.txt: message1') in status.getvalue()\n    assert 'index.txt:10: message2\\n' in status.getvalue()  # not colored\n    assert 'index.txt: message3\\n' in status.getvalue()  # not colored\n    assert colorize('red', 'index.txt:10: WARNING: message4') in warning.getvalue()\n    assert 'index.txt: CRITICAL: message5\\n' in warning.getvalue()  # not colored\n    assert colorize('darkred', 'index.txt:10: ERROR: message6') in warning.getvalue()\n", "def test_log_collector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    collector = logging.LogCollector()\n    with collector.collect():\n        logger.info('message1')\n        logger.warning('message2')\n\n    logs = collector.logs\n    assert len(logs) == 2\n    assert logs[0].getMessage() == 'message1'\n    assert logs[1].getMessage() == 'WARNING: message2'\n", "def testPendingLoggingContextManager(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logCollector = logging.LogCollector()\n    with logCollector.collect() as coll:\n        logger.info('message1')\n        logger.warning('message2')\n        logger.debug('message3')\n\n    assert len(coll.logs) == 3\n    assert coll.logs[0].getMessage() == 'message1'\n    assert coll.logs[1].getMessage() == 'message2'\n    assert coll.logs[2].getMessage() == 'message3'\n"], "sample_968": ["def test_py_object_references(app, status, warning):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: method1\\n\"\n            \"   .. py:classmethod:: method2\\n\"\n            \"   .. py:staticmethod:: method3\\n\"\n            \"   .. py:property:: prop\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n\n    assert 'example.Class' in domain.objects\n    assert domain.objects['example.Class'][2] == 'class'\n\n    assert 'example.Class.method1' in domain.objects\n    assert domain.objects['example.Class.method1'][2] == 'method'\n\n    assert 'example.Class.method2' in domain.objects\n    assert domain.objects['example.Class.method2'][2] == 'method'\n\n    assert 'example.Class.method3' in domain.objects\n    assert domain.objects['example.Class.method3'][2] == 'method'\n\n    assert 'example.Class.prop' in domain.objects\n    assert domain.objects['example.Class.prop'][2] == 'property'\n", "def test_get_full_qualified_name_unresolvable(app):\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    node = nodes.reference(reftarget='unknown')\n    assert domain.get_full_qualified_name(node) is None\n", "def test_info_field_list_with_variadic_types(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param Tuple[str, ...] items: blah blah\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree,\n                (nodes.target,\n                 addnodes.index,\n                 addnodes.index,\n                 [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_addname, \"example.\"],\n                                           [desc_name, \"Class\"])],\n                         [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[3][1][0][0], ([nodes.field_name, \"Parameters\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :param Tuple[str, ...] items:\n    assert_node(doctree[3][1][0][0][1][0],\n                ([addnodes.literal_strong, \"items\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"Tuple\"],\n                 [addnodes.literal_emphasis, \"[\"],\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 [addnodes.literal_emphasis, \", \"],\n                 [addnodes.literal_emphasis, \"...\"],\n                 [addnodes.literal_emphasis, \"]\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[3][1][0][0][1][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"Tuple\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n    assert_node(doctree[3][1][0][0][1][0][4], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n", "def test_get_full_qualified_name_for_module(app):\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # simple module reference\n    node = nodes.reference(reftarget='module1')\n    assert domain.get_full_qualified_name(node) == 'module1'\n\n    # module reference with py:module context\n    kwargs = {'py:module': 'module1'}\n    node = nodes.reference(reftarget='module2', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.module2'\n", "def test_pyattribute_with_optional_type(app, status, warning):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str]\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_addname, \"\"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_addname, \"\"],\n                                                     [desc_name, \"attr\"],\n                                                     [desc_annotation, ([desc_sig_punctuation, ':'],\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"Optional\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \"]\"])])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n"], "sample_972": ["def test_get_type_hints():\n        return a + b\n\n    hints = get_type_hints(func)\n    assert hints == {'a': int, 'b': str, 'return': str}\n\n        pass\n\n    class MyClass:\n        pass\n\n    hints = get_type_hints(func_with_forward_ref)\n    assert hints == {'a': MyClass, 'return': NoneType}\n", "def test_get_type_hints():\n        pass\n\n    assert get_type_hints(foo) == {'a': int, 'b': str, 'return': float}\n\n    class MyClass:\n            pass\n\n    assert get_type_hints(MyClass.__init__) == {'a': int, 'b': str, 'return': None}\n\n    # Test with a broken class (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n    class BrokenClass:\n            pass\n        __annotations__ = 1  # type: ignore\n\n    assert get_type_hints(BrokenClass.__init__) == {}\n", "def test_restify_type_ForwardRef_nested():\n    from typing import ForwardRef  # type: ignore\n    assert restify(ForwardRef(\"MyClass1[int]\")) == \":py:class:`MyClass1[int]`\"\n    assert restify(ForwardRef(\"List[MyClass1[int]]\")) == \":py:class:`~typing.List`\\\\ [:py:class:`MyClass1[int]`]\"\n    assert restify(ForwardRef(\"Union[MyClass1[int], str]\")) == \":py:obj:`~typing.Union`\\\\ [:py:class:`MyClass1[int]`, :py:class:`str`]\"\n", "def test_stringify_enum():\n    from enum import Enum\n\n    class Color(Enum):\n        RED = 1\n        GREEN = 2\n        BLUE = 3\n\n    assert stringify(Color) == 'tests.test_util_typing.test_stringify_enum.<locals>.Color'\n    assert stringify(Color, \"smart\") == '~tests.test_util_typing.test_stringify_enum.<locals>.Color'\n    assert stringify(Color.RED) == 'tests.test_util_typing.test_stringify_enum.<locals>.Color.RED'\n    assert stringify(Color.RED, \"smart\") == '~tests.test_util_typing.test_stringify_enum.<locals>.Color.RED'\n", "def test_get_type_hints():\n        pass\n\n    hints = get_type_hints(func)\n    assert hints == {'a': int, 'b': str, 'return': type(None)}\n\n    class MyClass:\n            pass\n\n    hints = get_type_hints(MyClass.method)\n    assert hints == {'a': int, 'b': str, 'return': type(None)}\n\n    # Test that get_type_hints doesn't raise an error on a function with no annotations\n        pass\n\n    hints = get_type_hints(func_no_annotations)\n    assert hints == {}\n\n    # Test that get_type_hints doesn't raise an error on a built-in function\n    hints = get_type_hints(len)\n    assert hints == {}\n"], "sample_974": ["def test_ccode_AugmentedAssignment():\n    expr = aug_assign(x, '+', y + z)\n    assert ccode(expr) == 'x += y + z;'\n    expr = aug_assign(x, '-', y + z)\n    assert ccode(expr) == 'x -= y + z;'\n    expr = aug_assign(x, '*', y + z)\n    assert ccode(expr) == 'x *= y + z;'\n    expr = aug_assign(x, '/', y + z)\n    assert ccode(expr) == 'x /= y + z;'\n    expr = aug_assign(x, '**', y + z)\n    assert ccode(expr) == 'x = pow(x, y + z);'\n", "def test_ccode_unsupported_function():\n    class CustomFunction(Function):\n        @classmethod\n            if x.is_integer:\n                return x\n\n    x = symbols('x')\n    f = CustomFunction(x)\n    with raises(NotImplementedError):\n        ccode(f)\n", "def test_ccode_Infinity():\n    assert ccode(oo) == \"HUGE_VAL\"\n    assert ccode(-oo) == \"-HUGE_VAL\"\n", "def test_ccode_For_nested():\n    f = For(x, Range(0, 10, 2), [\n        For(y, Range(0, 5, 1), [aug_assign(z, '+', x * y)])\n    ])\n    sol = ccode(f)\n    assert sol == (\"for (x = 0; x < 10; x += 2) {\\n\"\n                   \"   for (y = 0; y < 5; y += 1) {\\n\"\n                   \"      z += x*y;\\n\"\n                   \"   }\\n\"\n                   \"}\")\n", "def test_ccode_For_nested():\n    f = For(x, Range(0, 10, 2), [\n        For(y, Range(1, 8, 3), [\n            aug_assign(z, '+', x*y)\n        ])\n    ])\n    sol = ccode(f)\n    assert sol == (\"for (x = 0; x < 10; x += 2) {\\n\"\n                   \"   for (y = 1; y < 8; y += 3) {\\n\"\n                   \"      z += x*y;\\n\"\n                   \"   }\\n\"\n                   \"}\")\n"], "sample_973": ["def test_getall():\n    class Module:\n        __all__ = ['a', 'b']\n\n    assert inspect.getall(Module) == ['a', 'b']\n\n    class Module:\n        pass\n\n    assert inspect.getall(Module) is None\n\n    class Module:\n        __all__ = 'a'\n\n    with pytest.raises(ValueError):\n        inspect.getall(Module)\n\n    class Module:\n        __all__ = ['a', 1]\n\n    with pytest.raises(ValueError):\n        inspect.getall(Module)\n\n    class Module:\n        __all__ = ['a', 'b', 'a']\n\n    assert inspect.getall(Module) == ['a', 'b', 'a']\n", "def test_isenumclass():\n    class Foo:\n        pass\n\n    class Bar(enum.Enum):\n        A = 1\n\n    assert inspect.isenumclass(Foo) is False\n    assert inspect.isenumclass(Bar) is True\n\n", "def test_isabstractmethod():\n    class AbstractBaseClass:\n        @staticmethod\n        @types.coroutine\n            pass\n\n        @types.coroutine\n            pass\n\n            pass\n\n        @staticmethod\n            pass\n\n        @classmethod\n            pass\n\n    class AbstractClassExample(AbstractBaseClass):\n        @abc.abstractmethod\n            pass\n\n    assert not inspect.isabstractmethod(AbstractBaseClass.coroutine_meth)\n    assert not inspect.isabstractmethod(AbstractBaseClass.coroutine)\n    assert not inspect.isabstractmethod(AbstractBaseClass.normal_meth)\n    assert not inspect.isabstractmethod(AbstractBaseClass.static_meth)\n    assert not inspect.isabstractmethod(AbstractBaseClass.class_meth)\n    assert not inspect.isabstractmethod(AbstractClassExample().coroutine)\n    assert inspect.isabstractmethod(AbstractClassExample.do_something)\n", "def test_isasyncgenfunction():\n    async def asyncgen_func():\n        yield 1\n\n        pass\n\n    assert inspect.isasyncgenfunction(asyncgen_func) is True\n    assert inspect.isasyncgenfunction(normal_func) is False\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert inspect.getmro(C) == (C, B, A, object)\n\n"], "sample_975": ["def test_nsolve_non_square_matrix():\n    # Test that nsolve raises an error when given a non-square matrix\n    x, y, z = symbols('x y z')\n    f = Matrix([[x**2 + y**2 - 1], [x**2 - y**2 - 1], [x + y - 1]])\n    raises(ValueError, lambda: nsolve(f, (x, y), (1, 1)))\n", "def test_nsolve_no_derivative():\n    x = Symbol('x')\n    assert nsolve(sin(x)**2, x, pi/2) == pi/2\n", "def test_nsolve_system():\n    # Test that nsolve can solve multidimensional systems\n    x, y = symbols('x y')\n    f1 = x**2 + y**2 - 4\n    f2 = x**2 - y**2\n    sol = nsolve((f1, f2), (x, y), (1, 1))\n    assert abs(sol[0] - 1.4142135623730951) < 1e-12\n    assert abs(sol[1] - 1.4142135623730951) < 1e-12\n", "def test_nsolve_solvers():\n    import mpmath\n    x = Symbol('x')\n    f = sin(x)**2 - Float('0.5', 25)\n    mpmath.mp.dps = 25\n    sol1 = nsolve(f, x, 1, solver='newton')\n    sol2 = nsolve(f, x, 1, solver='secant')\n    sol3 = nsolve(f, x, 1, solver='mnewton')\n    for sol in (sol1, sol2, sol3):\n        assert abs(sin(sol)**2 - 0.5) < 1e-24\n", "def test_nsolve_rational_function():\n    x = Symbol('x')\n    # Test that nsolve can find roots of rational functions.\n    ans = nsolve((x**2 - 1) / (x - 1), 1.1)\n    assert ans == 1.0\n"], "sample_977": ["compilation error", "def test_known_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'mySin'}) == \"mySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'myCos'}) == \"myCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'myExp'}) == \"myExp[x]\"\n    assert mcode(x*y, user_functions={'Mul': 'myMul'}) == \"x*myMul[y]\"\n", "def test_user_functions():\n    x = symbols('x')\n    user_functions = {'f': 'NewF', 'g': 'NewG'}\n    assert mcode(f(x), user_functions=user_functions) == \"NewF[x]\"\n    assert mcode(sin(x), user_functions=user_functions) == \"Sin[x]\"\n    g = Function('g')\n    assert mcode(g(x), user_functions=user_functions) == \"NewG[x]\"\n", "def test_custom_functions():\n    assert mcode(sin(x), user_functions={'sin': 'mysin'}) == \"mysin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'mycos'}) == \"mycos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'myexp'}) == \"myexp[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': 'mysin', 'cos': 'mycos'}) == \"mysin[x] + mycos[x]\"\n"], "sample_976": ["def test_Wild_exclude():\n    # these tests only include Atoms\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n    p = Symbol(\"p\", positive=True)\n    k = Symbol(\"k\", integer=True)\n    n = Symbol(\"n\", integer=True, positive=True)\n\n    S = Wild(\"S\")\n    R = Wild(\"R\", exclude=[x, p])\n    Y = Wild(\"Y\", exclude=[x, y, k])\n    P = Wild(\"P\", exclude=[p])\n    K = Wild(\"K\", exclude=[k])\n    N = Wild(\"N\", exclude=[n])\n\n    given_patterns = [ x, y, p, k, -k, n, -n, sympify(-3), sympify(3),\n                       pi, Rational(3, 2), I ]\n\n    goodmatch = {\n        S: (x, y, p, k, n, -k, -n, -3, 3, pi, Rational(3, 2), I),\n        R: (y, k, n, -k, -n, -3, 3, pi, Rational(3, 2), I),\n        Y: (p, -n, -3, 3, pi, Rational(3, 2), I),\n        P: (x, y, k, n, -k, -n, -3, 3, pi, Rational(3, 2), I),\n        K: (x, y, p, n, -n, -3, 3, pi, Rational(3, 2), I),\n        N: (x, y, p, k, -k, -n, -3, 3, pi, Rational(3, 2), I),\n    }\n\n    for A in [S, R, Y, P, K, N]:\n        for pat in given_patterns:\n            d = pat.match(A)\n            if pat in goodmatch[A]:\n                assert d[A] in goodmatch[A]\n            else:\n                assert d is None\n", "def test_symbols_custom_cls():\n    class CustomSymbol(Symbol):\n        pass\n\n    x, y, z = symbols('x y z', cls=CustomSymbol)\n    assert isinstance(x, CustomSymbol)\n    assert isinstance(y, CustomSymbol)\n    assert isinstance(z, CustomSymbol)\n\n    CustomWild = type('CustomWild', (Wild, CustomSymbol), {})\n    w = symbols('w', cls=CustomWild)\n    assert isinstance(w, CustomWild)\n\n    raises(TypeError, lambda: symbols('x', cls=int))\n", "def test_symbols_iterator():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert list(symbols('x,y,z')) == [x, y, z]\n    assert tuple(symbols('x y z')) == (x, y, z)\n\n    x0 = Symbol('x0')\n    x1 = Symbol('x1')\n    x2 = Symbol('x2')\n\n    assert list(symbols('x0:3')) == [x0, x1, x2]\n    assert tuple(symbols('x:3')) == (x0, x1, x2)\n\n    a = Symbol('a')\n    b = Symbol('b')\n    c = Symbol('c')\n\n    assert set(symbols('a:c')) == {a, b, c}\n", "def test_Symbol_properties():\n    x = Symbol('x', positive=True, real=True)\n    y = Symbol('y', integer=True, real=True)\n    z = Symbol('z', real=True)\n    assert x.properties == {'positive': True, 'real': True}\n    assert y.properties == {'integer': True, 'real': True}\n    assert z.properties == {'real': True}\n", "def test_free_symbols():\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Symbol('f')\n    g = Symbol('g')\n\n    assert x.free_symbols == {x}\n    assert f(x).free_symbols == {x}\n    assert f(x, y).free_symbols == {x, y}\n    assert f(g(x, y)).free_symbols == {x, y}\n"], "sample_979": ["def test_matrixelement_as_coeff_mmul():\n    A = MatrixSymbol('A', 2, 2)\n    a11 = A[0, 0]\n    assert a11.as_coeff_mmul()[0] == 1\n    assert a11.as_coeff_mmul()[1] == a11\n    assert (3*a11).as_coeff_mmul()[0] == 3\n    assert (3*a11).as_coeff_mmul()[1] == a11\n", "def test_MatrixElement_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    A_re, A_im = A[0, 0].as_real_imag()\n    assert A_re == (A[0, 0] + conjugate(A[0, 0])) / 2\n    assert A_im == (A[0, 0] - conjugate(A[0, 0])) / (2 * I)\n", "def test_MatrixElement_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    assert isinstance(A[0, 0].as_real_imag(), tuple)\n", "def test\u0e40\u0e21\u0e15\u0e23ixExpr_canonicalize():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert (A*B).canonicalize() == MatMul(A, B)\n    assert (2*A).canonicalize() == MatMul(2, A)\n    assert (A + A).canonicalize() == MatAdd(A, A)\n    assert ZeroMatrix(n, n).canonicalize() == ZeroMatrix(n, n)\n    assert Identity(n).canonicalize() == Identity(n)\n", "def test_matrixelement_simplify():\n    i, j = symbols('i j')\n    A = MatrixSymbol('A', 2, 2)\n    assert simplify(A[i, j] + A[i, j]) == 2*A[i, j]\n    assert simplify(A[i, j] + A[i, j] - A[i, j]) == A[i, j]\n    assert simplify(A[i, j]*2 + A[i, j]*3) == 5*A[i, j]\n"], "sample_978": ["def test_repeated_degree_3():\n    d = 3\n    knots = [0, 0, 0, 1, 2, 3, 3, 3]\n    splines = bspline_basis_set(d, knots, x)\n\n    assert splines[0] == Piecewise(((-x**3/6 + x**2 - 2*x/3 + 1/6), And(x <= 1, x >= 0)),\n                                   ((x**3/6 - x**2 + 2*x/3 - 1/6), And(x <= 2, x >= 1)),\n                                   (0, True))\n    assert splines[1] == Piecewise(((x**3/6), And(x <= 1, x >= 0)),\n                                   ((-x**3/2 + 2*x**2 - 10*x/3 + 7/3), And(x <= 2, x >= 1)),\n                                   ((x**3/2 - 4*x**2 + 10*x - 22/3), And(x <= 3, x >= 2)),\n                                   (0, True))\n    assert splines[2] == Piecewise(((x**3/2 - x**2 + 4*x/3 - 5/6), And(x <= 2, x >= 1)),\n                                   ((-x**3 + 10*x**2/3 - 16*x/3 + 19/6), And(x <= 3, x >= 2)),\n                                   (0, True))\n", "def test_repeated_degree_3():\n    d = 3\n    knots = [0, 0, 0, 1, 2, 3, 4, 4, 4]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == 5  # len(knots) - d - 1 = 9 - 3 - 1 = 5\n    for spline in splines:\n        assert isinstance(spline, Piecewise)\n        assert spline.args[-1].cond == True\n", "def test_repeated_knots():\n    d = 3\n    knots = [0, 0, 0, 0, 1, 2, 3, 3, 3, 3]\n    splines = bspline_basis_set(d, knots, x)\n\n    b0 = Piecewise((-x**3 + 3*x**2 - 3*x + 1, Interval(0, 1).contains(x)),\n                   (0, True))\n    b1 = Piecewise((x**3 - 3*x**2 + 3*x, Interval(0, 1).contains(x)),\n                   (-3*x**2 + 6*x - 3, Interval(1, 2).contains(x)),\n                   (0, True))\n    assert splines[0] == b0\n    assert splines[1] == b1\n", "def test_edge_cases():\n    d = 0\n    knots = [0, 1]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == 1\n    assert splines[0] == Piecewise((1, Interval(0, 1).contains(x)),\n                                   (0, True))\n\n    d = 1\n    knots = [0, 0, 1]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == 1\n    assert splines[0] == Piecewise((1 - x, Interval(0, 1).contains(x)),\n                                   (0, True))\n\n    d = 2\n    knots = [0, 0, 0, 1]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == 1\n    assert splines[0] == Piecewise(((1 - x)**2, Interval(0, 1).contains(x)),\n                                   (0, True))\n", "def test_repeated_knots_at_ends():\n    d = 2\n    knots = [0, 0, 0, 1, 2, 3, 3, 3]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == len(knots) - d - 1\n    for spline in splines:\n        assert isinstance(spline, Piecewise)\n"], "sample_980": ["compilation error", "def test_cycle_structure():\n    p = Permutation([0])\n    assert p.cycle_structure == {1: 1}\n    p = Permutation([0, 1])\n    assert p.cycle_structure == {1: 2}\n    p = Permutation([0, 1, 2])\n    assert p.cycle_structure == {1: 3}\n    p = Permutation([1, 2, 0])\n    assert p.cycle_structure == {3: 1}\n    p = Permutation([2, 0, 1])\n    assert p.cycle_structure == {3: 1}\n    p = Permutation([1, 0, 3, 2])\n    assert p.cycle_structure == {2: 2}\n    p = Permutation([1, 2, 3, 0])\n    assert p.cycle_structure == {4: 1}\n    p = Permutation([1, 0, 3, 2, 4])\n    assert p.cycle_structure == {1: 1, 2: 2}\n    p = Permutation([1, 2, 0, 4, 3])\n    assert p.cycle_structure == {3: 1, 2: 1}\n    p = Permutation([1, 0, 3, 2, 5, 4])\n    assert p.cycle_structure == {1: 2, 2: 2}\n", "def test_af_rmul():\n    a = [0, 2, 1, 3]\n    b = [0, 1, 3, 2]\n    assert _af_rmul(a, b) == [0, 2, 3, 1]\n    assert _af_rmul(b, a) == [0, 3, 2, 1]\n    # when either permutation is empty, return the other\n    assert _af_rmul(a, []) == a\n    assert _af_rmul([], b) == b\n", "def test_cycle_structure():\n    p = Permutation(3)\n    assert p.cycle_structure == {1: 4}\n    p = Permutation(0, 4, 3)(1, 2)\n    assert p.cycle_structure == {2: 1, 3: 1}\n    p = Permutation(0, 2, 1, 4, 3)\n    assert p.cycle_structure == {3: 1, 2: 1}\n    p = Permutation(3)(1, 4, 2, 0)\n    assert p.cycle_structure == {4: 1, 1: 1}\n", "def test_cycle_structure():\n    p1 = Permutation(0, 1, 2, 4, 3)\n    assert p1.cycle_structure == {2: 1, 1: 3}\n    p2 = Permutation(0, 3)(1, 2)\n    assert p2.cycle_structure == {2: 2}\n    p3 = Permutation(0, 1, 2, 3)\n    assert p3.cycle_structure == {4: 1}\n    p4 = Permutation(0, 3, 1, 2)\n    assert p4.cycle_structure == {4: 1}\n    p5 = Permutation(1, 3, 0, 2)\n    assert p5.cycle_structure == {2: 2}\n    p6 = Permutation()\n    assert p6.cycle_structure == {}\n    p7 = Permutation(0, 1, 2)\n    assert p7.cycle_structure == {3: 1}\n"], "sample_981": ["def test_cycle_structure():\n    assert Permutation(3).cycle_structure == {1: 4}\n    assert Permutation(0, 6, 3)(1, 2).cycle_structure == {2: 1, 3: 1, 1: 2}\n    assert Permutation(4)(2, 5, 0, 3)(1, 6).cycle_structure == {2: 2, 4: 1}\n", "def test_inv():\n    p = Permutation([3, 1, 2, 0])\n    assert p.inv() == Permutation([3, 1, 2, 0])**-1 == ~Permutation([3, 1, 2, 0])\n    p = Permutation([0, 2, 1])\n    assert p.inv() == Permutation([0, 2, 1])**-1 == ~Permutation([0, 2, 1])\n", "def test_from_inversion_vector():\n    p = Permutation([1, 3, 0, 2])\n    assert Permutation.from_inversion_vector(p.inversion_vector()) == p\n    assert Permutation.from_inversion_vector([]) == Permutation([])\n    raises(ValueError, lambda: Permutation.from_inversion_vector([3, 4]))\n", "def test_cycle_structure():\n    p = Permutation(1, 2, 3)\n    assert p.cycle_structure == {3: 1}\n    p = Permutation(1, 2)(3, 4)\n    assert p.cycle_structure == {2: 2}\n    p = Permutation(1, 2)(3, 4)(5, 6)\n    assert p.cycle_structure == {2: 3}\n    p = Permutation(1, 2, 3)(4, 5, 6)\n    assert p.cycle_structure == {3: 2}\n\n    # Test singletons\n    p = Permutation(1, 2, 3)(4)\n    assert p.cycle_structure == {3: 1, 1: 1}\n    p = Permutation(1, 2, 3)(4)(5)\n    assert p.cycle_structure == {3: 1, 1: 2}\n", "compilation error"], "sample_982": ["def test_factorint_with_limit():\n    assert factorint(2**2*3**2, limit=2) == {2: 2, 3**2: 1}\n    assert factorint(2**2*3**2, limit=3) == {2: 2, 3: 2}\n    assert factorint(2**2*3**2, limit=4) == {2: 2, 3: 2}\n    assert factorint(2**2*3**2, limit=5) == {2: 2, 3: 2}\n", "def test_factorint_limit():\n    n = 2**10 * 3**5 * 5**3 * 7**2\n    assert factorint(n, limit=2**10) == {2: 10, 3: 5, 5: 3, 7: 2}\n    assert factorint(n, limit=3**5) == {2: 10, 3: 5, 5**3 * 7**2: 1}\n    assert factorint(n, limit=5**3) == {2: 10, 3: 5, 5: 3, 7**2: 1}\n    assert factorint(n, limit=7**2) == {2: 10, 3: 5, 5: 3, 7: 2}\n", "def test_primefactors():\n    assert primefactors(2) == [2]\n    assert primefactors(3) == [3]\n    assert primefactors(4) == [2]\n    assert primefactors(6) == [2, 3]\n    assert primefactors(12) == [2, 3]\n    assert primefactors(100) == [2, 5]\n    assert primefactors(1000000007) == [1000000007]\n    assert primefactors(-127) == [127]\n    assert primefactors(-2*3*5) == [2, 3, 5]\n    assert primefactors(-100) == [2, 5]\n    assert primefactors(1) == []\n    assert primefactors(0) == []\n    assert primefactors(1000000007**5) == [1000000007]\n", "def test_factorint_dict():\n    # Test factorint with a dictionary input\n    n = 2**3 * 3**2\n    d = {2: 3, 3: 2}\n    assert factorint(n) == d\n    assert factorint(d) == d\n    assert factorint(Mul(*[Pow(k, v) for k, v in d.items()], evaluate=False)) == d\n", "def test_pollard_pm1_verbose():\n    raises(ValueError, lambda: pollard_pm1(3, B=2, verbose=True))\n    raises(ValueError, lambda: pollard_pm1(3, verbose=True))\n    n = nextprime(2**16)*nextprime(2**17)\n    assert 'p-1' in capture(lambda: pollard_pm1(n, B=15, verbose=True))\n"], "sample_984": ["def test_Sample():\n    from sympy.stats import Normal\n    X = Normal('x1', 0, 1)\n    assert str(Sample(X)) == \"Sample([x1])\"\n", "def test_DMF():\n    from sympy.polys import Poly, ZZ\n    A = ZZ[x]\n    f = Poly(x**2 + 1, x, A)\n    assert str(f) == \"Poly(x**2 + 1, x, domain='ZZ[x]')\"\n", "def test_Sample():\n    from sympy.stats import Normal\n    X = Normal('x1', 0, 1)\n    assert str(X) == \"Normal(0, 1)\"\n    assert str(X.samples(5)) == \"Sample([Normal(0, 1).rv for _ in range(5)])\"\n    assert str(X.samples(5, seed=1234)) == \"Sample([Normal(0, 1).rv for _ in range(5)])\"\n", "def test_LatticeOp():\n    from sympy import meet, join\n    assert str(meet(x, y)) == \"meet(x, y)\"\n    assert str(join(x, y)) == \"join(x, y)\"\n", "def test_TensorProduct_printing():\n    from sympy.tensor.tensor import TensorProduct\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert str(TensorProduct(A, B)) == \"A*X*B\"\n"], "sample_983": ["def test_zip_row_op():\n    M = SparseMatrix.eye(3)*2\n    M[0, 1] = -1\n    M.zip_row_op(1, 0, lambda v, u: v + 2*u)\n    assert M == SparseMatrix([\n        [2, -1, 0],\n        [4,  0, 0],\n        [0,  0, 2]])\n", "def test_fill():\n    A = SparseMatrix(3, 3, {})\n    A.fill(1)\n    assert A == SparseMatrix(3, 3, {(i, j): 1 for i in range(3) for j in range(3)})\n\n    A = SparseMatrix(3, 3, {(0, 0): 1, (1, 1): 1, (2, 2): 1})\n    A.fill(0)\n    assert A == SparseMatrix(3, 3, {})\n", "def test_fill():\n    m = SparseMatrix.zeros(3)\n    assert len(m._smat) == 0\n    m.fill(1)\n    assert len(m._smat) == 9\n    assert m == SparseMatrix.ones(3)\n    m.fill(0)\n    assert len(m._smat) == 0\n    assert m == SparseMatrix.zeros(3)\n", "def test_row_col_operations():\n    s = SparseMatrix(4, 4, {})\n    s[0, 0] = 1\n    s[1, 1] = 2\n    s[2, 2] = 3\n    s[3, 3] = 4\n\n    assert s.row(0) == SparseMatrix(1, 4, [1, 0, 0, 0])\n    assert s.col(0) == SparseMatrix(4, 1, [1, 0, 0, 0])\n\n    s.row_swap(0, 1)\n    assert s.row(0) == SparseMatrix(1, 4, [0, 2, 0, 0])\n    assert s.row(1) == SparseMatrix(1, 4, [1, 0, 0, 0])\n\n    s.col_swap(0, 1)\n    assert s.col(0) == SparseMatrix(4, 1, [2, 0, 0, 0])\n    assert s.col(1) == SparseMatrix(4, 1, [0, 1, 0, 0])\n\n    s.row_del(0)\n    assert s == SparseMatrix([\n        [1, 0, 0, 0],\n        [0, 3, 0, 0],\n        [0, 0, 4, 0],\n    ])\n\n    s.col_del(0)\n    assert s == SparseMatrix([\n        [0, 0, 0],\n        [3, 0, 0],\n        [0, 4, 0],\n    ])\n\n    s.row_insert(1, SparseMatrix(1, 3, [5, 5, 5]))\n    assert s == SparseMatrix([\n        [0, 0, 0],\n        [5, 5, 5],\n        [3, 0, 0],\n        [0, 4, 0],\n    ])\n\n    s.col_insert(1, SparseMatrix(4, 1, [6, 6, 6, 6]))\n    assert s == SparseMatrix([\n        [0, 6, 0, 0],\n        [5, 6, 5, 5],\n        [3, 6, 0, 0],\n        [0, 6, ", "def test_sparse_matrix_fill():\n    s = SparseMatrix(3, 3, {})\n    s.fill(1)\n    assert s == SparseMatrix(3, 3, {(0, 0): 1, (0, 1): 1, (0, 2): 1, (1, 0): 1, (1, 1): 1, (1, 2): 1, (2, 0): 1, (2, 1): 1, (2, 2): 1})\n    s.fill(0)\n    assert s == SparseMatrix(3, 3, {})\n"], "sample_985": ["compilation error", "def test_MinMaxBase():\n    x, y, z = symbols('x y z')\n    assert isinstance(Min(x, y), MinMaxBase)\n    assert isinstance(Max(x, y), MinMaxBase)\n    assert Min(x, y)._find_localzeros([x, y]) == set([x, y])\n    assert Min(x, y)._new_args_filter([x, y]) == [x, y]\n    assert Min(x, y)._is_connected(x, y) == False\n    assert Min(x, x)._is_connected(x, x) == True\n    assert Max(x, y)._is_connected(x, y) == False\n    assert Max(x, x)._is_connected(x, x) == True\n", "def test_cbrt():\n    from sympy import Rational\n    from sympy.abc import x\n    assert cbrt(27) == 3\n    assert cbrt(Rational(27)) == 3\n    assert cbrt(-27) == 3*(-1)**(1/3)\n    assert cbrt(-27).doit() == 3*(-1)**(1/3)\n    assert cbrt(0) == 0\n    assert cbrt(x**3) != x\n    assert cbrt(x**6) != x**2\n    assert cbrt(8) == 2\n    assert cbrt(8*x**3) == 2*x\n    assert cbrt(3.5) == 3.5**(1/3)\n    assert cbrt(-8) == 2*(-1)**(1/3)\n    assert cbrt(-8).doit() == 2*(-1)**(1/3)\n    assert cbrt(x) == x**(1/3)\n", "def test_real_root():\n    from sympy import sqrt, root, Rational, real_root\n    from sympy.abc import x, n\n\n    assert real_root(-8, 3) == -2\n    assert root(-8, 3) == 2*(-1)**Rational(1, 3)\n    assert real_root(_) == -2\n    assert real_root(-32, 5) == -2\n    assert root(-32, 5, 5//2) == -2\n    assert sqrt(x**2) != x\n    assert real_root(sqrt(x**2)) != x\n    assert real_root(x**4) == x**4\n    assert real_root(x**3) == x**3\n    assert real_root(8, 3) == 2\n    assert real_root(-8, 3) == -2\n    assert real_root(16, 4) == 2\n    assert real_root(-16, 4) == -2\n    assert real_root(-16, 4) == real_root(-2**4, 4) == -2\n    assert real_root((2+2*I)**2) == 2*2**(S(1)/2)\n    assert real_root(x**2 + 2*x + 1) == x + 1\n    assert real_root((x+1)**3) == x + 1\n    assert real_root((x+1)**2) == Abs(x + 1)\n    assert real_root(x**2 + 1) == sqrt(x**2 + 1)\n", "def test_MinMaxBase():\n    # test arguments filtering\n    assert Min(1, Max(2, 3), -4, S.Infinity).args == (-4, Max(2, 3))\n    assert Max(-1, Min(2, 3), 4, S.NegativeInfinity).args == (Max(2, 3), 4)\n    assert Min(-1, -1, -1).args == (-1,)\n    assert Max(1, 1, 1).args == (1,)\n\n    # test _find_localzeros\n    assert Min._find_localzeros([-1, 0, 1]) == {-1}\n    assert Max._find_localzeros([-1, 0, 1]) == {1}\n\n    # test _is_connected\n    assert Min._is_connected(1, 2) is True\n    assert Max._is_connected(1, 2) is True\n    assert Min._is_connected(2, 1) is False\n    assert Max._is_connected(2, 1) is False\n    assert Min._is_connected(1, 1) is True\n    assert Max._is_connected(1, 1) is True\n\n    # test fdiff\n    assert Min(1, 2, 3).fdiff(1) == 1\n    assert Max(1, 2, 3).fdiff(1) == 0\n    assert Min(1, 2).fdiff(1) == 1\n    assert Max(1, 2).fdiff(1) == 1\n"], "sample_986": ["def test_issue_11179():\n    assert N(sqrt(2 + 3*I), 3, chop=True) == 1.6 + 0.8*I\n", "def test_evalf_floor_ceiling():\n    assert floor(sqrt(2)).evalf() == 1\n    assert ceiling(sqrt(2)).evalf() == 2\n    assert floor(-sqrt(2)).evalf() == -2\n    assert ceiling(-sqrt(2)).evalf() == -1\n    assert floor(pi).evalf() == 3\n    assert ceiling(pi).evalf() == 4\n    assert floor(E).evalf() == 2\n    assert ceiling(E).evalf() == 3\n", "def test_issue_11594():\n    from sympy import root, Rational\n    z = root(Rational(9, 4), 3)\n    assert z.n(3) == 1.13\n    z = root(-Rational(9, 4), 3)\n    assert z.n(3) == -1.13\n    z = root(-Rational(9, 4), 4)\n    assert z.n(3) == 0.707*I + 0.707\n    z = root(Rational(-9, 4), 3)\n    assert z.n(3) == -1.13\n    z = root(Rational(-9, 4), 4)\n    assert z.n(3) == 0.707*I + 0.707\n", "def test_evalf_hypsum():\n    from sympy import Sum, hyper\n    from sympy.abc import k\n    assert NS(Sum(1/factorial(k), (k, 0, oo))) == NS(E)\n    assert NS(Sum(1/factorial(k), (k, 1, oo))) == NS(E - 1)\n    assert NS(Sum((-1)**k/factorial(k), (k, 0, oo))) == NS(exp(-1))\n    assert NS(Sum((-1)**k/factorial(k), (k, 1, oo))) == NS(exp(-1) + 1)\n    assert NS(hyper((1, 1), (2,), 1)) == NS(Sum(1/k, (k, 1, oo)))\n", "def test_evalf_piecewise():\n    from sympy import Piecewise, symbols\n    x = symbols('x')\n    # Test that .evalf() is propagated through Piecewise arguments\n    assert Piecewise((x, x < 0), (x**2, x >= 0)).evalf() == \\\n        Piecewise((x.evalf(), x < 0), ((x**2).evalf(), x >= 0))\n    assert Piecewise((x, x < 0), (x**2, x >= 0)).evalf(subs={x: 1}) == 1.0\n"], "sample_987": ["def test_issue_10435():\n    f = Function('f')\n    assert f(1).n() == f(1)\n    assert f(1).n(2) == f(1)\n", "def test_issue_13071():\n    expr = Eq(x/5, y/10)\n    assert expr._eval_evalf() == expr\n    assert expr.lhs._eval_evalf() == x/5\n    assert expr.rhs._eval_evalf() == y/10\n    assert expr.lhs.evalf() == 0.2*x\n    assert expr.rhs.evalf() == 0.1*y\n", "def test_evalf_trig_periodic():\n    assert NS(sin(x + 2*pi, evaluate=False).subs(x, 1), 15) == NS(sin(1), 15)\n    assert NS(cos(x + 2*pi, evaluate=False).subs(x, 1), 15) == NS(cos(1), 15)\n    assert NS(sin(x + pi, evaluate=False).subs(x, 1), 15) == NS(-sin(1), 15)\n    assert NS(cos(x + pi, evaluate=False).subs(x, 1), 15) == NS(-cos(1), 15)\n", "def test_evalf_atan():\n    assert NS(atan(3), 15) == '1.24904577239825'\n    assert NS(atan(S(3)/4), 15) == '0.64350110879316'\n    assert NS(atan(pi), 15) == '1.26262725574591'\n    assert NS(atan(E), 15) == '1.12561766066696'\n    assert NS(atan(1/E), 15) == '0.881729428184005'\n    assert NS(atan(-1), 15) == '-0.785398163397448'\n    assert NS(atan(S.Half), 15) == '0.463647609000806'\n", "def test_evalf\u0131y_perfect_powers():\n    from sympy import simplify, cos, sin\n    assert simplify((2**3).evalf()) == 8\n    assert simplify((2**3.0).evalf()) == 8\n    assert simplify((2**(1/3)).evalf()) == 2**(1/3)\n    assert simplify((2**(1/3.0)).evalf()) == 1.25992104989487\n    assert simplify(cos(2*pi/3).evalf()) == -0.5\n    assert simplify(sin(2*pi/3).evalf()) == 0.866025403784439\n"], "sample_988": ["def test_issue_18750():\n    assert Eq(x/(x + 1), x).subs(x, oo) is S.true\n    assert Eq(x/(x + 1), x).subs(x, -oo) is S.false\n    assert Eq(x/(x + 1), x).subs(x, zoo) is S.false\n", "def test_relational_different_sympy_types():\n    from sympy.core.numbers import (Zero, One, NegativeOne, Half, Exp1, Pi,\n                                   GoldenRatio, EulerGamma, Catalan,\n                                   ImaginaryUnit)\n    \n    # Equality and Unequality with number and symbol\n    for num in [Zero(), One(), NegativeOne(), Half(), Exp1(), Pi(),\n                GoldenRatio(), EulerGamma(), Catalan(), ImaginaryUnit()]:\n        for op in [Eq, Ne]:\n            assert op(num, num) is (S.true if op is Eq else S.false)\n            assert op(num, x) == op(num, x, evaluate=False)\n            assert op(x, num) == op(x, num, evaluate=False)\n\n    # Equality with two different instances of same type\n    for num in [Zero(), One(), NegativeOne(), Half(), Exp1(), Pi(),\n                GoldenRatio(), EulerGamma(), Catalan(), ImaginaryUnit()]:\n        assert Eq(num, type(num)()) is S.true\n\n    # Test that equality is commutative\n    for num in [Zero(), One(), NegativeOne(), Half(), Exp1(), Pi(),\n                GoldenRatio(), EulerGamma(), Catalan(), ImaginaryUnit()]:\n        for op in [Eq, Ne]:\n            assert op(num, x) == op(x, num)\n\n    # Ordering with number and symbol\n    for num in [Zero(), One(), NegativeOne(), Half(), Exp1(), Pi(),\n                GoldenRatio(), EulerGamma(), Catalan()]:\n        for op in [Lt, Le, Gt, Ge]:\n            assert op(num, x) == op(num, x, evaluate=False)\n            assert op(x, num) == op(x, num, evaluate=False)\n\n    # Ordering with two different instances of same type\n    for num in [Zero(), One(), NegativeOne(), Half(), Exp1(), Pi(),\n                GoldenRatio(), EulerGamma(), Catalan()]:\n        assert Lt(num, type(num)()) is S.false\n        assert Le(num, type(num)()) is S.true\n        assert Gt(num, type(num)()) is S.false\n        assert Ge(num, type(num)()) is S.true\n", "def test_relational_substitution_with_derivative():\n    f = Function('f')\n    g = Function('g')\n    x = Symbol('x')\n    y = Symbol('y')\n    assert (Derivative(f(x), x) > 0).subs(f(x), g(x)) == Derivative(g(x), x) > 0\n    assert (Derivative(f(x), x) > 0).subs(f(x), y) == Derivative(y, x) > 0\n", "def test_issue_14364():\n    assert (1*pi < 3*pi) is S.true\n    assert (1*pi > 3*pi) is S.false\n    assert (1*pi <= 3*pi) is S.true\n    assert (1*pi >= 3*pi) is S.false\n    assert Eq(1*pi, 3*pi) is S.false\n    assert Ne(1*pi, 3*pi) is S.true\n", "def test_issue_16365():\n    assert Eq(1, 1) == Eq(1.0, 1.0)\n    assert Eq(1, 1) == Eq(1.0, 1)\n    assert Eq(1, 1) == Eq(1, 1.0)\n    assert Eq(1.0, 1.0) == Eq(1, 1)\n    assert Eq(1.0, 1.0) == Eq(1.0, 1)\n    assert Eq(1.0, 1.0) == Eq(1, 1.0)\n    assert Eq(1.0, 1) == Eq(1, 1)\n    assert Eq(1.0, 1) == Eq(1.0, 1.0)\n    assert Eq(1.0, 1) == Eq(1, 1.0)\n    assert Eq(1, 1.0) == Eq(1, 1)\n    assert Eq(1, 1.0) == Eq(1.0, 1.0)\n    assert Eq(1, 1.0) == Eq(1.0, 1)\n"], "sample_989": ["def test_integer_division():\n    assert 5 // S(2) == 2\n    assert -5 // S(2) == -3\n    assert 5 // -S(2) == -3\n    assert -5 // -S(2) == 2\n    assert 5 // S(2.5) == 2\n    assert -5 // S(2.5) == -3\n    assert 5 // -S(2.5) == -3\n    assert -5 // -S(2.5) == 2\n", "def test_Float_automatic_precision():\n    assert Float(\"1.23456789012345678901234567890123456789\") == Float(\"1.23456789012345678901234567890123456789\", '')\n    assert Float(\"1.23456789012345678901234567890123456789e-100\") == Float(\"1.23456789012345678901234567890123456789e-100\", '')\n", "def test_issue_10438():\n    assert Integer(2) / (Integer(1) / Integer(2)) == 4\n    assert Integer(2) / (S.One / Integer(2)) == 4\n    assert Integer(2) / (Integer(1) / S.One) == 2\n", "def test_mpmpath_context():\n    from sympy import log\n    from mpmath import mp\n    original_prec = mp.dps\n    original_exp = mp.exp\n    try:\n        mp.dps = 20\n        mp.exp = lambda x: log(x)  # something weird\n        assert Float(3.0) == 3.0\n        assert exp(3.0).evalf() == exp(3.0)\n    finally:\n        mp.dps = original_prec\n        mp.exp = original_exp\n", "def test_isqrt_newton_round():\n    from sympy.ntheory.residue_ntheory import discrete_log\n    p = 2**20 + 7\n    assert isqrt_newton_round(p**2) == p\n    assert isqrt_newton_round((p+1)**2) == p+1\n\n    assert isqrt_newton_round(2**400) == 2**200\n    assert isqrt_newton_round(2**400 + 1) == 2**200\n\n    assert isqrt_newton_round(10**50) == 10**25\n    assert isqrt_newton_round(10**50 + 1) == 10**25\n\n    assert isqrt_newton_round(10**100) == 10**50\n    assert isqrt_newton_round(10**100 + 1) == 10**50\n\n    # Check that the function works with large numbers\n    from sympy import randprime\n    q = randprime(2**1000, 2**1001)\n    assert isqrt_newton_round(q**2) == q\n"], "sample_990": ["def test_real_imag():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = x + I*y\n    assert sinh(z).as_real_imag()[0] == sinh(x)*cos(y)\n    assert sinh(z).as_real_imag()[1] == cosh(x)*sin(y)\n    assert cosh(z).as_real_imag()[0] == cosh(x)*cos(y)\n    assert cosh(z).as_real_imag()[1] == sinh(x)*sin(y)\n    assert tanh(z).as_real_imag()[0] == sinh(x)*cosh(x)/(sinh(x)**2 + cos(y)**2)\n    assert tanh(z).as_real_imag()[1] == sin(y)*cos(y)/(sinh(x)**2 + cos(y)**2)\n    assert coth(z).as_real_imag()[0] == sinh(x)*cosh(x)/(sinh(x)**2 + sin(y)**2)\n    assert coth(z).as_real_imag()[1] == -sin(y)*cos(y)/(sinh(x)**2 + sin(y)**2)\n    assert csch(z).as_real_imag()[0] == sinh(x)*cosh(x)/(sinh(x)**2*cosh(x)**2 + sin(y)**2*cosh(x)**2)\n    assert csch(z).as_real_imag()[1] == -sin(y)*cos(y)/(sinh(x)**2*cosh(x)**2 + sin(y)**2*cosh(x)**2)\n    assert sech(z).as_real_imag()[0] == cosh(x)*cos(y)/(sinh(x)**2*sin(y)**2 + cos(y)**2*cosh(x)**2)\n    assert sech(z).as_real_imag()[1] == -sinh(x)*sin(y)/(sinh(x)**2*sin(y)**2 + cos(y)**2*cosh(x)**2)\n", "def test_is_real():\n    x = Symbol('x', real=True)\n    z = Symbol('z')\n    assert sinh(x).is_real\n    assert cosh(x).is_real\n    assert tanh(x).is_real\n    assert coth(x).is_real\n    assert sech(x).is_real\n    assert csch(x).is_real\n    assert asinh(x).is_real\n    assert acosh(x).is_real is None\n    assert atanh(x).is_real\n    assert acoth(x).is_real\n    assert asech(x).is_real is None\n    assert acsch(x).is_real\n\n    assert sinh(z).is_real is None\n    assert cosh(z).is_real is None\n    assert tanh(z).is_real is None\n    assert coth(z).is_real is None\n    assert sech(z).is_real is None\n    assert csch(z).is_real is None\n    assert asinh(z).is_real is None\n    assert acosh(z).is_real is None\n    assert atanh(z).is_real is None\n    assert acoth(z).is_real is None\n    assert asech(z).is_real is None\n    assert acsch(z).is_real is None\n", "def test_acsch_series():\n    x = Symbol('x')\n    t6 = acsch(x).taylor_term(6, x)\n    assert t6 == 37*x**6/5760\n    assert acsch(x).taylor_term(8, x, t6, 0) == 127*x**8/40320\n", "def test_csch_expansion():\n    x, y = symbols('x,y')\n    assert csch(x+y).expand(trig=True) == (csch(x)*csch(y)*(cosh(x)*cosh(y) + sinh(x)*sinh(y)))/(cosh(x)*sinh(y) + cosh(y)*sinh(x))\n    assert csch(2*x).expand(trig=True) == csch(x)**2 / (2*cosh(x))\n    assert csch(3*x).expand(trig=True).expand() == csch(x)**3 / (4*cosh(x)**2 - 1)\n", "def test_trig_arg():\n    x, y = symbols('x,y')\n    assert sinh(I*x).simplify() == I*sin(x)\n    assert cosh(I*x).simplify() == cos(x)\n    assert tanh(I*x).simplify() == I*tan(x)\n    assert coth(I*x).simplify() == -I*cot(x)\n    assert sech(I*x).simplify() == sec(x)\n    assert csch(I*x).simplify() == -I*csc(x)\n"], "sample_992": ["def test_PythonCodePrinter_settings():\n    prntr = PythonCodePrinter({'fully_qualified_modules': False})\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.module_imports == {'math': {'pi'}}\n    assert prntr.doprint(acos(x)) == 'acos(x)'\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n", "def test_PythonCodePrinter_piecewise():\n    prntr = PythonCodePrinter()\n    expr = x**2 if x > 0 else x + 1\n    assert prntr.doprint(expr) == '((x**2) if (x > 0) else (x + 1))'\n    expr = x**2 if x > 0 else x + 1 if x == 0 else x - 1\n    assert prntr.doprint(expr) == '(((x**2) if (x > 0) else ((x + 1) if (x == 0) else (x - 1))))'\n", "def test_NumPyPrinter():\n    p = NumPyPrinter()\n    assert p.doprint(x**y) == 'x**y'\n    assert p.doprint(Mod(x, 2)) == 'numpy.mod(x, 2)'\n    assert p.doprint(And(x, y)) == 'numpy.logical_and.reduce((x, y))'\n    assert p.doprint(Or(x, y)) == 'numpy.logical_or.reduce((x, y))'\n    assert p.doprint(pi) == 'numpy.pi'\n    assert p.doprint(acos(x)) == 'numpy.arccos(x)'\n", "def test_NumPyPrinter_piecewise():\n    p = NumPyPrinter()\n    expr = Piecewise((x, x > 0), (y, True))\n    result = p.doprint(expr)\n    assert result == 'numpy.select([x > 0, True], [x, y], default=numpy.nan)'\n    assert 'numpy' in p.module_imports\n    assert any(m.startswith('numpy') for m in p.module_imports)\n", "def test_PythonCodePrinter_settings():\n    prntr = PythonCodePrinter({'fully_qualified_modules': False})\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.module_imports == {'math': {'pi'}}\n"], "sample_991": ["def test_product_empty():\n    assert product(1, (n, 1, 0)) == 1\n    assert product(n, (n, 1, 0)) == 1\n    assert product(n, (n, 1, 1)) == 1\n    assert Product(n, (n, 1, 0)).doit() == 1\n    assert Product(n, (n, 1, 1)).doit() == 1\n", "def test_issue_21443():\n    n = Symbol('n', integer=True, positive=True)\n    p = Product((-1)**n / n, (n, 1, oo))\n    assert p.is_convergent() is S.false\n    assert product((-1)**n / n, (n, 1, oo)) == p.doit()\n", "def test_Product_doit_deep():\n    # Test that doit works correctly with deep=True\n    p = Product(2**k, (k, 1, n))\n    assert p.doit(deep=True) == 2**(n*(n + 1)/2)\n\n    p = Product(2**k, (k, 1, n)).doit()\n    assert p.doit(deep=True) == 2**(n*(n + 1)/2)\n\n    # Test that deep=False returns an unevaluated Product object\n    p = Product(2**k, (k, 1, n))\n    assert isinstance(p.doit(deep=False), Product)\n", "def test_product_doit_with_deep_option():\n    assert Product(1 + 1/n, (n, 1, 3)).doit(deep=True) == 8*Rational(21, 20)\n    assert Product(1 + 1/n, (n, 1, 3)).doit(deep=False) == Product(Rational(21, 20), (n, 4, 3))\n", "def test_issue_20391():\n    n = Symbol('n', integer=True, positive=True)\n    p = Product(n**n, (n, 1, 10))\n    assert p.doit() == 1\n"], "sample_993": ["def test_FreeGroupElm_index():\n    assert x*y*z == (x*y*z)[0]*(x*y*z)[1]*(x*y*z)[2]\n    assert (x*y*z).index(x) == 0\n    assert (x*y*z).index(y) == 1\n    assert (x*y*z).index(z) == 2\n    raises(ValueError, lambda: (x*y*z).index(x*y))\n", "def test_FreeGroupElm_cyclic_reduction():\n    assert (x**2*y*x**-1).cyclic_reduction() == x*y\n    assert (x**-3*y**-1*x**5).cyclic_reduction() == y**-1*x**2\n    assert (x*y**2*x*y**2).cyclic_reduction() == x*y**2*x*y**2\n\n    assert (x**2*y*x**-1).cyclic_reduction(removed=True) == (x*y, x)\n    assert (x**-3*y**-1*x**5).cyclic_reduction(removed=True) == (y**-1*x**2, x**-3)\n    assert (x*y**2*x*y**2).cyclic_reduction(removed=True) == (x*y**2*x*y**2, x**0)\n", "def test_FreeGroupElm_cyclic_reduction():\n    w1 = x**2*y**-1*x**-1\n    assert w1.cyclic_reduction() == y**-1\n\n    w2 = x**-3*y**-1*x**5\n    assert w2.cyclic_reduction() == y**-1*x**2\n\n    w3 = x**-3*y**-2*x**3\n    assert w3.cyclic_reduction() == y**-2\n\n    w4 = x*y**2*x*y**2\n    assert w4.cyclic_reduction() == w4\n", "def test_FreeGroupElm_cyclic_reduction():\n    w1 = x**2*y**-1*x**-1\n    assert w1.cyclic_reduction() == x*y**-1\n    w2 = y**-3*x**5*y**3\n    assert w2.cyclic_reduction() == x**5\n    w3 = y**-3*x**-1*y**3\n    assert w3.cyclic_reduction() == x**-1\n", "def test_FreeGroupElm_is_dependent():\n    assert (x**4*y**-3).is_dependent(x**4*y**-2)\n    assert not (x**2*y**-1).is_dependent(x*y)\n    assert (x*y**2*x*y**2).is_dependent(x*y**2)\n    assert (x**12).is_dependent(x**-4)\n    assert not (x**2*y).is_dependent(y**-2*x**-2)\n\n    w1 = x*y**-2\n    w2 = x*y**-1\n    assert w1.is_dependent(w2)\n    assert w1.is_dependent(w2**-1)\n\n    w3 = x*y**-1*z\n    w4 = x*z*y**-1\n    assert not w3.is_dependent(w4)\n    assert not w3.is_dependent(w4**-1)\n"], "sample_994": ["def test_issue_13599():\n    assert 1/(1/S.Infinity) == S.Infinity\n    assert 1/(1/S.NegativeInfinity) == S.NegativeInfinity\n    assert 1/(1/S.ComplexInfinity) == S.ComplexInfinity\n\n    assert 1/S.Infinity == S.Zero\n    assert 1/S.NegativeInfinity == S.Zero\n    assert 1/S.ComplexInfinity == S.Zero\n    assert 1/S.NaN == S.NaN\n\n    assert S.Infinity/S.Infinity == S.NaN\n    assert S.NegativeInfinity/S.NegativeInfinity == S.NaN\n    assert S.ComplexInfinity/S.ComplexInfinity == S.NaN\n    assert S.Infinity/S.NegativeInfinity == S.NaN\n    assert S.NegativeInfinity/S.Infinity == S.NaN\n    assert S.Infinity/S.ComplexInfinity == S.NaN\n    assert S.NegativeInfinity/S.ComplexInfinity == S.NaN\n    assert S.ComplexInfinity/S.Infinity == S.NaN\n    assert S.ComplexInfinity/S.NegativeInfinity == S.NaN\n", "def test_mpf_ground():\n    from mpmath import mpf\n    x = mpf('1.0')\n    y = mpf('2.0')\n    assert (x + y).as_real_imag()[1] == 0.0\n    assert Float(x)._as_real_imag()[1] == 0.0\n    assert Float(y)._as_real_imag()[1] == 0.0\n    assert (Float(x) + Float(y)).as_real_imag()[1] == 0.0\n", "def test_issue_24282():\n    assert Rational('1e20/1e-2') == Integer(10**22)\n    assert Rational('0.1e20/1e-2') == Integer(10**21)\n", "def test_mpf_normlize():\n    from mpmath.libmp import from_man_exp, normalize, fnan\n    from sympy.core.numbers import mpf_norm\n\n    assert mpf_norm((1, 0, 1, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, -123, -1), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, -456, -2), 10) == fnan\n    assert mpf_norm((1, 0, -789, -3), 10) == fnan\n    assert mpf_norm((0, 17, 1, 3), 10) == (0, 17, 1, 3)\n    assert mpf_norm((0, 17, 10, 3), 10) == (0, 17, 10, 3)\n    assert mpf_norm((0, 17, 1000000, 3), 10) == (0, 17, 1000000, 3)\n    assert mpf_norm((0, 17, -1000000, 3), 10) == (1, 17, 1000000, 3)\n", "def test_Rational_invert():\n    r = Rational(4, 7)\n    assert r.invert(11) == 8\n    assert r.invert(S(11)) == 8\n    assert r.invert(11.) == 8\n    assert r.invert(S(11.)) == 8\n    assert r.invert(sqrt(2)) == 1/r\n    assert r.invert(sqrt(11)) == 1/r\n"], "sample_995": ["def test_Float_hash():\n    x = Float(1.23)\n    assert hash(x) == hash(x)\n    assert hash(x) != hash(Float(1.24))\n", "def test_mod_inverse_fractions():\n    assert mod_inverse(Rational(3, 11), 7) == 5\n    assert mod_inverse(Rational(11, 3), 7) == 3\n    x = Symbol('x')\n    raises(TypeError, lambda: mod_inverse(Rational(2, 3), x))\n", "def test_Rational_approximation_interval():\n    a = Rational(1, 3)\n    assert a.approximation_interval(Integer) == (0, 1)\n    assert a.approximation_interval(Rational) == (Rational(1, 4), Rational(1, 2))\n\n    b = Rational(2, 3)\n    assert b.approximation_interval(Integer) == (0, 1)\n    assert b.approximation_interval(Rational) == (Rational(1, 2), Rational(3, 4))\n", "def test_mod_inverse_2arg():\n    assert mod_inverse(3, 5) == 2\n    assert mod_inverse(3, 7) == 5\n    assert mod_inverse(1, 3) == 1\n    assert mod_inverse(0, 3) == 0\n    raises(ValueError, lambda: mod_inverse(2, 4))\n    raises(ValueError, lambda: mod_inverse(2, 1))\n    raises(ValueError, lambda: mod_inverse(3, 0))\n", "def test_Float_hash():\n    # Regression test for issue #14634\n    assert hash(Float(2.0)) == hash(2.0)\n"], "sample_996": ["def test_issue_empty_product():\n    n = Symbol('n', integer=True)\n    p = Product(n, (n, 1, 0)).doit()\n    assert p == 1\n\n    p = Product(n, (n, 5, 4)).doit()\n    assert p == 1\n\n    p = Product(n, (n, n, n-1)).doit()\n    assert p == 1\n", "def test_issue_21341():\n    n = Symbol('n')\n    k = Symbol('k')\n    p = Product(n**2 + 1 / 2**k, (k, 0, n-1)).doit()\n    assert p.subs(n, 2).doit() == S(65)/4\n", "def test_issue_19534():\n    a, n = symbols('a n')\n    p = Product(a, (n, 1, 0)).doit()\n    assert p == 1 / a\n", "def test_issue_Product_doit():\n    n = Symbol('n')\n    f = Function('f')\n    assert Product(f(n), (n, 1, 1)).doit() == f(1)\n    assert Product(f(n), (n, 2, 1)).doit() == 1 / f(2)\n", "def test_issue_17473():\n    n, k = symbols('n k', integer=True, positive=True)\n    expr = Product(n**(1/n), (n, 1, k)).doit()\n    assert expr.limit(k, oo) == 1\n"], "sample_997": ["def test_evaluateFalse():\n    # Ensure evaluateFalse handles subexpressions as expected\n    s = \"(x + 3) * (2 - 1)\"\n    expected = \"(x + 3)*(2 - 1)\"\n    assert evaluateFalse(s).body[0].value.func.id == \"Mul\"\n    assert str(evaluateFalse(s).body[0].value.args[0]) == \"(x + 3)\"\n    assert str(evaluateFalse(s).body[0].value.args[1]) == \"(2 - 1)\"\n\n    # Ensure evaluateFalse handles subtraction as expected\n    s = \"x - y\"\n    expected = \"x + (-y)\"\n    assert str(evaluateFalse(s).body[0].value) == expected\n\n    # Ensure evaluateFalse handles division as expected\n    s = \"x / y\"\n    expected = \"x*Pow(y, -1, evaluate=False)\"\n    assert str(evaluateFalse(s).body[0].value) == expected\n", "def test_implicit_application():\n    transformations = standard_transformations + (implicit_application,)\n    x = Symbol('x')\n    assert parse_expr(\"cot x + csc x\", transformations=transformations) == parse_expr(\"cot(x) + csc(x)\")\n", "def test_function_exponentiation():\n    transformations = standard_transformations + (function_exponentiation, )\n    x = Symbol('x')\n    assert parse_expr(\"sin**2(x)\", transformations=transformations) == sin(x)**2\n    assert parse_expr(\"sin**3(x)\", transformations=transformations) == sin(x)**3\n", "def test_function_exponentiation():\n    transformations = standard_transformations + (function_exponentiation,)\n    x = Symbol('x')\n    assert parse_expr('sin**2(x)', transformations=transformations) == sin(x)**2\n    assert parse_expr('sin**4(x)', transformations=transformations) == sin(x)**4\n", "def test_lambda_notation():\n    inputs = {\n        'lambda x: x**2': 'Lambda(x, x**2)',\n        'lambda x, y: x + y': 'Lambda((x, y), x + y)',\n    }\n    transformations = standard_transformations + (lambda_notation, )\n    for text, result in inputs.items():\n        assert str(parse_expr(text, transformations=transformations)) == result\n\n    # Test syntax error when using \"lambda\" alone\n    raises(TokenError, lambda: parse_expr('lambda', transformations=transformations))\n"], "sample_998": ["def test_Tuple_printing():\n    assert latex(Tuple(1, 2, 3)) == r\"\\left ( 1, \\quad 2, \\quad 3\\right )\"\n    assert latex(Tuple(1, 2, 3, x)) == r\"\\left ( 1, \\quad 2, \\quad 3, \\quad x\\right )\"\n", "def test_issue_14866():\n    from sympy.printing.latex import LatexPrinter\n    from sympy.core.function import AppliedUndef\n    A = Symbol('A')\n    B = Symbol('B')\n    f = Function('f')\n    base = f(A, B)\n    expr = base.diff(A)\n    assert LatexPrinter().doprint(expr) == r'\\frac{\\partial}{\\partial A} f{\\left (A,B \\right )}'\n    assert latex(expr) == r'\\frac{\\partial}{\\partial A} f{\\left (A,B \\right )}'\n\n    base = AppliedUndef('f', A, B)\n    expr = base.diff(A)\n    assert LatexPrinter().doprint(expr) == r'\\frac{\\partial}{\\partial A} f{\\left (A,B \\right )}'\n    assert latex(expr) == r'\\frac{\\partial}{\\partial A} f{\\left (A,B \\right )}'\n", "compilation error", "def test_latex_Tuple():\n    assert latex(Tuple(x, y, z)) == r'\\left ( x, \\quad y, \\quad z\\right )'\n", "def test_issue_19439():\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices, TensorHead\n    Lorentz = TensorIndexType(\"Lorentz\", dim=4)\n    m, n, p = tensor_indices(\"m n p\", Lorentz)\n    A = TensorHead(\"A\", [Lorentz]*2, comm=2)\n    assert latex(A(m, n)) == r\"A{\\left (m,n \\right )}\"\n    assert latex(A(n, m)) == r\"A{\\left (m,n \\right )}\"\n    assert latex(A(n, p)*A(m, n)) == r\"A{\\left (m,n \\right )} A{\\left (p,n \\right )}\"\n    assert latex(A(n, p)*A(n, m)) == r\"A{\\left (m,n \\right )} A{\\left (p,n \\right )}\"\n"], "sample_999": ["def test_issue_16394():\n    expr = symbols(\"x_0\")\n    assert latex(expr) == \"x_{0}\"\n    expr = symbols(\"x_0_1\")\n    assert latex(expr) == \"x_{0 1}\"\n    expr = symbols(\"x_0_1_2\")\n    assert latex(expr) == \"x_{0 1 2}\"\n    expr = symbols(\"x_0_12\")\n    assert latex(expr) == \"x_{0 12}\"\n", "def test_DMP_printing():\n    from sympy.polys.domains import ZZ, QQ\n    from sympy.polys.rings import ring\n\n    R, x, y, z = ring(\"x y z\", ZZ)\n    f = x**4 + 3*y*z + x**2*y + x + 1\n    assert latex(f) == \"x^{4} + 3 y z + x^{2} y + x + 1\"\n\n    R, x, y, z = ring(\"x y z\", QQ)\n    f = x**4 + y*z/3 + x**2*y + x + 1\n    assert latex(f) == \"x^{4} + \\frac{1}{3} y z + x^{2} y + x + 1\"\n", "def test_issue_16545():\n    assert latex(S.Half**-n) == r\"2^{n}\"\n", "def test_DMP_printing():\n    from sympy import ZZ\n    from sympy.polys.domains import ZZ\n    from sympy.polys.rings import ring\n    R, x,y = ring(\"x,y\", ZZ)\n    assert latex(R.convert(x**2 + 2 * x)) == r\"\\operatorname{DMP}{\\left( x^{2} + 2 x, x, y, domain=\\mathbb{Z} \\right)}\"\n    assert latex(R.convert(x/y)) == r\"\\operatorname{DMP}{\\left( \\frac{x}{y}, x, y, domain=\\mathbb{Z}\\left(y\\right) \\right)}\"\n    assert latex(R.convert(2.0*x + y)) == r\"\\operatorname{DMP}{\\left( 2.0 x + 1.0 y, x, y, domain=\\mathbb{R} \\right)}\"\n", "def test_issue_15781():\n    from sympy.parsing.sympy_parser import parse_expr\n    e = parse_expr(\"(1 + x)**-1\", evaluate=False)\n    assert latex(e) == r\"\\left(1 + x\\right)^{-1}\"\n"], "sample_1000": ["def test_octave_inline_piecewise_for_loops():\n    i = symbols('i', integer=True)\n    x = symbols('x')\n    f = Piecewise((x**2, x < i), (x**3, x < 2*i), (x**4, True))\n    assert mcode(f, assign_to='r', inline=True) == (\n        \"r = ((x < i).*(x.^2) + (~(x < i)).*( ...\\n\"\n        \"(x < 2*i).*(x.^3) + (~(x < 2*i)).*(x.^4)));\\n\")\n    assert mcode(f, assign_to='r', inline=False) == (\n        \"for i = i\\n\"\n        \"  if (x < i)\\n\"\n        \"    r = x.^2;\\n\"\n        \"  elseif (x < 2*i)\\n\"\n        \"    r = x.^3;\\n\"\n        \"  else\\n\"\n        \"    r = x.^4;\\n\"\n        \"  end\\n\"\n        \"end\\n\")\n", "def test_Indexed_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 3, 3)\n    i = Symbol('i', integer=True)\n    j = Symbol('j', integer=True)\n\n    assert mcode(A[i, j]) == \"A(i + 1, j + 1)\"\n    assert mcode(A[i, 2]) == \"A(i + 1, 3)\"\n    assert mcode(A[1, j]) == \"A(2, j + 1)\"\n    assert mcode(A[1, 2]) == \"A(2, 3)\"\n", "def test_max_min():\n    assert mcode(Max(x, y)) == \"max(x, y)\"\n    assert mcode(Min(x, y)) == \"min(x, y)\"\n    assert mcode(Max(x, y, z)) == \"max(x, max(y, z))\"\n    assert mcode(Min(x, y, z)) == \"min(x, min(y, z))\"\n    assert mcode(Max(x, Max(y, z))) == \"max(x, max(y, z))\"\n    assert mcode(Min(x, Min(y, z))) == \"min(x, min(y, z))\"\n    assert mcode(Max(x, Min(y, z))) == \"max(x, min(y, z))\"\n    assert mcode(Min(x, Max(y, z))) == \"min(x, max(y, z))\"\n", "def test_Indexed_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 1, 3)\n    i = Symbol('i', integer=True)\n\n    assert mcode(A[0, i]) == \"A(1, i + 1)\"\n    assert mcode(3 * A[0, i]) == \"3*A(1, i + 1)\"\n", "def test_Indexed_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    i = Symbol('i')\n    j = Symbol('j')\n\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(3 * A[i, j]) == \"3*A(i, j)\"\n\n    F = A[i, j].subs(A, B)\n    assert mcode(F) == \"B(i, j)\"\n"], "sample_1003": ["def test_build_options():\n    assert build_options((x, y, z), {'domain': 'ZZ'}) == {\n        'auto': False,\n        'domain': ZZ,\n        'gens': (x, y, z)\n    }\n\n    assert build_options({'opt': Options((x, y, z), {'domain': 'ZZ'})}) == {\n        'auto': False,\n        'domain': ZZ,\n        'gens': (x, y, z)\n    }\n\n    raises(OptionError, lambda: build_options((x, y, z), {'opt': Options((x, y, z), {'domain': 'ZZ'})}))\n", "def test_build_options():\n    assert build_options({}, {'domain': 'ZZ'}) == Options((), {'domain': 'ZZ'})\n    assert build_options({}, {'opt': Options((), {'domain': 'ZZ'})}) == Options((), {'domain': 'ZZ'})\n    assert build_options((x, y, z), {'domain': 'ZZ'}) == Options((x, y, z), {'domain': 'ZZ'})\n\n    raises(OptionError, lambda: build_options((x, y, z), {'gens': (x, y)}))\n", "def test_Composite_postprocess():\n    opt = {'composite': True}\n    Composite.postprocess(opt)\n\n    assert opt == {'composite': True}\n", "def test_Gens_excludes():\n    raises(OptionError, lambda: Options((x, y, z), {'gens': (x, y), 'wrt': (x, y)}))\n", "def test_Composite_preprocess():\n    assert Composite.preprocess(False) is False\n    assert Composite.preprocess(True) is True\n\n    assert Composite.preprocess(0) is False\n    assert Composite.preprocess(1) is True\n\n    raises(OptionError, lambda: Composite.preprocess(x))\n\n"], "sample_1001": ["def test_latex_radian():\n    expr1 = 90*radian\n    assert latex(expr1) == r\"90 \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\detokenize {radian}\"\n    expr3 = cos(x*radian + 90*radian)\n    assert latex(expr3) == r'\\cos{\\left (x \\detokenize {radian} + 90 \\detokenize {radian} \\right )}'\n", "def test_latex_radian():\n    expr1 = 2*radian\n    assert latex(expr1) == r\"2 \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\detokenize {radian}\"\n    expr3 = sin(x*radian + pi/2*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\detokenize {radian} + \\frac{\\pi}{2} \\detokenize {radian} \\right )}'\n", "def test_latex_MatPow():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(A**3) == r\"A^{3}\"\n    assert latex(A**-1) == r\"A^{-1}\"\n    assert latex(A**0) == r\"I\"\n", "def test_latex_radian():\n    expr1 = 90*radian\n    assert latex(expr1) == r\"90 \\operatorname{rad}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\operatorname{rad}\"\n    expr3 = cos(x*radian + 90*radian)\n    assert latex(expr3) == r'\\cos{\\left (x \\operatorname{rad} + 90 \\operatorname{rad} \\right )}'\n", "def test_latex_Tuple():\n    t = Tuple(x, y)\n    assert latex(t) == r\"\\left ( x, \\quad y\\right )\"\n"], "sample_1002": ["def test_mod_inverse_issue():\n    a = 2\n    m = S(3)/2\n    assert mod_inverse(a, m) == 1/a\n    a = 3\n    m = S(5)/2\n    assert mod_inverse(a, m) == 1/a\n    a = 3\n    m = S(5)/3\n    assert mod_inverse(a, m) == 1/a\n    a = 2\n    m = 4.5\n    assert mod_inverse(a, m) == 1/a\n    a = 3\n    m = 5.5\n    assert mod_inverse(a, m) == 1/a\n", "def test_mpf_norm_special():\n    assert mpf_norm((1, 0, -123, -1), 10) == mpf('inf')._mpf_\n    assert mpf_norm((1, 0, -456, -2), 10) == mpf('-inf')._mpf_\n    assert mpf_norm((0, 0, -789, -3), 10) == mpf('0')._mpf_\n", "def test_Rational_nonzero():\n    assert Rational(1, 2).is_nonzero == True\n    assert Rational(0, 2).is_nonzero == False\n    assert Rational(3, 0).is_nonzero == True\n    assert Rational(0, 0).is_nonzero == False\n", "def testPow_decimal_exponent():\n    assert Pow(2, 3.0, evaluate=False).n() == 8.0\n    assert Pow(2, 3.1, evaluate=False).n() == 2**3.1\n", "def test_Rational_precision():\n    # Make sure Rational inputs for keyword args work\n    assert Float('1.0', dps=Rational(15))._prec == 53\n    assert Float('1.0', precision=Rational(15))._prec == 15\n    assert type(Float('1.0', precision=Rational(15))._prec) == int\n    assert sympify(srepr(Float('1.0', precision=15))) == Float('1.0', precision=15)\n"], "sample_1004": ["def test_condition_is_false():\n    assert ConditionSet(x, Eq(1, 2), S.Reals) is S.EmptySet\n    assert ConditionSet(x, Eq(x, 1), S.Reals).subs(x, 2) is not S.EmptySet\n    assert ConditionSet(x, Eq(x, 1), Interval(2, 3)) is S.EmptySet\n    assert ConditionSet(x, Eq(x, 1), Interval(0, 2)) is not S.EmptySet\n", "def test_CondSet_with_unsolved_equations():\n    C = ConditionSet\n    I = S.Integers\n    eq1 = Eq(x + y, 1)\n    eq2 = Eq(x - y, 2)\n    c = C((x, y), {eq1, eq2}, I)\n    assert c.base_set == I\n    assert c.condition == FiniteSet(eq1, eq2)\n    assert c.sym == (x, y)\n    assert c.free_symbols == set()\n    assert c.contains(1) == And(eq1.subs(x, 1), eq2.subs(x, 1))\n    assert c.subs(x, z) == c\n", "def test_CondSet_lambda():\n    lamda = Symbol('lambda')\n    x = Symbol('x')\n    cond_set = ConditionSet(lamda, lamda < 1, Interval(0, 2))\n    assert cond_set.subs(lamda, x) == ConditionSet(x, x < 1, Interval(0, 2))\n    assert cond_set.subs(x, lamda) == cond_set\n", "def test_conditionset_lambda():\n    C = ConditionSet\n    I = S.Integers\n    f = Lambda(x, x < 1)\n    c = C(x, f(x), I)\n    assert c.condition == f(x)\n    assert c.base_set == I\n    assert c.free_symbols == set()\n    assert c.subs(x, y) == C(y, f(y), I)\n    assert c.contains(0) == True\n    assert c.contains(1) == False\n", "def test_CondSet_eq():\n    assert ConditionSet(x, x**2 > 4, Interval(1, 4)) == ConditionSet(x, x**2 > 4, Interval(1, 4))\n    assert ConditionSet(x, x**2 > 4, Interval(1, 4)) != ConditionSet(y, y**2 > 4, Interval(1, 4))\n    assert ConditionSet(x, x**2 > 4, Interval(1, 4)) != ConditionSet(x, x**2 > 4, Interval(1, 5))\n    assert ConditionSet(x, x**2 > 4, Interval(1, 4)) != ConditionSet(x, x**2 < 4, Interval(1, 4))\n"], "sample_1005": ["def test_latex_FreeModuleElement():\n    from sympy.polys.domains import QQ\n    Fuv, u,v = field(\"u,v\", QQ)\n    Fxyzt, x,y,z,t = field(\"x,y,z,t\", Fuv)\n\n    # Basic: scalar array\n    M = Fxyzt.convert(x)\n\n    assert latex(M) == \"x\"\n\n    M = Fxyzt.convert([1 / x, y])\n    M1 = Fxyzt.convert([1 / x, y, z])\n\n    M2 = tensorproduct(M1, M)\n    M3 = tensorproduct(M, M)\n\n    assert latex(M) == '\\\\left[\\\\begin{matrix}\\\\frac{1}{x} & y\\\\end{matrix}\\\\right]'\n    assert latex(M1) == \"\\\\left[\\\\begin{matrix}\\\\frac{1}{x} & y & z\\\\end{matrix}\\\\right]\"\n    assert latex(M2) == r\"\\left[\\begin{matrix}\" \\\n                        r\"\\left[\\begin{matrix}\\frac{1}{x^{2}} & \\frac{y}{x}\\\\\\frac{z}{x} & \\frac{w}{x}\\end{matrix}\\right] & \" \\\n                        r\"\\left[\\begin{matrix}\\frac{y}{x} & y^{2}\\\\y z & w y\\end{matrix}\\right] & \" \\\n                        r\"\\left[\\begin{matrix}\\frac{z}{x} & y z\\\\z^{2} & w z\\end{matrix}\\right]\" \\\n                        r\"\\end{matrix}\\right]\"\n    assert latex(M3) == r\"\"\"\\left[\\begin{matrix}\"\"\"\\\n            r\"\"\"\\left[\\begin{matrix}\\frac{1}{x^{2}} & \\frac{y}{x}\\\\\\frac{z}{x} & \\frac{w}{x}\\end{matrix}\\right] & \"\"\"\\\n            r\"\"\"\\left[\\begin{matrix}\\frac{y}{x} & y^{2}\\\\y z & w y\\end{matrix}\\right]\\\\\"\"\"\\\n            r\"\"\"\\left[\\begin{matrix}\\frac{z}{x} & y z\\\\z^{2} & w z\\end{matrix}\\right] & \"\"\"\\\n            r\"\"\"\\left[\\begin{matrix}\\frac{w}{x} & w y\\\\w z & w^{2}\\end{matrix}\\right]\"\"\"", "def test_latex_Determinant_printing():\n    from sympy import MatrixSymbol\n\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n\n    assert latex(A.det()) == r\"\\left|A\\right|\"\n    assert latex(A.det() + B.det()) == r\"\\left|A\\right| + \\left|B\\right|\"\n    assert latex(A.det() * B.det()) == r\"\\left|A\\right| \\left|B\\right|\"\n", "def test_issue_16695():\n    x = symbols('x')\n    l = Lambda(x, x + 1)\n    assert latex(l) == r\"\\left( x \\mapsto x + 1 \\right)\"\n", "def test_latex_Tuple():\n    from sympy import Tuple\n    assert latex(Tuple(x, y)) == r\"\\left( x, \\  y\\right)\"\n    assert latex(Tuple(x, y, z)) == r\"\\left( x, \\  y, \\  z\\right)\"\n    assert latex(Tuple(x, y, z, x)) == r\"\\left( x, \\  y, \\  z, \\  x\\right)\"\n", "def test_issue_11899():\n    expr = 2*x + 3*y + 4*z\n    assert latex(expr, fold_short_frac=True) == \"2 x + 3 y + 4 z\"\n    assert latex(expr, fold_short_frac=False) == \"2 x + 3 y + 4 z\"\n    expr = 2*x + 3*y + 4*z/5\n    assert latex(expr, fold_short_frac=True) == r\"2 x + 3 y + \\frac{4 z}{5}\"\n    assert latex(expr, fold_short_frac=False) == r\"2 x + 3 y + \\frac{4 z}{5}\"\n"], "sample_1006": ["def test_binomial_Mod():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).Mod(2) == Piecewise((0, Eq(Mod(n, 2)*Mod(k, 2), 0)), (1, True))\n    assert binomial(13, 5).Mod(2) == 1\n    assert binomial(12, 5).Mod(2) == 0\n    assert binomial(12, 4).Mod(2) == 0\n    assert binomial(x, 3).Mod(2) == binomial(x, 3).Mod(2)\n", "def test_factorial2_eval_Mod():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    a = Symbol('a', integer=True, nonnegative=True)\n    b = Symbol('b', integer=True, nonnegative=True)\n\n    assert factorial2(5, evaluate=False).subs(n, 5) == 15\n    assert factorial2(n).subs(n, 7) == 105\n    assert factorial2(k).subs(k, 8) == 384\n    assert factorial2(n).subs(n, -3) == 1/15\n    assert factorial2(k).subs(k, -2) == 1\n\n    # issue #14529\n    assert factorial2(-5, evaluate=False).subs(n, -5) == 1/15\n    assert factorial2(-6, evaluate=False).subs(n, -6) == 1/48\n\n    assert factorial2(n).subs(n, a) == factorial2(a)\n    assert factorial2(k).subs(k, b) == factorial2(b)\n\n    assert factorial2(n, evaluate=False).subs(n, oo) == oo\n    assert factorial2(n, evaluate=False).subs(n, -oo) == oo\n    assert factorial2(n, evaluate=False).subs(n, nan) == nan\n\n    assert factorial2(5, evaluate=False).subs(n, 5, simultaneous=True) == 15\n    assert factorial2(n).subs(n, 7, simultaneous=True) == 105\n    assert factorial2(k).subs(k, 8, simultaneous=True) == 384\n    assert factorial2(n).subs(n, -3, simultaneous=True) == 1/15\n    assert factorial2(k).subs(k, -2, simultaneous=True) == 1\n\n    # issue #14529\n    assert factorial2(-5, evaluate=False).subs(n, -5, simultaneous=True) == 1/15\n    assert factorial2(-6, evaluate=False).subs(n, -6, simultaneous=True) == 1/48\n\n    assert factorial2(n).subs(n, a, simultaneous=True) == factorial2(a)\n    assert factorial2(k).subs(k, b, simultaneous=True) == factorial2(b)\n\n    assert factorial2(n, evaluate=False).subs(n, oo, simultaneous=True) == oo\n    assert factorial2(n, evaluate=False).subs(n, -oo, simultaneous", "def test_factorial_Mod():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert factorial(5).Mod(3) == 0\n    assert factorial(n).Mod(k) == factorial(n) % k\n    assert factorial(5, evaluate=False).Mod(3) == 0\n    assert (factorial(n, evaluate=False) % k).subs({n: 5, k: 3}) == 0\n    assert factorial(11).Mod(7) == 0\n    assert factorial(10).Mod(7) == 0\n", "def test_factorial_mod():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert factorial(n).mod(k) == factorial(n % k) if (n % k).is_nonnegative else factorial(k + (n % k))\n    assert factorial(5).mod(4) == 0\n    assert factorial(4).mod(5) == 4\n    assert factorial(10).mod(7) == 6\n    assert factorial(-1).mod(5) == 1\n    assert factorial(-5).mod(5) == 0\n    assert factorial(n).mod(1) == 0\n    assert factorial(n).mod(0) == nan\n", "def test_binomial_Mod():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    m = Symbol('m', integer=True)\n\n    assert binomial(n, k).subs(n, m) == binomial(m, k)\n    assert binomial(n, k).subs(k, m) == binomial(n, m)\n\n    assert binomial(5, Mod(k, 3)).func == binomial\n    assert binomial(Mod(n, 5), k).func == binomial\n    assert binomial(Mod(n, 5), Mod(k, 3)).func == binomial\n\n    # Test Mod identities\n    assert Mod(binomial(5, k), 3) == Mod(binomial(Mod(5, 3), k), 3)\n    assert Mod(binomial(n, 3), 5) == Mod(binomial(n, Mod(3, 5)), 5)\n    assert Mod(binomial(n, k), 7) == Mod(binomial(Mod(n, 7), Mod(k, 7)), 7)\n\n    # Test Mod evaluation\n    assert binomial(6, Mod(3, 3)) == 20\n    assert binomial(Mod(6, 3), 3) == 20\n    assert binomial(Mod(6, 3), Mod(3, 3)) == 20\n"], "sample_1007": ["def test_factorial_Mod():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert factorial(n).Mod(k) == factorial(n) % k\n    assert factorial(n).Mod(k + 1) == factorial(n) % (k + 1)\n    assert factorial(n + 1).Mod(n + 1) == 0\n    assert factorial(n + 1).Mod(n + 2) == (n + 1) % (n + 2)\n\n    # Test Wilson's theorem for prime numbers\n    p = Symbol('p', prime=True)\n    assert factorial(p - 1).Mod(p) == -1 % p\n", "def test_factorial_mod():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True, positive=True)\n    assert factorial(n, evaluate=False).subs(n, m).mod(m) == 0\n    assert factorial(10**4, evaluate=False).mod(100) == 0\n    assert factorial(1000, evaluate=False).mod(10**3) == 0\n    assert factorial(10**6, evaluate=False).mod(1000) == 0\n    assert factorial(m, evaluate=False).mod(m) == 0\n    assert factorial(100, evaluate=False).mod(10) == 0\n    assert factorial(x, evaluate=False).mod(10).subs(x, 100) == 0\n", "def test_factorial_Mod():\n    n, k = symbols('n k', integer=True)\n    m = Symbol('m', positive=True, integer=True)\n\n    assert factorial(6, evaluate=False) % 4 == 0\n    assert factorial(5, evaluate=False) % 4 == 0\n    assert factorial(4, evaluate=False) % 4 == 0\n    assert factorial(3, evaluate=False) % 4 == 2\n    assert factorial(2, evaluate=False) % 4 == 2\n    assert factorial(1, evaluate=False) % 4 == 1\n    assert factorial(0, evaluate=False) % 4 == 1\n    assert factorial(-1, evaluate=False) % 4 == 1\n    assert factorial(-2, evaluate=False) % 4 == 1\n    assert factorial(-3, evaluate=False) % 4 == 1\n    assert factorial(n, evaluate=False) % m == factorial(n % m, evaluate=False) % m\n\n    # Wilson's theorem\n    assert factorial(m-1, evaluate=False) % m == -1 % m\n    assert factorial(m-1, evaluate=False) % m == (m-1) % m\n    assert factorial(m-1) % m == (m-1) % m\n\n    # Special case for m=6\n    assert factorial(n, evaluate=False) % 6 == Piecewise((0, (n-3 >= 0)), ((n-1)*(n-2), n-2 >= 0), ((n-1), n-1 >= 0), (1, True))\n", "def test_mod_factorial():\n    n = Symbol('n', integer=True)\n    assert Mod(factorial(n), 2) == Mod(n, 2)*factorial(n - 1)\n    assert Mod(factorial(n), n).subs(n, 5) == 0\n    assert Mod(factorial(n), n).subs(n, 7) == 0\n    assert Mod(factorial(n), n).subs(n, 11) == 0\n    assert Mod(factorial(n), n + 1).subs(n, 5) == 0\n    assert Mod(factorial(n), n + 1).subs(n, 7) == 0\n    assert Mod(factorial(n), n + 1).subs(n, 11) == 0\n    assert Mod(factorial(n), n + 2).subs(n, 5) == 0\n    assert Mod(factorial(n), n + 2).subs(n, 7) == 0\n    assert Mod(factorial(n), n + 2).subs(n, 11) == 0\n", "def test_factorial_Mod():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    p = Symbol('p', integer=True, positive=True)\n\n    assert factorial(n).Mod(p) == Mod(factorial(n), p)\n    assert factorial(5).Mod(7) == 6\n    assert factorial(6).Mod(7) == 6\n    assert factorial(8).Mod(7) == 0\n    assert factorial(12).Mod(7) == 0\n    assert factorial(5).Mod(5) == 0\n    assert factorial(7).Mod(5) == 4\n    assert factorial(11).Mod(23) == 0\n"], "sample_1008": ["def test_orientnew_with_axis_type():\n    N = ReferenceFrame('N')\n    q1 = dynamicsymbols('q1')\n    A = N.orientnew('A', 'Axis', [q1, N.x + N.y])\n    assert A.dcm(N) == Matrix([\n        [cos(q1), -sin(q1), 0],\n        [sin(q1), cos(q1), 0],\n        [0, 0, 1]\n    ])\n    assert N.dcm(A) == Matrix([\n        [cos(q1), sin(q1), 0],\n        [-sin(q1), cos(q1), 0],\n        [0, 0, 1]\n    ])\n", "def test_coordinate_symbol_equality():\n    \"\"\"Tests the equality of CoordinateSym instances.\"\"\"\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n\n    assert A[0] == A[0]\n    assert A[0] != A[1]\n    assert A[0] != B[0]\n\n    assert A.x == A[0]\n    assert A.y == A[1]\n    assert A.z == A[2]\n\n    assert A.x != B.x\n    assert A.y != B.y\n    assert A.z != B.z\n", "def test_coordinate_sym_hash():\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n    assert hash(A[0]) == hash(CoordinateSym('Ax', A, 0))\n    assert hash(A[1]) == hash(CoordinateSym('Ay', A, 1))\n    assert hash(A[2]) == hash(CoordinateSym('Az', A, 2))\n    assert hash(A[0]) != hash(B[0])\n    assert hash(A[0]) != hash(A[1])\n    assert hash(A[0]) != hash(A[2])\n", "def test_orientnew_respects_input_variables_with_indices():\n    N = ReferenceFrame('N')\n    q1 = dynamicsymbols('q1')\n    A = N.orientnew('a', 'Axis', [q1, N.z])\n\n    #build non-standard variable names and indices\n    name = 'b'\n    indices = [x+'1' for x in N.indices]\n    new_variables = ['notb_'+x+'1' for x in indices]\n    B = N.orientnew(name, 'Axis', [q1, N.z], indices=indices,\n                    variables=new_variables)\n\n    for j,var in enumerate(A.varlist):\n        assert var.name == A.name + '_' + A.indices[j]\n\n    for j,var in enumerate(B.varlist):\n        assert var.name == new_variables[j]\n", "def test_set_ang_acc():\n    q1, q2 = dynamicsymbols('q1 q2')\n    q1d, q2d = dynamicsymbols('q1 q2', 1)\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    N.set_ang_acc(A, q1d * A.z)\n    assert A.ang_acc_in(N) == q1d * A.z\n    assert N.ang_acc_in(A) == -q1d * A.z\n    assert A.ang_acc_in(B) == -q2d * B.x + q1d * A.z\n    assert B.ang_acc_in(A) == q2d * B.x - q1d * A.z\n    B.set_ang_acc(N, q2d * B.y)\n    assert B.ang_acc_in(N) == q2d * B.y\n    assert N.ang_acc_in(B) == -q2d * B.y\n    assert A.ang_acc_in(B) == -q1d * A.z + q2d * B.y\n    assert B.ang_acc_in(A) == q1d * A.z - q2d * B.y\n"], "sample_1009": ["def test_vector_magnitude_and_normalize():\n    x, y, z = symbols('x y z')\n    N = ReferenceFrame('N')\n    v = x * N.x + y * N.y + z * N.z\n    assert v.magnitude() == sqrt(x**2 + y**2 + z**2)\n    v2 = v.normalize()\n    assert v2.magnitude() == 1\n    assert v2 == v / v.magnitude()\n", "def test_vector_applyfunc():\n    x, y, z = symbols('x, y, z')\n    N = ReferenceFrame('N')\n    v = x*N.x + y*N.y + z*N.z\n\n    v_sin = v.applyfunc(sin)\n    assert (v_sin & N.x) == sin(x)\n    assert (v_sin & N.y) == sin(y)\n    assert (v_sin & N.z) == sin(z)\n\n    v_cos = v.applyfunc(cos)\n    assert (v_cos & N.x) == cos(x)\n    assert (v_cos & N.y) == cos(y)\n    assert (v_cos & N.z) == cos(z)\n\n    raises(TypeError, lambda: v.applyfunc(1))\n", "def test_vector_magnitude_normalize():\n    N = ReferenceFrame('N')\n    v1 = 3*N.x + 4*N.y\n    assert v1.magnitude() == 5\n    assert v1.normalize() == (3/5)*N.x + (4/5)*N.y\n    v2 = 1*N.x + 1*N.y + 1*N.z\n    assert v2.magnitude() == 3**0.5\n    assert v2.normalize() == (1/3**0.5)*N.x + (1/3**0.5)*N.y + (1/3**0.5)*N.z\n", "def test_vector_substitute():\n    x, y, z = symbols('x y z')\n    N = ReferenceFrame('N')\n    v1 = x * N.x + y * N.y + z * N.z\n    v2 = v1.subs({x: 2, y: 3, z: 1})\n    assert v2 == 2 * N.x + 3 * N.y + N.z\n\n    q = symbols('q')\n    A = N.orientnew('A', 'Axis', (q, N.x))\n    v3 = x * A.x + y * A.y + z * A.z\n    v4 = v3.subs({x: 2, y: 3, z: 1})\n    assert v4 == 2 * A.x + 3 * A.y + A.z\n\n    v5 = v3.subs({q: pi/2})\n    assert v5 == x * A.x + y * A.y + z * A.z\n\n    v6 = v3.subs([(x, 2), (y, 3), (z, 1), (q, pi/2)])\n    assert v6 == 2 * A.x + 3 * A.y + A.z\n", "def test_Vector_magnitude_normalize():\n    N = ReferenceFrame('N')\n    v1 = 3*N.x + 4*N.y\n    assert v1.magnitude() == 5\n    assert v1.normalize().magnitude() == 1\n    assert v1.normalize() == (3/5)*N.x + (4/5)*N.y\n"], "sample_1011": ["def test_Indexed_printing():\n    # test cases for Indexed printing\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase('A')\n\n    assert mcode(A[i, j, k]) == 'A(i, j, k)'\n    assert mcode(A[i, j, k]**2) == 'A(i, j, k).^2'\n    assert mcode(A[i, j, k]*A[i, j, k]) == 'A(i, j, k).^2'\n    assert mcode(sin(A[i, j, k])) == 'sin(A(i, j, k))'\n", "def test_Indexed_printing():\n    n = Symbol('n', integer=True)\n    i = Idx('i', n)\n    x = IndexedBase('x', shape=(n,))\n    y = IndexedBase('y', shape=(n,))\n    A = IndexedBase('A', shape=(n, n))\n\n    assert mcode(x[i]) == \"x(i + 1)\"\n    assert mcode(y[i]) == \"y(i + 1)\"\n    assert mcode(A[i, i]) == \"A(i + 1, i + 1)\"\n    assert mcode(A[i, i + 1]) == \"A(i + 1, i + 1 + 1)\"\n    assert mcode(A[i, i - 1]) == \"A(i + 1, i + 1 - 1)\"\n", "def test_Indexed_printing():\n    n, m = symbols('n m', integer=True)\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A', shape=(n, m))\n\n    assert mcode(A[1, 2]) == 'A(2, 3)'\n    assert mcode(A[i, j]) == 'A(i + 1, j + 1)'\n    assert mcode(A[i, 1]) == 'A(i + 1, 2)'\n    assert mcode(A[1, j]) == 'A(2, j + 1)'\n", "def test_Indexed_printing():\n    A = IndexedBase('A', shape=(1, 3))\n    i, j = symbols('i j', integer=True)\n    assert mcode(A[0, 0]) == \"A(1, 1)\"\n    assert mcode(A[0, j]) == \"A(1, j)\"\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(A[0, j]**2) == \"A(1, j).^2\"\n    assert mcode(A[0, j]*x) == \"x.*A(1, j)\"\n", "def test_Indexed_printing():\n    # test cases for Indexed printing\n    A = IndexedBase(\"A\", shape=(3, 3))\n    i = Idx(\"i\", 3)\n    j = Idx(\"j\", 3)\n\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(A[i, i]) == \"A(i, i)\"\n    assert mcode(A[i, 1]) == \"A(i, 2)\"\n    assert mcode(A[1, i]) == \"A(2, i)\"\n"], "sample_1012": ["def test_PythonCodePrinter_mathematical_constants():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(pi) == 'math.pi'\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n    assert prntr.doprint(zoo) == \"float('nan')\"\n\n    # Ensure that appropriate modules are imported\n    assert prntr.module_imports == {'math': {'pi'}}\n", "def test_python_code_printer_function_definition():\n    from sympy.codegen.ast import FunctionDefinition, Return\n    from sympy.core import symbols\n\n    x, y = symbols('x y')\n\n    printer = PythonCodePrinter()\n\n    expr = x + y\n    fd = FunctionDefinition(\"my_function\", (x, y), Return(expr))\n\n    expected_str = \"def my_function(x, y):\\n    return x + y\"\n    assert printer.doprint(fd) == expected_str\n", "def test_re_im_arg():\n    p = NumPyPrinter()\n    expr = x*y\n    assert p.doprint(expr.as_real_imag()[0]) == 'numpy.real(x*y)'\n    assert p.doprint(expr.as_real_imag()[1]) == 'numpy.imag(x*y)'\n    assert p.doprint(expr.as_polar()[1]) == 'numpy.angle(x*y)'\n", "def test_PythonCodePrinter_Matrices():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(SparseMatrix(3, 3, {(1, 1): 2})) == '[[0, 0, 0], [0, 2, 0], [0, 0, 0]]'\n    assert prntr.doprint(SparseMatrix(2, 2, {(0, 1): 2, (1, 1): 2})) == '[[0, 2], [0, 2]]'\n", "def test_sympy_printer():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert 'sympy' in p.module_imports\n"], "sample_1010": ["def test_issue_16483():\n    from sympy import TensorProduct\n    from sympy.tensor.array import MutableDenseNDimArray\n    arr1 = MutableDenseNDimArray([1, 2, 3], (3,))\n    arr2 = MutableDenseNDimArray([4, 5, 6], (3,))\n    assert latex(TensorProduct(arr1, arr2)) == r\" \\otimes \".join([latex(arr1), latex(arr2)])\n", "def test_Conjugate_latex_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(Adjoint(A)) == r\"A^{\\dagger}\"\n    assert latex(Adjoint(A.T)) == r\"\\left(A^{T}\\right)^{\\dagger}\"\n    assert latex(Adjoint(A.I)) == r\"\\left(A^{-1}\\right)^{\\dagger}\"\n    assert latex(Adjoint(A*2)) == r\"2^{\\dagger} A^{\\dagger}\"\n    assert latex(Adjoint(A**2)) == r\"\\left(A^{2}\\right)^{\\dagger}\"\n", "def test_issue_15738():\n    from sympy.printing.latex import latex\n    from sympy.core.function import _coeff_isneg\n    from sympy.core.numbers import E\n    from sympy.core.power import Pow\n    from sympy.core.symbol import Symbol\n\n    x = Symbol('x')\n    expr = Pow(E, x)\n\n    assert latex(expr) == \"e^{x}\"\n\n    expr = Pow(E, -x)\n    assert latex(expr) == r\"\\frac{1}{e^{x}}\"\n    assert _coeff_isneg(expr) is True\n", "def test_latex_finite_fields():\n    F = FiniteField(5)\n    assert latex(F) == r\"\\mathbb{F}_{5}\"\n", "def test_MatPow():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(A**3) == r\"A^{3}\"\n    assert latex(A**-2) == r\"A^{-2}\"\n    assert latex(A**(S(1)/2)) == r\"A^{\\frac{1}{2}}\"\n    assert latex(A**(3*S(1)/2)) == r\"A^{\\frac{3}{2}}\"\n    assert latex(A**S(3)/2) == r\"A^{3} \\cdot \\frac{1}{2}\"\n    assert latex(A**(S(3)/2)) == r\"A^{\\frac{3}{2}}\"\n    assert latex(A**-S(3)/2) == r\"- A^{- \\frac{3}{2}} \\cdot \\frac{1}{2}\"\n    assert latex(A**(S(3)/2)*A**2) == r\"A^{2} A^{\\frac{3}{2}}\"\n"], "sample_1013": ["def test_lambdify_nested_lists():\n    # Test that lambdify handles nested lists correctly\n    x, y, z = symbols('x y z')\n    f = lambdify([[x, y], z], x + y + z)\n    assert f([[1, 2], 3]) == 6\n", "def test_lambdify_with_DotProduct():\n    if not numpy:\n        skip(\"numpy not installed\")\n    v1, v2 = symbols('v1 v2')\n    a = IndexedBase('a')\n    b = IndexedBase('b')\n    i = symbols('i')\n    expr = DotProduct(a[i], b[i])\n    func = lambdify((v1, v2), expr.subs({a[i]: v1, b[i]: v2}), 'numpy')\n    assert func(numpy.array([1, 2]), numpy.array([3, 4])) == 11\n", "def test_lambdastr():\n    # Test for lambdastr\n    f1 = lambdastr(x, x**2)\n    assert f1 == 'lambda x: (x**2)'\n    f2 = lambdastr((x, y, z), [z, y, x])\n    assert f2 == 'lambda x,y,z: ([z, y, x])'\n    f3 = lambdastr(x, Piecewise((x, x < 1), (1/x, x >= 1)))\n    assert 'lambda x: (x if x < 1 else 1/x)' in f3\n    f4 = lambdastr(x, Piecewise((x, x < 1), (1/x, x >= 1)), dummify=True)\n    assert 'lambda x: (x if x < 1 else 1/x)' in f4\n    f5 = lambdastr(x, x + y)\n    assert f5 == 'lambda x: (x + y)'\n    f6 = lambdastr((x, (y, z)), x + y)\n    assert f6 == 'lambda _0,_1: (lambda x,y,z: (x + y))(_0,_1[0],_1[1])'\n", "def test_lambdify_with_tensor_expr():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    a = tensorflow.constant([1, 2, 3])\n    expr = a * x\n    f = lambdify(x, expr, 'tensorflow')\n    s = tensorflow.Session()\n    assert (s.run(f(2)) == [2, 4, 6]).all()\n", "def test_unsafe_lambdify():\n    # This test checks that a warning is raised when using lambdify with\n    # unsanitized input. The string 'os' is used as a symbol name to simulate\n    # unsanitized input. This is necessary because sympy's Symbols can't be\n    # used in a way that raises a warning with lambdify.\n\n    import warnings\n    warnings.simplefilter('always')\n    warnings.filterwarnings('error', category=UserWarning)\n\n    with raises(UserWarning) as warn:\n        lambdify('os', 'os', {'os': 1})\n        assert 'The symbols to lambdify are not safe to use.' in warn.message.args[0]\n"], "sample_1015": ["def test_ccode_struct():\n    x = symbols('x')\n    s = Type('struct { int a; float b; }')\n    var = Variable(x, type=s)\n    assert ccode(var) == 'struct {\\n int a;\\n float b;\\n } x'\n    assert ccode(Declaration(var)) == 'struct {\\n int a;\\n float b;\\n } x'\n    var2 = Variable(x, type=s, attrs={value_const})\n    decl2 = Declaration(var2)\n    assert ccode(decl2) == 'const struct {\\n int a;\\n float b;\\n } x'\n    var3 = Variable(x, type=s, value=1)\n    decl3 = Declaration(var3)\n    raises(NotImplementedError, lambda: ccode(decl3))\n", "def test_ccode_While():\n    i = symbols('i', integer=True)\n    body = [Assignment(i, i + 2)]\n    expr = While(i < 10, body)\n    assert ccode(expr) == (\n        'while (i < 10) {\\n'\n        '   i = i + 2;\\n'\n        '}'\n    )\n", "def test_ccode_PreDecrement():\n    xval = symbols('xval')\n    x = Variable(xval, type='int')\n    expr = PreDecrement(x)\n    assert ccode(expr) == '--xval'\n", "def test_ccode_TypeError():\n    raises(TypeError, lambda: ccode(x, assign_to=\"y\", contract=\"maybe\"))\n    raises(TypeError, lambda: ccode(x, human=\"yes\"))\n    raises(TypeError, lambda: ccode(x, allow_unknown_functions=\"True\"))\n", "def test_ccode_NaN():\n    assert ccode(nan, standard='c99') == 'NAN'\n    assert ccode(nan) == 'NAN'\n    assert ccode(Float('nan'), standard='c99') == 'NAN'\n    assert ccode(Float('nan')) == 'NAN'\n"], "sample_1014": ["def test_free_symbols():\n    x, y = symbols('x y')\n    md = ImmutableDenseNDimArray([[x, y], [x*y, x+y]])\n    assert md.free_symbols == {x, y}\n\n    sd = ImmutableSparseNDimArray(md)\n    assert sd.free_symbols == {x, y}\n\n    mdn = md.applyfunc(lambda x: x**2)\n    assert mdn.free_symbols == {x, y}\n\n    sdn = sd.applyfunc(lambda x: x**2)\n    assert sdn.free_symbols == {x, y}\n", "def test_mutable_immutable_conversion():\n    arr_mutable = MutableDenseNDimArray([1, 2, 3])\n    arr_immutable = arr_mutable.as_immutable()\n    assert isinstance(arr_immutable, ImmutableDenseNDimArray)\n    assert arr_mutable.tolist() == arr_immutable.tolist()\n\n    arr_immutable_2 = ImmutableDenseNDimArray([1, 2, 3])\n    arr_mutable_2 = arr_immutable_2.as_mutable()\n    assert isinstance(arr_mutable_2, MutableDenseNDimArray)\n    assert arr_immutable_2.tolist() == arr_mutable_2.tolist()\n\n    arr_mutable_3d = MutableDenseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    arr_immutable_3d = arr_mutable_3d.as_immutable()\n    assert isinstance(arr_immutable_3d, ImmutableDenseNDimArray)\n    assert arr_mutable_3d.tolist() == arr_immutable_3d.tolist()\n\n    arr_immutable_3d_2 = ImmutableDenseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    arr_mutable_3d_2 = arr_immutable_3d_2.as_mutable()\n    assert isinstance(arr_mutable_3d_2, MutableDenseNDimArray)\n    assert arr_immutable_3d_2.tolist() == arr_mutable_3d_2.tolist()\n", "def test_free_symbols():\n    from sympy.abc import x, y\n    md = ImmutableDenseNDimArray([[x, y], [x*y, x+y]])\n    assert md.free_symbols == {x, y}\n\n    sd = ImmutableSparseNDimArray(md)\n    assert sd.free_symbols == {x, y}\n\n    mdn = ImmutableDenseNDimArray([1, 2, 3])\n    assert mdn.free_symbols == set()\n\n    sdn = ImmutableSparseNDimArray([1, 2, 3])\n    assert sdn.free_symbols == set()\n", "def test_free_symbols():\n    from sympy.abc import x, y, z\n    md = ImmutableDenseNDimArray([[x, y], [x*z, x*y*z]])\n    assert md.free_symbols == {x, y, z}\n\n    sd = ImmutableSparseNDimArray(md)\n    assert sd.free_symbols == {x, y, z}\n\n    mdn = md.applyfunc(lambda x: x*3)\n    assert mdn.free_symbols == {x, y, z}\n\n    sdn = sd.applyfunc(lambda x: x/2)\n    assert sdn.free_symbols == {x, y, z}\n", "def test_free_symbols():\n    from sympy.abc import x, y, z\n    md = ImmutableDenseNDimArray([[x, y], [x*z, x*y*z]])\n    assert md.free_symbols == {x, y, z}\n\n    sd = ImmutableSparseNDimArray(md)\n    assert sd.free_symbols == {x, y, z}\n"], "sample_1017": ["def test_as_set_sqrt():\n    assert (x > 4).as_set() == Interval(4, oo, left_open=True)\n    assert (x >= 4).as_set() == Interval(4, oo)\n    assert (x < 4).as_set() == Interval(-oo, 4, left_open=True, right_open=True)\n    assert (x <= 4).as_set() == Interval(-oo, 4)\n    assert (x**2 > 4).as_set() == Interval(-oo, -2, left_open=True, right_open=True) | Interval(2, oo, left_open=True)\n    assert (x**2 >= 4).as_set() == Interval(-oo, -2, left_open=True, right_open=True) | Interval(2, oo)\n    assert (x**2 < 4).as_set() == Interval(-2, 2, left_open=True, right_open=True)\n    assert (x**2 <= 4).as_set() == Interval(-2, 2)\n", "def test_BooleanFunction_as_set():\n    assert (x > 1).as_set() == Interval(1, oo, left_open=True)\n    assert (x >= 1).as_set() == Interval(1, oo)\n    assert (x < 1).as_set() == Interval(-oo, 1, right_open=True)\n    assert (x <= 1).as_set() == Interval(-oo, 1)\n    assert And(x > 1, x < 2).as_set() == Interval(1, 2, left_open=True, right_open=True)\n    assert Or(x > 1, x < 2).as_set() == Union(Interval(-oo, 2, right_open=True), Interval(1, oo, left_open=True))\n", "def test_as_setazelelerinden():\n    # We only test the cases that are not already handled by the individual methods (e.g., Interval, etc.)\n    assert (x < 1).as_set() == Interval(-oo, 1, True, True)\n    assert (x <= 1).as_set() == Interval(-oo, 1)\n    assert (x > 1).as_set() == Interval(1, oo, True, True)\n    assert (x >= 1).as_set() == Interval(1, oo)\n    assert (x == 1).as_set() == Interval(1, 1)\n    assert (x != 1).as_set() == Interval(-oo, 1, True, True) | Interval(1, oo, True, True)\n", "def test_issue_20488():\n    raises(NotImplementedError, lambda: (x > y).as_set())\n    raises(NotImplementedError, lambda: And(x > y, x < 1).as_set())\n    raises(NotImplementedError, lambda: Or(x > y, x < 1).as_set())\n    raises(NotImplementedError, lambda: Not(x > y).as_set())\n", "def test_as_set_finite():\n    # issue 20873\n    assert (x < 1).as_set() == Interval(-oo, 1, right_open=True)\n    assert (x <= 1).as_set() == Interval(-oo, 1)\n    assert (x > 1).as_set() == Interval(1, oo, left_open=True)\n    assert (x >= 1).as_set() == Interval(1, oo)\n"], "sample_1016": ["def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert mcode(A[1:3, 1:3]) == \"A(2:3, 2:3)\"\n    assert mcode(A[1:, 1:]) == \"A(2:end, 2:end)\"\n    assert mcode(A[:, 1:]) == \"A(:, 2:end)\"\n    assert mcode(A[1:, :]) == \"A(2:end, :)\"\n    assert mcode(A[1:3, :]) == \"A(2:3, :)\"\n    assert mcode(A[:, 1:3]) == \"A(:, 2:3)\"\n    assert mcode(A[1:3, 0]) == \"A(2:3, 1)\"\n    assert mcode(A[0, 1:3]) == \"A(1, 2:3)\"\n", "def test_Indexed_printing():\n    A = IndexedBase('A')\n    i, j, k = symbols('i j k', integer=True)\n    assert mcode(A[i]) == \"A(i)\"\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(A[i, j, k]) == \"A(i, j, k)\"\n    assert mcode(3 * A[i]) == \"3*A(i)\"\n    assert mcode(A[i] + A[j]) == \"A(i) + A(j)\"\n", "def test_MatPow_printing():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert mcode(A**3) == \"A^3\"\n    assert mcode(A**-3) == \"A^(-3)\"\n    assert mcode(A**0) == \"eye(2)\"\n", "def test_octave_inline_if():\n    from sympy import Piecewise, And, Or, Not\n    x, y, z = symbols('x, y, z')\n    pw = Piecewise((x, x > 0), (y, Or(x <= 0, y < 0)), (z, True))\n    expected = (\"(x > 0).*x + (x <= 0 | y < 0).*(y) + ~(x > 0 | (x <= 0 | y < 0)).*(z)\")\n    assert mcode(pw) == expected\n", "def test_Indexed_printing():\n    # test cases for Indexed\n    A = IndexedBase(\"A\", shape=(3, 3))\n    B = IndexedBase(\"B\", shape=(3, 3))\n    C = IndexedBase(\"C\", shape=(3, 3))\n\n    i, j = symbols('i j', integer=True)\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(3 * A[i, j]) == \"3*A(i, j)\"\n\n    F = C[i, j].subs(C, A - B)\n    assert mcode(F) == \"(-B(i, j) + A(i, j))\"\n"], "sample_1018": ["def test_fcode_printing_of_assignments():\n    x, y = symbols('x y')\n    n, m = symbols('n m', integer=True)\n    A = IndexedBase('A')\n    i = Idx('i', n)\n    j = Idx('j', m)\n\n    assignment = Assignment(A[i, j], x*y)\n    assert fcode(assignment) == (\n        '      do i = 1, n\\n'\n        '         do j = 1, m\\n'\n        '            A(i, j) = x*y\\n'\n        '         end do\\n'\n        '      end do'\n    )\n\n    assignment = Assignment(A[i, j], x*y, align_right=False)\n    assert fcode(assignment) == (\n        '      do i = 1, n\\n'\n        '         do j = 1, m\\n'\n        '            A(i, j) = x*y\\n'\n        '         end do\\n'\n        '      end do'\n    )\n", "def test_fcode_GoTo():\n    x = symbols('x')\n    label = symbols('label', label=True)\n    assert fcode(GoTo(label)) == \"      go to label\"\n    assert fcode(GoTo(x)) == \"      go to x\"\n    assert fcode(GoTo([label, x])) == \"      go to (label, x), x\"\n", "def test_Piecewise_Assignment():\n    x = symbols('x')\n    p = Piecewise((x**2, x < 0), (x**3, x >= 0))\n    a = symbols('a')\n    assert fcode(Assignment(a, p)) == (\n        \"      if (x < 0) then\\n\"\n        \"         a = x**2\\n\"\n        \"      else\\n\"\n        \"         a = x**3\\n\"\n        \"      end if\"\n    )\n", "def test_fcode_Type():\n        assert fcode(expr, standard=95, source_format='free', **kwargs) == ref\n\n    check(integer, \"integer*4\")\n    check(float32, \"real*4\")\n    check(float64, \"real*8\")\n    check(real, \"real*8\")\n    check(integer, \"myint\", type_aliases={integer: 'myint'})\n    check(float32, \"myfloat\", type_aliases={float32: 'myfloat'})\n    check(float64, \"myfloat\", type_aliases={float64: 'myfloat'})\n    check(real, \"myfloat\", type_aliases={real: 'myfloat'})\n", "def test_fcode_Asignment():\n    x, y = symbols('x y')\n    n = symbols('n', integer=True)\n    A = IndexedBase('A')\n    B = IndexedBase('B')\n    i = Idx('i', n)\n    j = Idx('j', n)\n    f = Assignment(A[i, j], B[i, j])\n    assert fcode(f) == (\n        '      do i = 1, n\\n'\n        '         do j = 1, n\\n'\n        '            A(j, i) = B(j, i)\\n'\n        '         end do\\n'\n        '      end do'\n    )\n    f = Assignment(x, y)\n    assert fcode(f) == '      x = y'\n"], "sample_1020": ["def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'custom_sin'}) == \"custom_sin[x]\"\n    assert mcode(f(x), user_functions={f: 'custom_f'}) == \"custom_f[x]\"\n    assert mcode(sin(x) + f(x), user_functions={'sin': 'custom_sin', f: 'custom_f'}) == \"custom_sin[x] + custom_f[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'sin': 'MySin', 'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(f(x), user_functions={'f': 'MyF'}) == \"MyF[x]\"\n    assert mcode(sin(x), user_functions={'sin': [(lambda x: x.is_integer, 'MySin1'), \n                                                  (lambda x: True, 'MySin2')]}) == \"MySin2[x]\"\n    assert mcode(sin(1), user_functions={'sin': [(lambda x: x.is_integer, 'MySin1'), \n                                                  (lambda x: True, 'MySin2')]}) == \"MySin1[1]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'sin': 'MySin'}) == \"Cos[x]\"\n    assert mcode(sin(x), user_functions={'sin': [(lambda x: True, 'MySin')]}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'sin': [(lambda x: True, 'MySin')]}) == \"Cos[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'mySin'}) == \"mySin[x]\"\n    assert mcode(sin(x), user_functions={'cos': 'myCos'}) == \"Sin[x]\"\n    assert mcode(sin(x), user_functions={'sin': lambda x: True, 'sin': 'mySin'}) == \"Sin[x]\"\n    assert mcode(sin(x), user_functions={'sin': [(lambda x: True, 'mySin')]}) == \"mySin[x]\"\n    assert mcode(sin(x), user_functions={'sin': [(lambda x: False, 'mySin')]}) == \"Sin[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'mySin'}) == \"mySin[x]\"\n    assert mcode(sin(x), user_functions={'sin': lambda x: 'mySin'}) == \"mySin[x]\"\n    assert mcode(sin(x), user_functions={'sin': [(lambda x: True, 'mySin')]}) == \"mySin[x]\"\n    assert mcode(sin(x), user_functions={'sin': [(lambda x: False, 'mySin'), (lambda x: True, 'mySin2')]}) == \"mySin2[x]\"\n"], "sample_1021": ["def test_quaternion_equivalence():\n    q1 = Quaternion(1, 2, 3, 4)\n    M = q1.to_rotation_matrix()\n    q2 = Quaternion.from_rotation_matrix(M)\n\n    # Quaternions are equivalent up to a sign, so we compare their norms and\n    # normalized values\n    assert q1.norm() == q2.norm()\n    assert q1.normalize() == q2.normalize() or q1.normalize() == -q2.normalize()\n\n    # Test the other way around\n    q3 = Quaternion.from_axis_angle((1, 0, 0), pi/2)\n    M2 = q3.to_rotation_matrix()\n    q4 = Quaternion.from_rotation_matrix(M2)\n\n    assert q3.norm() == q4.norm()\n    assert q3.normalize() == q4.normalize() or q3.normalize() == -q4.normalize()\n", "def test_quaternion_axis_angle_from_to():\n    axis = (1, 2, 3)\n    angle = pi / 2\n    q = Quaternion.from_axis_angle(axis, angle)\n    axis_out, angle_out = q.to_axis_angle()\n    assert axis_out == tuple(i / sqrt(14) for i in axis)\n    assert trigsimp(angle_out) == angle\n", "def test_quaternion_real_field():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(1 + 2*I, 3 + 4*I, 5 + 6*I, 7 + 8*I, real_field=False)\n\n    assert q1.real_field\n    assert not q2.real_field\n\n    q3 = Quaternion(1, 2, 3, 4, real_field=False)\n    assert not q3.real_field\n\n    q4 = Quaternion(1 + 2*I, 3 + 4*I, 5 + 6*I, 7 + 8*I)\n    assert q4.a == 1\n    assert q4.b == 3\n    assert q4.c == 5\n    assert q4.d == 7\n", "def test_quaternion_to_from_rotation_matrix():\n    M = Matrix([[1, 0, 0], [0, 0, -1], [0, 1, 0]])\n    q1 = Quaternion.from_rotation_matrix(M)\n\n    # q1.to_rotation_matrix() should be equal to the original rotation matrix\n    assert q1.to_rotation_matrix().simplify() == M\n\n    # Also test that Quaternion.from_rotation_matrix preserves the quaternion's norm\n    q2 = Quaternion(1, 2, 3, 4)\n    M2 = q2.to_rotation_matrix()\n    q3 = Quaternion.from_rotation_matrix(M2)\n\n    assert q3.norm().simplify() == q2.norm().simplify()\n", "def test_quaternion_mul():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1.mul(q2) == Quaternion(-60, 12, 30, 24)\n    assert q1.mul(2) == Quaternion(2, 4, 6, 8)\n    assert q1 * q2 == Quaternion(-60, 12, 30, 24)\n    assert q1 * 2 == Quaternion(2, 4, 6, 8)\n\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field = False)\n    q4 = 2 + 3*I\n\n    assert q3.mul(q4) == Quaternion((2 + 3*I)*(3 + 4*I), (2 + 3*I)*(2 + 5*I), 0, (2 + 3*I)*(7 + 8*I))\n"], "sample_1022": ["def test_parse_expr_evaluate_False():\n    cases = {\n        '1 + x': 'Add(1, x, evaluate=False)',\n        '2*3': 'Mul(2, 3, evaluate=False)',\n        '2**3': 'Pow(2, 3, evaluate=False)',\n        'x - 2': 'Add(x, Mul(-1, 2, evaluate=False), evaluate=False)',\n        'x / 2': 'Mul(x, Pow(2, -1, evaluate=False), evaluate=False)',\n    }\n    for case, expected in cases.items():\n        assert str(parse_expr(case, evaluate=False)) == expected\n", "def test_all_implicit_steps_with_constants():\n    cases = {\n        '2x*pi': '2*x*pi',\n        'pi sin x': 'pi*sin(x)',\n        'E sin x': 'E*sin(x)',\n        'x y pi': 'x*y*pi',\n        'sin(2 * 3x * pi)': 'sin(2 * 3 * x * pi)',\n        'sin(x) (1 + cos(x) * pi)': 'sin(x) * (1 + cos(x) * pi)',\n        '(x + 2) sin(x) * pi': '(x + 2) * sin(x) * pi',\n        'sin(sin x * pi)': 'sin(sin(x) * pi)',\n        'sin pi!': 'sin(factorial(pi))',\n        'sin pi!!': 'sin(factorial2(pi))',\n        'x sin x * pi': 'x * sin(x) * pi',\n        'xy sin x * pi': 'x * y * sin(x) * pi',\n        '(x+2)(x+3) * pi': '(x + 2) * (x+3) * pi',\n        'x**2 + 2xy + y**2 * pi': 'x**2 + 2 * x * y + y**2 * pi',\n        'ln sin x * pi': 'ln(sin(x) * pi)',\n        'sin^2 x**2 * pi': 'sin(x**2)**2 * pi',\n        'sin**3(x) * pi': 'sin(x)**3 * pi',\n        'tan 3x * pi': 'tan(3*x) * pi',\n        'sin^2(3*E^(x)) * pi': 'sin(3*E**(x))**2 * pi',\n        'sin**2(E^(3x)) * pi': 'sin(E**(3*x))**2 * pi',\n        'sin^2 (3x*E^(x)) * pi': 'sin(3*x*E^x)**2 * pi',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (implicit_multiplication_application,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n", "def test_factorial_notation():\n    cases = {\n        'x!': 'factorial(x)',\n        'x!!': 'factorial2(x)',\n        '(x + 2)!': 'factorial(x + 2)',\n        'x!!!!': 'factorial2(factorial2(x))',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (factorial_notation,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n\n    # Check invalid cases\n    raises(TokenError, lambda: parse_expr('x!!!!!!', transformations=transformations2))\n    raises(TokenError, lambda: parse_expr('x!(', transformations=transformations2))\n    raises(TokenError, lambda: parse_expr('(x!)!)', transformations=transformations2))\n", "def test_evaluateFalse():\n    # Make sure evaluate=False prevents evaluation and preserves argument order\n    assert(parse_expr('2**3', evaluate=False) != parse_expr('2**3'))\n    assert(parse_expr('1 + x', evaluate=False).args == (1, sympy.Symbol('x')))\n    assert(parse_expr('x + 1', evaluate=False).args == (sympy.Symbol('x'), 1))\n    assert(parse_expr('x + 1', evaluate=False) != parse_expr('1 + x', evaluate=False))\n", "def test_implicit_multiplication_with_constants():\n    cases = {\n        '2 x': '2*x',\n        '2E': '2*E',\n        '2 pi': '2*pi',\n        'x 2': 'x*2',\n        'E 2': 'E*2',\n        'pi 2': 'pi*2',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (implicit_multiplication,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n\n    # Test that implicit multiplication is not applied when not needed\n    assert parse_expr('2*x', transformations=transformations2) == parse_expr('2*x')\n    assert parse_expr('E*2', transformations=transformations2) == parse_expr('E*2')\n    assert parse_expr('pi*2', transformations=transformations2) == parse_expr('pi*2')\n"], "sample_1019": ["def test_monotonic_sign_ratio():\n    F = _monotonic_sign\n    x = symbols('x')\n    assert F(x/2) is None\n    assert F(x/(-2)) is None\n    assert F(1/x) is None\n    assert F(1/(-x)) is None\n    assert F(-1/x) is None\n    assert F(-1/(-x)) is None\n\n    p = Dummy(positive=True)\n    assert F(1/p).is_positive\n    assert F(-1/p).is_negative\n    assert F(p/2).is_positive\n    assert F(-p/2).is_negative\n\n    n = Dummy(negative=True)\n    assert F(1/n).is_negative\n    assert F(-1/n).is_positive\n    assert F(n/2).is_negative\n    assert F(-n/2).is_positive\n\n    z = Dummy(zero=True)\n    assert F(1/z) is None\n    assert F(-1/z) is None\n    assert F(z/2).is_zero\n    assert F(-z/2).is_zero\n\n    np = Dummy(nonpositive=True)\n    assert F(np/2).is_nonpositive\n    assert F(-np/2).is_nonnegative\n\n    nn = Dummy(nonnegative=True)\n    assert F(nn/2).is_nonnegative\n    assert F(-nn/2).is_nonpositive\n", "def test__mask_nc():\n    from sympy import Symbol, sqrt, I, sin, cos\n    x, y = symbols('x y')\n    A, B = symbols('A B', commutative=False)\n    t = Symbol('t', positive=True)\n    f = Function('f')\n    # Test simple cases\n    assert _mask_nc(x, 'mask') == (x, {}, [])\n    assert _mask_nc(A, 'mask') == (_mask_nc(A, 'mask')[0], {Dummy('mask0'): A}, [])\n    assert _mask_nc(sqrt(x), 'mask') == (sqrt(x), {}, [])\n    assert _mask_nc(A + B, 'mask') == (A + B, None, [A, B])\n    assert _mask_nc(1 + I, 'mask') == (1 + I, {}, [])\n    assert _mask_nc(I*A, 'mask') == (I*Dummy('mask0'), {Dummy('mask0'): A}, [])\n    assert _mask_nc(A*B, 'mask') == (Dummy('mask0')*Dummy('mask1'), {Dummy('mask0'): A, Dummy('mask1'): B}, [])\n    assert _mask_nc(A - A, 'mask') == (0, {Dummy('mask0'): A}, [])\n    assert _mask_nc(A + A, 'mask') == (2*Dummy('mask0'), {Dummy('mask0'): A}, [])\n    # Test more complex cases\n    assert _mask_nc(1 + x*A + x*B, 'mask') == (1 + x*Dummy('mask0') + x*Dummy('mask1'), {Dummy('mask0'): A, Dummy('mask1'): B}, [])\n    assert _mask_nc(A*sqrt(x) + B*sqrt(x), 'mask') == (sqrt(x)*(Dummy('mask0') + Dummy('mask1')), {Dummy('mask0'): A, Dummy('mask1'): B}, [])\n    assert _mask_nc(A*x*t + B*y*t, 'mask') == (t*(Dummy('mask0')*x + Dummy('mask1')*y), {Dummy('mask0'): A, Dummy('mask1'): B}, [])\n    assert _mask_nc((A + B)*sin(x) + (A - B)*cos(x), 'mask') == (sin(x)*(Dummy('mask0') + Dummy('mask1')) + cos(x", "def test__mask_nc():\n    A, B, C = symbols('A:B,C', commutative=False)\n    x, y, z = symbols('x:y,z')\n\n    expr = A*B**2*C + 2*B*C\n    repl, d, syms = _mask_nc(expr)\n    assert repl == expr\n    assert d is None\n    assert syms == [A, B, C]\n\n    expr = A*(B + C)\n    repl, d, syms = _mask_nc(expr)\n    assert repl == expr\n    assert d is None\n    assert syms == [A, B, C]\n\n    expr = A*B**2 + B*C**2*A\n    repl, d, syms = _mask_nc(expr)\n    assert repl == expr\n    assert d is None\n    assert syms == [A, B, C]\n\n    expr = A*B**2 + x + y + z\n    repl, d, syms = _mask_nc(expr)\n    assert repl == expr\n    assert d is None\n    assert syms == [A, B]\n\n    expr = sin(A)\n    repl, d, syms = _mask_nc(expr, 'd')\n    assert repl == sin(d)\n    assert d == {d: A}\n    assert syms == []\n\n    expr = sin(2*A)\n    repl, d, syms = _mask_nc(expr, 'd')\n    assert repl == sin(2*A)\n    assert d is None\n    assert syms == [A]\n", "def test__mask_nc():\n    A, B, C = symbols('A,B,C', commutative=False)\n    x, y = symbols('x y')\n    eq = x*A + y*A + x*B + y*B\n    m, rep, nc = _mask_nc(eq)\n    assert nc == []\n    assert rep == { Dummy('mask0'): A, Dummy('mask1'): B }\n    assert m == x*rep[Dummy('mask0')] + y*rep[Dummy('mask0')] + x*rep[Dummy('mask1')] + y*rep[Dummy('mask1')]\n\n    eq = A*x + x*A\n    m, rep, nc = _mask_nc(eq)\n    assert nc == [A]\n    assert rep is None\n    assert m == eq\n", "def test_mask_nc():\n    A, B, C = symbols('A,B,C', commutative=False)\n    from sympy.physics.secondquant import Commutator\n    from sympy import factor\n    eq = x*Commutator(A, B) + x*Commutator(B, C)\n    masked, reps, _ = _mask_nc(eq)\n    assert reps is not None\n    assert masked.subs(reps) == eq\n    assert reps\n\n    eq = x*A + x*B\n    masked, reps, nc_symbols = _mask_nc(eq)\n    assert reps is None\n    assert masked == eq\n    assert nc_symbols == [A, B]\n"], "sample_1023": ["def test_cycle_length():\n    from sympy.ntheory.generate import cycle_length\n    func = lambda i: (i**2 + 1) % 51\n    assert next(cycle_length(func, 4)) == (6, 2)\n    assert list(cycle_length(func, 4, values=True)) == \\\n        [17, 35, 2, 5, 26, 14, 44, 50, 2, 5, 26, 14]\n    assert next(cycle_length(func, 4, nmax=5)) == (5, None)\n    assert list(cycle_length(func, 4, nmax=5, values=True)) == \\\n        [17, 35, 2, 5, 26]\n    func = lambda i: i + 1\n    assert next(cycle_length(func, 4)) == (1, None)\n    assert list(cycle_length(func, 4, values=True)) == []\n    assert next(cycle_length(func, 4, nmax=5)) == (1, None)\n    assert list(cycle_length(func, 4, nmax=5, values=True)) == []\n", "def test_sieve_repr():\n    s = Sieve()\n    assert repr(s).startswith(\"<prime sieve\")\n    assert \"totient\" in repr(s)\n    assert \"mobius\" in repr(s)\n", "compilation error", "def test_prime_default_sieve():\n    sieve = Sieve()\n    for i in range(2, 100):\n        if isprime(i):\n            assert sieve[i-1] == i\n    sieve._list = _array('l', [2, 3])\n    assert sieve[2] == 5\n\n", "def test_sieve_representation():\n    assert repr(sieve).startswith(\"<prime sieve\")\n    s = Sieve()\n    assert repr(s) == repr(sieve)\n"], "sample_1025": ["def test_issue_PythonCodePrinter_FullQualified():\n    prntr = PythonCodePrinter({'fully_qualified_modules': False})\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.doprint(acos(x)) == 'acos(x)'\n", "def test_module_format():\n    p = PythonCodePrinter()\n    assert p._module_format('math.sin') == 'math.sin'\n    p._settings['fully_qualified_modules'] = False\n    assert p._module_format('math.sin') == 'sin'\n    assert p._module_format('math.sin', register=False) == 'sin'\n", "def test_PythonCodePrinter_pieces():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-zoo) == \"float('-inf')\"\n    assert prntr.doprint(y > x) == \"(y > x)\"\n    assert prntr.doprint(y >= x) == \"(y >= x)\"\n", "def test_PythonCodePrinter_Pow():\n    p = PythonCodePrinter()\n    assert p.doprint(x**0.5) == 'math.sqrt(x)'\n    assert p.doprint(x**-1) == '1/x'\n    assert p.doprint(x**0) == '1'\n    assert p.doprint(x**2) == 'x**2'\n", "def test_PythonCodePrinter_fully_qualified_modules():\n    prntr = PythonCodePrinter({'fully_qualified_modules': False})\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.module_imports == {'math': {'pi'}}\n    assert prntr.doprint(acos(x)) == 'acos(x)'\n    assert prntr.module_imports == {'math': {'pi', 'acos'}}\n    prntr = PythonCodePrinter({'fully_qualified_modules': True})\n    assert prntr.doprint(pi) == 'math.pi'\n    assert prntr.module_imports == {'math': {'pi'}}\n    assert prntr.doprint(acos(x)) == 'math.acos(x)'\n    assert prntr.module_imports == {'math': {'pi', 'acos'}}\n"], "sample_1024": ["def test_issue_20721():\n    assert (S(-2)**Float(0.5)).is_real is False\n    assert (S(-3)**Float(0.5)).is_real is False\n    x = Symbol('x', real=True)\n    assert (x**Float(0.5)).is_real is None\n", "def test_mod_inverse_symbolic():\n    from sympy import symbols\n    x, y = symbols('x y')\n    raises(TypeError, lambda: mod_inverse(x, y))\n", "def test_Float_coercion():\n    assert Float(1.2, 3) == Float('1.2', 3)\n    assert Float(1.2, 3) == Float('1.2', 3)\n    assert Float(1.2) == Float('1.2')\n    assert Float(1.2, 3) == Float(Float('1.2', 3))\n    assert Float('1.2', 3) == Float(Float(1.2, 3))\n", "def test_comp_decimal():\n    assert comp(Decimal('1.23456789'), '1.23456789', '')\n    assert comp(Decimal('1.23456789'), '1.23456789', 'f')\n    assert comp(Decimal('1.23456789'), 1.23456789, '')\n    assert comp(Decimal('1.23456789'), 1.23456789, 'f')\n    assert not comp(Decimal('1.23456789'), '1.23456780', '')\n    assert not comp(Decimal('1.23456789'), '1.23456780', 'f')\n    assert not comp(Decimal('1.23456789'), 1.23456780, '')\n    assert not comp(Decimal('1.23456789'), 1.23456780, 'f')\n", "def test_mpfloat_to_Rational():\n    from mpmath import mpf\n    assert Rational(mpf(0.5)).limit_denominator() == Rational(1, 2)\n    assert Rational(mpf(3.14)).limit_denominator() == Rational(157, 50)\n"], "sample_1026": ["def test_lambdastr():\n    expr = x**2 + 3*y\n    expected = 'lambda x,y: (x**2 + 3*y)'\n    assert lambdastr((x, y), expr) == expected\n    assert lambdastr((x, y), expr, printer=None) == expected\n    assert lambdastr((x, y), expr, dummify=False) == expected\n    assert lambdastr((x, y), expr, dummify=True) == expected\n", "def test_lambdastr_derivative():\n    f = Function('f')(x)\n    fx = f.diff()\n    assert lambdastr((f, fx), f + fx) == 'lambda _0,_1: (_0 + _1)'\n    assert lambdastr((f, fx), f/fx) == 'lambda _0,_1: (_0/_1)'\n    assert lambdastr((f, fx), f/fx, dummify=False) == 'lambda f,diff(f(x), x): (f/diff(f(x), x))'\n    assert lambdastr((fx, f), f/fx, dummify=True) == 'lambda _0,_1: (_1/_0)'\n    assert lambdastr(fx, 1 + fx) == 'lambda _0: (1 + _0)'\n    assert lambdastr(fx, 1 + fx, dummify=True) == 'lambda _0: (1 + _0)'\n", "def test_lambdastr_nested():\n    f = lambdastr((x, (y, z)), x + y)\n    assert f == 'lambda _0,_1: (lambda x,y,z: (x + y))(_0,_1[0],_1[1])'\n    f = lambdastr(((x, y), z), x + y)\n    assert f == 'lambda _0,_1: (lambda x,y,z: (x + y))(_0[0],_0[1],_1)'\n    f = lambdastr((x, y, z), x + y)\n    assert f == 'lambda x,y,z: (x + y)'\n", "def test_issue_18468():\n    f = lambdify(x, Eq(x, 1))\n    assert f(1) == True\n    assert f(2) == False\n", "def test_lambdastr_with_Dummy():\n    d = Dummy('d')\n    f = lambdastr(d, d + 1)\n    assert f == 'lambda d: (d + 1)'\n    f = lambdastr((d, x), d + x)\n    assert f == 'lambda d,x: (d + x)'\n"], "sample_1029": ["def test_Cycle():\n    from sympy.combinatorics import Cycle\n    sT(Cycle((1, 2, 3)), \"Cycle((1, 2, 3))\")\n", "def test_MonogenicFiniteExtension():\n    A = FiniteExtension(Poly(x**2 + 1, x))\n    assert srepr(A) == \"FiniteExtension(Poly(x**2 + 1, x, domain='ZZ'))\"\n", "def test_AlgebraicField():\n    from sympy.polys.fields import AlgebraicField\n    from sympy.polys.domains import QQ\n    f = Poly(x**2 + 1, x, QQ)\n    assert srepr(AlgebraicField(QQ, f)) == \\\n        \"AlgebraicField(RationalField, Poly(x**2 + 1, x, domain='QQ'))\"\n", "def test_Cycle():\n    from sympy.combinatorics import Cycle\n    sT(Cycle((1, 2, 3)), \"Cycle((1, 2, 3))\")\n", "def test_MatrixBase_SpecialCases():\n    m = Matrix(3, 0, [])\n    sT(m, \"MutableDenseMatrix(3, 0, [])\")\n    m = Matrix(0, 3, [])\n    sT(m, \"MutableDenseMatrix(0, 3, [])\")\n    m = Matrix(0, 0, [])\n    sT(m, \"MutableDenseMatrix([])\")\n"], "sample_1028": ["def test_issue_19063():\n    r, phi = symbols('r phi', real=True)\n    assert sqrt(r**2)*exp(I*phi) == r*exp(I*phi)\n    assert sqrt(r**2)*I*exp(I*phi) == I*r*exp(I*phi)\n    assert I*sqrt(r**2)*exp(I*phi) == I*r*exp(I*phi)\n    assert sqrt(r**2)*exp(-I*phi) == r*exp(-I*phi)\n    assert sqrt(r**2)*I*exp(-I*phi) == I*r*exp(-I*phi)\n    assert I*sqrt(r**2)*exp(-I*phi) == I*r*exp(-I*phi)\n", "def test_Mul_divmod():\n    assert divmod(x*y, y) == (x, 0)\n    assert divmod(x*y, x) == (y, 0)\n    assert divmod(2*x, x) == (2, 0)\n    assert divmod(x, 1) == (x, 0)\n    assert divmod(x, -1) == (-x, 0)\n    assert divmod(-x, 1) == (-x, 0)\n    assert divmod(-x, -1) == (x, 0)\n", "def test_issue_15971():\n    assert Mod(Rational(1, 3), 3) == 1\n    assert Mod(-Rational(1, 3), 3) == 2\n    assert Mod(Rational(2, 3), 3) == 2\n    assert Mod(-Rational(2, 3), 3) == 1\n", "def test_issue_17246():\n    assert Mod(Mod(x, 3), 2) == Mod(x, 2)\n    assert Mod(Mod(x, 4), 2) == Mod(x, 2)\n", "def test_Mul_is_infinite():\n    x = Symbol('x')\n    assert (x * oo).is_infinite\n    assert (x * oo).is_finite is False\n    assert (x * -oo).is_infinite\n    assert (x * -oo).is_finite is False\n    assert (x * zoo).is_infinite\n    assert (x * zoo).is_finite is False\n    assert (x * nan).is_infinite is None\n    assert (x * nan).is_finite is None\n"], "sample_1027": ["def test_issue_16438():\n    assert Poly(x**2 + 1, modulus=2).is_irreducible is True\n    assert Poly(x**2 + 1, modulus=3).is_irreducible is True\n    assert Poly(x**2 + 1, modulus=5).is_irreducible is True\n    assert Poly(x**2 + 1, modulus=7).is_irreducible is True\n\n    assert Poly(x**2 + 9, modulus=3).is_irreducible is False\n\n    assert Poly(x**4 + 1, modulus=2).is_irreducible is True\n    assert Poly(x**4 + 1, modulus=3).is_irreducible is True\n    assert Poly(x**4 + 1, modulus=5).is_irreducible is True\n    assert Poly(x**4 + 1, modulus=7).is_irreducible is True\n    assert Poly(x**4 + 1, modulus=11).is_irreducible is True\n\n    assert Poly(x**4 + 81, modulus=3).is_irreducible is False\n\n    assert Poly((x**5 + 1)/(x + 1), modulus=2).is_irreducible is True\n    assert Poly((x**5 + 1)/(x + 1), modulus=3).is_irreducible is True\n    assert Poly((x**5 + 1)/(x + 1), modulus=5).is_irreducible is True\n    assert Poly((x**5 + 1)/(x + 1), modulus=7).is_irreducible is True\n    assert Poly((x**5 + 1)/(x + 1), modulus=11).is_irreducible is True\n\n    assert Poly(x**2 + 1, modulus=2).as_dict() == {(2,): 1, (0,): 1}\n    assert Poly(x**2 + 1, modulus=3).as_dict() == {(2,): 1, (0,): 1}\n    assert Poly(x**2 + 1, modulus=5).as_dict() == {(2,): 1, (0,): 1}\n    assert Poly(x**2 + 1, modulus=7).as_dict() == {(2,): 1, (0,): 1}\n\n    assert Poly(x**2 + ", "def test_issue_19525():\n    g = x**2*y**2 + x**2*y + x*y**2 + x*y + x + y**2 + y + 1\n    assert Poly(g).terms_gcd() == x*y + 1\n    assert terms_gcd(g) == x*y + 1\n\n    g = x**3*y**2 + x**3*y + x**2*y**2 + x**2*y + x*y**2 + x*y + x + y + 1\n    assert Poly(g).terms_gcd() == x*y + 1\n    assert terms_gcd(g) == x*y + 1\n\n    g = x**5*y**4 + x**5*y**3 + x**4*y**4 + x**4*y**3 + x**3*y**4 + x**3*y**3 + x**2*y**4 + x**2*y**3 + x*y**4 + x*y**3 + x*y**2 + x*y + x + y + 1\n    assert Poly(g).terms_gcd() == x*y + 1\n    assert terms_gcd(g) == x*y + 1\n\n    g = x**7*y**6 + x**7*y**5 + x**6*y**6 + x**6*y**5 + x**5*y**6 + x**5*y**5 + x**4*y**6 + x**4*y**5 + x**3*y**6 + x**3*y**5 + x**2*y**6 + x**2*y**5 + x*y**6 + x*y**5 + x*y**4 + x*y**3 + x*y**2 + x*y + x + y + 1\n    assert Poly(g).terms_gcd() == x*y + 1\n    assert terms_gcd(g) == x*y + 1\n", "def test_issue_15355():\n    assert factor(8*x**5*y**5*x**3*y**3*x**2*y**2) == \\\n        8*x**10*y**10\n", "def test_Poly_spaces():\n    assert str(Poly(x**2 + x + 1)) == 'x**2 + x + 1'\n    assert str(Poly(x**2 + x + 1, x)) == 'x**2 + x + 1'\n\n    assert str(Poly(x**2 + x + 1, modulus=3)) == 'x**2 + x + 1'\n    assert str(Poly(x**2 + x + 1, modulus=3, domain='QQ')) == 'x**2 + x + 1'\n\n    assert str(Poly(x**2 + x + 1, domain='ZZ')) == 'x**2 + x + 1'\n    assert str(Poly(x**2 + x + 1, domain='QQ')) == 'x**2 + x + 1'\n\n    assert str(Poly(x**2 + x + 1, x, modulus=3)) == 'x**2 + x + 1'\n    assert str(Poly(x**2 + x + 1, x, modulus=3, domain='QQ')) == 'x**2 + x + 1'\n\n    assert str(Poly(x**2 + x + 1, x, domain='ZZ')) == 'x**2 + x + 1'\n    assert str(Poly(x**2 + x + 1, x, domain='QQ')) == 'x**2 + x + 1'\n\n    assert str(Poly(x**2 + 1, x, domain=FF(2))) == 'x**2 + 1'\n\n    assert str(Poly(x**2 + 1, x, domain=ZZ[x])) == 'x**2 + 1'\n\n    assert str(Poly(x**2 + 1, x, y, domain='QQ[y]')) == 'x**2 + 1'\n    assert str(Poly(x**2 + 1, x, y, z, domain='QQ[y,z]')) == 'x**2 + 1'\n\n    assert str(Poly(x**2 + 1, x, domain=ZZ[y])) == 'x**2 + 1'\n    assert str(Poly(x**2 + 1, x, domain=ZZ[y, z])) == 'x**2 + 1'\n\n    assert str(Poly(x**2 + 1, x, y, domain=ZZ[y])) == '", "def test_issue_14541():\n    assert factor_list(2**(2*x) + 2**(x + 3)) == (8, [(2**x, 1), (2**x + 8, 1)])\n    assert factor_list(2**(2*x) + 2**(x + 3), gaussian=True) == (8, [(2**x, 1), (2**x + 8, 1)])\n    assert factor_list(2**(2*x) + 2**(x + 3), extension=None) == (8, [(2**x, 1), (2**x + 8, 1)])\n    assert factor_list(2**(2*x) + 2**(x + 3), domain='ZZ') == (8, [(2**x, 1), (2**x + 8, 1)])\n    assert factor_list(2**(2*x) + 2**(x + 3), domain='QQ') == (8, [(2**x, 1), (2**x + 8, 1)])\n    assert factor_list(2**(2*x) + 2**(x + 3), domain='RR') == (8, [(2**x, 1), (2**x + 8, 1)])\n    assert factor_list(2**(2*x) + 2**(x + 3), modulus=3) == (2, [(2**x + 1, 2)])\n"], "sample_1031": ["def test_get_dimensional_expr():\n    Quantity.SI_quantity_scale_factors = {}\n    Quantity.SI_quantity_dimension_map = {}\n    dim = Quantity.get_dimensional_expr(1*meter/second)\n    assert dim == Dimension(length/time)\n\n    Quantity.SI_quantity_scale_factors = {}\n    Quantity.SI_quantity_dimension_map = {}\n    dim = Quantity.get_dimensional_expr(1*meter**2/second**2)\n    assert dim == Dimension(length**2/time**2)\n", "def test_get_unit_system():\n    A = Quantity(\"A\")\n    A.set_dimension(current)\n    A.set_scale_factor(S.One)\n\n    Js = Quantity(\"Js\")\n    Js.set_dimension(action)\n    Js.set_scale_factor(S.One)\n\n    mksa = UnitSystem((m, kg, s, A), (Js,))\n    assert UnitSystem.get_unit_system(\"MS\") is None  # No system with this name exists yet\n    ms = UnitSystem((m, s), (c,), name=\"MS\")\n    assert UnitSystem.get_unit_system(\"MS\") is ms\n", "def test_get_unit():\n    ms = UnitSystem((m, s), (c,))\n    assert ms.get_unit(length) == m\n    assert ms.get_unit(velocity) == c\n    raises(KeyError, lambda: ms.get_unit(mass))\n", "def test_quantity_scale_factors_and_dimensions_match():\n    # Test that scale factors and dimensions match for all quantities\n    for _scale_factor, _dimension in zip(\n            Quantity.SI_quantity_scale_factors.values(),\n            Quantity.SI_quantity_dimension_map.values()):\n        dimex = Quantity.get_dimensional_expr(_scale_factor)\n        if dimex != 1:\n            dim = Dimension(dimex)\n            if not dimsys_default.equivalent_dims(_dimension, dim):\n                assert False, f\"quantity value and dimension mismatch for scale factor {_scale_factor}\"\n", "def test_get_dimensional_expr():\n    assert Quantity.get_dimensional_expr(c.scale_factor) == 1\n    assert Quantity.get_dimensional_expr(c.scale_factor * m.scale_factor / s.scale_factor) == m.dimension / s.dimension\n"], "sample_1030": ["def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    from sympy.geometry.util import are_coplanar\n\n    # points\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2))\n    assert not are_coplanar(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 3, 2))\n\n    # lines\n    assert are_coplanar(Line3D((0, 0, 0), (1, 1, 1)), Line3D((1, 1, 1), (2, 2, 2)))\n    assert not are_coplanar(Line3D((0, 0, 0), (1, 1, 1)), Line3D((1, 1, 1), (2, 3, 2)))\n\n    # plane and points\n    p = Plane((0, 0, 0), (1, 1, 1), (2, 2, 2))\n    assert are_coplanar(p, Point3D(3, 3, 3))\n    assert not are_coplanar(p, Point3D(1, 1, 2))\n\n    # plane and lines\n    assert are_coplanar(p, Line3D((0, 0, 0), (1, 1, 1)))\n    assert not are_coplanar(p, Line3D((0, 0, 0), (1, 1, 2)))\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert are_coplanar(a, b, c) == False\n    assert are_coplanar(a, b) == False\n    assert are_coplanar(a, a) == True\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2)) == True\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 3)) == False\n    assert are_coplanar(Plane((0, 0, 0), (1, 1, 1), (2, 2, 2)), Point3D(3, 3, 3)) == True\n    assert are_coplanar(Plane((0, 0, 0), (1, 1, 1), (2, 2, 2)), Point3D(3, 3, 4)) == False\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    d = Plane(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(1, 1, 0))\n    assert not are_coplanar(a, b, c)\n    assert are_coplanar(a, d)\n    assert are_coplanar(b, d)\n    assert are_coplanar(c, d)\n    assert are_coplanar(a, a)\n    assert are_coplanar(d, d)\n    raises(TypeError, lambda: are_coplanar(a, 3))\n", "def test_are_coplanar():\n    a = Point3D(5, 0, 0)\n    b = Point3D(1, -1, 1)\n    c = Point3D(0, -2, 0)\n    d = Point3D(3, 1, 1)\n    e = Point3D(0, -1, 0)\n    f = Point3D(5, -1, 9)\n    assert are_coplanar(a, b, c, d) is False\n    assert are_coplanar(a, b, e, f) is False\n    assert are_coplanar(a, b, c, Line3D(c, d)) is False\n    assert are_coplanar(a, b, Line3D(c, d), e) is False\n    assert are_coplanar(a, Line3D(b, c), Line3D(d, e)) is False\n    assert are_coplanar(a, b, Plane(c, d, e)) is True\n    assert are_coplanar(a, Line3D(b, c), Plane(d, e, f)) is True\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    a = Point3D(1, 2, 3)\n    b = Point3D(4, 5, 6)\n    c = Point3D(7, 8, 9)\n    d = Point3D(10, 11, 12)\n\n    assert are_coplanar(a, b, c) is False\n    assert are_coplanar(a, b, c, d) is False\n\n    p = Plane(a, b, c)\n    assert are_coplanar(p, a, b, c) is True\n    assert are_coplanar(a, b, c, p) is True\n    assert are_coplanar(p, a, b, c, d) is False\n\n    l = Line3D(a, b)\n    assert are_coplanar(l, a, b, c) is False\n    assert are_coplanar(l, a, b, c, d) is False\n\n    assert are_coplanar(a, b) is False\n    assert are_coplanar(a, a) is False\n"], "sample_1032": ["def test_real_root_issue():\n    from sympy.abc import x\n    assert real_root(root(-3, 3), 3) == real_root(-3, 3)\n    assert real_root(root(-2, 4), 4) == real_root(-2, 4)\n    assert real_root(root(-x, 5), 5).subs(x, 32) == real_root(-32, 5)\n    assert real_root(root(-x, 6), 6).subs(x, 64) == real_root(-64, 6)\n    assert real_root(root(-x, 7), 7).subs(x, 128) == real_root(-128, 7)\n", "def test_issue_19664():\n    from sympy.abc import x, y\n    assert Min(x, y).nargs == 2\n    assert Max(x, y).nargs == 2\n    assert Min(x, y, 2).nargs == 3\n    assert Max(x, y, 2).nargs == 3\n    assert Min(x, x).nargs == 1\n    assert Max(x, x).nargs == 1\n    assert Min(x, y, x).nargs == 2\n    assert Max(x, y, x).nargs == 2\n", "def test_incompatible_args():\n    from sympy.abc import x\n    from sympy import I, pi, E\n\n    # Min/Max do not support non-comparable arguments\n    non_comparable_args = [I, pi, E, x + I, x**2 + 2*x + 1]\n    for arg in non_comparable_args:\n        raises(ValueError, lambda: Min(arg, x))\n        raises(ValueError, lambda: Max(arg, x))\n\n    # Min/Max do not support complex infinity\n    complex_infinity = S.ComplexInfinity\n    raises(ValueError, lambda: Min(complex_infinity, x))\n    raises(ValueError, lambda: Max(complex_infinity, x))\n", "def test_issue_14133():\n    from sympy.abc import x, y\n    assert Min(x, y).diff(x) == Heaviside(y - x)\n    assert Min(x, y).diff(y) == Heaviside(x - y)\n    assert Max(x, y).diff(x) == Heaviside(x - y)\n    assert Max(x, y).diff(y) == Heaviside(y - x)\n", "def test_incompatible_args():\n    from sympy.abc import x\n    from sympy import I\n    assert Min(x, I).func is Min  # remains unevaluated\n    assert Max(x, I).func is Max  # remains unevaluated\n    assert Min(x, I).is_real is None\n    assert Max(x, I).is_real is None\n    assert Min(x, 1).subs(x, I).func is Min  # remains unevaluated\n    assert Max(x, 1).subs(x, I).func is Max  # remains unevaluated\n    assert Min(x, 1).subs(x, I).is_real is None\n    assert Max(x, 1).subs(x, I).is_real is None\n    raises(ValueError, lambda: Min(x, I).subs(x, I))\n    raises(ValueError, lambda: Max(x, I).subs(x, I))\n"], "sample_1033": ["def test_issue_16997():\n    a = Symbol('a', positive=True)\n    b = Symbol('b', positive=True)\n    assert (a**2 + 2*a*b + b**2).is_positive is True\n    assert ((a + b)**4).is_positive is True\n", "def test_issue_15980():\n    assert (zoo + 1).is_positive\n    assert (zoo + 1).is_negative is False\n    assert (zoo - 1).is_positive\n    assert (zoo - 1).is_negative is False\n    assert (-zoo + 1).is_positive is False\n    assert (-zoo + 1).is_negative\n    assert (-zoo - 1).is_positive is False\n    assert (-zoo - 1).is_negative\n    assert (oo + 1).is_positive\n    assert (oo + 1).is_negative is False\n    assert (oo - 1).is_positive\n    assert (oo - 1).is_negative is False\n    assert (-oo + 1).is_positive is False\n    assert (-oo + 1).is_negative\n    assert (-oo - 1).is_positive is False\n    assert (-oo - 1).is_negative\n", "def test_issue_15923():\n    a = Symbol('a', real=True)\n    b = Symbol('b', real=True)\n    assert (a*b).is_positive == None\n    assert (a*b).is_negative == None\n\n    c = Symbol('c', positive=True)\n    assert (a*c).is_positive == None\n    assert (a*c).is_negative == None\n    assert (c*a).is_positive == None\n    assert (c*a).is_negative == None\n\n    d = Symbol('d', negative=True)\n    assert (a*d).is_positive == None\n    assert (a*d).is_negative == None\n    assert (d*a).is_positive == None\n    assert (d*a).is_negative == None\n", "def test_issue_17370():\n    from sympy import symbols, oo, limit\n    x = symbols('x', extended_real=True)\n    assert limit(1/x, x, oo) == 0\n    assert limit(1/x, x, -oo) == 0\n", "def test_Add_is_commutative():\n    a, b = symbols('a b', commutative=False)\n    x, y = symbols('x y')\n    assert Add(x, y).is_commutative\n    assert Add(a, b).is_commutative is None\n    assert Add(x, a).is_commutative is None\n"], "sample_1034": ["def test_WGate_represent():\n    nqubits = 2\n    wgate = WGate(nqubits)\n    representation = represent(wgate, nqubits=nqubits)\n    assert representation.is_Unitary\n", "def test_apply_grover_iterations():\n    nqubits = 4\n    basis_states = superposition_basis(nqubits)\n    v = OracleGate(nqubits, return_one_on_two)\n    iterations = floor(sqrt(2**nqubits)*(pi/4))\n    iterated = apply_grover(return_one_on_two, nqubits, iterations=iterations)\n    manual_iterations = basis_states\n    for _ in range(iterations):\n        manual_iterations = grover_iteration(manual_iterations, v)\n        manual_iterations = qapply(manual_iterations)\n    assert iterated == manual_iterations\n", "def test_OracleGate_eval_args():\n    # Test that an error is raised when the number of arguments is incorrect\n    try:\n        OracleGate._eval_args((1,))\n        assert False\n    except QuantumError:\n        assert True\n\n    try:\n        OracleGate._eval_args((1, 2, 3))\n        assert False\n    except QuantumError:\n        assert True\n\n    # Test that an error is raised when the first argument is not an integer\n    try:\n        OracleGate._eval_args((1.5, lambda qubits: qubits == IntQubit(0)))\n        assert False\n    except TypeError:\n        assert True\n\n    # Test that an error is raised when the second argument is not a callable\n    try:\n        OracleGate._eval_args((1, 2))\n        assert False\n    except TypeError:\n        assert True\n", "def test_apply_grover_iterations():\n    nqubits = 4\n    iterations = 2\n    basis_states = superposition_basis(nqubits)\n    v = OracleGate(nqubits, return_one_on_two)\n    expected = grover_iteration(grover_iteration(basis_states, v), v)\n    assert apply_grover(return_one_on_two, nqubits, iterations=iterations) == qapply(expected)\n", "def test_grover_iterations():\n    nqubits = 2\n    iterations = 1\n    v = OracleGate(nqubits, return_one_on_one)\n    basis_states = superposition_basis(nqubits)\n    expected = IntQubit(1, nqubits=nqubits)\n    assert qapply(apply_grover(return_one_on_one, nqubits, iterations)) == expected\n\n    nqubits = 4\n    iterations = 3\n    v = OracleGate(nqubits, return_one_on_two)\n    basis_states = superposition_basis(nqubits)\n    expected = (-13*basis_states)/64 + 264*IntQubit(2, nqubits)/256\n    assert qapply(apply_grover(return_one_on_two, nqubits, iterations)) == expected\n"], "sample_1035": ["def test_measure_all():\n    from sympy.physics.quantum.qubit import IntQubit, measure_all\n\n    # Create a superposition state\n    state = (IntQubit(0) + IntQubit(1))/sqrt(2)\n\n    # Measure the state\n    result = measure_all(state)\n\n    # Check if the result is correct\n    assert len(result) == 2\n    assert result[0][0] == IntQubit(0)\n    assert result[1][0] == IntQubit(1)\n    assert result[0][1] == 0.5\n    assert result[1][1] == 0.5\n", "def test_measure_all():\n    nqubits = 2\n    basis_states = superposition_basis(nqubits)\n    measurement_result = measure_all(basis_states)\n    expected = [(IntQubit(0, nqubits=nqubits), 1/4), \n                (IntQubit(1, nqubits=nqubits), 1/4), \n                (IntQubit(2, nqubits=nqubits), 1/4), \n                (IntQubit(3, nqubits=nqubits), 1/4)]\n    assert measurement_result == expected\n", "def test_apply_grover_with_multiple_iterations():\n    nqubits = 4\n    v = OracleGate(nqubits, return_one_on_two)\n    basis_states = superposition_basis(nqubits)\n    expected = (-13*basis_states)/64 + 264*IntQubit(2, nqubits)/256\n\n    state = basis_states\n    for _ in range(3):\n        state = qapply(grover_iteration(state, v))\n\n    assert state == qapply(expected)\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import measure_all\n    q = IntQubit(0)/sqrt(2) + IntQubit(1)/sqrt(2)\n    result = measure_all(q)\n    assert len(result) == 2\n    assert result[0][0] == IntQubit(0)\n    assert result[1][0] == IntQubit(1)\n    assert result[0][1] == 1/2\n    assert result[1][1] == 1/2\n", "def test_measure_all():\n    nqubits = 2\n    basis_states = superposition_basis(nqubits)\n    measurement = measure_all(basis_states)\n    expected_measurement = [(IntQubit(0, nqubits=nqubits), 1/4), \n                            (IntQubit(1, nqubits=nqubits), 1/4), \n                            (IntQubit(2, nqubits=nqubits), 1/4), \n                            (IntQubit(3, nqubits=nqubits), 1/4)]\n    assert measurement == expected_measurement\n"], "sample_1036": ["def test_matmul_as_coeff_Mul():\n    expr = MatMul(2, A, 2)\n    coeff, terms = expr.as_coeff_Mul()\n    assert coeff == 4\n    assert terms == A\n", "def test_Mul_is_commutative():\n    assert Mul(2, 3).is_commutative\n    assert not Mul(A, B).is_commutative\n    assert Mul(A, A.T).is_commutative\n    assert Mul(A, 2).is_commutative\n    assert Mul(2, A).is_commutative\n", "def test_matmul_commutative_args():\n    assert MatMul(C, 2, D).args == (C, 2, D)\n    assert MatMul(2, C, D).args == (2, C, D)\n    assert MatMul(C, D, 2).args == (C, D, 2)\n    assert MatMul(2, C, 2, D).args == (2*2, C, D)\n", "def test_Mul_eval():\n    # issue 7310\n    A, B, C = symbols('A B C', commutative=False)\n    assert Mul(A, Mul(B, C), evaluate=False).args == (A, B, C)\n    assert Mul(A, Mul(B, C)).args == (A, B, C)\n    assert Mul(Mul(A, B), C, evaluate=False).args == (A, B, C)\n    assert Mul(Mul(A, B), C).args == (A, B, C)\n", "def test_Mul_AssocOp():\n    # Test Mul as an AssocOp\n    assert Mul(A, B, C).args == (A, B, C)\n    assert Mul(A, B, C).as_two_terms() == (A, B*C)\n    assert Mul(A, B, C).as_coeff_Mul() == (1, A*B*C)\n"], "sample_1037": ["def test_MatMul_doit():\n    M = Matrix([[1, 2], [3, 4]])\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert MatMul(M, A).doit() == MatMul(M, A)\n    assert MatMul(A, M).doit() == MatMul(A, M)\n    assert MatMul(M, M).doit() == M**2\n    assert MatMul(M, M, A).doit() == MatMul(M**2, A)\n    assert MatMul(A, M, M).doit() == MatMul(A, M**2)\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    M = Matrix([[1, 2], [3, 4]])\n    assert MatMul(A, B).doit() == A*B\n    assert MatMul(A, M).doit() == A*M\n    assert MatMul(M, A).doit() == M*A\n    assert MatMul(A, M, B).doit() == A*M*B\n    assert MatMul(M, A, M).doit() == M*A*M\n    assert MatMul(A, A, A).doit() == A**3\n    assert MatMul(A, A, A, A).doit() == A**4\n", "def test_MatMul_entry():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert (A*B)[0, 0] == A[0, 0]*B[0, 0] + A[0, 1]*B[1, 0]\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    assert MatMul(A, B).doit() == A*B\n    assert MatMul(A, B, C).doit() == A*B*C\n    assert MatMul(A, B, C, D).doit() == A*B*C*D\n\n    # Test with Identity\n    I = Identity(2)\n    assert MatMul(A, I).doit() == A\n    assert MatMul(I, A).doit() == A\n    assert MatMul(A, I, B).doit() == A*B\n    assert MatMul(A, I, I, B).doit() == A*B\n\n    # Test with ZeroMatrix\n    Z = ZeroMatrix(2, 2)\n    assert MatMul(A, Z).doit() == Z\n    assert MatMul(Z, A).doit() == Z\n    assert MatMul(A, Z, B).doit() == Z\n    assert MatMul(A, Z, I, B).doit() == Z\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatMul(A, B).doit() == A*B\n    assert MatMul(A, B).doit(deep=False) == A*B\n\n    A = MatrixSymbol('A', 2, 2)\n    B = ImmutableMatrix([[1, 2], [3, 4]])\n    assert MatMul(A, B).doit() == Matrix([\n        [A[0, 0] + 2*A[0, 1], A[0, 1] + 2*A[0, 1] + 2*A[0, 0]],\n        [A[1, 0] + 2*A[1, 1], A[1, 1] + 2*A[1, 1] + 2*A[1, 0]]\n    ]) != MatMul(A, B)\n"], "sample_1038": ["def test_matrix_derivative():\n    from sympy import Piecewise\n    i, j = symbols('i j')\n    A = MatrixSymbol('A', n, m)\n    x = symbols('x')\n    B = MatrixSymbol('B', m, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, m)\n\n    # First line is A, second is B\n    assert _matrix_derivative(A*B, A) == _LeftRightArgs(\n        first=Identity(n),\n        second=B,\n        higher=None,\n        transposed=False\n    )\n    assert _matrix_derivative(A*B, B) == _LeftRightArgs(\n        first=A,\n        second=Identity(m),\n        higher=None,\n        transposed=False\n    )\n\n    # First line is B, second is A\n    assert _matrix_derivative(B.T*A.T, A) == _LeftRightArgs(\n        first=B,\n        second=Identity(n),\n        higher=None,\n        transposed=True\n    )\n    assert _matrix_derivative(B.T*A.T, B) == _LeftRightArgs(\n        first=Identity(m),\n        second=A,\n        higher=None,\n        transposed=True\n    )\n\n    assert _matrix_derivative(A*B, x) == _LeftRightArgs(\n        ZeroMatrix(n, n),\n        ZeroMatrix(m, m),\n        ZeroMatrix(n, m),\n        False,\n    )\n\n    assert _matrix_derivative(A + B.T, A) == _LeftRightArgs(\n        Identity(n),\n        Identity(m),\n        None,\n        False\n    )\n\n    assert _matrix_derivative(A + B.T, B) == _LeftRightArgs(\n        Identity(m),\n        Identity(n),\n        None,\n        True\n    )\n\n    # First line is A, second is Identity(n)\n    assert _matrix_derivative(A*Identity(n), A) == _LeftRightArgs(\n        Identity(n),\n        Identity(n),\n        None,\n        False\n    )\n\n    assert _matrix_derivative(A/C, A) == _LeftRightArgs(\n        Inverse(C),\n        Identity(m),\n        None,\n        False,\n    )\n\n    assert _matrix_derivative(A/C, C) == _LeftRightArgs(\n        -A*C.inv()*C.inv(),\n        C.inv(),\n        None,\n        True\n    )\n\n    assert _matrix_derivative(A/C, D) == _LeftRightArgs(\n        ZeroMatrix(n, n),\n        ZeroMatrix(n, m),\n       ", "def testMatrixElement_with_Symbol():\n    n, m = symbols('n m', integer=True)\n    A = MatrixSymbol('A', n, m)\n    i = symbols('i')\n    assert A[i, 0] == A[i, 0]\n    assert A[0, i] == A[0, i]\n    raises(TypeError, lambda: A[i, i])\n", "compilation error", "def test_MatMul_doit():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert MatMul(A, B).doit() == MatMul(A, B)\n    assert MatMul(A, B).doit(deep=False) == MatMul(A, B)\n    assert MatMul(A, Identity(m)).doit() == A\n    assert MatMul(Identity(n), A).doit() == A\n    assert MatMul(A, ZeroMatrix(m, l)).doit() == ZeroMatrix(n, l)\n    assert MatMul(ZeroMatrix(n, m), A).doit() == ZeroMatrix(n, l)\n", "def test_factor_terms():\n    A = MatrixSymbol(\"A\", n, n)\n    B = MatrixSymbol(\"B\", n, n)\n    assert factor_terms(A + A*B*A) == A*(Identity(n) + B*A)\n"], "sample_1039": ["def test_print_integer():\n    assert mpp._print(-1) == '<mo>-</mo><mn>1</mn>'\n    assert mp._print(-1) == '<cn>-1</cn>'\n    assert mpp._print(1) == '<mn>1</mn>'\n    assert mp._print(1) == '<cn>1</cn>'\n", "def test_print_assoc_op():\n    from sympy.core import AssocOp\n    class TestAssocOp(AssocOp):\n        identity = 0\n\n    expr = TestAssocOp(x, y)\n    assert mpp.doprint(expr) == '<mrow><mi>testassocop</mi><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mp.doprint(expr) == '<apply><testassocop><ci>x</ci><ci>y</ci></apply>'\n", "def test_print_int():\n    assert mpp.doprint(1) == '<mn>1</mn>'\n    assert mp.doprint(1) == '<cn>1</cn>'\n", "def test_print_NegativeInfinity():\n    assert mpp.doprint(-oo) == '<mrow><mo>\u2212</mo><mi>&#x221E;</mi></mrow>'\n    assert mp.doprint(-oo) == '<apply><minus/><infinity/></apply>'\n", "def test_presentation_mathml_Pow():\n    mml_1 = mpp._print(x**-1)\n    assert mml_1.nodeName == 'mfrac'\n    assert mml_1.childNodes[0].nodeName == 'mn'\n    assert mml_1.childNodes[0].childNodes[0].nodeValue == '1'\n    assert mml_1.childNodes[1].nodeName == 'mi'\n    assert mml_1.childNodes[1].childNodes[0].nodeValue == 'x'\n\n    mml_2 = mpp._print(x**(1/2))\n    assert mml_2.nodeName == 'msqrt'\n    assert mml_2.childNodes[0].nodeName == 'mi'\n    assert mml_2.childNodes[0].childNodes[0].nodeValue == 'x'\n\n    mml_3 = mpp._print(x**-2)\n    assert mml_3.nodeName == 'mfrac'\n    assert mml_3.childNodes[0].nodeName == 'mn'\n    assert mml_3.childNodes[0].childNodes[0].nodeValue == '1'\n    assert mml_3.childNodes[1].nodeName == 'msup'\n    assert mml_3.childNodes[1].childNodes[0].nodeName == 'mi'\n    assert mml_3.childNodes[1].childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mml_3.childNodes[1].childNodes[1].nodeName == 'mn'\n    assert mml_3.childNodes[1].childNodes[1].childNodes[0].nodeValue == '2'\n"], "sample_1040": ["def test_print_function():\n    f = Function('f')\n    g = Function('g')\n    expr = f(x, g(y))\n    assert mpp.doprint(expr) == '<mrow><mi>f</mi><mfenced><mi>x</mi><mrow><mi>g</mi><mfenced><mi>y</mi></mfenced></mrow></mfenced></mrow>'\n    assert mp.doprint(expr) == '<apply><f/><ci>x</ci><apply><g/><ci>y</ci></apply></apply>'\n", "def test_print_assocop():\n    A = Basic(1, 2)\n    B = Basic(2, 3)\n    C = Basic(3, 4)\n    expr = Basic(A, B, C)\n    assert mpp.doprint(expr) == '<mrow><mi>basic</mi><mrow><mi>basic</mi><mfenced><mn>1</mn><mn>2</mn></mfenced></mrow><mrow><mi>basic</mi><mfenced><mn>2</mn><mn>3</mn></mfenced></mrow><mrow><mi>basic</mi><mfenced><mn>3</mn><mn>4</mn></mfenced></mrow></mrow>'\n    assert mp.doprint(expr) == '<basic><basic><cn>1</cn><cn>2</cn></basic><basic><cn>2</cn><cn>3</cn></basic><basic><cn>3</cn><cn>4</cn></basic></basic>'\n", "def test_print_negative_infinity():\n    assert mpp.doprint(-oo) == '<mrow><mo>-</mo><mi>&#x221E;</mi></mrow>'\n    assert mp.doprint(-oo) == '<apply><minus/><infinity/></apply>'\n", "def test_print_empty_matrix():\n    A = Matrix()\n    assert mpp.doprint(A) == '<mtable/>'\n    assert mp.doprint(A) == '<matrix/>'\n", "def test_print_assocop():\n    class TestAssocOp(Basic, AssocOp):\n        pass\n    expr = TestAssocOp(1, 2)\n    assert mpp.doprint(expr) == '<mrow><mi>testassocop</mi><mfenced><mn>1</mn><mn>2</mn></mfenced></mrow>'\n    assert mp.doprint(expr) == '<apply><testassocop/><cn>1</cn><cn>2</cn></apply>'\n"], "sample_1041": ["def test_matrix_derivative():\n    from sympy import sin, cos\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    A_diff = MatrixSymbol('A', 2, 2)\n    f = sin(x)*A\n    assert f.diff(x) == cos(x)*A_diff\n", "def test_matrix_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    x = symbols('x')\n    \n    f = x*A*B\n    assert f.diff(x) == A*B\n    \n    f = A*B*x\n    assert f.diff(x) == A*B\n    \n    f = A*x*B\n    raises(NotImplementedError, lambda: f.diff(x))\n\n    # Higher order matrix derivative\n    f = x**3*A*B\n    assert f.diff(x) == 3*x**2*A*B\n    assert f.diff(x, x) == 6*x*A*B\n    assert f.diff(x, x, x) == 6*A*B\n    assert f.diff(x, x, x, x) == ZeroMatrix(n, l)\n", "def test_applyfunc():\n    A = MatrixSymbol('A', 2, 2)\n    f = lambda x: x**2\n    assert A.applyfunc(f).as_explicit() == Matrix([\n        [A[0, 0]**2, A[0, 1]**2],\n        [A[1, 0]**2, A[1, 1]**2]\n    ])\n    B = ImmutableMatrix([[1, 2], [3, 4]])\n    assert B.applyfunc(f) == Matrix([\n        [1, 4],\n        [9, 16]\n    ])\n", "def test_matrixelemnt_eq():\n    A = MatrixSymbol(\"A\", n, n)\n    assert Eq(A[0, 0], A[0, 0]) == True\n    assert Eq(A[0, 0], A[1, 1]) == False\n    assert Eq(A[1, 1], A[0, 0]) == False\n    assert Eq(A[0, 0], A[0, 1]) == False\n    assert Eq(A[0, 0], 1) == False\n", "def test_equality():\n    A = MatrixSymbol(\"A\", n, n)\n    B = MatrixSymbol(\"B\", n, n)\n    assert A.equals(A)\n    assert not A.equals(B)\n    assert A.equals(A.as_explicit())\n    assert not A.equals(B.as_explicit())\n    assert A.as_explicit().equals(A)\n    assert not B.as_explicit().equals(A)\n"], "sample_1042": ["def test_IndexedBase_label():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A')\n    assert A.label == Symbol('A')\n    assert A[i, j].label == Symbol('A')\n    B = IndexedBase('B', shape=(2, 3))\n    assert B.label == Symbol('B')\n    assert B[i, j].label == Symbol('B')\n", "def test_IndexedBase_strides():\n    i, j, k = symbols('i j k', integer=True)\n    m, n, o, p = symbols('m n o p', integer=True)\n    A = IndexedBase('A', strides=(m, n))\n    assert A.strides == Tuple(m, n)\n    assert A[i, j].base.strides == Tuple(m, n)\n    B = IndexedBase('B', strides=(o, p))\n    assert B.strides == Tuple(o, p)\n    assert B[i, j].base.strides == Tuple(o, p)\n    raises(TypeError, lambda: IndexedBase('C', strides=5))\n", "def test_IndexedBase_str():\n    A = IndexedBase(\"A\")\n    assert str(A) == \"A\"\n", "def test_IndexedBase_shape_with_non_Symbol():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', shape=(2, 3))\n    assert a.shape == Tuple(2, 3)\n    assert a[i, j].shape == Tuple(2, 3)\n    assert a[i, j].ranges == [(0, 1), (0, 2)]\n", "def test_IndexedBase_shape_with_Infinity():\n    i, j, m, n = symbols('i j m n', integer=True)\n    a = IndexedBase('a', shape=(m, oo))\n    b = IndexedBase('b', shape=(n, m))\n    assert b.shape == Tuple(n, m)\n    assert a[i, j] != b[i, j]\n    assert a[i, j] == b[i, j].subs(n, m).subs(m, oo)\n    assert a.func(*a.args) == a\n    assert a[i, j].func(*a[i, j].args) == a[i, j]\n    raises(IndexException, lambda: a[i])\n    raises(IndexException, lambda: a[i, i, j])\n    F = IndexedBase(\"F\", shape=m)\n    assert F.shape == Tuple(m)\n    assert F[i].subs(i, j) == F[j]\n    raises(IndexException, lambda: F[i, j])\n"], "sample_1043": ["def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n    assert mcode(sin(x), user_functions={'sin': lambda x: 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': lambda x: 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': lambda x: 'MyExp'}) == \"MyExp[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n    assert mcode(conjugate(x), user_functions={'conjugate': 'MyConjugate'}) == \"MyConjugate[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n    assert mcode(f(x), user_functions={f: 'MyF'}) == \"MyF[x]\"\n", "def test_MCodePrinter_settings():\n    from sympy.printing.mathematica import MCodePrinter\n    printer = MCodePrinter({'human': False})\n    assert printer.doprint(x) == 'x'\n    assert printer.doprint(sin(x)) == 'Sin[x]'\n    printer = MCodePrinter({'human': True})\n    assert printer.doprint(x) == 'x'\n    assert printer.doprint(sin(x)) == 'Sin[x]'\n\n    printer = MCodePrinter({'allow_unknown_functions': True})\n    assert printer.doprint(f(x)) == 'f[x]'\n    printer = MCodePrinter({'allow_unknown_functions': False})\n    assert printer.doprint(f(x)) == 'f[x]'\n\n    printer = MCodePrinter({'order': None})\n    assert printer.doprint(x + sin(x)) == 'x + Sin[x]'\n    printer = MCodePrinter({'order': 'none'})\n    assert printer.doprint(x + sin(x)) == 'x + Sin[x]'\n\n    printer = MCodePrinter({'full_prec': False})\n    assert printer.doprint(3.14159265358979323846264338327950288419716939937511) == \\\n        '3.141592653589793'\n    printer = MCodePrinter({'full_prec': True})\n    assert printer.doprint(3.14159265358979323846264338327950288419716939937511) == \\\n        '3.14159265358979323846264338327950288419716939937511'\n\n    printer = MCodePrinter({'precision': 10})\n    assert printer.doprint(3.14159265358979323846264338327950288419716939937511) == \\\n        '3.141592654'\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n    assert mcode(conjugate(x), user_functions={'conjugate': 'MyConjugate'}) == \\\n        \"MyConjugate[x]\"\n"], "sample_1044": ["def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True)\n    assert (x**y).is_integer is True\n    assert (x**2.5).is_integer is None\n    assert (2.5**x).is_integer is None\n    assert (2**x).is_integer is True\n", "def test_Pow_is_even_odd():\n    z = Symbol('z', integer=True)\n    assert (z**2).is_even\n    assert (z**4).is_even\n    assert (z**6).is_even\n    assert (z**2).is_odd is False\n    assert (z**4).is_odd is False\n    assert (z**6).is_odd is False\n\n    assert (z**3).is_odd\n    assert (z**5).is_odd\n    assert (z**7).is_odd\n    assert (z**3).is_even is False\n    assert (z**5).is_even is False\n    assert (z**7).is_even is False\n\n    assert (z**0).is_even\n    assert (z**0).is_odd is False\n\n    assert (z**1).is_even is False\n    assert (z**1).is_odd\n\n    z2 = Symbol('z2', integer=False)\n    assert (z2**2).is_even is None\n    assert (z2**4).is_even is None\n    assert (z2**6).is_even is None\n    assert (z2**2).is_odd is None\n    assert (z2**4).is_odd is None\n    assert (z2**6).is_odd is None\n\n    assert (z2**3).is_odd is None\n    assert (z2**5).is_odd is None\n    assert (z2**7).is_odd is None\n    assert (z2**3).is_even is None\n    assert (z2**5).is_even is None\n    assert (z2**7).is_even is None\n\n    assert (z2**0).is_even\n    assert (z2**0).is_odd is False\n\n    assert (z2**1).is_even is None\n    assert (z2**1).is_odd is None\n", "def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True)\n    assert (x**y).is_integer is True\n    assert (x**(y + S.Half)).is_integer is None\n    assert (x**(y - S.Half)).is_integer is None\n    assert (x**S.Half).is_integer is None\n    z = Symbol('z')\n    assert (x**z).is_integer is None\n    assert (z**y).is_integer is None\n    assert (z**x).is_integer is None\n", "def test_Pow_is_nonzero():\n    x = Symbol('x', real=True)\n    z = Symbol('z', zero=True)\n    n = Symbol('n', integer=True, positive=True)\n    assert (x**2).is_nonzero is None\n    assert (x**2).is_zero is None\n    assert (S.Pi**n).is_nonzero\n    assert (z**n).is_zero\n", "def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True)\n    assert (x**y).is_integer\n    assert (x**(y + 1)).is_integer\n\n    x = Symbol('x', integer=True)\n    y = Symbol('y', noninteger=True)\n    assert (x**y).is_integer is False\n    assert (x**(y + 1)).is_integer is False\n\n    x = Symbol('x', noninteger=True)\n    y = Symbol('y', integer=True)\n    assert (x**y).is_integer is False\n    assert (x**(y + 1)).is_integer is False\n\n    x = Symbol('x', noninteger=True)\n    y = Symbol('y', noninteger=True)\n    assert (x**y).is_integer is None\n    assert (x**(y + 1)).is_integer is None\n"], "sample_1045": ["def test_mod_inverse_issue_14734():\n    a = mod_inverse(3, 11)\n    assert a == 4\n    assert type(a) is Integer\n    a = mod_inverse(2, 5)\n    assert a == 3\n    assert type(a) is Integer\n    a = mod_inverse(2, -5)\n    assert a == -3\n    assert type(a) is Integer\n    a = mod_inverse(-2, 5)\n    assert a == 3\n    assert type(a) is Integer\n    a = mod_inverse(-2, -5)\n    assert a == -3\n    assert type(a) is Integer\n", "def test_issue_16678():\n    assert (S.Infinity*Symbol('x')).is_finite is False\n    assert (S.NegativeInfinity*Symbol('x')).is_finite is False\n", "def test_issue_6125():\n    assert (-S.Pi/4) * (S(1)/S.Sqrt(2) + S.I/S.Sqrt(2)) == -S.I/S.Sqrt(2)\n    assert (S.Pi/4) * (S(1)/S.Sqrt(2) + S.I/S.Sqrt(2)) == S.I/S.Sqrt(2)\n    assert (-S.Pi/4) * (S(1)/S.Sqrt(2) - S.I/S.Sqrt(2)) == S.I/S.Sqrt(2)\n    assert (S.Pi/4) * (S(1)/S.Sqrt(2) - S.I/S.Sqrt(2)) == -S.I/S.Sqrt(2)\n", "def test_issue_10368():\n    from fractions import Fraction\n    f = Fraction(-19, 3)\n    i = Integer(f)\n    assert i == -6\n    assert int(f) == -6\n    assert i.is_Integer\n", "def test_issue_13648():\n    assert Integer(0).gcd(Integer(0)) == 0\n    assert Integer(0).gcd(Integer(1)) == 1\n    assert Integer(0).gcd(Integer(-1)) == 1\n    assert Integer(0).gcd(Integer(2)) == 2\n    assert Integer(0).gcd(Integer(-2)) == 2\n"], "sample_1047": ["def test_issue_symbol_nonnegative():\n    x = Symbol('x', nonnegative=True)\n    assert x.is_nonnegative is True\n    assert x.is_positive is None\n    assert x.is_zero is None\n    assert x.is_negative is False\n    assert x.is_nonpositive is None\n", "def test_issue_8613():\n    x = Symbol('x', real=True, finite=True)\n    assert x.is_complex\n    assert x.is_extended_real\n    x = Symbol('x', real=True, infinite=True)\n    assert x.is_complex\n    assert x.is_extended_real\n\n    x = Symbol('x', complex=True)\n    assert x.is_complex\n    assert x.is_extended_real is None\n\n    x = Symbol('x', imaginary=True)\n    assert x.is_complex\n    assert x.is_extended_real is False\n", "def test_symbol_nonnegative_nonpositive():\n    x = Symbol('x', nonnegative=True, nonpositive=True)\n    assert x.is_nonnegative is True\n    assert x.is_nonpositive is True\n    assert x.is_positive is False\n    assert x.is_negative is False\n    assert x.is_zero is True\n", "compilation error", "def test_symbol_nonzero_real():\n    # issue 19455\n    x = Symbol('x', nonzero=True, real=True)\n    assert x.is_positive is None\n    assert x.is_nonpositive is None\n    assert x.is_negative is None\n    assert x.is_nonnegative is None\n"], "sample_1046": ["def test_tensor_element():\n    L = TensorIndexType(\"L\", dim=4)\n    A = tensorhead(\"A\", [L, L], [[1], [1]])\n    B = tensorhead(\"B\", [L, L], [[1], [1]])\n    i, j, k = tensor_indices(\"i j k\", L)\n\n    te = TensorElement(A(i, j), {i: 1})\n\n    assert str(te) == \"A(L_0, j)\"\n    assert te.get_free_indices() == [j]\n    assert te.get_indices() == [j]\n\n    te2 = TensorElement(A(i, j), {j: 1})\n    assert str(te2) == \"A(i, L_0)\"\n\n    te3 = TensorElement(A(i, j), {i: 1, j: 2})\n    assert str(te3) == \"A(L_0, L_1)\"\n\n    te4 = TensorElement(A(i, j), {})\n    assert str(te4) == \"A(i, j)\"\n\n    te5 = TensorElement(A(i, j) + B(i, j), {i: 1, j: 2})\n    assert str(te5) == \"A(L_0, L_1) + B(L_0, L_1)\"\n\n    te6 = TensorElement(A(i, j), {i: 1}) + TensorElement(B(i, j), {i: 1})\n    assert str(te6) == \"A(L_0, j) + B(L_0, j)\"\n", "def test_TensorElement():\n    L = TensorIndexType(\"L\", dim=4)\n    A = tensorhead(\"A\", [L, L, L, L], [[1], [1], [1], [1]])\n    i0, i1, i2, i3 = tensor_indices(\"i0:4\", L)\n\n    te = TensorElement(A(i0, i1, i2, i3), {i0: 1})\n    assert te.index_types == [L, L, L]\n    assert te.free == [(i1, 0), (i2, 1), (i3, 2)]\n    assert te.dum == []\n\n    te = TensorElement(A(i0, i1, i2, i3), {i0: 1, i1: 2})\n    assert te.index_types == [L, L]\n    assert te.free == [(i2, 0), (i3, 1)]\n    assert te.dum == []\n", "def test_TensorElement():\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices('i,j,k,l', L)\n    A = tensorhead(\"A\", [L, L], [[1], [1]])\n\n    t = A(i, j)\n    te = TensorElement(t, {i: 2})\n    assert te.get_free_indices() == [j]\n    assert te == A(2, j)\n\n    te = TensorElement(t, {j: 2})\n    assert te.get_free_indices() == [i]\n    assert te == A(i, 2)\n\n    te = TensorElement(t, {i: 2, j: 3})\n    assert te.get_free_indices() == []\n    assert te == A(2, 3)\n\n    raises(ValueError, lambda: TensorElement(t, {k: 2}))\n", "def test_tensor_element():\n    L = TensorIndexType(\"L\", dim=4)\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A = tensorhead(\"A\", [L, L], [[1], [1]])\n    B = tensorhead(\"B\", [L, L], [[1], [1]])\n\n    te1 = TensorElement(A(i, j), {i: 2})\n    assert te1.get_free_indices() == [j]\n    assert te1.get_indices() == [j]\n\n    te2 = TensorElement(A(i, j), {i: 2, j: 1})\n    assert te2.get_free_indices() == []\n    assert te2.get_indices() == []\n\n    te3 = TensorElement(A(i, j) + B(i, j), {i: 2})\n    assert te3.get_free_indices() == [j]\n    assert te3.get_indices() == [j]\n\n    te4 = TensorElement(A(i, j)*B(j, k), {i: 2, j: 1})\n    assert te4.get_free_indices() == [k]\n    assert te4.get_indices() == [k]\n", "def test_TensorIndexType_metric_data_removal():\n    L = TensorIndexType(\"L\")\n    L.data = [[1, 0], [0, 1]]\n    L._components_data_full_destroy()\n    assert L.data is None\n"], "sample_1050": ["def test_PythonCodePrinter_print_Piecewise_with_multiple_conditions():\n    prntr = PythonCodePrinter()\n    piecewise = Piecewise((1, Eq(x, 0)), (2, x > 6), (3, x < 0))\n    assert prntr.doprint(piecewise) == '((1) if (x == 0) else (2) if (x > 6) else (3) if (x < 0) else None)'\n", "def test_print_Assignment_with_SparseMatrix():\n    n = SciPyPrinter()\n    A = SparseMatrix(2, 2, {(0, 1): x})\n    B = SparseMatrix(2, 2, {(0, 0): y})\n    expr = Assignment(A, B)\n    assert n.doprint(expr) == 'A = scipy.sparse.coo_matrix([y], ([0], [0]), shape=(2, 2))'\n    assert 'scipy.sparse' in n.module_imports\n", "def test_PythonCodePrinter_print_Piecewise():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6), (3, True))) == '((1) if (x == 0) else (2) if (x > 6) else 3)'\n    assert prntr.doprint(Piecewise((2, Le(x, 0)),\n                        (3, Gt(x, 0)), (4, True), evaluate=False)) == '((2) if (x <= 0) else'\\\n                                                        ' (3) if (x > 0) else 4)'\n", "def test_NumPyPrinter_print_Mod():\n    p = NumPyPrinter()\n    assert p.doprint(Mod(x, 2)) == 'numpy.mod(x, 2)'\n", "def test_PythonCodePrinter_print_ComplexInfinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(zoo) == \"float('nan')\"\n"], "sample_1048": ["def test_parabola_equation():\n    l1 = Line(Point(1, -2), Point(-1,-2))\n    l2 = Line(Point(1, 2), Point(-1,2))\n    l3 = Line(Point(1, 0), Point(-1,0))\n\n    p1 = Point(0,0)\n    p2 = Point(0, -2)\n    p3 = Point(120, -12)\n    parabola1 = Parabola(p1, l1)\n\n    x, y = symbols('x y')\n    assert parabola1.equation() == -x**2 - 16*y + 64\n    assert parabola1.equation(x='f') == -symbols('f')**2 - 16*y + 64\n    assert parabola1.equation(y='z') == -x**2 - 16*symbols('z') + 64\n\n    parabola2 = Parabola(p2, l2)\n    assert parabola2.equation() == x**2 + 8*y + 16\n    assert parabola2.equation(x='g') == symbols('g')**2 + 8*y + 16\n    assert parabola2.equation(y='w') == x**2 + 8*symbols('w') + 16\n", "def test_parabola_equation():\n    x, y = symbols('x y', real=True)\n    p1 = Point(0, 0)\n    l1 = Line(Point(1, -2), Point(-1,-2))\n    l2 = Line(Point(1, 2), Point(-1,2))\n    l3 = Line(Point(1, 0), Point(-1,0))\n\n    parabola1 = Parabola(p1, l1)\n    parabola2 = Parabola(p1, l2)\n    parabola3 = Parabola(p1, l3)\n\n    # test equation with default symbols\n    assert parabola1.equation() == -x**2 - 8*y + 16\n    assert parabola2.equation() == x**2 - 8*y - 16\n    assert parabola3.equation() == y**2 - 8*x\n\n    # test equation with custom symbols\n    assert parabola1.equation(x='a', y='b') == -a**2 - 8*b + 16\n    assert parabola2.equation(x='c', y='d') == c**2 - 8*d - 16\n    assert parabola3.equation(x='e', y='f') == f**2 - 8*e\n", "def test_parabola_equation():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    p3 = Point(0, 4)\n    p4 = Point(6, 0)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    d3 = Line(Point(4, 0), slope=oo)\n    d4 = Line(Point(7, 6), slope=0)\n    pa1 = Parabola(None, d2)\n    pa2 = Parabola(directrix=d1)\n    pa3 = Parabola(p1, d1)\n    pa4 = Parabola(p2, d2)\n    pa5 = Parabola(p2, d4)\n    pa6 = Parabola(p3, d2)\n    pa7 = Parabola(p2, d1)\n    pa8 = Parabola(p4, d1)\n    pa9 = Parabola(p4, d3)\n    x, y = symbols('x y')\n\n    # equations of parabolas\n    assert pa1.equation() == -x**2 - 12*y + 36\n    assert pa2.equation() == -y**2 + 8*x\n    assert pa3.equation() == -y**2 + 8*x\n    assert pa4.equation() == -x**2 - 14*y + 98\n    assert pa5.equation() == -x**2 - 14*y + 98\n    assert pa6.equation() == -x**2 - 8*y + 16\n    assert pa7.equation() == -y**2 + 4*x + 10\n    assert pa8.equation() == -y**2 + 8*x\n    assert pa9.equation() == -y**2 + 8*x\n\n    # custom symbols in equation\n    assert pa1.equation(x='f', y='g') == -f**2 - 12*g + 36\n    assert pa1.equation(x='x1', y='y1') == -x1**2 - 12*y1 + 36\n\n    # equation of parabola with more complex expression\n    a, b = symbols('a", "def test_parabola_equation():\n    p1 = Point(0, 0)\n    l1 = Line(Point(5, 8), Point(7, 8))\n    p2 = Point(0, 0)\n    l2 = Line(Point(3, 6), Point(5, 6))\n    p3 = Point(1, 1)\n    l3 = Line(Point(1, 5), Point(3, 5))\n    p4 = Point(1, 1)\n    l4 = Line(Point(5, 1), Point(7, 1))\n\n    parabola1 = Parabola(p1, l1)\n    parabola2 = Parabola(p2, l2)\n    parabola3 = Parabola(p3, l3)\n    parabola4 = Parabola(p4, l4)\n\n    assert parabola1.equation() == -parabola1.x**2 - 16*parabola1.y + 64\n    assert parabola2.equation() == -parabola2.x**2 - 8*parabola2.y + 24\n    assert parabola3.equation() == -(parabola3.x - 1)**2 - 8*parabola3.y + 33\n    assert parabola4.equation() == -(parabola4.y - 1)**2 - 8*parabola4.x + 33\n\n    x, y = symbols('x y', real=True)\n    assert parabola1.equation(x) == -x**2 - 16*y + 64\n    assert parabola2.equation(y='z') == -x**2 - 8*y + 24\n    assert parabola3.equation(x, y='z') == -(x - 1)**2 - 8*y + 33\n    assert parabola4.equation(x='f', y='g') == -(y - 1)**2 - 8*x + 33\n", "def test_parabola_axis_of_symmetry():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.axis_of_symmetry == Line2D(Point2D(0, 0), Point2D(0, 1))\n\n    p2 = Parabola(Point(1, 1), Line(Point(1, 8), Point(1, 9)))\n    assert p2.axis_of_symmetry == Line2D(Point2D(1, 1), Point2D(2, 1))\n\n    p3 = Parabola(Point(-3, -4), Line(Point(0, -4), Point(1, -4)))\n    assert p3.axis_of_symmetry == Line2D(Point2D(-3, -4), Point2D(-3, -3))\n\n    p4 = Parabola(Point(0, 0), Line(Point(0, 5), Point(0, 7)))\n    assert p4.axis_of_symmetry == Line2D(Point2D(0, 0), Point2D(1, 0))\n\n    p5 = Parabola(Point(1, 2), Line(Point(1, 3), Point(1, 4)))\n    assert p5.axis_of_symmetry == Line2D(Point2D(1, 2), Point2D(2, 2))\n"], "sample_1049": ["def test_arbitrary_point():\n    u, v = symbols('u v', real=True)\n    x, y, z = symbols('x y z', real=True)\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(1, 2, 3)\n    pl3 = Plane(p1, p2, p3)\n    pl4 = Plane(p1, normal_vector=(1, 1, 1))\n\n    # check that arbitrary_point returns a point in the plane\n    assert pl3.arbitrary_point(u, v) in pl3\n    assert pl4.arbitrary_point(u, v) in pl4\n\n    # check that arbitrary_point returns a point on the circle\n    assert pl3.arbitrary_point(u) in pl3\n    assert pl4.arbitrary_point(u) in pl4\n\n    # check that arbitrary_point(u, v) and arbitrary_point(u) are different\n    assert pl3.arbitrary_point(u, v) != pl3.arbitrary_point(u)\n    assert pl4.arbitrary_point(u, v) != pl4.arbitrary_point(u)\n", "def test_plane_equals():\n    p1 = Plane(Point3D(0, 0, 0), normal_vector=(1, 1, 1))\n    p2 = Plane(Point3D(0, 0, 0), normal_vector=(2, 2, 2))\n    p3 = Plane(Point3D(0, 0, 0), normal_vector=(1, 2, 3))\n    assert p1.equals(p1)\n    assert p1.equals(p2)\n    assert not p1.equals(p3)\n    assert not p1.equals(\"not a plane\")\n", "def test_plane_are_concurrent():\n    p1 = Plane((0, 0, 0), (1, 1, 1))\n    p2 = Plane((1, 1, 1), (1, 1, 1))\n    p3 = Plane((1, 1, 0), (0, 0, 1))\n    assert Plane.are_concurrent(p1, p2) is True\n    assert Plane.are_concurrent(p1, p3) is False\n", "def test_is_coplanar():\n    p1 = Point3D(1, 1, 1)\n    p2 = Point3D(2, 2, 2)\n    p3 = Point3D(3, 3, 3)\n    p4 = Point3D(4, 4, 4)\n    pl1 = Plane(p1, p2, p3)\n    pl2 = Plane(p1, p2, p3)\n    pl3 = Plane(p1, p2, p4)\n    assert pl1.is_coplanar(pl2)\n    assert not pl1.is_coplanar(pl3)\n    assert pl1.is_coplanar(p1)\n    assert pl1.is_coplanar(p2)\n    assert pl1.is_coplanar(p3)\n    assert not pl1.is_coplanar(p4)\n    assert pl1.is_coplanar(Line3D(p1, p2))\n    assert not pl1.is_coplanar(Line3D(p1, p4))\n", "def test_plane_arbitrary_point():\n    u, v = symbols('u v', real=True)\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(1, 2, 3)\n    pl3 = Plane(p1, p2, p3)\n\n    # arbitrary point with 2 parameters\n    a = pl3.arbitrary_point(u, v)\n    assert a in pl3\n    assert len(a.free_symbols) == 2\n    assert a.subs({u: 0, v: 0}) == p1\n\n    # arbitrary point with 1 parameter\n    b = pl3.arbitrary_point(u)\n    assert b in pl3\n    assert len(b.free_symbols) == 1\n    assert b.subs(u, 0) == p1\n"], "sample_1051": ["def test_dotprint_rankdir():\n    text = dotprint(x+2, rankdir='LR')\n    assert 'rankdir=\"LR\"' in text\n    text = dotprint(x+2)\n    assert 'rankdir=\"TD\"' in text\n", "def test_dotprint_custom_graph_style():\n    text = dotprint(x+2, repeat=False, rankdir='LR', ordering='in')\n    assert 'rankdir=\"LR\"' in text\n    assert 'ordering=\"in\"' in text\n    assert 'digraph' in text\n", "def test_dotprint_graphstyle():\n    text = dotprint(x + 2, rankdir='LR')\n    assert 'rankdir=\"LR\"' in text\n    text = dotprint(x + 2, ordering='in')\n    assert 'ordering=\"in\"' in text\n    text = dotprint(x + 2, label='My Graph')\n    assert 'label=\"My Graph\"' in text\n", "def test_dotprint_styles():\n    styles = [(Basic, {'color': 'red', 'shape': 'box'}),\n              (Expr,  {'color': 'green'})]\n    text = dotprint(x+2, styles=styles)\n    assert '\"color\"=\"green\"' in text\n    assert '\"shape\"=\"ellipse\"' in text\n    text = dotprint(x+2, styles=styles, labelfunc=srepr)\n    assert '\"color\"=\"green\"' in text\n    assert '\"shape\"=\"ellipse\"' in text\n    assert \"Add(Symbol('x'), Integer(2))\" in text\n    assert 'Symbol(\\'x\\')' in text\n    assert 'Integer(2)' in text\n", "def test_dotprint_styles():\n    text = dotprint(x + 2, styles=[(Basic, {'color': 'red'})])\n    assert '\"color\"=\"red\"' in text\n    text = dotprint(x + 2, styles=[(Expr, {'shape': 'box'})])\n    assert '\"shape\"=\"box\"' in text\n    text = dotprint(x + 2, graphstyle={'rankdir': 'LR'})\n    assert '\"rankdir\"=\"LR\"' in text\n    text = dotprint(x + 2, graphstyle={'ordering': 'in'})\n    assert '\"ordering\"=\"in\"' in text\n"], "sample_1052": ["def test_octave_code_with_matrix():\n    n = symbols('n')\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    expr = A*B*C\n    result = codegen(('test', expr), 'octave', 'test', header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        'function [out_0000] = test(A, B, C)\\n'\n        '  out_0000 = A*B*C;\\n'\n        'end\\n'\n    )\n    assert source == expected\n", "compilation error", "def test_fcodegen_cse():\n    a, b, c, d = symbols('a b c d')\n    e = MatrixSymbol('e', 3, 1)\n    name_expr = (\"test\", [Equality(e, Matrix([[a*b], [a*b + c*d], [a*b*c*d]]))])\n    generator = FCodeGen(cse=True)\n    result = codegen(name_expr, code_gen=generator, header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        'subroutine test(a, b, c, d, e)\\n'\n        'implicit none\\n'\n        'REAL*8, intent(in) :: a\\n'\n        'REAL*8, intent(in) :: b\\n'\n        'REAL*8, intent(in) :: c\\n'\n        'REAL*8, intent(in) :: d\\n'\n        'REAL*8, intent(out), dimension(1:3) :: e\\n'\n        'REAL*8 :: x0\\n'\n        'REAL*8 :: x1\\n'\n        'x0 = a*b\\n'\n        'x1 = c*d\\n'\n        'e(1) = x0\\n'\n        'e(2) = x0 + x1\\n'\n        'e(3) = x0*x1\\n'\n        'end subroutine\\n'\n    )\n    assert source == expected\n", "def test_julia_code():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y)*z\n    routine = make_routine(\"test\", expr)\n    code_gen = JuliaCodeGen()\n    source = get_string(code_gen.dump_jl, [routine])\n    expected = (\n        \"function test(x, y, z)\\n\"\n        \"    test_result = z*(x + y)\\n\"\n        \"    return test_result\\n\"\n        \"end\\n\"\n    )\n    assert source == expected\n", "def test_octave_codegen():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y)*z\n    result = codegen((\"test\", expr), \"octave\", \"file\", header=False, empty=False)\n    expected = (\n        \"function out = test(x, y, z)\\n\"\n        \"  out = z.*(x + y);\\n\"\n        \"end\\n\"\n    )\n    assert result[0][1] == expected\n"], "sample_1054": ["def test_issue_21034():\n    circle = ComplexRegion(Interval(1, 2) * Interval(0, 2 * pi, True, True), polar=True)\n    assert 2 in circle.boundary\n    assert 1 not in circle.boundary\n    assert 2 * I in circle.boundary\n    assert 1 + 2 * I in circle\n    assert 2 + 2 * I in circle.boundary\n", "def test_issue_12274():\n    from sympy import atan2, exp_polar\n\n    a, b = Interval(0, 1), Interval(0, pi)\n    c, d = Interval(2, 3), Interval(pi, 3 * pi / 2)\n    cp1 = ComplexRegion(a * b, polar=True)\n    cp2 = ComplexRegion(c * d, polar=True)\n\n    assert cp1.intersect(cp2) == S.EmptySet\n    assert cp1.intersect(cp1) == cp1\n\n    p1 = Union(cp1, cp2)\n    assert p1.intersect(cp1) == cp1\n    assert p1.intersect(cp2) == cp2\n\n    assert ImageSet(Lambda(x, exp_polar(x)), Interval(0, pi)) == ComplexRegion(Interval(1, exp(1))*Interval(0, pi), polar=True)\n\n    assert ImageSet(Lambda((x, y), x + y*I), Interval(-1, 1)*Interval(-1, 1)).intersect(S.Reals) == Interval(-1, 1)\n\n    assert ImageSet(Lambda((r, theta), r*exp_polar(I*theta)), Interval(0, 1)*Interval(0, 2*pi)).intersect(S.Reals) == Interval(-1, 1)\n    assert ImageSet(Lambda((r, theta), r*exp_polar(I*theta)), Interval(0, 1)*Interval(0, pi)).intersect(S.Reals) == Interval(0, 1)\n    assert ImageSet(Lambda((r, theta), r*exp_polar(I*theta)), Interval(0, 1)*Interval(pi, 2*pi)).intersect(S.Reals) == Interval(-1, 0)\n\n    assert ImageSet(Lambda((x, y), x + y*I), Interval(0, 1)*Interval(0, 1)).intersect(S.Reals) == Interval(0, 1)\n\n    assert ImageSet(Lambda((x, y), x + y*I), Interval(0, 1)*Interval(-1, 1)).intersect(S.Reals) == Interval(0, 1)\n\n    assert ImageSet(Lambda((r, theta), r*exp_polar(I*theta)), Interval(1, 2)*Interval(0, 2*pi)).intersect(S.Reals) == Interval(-2, -1).union", "def test_issue_12172():\n    c1 = ComplexRegion(Interval(1, 2)*Interval(1, 3))\n    c2 = ComplexRegion(Interval(2, 3)*Interval(1, 2))\n    assert c1.union(c2) == ComplexRegion(Union(Interval(1, 2)*Interval(1, 3), Interval(2, 3)*Interval(1, 2)))\n    assert c1.intersect(c2) == ComplexRegion(Interval(2, 2)*Interval(1, 2))\n", "def test_issue_12695():\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    assert c1.difference(c2) == ComplexRegion(Interval(0, 1)*Interval(S.Pi, 2*S.Pi), polar=True)\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(S.Pi/2, 3*S.Pi/2), polar=True)\n    assert c1.difference(c2) == Union(\n        ComplexRegion(Interval(0, 1)*Interval(0, S.Pi/2), polar=True),\n        ComplexRegion(Interval(0, 1)*Interval(3*S.Pi/2, 2*S.Pi), polar=True))\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, 4*S.Pi), polar=True)\n    assert c1.difference(c2) == S.EmptySet\n", "def test_issue_13541():\n    interval = Interval(1, 2)\n    assert Range(1, 3).intersect(interval) == interval\n    assert Range(1, 3).intersect(Interval(2, 3)) == FiniteSet(2)\n    assert Range(1, 3).intersect(Interval(3, 4)) == S.EmptySet\n    assert Range(1, 3).intersect(Interval(0, 1)) == S.EmptySet\n    assert Range(1, 3).intersect(Interval(0, 2)) == Interval(1, 2)\n"], "sample_1053": ["def test_issue_16286():\n    assert Float(1.12345678901234567890, 53) == Float(1.1234567890123457, 53)\n    assert Float(1.12345678901234567890, 50) == Float(1.1234567890123458, 50)\n    assert Float(1.12345678901234567890, 48) == Float(1.123456789012346, 48)\n    assert Float(1.12345678901234567890, 45) == Float(1.12345678901235, 45)\n    assert Float(1.12345678901234567890, 40) == Float(1.1234567890124, 40)\n    assert Float(1.12345678901234567890, 35) == Float(1.123456789012, 35)\n    assert Float(1.12345678901234567890, 30) == Float(1.12345678901, 30)\n    assert Float(1.12345678901234567890, 25) == Float(1.123456789, 25)\n    assert Float(1.12345678901234567890, 20) == Float(1.12345679, 20)\n    assert Float(1.12345678901234567890, 15) == Float(1.1234568, 15)\n    assert Float(1.12345678901234567890, 10) == Float(1.123457, 10)\n    assert Float(1.12345678901234567890, 5) == Float(1.12346, 5)\n    assert Float(1.12345678901234567890, 1) == Float(1.1, 1)\n", "def test_mod_inverse_with_rational():\n    assert mod_inverse(Rational(3, 4), 5) == 4\n    assert mod_inverse(Rational(3, 4), 10) == 2\n    assert mod_inverse(Rational(3, 4), 11) == 10\n    assert mod_inverse(Rational(3, 4), 12) == 4\n", "def test_mod_inverse_integer():\n    assert mod_inverse(3, 11) == 4\n    assert mod_inverse(5, 11) == 9\n    assert mod_inverse(21124921, 521512) == 7713\n    assert mod_inverse(124215421, 5125) == 2981\n    assert mod_inverse(214, 12515) == 1579\n    assert mod_inverse(5823991, 3299) == 1442\n    assert mod_inverse(123, 44) == 39\n    assert mod_inverse(2, 5) == 3\n    assert mod_inverse(-2, 5) == 2\n    assert mod_inverse(2, -5) == -2\n    assert mod_inverse(-2, -5) == -3\n    assert mod_inverse(-3, -7) == -5\n", "def test_float_approximation_interval():\n    from sympy.polys.domains.groundtypes import PythonRational\n\n    # Test exact Float instances\n    assert Float('0').approximation_interval(PythonRational) == (PythonRational(0), PythonRational(0))\n    assert Float('-5').approximation_interval(PythonRational) == (PythonRational(-5), PythonRational(-5))\n    assert Float('2.5').approximation_interval(PythonRational) == (PythonRational(5, 2), PythonRational(5, 2))\n\n    # Test inexact Float instances\n    assert Float('0.1', 1).approximation_interval(PythonRational) == (PythonRational(0), PythonRational(1, 5))\n    assert Float('-0.1', 1).approximation_interval(PythonRational) == (PythonRational(-1, 5), PythonRational(0))\n    assert Float('1.2', 2).approximation_interval(PythonRational) == (PythonRational(6, 5), PythonRational(7, 5))\n\n    # Test Float instances created from exact Rational instances\n    assert Float(Rational(3, 4)).approximation_interval(PythonRational) == (PythonRational(3, 4), PythonRational(3, 4))\n    assert Float(Rational(-1, 2)).approximation_interval(PythonRational) == (PythonRational(-1, 2), PythonRational(-1, 2))\n\n    # Test Float instances created from inexact Rational instances\n    assert Float(Rational(1, 3), 2).approximation_interval(PythonRational) == (PythonRational(1, 4), PythonRational(1, 2))\n    assert Float(Rational(-2, 3), 2).approximation_interval(PythonRational) == (PythonRational(-3, 4), PythonRational(-1, 2))\n", "def test_Rational_order():\n    assert Rational(1, 3).order() == 1\n    assert Rational(1, 4).order() == 1\n    assert Rational(2, 3).order() == 1\n    assert Rational(3, 4).order() == 1\n    assert Rational(22, 7).order() == 1\n"], "sample_1055": ["def test_padded_key_large_symbols():\n    large_symbols = ''.join([chr(i) for i in range(256)])\n    key = 'small'\n    assert padded_key(key, large_symbols)\n    assert len(padded_key(key, large_symbols)) == len(large_symbols)\n    assert list(padded_key(key, large_symbols)).count(key[0]) == 1\n", "def test_uniq():\n    assert uniq(\"abcde\") == \"abcde\"\n    assert uniq(\"aabbccddeeff\") == \"abcdef\"\n    assert uniq(\"ffff\") == \"f\"\n    assert uniq(\"\") == \"\"\n", "def test_decipher_vigenere():\n    assert decipher_vigenere(\"ACE\", \"ABC\") == \"ABC\"\n    assert decipher_vigenere(\"ACA\", \"ABC\", symbols=\"ABCD\") == \"ABC\"\n    assert decipher_vigenere(\"ACC\", \"AB\", symbols=\"ABCD\") == \"ABC\"\n    assert decipher_vigenere(\"AC\", \"ABC\", symbols=\"ABCD\") == \"AB\"\n    assert decipher_vigenere(\"A\", \"ABC\", symbols=\"ABCD\") == \"A\"\n", "def test_decipher_atbash():\n    assert decipher_atbash('ZYX', symbols=None) == 'ABC'\n    assert decipher_atbash('ZYX', symbols=None) == 'ABC'\n    assert decipher_atbash(decipher_atbash('ZYX', symbols=None), symbols=None) == 'ZYX'\n", "def test_padded_key_length():\n    raises(ValueError, lambda: padded_key('abcd', 'abc'))\n"], "sample_1057": ["def test_render_module_imports():\n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules': True})\n    printer.module_imports = ['math']\n    assert render_as_module(ast, standard='python3') == \\\n        'import math\\n\\nprint(\"coordinate: %12.5g %12.5g\" % (x, y))'\n\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules': False})\n    printer.module_imports = {'math': ['sin', 'cos']}\n    assert render_as_module(ast, standard='python3') == \\\n        'from math import sin, cos\\n\\nprint(\"coordinate: %12.5g %12.5g\" % (x, y))'\n", "def test_render_as_module_with_imports():\n    ast = Print('math.sin(x)', \"sin(x): %12.5g\")\n    assert render_as_module(ast, standard='python3') == \\\n        'import math\\n\\nprint(\"sin(x): %12.5g\" % (math.sin(x),))'\n    assert render_as_module(ast, standard='python3', fully_qualified_modules=False) == \\\n        'from math import sin\\n\\nprint(\"sin(x): %12.5g\" % (sin(x),))'\n", "def test_render_with_imports():\n    ast = Print('math.sin(math.pi/2)', \"coordinate: %12.5g\")\n    assert render_as_module(ast, standard='python3') == \\\n        'import math\\n\\nprint(\"coordinate: %12.5g\" % (math.sin(math.pi/2),))'\n    assert render_as_module(ast, standard='python2') == \\\n        'import math\\n\\nprint \"coordinate: %12.5g\" % (math.sin(math.pi/2),)'\n", "def test_render_with_imports():\n    from sympy import sin, cos\n    ast = Print([sin('x'), cos('x')], \"coordinate: %12.5g %12.5g\")\n    result = render_as_module(ast, standard='python3')\n    assert 'import sympy' in result or 'from sympy import sin, cos' in result\n    assert 'print(\"coordinate: %12.5g %12.5g\" % (sin(x), cos(x)))' in result\n", "def test_render_with_imports():\n    ast = Print('math.sin(math.pi/2)', \"The sine of pi/2 is %12.5g\")\n    expected_imports_py3 = \"import math\\n\\n\"\n    expected_imports_py2 = \"import math\\n\\n\"\n    expected_pystr = 'print(\"The sine of pi/2 is %12.5g\" % (math.sin(math.pi/2),))'\n    assert render_as_module(ast, standard='python3') == expected_imports_py3 + expected_pystr\n    assert render_as_module(ast, standard='python2') == expected_imports_py2 + expected_pystr\n"], "sample_1056": ["def test_numexpr_blacklisted():\n    ne = NumExprPrinter()\n    raises(TypeError, lambda: ne.doprint(Matrix([[1, 2], [3, 4]])))\n    raises(TypeError, lambda: ne.doprint([1, 2, 3]))\n    raises(TypeError, lambda: ne.doprint((1, 2, 3)))\n    raises(TypeError, lambda: ne.doprint({'a': 1, 'b': 2}))\n", "def test_numexpr_functions():\n    # Test printing of NumExpr supported functions\n    x, y, z = symbols('x,y,z')\n    n = NumExprPrinter()\n    assert n._print(sin(x)) == 'sin(x)'\n    assert n._print(cos(x)) == 'cos(x)'\n    assert n._print(tan(x)) == 'tan(x)'\n    assert n._print(x**y) == 'x**y'\n    assert n._print(sqrt(x)) == 'sqrt(x)'\n    assert n._print(abs(x)) == 'abs(x)'\n    assert n._print(x + y) == 'x + y'\n    assert n._print(x - y) == 'x - y'\n    assert n._print(x * y) == 'x * y'\n    assert n._print(x / y) == 'x / y'\n    assert n._print(x**2 + 2*x*y + y**2) == 'x**2 + 2*x*y + y**2'\n    assert n._print(1 / x) == '1 / x'\n    assert n._print(1 / (x + y)) == '1 / (x + y)'\n", "def test_lambda_printer_boolean():\n    assert LambdaPrinter()._print_And(x > 0, y > 0) == '((x > 0) and (y > 0))'\n    assert LambdaPrinter()._print_Or(x > 0, y > 0) == '((x > 0) or (y > 0))'\n    assert LambdaPrinter()._print_Not(x > 0) == '(not (x > 0))'\n    assert LambdaPrinter()._print_BooleanTrue(True) == 'True'\n    assert LambdaPrinter()._print_BooleanFalse(False) == 'False'\n", "def test_Boolean():\n    assert lambdarepr(True) == \"True\"\n    assert lambdarepr(False) == \"False\"\n    x = symbols('x')\n    assert lambdarepr(x < 1) in [\"x < 1\", \"1 > x\"]\n    assert lambdarepr(x <= 1) in [\"x <= 1\", \"1 >= x\"]\n    assert lambdarepr(x > 1) in [\"x > 1\", \"1 < x\"]\n    assert lambdarepr(x >= 1) in [\"x >= 1\", \"1 <= x\"]\n    assert lambdarepr(x == 1) in [\"x == 1\", \"1 == x\"]\n    assert lambdarepr(x != 1) in [\"x != 1\", \"1 != x\"]\n    assert lambdarepr(And(x < 1, x > 1)) == \"(x < 1) and (x > 1)\"\n    assert lambdarepr(Or(x < 1, x > 1)) == \"(x < 1) or (x > 1)\"\n    assert lambdarepr(Not(x < 1)) == \"not (x < 1)\"\n", "def test_numexpr_functions():\n    p = NumExprPrinter()\n    x_numexpr = p._print(sin(x))\n    assert x_numexpr == \"sin(x)\"\n    x_numexpr = p._print(sqrt(x))\n    assert x_numexpr == \"sqrt(x)\"\n    x_numexpr = p._print(x**y)\n    assert x_numexpr == \"x**y\"\n    x_numexpr = p._print(abs(x))\n    assert x_numexpr == \"abs(x)\"\n    x_numexpr = p._print(x + y)\n    assert x_numexpr == \"x + y\"\n    x_numexpr = p._print(x - y)\n    assert x_numexpr == \"x - y\"\n    x_numexpr = p._print(x * y)\n    assert x_numexpr == \"x * y\"\n    x_numexpr = p._print(x / y)\n    assert x_numexpr == \"x / y\"\n"], "sample_1058": ["def test_SciPyPrinter_print_assoc_legendre():\n    p = SciPyPrinter()\n    expr = Piecewise((1, Eq(x, 0)), (2, x>6))\n    assert p.doprint(expr) == 'numpy.select([(x == 0), (x > 6)], [1, 2], default=numpy.nan)'\n    from sympy.functions.special.polynomials import assoc_legendre\n    expr = assoc_legendre(1, 2, x)\n    assert p.doprint(expr) == 'scipy.special.lpmv(2, 1, x)'\n", "def test_PythonCodePrinter_print_Relational():\n    prntr = PythonCodePrinter()\n    assert prntr._print_Relational(Eq(x, y)) == '(x == y)'\n    assert prntr._print_Relational(Le(x, y)) == '(x <= y)'\n    assert prntr._print_Relational(Gt(x, y)) == '(x > y)'\n    assert prntr._print_Relational(Eq(x, y)) == '(x == y)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A.inv()) == 'sympy.Matrix(A).inv()'\n    assert p.doprint(A**3) == 'sympy.Matrix(A)**3'\n", "def test_SymPyPrinter():\n    s = SymPyPrinter()\n    assert s.doprint(pi) == 'sympy.pi'\n    expr = acos(x)\n    assert s.doprint(expr) == 'sympy.acos(x)'\n    assert s.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert s.doprint(sign(x)) == 'sympy.sign(x)'\n    assert s.doprint(And(x, y)) == 'sympy.And(x, y)'\n    assert s.doprint(Or(x, y)) == 'sympy.Or(x, y)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == \"sympy.MatrixPower(A, -1)\"\n    assert p.doprint(A**5) == \"sympy.MatrixPower(A, 5)\"\n"], "sample_1060": ["def test__print_Assignment():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(Assignment(x, None)) == 'x = None'\n    assert prntr.doprint(Assignment(x, zoo)) == 'x = float(\\'nan\\')'\n    assert prntr.doprint(Assignment(x, oo)) == 'x = float(\\'inf\\')'\n", "def test_PythonCodePrinter_print_Pow():\n    p = PythonCodePrinter()\n    assert p.doprint(x**0.5) == 'math.sqrt(x)'\n    assert p.doprint(x**-1) == '1/x'\n    assert p.doprint(x**2) == 'x**2'\n", "def test_NumPyPrinter_print_arg():\n    n = NumPyPrinter()\n    assert n.doprint(x.as_numer_denom()[0].as_real_imag()[1]) == 'numpy.angle(x)'\n    assert n.doprint(x.as_numer_denom()[0].as_real_imag()[0]) == 'numpy.real(x)'\n", "def test_PythonCodePrinter_ITE():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)), (2, x>6)).rewrite('ITE')) == '(2 if x > 6 else (1 if x == 0 else None))'\n    assert prntr.doprint(Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False).rewrite('ITE')) == '(3 if x > 0 else (2 if x <= 0 else None))'\n", "def test_PythonCodePrinter_print_Pow():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.doprint(x**-1) == '1/x'\n    assert prntr.doprint(x**0.5) == 'x**0.5'\n    assert prntr.doprint(x**(1/2)) == 'x**(1/2)'\n"], "sample_1062": ["def testCTR1():\n    eq = sin(x)**4 - cos(y)**2 + sin(y)**2 + 2*cos(x)**2\n    assert CTR1[0](eq) == TR0(TR5(eq))\n    assert CTR1[1](eq) == TR0(TR6(eq))\n    assert CTR1[2](eq) == eq\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(-tan(x)**2 + 1) == sec(x)**(-2)\n    assert TR22(-cot(x)**2 + 1) == csc(x)**(-2)\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n    assert TR22(1 + tan(x)**4) == sec(x)**4 - sec(x)**2 + 1\n    assert TR22(1 + cot(x)**4) == csc(x)**4 - csc(x)**2 + 1\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 - tan(x)**2) == 1 - tan(x)**2\n    assert TR22(1 - cot(x)**2) == 1 - cot(x)**2\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 + tan(x)**4) == (1 + tan(x)**2)**2\n    assert TR22(1 + cot(x)**4) == (1 + cot(x)**2)**2\n    assert TR22(tan(x)**2 + cot(x)**2 + 2) == sec(x)**2 + csc(x)**2\n"], "sample_1061": ["def test_integer_nthroot_negative():\n    assert integer_nthroot(27, -3) == (-3, True)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(-27, -3) == (3, True)\n    assert integer_nthroot(27, 4) == (1, False)\n    assert integer_nthroot(-27, 4) == (1, False)\n", "def test_integer_nthroot_repeated_square():\n    assert integer_nthroot(10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, 2)[0] == 100000000000000000000000000000000000000000000000000000000000000\n", "def test_cbrt():\n    assert cbrt(2) == Pow(2, Rational(1, 3), evaluate=False)\n    assert cbrt(27) == 3\n    assert cbrt(-27) == -3\n    assert cbrt(2.5) == Pow(2.5, Rational(1, 3)).evalf()\n    assert cbrt(2.5 + 3j) == Pow(2.5 + 3j, Rational(1, 3)).evalf()\n    assert cbrt(2.5 + 3j, 3) == Pow(2.5 + 3j, Rational(1, 3)).evalf(3)\n", "def test_integer_nthroot_negative():\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(-5, 4))\n    raises(ValueError, lambda: integer_nthroot(-12345, 10))\n", "def test_integer_log_issue_21006():\n    assert integer_log(16, 2) == (4, True)\n    assert integer_log(2**1024 + 1, 2) == (1024, False)\n    assert integer_log(2**1024, 2) == (1024, True)\n    assert integer_log(2**1024 - 1, 2) == (1023, False)\n"], "sample_1059": ["def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi_normalized(n, a, b, x) == jacobi(n, a, b, x)/sqrt(2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)/((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)))\n    assert jacobi_normalized(0, a, b, x) == 1/sqrt(2**(a + b + 1)*gamma(a + 1)*gamma(b + 1)/((a + b + 1)*gamma(a + b + 1)))\n    assert jacobi_normalized(1, a, b, x) == (a/2 - b/2 + x*(a/2 + b/2 + 1))/sqrt((2**(a + b + 1)*gamma(a + 2)*gamma(b + 2))/((a + b + 3)*gamma(a + b + 2)))\n    assert jacobi_normalized(n, 0, 0, x) == legendre(n, x)\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi_normalized(n, a, b, x) == jacobi(n, a, b, x)/sqrt(2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)/((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)))\n\n    # check normalization\n    m = Symbol(\"m\", integer=True)\n    assert integrate(jacobi_normalized(n, a, b, x)*jacobi_normalized(m, a, b, x)*(1-x)**a*(1+x)**b, (x, -1, 1)) == KroneckerDelta(n, m)\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi_normalized(0, a, b, x) == 1/sqrt(gamma(a + 1)*gamma(b + 1)/(gamma(a + b + 1)))\n\n    assert jacobi_normalized(1, a, b, x) == (a/2 - b/2 + x*(a/2 + b/2 + 1))/(\n        sqrt(2**(a + b + 1)*gamma(a + 2)*gamma(b + 2)/((a + b + 3)*gamma(a + b + 2))))\n\n    nfactor = (S(2)**(a + b + 1) * (gamma(n + a + 1) * gamma(n + b + 1))\n               / (2*n + a + b + 1) / (factorial(n) * gamma(n + a + b + 1)))\n\n    assert jacobi_normalized(n, a, b, x) == jacobi(n, a, b, x) / sqrt(nfactor)\n\n    X = jacobi_normalized(n, a, b, x)\n    assert isinstance(X, jacobi)\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi_normalized(0, a, b, x) == 1/sqrt(2**(a + b + 1)*gamma(a + 1)*gamma(b + 1)/(gamma(a + b + 1)))\n    assert jacobi_normalized(1, a, b, x) == (a/2 - b/2 + x*(a/2 + b/2 + 1))/sqrt((a + b + 3)*(a + b + 2)*2**(a + b + 1)*gamma(a + 1)*gamma(b + 1)/(gamma(a + b + 2)))\n\n    assert jacobi_normalized(n, a, a, x) == jacobi(n, a, a, x)/sqrt(2**(2*a + 1)*gamma(a + n + 1)**2/((2*a + 2*n + 1)*factorial(n)*gamma(2*a + n + 1)))\n    assert jacobi_normalized(n, a, b, 0) == 2**(-n)*gamma(a + n + 1)*hyper((-b - n, -n), (a + 1,), -1)/(sqrt((2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)/((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)))*factorial(n)*gamma(a + 1))\n    assert jacobi_normalized(n, a, b, 1) == RisingFactorial(a + 1, n)/(sqrt((2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)/((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)))*factorial(n))\n\n    assert jacobi_normalized(n, 1, 1, x) == jacobi(n, 1, 1, x)/sqrt(((n + 2)*(n + 1)*2**(n + 2)*factorial(n + 1)**2)/((2*n + 3)*factorial(n)**2*(n + 1)**2))\n\n    assert diff(jacobi_normalized(n, a,", "def test_hermite_conjugate():\n    n = Symbol(\"n\")\n    assert conjugate(hermite(n, x)) == hermite(n, conjugate(x))\n    assert conjugate(hermite(2, x)) == hermite(2, conjugate(x))\n    assert conjugate(hermite(n, 1)) == hermite(n, 1)\n    assert conjugate(hermite(2, 1)) == hermite(2, 1)\n"], "sample_1063": ["def test_lambdify_function_type():\n    f = lambdify(x, sin(x))\n    assert type(f) == FunctionType\n", "def test_lambdify_use_imps():\n    # Test lambdify with use_imps=True\n    f = implemented_function('f', lambda x: x+1)\n    assert lambdify(x, f(x), use_imps=True)(2) == 3\n    assert lambdify(x, f(x), use_imps=False)(2) == f(2)\n", "def test_lambdify_with_module_dict():\n    # Test that lambdify can handle a dictionary with modules as values.\n    # See: https://github.com/sympy/sympy/issues/17838\n    d = {'sin': math.sin, 'cos': math.cos}\n    f = lambdify(x, sin(x) + cos(x), d)\n    assert f(0) == 1\n", "def test_lambdify_lambda():\n    lam = Lambda(x, x**2)\n    f = lambdify(x, lam(x))\n    assert f(3) == 9\n    f = lambdify(x, lam)\n    assert f(3) == 9\n", "def test_lambdastr_nested_args():\n    x, y, z = symbols('x y z')\n    f = lambdastr((x, (y, z)), x + y)\n    assert f == 'lambda _0,_1: (lambda x,y,z: (x + y))(_0,_1[0],_1[1])'\n"], "sample_1064": ["def test_tensorflow_Piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    x = Symbol('x')\n    expr = Piecewise((x, x > 0), (x**2, True))\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.where(x > 0, x, tensorflow.math.pow(x, 2))\"\n\n    expr = Piecewise((x, x > 0), (x**2, x < 0), (sin(x), True))\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.where(x > 0, x, tensorflow.where(x < 0, tensorflow.math.pow(x, 2), tensorflow.math.sin(x)))\"\n\n    f = lambdify(x, expr, 'tensorflow')\n    graph = tf.Graph()\n    with graph.as_default():\n        session = tf.compat.v1.Session(graph=graph)\n        y = session.run(f(1))\n        assert y == 1\n\n        y = session.run(f(-1))\n        assert y == 1\n\n        y = session.run(f(0))\n        assert y == 0.0\n", "def test_tensorflow_Pow():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = x ** 0.5\n    assert tensorflow_code(expr) == \"tensorflow.math.sqrt(x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = x ** 3\n    assert tensorflow_code(expr) == \"tensorflow.math.pow(x, 3)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n", "def test_tensorflow_piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert tensorflow_code(expr) == \\\n        'tensorflow.where(x < 1, x, x**2)'\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = Piecewise((x**2, x < 1), (x, True))\n    assert tensorflow_code(expr) == \\\n        'tensorflow.where(x < 1, x**2, x)'\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = Piecewise((x, x < 1), (x**2, x < 2), (x**3, True))\n    assert tensorflow_code(expr) == \\\n        'tensorflow.where(x < 1, x, tensorflow.where(x < 2, x**2, x**3))'\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = Piecewise((x**2, x < 1), (x, x < 2), (x**3, True))\n    assert tensorflow_code(expr) == \\\n        'tensorflow.where(x < 1, x**2, tensorflow.where(x < 2, x, x**3))'\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n", "def test_tensorflow_Max():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Max(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.maximum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.randint(0, 100))\n\n    expr = Max(x, y, z)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.math.maximum(\" \\\n            \"tensorflow.math.maximum(x, y), z)\"\n    _compare_tensorflow_scalar((x, y, z), expr, rng=lambda: random.randint(0, 100))\n\n", "def test_tensorflow_Piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Piecewise((x, x < 0), (x**2, True))\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.where(x < 0, x, tensorflow.math.pow(x, 2))\"\n\n    expr = Piecewise((x, x < 0), (x**2, x < 1), (x**3, True))\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.where(x < 0, x, tensorflow.where(x < 1, tensorflow.math.pow(x, 2), tensorflow.math.pow(x, 3)))\"\n\n    f = lambdify((x,), expr, 'tensorflow')\n    graph = tf.Graph()\n    with graph.as_default():\n        session = tf.compat.v1.Session(graph=graph)\n        x_val = tf.constant(-.5)\n        y = session.run(f(x_val))\n        assert y == session.run(tf.where(x_val < 0, x_val, tf.where(x_val < 1, x_val**2, x_val**3)))\n\n    expr = Max(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.maximum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr)\n\n    expr = Min(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.minimum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr)\n"], "sample_1065": ["def test_factorial2_diff():\n    n = Symbol('n', integer=True)\n    assert factorial2(n).diff(n) == \\\n        2**(n/2)*gamma(n/2 + 1)*(polygamma(0, n/2 + 1)/2 + n*polygamma(0, n/2 + 1)/2 - n*polygamma(0, n/2 + S(3)/2)/2 + polygamma(0, n/2 + S(3)/2)/2)\n    raises(ArgumentIndexError, lambda: factorial2(n).fdiff(2))\n", "def test_factorial2_diff():\n    n = Symbol('n', integer=True)\n\n    assert factorial2(n).diff(n) == \\\n        2**((n - 1)/2)*factorial(n/2)*(\n            polygamma(0, 1 + n/2)/2 + log(2)/2 + polygamma(0, n/2 + 1))\n    assert factorial2(n**2).diff(n) == \\\n        n*(polygamma(0, n**2/2 + 1)/2 + log(2)/2 + polygamma(0, n**2/2 + 1/2))*2**(n**2/2)*factorial(n**2/2)\n\n    raises(ArgumentIndexError, lambda: factorial2(n**2).fdiff(2))\n", "def test_factorial_simplify():\n    x = Symbol('x')\n    assert simplify(factorial(x + 1)/x - factorial(x)) == 0\n    assert simplify(factorial(x)/(x - 1) - factorial(x - 1)) == 0\n", "def test_factorial2_diff():\n    n = Symbol('n', integer=True)\n\n    assert factorial2(n).diff(n) == \\\n        2**((n - 1)/2)*gamma(n/2 + 1)*(polygamma(0, n/2 + 1)/2 - log(2))\n    assert factorial2(2*n).diff(n) == \\\n        2**n*gamma(n + 1)*(polygamma(0, n + 1) - log(2))\n    raises(ArgumentIndexError, lambda: factorial2(n**2).fdiff(2))\n", "def test_factorial2_eval_special_cases():\n    assert factorial2(0) == 1\n    assert factorial2(1) == 1\n    assert factorial2(-1) == 1\n    assert factorial2(-3) == 1\n    assert factorial2(-5) == 1\n    assert factorial2(2) == 2\n    assert factorial2(4) == 8\n    assert factorial2(6) == 48\n    assert factorial2(3) == 3\n    assert factorial2(5) == 15\n    assert factorial2(7) == 105\n"], "sample_1067": ["def test_match_floating_point_numbers():\n    x = Symbol('x')\n    a = Wild('a')\n    assert (1.1*x).match(a*x) == {a: 1.1}\n    assert (1.1*x).match(a) == {a: 1.1*x}\n", "def test_issue_unevaluated_Mul():\n    from sympy import sqrt\n    from sympy.abc import x\n    assert _unevaluated_Mul(3, 2, x) == _unevaluated_Mul(6, x)\n    assert _unevaluated_Mul(3, 2, 3) == 18\n    assert _unevaluated_Mul(S(3.0), x, S(2)) == _unevaluated_Mul(S(6.0), x)\n    assert _unevaluated_Mul(sqrt(3), sqrt(2)) == _unevaluated_Mul(sqrt(3), sqrt(2))\n    assert _unevaluated_Mul(sqrt(2), sqrt(3)) == _unevaluated_Mul(sqrt(3), sqrt(2))\n    assert _unevaluated_Mul(sqrt(3), sqrt(3)) == 3\n    assert _unevaluated_Mul(sqrt(2), sqrt(2)) == 2\n", "def test_unevaluated_Mul():\n    from sympy import S, sqrt, Mul\n    from sympy.abc import x\n\n    a = Mul(3.0, x, 2, evaluate=False)\n    assert a.args[0] == 6.0\n    assert a.args[1] == x\n\n    b = Mul(sqrt(2), sqrt(3), evaluate=False)\n    c = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert b == c\n\n    d = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert Mul(d) == b\n    assert Mul(d) != Mul(*d.args)\n", "def test_issue_7455():\n    x, y = symbols('x y')\n    p, q = symbols('p q', cls=Wild, exclude=(x, y))\n    f = Function('f')\n\n    assert (f(x) + f(y)).match(f(p) + f(q)) in [{p: x, q: y}, {p: y, q: x}]\n    assert (f(x) + f(2)).match(f(p) + f(q)) in [{p: x, q: 2}, {p: 2, q: x}]\n", "def test_issue_7351():\n    x = Symbol('x')\n    a = Wild('a')\n    b = Wild('b')\n    expr = -2*x**2 + 2*x + 3\n    pattern = a*x**2 + b*x + 3\n    assert expr.match(pattern) == {a: -2, b: 2}\n"], "sample_1066": ["def test_print_matrix_element():\n    X = MatrixSymbol('X', 2, 2)\n    assert mathml(X[0,0], printer='presentation') == \\\n        '<msub><mfenced close=\"\" open=\"\"><mi>X</mi></mfenced>'\\\n        '<mfenced close=\"\" open=\"\"><mn>0</mn><mn>0</mn></mfenced></msub>'\n    assert mathml(X[0,1], printer='presentation') == \\\n        '<msub><mfenced close=\"\" open=\"\"><mi>X</mi></mfenced>'\\\n        '<mfenced close=\"\" open=\"\"><mn>0</mn><mn>1</mn></mfenced></msub>'\n    assert mathml(X[1,0], printer='presentation') == \\\n        '<msub><mfenced close=\"\" open=\"\"><mi>X</mi></mfenced>'\\\n        '<mfenced close=\"\" open=\"\"><mn>1</mn><mn>0</mn></mfenced></msub>'\n    assert mathml(X[1,1], printer='presentation') == \\\n        '<msub><mfenced close=\"\" open=\"\"><mi>X</mi></mfenced>'\\\n        '<mfenced close=\"\" open=\"\"><mn>1</mn><mn>1</mn></mfenced></msub>'\n    assert mathml(X[0, x], printer='presentation') == \\\n        '<msub><mfenced close=\"\" open=\"\"><mi>X</mi></mfenced>'\\\n        '<mfenced close=\"\" open=\"\"><mn>0</mn><mi>x</mi></mfenced></msub>'\n", "def test_print_Interval():\n    assert mathml(Interval(0, 1), printer='content') == \\\n        '<apply><interval closure=\"closed\"><cn>0</cn><cn>1</cn></interval></apply>'\n    assert mathml(Interval(0, 1, left_open=True), printer='content') == \\\n        '<apply><interval closure=\"open-closed\"><cn>0</cn><cn>1</cn></interval></apply>'\n    assert mathml(Interval(0, 1, right_open=True), printer='content') == \\\n        '<apply><interval closure=\"closed-open\"><cn>0</cn><cn>1</cn></interval></apply>'\n    assert mathml(Interval(0, 1, left_open=True, right_open=True), printer='content') == \\\n        '<apply><interval closure=\"open\"><cn>0</cn><cn>1</cn></interval></apply>'\n", "def test_mathml_hbar():\n    assert mathml(hbar, printer='presentation') == '<mi>&#x210F;</mi>'\n    assert mathml(hbar, printer='content') == '<hbar/>'\n", "def test_print_MatMul():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert mathml(X*Y, printer='presentation') == \\\n        '<mrow><mi>X</mi><mo>&InvisibleTimes;</mo><mi>Y</mi></mrow>'\n    assert mathml(-X*Y, printer='presentation') == \\\n        '<mrow><mo>-</mo><mrow><mi>X</mi><mo>&InvisibleTimes;</mo><mi>Y</mi></mrow></mrow>'\n    assert mathml(X + X*Y, printer='presentation') == \\\n        '<mrow><mi>X</mi><mo>+</mo><mrow><mi>X</mi><mo>&InvisibleTimes;</mo><mi>Y</mi></mrow></mrow>'\n", "def test_print_MatMul():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert mathml(A * B, printer='presentation') == \\\n        '<mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi></mrow>'\n    assert mathml(-(A * B), printer='presentation') == \\\n        '<mrow><mo>-</mo><mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi></mrow></mrow>'\n    assert mathml(-A * B, printer='presentation') == \\\n        '<mrow><mo>-</mo><mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi></mrow></mrow>'\n    assert mathml(A * -B, printer='presentation') == \\\n        '<mrow><mo>-</mo><mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi></mrow></mrow>'\n    assert mathml(A * B * A, printer='presentation') == \\\n        '<mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi><mo>&InvisibleTimes;</mo><mi>A</mi></mrow>'\n"], "sample_1068": ["def test_automatic_user_function_rewrite():\n    Li = Function('Li')\n    logint = Function('logint')\n    user_functions = {\n        Li: \"logint(x) - logint(2)\",\n        logint: \"logint\"\n    }\n\n    assert octave_code(Li(x), user_functions=user_functions) == 'logint(x) - logint(2)'\n    assert octave_code(logint(x), user_functions=user_functions) == 'logint(x)'\n", "def test_octave_matrix_solve():\n    n = Symbol('n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    x = MatrixSymbol('x', n, 1)\n    assert mcode(A*x) == 'A*x'\n    assert mcode(A*B) == 'A*B'\n    assert mcode(B*A) == 'B*A'\n    assert mcode(2*A*B) == '2*A*B'\n    assert mcode(A*(B + 3*Identity(n))) == 'A*(3*eye(n) + B)'\n    assert mcode(A**-1) == 'A^(-1)'\n    assert mcode(A**3) == 'A^3'\n    assert mcode(A**0.5) == 'A^(1/2)'\n    assert mcode(A**-2) == 'A^(-2)'\n    assert mcode(A**-0.5) == 'A^(-1/2)'\n    assert mcode(A**-1*x) == 'A^(-1)*x'\n    assert mcode(A**-1*B) == 'A^(-1)*B'\n    assert mcode(B*A**-1) == 'B*A^(-1)'\n", "def test_log():\n    assert octave_code(log(x)) == 'log(x)'\n    assert octave_code(log(x, 10)) == 'log10(x)'\n    assert octave_code(log(x, 2)) == 'log2(x)'\n    assert octave_code(log(x, y)) == 'log(x)/log(y)'\n", "def test_Indexed_printing():\n    # test cases for Indexed printing\n    from sympy.tensor import IndexedBase, Idx\n    A = IndexedBase(\"A\")\n    i = Idx(\"i\", 3)\n    j = Idx(\"j\", 3)\n\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(A[i, 1]) == \"A(i, 2)\"\n    assert mcode(3 * A[i, j]) == \"3*A(i, j)\"\n", "compilation error"], "sample_1070": ["def test_exp_polar_subs():\n    x, y = symbols('x y', polar=True)\n    assert exp_polar(x).subs(x, y) == exp_polar(y)\n    assert exp_polar(x).subs(x, 0) == 1\n    assert exp_polar(x).subs(x, I*pi) == -1\n", "def test_log_refine():\n    from sympy import refine, Q\n    x = symbols('x', real=True)\n    assert refine(log(x), Q.is_positive(x)) == log(x)\n    assert refine(log(x), Q.is_negative(x)) == log(-x) + I*pi\n", "def test_issue_19649():\n    x, y = symbols('x y')\n    assert log(abs(x)).rewrite(sign) == log(sign(x)*x)\n    assert log(abs(y)).rewrite(sign) == log(sign(y)*y)\n", "def test_issue_exp_polar_subs():\n    from sympy import exp_polar, I, pi\n    x = symbols('x')\n    assert (exp_polar(2*pi*I*x)).subs(x, 1) == exp_polar(2*pi*I)\n    assert (exp_polar(2*pi*I*x)).subs(x, 2) == exp_polar(4*pi*I)\n", "def test_refine():\n    x = Symbol('x', real=True)\n    assert refine(exp(pi*I*x), Q.even(x)) == 1\n    assert refine(exp(pi*I*x), Q.odd(x)) == -1\n    assert refine(log(exp(pi*I*x)), Q.even(x)) == 0\n    assert refine(log(exp(pi*I*x)), Q.odd(x)) == I*pi\n"], "sample_1069": ["def test_glsl_code():\n    A = Matrix([[1, sin(2/x), 3*pi/x/5]])\n    assert glsl_code(A) == \"vec3(1, sin(2.0/x), 3*3.141592653589793/x/5)\"\n    assert glsl_code(A, assign_to='a') == \"vec3 a = vec3(1, sin(2.0/x), 3*3.141592653589793/x/5);\"\n    assert glsl_code(A.T) == \"float[3](1, sin(2.0/x), 3*3.141592653589793/x/5)\"\n    assert glsl_code(A.T, assign_to='a') == \"float[3] a = float[3](1, sin(2.0/x), 3*3.141592653589793/x/5);\"\n", "def test_MatrixElement_printing_indexed():\n    # test cases for MatrixElement printing with Indexed\n    A = MatrixSymbol(\"A\", 1, 3)\n    i = symbols('i', integer=True)\n    assert mcode(A[0, i]) == \"A(1, i + 1)\"\n    assert mcode(A[i, 0]) == \"A(i + 1, 1)\"\n", "def test_GLSLPrinter_printmethod():\n    p = GLSLPrinter()\n    assert p.printmethod == \"_glsl\"\n", "def test_glsl_code():\n    # Test GLSL printer with different inputs\n    assert glsl_code(sin(x)*y) == 'y*sin(x)'\n    assert glsl_code(sin(x)*y, assign_to='result') == 'float result = y*sin(x);'\n    assert glsl_code(sin(x)*y, use_operators=False) == 'mul(y, sin(x))'\n", "def test_glsl_code_Lambda():\n    f = Lambda(x, x**2)\n    assert glsl_code(f) == \"x^2\"\n    assert glsl_code(f, assign_to='y') == \"y = x^2;\"\n"], "sample_1071": ["def test_check_dimensions_with_prefixes():\n    x = symbols('x')\n    assert check_dimensions(kilo*inch + x) == kilo*inch + x\n    assert check_dimensions(mega*length + x) == mega*length + x\n    raises(ValueError, lambda: check_dimensions(kilo*inch + 1))\n    raises(ValueError, lambda: check_dimensions(mega*length + 1))\n    raises(ValueError, lambda: check_dimensions(kilo*length + mega*time))\n    raises(ValueError, lambda: check_dimensions(kilo*meter + mega*second))\n    raises(ValueError, lambda: check_dimensions(2*kilo*meter + mega*second))\n    raises(ValueError, lambda: check_dimensions(2*kilo*meter + 3*mega*second))\n", "def test_check_dimensions_with_sums_of_units():\n    assert check_dimensions(meter + kilometer) == meter + kilometer\n    assert check_dimensions(meter - kilometer) == meter - kilometer\n    assert check_dimensions(meter + centimeter) == meter + centimeter\n    assert check_dimensions(2 * meter + 3 * kilometer) == 2 * meter + 3 * kilometer\n    assert check_dimensions(meter + kilometer * (1 + 1)) == meter + kilometer * 2\n    raises(ValueError, lambda: check_dimensions(meter + second + kilometer))\n    raises(ValueError, lambda: check_dimensions(meter + kilometer + centimeter + second))\n", "def test_convert_to_with_units_of_same_dimension():\n    assert convert_to(mile + kilometer, meter) == 326168*meter/125\n    assert convert_to(kilometer + mile, meter) == 326168*meter/125\n    assert convert_to(2*kilometer + 3*mile, meter) == 853504*meter/125\n    assert convert_to(3*mile + 2*kilometer, meter) == 853504*meter/125\n", "def test_convert_to_with_complex_exponents():\n    assert convert_to(mile**2, kilometer**2) == 64516*kilometer**2/15625\n    assert convert_to(mile**-2, kilometer**-2) == 15625/(64516*kilometer**2)\n    assert convert_to(mile**0.5, kilometer**0.5) == kilometer**0.5*15811386305**(1/2)/78125\n    assert convert_to(mile**-0.5, kilometer**-0.5) == 78125/(kilometer**0.5*15811386305**(1/2))\n", "def test_convert_to_with_powers():\n    assert convert_to(meter**2, kilometer**2) == kilometer**2/1000000\n    assert convert_to(meter**2, centimeter**2) == 10000*centimeter**2\n    assert convert_to(newton, kilogram*meter/second**2) == kilogram*meter/second**2\n    assert convert_to(newton, gram*centimeter/second**2) == 100000*gram*centimeter/second**2\n    assert convert_to(joule, kilogram*meter**2/second**2) == kilogram*meter**2/second**2\n    assert convert_to(joule, gram*centimeter**2/second**2) == 10000000*gram*centimeter**2/second**2\n"], "sample_1073": ["def test_sqrtdenest_symbolic_depth():\n    x = Symbol('x', positive=True)\n    w = 2 + sqrt(x + 3) + sqrt(x + 2)\n    z = sqrt((w**2).expand())\n    assert sqrtdenest(z) == w\n    w = 2 + sqrt(x + 3) + sqrt(x + 2) + sqrt(x + 1)\n    z = sqrt((w**2).expand())\n    assert sqrtdenest(z) == w\n    w = sqrt(x + 2) + sqrt(x + 3)\n    z = sqrt((w**2).expand())\n    assert sqrtdenest(z) == w\n    w = sqrt(x + 1) + sqrt(x + 2)\n    z = sqrt((w**2).expand())\n    assert sqrtdenest(z) == w\n", "def test_sqrt_depth():\n    assert sqrtdenest.sqrt_depth(1 + r2*(1 + r3)) == 1\n    assert sqrtdenest.sqrt_depth(1 + r2*sqrt(1 + r3)) == 2\n    assert sqrtdenest.sqrt_depth(r2) == 0\n    assert sqrtdenest.sqrt_depth(1 + sqrt(2 + sqrt(2))) == 2\n    assert sqrtdenest.sqrt_depth(sqrt(2 + sqrt(2 + sqrt(2)))) == 3\n", "def test_sqrt_biquadratic_denest():\n    from sympy.abc import a\n    w = 2 + sqrt(2) + sqrt(3) + (1 + sqrt(3))*sqrt(2 + sqrt(2) + 5*sqrt(3))\n    z = sqrt((w**2).expand())\n    assert sqrtdenest(z) == w.expand()\n\n    w = sqrt(sqrt(2 + a) + 1) + 1 + sqrt(2)\n    z = sqrt((w**2).expand())\n    assert sqrtdenest(z) == w\n", "def test_sqrt_match():\n    z = sqrt(1 + sqrt(2) + sqrt(2)*sqrt(3) +  2*sqrt(1+sqrt(5)))\n    a, b, r = _sqrt_match(z**2)\n    assert a == 1 + sqrt(2) + sqrt(6)\n    assert b == 2\n    assert r == 1 + sqrt(5)\n    z = sqrt(16 - 2*sqrt(29) + 2*sqrt(55 - 10*sqrt(29)))\n    a, b, r = _sqrt_match(z**2)\n    assert a == 16 - 2*sqrt(29)\n    assert b == 2\n    assert r == 55 - 10*sqrt(29)\n    assert _sqrt_match(sqrt(4 + 3*S.ImaginaryUnit)) == []\n", "def test_sqrtdenest_depth():\n    z = sqrt(2 + sqrt(2 + sqrt(2 + sqrt(2))))\n    assert sqrtdenest(z) == z\n    z = sqrt(2 + sqrt(2 + sqrt(2 + sqrt(3))))\n    assert sqrtdenest(z) == z\n    assert sqrtdenest(sqrt(sqrt(2 + r2) + 2)) == sqrt(sqrt(2 + r2) + 2)\n    assert sqrtdenest(sqrt(2*r6/7 + 2*r7/7 + 2*sqrt(42)/7 + 2)) == \\\n        sqrt(2*r6/7 + 2*r7/7 + 2*sqrt(42)/7 + 2)\n"], "sample_1072": ["def test_frac_nonzero():\n    x = Symbol('x', real=True)\n    assert frac(x).is_nonzero is None\n    assert frac(0).is_nonzero == False\n    assert frac(1).is_nonzero == False\n    assert frac(2.5).is_nonzero == True\n    assert frac(-2.5).is_nonzero == True\n", "def test_floor_ceiling_frac_rewrites():\n    x = symbols('x')\n    assert floor(x).rewrite(frac) == x - frac(x)\n    assert ceiling(x).rewrite(frac) == x + frac(-x)\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n    assert floor(x).rewrite(ceiling) == -ceiling(-x)\n    assert ceiling(x).rewrite(floor) == -floor(-x)\n\n    # Test rewrite with I\n    assert floor(I*x).rewrite(frac) == I*x - frac(I*x)\n    assert ceiling(I*x).rewrite(frac) == I*x + frac(-I*x)\n    assert frac(I*x).rewrite(floor) == I*x - floor(I*x)\n    assert frac(I*x).rewrite(ceiling) == I*x + ceiling(-I*x)\n\n    # Test rewrite with non-integer numbers\n    assert floor(2.5).rewrite(frac) == 2.5 - frac(2.5)\n    assert ceiling(2.5).rewrite(frac) == 2.5 + frac(-2.5)\n    assert frac(2.5).rewrite(floor) == 2.5 - floor(2.5)\n    assert frac(2.5).rewrite(ceiling) == 2.5 + ceiling(-2.5)\n", "def test_frac_is_imaginary():\n    assert frac(2*I).is_imaginary\n    assert frac(2*I).is_real is False\n    assert frac(I*y).is_imaginary is None\n    assert frac(I*y).is_real is False\n    assert frac(x + 2*I).is_imaginary is None\n    assert frac(x + 2*I).is_real is False\n    assert frac(x + I*y).is_imaginary is None\n    assert frac(x + I*y).is_real is None\n", "def test_floor_ceiling_with_non_real_numbers():\n    x = Symbol('x', real=False)\n    assert floor(x).is_integer is None\n    assert ceiling(x).is_integer is None\n    assert floor(x).is_real is None\n    assert ceiling(x).is_real is None\n    assert floor(x).is_finite is None\n    assert ceiling(x).is_finite is None\n    assert floor(x).is_zero is None\n    assert ceiling(x).is_zero is None\n\n    assert floor(pi + I).is_integer is None\n    assert ceiling(pi + I).is_integer is None\n    assert floor(pi + I).is_real is False\n    assert ceiling(pi + I).is_real is False\n    assert floor(pi + I).is_finite\n    assert ceiling(pi + I).is_finite\n    assert floor(pi + I).is_zero is False\n    assert ceiling(pi + I).is_zero is False\n\n    assert floor(I).is_integer\n    assert ceiling(I).is_integer\n    assert floor(I).is_real is False\n    assert ceiling(I).is_real is False\n    assert floor(I).is_finite\n    assert ceiling(I).is_finite\n    assert floor(I).is_zero is False\n    assert ceiling(I).is_zero is False\n\n    i = Symbol('i', imaginary=True)\n    assert floor(i).is_integer\n    assert ceiling(i).is_integer\n    assert floor(i).is_real is False\n    assert ceiling(i).is_real is False\n    assert floor(i).is_finite\n    assert ceiling(i).is_finite\n    assert floor(i).is_zero is False\n    assert ceiling(i).is_zero is False\n", "def test_floor_ceiling_frac_on_Rational():\n    assert floor(Rational(5, 2)) == 2\n    assert ceiling(Rational(5, 2)) == 3\n    assert frac(Rational(5, 2)) == Rational(1, 2)\n    assert floor(Rational(-5, 2)) == -3\n    assert ceiling(Rational(-5, 2)) == -2\n    assert frac(Rational(-5, 2)) == Rational(1, 2)\n\n    assert floor(Rational(3, 4)) == 0\n    assert ceiling(Rational(3, 4)) == 1\n    assert frac(Rational(3, 4)) == Rational(3, 4)\n    assert floor(Rational(-3, 4)) == -1\n    assert ceiling(Rational(-3, 4)) == 0\n    assert frac(Rational(-3, 4)) == Rational(1, 4)\n"], "sample_1075": ["def test_beta_eval_is_real():\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n    assert beta(x, y)._eval_is_real() is True\n\n    x, y = Symbol('x'), Symbol('y')\n    assert beta(x, y)._eval_is_real() is None\n\n    x, y = Symbol('x', real=False), Symbol('y', real=True)\n    assert beta(x, y)._eval_is_real() is False\n\n    x, y = Symbol('x', real=True), Symbol('y', real=False)\n    assert beta(x, y)._eval_is_real() is False\n", "def test_beta_eval_is_real():\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n    z, w = Symbol('z'), Symbol('w')\n\n    assert beta(x, y)._eval_is_real() is True\n    assert beta(z, w)._eval_is_real() is None\n", "def test_beta_eval_is_real():\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n    assert beta(x, y)._eval_is_real() == True\n    z = Symbol('z')\n    assert beta(z, y)._eval_is_real() == y.is_real\n    assert beta(x, z)._eval_is_real() == x.is_real\n", "def test_beta_is_real():\n    x, y = Symbol('x'), Symbol('y')\n\n    assert beta(x, y).is_real == None\n    assert beta(1, 2).is_real == True\n    assert beta(1 + 1j, 2).is_real == False\n    assert beta(1, 2 + 1j).is_real == False\n", "def test_beta_eval_is_real():\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n    assert beta(x, y).is_real\n\n    x, y = Symbol('x', real=False), Symbol('y', real=True)\n    assert not beta(x, y).is_real\n\n    x, y = Symbol('x', real=True), Symbol('y', real=False)\n    assert not beta(x, y).is_real\n\n    x, y = Symbol('x', real=False), Symbol('y', real=False)\n    assert not beta(x, y).is_real\n"], "sample_1074": ["def test_normal_closure2():\n    a = Permutation(0, 1, 2, 3, 4)\n    b = Permutation(0, 4)\n    G = PermutationGroup([a, b])\n    N = G.normal_closure([a**2])\n    assert N.order() == 10\n    assert N.is_subgroup(G)\n", "def test_is_transitive():\n    G = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2, 3))\n    assert G.is_transitive()\n    assert G.is_transitive(strict=False)\n    H = PermutationGroup(Permutation(0, 1), Permutation(3))\n    assert not H.is_transitive()\n    assert H.is_transitive(strict=False)\n", "def test_polycyclic_group():\n    a = Permutation([0, 1, 2])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    assert G.polycyclic_group()\n", "def test_homomorphism():\n    a = Permutation([1, 2, 0, 4, 5, 3])\n    b = Permutation([1, 0, 2, 4, 3, 5])\n    G = PermutationGroup([a, b])\n    gens = G.generators\n    phi = homomorphism(G, G, gens, [g**2 for g in gens])\n    ker = phi.kernel()\n    assert ker.order() == 2\n    H = phi.image()\n    assert H.order() == G.order() // ker.order()\n    assert is_isomorphic(H, G.quotient(ker))\n", "def test_polycyclic_group():\n    a = Permutation([0, 1, 2])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    P = G.polycyclic_group()\n    assert P.series == G.composition_series()\n    assert P.gens == G.strong_gens\n    assert P.relative_orders == [x.order() for x in G.strong_gens]\n"], "sample_1076": ["def test_MatMul():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    prntr = NumPyPrinter()\n    assert prntr.doprint(A@B) == '(A).dot(B)'\n", "def test_printing_pretty_default():\n    from sympy import sin, cos\n    from sympy.abc import x, y\n    expr = sin(x) + cos(y)**2\n\n    prntr = PythonCodePrinter({'standard' : 'python3'})\n    assert prntr.doprint(expr) == 'math.sin(x) + math.cos(y)**2'\n\n    prntr = NumPyPrinter({'standard' : 'python3'})\n    assert prntr.doprint(expr) == 'numpy.sin(x) + numpy.cos(y)**2'\n\n    prntr = SciPyPrinter({'standard' : 'python3'})\n    assert prntr.doprint(expr) == 'numpy.sin(x) + numpy.cos(y)**2'\n\n    prntr = MpmathPrinter({'standard' : 'python3'})\n    assert prntr.doprint(expr) == 'mpmath.sin(x) + mpmath.cos(y)**2'\n\n    prntr = SymPyPrinter({'standard' : 'python3'})\n    assert prntr.doprint(expr) == 'sympy.sin(x) + sympy.cos(y)**2'\n", "def test_issue_BlockMatrix():\n    from sympy import BlockMatrix, ones\n    M = BlockMatrix([[ones(2, 2), ones(2, 1)]])\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(M) == 'numpy.block([[numpy.ones((2, 2)), numpy.ones((2, 1))]])'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(M) == 'numpy.block([[numpy.ones((2, 2)), numpy.ones((2, 1))]])'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(M) == '[[(1, 1, 1, 1), (1, 1, 1, 1)]]'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(M) == '[[[mpmath.mpf(1), mpmath.mpf(1), mpmath.mpf(1), mpmath.mpf(1)], [mpmath.mpf(1), mpmath.mpf(1), mpmath.mpf(1), mpmath.mpf(1)]]]'\n", "def test_pycode_Assignment():\n    v = symbols('v')\n    expr = v**2\n    assignment = Assignment(v, expr)\n    assert pycode(assignment) == 'v = v**2'\n", "compilation error"], "sample_1077": ["def test_ComplexRegion_plot():\n    # Test plotting of rectangular ComplexRegion\n    region = ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n    assert region._complex_to_real() == Interval(-1, 1)*Interval(-1, 1)\n\n    # Test plotting of polar ComplexRegion\n    region = ComplexRegion(Interval(0, 2)*Interval(0, pi), polar=True)\n    assert region._complex_to_real() == Interval(-2, 2)*Interval(-2, 2)\n", "def test_ComplexRegion_from_Interval():\n    from sympy import I\n    c1 = ComplexRegion.from_real(Interval(1, 3))\n    assert c1 == ComplexRegion(Interval(1, 3) * FiniteSet(0))\n    assert 2 in c1\n    assert 2 + I not in c1\n    assert 5 not in c1\n", "def test_ComplexRegion_is_subset():\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    right_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(-S.Pi/2, S.Pi/2), polar=True)\n    upper_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n\n    assert right_half_unit_disk.is_subset(unit_disk)\n    assert upper_half_unit_disk.is_subset(unit_disk)\n    assert not unit_disk.is_subset(right_half_unit_disk)\n    assert not unit_disk.is_subset(upper_half_unit_disk)\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(-3, 3)*Interval(-4, 4))\n    assert c1.is_subset(c2)\n    assert not c2.is_subset(c1)\n", "def test_imageset_union():\n    n = Dummy()\n    s1 = ImageSet(Lambda(n, n), S.Integers)\n    s2 = ImageSet(Lambda(n, 2*n), S.Integers)\n    assert s1.union(s2) == ImageSet(Lambda(n, n), S.Integers)\n    assert s1.union(FiniteSet(1)) == s1\n    assert s1.union(Interval(1, 2)) == Union(s1, Interval(1, 2), evaluate=False)\n", "def test_imageset_Lambda():\n    assert ImageSet(Lambda(x, x**2), S.Naturals).lamda == Lambda(x, x**2)\n    assert ImageSet(Lambda(x, x**2), S.Naturals).lamda != Lambda(x, x)\n    assert ImageSet(Lambda(x, x**2), S.Naturals) != ImageSet(Lambda(x, x), S.Naturals)\n"], "sample_1078": ["def test_IndexedBase_constructor():\n    i, j, m, n = symbols('i j m n', integer=True)\n    a = IndexedBase('a')\n    assert a.shape is None\n    assert a.label == Symbol('a')\n    assert a.free_symbols == {a}\n\n    a = IndexedBase('a', shape=(m, m))\n    assert a.shape == Tuple(m, m)\n    assert a.label == Symbol('a')\n    assert a.free_symbols == {a, m}\n\n    a = IndexedBase('a', shape=(m, n))\n    assert a.shape == Tuple(m, n)\n    assert a.label == Symbol('a')\n    assert a.free_symbols == {a, m, n}\n\n    raises(TypeError, lambda: IndexedBase('a', shape='wrong'))\n    raises(TypeError, lambda: IndexedBase('a', shape=(m, 'wrong')))\n", "def test_IndexedBase_with_shape():\n    i, j, k = symbols('i j k', integer=True)\n    a = IndexedBase('a', shape=(3, 4))\n    assert a.shape == (3, 4)\n    assert a[i, j].shape == (3, 4)\n    raises(IndexException, lambda: a[i])\n    raises(IndexException, lambda: a[i, j, k])\n    assert a.func(*a.args) == a\n    assert a[i, j].func(*a[i, j].args) == a[i, j]\n", "def test_IndexedBase_strides():\n    A = IndexedBase(\"A\", strides=(1, 3))\n    assert A.strides == (1, 3)\n    A = IndexedBase(\"A\", strides=(2,))\n    assert A.strides == (2,)\n    A = IndexedBase(\"A\")\n    assert A.strides is None\n", "def test_IndexedBase_strides():\n    i, j, k = symbols('i j k', integer=True)\n    m, n, p = symbols('m n p', integer=True)\n    A = IndexedBase('A', strides=(m, n, p))\n    assert A.strides == (m, n, p)\n    assert A[i, j, k].base.strides == (m, n, p)\n", "def test_IndexedBase_strides():\n    m, n, o, p = symbols('m n o p', integer=True)\n    A = IndexedBase('A', strides=(m, n))\n    B = IndexedBase('A', strides=(m, n, o, p))\n    i, j, k, l = symbols('i j k l', cls=Idx)\n\n    assert A.strides == (m, n)\n    assert B.strides == (m, n, o, p)\n    assert A[i, j].base.strides == (m, n)\n    assert B[i, j, k, l].base.strides == (m, n, o, p)\n"], "sample_1079": ["def test_orthogonal_direction():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point2D(1, 2)\n    p3 = Point(1, 2, 3, 4)\n\n    assert p1.orthogonal_direction == Point3D(-2, 1, 0)\n    assert p2.orthogonal_direction == Point2D(-2, 1)\n    assert p3.orthogonal_direction == Point(-2, 1, 0, 0)\n", "def test_orthogonal_direction():\n    p1 = Point(1, 2, 3)\n    p2 = p1.orthogonal_direction\n    assert Point.is_collinear(p1, p1.origin, p2) is False\n    assert p2.dot(p1) == 0\n    p1 = Point(1, 1)\n    p2 = p1.orthogonal_direction\n    assert Point.is_collinear(p1, p1.origin, p2) is False\n    assert p2.dot(p1) == 0\n    p1 = Point(0, 1)\n    p2 = p1.orthogonal_direction\n    assert Point.is_collinear(p1, p1.origin, p2) is False\n    assert p2.dot(p1) == 0\n", "def test_bounds():\n    assert Point2D(1, 2).bounds == (1, 2, 1, 2)\n", "def test_affine_rank():\n    p1 = Point(0, 0)\n    p2 = Point(1, 0)\n    p3 = Point(1, 1)\n    p4 = Point(2, 2)\n    assert Point.affine_rank() == -1\n    assert Point.affine_rank(p1) == 0\n    assert Point.affine_rank(p1, p2) == 1\n    assert Point.affine_rank(p1, p2, p3) == 2\n    assert Point.affine_rank(p1, p2, p3, p4) == 2\n", "def test_orthogonal_direction():\n    p1 = Point(1, 2, 3)\n    assert p1.orthogonal_direction == Point(-2, 1, 0)\n\n    p2 = Point(0, 0, 0)\n    raises(ZeroDivisionError, lambda: p2.orthogonal_direction)\n\n    p3 = Point(1, 0, 0)\n    assert p3.orthogonal_direction == Point(0, 1, 0)\n\n    p4 = Point(0, 1, 0)\n    assert p4.orthogonal_direction == Point(-1, 0, 0)\n"], "sample_1080": ["def test_refine_with_non_Atom():\n    from sympy import Integral\n    expr = Integral(x**2 + 1, x)\n    assert refine(expr) == expr\n", "compilation error", "def test_refine_Pow_non_integer():\n    from sympy import Symbol, Q, Rational, refine, Pow\n    x = Symbol('x')\n    assert refine(Pow(x, Rational(1, 3)), Q.real(x)) == Pow(x, Rational(1, 3))\n    assert refine(Pow(x, Rational(1, 3)), Q.positive(x)) == Pow(x, Rational(1, 3))\n    assert refine(Pow(-x, Rational(1, 3)), Q.positive(x)) == Pow(-x, Rational(1, 3))\n", "def test_refine_mul():\n    assert refine(Abs(x*y), Q.real(x) & Q.real(y)) == Abs(x*y)\n    assert refine(Abs(x*y), Q.real(x) & Q.positive(y)) == Abs(x)*y\n    assert refine(Abs(x*y), Q.positive(x) & Q.real(y)) == x*Abs(y)\n    assert refine(Abs(x*y), Q.positive(x) & Q.positive(y)) == x*y\n    assert refine(Abs(x*y*z), Q.real(x) & Q.real(y) & Q.real(z)) == Abs(x*y*z)\n    assert refine(Abs(x*y*z), Q.real(x) & Q.real(y) & Q.positive(z)) == Abs(x*y)*z\n    assert refine(Abs(x*y*z), Q.real(x) & Q.positive(y) & Q.real(z)) == Abs(x)*y*Abs(z)\n    assert refine(Abs(x*y*z), Q.positive(x) & Q.real(y) & Q.real(z)) == x*Abs(y*z)\n    assert refine(Abs(x*y*z), Q.positive(x) & Q.positive(y) & Q.real(z)) == x*y*Abs(z)\n    assert refine(Abs(x*y*z), Q.positive(x) & Q.positive(y) & Q.positive(z)) == x*y*z\n", "def test_refine_Mul():\n    assert refine(Abs(x*y), Q.positive(x) & Q.real(y)) == x*Abs(y)\n    assert refine(Abs(x*y*z), Q.positive(x) & Q.real(y) & Q.real(z)) == x*Abs(y*z)\n    assert refine(Abs(x*y), Q.negative(x) & Q.real(y)) == -x*Abs(y)\n    assert refine(Abs(x*y), Q.real(x) & Q.real(y)) == Abs(x*y)\n"], "sample_1081": ["def test_abundance():\n    assert abundance(12) == 16\n    assert abundance(6) == 6\n    assert abundance(10) == -2\n    assert abundance(945) == 1905\n    assert abundance(2**4 * 3**2) == 2**4 * 3**2\n    assert abundance(2**6 * 3**3) == 2**6 * 3**3 * 7\n", "def test_abundance():\n    assert abundance(6) == 0\n    assert abundance(12) == 4\n    assert abundance(15) == -9\n    assert abundance(28) == 0\n    assert abundance(945) == 190\n    assert abundance(1) == -1\n    assert abundance(2) == -1\n    assert abundance(3) == -2\n    assert abundance(4) == -3\n    assert abundance(5) == -4\n", "def test_abundance():\n    assert abundance(6) == 0\n    assert abundance(20) == 2\n    assert abundance(15) == -3\n    assert abundance(945) == 549\n    assert abundance(12) == 4\n", "def test_abundance():\n    assert abundance(10) == -2\n    assert abundance(12) == 4\n    assert abundance(1) == 0\n    assert abundance(945) == 190\n    assert abundance(6) == 0\n", "def test_abundance():\n    assert abundance(12) == 16\n    assert abundance(18) == 21\n    assert abundance(20) == 22\n    assert abundance(945) == 1905\n    assert abundance(6) == 0\n    assert abundance(28) == 0\n    assert abundance(496) == 0\n    assert abundance(8128) == 0\n"], "sample_1082": ["def test_is_finite():\n    x = Symbol('x')\n    assert sinh(x).is_finite is None\n    assert cosh(x).is_finite is None\n    assert tanh(x).is_finite is None\n    assert coth(x).is_finite is None\n    assert sech(x).is_finite is None\n    assert csch(x).is_finite is None\n\n    assert sinh(1).is_finite is True\n    assert cosh(1).is_finite is True\n    assert tanh(1).is_finite is True\n    assert coth(1).is_finite is True\n    assert sech(1).is_finite is True\n    assert csch(1).is_finite is True\n\n    assert sinh(oo).is_finite is False\n    assert cosh(oo).is_finite is False\n    assert tanh(oo).is_finite is True\n    assert coth(oo).is_finite is True\n    assert sech(oo).is_finite is True\n    assert csch(oo).is_finite is True\n\n    assert sinh(zoo).is_finite is False\n    assert cosh(zoo).is_finite is False\n    assert tanh(zoo).is_finite is False\n    assert coth(zoo).is_finite is False\n    assert sech(zoo).is_finite is False\n    assert csch(zoo).is_finite is False\n", "def test_issue_21457():\n    from sympy import Symbol\n    x = Symbol('x', real=True)\n    assert asinh(x).is_real is True\n    assert asinh(x).is_finite is True\n    assert acsch(x).is_real is True\n    assert acsch(x).is_finite is True\n    assert asech(x).is_real is None\n    assert asech(x).is_finite is None\n", "def test_hyperbolic_rewrite():\n    x = Symbol('x')\n    assert sinh(x).rewrite(csch) == 1/csch(x)\n    assert cosh(x).rewrite(sech) == 1/sech(x)\n    assert tanh(x).rewrite(coth) == 1/coth(x)\n    assert coth(x).rewrite(tanh) == 1/tanh(x)\n    assert csch(x).rewrite(sinh) == 1/sinh(x)\n    assert sech(x).rewrite(cosh) == 1/cosh(x)\n    assert asinh(x).rewrite(acsch) == acsch(1/x)\n    assert acosh(x).rewrite(asech) == asech(1/x)\n    assert atanh(x).rewrite(acoth) == acoth(1/x)\n    assert acoth(x).rewrite(atanh) == atanh(1/x)\n    assert asech(x).rewrite(acosh) == acosh(1/x)\n    assert acsch(x).rewrite(asinh) == asinh(1/x)\n", "def test_simplify_hyperbolic_trigonometric():\n    x = Symbol('x')\n    assert cos(I*x).simplify() == cosh(x)\n    assert sin(I*x).simplify() == I*sinh(x)\n    assert cosh(I*x).simplify() == cos(x)\n    assert sinh(I*x).simplify() == I*sin(x)\n    assert tan(I*x).simplify() == I*tanh(x)\n    assert cot(I*x).simplify() == -I*coth(x)\n    assert sec(I*x).simplify() == sech(x)\n    assert csc(I*x).simplify() == -I*csch(x)\n\n    assert tanh(I*x).simplify() == I*tan(x)\n    assert coth(I*x).simplify() == -I*cot(x)\n    assert sech(I*x).simplify() == sec(x)\n    assert csch(I*x).simplify() == -I*csc(x)\n", "def test_is_zero():\n    x = Symbol('x')\n    assert sinh(x).is_zero is None\n    assert cosh(x).is_zero is None\n    assert tanh(x).is_zero is None\n    assert coth(x).is_zero is None\n    assert csch(x).is_zero is None\n    assert sech(x).is_zero is None\n\n    assert asinh(x).is_zero is None\n    assert acosh(x).is_zero is None\n    assert atanh(x).is_zero is None\n    assert acoth(x).is_zero is None\n    assert asech(x).is_zero is None\n    assert acsch(x).is_zero is None\n\n    assert sinh(0).is_zero is True\n    assert cosh(0).is_zero is False\n    assert tanh(0).is_zero is True\n    assert coth(0).is_zero is False\n    assert csch(0).is_zero is False\n    assert sech(0).is_zero is False\n\n    assert asinh(0).is_zero is True\n    assert acosh(1).is_zero is True\n    assert atanh(0).is_zero is True\n    assert acoth(oo).is_zero is True\n    assert asech(1).is_zero is True\n    assert acsch(oo).is_zero is True\n"], "sample_1084": ["def test_ProductSet():\n    s1 = Interval(1, 3)\n    s2 = Interval(4, 6)\n    s3 = Interval(7, 9)\n    p1 = ProductSet(s1, s2)\n    p2 = ProductSet(s1, s2, s3)\n\n    assert (2, 5) in p1\n    assert (1, 4) in p1\n    assert (3, 6) in p1\n    assert (8, 5) not in p1\n    assert (2, 5, 8) in p2\n    assert (1, 4, 7) in p2\n    assert (3, 6, 9) in p2\n\n    assert p1.intersect(p2) == p1\n\n    s4 = Interval(1, 4)\n    p3 = ProductSet(s1, s4)\n\n    assert p1.intersect(p3) == ProductSet(s1, s2.intersect(s4))\n\n    assert len(p1) == 4\n    assert len(p2) == 8\n\n    assert p1.measure == 4\n    assert p2.measure == 8\n    assert ProductSet(Interval(1, 1), Interval(1, 1)).measure == 0\n\n    p4 = ProductSet(Interval(0, 2), Interval(0, 2), Interval(0, 2))\n    assert (1, 1, 1) in p4\n    assert (2, 2, 2) in p4\n    assert (1, 2, 3) not in p4\n\n    p5 = ProductSet(Interval(0, 2), Interval(0, 2), Interval(0, 2), Interval(0, 2))\n    assert (1, 1, 1, 1) in p5\n    assert (2, 2, 2, 2) in p5\n    assert (1, 2, 3, 4) not in p5\n", "def test_issue_20408():\n    c1 = ComplexRegion(Interval(1, 2)*Interval(2, 3))\n    c2 = ComplexRegion(Interval(1, 5)*Interval(1, 3))\n    R = Union(c1, c2)\n    assert Intersection(R, S.Reals) == Interval(1, 5)\n", "def test_Range_start_end():\n    assert Range(1, 10, 2).start == 1\n    assert Range(1, 10, 2).end == 9\n    assert Range(10, 1, -2).start == 10\n    assert Range(10, 1, -2).end == 2\n    assert Range(oo, 1, -1).start == oo\n    assert Range(oo, 1, -1).end == 2\n    assert Range(-oo, 1, 1).start == -oo\n    assert Range(-oo, 1, 1).end == 0\n    raises(ValueError, lambda: Range(x, 1, 1).start)\n    raises(ValueError, lambda: Range(x, 1, 1).end)\n", "def test_Range_simplify():\n    assert Range(3, 7).simplify() == Range(3, 7)\n    assert Range(3, 3).simplify() == S.EmptySet\n    assert Range(x, x + 3).simplify() == Range(x, x + 3)\n    assert Range(x, x).simplify() == S.EmptySet\n", "def test_issue_19662():\n    assert ImageSet(Lambda(x, 2*x), S.Naturals).intersect(\n        ImageSet(Lambda(x, 3*x), S.Naturals)) == ImageSet(Lambda(x, 6*x), S.Naturals)\n"], "sample_1083": ["def test_recursive_simplifications():\n    x = Symbol('x')\n    assert sinh(x).rewrite(csch).rewrite(sinh) == sinh(x)\n    assert cosh(x).rewrite(sech).rewrite(cosh) == cosh(x)\n    assert tanh(x).rewrite(coth).rewrite(tanh) == tanh(x)\n    assert coth(x).rewrite(tanh).rewrite(coth) == coth(x)\n    assert csch(x).rewrite(sinh).rewrite(csch) == csch(x)\n    assert sech(x).rewrite(cosh).rewrite(sech) == sech(x)\n", "def test_hyperbolic_rewrites():\n    x = Symbol('x')\n    assert sinh(x).rewrite('tractable') == (exp(x) - exp(-x))/2\n    assert cosh(x).rewrite('tractable') == (exp(x) + exp(-x))/2\n    assert tanh(x).rewrite('tractable') == (exp(x) - exp(-x))/(exp(x) + exp(-x))\n    assert coth(x).rewrite('tractable') == (exp(x) + exp(-x))/(exp(x) - exp(-x))\n    assert sech(x).rewrite('tractable') == 1 / ((exp(x) + exp(-x))/2)\n    assert csch(x).rewrite('tractable') == 1 / ((exp(x) - exp(-x))/2)\n", "def test_hyperbolic_is_finite():\n    x = Symbol('x')\n    assert sinh(x).is_finite is None\n    assert cosh(x).is_finite is None\n    assert tanh(x).is_finite is None\n    assert coth(x).is_finite is None\n    assert csch(x).is_finite is None\n    assert sech(x).is_finite is None\n    assert asinh(x).is_finite is None\n    assert acosh(x).is_finite is None\n    assert atanh(x).is_finite is None\n    assert acoth(x).is_finite is None\n\n    assert sinh(oo).is_finite is False\n    assert cosh(oo).is_finite is False\n    assert tanh(oo).is_finite is True\n    assert coth(oo).is_finite is True\n    assert csch(oo).is_finite is True\n    assert sech(oo).is_finite is True\n    assert asinh(oo).is_finite is False\n    assert acosh(oo).is_finite is False\n    assert atanh(oo).is_finite is True\n    assert acoth(oo).is_finite is True\n\n    assert sinh(nan).is_finite is False\n    assert cosh(nan).is_finite is False\n    assert tanh(nan).is_finite is False\n    assert coth(nan).is_finite is False\n    assert csch(nan).is_finite is False\n    assert sech(nan).is_finite is False\n    assert asinh(nan).is_finite is False\n    assert acosh(nan).is_finite is False\n    assert atanh(nan).is_finite is False\n    assert acoth(nan).is_finite is False\n\n    x = Symbol('x', finite=True)\n    assert sinh(x).is_finite is True\n    assert cosh(x).is_finite is True\n    assert tanh(x).is_finite is True\n    assert coth(x).is_finite is True\n    assert csch(x).is_finite is True\n    assert sech(x).is_finite is True\n    assert asinh(x).is_finite is True\n    assert acosh(x).is_finite is True\n    assert atanh(x).is_f", "def test_hyperbolic_finite():\n    x = Symbol('x')\n    k = Symbol('k', real=True)\n    n = Symbol('n', integer=True)\n\n    assert sinh(k).is_finite is True\n    assert cosh(k).is_finite is True\n    assert tanh(k).is_finite is True\n    assert sech(k).is_finite is True\n    assert csch(k).is_finite is True\n\n    assert sinh(n).is_finite is True\n    assert cosh(n).is_finite is True\n    assert tanh(n).is_finite is True\n    assert sech(n).is_finite is True\n    assert csch(n).is_finite is True\n\n    assert sinh(x).is_finite is None\n    assert cosh(x).is_finite is None\n    assert tanh(x).is_finite is None\n    assert sech(x).is_finite is None\n    assert csch(x).is_finite is None\n\n    assert coth(k).is_finite is True\n    assert coth(n).is_finite is True\n    assert coth(x).is_finite is None\n", "def test_finite_assumptions():\n    p = Symbol('p', finite=True)\n    assert sinh(p).is_finite is True\n    assert cosh(p).is_finite is True\n    assert tanh(p).is_finite is True\n    assert csch(p).is_finite is True\n    assert sech(p).is_finite is True\n    assert coth(p).is_finite is True\n    assert asinh(p).is_finite is True\n    assert acosh(p + 1).is_finite is True\n    assert atanh(p).is_finite is True\n    assert acoth(p).is_finite is True\n    assert asech(p).is_finite is True\n    assert acsch(p).is_finite is True\n"], "sample_1086": ["def test_issue_18149():\n    from sympy.printing.str import StrPrinter\n    from sympy.core.expr import UnevaluatedExpr\n    from sympy import sqrt\n\n    class CustomStrPrinter(StrPrinter):\n            return \"uneval({})\".format(self._print(expr.args[0]))\n\n    x = UnevaluatedExpr(sqrt(2))\n    assert CustomStrPrinter().doprint(x) == \"uneval(sqrt(2))\"\n", "def test_algebraic_number():\n    from sympy import AlgebraicNumber\n    a = AlgebraicNumber(sqrt(2) + sqrt(3))\n    assert str(a) == str(sqrt(2) + sqrt(3))\n    assert str(AlgebraicNumber((sqrt(2) + sqrt(3)).evalf())) == str(sqrt(2) + sqrt(3))\n", "def test_predicate():\n    from sympy.logic.predicates import Predicate\n    assert str(Predicate(\"P\")) == \"Q.P\"\n", "def test_FiniteSet_with_non_integer_elements():\n    from sympy import sqrt\n    assert str(FiniteSet(sqrt(2), sqrt(3), sqrt(5))) == 'FiniteSet(sqrt(2), sqrt(3), sqrt(5))'\n", "def test_Predicate():\n    from sympy.logic.predicates import Predicate\n    assert str(Predicate(\"P\")) == \"Q.P\"\n"], "sample_1085": ["def test_igcdex_coverage():\n    assert unchanged(igcdex, 0, 2) == (0, 1, 2)\n    assert unchanged(igcdex, 2, 0) == (1, 0, 2)\n", "def test_issue_21433():\n    assert unchanged(Integer, Integer(3))\n    assert unchanged(Rational, Rational(3, 4))\n", "def test_NegativeOne_power():\n    assert S.NegativeOne**12 is S.One\n    assert S.NegativeOne**13 is S.NegativeOne\n", "def test_mod_inverse_with_Rational():\n    assert mod_inverse(Rational(3, 4), 5) == 3\n    assert mod_inverse(Rational(7, 9), 11) == 5\n    assert mod_inverse(Rational(-3, 4), 5) == 2\n    assert mod_inverse(Rational(1, 2), 5) == 3\n    assert mod_inverse(Rational(1, 2), 3) == 2\n", "def test_mpf_norm_rounding_mode():\n    # Test mpf_norm with different rounding modes\n    from mpmath.libmp import mpf_norm, round_nearest, round_floor, round_ceil\n    mpf = (0, 123456789012345, -15, 63)\n    assert mpf_norm(mpf, 53, round_nearest) == mpf\n    assert mpf_norm(mpf, 53, round_floor) == (0, 123456789012345, -15, 53)\n    assert mpf_norm(mpf, 53, round_ceil) == (0, 123456789012345, -15, 53)\n    assert mpf_norm(mpf, 20, round_nearest) == (0, 123, -15, 20)\n    assert mpf_norm(mpf, 20, round_floor) == (0, 122, -15, 20)\n    assert mpf_norm(mpf, 20, round_ceil) == (0, 124, -15, 20)\n"], "sample_1087": ["def test_w_polys():\n    w1, w2 = w_polys()\n    assert w1.is_Poly and w2.is_Poly\n    assert w1.gens == (x, y, z) and w2.gens == (x, y)\n    assert w1.domain == ZZ and w2.domain == ZZ\n", "def test_w_polys():\n    w1, w2 = w_polys()\n    assert w1 != 0 and w2 != 0\n    assert w1.domain == ZZ and w2.domain == ZZ\n    assert w1.gens == (x, y, z) and w2.gens == (x, y)\n", "def test_w_polys():\n    w1, w2 = w_polys()\n    assert w1.is_Poly and w2.is_Poly\n\n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n    assert all(f.is_Poly for f in [f0, f1, f2, f3, f4, f5, f6])\n\n    assert w1.domain == ZZ and w2.domain == ZZ\n    assert all(f.domain == ZZ for f in [f0, f1, f2, f3, f4, f5, f6])\n", "def test_f_polys():\n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n    \n    assert f0.coeff_monomial(x**2*y*z**2) == 1\n    assert f1.coeff_monomial(x**3*y*z) == 1\n    assert f2.coeff_monomial(x**5*y**3) == 1\n    assert f3.coeff_monomial(x**5*y**2) == 1\n    assert f4.coeff_monomial(x**9*y**8*z) == -1\n    assert f5.coeff_monomial(x**3) == -1\n    assert f6.coeff_monomial(x**4*y) == 2115\n", "def test_f_polys():\n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n\n    assert f0 == 4*x**2*y*z**2 + 2*x**2*y*z + 3*x**2*y + 2*x**2 + 3*x + 4*y**2*z**2 + 5*y**2*z + 6*y**2 + y*z**2 + 2*y*z + y + 1\n    assert f1 == x**3*y*z + x**2*y**2*z**2 + x**2*y**2 + 20*x**2*y*z + 30*x**2*y + x**2*z**2 + 10*x**2*z + x*y**3*z + 30*x*y**2*z + 20*x*y**2 + x*y*z**3 + 10*x*y*z**2 + x*y*z + 610*x*y + 20*x*z**2 + 230*x*z + 300*x + y**2*z**2 + 10*y**2*z + 30*y*z**2 + 320*y*z + 200*y + 600*z + 6000\n    assert f2 == x**5*y**3 + x**5*y**2*z + x**5*y*z**2 + x**5*z**3 + x**3*y**2 + x**3*y*z + 90*x**3*y + 90*x**3*z + x**2*y**2*z - 11*x**2*y**2 + x**2*z**3 - 11*x**2*z**2 + y*z - 11*y + 90*z - 990\n    assert f3 == x**5*y**2 + x**4*z**4 + x**4 + x**3*y**3*z + x**3*z + x**2*y**4 + x**2*y**3*z**3 + x**2*y*z**5 + x**2*y*z + x*y**2*z**4 + x*y**2 + x*y*z**7 + x*y*z**3 + x*y*z**2 + y**2*z + y*z**4\n    assert f4 == -x**9*y**8*z - x**8*y**5*z**3 - x**7*y"], "sample_1088": ["def test_rational_interpolate_symbolic_coordinates():\n    from sympy import symbols\n    x = symbols('x')\n    data = [(1, -1), (2, 0), (3, 2), (4, 22/5), (5, 7), (6, 68/7)]\n    result = rational_interpolate(data, 2, X=x)\n    assert result == (3*x**2 - 7*x + 2)/(x + 1)\n    assert result.subs(x, 4) == 22/5\n    xdata = [1, 2, symbols('a'), 4, 5, 6]\n    ydata = [-1, 0, symbols('b'), 22/5, 7, 68/7]\n    result = rational_interpolate(list(zip(xdata, ydata)), 2)\n    assert result.is_rational_function(x)\n", "def test_symmetrize_special_cases():\n    raises(ComputationFailed, lambda: symmetrize(x/sin(x), x))\n    raises(ComputationFailed, lambda: symmetrize(x**2/sin(x), x))\n    raises(ComputationFailed, lambda: symmetrize(x**2/sin(x**2), x))\n    raises(PolificationFailed, lambda: symmetrize(1/sin(x), x))\n", "def test_symmetrize_polynomials_with_zero_coefficients():\n    assert symmetrize(0*x**2 + 1, x, y) == (1, 0)\n    assert symmetrize(x**2 + 0*x*y + y**2, x, y) == (x**2 + y**2, 0)\n    assert symmetrize(x**2 + 0*x + 1, x, y) == ((x + y)**2 - 2*x*y + 1, -2*y**2 + y)\n", "def test_viete_symbolic_roots():\n    a, b, c, r = symbols('a b c r')\n    f = x**3 + a*x**2 + b*x + c\n    roots = [r + i for i in range(3)]\n    result = viete(f, roots, x)\n    assert len(result) == 3\n    s1, s2, s3 = [sum(a*b for a, b in zip(result[i][0].as_coeff_Mul()[1].args, [1, -1, 1])) for i in range(3)]\n    assert s1 == -a\n    assert s2 == b\n    assert s3 == -c\n", "def test_symmetrize_formal():\n    assert symmetrize(x, formal=True) == (x, 0, [(x, x)])\n    assert symmetrize(x + 1, formal=True) == (x + 1, 0, [(x, x)])\n    assert symmetrize(x**2, formal=True) == (x**2, 0, [(x, x)])\n\n    s1, s2 = symbols('s1 s2')\n    assert symmetrize(x + y, formal=True) == (s1, 0, [(s1, x + y), (s2, x*y)])\n    assert symmetrize(x + y + 1, formal=True) == (s1 + 1, 0, [(s1, x + y), (s2, x*y)])\n\n    assert symmetrize([x, y], formal=True) == ([(x, 0), (y, 0)], [(x, x)])\n    assert symmetrize([x + 1, y], formal=True) == ([(x + 1, 0), (y, 0)], [(x, x)])\n"], "sample_1089": ["def test_enumerate_S1():\n    w, x, y, z = symbols('w,x,y,z')\n    p = w*x*y*z\n    e = (w + x + y + z)**20\n    result = p*e.expand()\n    result_gcd = gcd_terms(result).as_expr()\n    assert result_gcd.func == Mul\n", "def test_issue_18764():\n    from sympy import Dummy\n    assert _monotonic_sign(Dummy(zero=True, nonnegative=True)) == 0\n    assert _monotonic_sign(Dummy(zero=True, nonpositive=True)) == 0\n    assert _monotonic_sign(Dummy(positive=True)) == Dummy('pos', positive=True)\n    assert _monotonic_sign(Dummy(negative=True)) == Dummy('neg', negative=True)\n    assert _monotonic_sign(Dummy(nonnegative=True)) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(Dummy(nonpositive=True)) == Dummy('npos', nonpositive=True)\n", "def test_issue_19462():\n    from sympy import symbols, Function, Eq\n    a = symbols('a', commutative=False)\n    f = Function('f')\n    eq = Eq(f(a) + f(a), 0)\n    assert factor_nc(eq.lhs) == 2*f(a)\n", "def test_gcd_terms_dict():\n    assert gcd_terms(Dict({x*y: x, x: y})) == Dict({x*y: x, x: y})\n    assert gcd_terms(Dict({x: x, x*y: x*y})) == Dict({x: 1, x*y: y})\n    assert gcd_terms(Dict({x**2 + x*y: x, x*y: x*y})) == Dict({x**2 + x*y: x, x*y: x*y})\n", "def test_mask_nc():\n    nc = symbols('nc', commutative=False)\n    x, y, z = symbols('x y z')\n    eq = x + y + z + nc\n    masked, reps, nc_symbols = _mask_nc(eq)\n    assert reps == {}\n    assert len(nc_symbols) == 1\n    assert masked == eq\n    eq = x + y + z + nc*(x + y)\n    masked, reps, nc_symbols = _mask_nc(eq)\n    assert reps == {}\n    assert len(nc_symbols) == 2\n    assert masked == eq\n\n    class Foo(Basic):\n        pass\n    eq = Foo() + Foo()\n    masked, reps, nc_symbols = _mask_nc(eq)\n    assert len(reps) == 1\n    assert len(nc_symbols) == 1\n    assert reps[nc_symbols[0]] == Foo()\n    assert masked == nc_symbols[0] + nc_symbols[0]\n"], "sample_1091": ["def test_binary_symbols_edge_cases():\n    assert Eq(x, True).binary_symbols == set([x])\n    assert Eq(True, x).binary_symbols == set([x])\n    assert Eq(x, False).binary_symbols == set([x])\n    assert Eq(False, x).binary_symbols == set([x])\n    assert Eq(True, True).binary_symbols == set()\n    assert Eq(False, False).binary_symbols == set()\n    assert Eq(True, False).binary_symbols == set()\n    assert Eq(False, True).binary_symbols == set()\n    assert Eq(1, 2).binary_symbols == set()\n", "def test_nonpolymonial_relations_with_simplification():\n    assert Eq(sin(x)**2 + cos(x)**2, 1).simplify() == True\n    assert Eq(x**2 + 2*x + 1, (x + 1)**2).simplify() == True\n    assert Ne(x**2 + 2*x + 1, (x + 1)**2).simplify() == False\n    assert Ge(log(x), log(x**2)).simplify() == Le(x, x**2)\n    assert Lt(log(x), log(x**2)).simplify() == Lt(x, x**2)\n    assert Eq(sqrt(x**2), abs(x)).simplify() == True\n", "def test_relational_with_set():\n    assert Eq({1, 2, 3}, {1, 2, 3}).simplify() == True\n    assert Eq({1, 2, 3}, {1, 2, 4}).simplify() == False\n    assert Eq(FiniteSet(1, 2, 3), FiniteSet(1, 2, 3)).simplify() == True\n    assert Eq(FiniteSet(1, 2, 3), FiniteSet(1, 2, 4)).simplify() == False\n    assert Eq(Interval(1, 2), Interval(1, 2)).simplify() == True\n    assert Eq(Interval(1, 2), Interval(1, 3)).simplify() == False\n", "def test_relational_evaluate():\n    # Test that the evaluate parameter is correctly passed to the relational classes\n    assert Eq(x, x, evaluate=False).doit() == S.true\n    assert Ne(x, x, evaluate=False).doit() == S.false\n    assert Ge(x, x, evaluate=False).doit() == S.true\n    assert Le(x, x, evaluate=False).doit() == S.true\n    assert Gt(x, x, evaluate=False).doit() == S.false\n    assert Lt(x, x, evaluate=False).doit() == S.false\n", "def test_relate_simplify():\n    from sympy.solvers.solveset import linear_coeffs\n    # issue 18585\n    assert Eq(x + log(1 + 1/x), x*log(x + 1/x) + 1).simplify() == Eq(1, x*log(x + 1/x) - log(1 + 1/x))\n    assert simplify(Eq(x + log(1 + 1/x), x*log(x + 1/x) + 1)) == Eq(x*log(x + 1/x) - log(1 + 1/x), 1)\n    assert simplify(Eq(x + log(1 + 1/x), x*log(x + 1/x) + 2)) == Eq(x*log(x + 1/x) - log(1 + 1/x), 2)\n    assert simplify(Eq(x + log(1 + 1/x), x*log(x + 1/x))) == Eq(x*log(x + 1/x) - log(1 + 1/x), 0)\n    assert simplify(Eq(2*x + 2*log(1 + 1/x), 2*x*log(x + 1/x) + 2)) == Eq(x*log(x + 1/x) - log(1 + 1/x), 1)\n    assert simplify(Eq(2*x + 2*log(1 + 1/x), 2*x*log(x + 1/x) + 3)) == Eq(x*log(x + 1/x) - log(1 + 1/x), Rational(3,2))\n\n    assert simplify(Ge(x + log(1 + 1/x), x*log(x + 1/x) + 2)) == Ge(x*log(x + 1/x) - log(1 + 1/x), 2)\n    assert simplify(Gt(x + log(1 + 1/x), x*log(x + 1/x) + 2)) == Gt(x*log(x + 1/x) - log(1 + 1/x), 2)\n    assert simplify(Le(x + log(1 + 1/x), x*log(x + 1/x) + 2)) == Le(x*log(x + 1/x) - log(1 + 1/x), 2)\n    assert simplify(Lt(x + log(1 + 1/x), x*log(x + 1/x) + 2)) =="], "sample_1090": ["def test_simplify_nested():\n    with evaluate(False):\n        expr = Mul(Add(x, x), Add(y, y))\n        assert expr.args == ((x + x), (y + y))\n        assert expr.args[0].args == (x, x)\n        assert expr.args[1].args == (y, y)\n\n        expr = Pow(Add(x, x), 2)\n        assert expr.args == ((x + x), 2)\n        assert expr.args[0].args == (x, x)\n\n        expr = Add(Mul(x, x), Mul(y, y))\n        assert expr.args == (x**2, y**2)\n        assert expr.args[0].args == (x, 2)\n        assert expr.args[1].args == (y, 2)\n\n        expr = Mul(Pow(x, 2), Pow(y, 2))\n        assert expr.args == (x**2, y**2)\n        assert expr.args[0].args == (x, 2)\n        assert expr.args[1].args == (y, 2)\n", "def test_mul():\n    with evaluate(False):\n        p = oo * oo\n        assert isinstance(p, Mul) and p.args == (oo, oo)\n        p = 5 * oo\n        assert isinstance(p, Mul) and p.args == (5, oo)\n        p = oo * 5\n        assert isinstance(p, Mul) and p.args == (oo, 5)\n        p = oo * -5\n        assert isinstance(p, Mul) and p.args == (oo, -5)\n        p = -5 * oo\n        assert isinstance(p, Mul) and p.args == (-5, oo)\n        p = -oo * -5\n        assert isinstance(p, Mul) and p.args == (-oo, -5)\n        p = -5 * -oo\n        assert isinstance(p, Mul) and p.args == (-5, -oo)\n    with evaluate(False):\n        expr = x * x\n        assert isinstance(expr, Mul)\n        assert expr.args == (x, x)\n\n        with evaluate(True):\n            assert (x * x).args == (x**2,)\n\n        assert (x * x).args == (x, x)\n\n    assert isinstance(x * x, Pow)\n\n    with evaluate(False):\n        assert S.One * 2 == Mul(1, 2)\n        assert 2 * S.One == Mul(1, 2)\n\n        assert S(4) / 2 == Mul(4, S.One / 2)\n        assert S.One / 2 * 4 == Mul(S.One / 2, 4)\n\n        assert S(2) ** 3 == Pow(2, 3)\n        assert 3 ** S(2) == Pow(3, 2)\n\n        assert S(6) / 3 == Mul(6, S.One / 3)\n        assert S.One / 3 * 6 == Mul(S.One / 3, 6)\n\n        assert 9 ** S(2) == Pow(9, 2)\n        assert S(2) ** 9 == Pow(2, 9)\n\n        assert S(2) / 2 == Mul(2, S.One / 2)\n        assert S.One / 2 * 2 == Mul(S.One / 2, 2)\n\n        assert S(2) / 3 * 4 == Mul(S(2) / 3, 4)\n        assert 4", "def test_nested2():\n    with evaluate(False):\n        expr = (x + x) * (y + y)\n        assert expr.args == ((x + x), (y + y))\n        assert expr.args[0].args == (x, x)\n        assert expr.args[1].args == (y, y)\n\n    with evaluate(True):\n        expr = (x + x) * (y + y)\n        assert expr.args == (2, x, 2, y)\n        assert isinstance(expr, Mul)\n", "def test_div():\n    with evaluate(False):\n        p = 5 / oo\n        assert isinstance(p, Mul) and p.args == (5, 1 / oo)\n        p = 5 / (-oo)\n        assert isinstance(p, Mul) and p.args == (5, 1 / -oo)\n        p = oo / 5\n        assert isinstance(p, Mul) and p.args == (oo, 1 / 5)\n        p = oo / oo\n        assert isinstance(p, Mul) and p.args == (oo, 1 / oo)\n        p = -oo / oo\n        assert isinstance(p, Mul) and p.args == (-oo, 1 / oo)\n        p = oo / -oo\n        assert isinstance(p, Mul) and p.args == (oo, 1 / -oo)\n        p = -oo / -oo\n        assert isinstance(p, Mul) and p.args == (-oo, 1 / -oo)\n\n    with evaluate(True):\n        p = 5 / oo\n        assert p == 0\n        p = 5 / (-oo)\n        assert p == 0\n        p = oo / 5\n        assert p == oo\n        p = oo / oo\n        assert p == nan\n        p = -oo / oo\n        assert p == nan\n        p = oo / -oo\n        assert p == nan\n        p = -oo / -oo\n        assert p == nan\n", "def test_mul():\n    with evaluate(False):\n        p = oo * oo\n        assert isinstance(p, Mul) and p.args == (oo, oo)\n        p = 5 * oo\n        assert isinstance(p, Mul) and p.args == (5, oo)\n        p = oo * 5\n        assert isinstance(p, Mul) and p.args == (5, oo)\n        p = -oo * 5\n        assert isinstance(p, Mul) and p.args == (-5, oo)\n        p = 5 * -oo\n        assert isinstance(p, Mul) and p.args == (-5, oo)\n        p = -oo * -5\n        assert isinstance(p, Mul) and p.args == (5, oo)\n        p = -5 * -oo\n        assert isinstance(p, Mul) and p.args == (5, oo)\n        p = oo * nan\n        assert isinstance(p, Mul) and p.args == (oo, nan)\n        p = nan * oo\n        assert isinstance(p, Mul) and p.args == (oo, nan)\n        p = 5 * nan\n        assert isinstance(p, Mul) and p.args == (5, nan)\n        p = nan * 5\n        assert isinstance(p, Mul) and p.args == (5, nan)\n        p = -5 * nan\n        assert isinstance(p, Mul) and p.args == (-5, nan)\n        p = nan * -5\n        assert isinstance(p, Mul) and p.args == (-5, nan)\n"], "sample_1092": ["def test_cse_reps_toposort():\n    # Test that reps_toposort correctly sorts replacements to avoid circular substitutions\n    reps = [(x0, x + y), (x1, x0 + z), (x2, x1 + w), (x3, x2 + x0)]\n    reps_sorted = cse_main.reps_toposort(reps)\n    assert reps_sorted == [(x0, x + y), (x1, x0 + z), (x2, x1 + w), (x3, x2 + x0)]\n    # Test that reps_toposort correctly handles dependencies between replacements\n    reps = [(x0, x + y), (x1, x0 + z), (x2, x0 + w), (x3, x2 + x1)]\n    reps_sorted = cse_main.reps_toposort(reps)\n    assert reps_sorted == [(x0, x + y), (x1, x0 + z), (x2, x0 + w), (x3, x2 + x1)]\n", "def test_cse_with_function():\n    from sympy import Function\n    f = Function('f')\n    exprs = [f(x) + f(x), f(x) + f(y)]\n    subst, red = cse(exprs)\n    assert len(subst) > 0\n    assert red[0] == 2 * subst[0][0]\n    assert red[1] == subst[0][0] + f(y)\n", "def test_cse_unevaluated_derivative():\n    from sympy import Derivative, sin\n    f = Function('f')\n    x1 = Symbol('x1')\n    expr = Derivative(f(x1), x1, evaluate=False)\n    assert cse(expr) == ([], [expr])\n    assert cse(expr + sin(x1)) == ([(x0, sin(x1))], [expr + x0])\n", "def test_cse_with_matrices():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    exprs = [A*B, A*B*A]\n    subst, red = cse(exprs)\n    assert len(subst) > 0, \"Failed to identify a CSE\"\n    assert isinstance(subst[0][1], MatrixExpr), \"Failed to identify matrix expression\"\n", "def test_issue_18638():\n    from sympy import UnevaluatedExpr\n    x = symbols('x')\n    expr = UnevaluatedExpr(x) + UnevaluatedExpr(x)\n    subst, red = cse(expr)\n    assert subst == [(x0, x)]\n    assert red == [x0 + x0]\n"], "sample_1093": ["def test_issue_fresnel_s():\n    from sympy import fresnels\n\n    expr1 = fresnels(x)\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.fresnels(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.fresnel(x)[0]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with NumPy:\\n  # fresnels\\nfresnels(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python:\\n  # fresnels\\nfresnels(x)'\n", "def test_printing_CodegenArray():\n    from sympy.codegen.array_utils import (CodegenArrayContraction,\n                                           CodegenArrayTensorProduct,\n                                           CodegenArrayElementwiseAdd)\n    from sympy import MatrixSymbol, symbols\n    m, n = symbols('m n', integer=True)\n    A = MatrixSymbol(\"A\", m, n)\n    B = MatrixSymbol(\"B\", m, n)\n    C = MatrixSymbol(\"C\", m, n)\n\n    assert pycode(CodegenArrayTensorProduct(A, B)) == 'numpy.einsum(A, [0, 1], B, [2, 3])'\n    assert pycode(CodegenArrayContraction(CodegenArrayTensorProduct(A, B))) == \\\n        'numpy.einsum(A, [0, 1], B, [0, 2])'\n\n    assert pycode(CodegenArrayElementwiseAdd(A, B)) == 'numpy.add(A, B)'\n    assert pycode(CodegenArrayElementwiseAdd(A, B, C)) == 'numpy.add(numpy.add(A, B), C)'\n", "def test_FloatingPointNumbers():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(S.FloatPriorityDict(1.0)) == '(1.0)'\n    assert prntr.doprint(S.FloatPriorityDict('nan')) == \"float('nan')\"\n    assert prntr.doprint(S.FloatPriorityDict('inf')) == \"float('inf')\"\n    assert prntr.doprint(S.FloatPriorityDict('-inf')) == \"float('-inf')\"\n\n    prntr = NumPyPrinter()\n\n    assert prntr.doprint(S.FloatPriorityDict(1.0)) == '(1.0)'\n    assert prntr.doprint(S.FloatPriorityDict('nan')) == 'numpy.nan'\n    assert prntr.doprint(S.FloatPriorityDict('inf')) == 'numpy.PINF'\n    assert prntr.doprint(S.FloatPriorityDict('-inf')) == 'numpy.NINF'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n\n    smat = SparseMatrix(2, 5, {(0, 1): 3})\n    assert p.doprint(smat) == 'sympy.Matrix([[0, 3, 0, 0, 0], [0, 0, 0, 0, 0]])'\n\n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.Exp1) == 'sympy.E'\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == \"sympy.MatrixInverse(A)\"\n    assert p.doprint(A**5) == \"A**5\"\n    assert p.doprint(Identity(3)) == \"sympy.eye(3)\"\n", "def test_printing_Assignment():\n    from sympy.codegen.ast import Assignment\n    v = MatrixSymbol('v', 3, 1)\n    expr = Assignment(v, [x, y, z])\n    assert pycode(expr) == 'v = [x, y, z]'\n"], "sample_1094": ["def test_find():\n    x, y = symbols('x y')\n    e = (x + y)*(x - y)\n    assert e.find(x) == {x}\n    assert e.find(x+y) == {x + y}\n    assert e.find(lambda u: u.is_Mul) == {e}\n    assert e.find(x, group=True) == {x: 2}\n", "def test_constructor_postprocessors():\n    from sympy.core.function import FunctionClass\n\n    class CustomFunction(Function):\n        pass\n\n    CustomFunction._constructor_postprocessor_mapping = {\n        Basic: [\n            lambda x: x.func(*[i**2 for i in x.args])\n        ]\n    }\n\n    class NewAdd(Basic):\n        pass\n\n    CustomFunction._constructor_postprocessor_mapping[NewAdd] = [\n        lambda x: x.func(*[i**3 for i in x.args])\n    ]\n\n    x = Symbol('x')\n    assert CustomFunction(x, x) == CustomFunction(x**2, x**2)\n    assert NewAdd(x, x)._exec_constructor_postprocessors(CustomFunction(x, x)) == CustomFunction(x**3, x**3)\n", "def test_replace_issue_21715():\n    # This test checks if the replace function correctly handles a case where\n    # the same symbol appears in the replacement rule and in the original expression.\n    x = symbols('x')\n    expr = 2*x + x**2\n    assert expr.replace(x**2, x) == 2*x + x\n    assert expr.replace(x**2, x, exact=True) == 2*x + x\n    assert expr.replace(x**2, x, exact=False) == 2*x + x\n", "def test_replace_map():\n    from sympy import Wild\n    x, y, z = symbols('x y z')\n    a, b = Wild('a'), Wild('b')\n    e = sin(x) + sin(y)\n    r = e.replace(sin(a), tan(a), map=True)\n    assert r == (tan(x) + tan(y), {sin(x): tan(x), sin(y): tan(y)})\n    assert e.replace(sin(a), tan(a)) == tan(x) + tan(y)\n\n    f = x + 2*y\n    r = f.replace(a*b, lambda a, b: a + b, map=True, exact=False)\n    assert r == (x + 2, {2*y: 2})\n    assert f.replace(a*b, lambda a, b: a + b, exact=False) == x + 2\n    assert f.replace(a*b, lambda a, b: a + b, map=True, exact=True) == (x + 2*y, {})\n    assert f.replace(a*b, lambda a, b: a + b, exact=True) == x + 2*y\n\n    e = sin(x)*sin(y)\n    r = e.replace(sin(a)*sin(b), lambda a, b: cos(a - b), map=True)\n    assert r == (cos(x - y), {sin(x)*sin(y): cos(x - y)})\n    assert e.replace(sin(a)*sin(b), lambda a, b: cos(a - b)) == cos(x - y)\n\n    assert (2*x*y).replace(a*b, a + b) == 2*x*y\n    assert (2*x*y).replace(a*b, a + b, exact=False) == 2 + x + y\n\n    assert z.replace(a, b, map=True) == (z, {})\n    assert z.replace(a, b) == z\n", "def test_count_ops_visual():\n    x, y, z = symbols('x y z')\n    expr1 = sin(x) + cos(x) + 2*x\n    expr2 = expr1.diff(x)\n    assert expr1.count_ops(visual=True) == 6\n    assert expr2.count_ops(visual=True) == 10\n"], "sample_1095": ["def test_unrank_lex():\n    assert Permutation.unrank_lex(3, 0).array_form == [0, 1, 2]\n    assert Permutation.unrank_lex(3, 5).array_form == [2, 1, 0]\n    assert Permutation.unrank_lex(5, 10).array_form == [0, 2, 4, 1, 3]\n    assert Permutation.unrank_lex(5, 119).array_form == [4, 3, 2, 1, 0]\n", "def test_permutation_signature():\n    p = Permutation(0, 1, 2)\n    assert p.signature() == -1\n    p = Permutation(1, 2, 0)\n    assert p.signature() == -1\n    p = Permutation(0, 2, 1)\n    assert p.signature() == 1\n    p = Permutation(2, 0, 1)\n    assert p.signature() == 1\n    p = Permutation(2, 1, 0)\n    assert p.signature() == 1\n    p = Permutation(1, 0, 2)\n    assert p.signature() == 1\n", "def test_AppliedPermutation_subs():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    ap = AppliedPermutation(p, x)\n    assert ap.subs(x, 0) == Integer(1)\n    assert ap.subs(x, 1) == Integer(2)\n    assert ap.subs(x, 2) == Integer(0)\n    assert ap.subs(x, x) == ap\n", "def test_commutator():\n    p = Permutation(0, 1, 2)\n    q = Permutation(1, 2, 0)\n    assert p.commutator(q) == Permutation(0, 2, 1)\n    assert q.commutator(p) == Permutation(0, 2, 1)\n    assert p.commutator(p) == Permutation()\n", "def test_permutation_unrank_lex():\n    # Test permutations of different sizes\n    for size in range(1, 6):\n        for rank in range(factorial(size)):\n            perm = Permutation.unrank_lex(size, rank)\n            assert perm.rank() == rank\n            assert perm.size == size\n"], "sample_1096": ["def test_IndexedBase_with_range():\n    n, m = symbols('n m', integer=True)\n    a = IndexedBase('a', shape=(n, m), strides='C')\n    i, j = symbols('i j', integer=True)\n    assert a[i, j].base == a\n    assert a[i, j].shape == (n, m)\n    assert a.strides == 'C'\n    assert a.offset == 0\n", "def test_IndexedBase_with_Symbol_label():\n    i, j = symbols('i j', integer=True)\n    x = symbols('x')\n    A = IndexedBase(x)\n    assert A[i, j] == Indexed(x, i, j)\n    assert A[i, j].base == A\n", "def test_indexed_without_range():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A')\n    assert A[i, j].ranges == [None, None]\n    assert A[i, j].shape is None\n    raises(IndexException, lambda: A[i, j].ranges == [(1, 2), (3, 4)])\n    raises(IndexException, lambda: A[i, j].shape == (2, 3))\n", "def test_IndexedBase_free_symbols():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A')\n    B = IndexedBase('B', shape=(i, j))\n    assert A.free_symbols == set()\n    assert B.free_symbols == {i, j}\n", "def test_IndexedBase_constructor():\n    i, j = symbols('i j', integer=True)\n    o, p = symbols('o p', integer=True)\n\n    # Test constructor with label as string\n    A = IndexedBase('A')\n    assert A.label == Symbol('A')\n\n    # Test constructor with label as Symbol\n    A = IndexedBase(Symbol('A'))\n    assert A.label == Symbol('A')\n\n    # Test constructor with shape\n    A = IndexedBase('A', shape=(o, p))\n    assert A.shape == Tuple(o, p)\n\n    # Test constructor with assumptions\n    A = IndexedBase('A', real=True)\n    assert A.is_real\n\n    # Test constructor with invalid shape\n    raises(TypeError, lambda: IndexedBase('A', shape='invalid'))\n\n    # Test constructor with invalid assumptions\n    raises(TypeError, lambda: IndexedBase('A', invalid_assumption=True))\n"], "sample_1097": ["def test_BlockMatrix_determinant_blockdiagonal():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    C = MatrixSymbol('C', l, l)\n    X = BlockDiagMatrix(A, B, C)\n    assert det(X) == det(A) * det(B) * det(C)\n", "def test_blockcut_on_BlockMatrix():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, k)\n    C = MatrixSymbol('C', l, m)\n    D = MatrixSymbol('D', l, k)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    cut_X = blockcut(X, (n, l), (m, k))\n    assert cut_X == BlockMatrix([[A, B], [C, D]])\n    assert cut_X.blocks[0, 0] == A\n    assert cut_X.blocks[0, 1] == B\n    assert cut_X.blocks[1, 0] == C\n    assert cut_X.blocks[1, 1] == D\n", "def test_BlockMatrix_equals():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = BlockMatrix([[A, B], [C, D]])\n    Z = BlockMatrix([[A, C], [B, D]])\n\n    assert X.equals(Y)\n    assert not X.equals(Z)\n    assert X.equals(X)\n    assert not X.equals(A)\n", "def test_block_collapse_trace():\n    A, B, C, D = [MatrixSymbol(s, 3, 3) for s in 'ABCD']\n    X = BlockMatrix([[A, B], [C, D]])\n    assert block_collapse(Trace(X)) == trace(A) + trace(D)\n", "def test_block_collapse_determinant():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    X = BlockMatrix([[A, B], [C, A]])\n\n    from sympy import assuming, Q\n    with assuming(Q.invertible(A)):\n        assert block_collapse(det(X)) == det(A) * det(A - C*A.I*B)\n"], "sample_1098": ["def test_hyper_simplify():\n    a, b, c, z = symbols('a b c z')\n    assert hyper([a, b], [c], z).simplify() == hyper([a, b], [c], z)\n    assert hyper([a, 0], [c], z).simplify() == hyper([a], [c], z)\n    assert hyper([0, 0], [c], z).simplify() == 1\n    assert hyper([a, b], [0], z).simplify() == hyper([a, b], [], z)\n    assert hyper([a, b], [0, 0], z).simplify() == 1 + a*b*z\n    assert hyper([a, b, c], [a, b], 1).simplify() == gamma(a)*gamma(b)/(gamma(a - c + 1)*gamma(b - c + 1))\n", "def test_hyper_fdiff():\n    from sympy import hyperexpand\n    a, b, c, z = symbols('a b c z')\n    h = hyper((a, b), (c,), z)\n    assert h.fdiff(3) == a*b/c * hyper((a + 1, b + 1), (c + 1,), z)\n    assert hyperexpand(h.fdiff(3)) == hyperexpand(a*b/c * hyper((a + 1, b + 1), (c + 1,), z))\n", "def test_hyper_conjugate():\n    from sympy import conjugate\n    a, b, c, z = symbols('a b c z')\n    assert conjugate(hyper([a, b], [c], z)) == hyper([conjugate(a), conjugate(b)], [conjugate(c)], conjugate(z))\n", "def test_hyper_simplify():\n    from sympy import simplify\n    a, b, c = symbols('a b c')\n    assert simplify(hyper([], [], x)) == exp(x)\n    assert simplify(hyper([1, 1], [2], -x)) == log(1 + x)\n    assert simplify(hyper([], [S(1)/2], -x**2/4)) == cos(x)\n    assert simplify(x*hyper([S(1)/2, S(1)/2], [S(3)/2], x**2)) == asin(x)\n", "def test_meijerg_unpolarify():\n    from sympy import exp_polar\n    a = exp_polar(2*pi*I)*x\n    b = x\n    assert meijerg([], [], [0], [], a).argument == b\n    assert meijerg([0], [], [0], [], a).argument == a\n    assert meijerg([], [0], [0], [], a).argument == b\n    assert meijerg([0, 1], [], [0, 1], [], a).argument == a\n    assert meijerg([0], [0], [0], [0], exp_polar(2*pi*I)).argument == 1\n"], "sample_1099": ["def test_eval_partial_derivative_mixed_expr3():\n\n    tau, alpha, beta = symbols(\"tau alpha beta\")\n\n    base_expr3 = A(i)*A(-i)*tau**alpha + beta*H(j, -j)\n\n    vector_expression = PartialDerivative(base_expr3, A(k))._perform_derivative()\n    tensor_expression = PartialDerivative(base_expr3, H(k, m))._perform_derivative()\n    scalar_expression_tau = PartialDerivative(base_expr3, tau)._perform_derivative()\n    scalar_expression_alpha = PartialDerivative(base_expr3, alpha)._perform_derivative()\n    scalar_expression_beta = PartialDerivative(base_expr3, beta)._perform_derivative()\n\n    assert  (vector_expression -\n        (tau**alpha*L.delta(L_0, -k)*A(-L_0) + tau**alpha*A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k))).expand() == 0\n\n    assert (tensor_expression - beta*L.delta(j, -k)*L.delta(-j, m)) == 0\n\n    assert scalar_expression_tau - alpha*1/tau*tau**alpha*A(L_0)*A(-L_0) == 0\n\n    assert scalar_expression_alpha - tau**alpha*A(L_0)*A(-L_0)*log(tau) == 0\n\n    assert scalar_expression_beta - H(L_0, -L_0) == 0\n", "def test_doit_partial_derivative():\n    # check if doit() does not modify the object\n    expr = PartialDerivative(A(i), A(j))\n    assert expr.doit() == expr\n\n    # check that doit() returns the expanded form of partial derivatives\n    expr1 = PartialDerivative(A(i) + B(i), A(j))\n    assert expr1.doit() == expr1._expand_partial_derivative()\n\n    expr2 = PartialDerivative(A(i)*B(j), A(k))\n    assert expr2.doit() == expr2._expand_partial_derivative()\n\n    # check that doit() works correctly for higher order derivatives\n    expr3 = PartialDerivative(A(i), (A(j), A(k)))\n    assert expr3.doit() == expr3._expand_partial_derivative()\n\n    expr4 = PartialDerivative(A(i)*B(j), (A(k), A(m)))\n    assert expr4.doit() == expr4._expand_partial_derivative()\n", "def test_eval_partial_derivative_non_tensor_valued_input():\n    from sympy import sin, cos\n\n    expr1 = PartialDerivative(sin(A(i)), A(j))\n    expr2 = PartialDerivative(cos(A(i)), A(j))\n\n    assert expr1._perform_derivative() - cos(A(L_0)) * L.delta(L_0, -j) == 0\n    assert expr2._perform_derivative() + sin(A(L_0)) * L.delta(L_0, -j) == 0\n", "def test_eval_partial_derivative_single_2nd_rank_tensors_by_tensor_for_fully_contraced_indices():\n\n    expr1 = PartialDerivative(H(i, -i), H(m, -m))\n    assert expr1._perform_derivative() - L.delta(L_0, -L_0) * L.delta(-L_1, L_1) == 0\n\n    expr2 = PartialDerivative(H(i, -i), H(-m, m))\n    assert expr2._perform_derivative() - L.metric(L_0, L_2) * L.delta(-L_2, m) * L.metric(-L_0, -L_1) * L.delta(L_1, -m) == 0\n\n    expr3 = PartialDerivative(H(-i, i), H(m, -m))\n    assert expr3._perform_derivative() - L.delta(-L_0, m) * L.delta(L_0, -m) == 0\n\n    expr4 = PartialDerivative(H(-i, i), H(-m, m))\n    assert expr4._perform_derivative() - L.metric(-L_0, L_2) * L.delta(L_2, -m) * L.metric(L_0, -L_1) * L.delta(-L_1, m) == 0\n", "def test_eval_partial_derivative_multi_2nd_rank_tensors_by_tensor():\n\n    expr1 = PartialDerivative(H(i, j)*H(k, m), H(m1, m2))\n    assert (expr1._perform_derivative() -\n            H(i, j)*L.delta(k, -m1)*L.delta(m, -m2) -\n            H(k, m)*L.delta(i, -m1)*L.delta(j, -m2)) == 0\n\n    expr2 = PartialDerivative(H(i, j)*H(k, m), H(-m1, m2))\n    assert (expr2._perform_derivative() -\n            H(i, j)*L.metric(k, L_0)*L.delta(-L_0, m1)*L.delta(m, -m2) -\n            H(k, m)*L.metric(i, L_0)*L.delta(-L_0, m1)*L.delta(j, -m2)) == 0\n\n    expr3 = PartialDerivative(H(i, j)*H(k, m), H(m1, -m2))\n    assert (expr3._perform_derivative() -\n            H(i, j)*L.delta(k, -m1)*L.metric(m, L_0)*L.delta(-L_0, m2) -\n            H(k, m)*L.delta(i, -m1)*L.metric(j, L_0)*L.delta(-L_0, m2)) == 0\n\n    expr4 = PartialDerivative(H(i, j)*H(k, m), H(-m1, -m2))\n    assert (expr4._perform_derivative() -\n            H(i, j)*L.metric(k, L_0)*L.delta(-L_0, m1)*L.metric(m, L_1)*L.delta(-L_1, m2) -\n            H(k, m)*L.metric(i, L_0)*L.delta(-L_0, m1)*L.metric(j, L_1)*L.delta(-L_1, m2)) == 0\n"], "sample_1100": ["def test_Pow_is_algebraic():\n    from sympy import sqrt, I\n    a = Symbol('a', algebraic=True)\n    b = Symbol('b', algebraic=True)\n    n = Symbol('n', integer=True)\n    assert (a**n).is_algebraic is True\n    assert (a**x).is_algebraic is None\n    assert ((1 + I)**(1/2)).is_algebraic is True\n    assert ((1 + I)**(1/3)).is_algebraic is True\n    assert ((1 + I)**n).is_algebraic is True\n    assert ((1 + I)**x).is_algebraic is None\n    assert ((1 + sqrt(2))**(1/2)).is_algebraic is True\n    assert ((1 + sqrt(2))**(1/3)).is_algebraic is True\n    assert ((1 + sqrt(2))**n).is_algebraic is True\n    assert ((1 + sqrt(2))**x).is_algebraic is None\n    assert (a**b).is_algebraic is None\n", "def test_issue_19624():\n    assert Mod((2*x - 1)*(x - 1), x - 1) == Mod(0, x - 1)\n    assert Mod((2*x - 1)*(x - 1) + 1, x - 1) == Mod(1, x - 1)\n    assert Mod((2*x - 1)*(x - 1) + x, x - 1) == Mod(1, x - 1)\n", "def test_Pow_rewriting():\n    n = symbols('n', negative=True)\n    p = symbols('p', positive=True)\n    z = symbols('z', zero=True)\n\n    assert Pow(-1, n, evaluate=False).rewrite(exp) == exp(I*pi*n)\n    assert Pow(-1, p, evaluate=False).rewrite(exp) == exp(I*pi*p)\n\n    assert Pow(I, n, evaluate=False).rewrite(exp) == exp(-pi*n/2)\n    assert Pow(I, p, evaluate=False).rewrite(exp) == exp(-pi*p/2)\n\n    assert Pow(-I, n, evaluate=False).rewrite(exp) == exp(pi*n/2)\n    assert Pow(-I, p, evaluate=False).rewrite(exp) == exp(pi*p/2)\n\n    assert Pow(I, z, evaluate=False).rewrite(exp) == exp(-pi*z/2)\n    assert Pow(-1, z, evaluate=False).rewrite(exp) == exp(pi*z*I)\n", "def test_issue_19237():\n    assert sign(zoo) == 1\n    assert sign(-zoo) == -1\n", "def test_Pow_periodicity():\n    assert sin(x)**I == sin(x + 2*pi)**I\n    assert (sin(x)**I).subs(x, 2*pi) == 1\n    assert sin(x)**I != sin(x + 2*pi)**-I\n"], "sample_1101": ["def test_schur_number_eval():\n    assert SchurNumber(5).eval() is None\n    assert SchurNumber(-1) is None\n    raises(ValueError, lambda: SchurNumber(-1).eval())\n    raises(ValueError, lambda: SchurNumber(S.Half).eval())\n    raises(ValueError, lambda: SchurNumber(-S.Half).eval())\n", "def test_schur_number_eval():\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(-5))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    raises(ValueError, lambda: SchurNumber(Rational(3, 4)))\n    k = symbols(\"k\", positive=True, integer=True)\n    assert SchurNumber(k).args[0] == k\n    assert SchurNumber(5).args[0] == 5\n", "def test_SchurNumber_eval():\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(-5))\n    raises(ValueError, lambda: SchurNumber(3.5))\n    raises(ValueError, lambda: SchurNumber(-3.5))\n\n    assert SchurNumber(5).lower_bound() == 121\n    assert SchurNumber(1) == 1\n", "def test_schur_number_eval():\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(-5))\n    assert SchurNumber(5).args == (5,)\n    assert SchurNumber(6).args == (6,)\n    assert SchurNumber(7).args == (7,)\n    k = symbols(\"k\", positive=True, integer=True)\n    assert SchurNumber(k).args == (k,)\n    assert SchurNumber(k).lower_bound() == (3**k - 1)/2\n", "def test_generate_next_list_and_schur_subset_number():\n    assert _schur_subsets_number(1) == 1\n    assert _schur_subsets_number(2) == 1\n    assert _schur_subsets_number(3) == 1\n    assert _schur_subsets_number(4) == 2\n    assert _schur_subsets_number(13) == 3\n\n    current_list = [[1, 4], [2, 3]]\n    new_list = _generate_next_list(current_list, 13)\n    expected_list = [[3, 12], [6, 9], [1, 4, 7, 10]]\n    assert new_list == expected_list\n"], "sample_1104": ["def test_RandomSymbol():\n    from sympy.stats import RandomSymbol\n    assert str(RandomSymbol(Symbol(\"x\"))) == \"x\"\n", "def test_issue_16862():\n    M = Matrix([[x**+1, 1], [y, x + y]])\n    assert sstr(M, abbrev=True) == \"Matrix([\\n[x,     1],\\n[y, x + y]])\"\n    assert sstr(M) == \"Matrix([\\n[x,     1],\\n[y, x + y]])\"\n", "def test_issue_18882():\n    assert str(Tr(x**2)) == 'Tr(x**2)'\n", "def test_issue_19558():\n    from sympy import solve\n    from sympy.abc import x, y\n\n    assert str(solve(x + y, (x, y))) == \"{(-y, y)}\"\n", "def test_issue_16997():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert str(A.det(method='lu')) == 'A.det(method=\"LU\")'\n"], "sample_1103": ["def test_issue_10162():\n    e = (1 + 1.0*I)**(1 + 1.0*I)\n    assert im(e.expand()) == re((1 + I)**(1 + I)).n()\n    assert re(e.expand()) == im((1 + I)**(1 + I)).n()\n\n    a, b, c = symbols('a b c', extended_real=True)\n    assert (a + b*I)**c == exp(c*log(a + b*I))\n    assert log(a + b*I) == log(sqrt(a**2 + b**2)) + atan2(b, a)*I\n", "def test_Mul_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True)\n    assert (x*y).is_integer is True\n    assert (x*y).is_integer is True\n    assert (x*y*y).is_integer is True\n    assert (x/S(3)).is_integer is None\n    assert (x/3).is_integer is None\n    assert (Rational(1, 2)*x).is_integer is None\n    assert (0.5*x).is_integer is None\n    z = Symbol('z')\n    assert (x*z).is_integer is None\n    assert (x*y*z).is_integer is None\n", "def test_issue_19338():\n    assert (x + y + sqrt(x**2 - 2*x*y + y**2)).simplify() == x + y + Abs(x - y)\n    assert (x + y + sqrt(x**2 + 2*x*y + y**2)).simplify() == x + y + x + y\n", "def test_Pow_is_composite():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True, positive=True)\n    assert (x**y).is_composite is None  # x could be a prime\n    assert (4**y).is_composite is True\n    assert ((2*x)**y).is_composite is None\n    assert ((4*x)**y).is_composite is True\n", "def test_Pow_is_algebraic():\n    x = Symbol('x')\n    assert (2**x).is_algebraic is None\n    assert (2**Rational(1, 3)).is_algebraic is True\n    assert (2**Rational(-1, 3)).is_algebraic is True\n    assert (2**Rational(5, 3)).is_algebraic is True\n    assert (2**Rational(-5, 3)).is_algebraic is True\n    assert (2**pi).is_algebraic is None\n    assert (2**I).is_algebraic is None\n    assert (2**(2*I)).is_algebraic is None\n    assert ((-2)**Rational(1, 3)).is_algebraic is True\n    assert ((-2)**Rational(-1, 3)).is_algebraic is True\n    assert ((-2)**Rational(5, 3)).is_algebraic is True\n    assert ((-2)**Rational(-5, 3)).is_algebraic is True\n    assert ((-2)**pi).is_algebraic is None\n    assert ((-2)**I).is_algebraic is None\n    assert ((-2)**(2*I)).is_algebraic is None\n    assert (x**Rational(1, 3)).is_algebraic is None\n    assert (x**Rational(-1, 3)).is_algebraic is None\n    assert (x**Rational(5, 3)).is_algebraic is None\n    assert (x**Rational(-5, 3)).is_algebraic is None\n    assert (x**pi).is_algebraic is None\n    assert (x**I).is_algebraic is None\n    assert (x**(2*I)).is_algebraic is None\n"], "sample_1105": ["def test_issue_14341():\n    X = MatrixSymbol('X', 2, 2)\n    assert MatMul(X, X.T, X, X.T).doit() == X*X.T*X*X.T\n", "def test_MatMul_inputs():\n    # Test invalid inputs\n    with pytest.raises(TypeError):\n        MatMul(1, 'a')\n\n    # Test matrix with no elements\n    with pytest.raises(TypeError):\n        MatMul(MatrixSymbol('A', 0, 1), MatrixSymbol('B', 1, 1))\n\n    # Test non-aligned matrices\n    with pytest.raises(ShapeError):\n        MatMul(MatrixSymbol('A', 1, 2), MatrixSymbol('B', 3, 1))\n\n    # Test non-matrix inputs with more than one args\n    with pytest.raises(TypeError):\n        MatMul(1, 2, 3)\n", "def test_derivative_matrix_lines():\n    from sympy.abc import x\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    expr = MatMul(A, B, C)\n    A_diff = Matrix([\n        [x, x],\n        [x, x]\n    ])\n    B_diff = Matrix([\n        [x, x],\n        [x, x]\n    ])\n    C_diff = Matrix([\n        [x, x],\n        [x, x]\n    ])\n    A_lines = A_diff._eval_derivative_matrix_lines(x)\n    B_lines = B_diff._eval_derivative_matrix_lines(x)\n    C_lines = C_diff._eval_derivative_matrix_lines(x)\n    result_lines = expr._eval_derivative_matrix_lines(x)\n    assert len(result_lines) == len(A_lines) + len(B_lines) + len(C_lines)\n", "def test_entry():\n    M = MatMul(A, B)\n    assert M._entry(1, 2) == Sum(A[1, k]*B[k, 2], (k, 0, m-1)).doit()\n    assert M._entry(1, 2, expand=False) == Sum(A[1, k]*B[k, 2], (k, 0, m-1))\n", "def test_entry():\n    assert (MatMul(A, B))._entry((0, 0), (0, 0)) == A[0, 0]*B[0, 0]\n    assert (MatMul(C, D))._entry((1, 1), (1, 1)) == C[1, 1]*D[1, 1]\n    assert (MatMul(C, D, E))._entry((1, 1), (1, 1)) == Sum(C[1, 0]*D[0, 0]*E[0, 1], (0, 0, n-1))\n"], "sample_1102": ["def test_issue_18613():\n    expr = sin(x**2) + x\n    with warns_deprecated_sympy():\n        factor(expr)\n\n    with warns_deprecated_sympy():\n        factor(expr, deep=True)\n", "def test_issue_18613():\n    # test that warning is raised when mixing Poly with non-polynomial expressions in binary operations\n    expr = x + 1\n    poly = Poly(expr, x)\n    nonpoly = sin(x)\n\n    with warns_deprecated_sympy():\n        poly * nonpoly\n    with warns_deprecated_sympy():\n        poly + nonpoly\n    with warns_deprecated_sympy():\n        poly - nonpoly\n", "def test_issue_20115():\n    p = Poly((x - 2)**2*(x - 1))\n    assert p.real_roots(multiple=False) == [(2, 2), (1, 1)]\n    assert p.real_roots() == [2, 2, 1]\n", "def test_issue_19238():\n    a = Symbol('a')\n    b = Symbol('b')\n    c = Symbol('c')\n    p = a**2 + b**2 - c**2\n    f = factor(p)\n    assert not f.is_Mul\n", "def test_issue_20262():\n    f = Poly(2*x**2 + 3*x + 4, x)\n    g = Poly(x**2 + 1, x)\n    assert f.div(g)[1] == Poly(3*x + 2, x)\n    assert f.div(g, auto=False)[1] == Poly(2*x**2 + 3*x + 4, x)\n"], "sample_1106": ["def test_MatAdd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = eye(2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n    assert X.doit() == MatAdd(A, Matrix([[2, 2], [3, 5]]))\n", "def test_matadd_doit():\n    assert MatAdd(A, ZeroMatrix(n, m), evaluate=False).doit() == A\n    assert MatAdd(ZeroMatrix(n, m), A, evaluate=False).doit() == A\n    assert MatAdd(A, -A, evaluate=False).doit() == ZeroMatrix(n, m)\n    assert MatAdd(A, B, -A, -B, evaluate=False).doit() == ZeroMatrix(n, m)\n", "def test_matadd_evaluate():\n    assert MatAdd(C, C, evaluate=True) == MatAdd(C, C).doit()\n    assert MatAdd(C, ZeroMatrix(n, n), evaluate=True) == C\n    assert MatAdd(ZeroMatrix(n, n), C, evaluate=True) == C\n", "def test_MatAdd_doit():\n    assert MatAdd(A, ZeroMatrix(n, m), evaluate=False).doit() == A\n    assert MatAdd(A, B, -B).doit() == A\n    assert MatAdd(A, -A).doit() == ZeroMatrix(n, m)\n    assert MatAdd(A, A).doit() == 2*A\n", "def test_matadd_identity():\n    assert MatAdd() == ZeroMatrix(n, n)\n    assert MatAdd(A) == A\n    assert MatAdd(A, ZeroMatrix(n, m)) == A\n    assert MatAdd(ZeroMatrix(n, m), A) == A\n"], "sample_1108": ["def test_roundrobin():\n    A = [1, 2, 3]\n    B = ['a', 'b']\n    C = [True, False, True, False]\n    assert list(roundrobin(A, B, C)) == [\n        1, 'a', True, 2, 'b', False, 3, True, False\n    ]\n    assert list(roundrobin(A, B)) == [1, 'a', 2, 'b', 3]\n", "def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert ''.join(gen) == 'ADEBFC'\n    gen = roundrobin([1, 2], [3, 4, 5], [6, 7, 8, 9])\n    assert list(gen) == [1, 3, 6, 2, 4, 7, 5, 8, 9]\n    gen = roundrobin([1, 2], [3], [4, 5, 6, 7, 8, 9])\n    assert list(gen) == [1, 3, 4, 2, 5, 6, 7, 8, 9]\n", "def test_roundrobin():\n    from itertools import islice\n    a = 'ABC'\n    b = (1, 2)\n    c = ['w', 'x', 'y', 'z']\n    assert list(roundrobin(a, b, c)) == [\n        'A', 1, 'w', 'B', 2, 'x', 'C', 'y', 'z']\n    assert list(roundrobin(a, b, c, a)) == [\n        'A', 1, 'w', 'A', 'B', 2, 'x', 'C', 'y', 'z']\n    assert list(islice(roundrobin(a, c), 7)) == [\n        'A', 'w', 'B', 'x', 'C', 'y', 'z']\n    assert list(roundrobin(a, c, a, b, a, c)) == [\n        'A', 'w', 'A', 1, 'A', 'B', 'x', 'C', 'y', 'z', 2]\n", "def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == \\\n        ['A', 'D', 'E', 'B', 'F', 'C']\n", "def test_roundrobin():\n    from itertools import islice\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert list(islice(roundrobin('ABC', 'D', 'EF'), 3)) == ['A', 'D', 'E']\n"], "sample_1107": ["def test_least_rotation():\n    assert least_rotation([3, 1, 5, 1, 2]) == 3\n    assert rotate_left([3, 1, 5, 1, 2], 3) == [1, 2, 3, 1, 5]\n    assert least_rotation([1, 1, 1, 1, 2]) == 4\n    assert rotate_left([1, 1, 1, 1, 2], 4) == [2, 1, 1, 1, 1]\n", "def test_signed_permutations():\n    assert list(signed_permutations((0, 1, 2))) == [\n        (0, 1, 2), (0, 1, -2), (0, -1, 2), (0, -1, -2),\n        (0, 2, 1), (0, 2, -1), (0, -2, 1), (0, -2, -1),\n        (1, 0, 2), (1, 0, -2), (1, 2, 0), (1, 2, -0),\n        (1, -2, 0), (1, -2, -0), (1, -0, 2), (1, -0, -2),\n        (-1, 0, 2), (-1, 0, -2), (-1, 2, 0), (-1, 2, -0),\n        (-1, -2, 0), (-1, -2, -0), (-1, -0, 2), (-1, -0, -2),\n        (2, 0, 1), (2, 0, -1), (2, 1, 0), (2, 1, -0),\n        (2, -1, 0), (2, -1, -0), (2, -0, 1), (2, -0, -1),\n        (-2, 0, 1), (-2, 0, -1), (-2, 1, 0), (-2, 1, -0),\n        (-2, -1, 0), (-2, -1, -0), (-2, -0, 1), (-2, -0, -1),\n    ]\n", "def test_roundrobin():\n    assert list(roundrobin([1, 2, 3], 'abc', [4, 5])) == [\n        1, 'a', 4, 2, 'b', 5, 3, 'c'\n    ]\n    assert list(roundrobin([1, 2], 'abc', [4, 5, 6, 7])) == [\n        1, 'a', 4, 2, 'b', 5, 'c', 6, 7\n    ]\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n"], "sample_1109": ["def test_issue_floor_fraction():\n    x = Symbol('x')\n    assert floor(x/2) == floor(x/2)\n    assert floor(-x/2) == floor(-x/2)\n    assert floor(x/3) == floor(x/3)\n    assert floor(-x/3) == floor(-x/3)\n    assert floor(1/x) == floor(1/x)\n    assert floor(-1/x) == floor(-1/x)\n", "def test_frac_with_real_and_imaginary_parts():\n    x, y = symbols('x, y', real=True)\n    assert frac(x + y*I) == frac(x) + I*frac(y)\n    assert frac(x - y*I) == frac(x) - I*frac(y)\n    assert frac(x + I) == frac(x)\n    assert frac(x - I) == frac(x)\n    assert frac(y*I) == I*frac(y)\n    assert frac(-y*I) == -I*frac(y)\n", "def test_frac_complex():\n    assert frac(2 + 3*I) == 2*I + frac(3*I)\n    assert frac(I) == I\n    assert frac(-I) == -I\n    assert frac(2 + I) == I\n    assert frac(-2 - I) == -I\n    assert frac(0.5 + I) == 0.5 + I\n    assert frac(-0.5 - I) == -0.5 - I\n    assert frac(2.5 + 3*I) == 0.5 + I*frac(3)\n    assert frac(-2.5 - 3*I) == -0.5 - I*frac(3)\n    assert frac(1 + I) == I\n    assert frac(-1 - I) == -I\n    assert frac(1.5 + I) == 0.5 + I\n    assert frac(-1.5 - I) == -0.5 - I\n    assert frac(2 + I).rewrite(floor) == 2*I + (2 + I - floor(2 + I))\n    assert frac(2 + I).rewrite(ceiling) == 2*I + (2 + I + ceiling(-2 - I))\n", "def test_frac_complex():\n    c = I + 1\n    assert frac(c) == I + 1 - floor(c)\n    c = -I + 1\n    assert frac(c) == -I + 1 - floor(c)\n    c = 1 + 2*I\n    assert frac(c) == 2*I + frac(1)\n", "def test_frac_rewrites():\n    x = symbols('x', real=True)\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n    assert frac(x).rewrite(frac) == frac(x)\n    assert frac(-x).rewrite(floor) == -x - floor(-x)\n    assert frac(-x).rewrite(ceiling) == -x + ceiling(x)\n    assert frac(-x).rewrite(frac) == frac(-x)\n"], "sample_1110": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 1, 5)\n    D = MatrixSymbol(\"D\", 3, 4)\n    assert p.doprint(A**(-1)) == \"sympy.MatrixInverse(A)\"\n    assert p.doprint(A**5) == \"sympy.MatrixPower(A, 5)\"\n    assert p.doprint(Identity(3)) == \"sympy.eye(3)\"\n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert p.doprint(S.NaN) == 'sympy.nan'\n    assert p.doprint(S.Infinity) == 'sympy.oo'\n    assert p.doprint(S.NegativeInfinity) == '-sympy.oo'\n", "def test_MpmathPrinter_print_seq():\n    m = MpmathPrinter()\n\n    assert m._print_seq(range(2)) == '(0, 1,)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert p.doprint(A**(-1)) == \"sympy.Matrix(A).inv()\"\n    assert p.doprint(A**5) == \"sympy.Matrix(A)**5\"\n    assert p.doprint(A/B) == \"sympy.Matrix(A) * sympy.Matrix(B).inv()\"\n    assert p.doprint(Identity(3)) == \"sympy.eye(3)\"\n", "def test_PythonCodePrinter_print_Indexed():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(p[0, 1, 2]) == 'p[0, 1, 2]'\n    assert prntr.doprint(p[0, 1, 2, 3]) == 'p[0, 1, 2, 3]'\n", "def test_CodegenArrayTensorProduct():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n    from sympy.abc import k\n    expr = CodegenArrayTensorProduct(x*y, [[0, 1]], [k])\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.einsum(x*y, [0, 1], [k])'\n"], "sample_1111": ["def test_rescale():\n    y = [1, 2, 3, 4, 5]\n    W = 5\n    H = 10\n    mi = 1\n    ma = 5\n    assert rescale(y, W, H, mi, ma) == [0, 2, 4, 6, 8]\n\n    y = [1, 2, None, 4, 5]\n    assert rescale(y, W, H, mi, ma) == [0, 2, None, 6, 8]\n\n    y = [1, 2, float('inf'), 4, 5]\n    assert rescale(y, W, H, mi, ma) == [0, 2, None, 6, 8]\n\n    y = [1, 2, float('nan'), 4, 5]\n    assert rescale(y, W, H, mi, ma) == [0, 2, None, 6, 8]\n", "def test_linspace():\n    assert linspace(0, 10, 5) == [0, 2.5, 5, 7.5, 10]\n    assert linspace(-5, 5, 11) == [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n    assert linspace(10, 0, 5) == [10, 7.5, 5, 2.5, 0]\n", "def test_textplot_str_precision():\n    x = Symbol('x')\n    lines = list(textplot_str(x**2, 0, 2))\n    assert len(lines) == 22\n\n    for h, line in enumerate(lines):\n        if h == 0 or h == 10 or h == 21:\n            parts = line.split('|')\n            assert len(parts) == 2\n            precision = len(parts[0].strip())\n            assert precision <= 7\n", "def test_constant_expression():\n    lines = [\n        '     10 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '     -0 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(10, -1, 1)) == lines\n", "def test_invalid_input():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test that an expression with more than one variable raises an error\n    try:\n        list(textplot_str(x + y, 0, 1))\n    except ValueError as e:\n        assert str(e) == \"The expression must have a single variable. (Got {x, y})\"\n    else:\n        assert False, \"Expected ValueError not raised\"\n\n    # Test that a non-numeric interval raises an error\n    try:\n        list(textplot_str(x, 'a', 1))\n    except TypeError as e:\n        assert str(e) == \"a and b should be numbers\"\n    else:\n        assert False, \"Expected TypeError not raised\"\n\n    # Test that a non-numeric value for W raises an error\n    try:\n        list(textplot_str(x, 0, 1, W='a'))\n    except TypeError as e:\n        assert str(e) == \"W and H should be integers\"\n    else:\n        assert False, \"Expected TypeError not raised\"\n\n    # Test that a non-numeric value for H raises an error\n    try:\n        list(textplot_str(x, 0, 1, H='a'))\n    except TypeError as e:\n        assert str(e) == \"W and H should be integers\"\n    else:\n        assert False, \"Expected TypeError not raised\"\n"], "sample_1112": ["def test_is_palindromic_non_default_base():\n    assert is_palindromic(0o171, 8)\n    assert is_palindromic(0o121, 8)\n    assert not is_palindromic(88, 8)\n    assert not is_palindromic(0o123, 8)\n", "def test_is_palindromic_edge_cases():\n    assert is_palindromic(0)  # single digit number\n    assert is_palindromic(1)  # single digit number\n    assert is_palindromic(-1)  # single digit negative number\n    assert is_palindromic(0, 2)  # single digit number in base 2\n    assert is_palindromic(0, 8)  # single digit number in base 8\n    assert is_palindromic(0, 16)  # single digit number in base 16\n", "def test_digits_invalid_base():\n    raises(ValueError, lambda: digits(123, 1))\n    raises(ValueError, lambda: digits(123, 0))\n    raises(ValueError, lambda: digits(123, -2))\n    raises(ValueError, lambda: digits(-123, 1))\n    raises(ValueError, lambda: digits(-123, 0))\n    raises(ValueError, lambda: digits(-123, -2))\n", "def test_digits_errors():\n    raises(ValueError, lambda: digits(123, 1))\n    raises(ValueError, lambda: digits(123, 0))\n    raises(ValueError, lambda: digits(123, -1))\n    raises(ValueError, lambda: digits(123, b='a'))\n    raises(ValueError, lambda: digits(123, digits='a'))\n    raises(ValueError, lambda: digits(123, digits=-1))\n", "def test_edge_cases():\n    raises(ValueError, lambda: digits(123, 1))\n    raises(ValueError, lambda: digits(123, 0))\n    raises(ValueError, lambda: digits(123, -1))\n    raises(ValueError, lambda: digits(123, 2, 0))\n    raises(ValueError, lambda: digits(123, 2, -1))\n    assert digits(0, 10) == [10, 0]\n    assert digits(0, 2) == [2, 0]\n    assert digits(0, 16) == [16, 0]\n    assert count_digits(0, 10) == {}\n    assert count_digits(0, 2) == {}\n    assert count_digits(0, 16) == {}\n    assert is_palindromic(0)\n    assert is_palindromic(0, 2)\n    assert is_palindromic(0, 16)\n"], "sample_1113": ["def test_block_diag_matrix():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    C = BlockDiagMatrix(A, B)\n\n    assert C.shape == (5, 5)\n    assert C.blockshape == (2, 2)\n    assert C.rowblocksizes == [2, 3]\n    assert C.colblocksizes == [2, 3]\n    assert C.blocks == ImmutableMatrix([[A, ZeroMatrix(2, 3)], [ZeroMatrix(3, 2), B]])\n\n    assert C.transpose() == BlockDiagMatrix(A.T, B.T)\n    assert C.inverse() == BlockDiagMatrix(A.I, B.I)\n\n    D = BlockDiagMatrix(A, B)\n    assert (C + D) == BlockDiagMatrix(A + A, B + B)\n    assert (C * D) == BlockDiagMatrix(A * A, B * B)\n", "def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (1, 3))\n    assert B.blocks[0, 0].shape == (1, 1)\n    assert B.blocks[0, 1].shape == (1, 3)\n    assert B.blocks[1, 0].shape == (3, 1)\n    assert B.blocks[1, 1].shape == (3, 3)\n    assert B.as_explicit() == M\n", "def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (1, 3))\n    assert isinstance(B, BlockMatrix)\n    assert B.blocks.shape == (2, 2)\n    assert B.blocks[0, 1] == MatrixSlice(M, (0, 1), (1, 4))\n", "def test_block_collapse():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, n)\n\n    X = BlockMatrix([[A, ZeroMatrix(n, l)], [ZeroMatrix(m, m), B]])\n    Y = BlockMatrix([[ZeroMatrix(n, n), C], [ZeroMatrix(m, l), ZeroMatrix(m, n)]])\n\n    Z = X + Y\n    assert isinstance(Z, BlockMatrix)\n    assert Z.blocks[0, 0] == A\n    assert Z.blocks[0, 1] == C\n    assert Z.blocks[1, 0] == ZeroMatrix(m, m)\n    assert Z.blocks[1, 1] == B\n\n    Z = X * Y\n    assert isinstance(Z, BlockMatrix)\n    assert Z.blocks[0, 0] == ZeroMatrix(n, n)\n    assert Z.blocks[0, 1] == A*C\n    assert Z.blocks[1, 0] == ZeroMatrix(m, m)\n    assert Z.blocks[1, 1] == ZeroMatrix(m, n)\n\n    Z = block_collapse(X + Y)\n    assert isinstance(Z, BlockMatrix)\n    assert Z.blocks[0, 0] == A\n    assert Z.blocks[0, 1] == C\n    assert Z.blocks[1, 0] == ZeroMatrix(m, m)\n    assert Z.blocks[1, 1] == B\n\n    Z = block_collapse(X * Y)\n    assert isinstance(Z, BlockMatrix)\n    assert Z.blocks[0, 0] == ZeroMatrix(n, n)\n    assert Z.blocks[0, 1] == A*C\n    assert Z.blocks[1, 0] == ZeroMatrix(m, m)\n    assert Z.blocks[1, 1] == ZeroMatrix(m, n)\n", "def test_block_diagonal_matrix():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 2, 2)\n\n    block_diag = BlockDiagMatrix(A, B, C)\n    assert block_diag.blocks[0, 0] == A\n    assert block_diag.blocks[1, 1] == B\n    assert block_diag.blocks[2, 2] == C\n\n    assert block_diag.blockshape == (3, 3)\n    assert block_diag.shape == (7, 7)\n\n    assert block_diag._eval_transpose() == BlockDiagMatrix(A.T, B.T, C.T)\n    assert block_diag._eval_inverse() == BlockDiagMatrix(A.I, B.I, C.I)\n"], "sample_1114": ["def test_issue_18999():\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True)\n    assert Range(n, m).is_finite_set is None\n    assert Range(n, n + m).is_finite_set is None\n    assert Range(n, n - m).is_finite_set is None\n    assert Range(n, n + 10).is_finite_set is True\n    assert Range(n, n - 10).is_finite_set is True\n    assert Range(n, n).is_finite_set is True\n", "def test_issue_18999():\n    n = Symbol('n', integer=True)\n    assert Range(n, n + 50).is_finite_set is True\n    assert Range(n, n - 50).is_finite_set is True\n    assert Range(n, n + 50).size == 50\n    assert Range(n, n - 50).size == 50\n    assert Range(n, n + 50, 2).is_finite_set is True\n    assert Range(n, n - 50, -2).is_finite_set is True\n    assert Range(n, n + 50, 2).size == 25\n    assert Range(n, n - 50, -2).size == 25\n", "def test_issue_19623():\n    # Check that ImageSet and Range can intersect\n    assert Intersection(ImageSet(Lambda(x, x), S.Integers), Range(10)) == Range(10)\n", "def test_issue_19847():\n    assert Range(1, 10, 2).as_relational(x) == \\\n        And(Eq(x, floor(x)), x >= 1, x <= 9)\n    raises(ValueError, lambda: Range(x, y, 2).as_relational(x))\n", "def test_issue_19455():\n    assert Range(-oo, 1).contains(2) == False\n    assert Range(-oo, -1, -2).contains(0) == False\n    assert Range(1, oo).contains(-1) == False\n    assert Range(1, oo, 2).contains(2) == True\n    assert Range(-oo, 1, -2).contains(0) == False\n    assert Range(1, oo, 2).contains(1) == False\n"], "sample_1116": ["def test_inverse_derivative_matrix_lines():\n    from sympy import Symbol\n    A = MatrixSymbol('A', 2, 2)\n    x = Symbol('x')\n    lines = Inverse(A)._eval_derivative_matrix_lines(x)\n    for line in lines:\n        assert line.first_pointer == -A.T\n        assert line.second_pointer == A.I\n", "def test_refine_unitary():\n    assert refine(C.I, Q.unitary(C)) == C.conjugate()\n", "def test_inverse_derivative_matrix_lines():\n    from sympy.abc import x\n    A = MatrixSymbol('A', 2, 2)\n    A_diff = A.diff(x)\n    assert (Inverse(A)._eval_derivative_matrix_lines(x) ==\n            [A_difflei(first_pointer=-A.T, second_pointer=Inverse(A))])\n", "def test_derivative_matrix_lines():\n    from sympy.symbols import x\n    A = MatrixSymbol('A', 2, 2)\n    A_diff = A.diff(x)\n    A_inv = A.I\n    lines = A_inv._eval_derivative_matrix_lines(x)\n    assert len(lines) == 1\n    assert lines[0].first_pointer == -A_inv.T * A_diff\n    assert lines[0].second_pointer == A_inv\n", "def test_refine_unitary():\n    X = MatrixSymbol('X', 2, 2)\n    assert refine(X.I, Q.unitary(X)) == X.conjugate()\n"], "sample_1115": ["def test_TensorElement():\n    Lorentz = TensorIndexType(\"Lorentz\", dummy_name=\"L\")\n    i, j = tensor_indices(\"i j\", Lorentz)\n\n    # Create a tensor element with a tensor head\n    A = TensorHead(\"A\", [Lorentz]*2)\n    te = TensorElement(A(i, j), {i: 2})\n\n    # Check that the tensor element has the correct free indices\n    assert te.get_free_indices() == [j]\n    assert te.free_args == [j]\n\n    # Check that the tensor element has the correct properties\n    assert te.coeff == 1\n    assert te.nocoeff == te\n\n    # Check that the tensor element can be replaced with another index\n    te2 = te._replace_indices({j: i})\n    assert te2.get_free_indices() == [i]\n", "def test_tensor_element():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    i, j, k, l, m, n, p, q = tensor_indices('i,j,k,l,m,n,p,q', Lorentz)\n    R = TensorHead('R', [Lorentz]*4, TensorSymmetry.riemann())\n    te = TensorElement(R(i,j,k,l), {i: 1})\n    assert get_free_indices(te) == [j, k, l]\n    assert get_indices(te) == [1, j, k, l]\n    assert te.rank == 3\n    assert te.free_args == [j, k, l]\n    assert te.data == R.data\n", "def test_tensor_element():\n    Lorentz = TensorIndexType(\"Lorentz\", dim=4)\n    i, j, k, l = tensor_indices(\"i j k l\", Lorentz)\n    A = TensorHead(\"A\", [Lorentz]*2)\n\n    expr = TensorElement(A(i, j), {i: 2})\n    assert expr.free == [(j, 1)]\n    assert expr.free_args == [j]\n    assert expr.dum == []\n    assert expr.coeff == 1\n\n    expr = TensorElement(A(i, j), {j: 3})\n    assert expr.free == [(i, 0)]\n    assert expr.free_args == [i]\n    assert expr.dum == []\n    assert expr.coeff == 1\n\n    expr = TensorElement(A(i, j), {i: 2, j: 3})\n    assert expr.free == []\n    assert expr.free_args == []\n    assert expr.dum == []\n    assert expr.coeff == 1\n\n    expr = TensorElement(A(i, j), {i: 5})\n    assert expr.free == [(j, 1)]\n    assert expr.free_args == [j]\n    assert expr.dum == []\n    assert expr.coeff == 1\n\n    raises(ValueError, lambda: expr._extract_data({}))\n", "def test_tensor_element():\n    Lorentz = TensorIndexType(\"Lorentz\", dim=4)\n    mu, nu = tensor_indices('mu nu', Lorentz)\n    A = TensorHead(\"A\", [Lorentz, Lorentz])\n    te = TensorElement(A(mu, nu), {mu: 1})\n    assert te.free == [(nu, 1)]\n    assert te.free_args == [nu]\n    assert te.get_free_indices() == [nu]\n    assert te.indices == [1, nu]\n\n    te2 = TensorElement(A(mu, nu), {mu: 1, nu: 2})\n    assert te2.free == []\n    assert te2.free_args == []\n    assert te2.get_free_indices() == []\n    assert te2.indices == [1, 2]\n", "def test_TensorHead_Lorentz_type():\n    i, j = symbols(\"i j\")\n    Lorentz = TensorIndexType(\"Lorentz\")\n    A = TensorHead(\"A\", [Lorentz, Lorentz])\n    assert A(i, j) == A(j, i)\n"], "sample_1117": ["def test_matrix_element_sets_inverse():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.real_elements(X.I), Q.real_elements(X) & Q.invertible(X))\n    assert ask(Q.complex_elements(X.I), Q.complex_elements(X) & Q.invertible(X))\n    assert not ask(Q.integer_elements(X.I), Q.integer_elements(X) & Q.invertible(X))\n", "def test_integer_elements_inverse():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.integer_elements(X.I), Q.integer_elements(X) & Q.fullrank(X)) is None\n    assert ask(Q.integer_elements(X.I), Q.integer_elements(X) & Q.det(X).equals(1)) is True\n", "def test_matrix_element_sets_inverse_transpose():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.real_elements(X.I), Q.real_elements(X) & Q.invertible(X))\n    assert ask(Q.integer_elements(X.I), Q.integer_elements(X) & Q.invertible(X)) is None\n    assert ask(Q.complex_elements(X.I), Q.complex_elements(X) & Q.invertible(X))\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n    assert ask(Q.complex_elements(X.T), Q.complex_elements(X))\n", "def test_matrix_element_sets_transpose_inverse():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n    assert ask(Q.complex_elements(X.T), Q.complex_elements(X))\n    assert ask(Q.real_elements(X.I), Q.real_elements(X) & Q.invertible(X))\n    assert ask(Q.integer_elements(X.I), Q.integer_elements(X) & Q.invertible(X)) is None\n    assert ask(Q.complex_elements(X.I), Q.complex_elements(X) & Q.invertible(X))\n", "def test_matrix_element_sets_inverse_transpose():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.real_elements(X.I), Q.real_elements(X) & Q.invertible(X))\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.complex_elements(X.I), Q.complex_elements(X) & Q.invertible(X))\n    assert ask(Q.complex_elements(X.T), Q.complex_elements(X))\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n    assert not ask(Q.integer_elements(X.I), Q.integer_elements(X))\n"], "sample_1118": ["def test_matpow():\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 3).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n    raises(NonSquareMatrixError, lambda: MatPow(A, -2).doit())\n\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, -2).shape == (n, n)\n    assert MatPow(C, 2).base == C\n    assert MatPow(C, 2).exp == 2\n\n    A = MatrixSymbol('A', 3, 3)\n    assert MatPow(MatPow(A, 2), 3).doit() == MatPow(A, 6).doit()\n    assert MatPow(MatPow(A, -2), 3).doit() == MatPow(A, -6).doit()\n", "def test_matpow_doit():\n    A = MatrixSymbol('A', 3, 3)\n    assert MatPow(A, 0).doit() == Identity(3)\n    assert MatPow(A, 1).doit() == A\n    assert MatPow(A, -1).doit() == Inverse(A)\n    assert MatPow(A, 2).doit() == A*A\n    assert MatPow(A, 3).doit() == A*A*A\n    assert MatPow(Identity(3), 3).doit() == Identity(3)\n    assert MatPow(ZeroMatrix(3, 3), 3).doit() == ZeroMatrix(3, 3)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(3, 3), -1).doit())\n", "def test_matpow():\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, -1).doit() == C.I\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 3).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n\n    # Test _entry method\n    assert (MatPow(C, 2))._entry(0, 0) == (C*C)._entry(0, 0)\n    assert (MatPow(C, -2))._entry(0, 0) == (C.I*C.I)._entry(0, 0)\n\n    # Test doit for MatPow with PermutationMatrix\n    from sympy.matrices.expressions import PermutationMatrix\n    p = PermutationMatrix((1, 0))\n    assert MatPow(p, 2).doit() == Identity(2)\n\n    # Test doit for MatPow with other MatPow\n    assert MatPow(MatPow(C, 2), 3).doit() == MatPow(C, 6).doit()\n", "def test_matpow():\n    assert MatPow(C, n).shape == C.shape\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(Identity(n), n) == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), n) == ZeroMatrix(n, n)\n    assert MatPow(C, -1) == Inverse(C)\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n    raises(TypeError, lambda: MatPow(n, n))\n", "def test_matpow_doit():\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 3).doit() == ZeroMatrix(n, n)\n\n    # MatPow of MatPow should be simplified\n    assert MatPow(MatPow(C, 2), 3).doit() == MatPow(C, 6).doit()\n    assert MatPow(MatPow(C, -2), -3).doit() == MatPow(C, 6).doit()\n\n    # MatPow of non-square matrix should raise error\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n"], "sample_1119": ["def test_inverse_matpow_simplification():\n    A = MatrixSymbol('A', 3, 3)\n    assert Inverse(A**3) == (A**-1)**3\n    assert Inverse(A**-3) == A**3\n    assert Inverse(A**0) == Identity(3)\n    assert (A**-3).inverse() == A**3\n    assert (A**0).inverse() == Identity(3)\n    assert (A**3).inverse() == (A**-1)**3\n", "def test_matpow_non_integer_power():\n    A = MatrixSymbol('A', 3, 3)\n    raises(TypeError, lambda: MatPow(A, 1.2))\n    raises(TypeError, lambda: MatPow(A, S(2)/3))\n", "def test_inverse_matmul_canonicalization():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert Inverse(A*B).doit(inv_expand=True) == B.I*A.I\n    assert Inverse(A*B).doit(inv_expand=False) == Inverse(A*B)\n", "def test_matpow_inverse_subs():\n    A = MatrixSymbol('A', 3, 3)\n    assert (MatPow(A, -2)).subs(A, eye(3)) == eye(3)\n    assert (MatPow(A, -2)).subs(A, ZeroMatrix(3, 3)) == ZeroMatrix(3, 3).I**2\n    assert (MatPow(A, -2)).doit().subs(A, ZeroMatrix(3, 3)) == ZeroMatrix(3, 3).I**2\n", "def test_inverse_matpow():\n    A = MatrixSymbol('A', 3, 3)\n    assert (A ** -2).doit() == Inverse(A) ** 2\n    assert (A ** -2).doit(inv_expand=False) == Inverse(A ** 2)\n"], "sample_1120": ["def test_matrix_derivative():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n    expr = 2*x*A\n    result = diff(expr, A)\n    assert result == 2*x*Identity(2)\n", "def test_MatrixElement_eq():\n    A = MatrixSymbol('A', 2, 2)\n    assert Eq(A[0, 0], A[0, 0]) == S.true\n    assert Eq(A[0, 0], A[1, 0]) == S.false\n    assert Eq(A[0, 0], 0) == S.false\n    assert Eq(A[0, 0], A[0, 1]) == S.false\n", "def test_equals():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert A.equals(B) is None\n    assert A.equals(A) is None\n    A_val = Matrix([[1, 2], [3, 4]])\n    B_val = Matrix([[1, 2], [3, 5]])\n    assert A_val.equals(B_val) == False\n    assert A_val.equals(A_val) == True\n", "def test_applyfunc():\n    A = MatrixSymbol('A', 2, 2)\n    f = lambda x: x**2\n    assert (A.applyfunc(f).as_explicit() == Matrix([[A[0, 0]**2, A[0, 1]**2], [A[1, 0]**2, A[1, 1]**2]])).simplify()\n", "def test_matrix_element_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A[0, 0] + A[1, 1]\n    assert expr.subs(A, B) == B[0, 0] + B[1, 1]\n    assert expr.subs(A, Matrix([[1, 2], [3, 4]])) == 1 + 4\n"], "sample_1121": ["def test_Mul_is_algebraic():\n    a = Symbol('a', algebraic=True)\n    b = Symbol('b', algebraic=True)\n    c = Symbol('c', algebraic=False)\n    assert (a * b).is_algebraic is True\n    assert (a * c).is_algebraic is False\n    assert (a * 1).is_algebraic is True\n    assert ((3 + 4*I) * a).is_algebraic is True\n    assert ((3 + 4*I) * (a + b)).is_algebraic is True\n", "def test_Mul_with_Symbol():\n    a = Symbol('a')\n    b = Symbol('b')\n    c = Symbol('c')\n    assert Mul(a, b, c) == a * b * c\n    assert Mul(a, b, c).is_Mul\n", "def test_Mul_with_oo_in_coeff():\n    assert (x*S.Infinity).is_Mul\n    assert (x*S.Infinity).is_finite is False\n    assert (x*S.Infinity).is_zero is False\n    assert (x*S.Infinity).is_number is False\n    assert (x*S.Infinity).is_commutative is None\n\n    assert (x*S.NegativeInfinity).is_Mul\n    assert (x*S.NegativeInfinity).is_finite is False\n    assert (x*S.NegativeInfinity).is_zero is False\n    assert (x*S.NegativeInfinity).is_number is False\n    assert (x*S.NegativeInfinity).is_commutative is None\n\n    assert (x*S.ComplexInfinity).is_Mul\n    assert (x*S.ComplexInfinity).is_finite is False\n    assert (x*S.ComplexInfinity).is_zero is False\n    assert (x*S.ComplexInfinity).is_number is False\n    assert (x*S.ComplexInfinity).is_commutative is None\n", "def test_Mul_as_coeff_Mul():\n    assert (2*x*y).as_coeff_Mul() == (2, x*y)\n    assert (0*x*y).as_coeff_Mul() == (0, 1)\n    assert (x*y).as_coeff_Mul() == (1, x*y)\n    assert (-2*x*y).as_coeff_Mul() == (-2, x*y)\n    assert (-x*y).as_coeff_Mul() == (-1, x*y)\n", "def test_Mul_eval_derivative():\n    a = Symbol('a')\n    b = Symbol('b')\n    f = a * b\n    assert f.diff(a) == b\n    assert f.diff(b) == a\n    assert f.diff(a, 2) == S.Zero\n    assert f.diff(b, 2) == S.Zero\n"], "sample_1122": ["def test_issue_17360():\n    from sympy import acos, asin, exp_polar\n    z = Symbol('z')\n    assert abs(acos(z)).rewrite(exp_polar) == acos(z)\n    assert abs(asin(z)).rewrite(exp_polar) == asin(z)\n", "def test_unbranched_argument():\n    from sympy import unbranched_argument, exp_polar, pi, I\n    assert unbranched_argument(exp_polar(2*pi*I)) == 2*pi\n    assert unbranched_argument(exp_polar(-7*pi*I)) == -7*pi\n    assert unbranched_argument(exp_polar(5 - 3*pi*I/4)) == -3*pi/4\n", "def test_issue_17969():\n    # Ensure that re and im work with MatrixSymbol\n    from sympy import MatrixSymbol, re, im\n    A = MatrixSymbol('A', 2, 2)\n    assert re(A) == (A + A.conjugate()) / 2\n    assert im(A) == (A - A.conjugate()) / (2 * I)\n", "def test_issue_14238_matrix():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    M = Matrix([[x, y], [y, x]])\n    assert Abs(M) == Matrix([[Abs(x), Abs(y)], [Abs(y), Abs(x)]])\n    M = ImmutableMatrix([[x, y], [y, x]])\n    assert Abs(M) == ImmutableMatrix([[Abs(x), Abs(y)], [Abs(y), Abs(x)]])\n    M = SparseMatrix([[x, y], [y, x]])\n    assert Abs(M) == SparseMatrix([[Abs(x), Abs(y)], [Abs(y), Abs(x)]])\n    M = ImmutableSparseMatrix([[x, y], [y, x]])\n    assert Abs(M) == ImmutableSparseMatrix([[Abs(x), Abs(y)], [Abs(y), Abs(x)]])\n    M = MatrixSymbol('M', 2, 2)\n    assert Abs(M) == M.applyfunc(Abs)\n", "def test_abs_diff():\n    x = Symbol('x', real=True)\n    f = Function('f', real=True)\n    assert Abs(f(x)).diff(f(x)) == sign(f(x))\n    assert Abs(f(x)).diff(f(x), 2) == DiracDelta(f(x))\n"], "sample_1123": ["def test_conditionset_as_relational():\n    relational = ConditionSet(x, x > 0, S.Reals).as_relational(1)\n    assert relational == And(1 > 0, Contains(1, S.Reals))\n\n    relational = ConditionSet(x, x > 0, S.Reals).as_relational(-1)\n    assert relational == And(-1 > 0, Contains(-1, S.Reals))\n\n    relational = ConditionSet(x, x > 0, S.Reals).as_relational(y)\n    assert relational == And(y > 0, Contains(y, S.Reals))\n", "def test_CondSet_as_relational():\n    sin_sols_principal = ConditionSet(x, Eq(sin(x), 0),\n                                      Interval(0, 2*pi, False, True))\n    assert sin_sols_principal.as_relational(pi) == And(Eq(sin(pi), 0), Contains(pi, Interval(0, 2*pi, False, True)))\n    assert sin_sols_principal.as_relational(3*pi) == And(Eq(sin(3*pi), 0), Contains(3*pi, Interval(0, 2*pi, False, True)))\n    assert sin_sols_principal.as_relational(5) == And(Eq(sin(5), 0), Contains(5, Interval(0, 2*pi, False, True)))\n", "def test_CondSet_sym_property():\n    I = S.Integers\n    c = ConditionSet(x, x < 1, I)\n    assert c.sym == x\n    assert c.condition == x < 1\n    assert c.base_set == I\n\n    c = ConditionSet((x, y), Eq(x + 1, 0) & Eq(x + y, 0), S.Reals)\n    assert c.sym == (x, y)\n    assert c.condition == Eq(x + 1, 0) & Eq(x + y, 0)\n    assert c.base_set == S.Reals\n\n    c = ConditionSet(x + 1, x + 1 < 1, I)\n    assert isinstance(c.sym, Symbol)\n    assert c.condition == x + 1 < 1\n    assert c.base_set == I\n", "def test_ConditionSet_as_relational():\n    cs = ConditionSet(x, x**2 > 4, S.Reals)\n    assert cs.as_relational(5) == And(5**2 > 4, Contains(5, S.Reals))\n    assert cs.as_relational(1) == And(1**2 > 4, Contains(1, S.Reals))\n    assert cs.as_relational(w) == And(w**2 > 4, Contains(w, S.Reals))\n    assert cs.as_relational(0) == And(0**2 > 4, Contains(0, S.Reals))\n", "def test_conditionset_lambda():\n    C = ConditionSet\n    I = S.Integers\n    c = C(x, x < 1, I)\n    assert c._eval_subs(x, lambda x: x**2) == C(x, x < 1, I)\n    c = C(lambda x: x**2, lambda x: x < 1, I)\n    assert c._eval_subs(x, y) == C(lambda y: y**2, lambda y: y < 1, I)\n\n    raises(ValueError, lambda: C(lambda x, y: x + y, lambda x: x < 1, I))\n    raises(ValueError, lambda: C(lambda x: x**2, lambda x, y: x + y < 1, I))\n"], "sample_1124": ["def test_FracElement_to_poly():\n    F, x, y = field(\"x,y\", ZZ)\n    f = (x**2 + 3*y)/y\n\n    raises(ValueError, lambda: f.to_poly())\n    assert (f.limit_denom()).to_poly() == x**2 + 3*y\n", "def test_FracField_to_ring():\n    F, x, y, z = field(\"x,y,z\", ZZ)\n    R = F.to_ring()\n\n    assert R == PolyRing(F.symbols, F.domain, F.order)\n    assert R != PolyRing(F.symbols, QQ, F.order)\n", "def test_FracField_to_domain():\n    F, x,y,z = field(\"x,y,z\", ZZ)\n    assert F.to_domain() == F\n", "def test_FracElement_to_poly():\n    F, x,y,z = field(\"x,y,z\", ZZ)\n    f = (x**2 + 3*y)/z\n    raises(ValueError, lambda: f.to_poly())\n    f = (x**2 + 3*y)/1\n    assert f.to_poly() == x**2 + 3*y\n\n    Fuv, u,v = field(\"u,v\", ZZ)\n    Fxyzt, x,y,z,t = field(\"x,y,z,t\", Fuv)\n    f = ((u + 1)*x*y + 1)/((v - 1)*z - t*u*v - 1)\n    raises(ValueError, lambda: f.to_poly())\n    f = ((u + 1)*x*y + 1)/1\n    assert f.to_poly() == (u + 1)*x*y + 1\n", "def test_FracElement_set_field():\n    F, x,y = field(\"x,y\", QQ)\n    f = (7*x - 9)/y\n\n    F2 = FracField(\"x,y\", QQ, lex)\n    g = f.set_field(F2)\n\n    assert f == g\n    assert f is not g\n"], "sample_1125": ["def test_operator_getitem():\n    O = Operator('O')\n    assert O[0] == O.label[0]\n    assert O.args[0] == O.label[0]\n    assert O.label == ('O',)\n", "def test_hermitian_operator():\n    H = Operator('H', is_hermitian=True)\n    assert Dagger(H) == H\n    assert H.inv() == H\n    assert H._eval_power(-1) == H\n\n    O = Operator('O')\n    assert Dagger(O) != O\n\n    U = Operator('U', is_unitary=True)\n    assert Dagger(U) == U.inv()\n    assert U.inv() == U._eval_inverse()\n", "def test_operator():\n    A = Operator('A')\n    B = Operator('B')\n    assert A*B != B*A\n    assert A.is_commutative is False\n    assert B.is_commutative is False\n    assert (A+B)**3 == A*B*A + A*B**2 + A**2*B + A**3 + B*A*B + B*A**2 + B**2*A + B**3\n    assert A.inv() == A**(-1)\n    assert A*A.inv() == 1\n", "def test_operator_mul():\n    O = Operator('O')\n    O2 = Operator('O2')\n    I = IdentityOperator()\n\n    assert O*I == O\n    assert I*O == O\n    assert O*I*O2 == O*O2\n    assert O*(I*O2) == O*O2\n    assert (O*I)*O2 == O*O2\n", "def test_hermitian_operator():\n    H = Operator('H', is_hermitian=True)\n    assert Dagger(H) == H\n    assert Dagger(H*I) == -I*H\n"], "sample_1126": ["def test_dagger_add():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A + Dagger(B)) == Dagger(A) + Dagger(Dagger(B))\n    assert Dagger(Dagger(A) + Dagger(B)) == Dagger(Dagger(A)) + Dagger(Dagger(B))\n", "def test_power():\n    O = Operator('O')\n    assert Dagger(O**2) == Dagger(O)**2\n    assert Dagger(O**3) == Dagger(O)**3\n    assert Dagger(O**4) == Dagger(O)**4\n", "def test_outer_product_dagger():\n    from sympy.physics.quantum import OuterProduct, Ket, Bra\n    a = Ket('a')\n    b = Bra('b')\n    ab = OuterProduct(a, b)\n    assert Dagger(ab) == OuterProduct(b.dual, a.dual)\n    assert Dagger(ab).dual == ab\n", "def test_dagger_powers():\n    O = Operator('O')\n    assert Dagger(O**2) == Dagger(O)**2\n    assert Dagger(O**3) == Dagger(O)**3\n    assert Dagger(O**4) == Dagger(O)**4\n", "def test_dagger_add():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A + Dagger(B)) == Dagger(A) + Dagger(Dagger(B))\n    assert Dagger(Dagger(A) + B) == Dagger(Dagger(A)) + Dagger(B)\n"], "sample_1128": ["def test_point_locatenew():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * N.x + 5 * B.x)\n    assert P.pos_from(O) == 10 * N.x + 5 * B.x\n    Q = P.locatenew('Q', 10 * N.y + 5 * B.y)\n    assert Q.pos_from(P) == 10 * N.y + 5 * B.y\n    assert Q.pos_from(O) == 10 * N.x + 10 * N.y + 5 * B.x + 5 * B.y\n    assert O.pos_from(Q) == -10 * N.x - 10 * N.y - 5 * B.x - 5 * B.y\n\n    #Test other directions\n    P = O.locatenew('P', 10 * N.x)\n    assert P.pos_from(O) == 10 * N.x\n\n    P = O.locatenew('P', 5 * B.x)\n    assert P.pos_from(O) == 5 * B.x\n", "def test_auto_point_vel_multiple_velocities_at_different_levels():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, u1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P1.set_vel(B, u2 * B.z)\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * B.z)\n    P3 = Point('P3')\n    P3.set_pos(P2, 10 * q1 * B.y)\n    P4 = Point('P4')\n    P4.set_pos(P3, q1 * B.x)\n    assert P4.vel(B) == (u1 + q1.diff(t)) * B.x + (10 * q1.diff(t)) * B.y + (u2 + q1.diff(t)) * B.z\n", "def test_auto_point_vel_if_tree_has_vel_but_inappropriate_frame_for_vel():\n    q1, q2 = dynamicsymbols('q1 q2')\n    B = ReferenceFrame('B')\n    S = ReferenceFrame('S')\n    P = Point('P')\n    P.set_vel(B, q1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P1.set_vel(S, q1 * S.x)\n    raises(ValueError, lambda : P1.vel(B)) # P1's velocity is defined but not in B or any frame that is connected to B\n", "def test_auto_point_vel_multiple_paths():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, u1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P1.set_vel(B, q1 * B.z)\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * B.z)\n    P3 = Point('P3')\n    P3.set_pos(P2, 10 * q1 * B.y)\n    O = Point('O')\n    O.set_vel(B, u2 * B.y)\n    P3.set_pos(O, q1 * B.x + q2 * B.y + q1 * B.z)\n    assert P3.vel(B) == (u2 + 11 * q1.diff(t)) * B.y + u1 * B.x + 2 * q1.diff(t) * B.z\n", "def test_auto_point_vel_connected_frames_multiple_path():\n    t = dynamicsymbols._t\n    q, q1, q2, u = dynamicsymbols('q q1 q2 u')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    C = ReferenceFrame('C')\n    O = Point('O')\n    O.set_vel(N, u * N.x)\n    P = Point('P')\n    P.set_pos(O, q1 * N.x + q2 * B.y)\n    P1 = Point('P1')\n    P1.set_pos(O, q2 * B.y + q1 * C.z)\n    P.set_pos(P1, q1 * N.x - q1 * C.z)\n    N.orient(B, 'Axis', (q, B.x))\n    C.orient(B, 'Axis', (q, B.x))\n    assert P.vel(N) == (u + q1.diff(t)) * N.x + q2.diff(t) * B.y - q2 * q.diff(t) * B.z\n"], "sample_1127": ["def test_is_alternating():\n    a = Permutation(0, 1, 2)\n    b = Permutation(0, 1, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 2, 1)\n    b = Permutation(1, 2, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 1, 2, 3)\n    b = Permutation(0, 3)(1, 2)\n    assert PermutationGroup(a, b).is_alternating == False\n", "def test_is_alternating():\n    a = Permutation(0, 1, 2)\n    b = Permutation(0, 1, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 2, 1)\n    b = Permutation(1, 2, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 1, 2, 3)\n    b = Permutation(0, 3)(1, 2)\n    assert PermutationGroup(a, b).is_alternating == False\n", "def test_is_alternating():\n    a = Permutation(0, 1, 2)\n    b = Permutation(0, 1, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 2, 1)\n    b = Permutation(1, 2, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 1, 2, 3)\n    b = Permutation(0, 3)(1, 2)\n    assert PermutationGroup(a, b).is_alternating == False\n", "def test_symmetric_group_order():\n    for i in range(1, 10):\n        G = SymmetricGroup(i)\n        assert G.order() == factorial(i)\n", "def test_is_transitive():\n    a = Permutation([2, 0, 1, 3, 4, 5])\n    b = Permutation([2, 1, 3, 4, 5, 0])\n    g = PermutationGroup([a, b])\n    assert g.is_transitive() is True\n    assert g.is_transitive(strict=False) is True\n\n    a = Permutation([1, 0, 2, 3, 4, 5])\n    b = Permutation([0, 1, 2, 4, 5, 3])\n    g = PermutationGroup([a, b])\n    assert g.is_transitive() is False\n    assert g.is_transitive(strict=False) is False\n"], "sample_1129": ["def test_diagonal_matrix():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 3, 3)\n    D = MatrixSymbol(\"D\", 1, 3)\n    assert n.doprint(DiagMatrix(D)) == 'numpy.diagflat(D)'\n    assert n.doprint(DiagonalMatrix(A)) == 'numpy.multiply(A, numpy.eye(3, 3))'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n\n    expr = sign(x)\n    assert p.doprint(expr) == 'sympy.functions.elementary.complexes.sign(x)'\n    assert 'sympy' in p.module_imports\n\n    expr = Piecewise((1, Eq(x, 0)),\n                     (2, x > 6))\n    assert p.doprint(expr) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6))'\n\n    expr = expm1(x)\n    assert p.doprint(expr) == 'sympy.exp(x) - 1'\n", "def test_SymPyPrinter():\n    prntr = SymPyPrinter()\n\n    assert not prntr.module_imports\n\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.doprint(Mod(x, 2)) == 'x % 2'\n    assert prntr.doprint(And(x, y)) == 'x & y'\n    assert prntr.doprint(pi) == 'sympy.pi'\n\n    assert prntr.module_imports == {'sympy': {'pi'}}\n\n    assert prntr.doprint(x**Rational(1, 2)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert prntr.module_imports == {'sympy': {'pi', 'sqrt'}}\n\n    assert prntr.doprint(acos(x)) == 'sympy.acos(x)'\n    assert prntr.module_imports == {'sympy': {'pi', 'sqrt', 'acos'}}\n\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6), (sympy.nan, True))'\n", "def test_SymPyPrinter():\n    prntr = SymPyPrinter()\n\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.doprint(Mod(x, 2)) == 'x % 2'\n    assert prntr.doprint(And(x, y)) == 'sympy.And(x, y)'\n    assert prntr.doprint(Or(x, y)) == 'sympy.Or(x, y)'\n\n    assert prntr.doprint(pi) == 'sympy.pi'\n    assert prntr.doprint(x**Rational(1, 2)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(acos(x)) == 'sympy.acos(x)'\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6))'\n    assert prntr.doprint(sign(x)) == 'sympy.sign(x)'\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(KroneckerDelta(x,y)) == 'sympy.KroneckerDelta(x, y)'\n", "def test_SymPyPrinter_print():\n    from sympy import horner, sin, sinh, cos, cosh, tan, tanh, acos, asin, atan, \\\n        acosh, asinh, atanh, atan2, exp, log, sqrt, lowergamma, uppergamma, \\\n        beta, gamma, erf, erfc, polygamma, digamma, loggamma, sign, floor, \\\n        ceiling, frac, pi, E, EulerGamma, Catalan, GoldenRatio, I, re, im, \\\n        conjugate, arg, Abs, exp_polar, sin_polar, tan_polar, cos_polar\n\n    p = SymPyPrinter()\n\n    # Test printing of sympy functions\n    assert p.doprint(sin(x)) == 'sympy.sin(x)'\n    assert p.doprint(sinh(x)) == 'sympy.sinh(x)'\n    assert p.doprint(cos(x)) == 'sympy.cos(x)'\n    assert p.doprint(cosh(x)) == 'sympy.cosh(x)'\n    assert p.doprint(tan(x)) == 'sympy.tan(x)'\n    assert p.doprint(tanh(x)) == 'sympy.tanh(x)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(asin(x)) == 'sympy.asin(x)'\n    assert p.doprint(atan(x)) == 'sympy.atan(x)'\n    assert p.doprint(acosh(x)) == 'sympy.acosh(x)'\n    assert p.doprint(asinh(x)) == 'sympy.asinh(x)'\n    assert p.doprint(atanh(x)) == 'sympy.atanh(x)'\n    assert p.doprint(atan2(x, y)) == 'sympy.atan2(x, y)'\n    assert p.doprint(exp(x)) == 'sympy.exp(x)'\n    assert p.doprint(log(x)) == 'sympy.log(x)'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(lowergamma(x, y)) == 'sympy.lowergamma(x, y)'\n    assert p.doprint(uppergamma(x, y)) == 'sympy.uppergamma(x, y)'\n    assert p.doprint(beta(x, y))"], "sample_1130": ["def test_locatenew():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = P1.locatenew('P2', 10 * N.x)\n    assert P2.pos_from(P1) == 10 * N.x\n    assert P1.pos_from(P2) == -10 * N.x\n", "def test_auto_vel_multiple_points_with_no_defined_velocity():\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    Q = Point('Q')\n    R = Point('R')\n    O.set_pos(P, N.x)\n    P.set_pos(Q, N.y)\n    Q.set_pos(R, N.z)\n    raises(ValueError, lambda: R.vel(N))\n", "def test_auto_vel_mixed_paths():\n    q, u = dynamicsymbols('q u')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    Q = Point('Q')\n    P.set_vel(N, u * N.x)\n    Q.set_vel(N, q * N.y)\n    O.set_pos(P, q * N.z)\n    O.set_pos(Q, u * N.y)\n    with warnings.catch_warnings(): \n        warnings.simplefilter(\"ignore\")\n        assert O.vel(N) == u.diff(dynamicsymbols._t) * N.y + u * N.x\n", "def test_point_vel_with_intermediate_frame():\n    q, u = dynamicsymbols('q u')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    O.set_vel(N, u * N.x)\n    P.set_pos(O, q * B.y)\n    raises(ValueError, lambda: P.vel(N)) # B's orientation wrt N is not defined\n    N.orient(B, 'Axis', (q, B.x))\n    B.set_ang_vel(N, u * B.x)\n    assert P.vel(N) == u * N.x + q.diff(dynamicsymbols._t) * B.y - u * q * B.z\n", "def test_point_acc():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    qdd, q2dd = dynamicsymbols('q q2', 2)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    B.set_ang_vel(N, 5 * B.y)\n    O = Point('O')\n    P = O.locatenew('P', q * B.x)\n    P.set_vel(B, qd * B.x + q2d * B.y)\n    O.set_vel(N, 0)\n    assert P.acc(N) == P.a1pt_theory(O, N, B)\n"], "sample_1131": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == \"sympy.Matrix(A).inv()\"\n    assert p.doprint(A**5) == \"sympy.Matrix(A)**5\"\n    assert p.doprint(Identity(3)) == \"sympy.eye(3)\"\n", "def test_SparseMatrix():\n    from sympy import SparseMatrix\n\n    A = SparseMatrix(2, 2, {(0, 1): 3})\n    p = SciPyPrinter()\n    assert p.doprint(A) == 'scipy.sparse.coo_matrix(([3], ([0], [1])), shape=(2, 2))'\n\n    p = NumPyPrinter()\n    assert p.doprint(A) == '  # Not supported in Python with NumPy:\\n  # SparseMatrix\\nSparseMatrix(2, 2, {(0, 1): 3})'\n\n    p = PythonCodePrinter()\n    assert p.doprint(A) == '  # Not supported in Python:\\n  # SparseMatrix\\nSparseMatrix(2, 2, {(0, 1): 3})'\n", "def test_lambertw():\n    from sympy import lambertw\n\n    expr = lambertw(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.lambertw(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # lambertw\\nlambertw(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # lambertw\\nlambertw(x)'\n", "def test_lambertw():\n    from sympy import lambertw\n\n    expr = lambertw(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.lambertw(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # LambertW\\nlambertw(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # LambertW\\nlambertw(x)'\n", "def test_symPyPrinter():\n    from sympy import sqrt, exp, symbols, MatrixSymbol, Idx\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    x = symbols(\"x\")\n    i = symbols(\"i\")\n    n = symbols(\"n\")\n\n    prntr = SymPyPrinter()\n\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(exp(x)) == 'sympy.exp(x)'\n    assert prntr.doprint(A**-1) == 'sympy.Matrix(A).inv()'\n    assert prntr.doprint(A**5) == 'sympy.Matrix(A)**5'\n    assert prntr.doprint(sqrt(A)) == 'sympy.Matrix(A).sqrt()'\n\n    assert prntr.doprint(Idx(i, n)) == 'i'\n"], "sample_1132": ["def test_multiset():\n    # multiset is used by multiset_combinations so this tests some of the\n    # same code paths but here we test the output of multiset itself\n    assert multiset('aabbc') == {'a': 2, 'b': 2, 'c': 1}\n    assert multiset('abc') == {'a': 1, 'b': 1, 'c': 1}\n    assert multiset([list('abc'), 'a', 'a']) == {\n        list('abc'): 1, 'a': 2}\n    assert multiset(((1, 2), (1, 2), 3, 4, 4, 4, 'a')) == {\n        (1, 2): 2, 3: 1, 4: 3, 'a': 1}\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C'\n    ]\n    assert list(roundrobin([0, 1], [2, 3, 4])) == [0, 2, 1, 3, 4]\n", "def test_roundrobin():\n    from itertools import islice\n    assert ''.join(roundrobin('ABC', 'D', 'EF')) == 'ADEBFC'\n    assert ''.join(roundrobin('ABC', 'D', 'EF', 'G')) == 'ADGBEFC'\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([0, 1], [2, 3], [4])) == [\n        0, 2, 4, 1, 3]\n    assert list(roundrobin([0, 1, 2], islice('abcdefg', 3), [8, 9])) == [\n        0, 'a', 8, 1, 'b', 9, 2]\n", "def test_roundrobin():\n    iterables = [['A', 'B', 'C'], ['D'], ['E', 'F']]\n    expected = ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin(*iterables)) == expected\n", "def test_roundrobin():\n    from itertools import islice\n    items = [Symbol(i) for i in 'abcdefg']\n    slices = [items[:2], items[2:5], items[5:]]\n    assert list(roundrobin(*slices)) == [\n        items[0], items[2], items[5],\n        items[1], items[3], items[6],\n        items[4]]\n"], "sample_1133": ["def test_refraction_angle_invalid_inputs():\n    m1 = Medium('m1', n=1)\n    m2 = Medium('m2', n=1.33)\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    raises(ValueError, lambda: refraction_angle(r1, m2, m1, n))  # n1 < n2 for TIR\n    raises(ValueError, lambda: refraction_angle(2, m1, m2))  # angle of incidence not in range [0:pi/2)\n    raises(TypeError, lambda: refraction_angle(r1, m1, 'n2'))  # invalid n2\n    raises(TypeError, lambda: refraction_angle(r1, 'n1', m2))  # invalid n1\n    raises(TypeError, lambda: refraction_angle(r1, m1, m2, 'n'))  # invalid normal\n    raises(ValueError, lambda: refraction_angle(r1, m1, m2, n, P))  # both normal and plane provided\n    raises(TypeError, lambda: refraction_angle('incident', m1, m2, n))  # invalid incident\n", "def test_refractive_index_of_medium():\n    m1 = Medium('m1', n=1.33)\n    assert refractive_index_of_medium(m1) == 1.33\n    assert refractive_index_of_medium(1.33) == 1.33\n    assert refractive_index_of_medium(1) == 1\n", "def test_refractive_index_of_medium():\n    n = 1.33\n    m = Medium('m', n=n)\n    assert refractive_index_of_medium(m) == n\n    assert refractive_index_of_medium(n) == n\n    raises(TypeError, lambda: refractive_index_of_medium('m'))\n", "def test_refractive_index_of_medium():\n    m = Medium('m', n=2)\n    assert refractive_index_of_medium(m) == 2\n    assert refractive_index_of_medium(2) == 2\n    raises(TypeError, lambda: refractive_index_of_medium('m'))\n", "def test_refractive_index_of_medium():\n    m1 = Medium('m1', n=1.33)\n    m2 = 2\n    assert refractive_index_of_medium(m1) == 1.33\n    assert refractive_index_of_medium(m2) == 2\n"], "sample_1135": ["def test_Mul_is_algebraic():\n    a = Symbol('a', algebraic=True)\n    b = Symbol('b', algebraic=True)\n    c = Symbol('c', algebraic=False)\n    d = Symbol('d', algebraic=False)\n    r = Symbol('r', rational=True)\n    assert (a * b).is_algebraic is True\n    assert (a * c).is_algebraic is None\n    assert (c * d).is_algebraic is None\n    assert (r * a).is_algebraic is True\n    assert (r * c).is_algebraic is False\n", "def test_issue_18898():\n    a = Symbol('a', nonnegative=True)\n    b = Symbol('b', nonnegative=True)\n    assert (a*b)**0.5 == sqrt(a)*sqrt(b)\n", "def test_Mul_hermitian_antihermitian():\n    x = Symbol('x', hermitian=True)\n    y = Symbol('y', antihermitian=True)\n    z = Symbol('z')\n\n    assert (2*x).is_hermitian is True\n    assert (2*x).is_antihermitian is False\n\n    assert (2*y).is_hermitian is False\n    assert (2*y).is_antihermitian is True\n\n    assert (2*z).is_hermitian is None\n    assert (2*z).is_antihermitian is None\n\n    assert (-x).is_hermitian is True\n    assert (-x).is_antihermitian is False\n\n    assert (-y).is_hermitian is False\n    assert (-y).is_antihermitian is True\n\n    assert (-z).is_hermitian is None\n    assert (-z).is_antihermitian is None\n\n    assert (x*y).is_hermitian is False\n    assert (x*y).is_antihermitian is True\n\n    assert (y*x).is_hermitian is False\n    assert (y*x).is_antihermitian is True\n\n    assert (x*z).is_hermitian is None\n    assert (x*z).is_antihermitian is None\n\n    assert (z*x).is_hermitian is None\n    assert (z*x).is_antihermitian is None\n\n    assert (y*z).is_hermitian is None\n    assert (y*z).is_antihermitian is None\n\n    assert (z*y).is_hermitian is None\n    assert (z*y).is_antihermitian is None\n", "def test_Mul_with_NaN():\n    assert (x * nan).is_NaN\n    assert (nan * y).is_NaN\n    assert (2 * nan).is_NaN\n    assert (nan * 3).is_NaN\n    assert (x * nan * y).is_NaN\n    assert (nan * x * y).is_NaN\n    assert (x * y * nan).is_NaN\n", "def test_Mul_as_coefficients_dict():\n    from sympy.abc import a, x, y\n    assert (3*a*x*y).as_coefficients_dict() == {a*x*y: 3}\n    assert _[a] == 0\n"], "sample_1134": ["def test_latex_PartialDerivative():\n    from sympy.tensor.toperators import PartialDerivative\n    x, y = symbols('x y')\n    f = Function('f')\n    assert latex(PartialDerivative(f(x, y), x)) == r'\\frac{\\partial}{\\partial x} f{\\left(x, y \\right)}'\n    assert latex(PartialDerivative(f(x, y), x, x)) == r'\\frac{\\partial^{2}}{\\partial x^{2}} f{\\left(x, y \\right)}'\n    assert latex(PartialDerivative(f(x, y), x, y)) == r'\\frac{\\partial^{2}}{\\partial x \\partial y} f{\\left(x, y \\right)}'\n", "def test_decimal_separator_decimal():\n    assert latex(123.456, decimal_separator='period') == r'123.456'\n    assert latex(123.456, decimal_separator='comma') == r'123{,}456'\n    assert latex(-123.456, decimal_separator='period') == r'-123.456'\n    assert latex(-123.456, decimal_separator='comma') == r'-123{,}456'\n", "def test_latex_adjoint():\n    from sympy import Adjoint, MatrixSymbol, latex, Symbol\n    X = MatrixSymbol('X', 2, 2)\n    omega = Symbol('omega')\n    omega_adjoint = Adjoint(omega * X)\n    assert latex(omega_adjoint) == r\"(\\omega X)^{\\dagger}\"\n    omega_adjoint = Adjoint(omega * X)\n    assert latex(omega_adjoint) == r\"(\\omega X)^{\\dagger}\"\n    omega_adjoint = Adjoint(X * omega)\n    assert latex(omega_adjoint) == r\"(X \\omega)^{\\dagger}\"\n    omega_adjoint = Adjoint(omega * X * omega)\n    assert latex(omega_adjoint) == r\"(\\omega X \\omega)^{\\dagger}\"\n    omega_adjoint = Adjoint(X * omega * X)\n    assert latex(omega_adjoint) == r\"(X \\omega X)^{\\dagger}\"\n    omega_adjoint = Adjoint(omega)\n    assert latex(omega_adjoint) == r\"\\omega^{\\dagger}\"\n", "def test_latex_decimal_separator_decimal():\n    assert Float(1.5).as_decimal() == \"1.5\"\n    assert latex(1.5, decimal_separator='comma') == r\"1{,}5\"\n    assert latex(1.5, decimal_separator='period') == r\"1.5\"\n", "def test_multiline_latex_fold_short_frac():\n    a, b, c, d, e, f = symbols('a b c d e f')\n    expr = a/b + c/d + e/f\n    expected = r\"\\begin{align*}\" + \"\\n\"\\\n    r\"f = & a / b \\nonumber\\\\\" + \"\\n\"\\\n    r\"& + c / d \\nonumber\\\\\" + \"\\n\"\\\n    r\"& + e / f\" + \"\\n\"\\\n    r\"\\end{align*}\"\n    assert multiline_latex(f, expr, fold_short_frac=True) == expected\n"], "sample_1136": ["def test_issue_20429():\n    f = Poly(1.0*x**2 + 2.0, x)\n    assert f.subs(x, 1.0) == 3.0\n    assert f.subs({x: 1.0}) == 3.0\n", "def test_issue_20848():\n    expr = sin(x)**2 + cos(x)**2 - 1\n    assert factor(expr, deep=True) == 0\n", "def test_Poly_get_domain_error():\n    poly = Poly(x + y, x, y, domain='QQ')\n    raises(CoercionFailed, lambda: poly.get_domain().of_type(RR))\n", "def test_issue_20564():\n    p = Poly(x**2 + 2*x + 1, domain='ZZ')\n    assert p.is_primitive is True\n    assert p.is_monic is True\n    assert p.LC() == 1\n    assert p.content() == 1\n\n    p = Poly(x**2 + 2*x + 1, domain='QQ')\n    assert p.is_primitive is False\n    assert p.is_monic is True\n    assert p.LC() == 1\n    assert p.content() == 1\n\n    p = Poly(2*x**2 + 4*x + 2, domain='ZZ')\n    assert p.is_primitive is True\n    assert p.is_monic is False\n    assert p.LC() == 2\n    assert p.content() == 2\n\n    p = Poly(2*x**2 + 4*x + 2, domain='QQ')\n    assert p.is_primitive is True\n    assert p.is_monic is False\n    assert p.LC() == 2\n    assert p.content() == 1\n", "def test_EX():\n    a = EX(2*x)\n    assert isinstance(a.ex, Add) is True\n    assert a.ex == 2*x\n    \n    b = EX(2*x + 3)\n    assert isinstance(b.ex, Add) is True\n    assert b.ex == 2*x + 3\n    \n    c = EX((x + 2)*(x + 3))\n    assert isinstance(c.ex, Mul) is True\n    assert c.ex == (x + 2)*(x + 3)\n\n    d = EX(x + 2, x)\n    assert isinstance(d.ex, Add) is True\n    assert d.free_symbols == {x}\n    \n    e = EX(x + 2, y)\n    assert isinstance(e.ex, Add) is True\n    assert e.free_symbols == {x}\n    \n    f = EX(sqrt(2))\n    assert isinstance(f.ex, Pow) is False\n    \n    g = EX(sqrt(2) + pi)\n    assert isinstance(g.ex, Add) is True\n    \n    h = EX(sqrt(2)*pi)\n    assert isinstance(h.ex, Mul) is True\n"], "sample_1137": ["def test_quantity_simplify():\n    from sympy.physics.units.util import quantity_simplify\n    from sympy.physics.units.prefixes import kilo\n    from sympy.physics.units import foot, inch\n\n    # Test with prefixes\n    expr = kilo*foot*inch\n    assert quantity_simplify(expr) == 250*foot**2/3\n\n    # Test with quantities of the same dimension\n    expr = foot - 6*inch\n    assert quantity_simplify(expr) == foot/2\n", "def test_check_dimensions():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    assert check_dimensions(u + v) == u + v\n    assert check_dimensions(u - v) == u - v\n    assert check_dimensions(u + 1*u) == u + u\n\n    raises(ValueError, lambda: check_dimensions(u + w))\n    raises(ValueError, lambda: check_dimensions(u - w))\n    raises(ValueError, lambda: check_dimensions(u + 1))\n    raises(ValueError, lambda: check_dimensions(u - 1))\n", "def test_check_dimensions_on_addition():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    # Test that adding quantities with different dimensions raises an error\n    raises(ValueError, lambda: check_dimensions(u + w))\n\n    # Test that adding quantities with the same dimensions does not raise an error\n    assert check_dimensions(u + v) == u + v\n", "def test_check_dimensions():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    check_dimensions(u + v)\n    check_dimensions(u - v)\n    check_dimensions(u + u)\n    check_dimensions(u * w)\n    check_dimensions(u / w)\n\n    raises(ValueError, lambda: check_dimensions(u + w))\n    raises(ValueError, lambda: check_dimensions(u - w))\n    raises(ValueError, lambda: check_dimensions(u + 1))\n    raises(ValueError, lambda: check_dimensions(u - 1))\n    raises(ValueError, lambda: check_dimensions(1 - exp(u / w)))\n    raises(ValueError, lambda: check_dimensions(1 + exp(u / w)))\n", "def test_check_dimensions():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    check_dimensions(u + v)\n    check_dimensions(u - v)\n    check_dimensions(u * w)\n    check_dimensions(u / w)\n\n    raises(ValueError, lambda: check_dimensions(u + w))\n    raises(ValueError, lambda: check_dimensions(u - w))\n    raises(ValueError, lambda: check_dimensions(u + 1))\n    raises(ValueError, lambda: check_dimensions(u - 1))\n    raises(ValueError, lambda: check_dimensions(1 - exp(u / w)))\n"], "sample_1138": ["def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 - tan(x)**2) == 1 - sec(x)**2 + 1\n    assert TR22(tan(x)**2 - 1) == sec(x)**2 - 2\n    assert TR22(1 - cot(x)**2) == 1 - csc(x)**2 + 1\n    assert TR22(cot(x)**2 - 1) == csc(x)**2 - 2\n", "def test_TR111():\n    assert TR111(tan(x)) == tan(x)\n    assert TR111(1/tan(x)) == cot(x)\n    assert TR111(1/cot(x)) == tan(x)\n    assert TR111(sin(x)) == sin(x)\n    assert TR111(1/sin(x)) == csc(x)\n    assert TR111(1/csc(x)) == sin(x)\n    assert TR111(cos(x)) == cos(x)\n    assert TR111(1/cos(x)) == sec(x)\n    assert TR111(1/sec(x)) == cos(x)\n    assert TR111(cot(x)**-2) == tan(x)**2\n", "def testCTR1():\n    a = pi*Rational(3, 7)\n    b = pi*Rational(4, 7)\n    assert (CTR1[0](sin(a)**2 + cos(b)**2) ==\n            sin(2*a)/2 + cos(2*b)/2 + S.Half)\n    assert CTR1[1](sin(a)**2 - cos(b)**2) == sin(a)**2 - cos(b)**2\n"], "sample_1139": ["compilation error", "def test_issue_18146():\n    e = Symbol('e', integer=True, even=True)\n    o = Symbol('o', integer=True, odd=True)\n    assert Range(5).contains(e) == And(e >= 0, e <= 4)\n    assert Range(1).contains(e) == Eq(e, 0)\n    assert Range(-oo, 5, 1).contains(e) == (e <= 4)\n    assert Range(-oo, oo).contains(e) == True\n    assert Range(0, 8, 2).contains(e) == And(e >= 0, e <= 6)\n    assert Range(0, 8, 2).contains(2*e) == And(2*e >= 0, 2*e <= 6)\n    assert Range(0, 8, 2).contains(o) == False\n    assert Range(1, 9, 2).contains(e) == False\n    assert Range(1, 9, 2).contains(o) == And(o >= 1, o <= 7)\n    assert Range(8, 0, -2).contains(o) == False\n    assert Range(9, 1, -2).contains(o) == And(o >= 3, o <= 9)\n    assert Range(-oo, 8, 2).contains(e) == (e <= 6)\n    assert Range(8, -oo, -2).contains(e) == (e >= 0)\n    assert Range(-oo, 8, 2).contains(o) == False\n    assert Range(8, -oo, -2).contains(o) == False\n    assert Range(-oo, 1, 2).contains(o) == False\n    assert Range(1, -oo, -2).contains(o) == (o >= 1)\n", "def test_issue_18999():\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True)\n    assert Range(n, n + 20, 2).size == 10\n    assert Range(n, n + 20, 2).is_finite_set == True\n    assert Range(n, 0).is_finite_set == True\n    assert Range(-3, n + 7).is_finite_set == True\n    assert Range(n, m).is_finite_set == True\n    assert Range(n + m, m - n).is_finite_set == True\n    assert Range(n, n + m + n).is_finite_set == True\n    assert Range(n, oo).is_finite_set == False\n    assert Range(-oo, n).is_finite_set == False\n    assert Range(n, -oo).is_finite_set == True\n    assert Range(oo, n).is_finite_set == True\n", "def test_issue_19047():\n    assert Range(1, 3).as_relational(x) == Eq(x, 1) | Eq(x, 2)\n    assert Range(1, 4, 2).as_relational(x) == Eq(x, 1) | Eq(x, 3)\n    assert Range(1, oo, 2).as_relational(x) == (x >= 1) & Eq(x % 2, 1) & Eq(x, floor(x))\n", "def test_issue_19382():\n    from sympy import simplify, I, sqrt, exp, pi\n    from sympy.abc import x, n\n    sets = [S.Naturals, S.Naturals0, S.Integers, S.Rationals, S.Reals]\n    f1 = ImageSet(Lambda(n, -2*n), S.Integers)\n    assert simplify(f1.intersect(Interval(-100, 0))) == Range(-98, 0, 2)\n    for s in sets:\n        assert (simplify(ImageSet(Lambda(x, x), s).intersect(Interval(-1, 1))) ==\n                s.intersect(Interval(-1, 1)))\n    f2 = ImageSet(Lambda(n, I**n), S.Naturals)\n    assert simplify(f2.intersect(Interval(-1, 1))) == FiniteSet(-1, 1)\n    f3 = ImageSet(Lambda(n, sqrt(n)), S.Naturals)\n    assert simplify(f3.intersect(Interval(1, 10))) == FiniteSet(sqrt(1), sqrt(4), sqrt(9))\n    f4 = ImageSet(Lambda(n, exp(n)), S.Integers)\n    assert simplify(f4.intersect(Interval(1, exp(1)))) == FiniteSet(exp(0))\n    f5 = ImageSet(Lambda(n, pi*n), S.Integers)\n    assert simplify(f5.intersect(Interval(-pi, 2*pi))) == FiniteSet(-pi, 0, pi, 2*pi)\n"], "sample_1141": ["def test_matrixelement_with_one_symbolic_index():\n    M = Matrix([[1, 2], [3, 4]])\n    i = symbols(\"i\")\n    assert M[i, 0] == MatrixElement(M, i, 0)\n    assert M[0, i] == MatrixElement(M, 0, i)\n    assert M[i, 0].subs(i, 1) == 3\n    assert M[0, i].subs(i, 1) == 2\n    raises(ValueError, lambda: M[i, i])\n", "def test_matrixsymbol_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    assert (A*B).subs({A: C, B: D}) == C*D\n    assert A.subs(B, D) == A\n    assert (A*B).subs(A, D) == D*B\n    assert (A*B).subs(B, D) == A*D\n", "def test_MatrixSymbol_differentiation():\n    assert A.diff(x) == ZeroMatrix(n, m)\n    assert (A*B).diff(x) == ZeroMatrix(n, l)\n    assert (A+B).diff(x) == ZeroMatrix(n, m)\n", "def test_matrix_power():\n    A = MatrixSymbol('A', 2, 2)\n    assert A**0 == Identity(2)\n    assert A**2 == A*A\n    assert A**-2 == (A**-1)**2\n    assert A**-3 == A**-1 * A**-2\n    B = MatrixSymbol('B', 2, 2)\n    assert (A*B)**2 == A*B*A*B\n", "def test_issue_23639():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.det().diff(A[0, 0]) == A[1, 1]\n"], "sample_1140": ["def test_pretty_denest_ordering():\n    expr = (x**2 + x + 1) + (x**2 + x + 1)\n    ascii_str = \\", "def test_pretty_Curl():\n    e = CoordSys3D('e')\n    assert pretty(Curl(e.x*e.i+3*e.y*e.j)) == 'Curl(e_x*i_e + 3*e_y*j_e)'\n    assert upretty(Curl(e.x*e.i+3*e.y*e.j)) == '\u2207\u00d7(e_x\u22c5i_e + 3\u22c5e_y\u22c5j_e)'\n", "def test_diffgeom_vector_fields():\n    from sympy.diffgeom import Manifold, Patch, CoordSystem, BaseVectorField\n    x, y = symbols('x y', real=True)\n    m = Manifold('M', 2)\n    p = Patch('P', m)\n    rect = CoordSystem('rect', p, [x, y])\n    assert pretty(BaseVectorField(rect, 0)) == 'd/dx'\n    assert pretty(BaseVectorField(rect, 1)) == 'd/dy'\n", "def test_pretty_NDimArray():\n    from sympy import Array\n    a = Array([[1, 2], [3, 4]])\n    assert pretty(a) == '[[1, 2], [3, 4]]'\n    assert upretty(a) == '[[1, 2], [3, 4]]'\n    assert pretty(a.inv()) == \"[[-2, 1], [3/2, -1/2]]\"\n    assert pretty(a.inv(method='LU')) == \"[[-2, 1], [3/2, -1/2]]\"\n    assert pretty(a.inv(method='ADJ')) == \"[[-2, 1], [3/2, -1/2]]\"\n", "def test_pretty_printing_of_units():\n    from sympy.physics.units import joule, meter, kilogram, second\n    expr = joule / (meter**2 * kilogram * second**2)\n    assert pretty(expr) == \"kg*m**2/s**2/(kg*m**2/s**2)\"\n    assert upretty(expr) == \"kg\u22c5m**2/s**2/(kg\u22c5m**2/s**2)\"\n"], "sample_1142": ["def test_matrix_element():\n    A = MatrixSymbol('A', n, m)\n    assert A[0, 0].is_MatrixElement\n    assert A[0, 0].parent == A\n    assert A[0, 0].i == 0\n    assert A[0, 0].j == 0\n    assert A[0, 0].is_commutative\n    assert A[0, 0].is_symbol\n    assert A[0, 0].diff(A[0, 0]) == 1\n", "def test_MatrixElement_applyfunc():\n    A = MatrixSymbol('A', 2, 2)\n    expr = MatrixElement(A, 0, 0)\n    f = lambda x: x**2\n    assert expr.applyfunc(f) == f(A[0, 0])\n", "def test_matrix_symbol_as_mutable():\n    A = MatrixSymbol('A', 2, 2)\n    A_mutable = A.as_mutable()\n    assert A_mutable.is_MutableDenseMatrix\n    assert A_mutable == Matrix([\n        [A[0, 0], A[0, 1]],\n        [A[1, 0], A[1, 1]]\n    ])\n", "def test_applyfunc():\n    from .applyfunc import ElementwiseApplyFunction\n    A = MatrixSymbol('A', 2, 2)\n    A_applied = A.applyfunc(lambda x: x**2)\n    assert isinstance(A_applied, ElementwiseApplyFunction)\n    assert A_applied.expr == A\n    assert A_applied.function(lambda x: x**2) == A_applied\n", "def test_get_postprocessor():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n\n    # Test MatMul postprocessor\n    postprocessor = get_postprocessor(Mul)\n    assert postprocessor(2*A) == MatMul(2, A).doit()\n    assert postprocessor(A*2) == MatMul(2, A).doit()\n    assert postprocessor(A*B) == MatMul(A, B).doit()\n\n    # Test MatAdd postprocessor\n    postprocessor = get_postprocessor(Add)\n    assert postprocessor(A + 2) == MatAdd(A, 2).doit()\n    assert postprocessor(2 + A) == MatAdd(2, A).doit()\n    assert postprocessor(A + B) == MatAdd(A, B).doit()\n"], "sample_1143": ["def test_Exp1_simplify():\n    assert E.simplify() == E\n", "def test_mod_inverse_issue_2807():\n    assert mod_inverse(0, 1) == S.NaN\n    assert mod_inverse(0, 5) == S.NaN\n    assert mod_inverse(1, 1) == 1 % 1\n    assert mod_inverse(2, 1) == 1 % 1\n", "def test_EulerGamma_rewriting():\n    from sympy import sin\n    assert EulerGamma.rewrite(sin) == sin(pi/2 - pi*EulerGamma)\n", "def test_issue_21373():\n    with raises(ZeroDivisionError):\n        mod_inverse(2, 0)\n", "def test_Float_comparison():\n    a = Float(0.2, 3)\n    b = Float(0.2, 4)\n    assert a != b\n    assert a == Float(0.2, 3)\n    assert b == Float(0.2, 4)\n    assert a != S(0.2)\n    assert b != S(0.2)\n    assert a != Rational(1, 5)\n    assert b != Rational(1, 5)\n    assert a < b\n    assert b > a\n    assert a <= b\n    assert b >= a\n    assert a <= a\n    assert a >= a\n"], "sample_1145": ["def test_refine_Pow_with_Rational():\n    x = Symbol('x', real=True)\n    assert refine((x**3)**Rational(1, 3), Q.positive(x)) == x\n    assert refine((x**3)**Rational(1, 3), Q.negative(x)) == -x\n    assert refine((-x**3)**Rational(1, 3), Q.positive(x)) == -x\n    assert refine((-x**3)**Rational(1, 3), Q.negative(x)) == x\n", "def test_refine_expr_with_unknown_assumptions():\n    assert refine(Abs(x)) == Abs(x)\n    assert refine(sqrt(x**2)) == sqrt(x**2)\n    assert refine((-1)**x) == (-1)**x\n    assert refine(re(x)) == re(x)\n    assert refine(im(x)) == im(x)\n    assert refine(arg(x)) == arg(x)\n    assert refine(sign(x)) == sign(x)\n", "def test_refine_abs_with_mul():\n    assert refine(Abs(x * y * z), Q.real(x) & Q.real(y) & Q.positive(z)) == Abs(x * y) * z\n    assert refine(Abs(x * y * z), Q.real(x) & Q.real(y) & Q.negative(z)) == Abs(x * y) * -z\n    assert refine(Abs(x * y * z), Q.positive(x) & Q.real(y) & Q.real(z)) == x * Abs(y * z)\n    assert refine(Abs(x * y * z), Q.negative(x) & Q.real(y) & Q.real(z)) == -x * Abs(y * z)\n", "def test_refine_re_im():\n    x = Symbol('x', complex=True)\n    assert refine(re(x), Q.real(x)) == x\n    assert refine(im(x), Q.real(x)) == 0\n    assert refine(re(x), Q.imaginary(x)) == 0\n    assert refine(im(x), Q.imaginary(x)) == -I*x\n\n    assert refine(re(x) + im(x), Q.real(x)) == x\n    assert refine(re(x) + im(x), Q.imaginary(x)) == -I*x\n\n    y = Symbol('y', real=True)\n    z = Symbol('z', imaginary=True)\n    assert refine(re(x*y), Q.real(x)) == x*y\n    assert refine(re(x*z), Q.real(x)) == 0\n    assert refine(im(x*y), Q.real(x)) == 0\n    assert refine(im(x*z), Q.real(x)) == -I*x*z\n", "def test_refine_reduces_nested_powers_of_minus_one():\n    # Test that refine reduces nested powers of -1\n    assert refine((-1)**((-1)**x)) == (-1)**x\n    assert refine((-1)**((-1)**((-1)**x))) == (-1)**x\n    assert refine((-1)**((-1)**((-1)**((-1)**x)))) == (-1)**x\n"], "sample_1144": ["def test_requires_partial_free_symbols():\n    x, y, z = symbols('x y z')\n    f = x * y + z\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, y)) is True\n    assert requires_partial(Derivative(f, z)) is True\n\n    # check with non-Iterable free_symbols\n    f.free_symbols = None\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, y)) is True\n    assert requires_partial(Derivative(f, z)) is True\n", "def test_requires_partial_free_symbols():\n    x, y, z, t = symbols('x y z t')\n    f = x * y\n    g = x + y\n    assert requires_partial(Derivative(f, x).expr) is True\n    assert requires_partial(Derivative(g, x).expr) is True\n    h = Derivative(f, x).expr\n    h._free_symbols = {x, y, z}\n    assert requires_partial(h) is True\n    h._free_symbols = {x}\n    assert requires_partial(h) is False\n    i = Derivative(g, x).expr\n    i._free_symbols = {x, y, z, t}\n    assert requires_partial(i) is True\n    i._free_symbols = {x}\n    assert requires_partial(i) is False\n", "def test_split_super_sub_edge_cases():\n    # Test with only superscripts\n    assert split_super_sub(\"x^a^b\") == (\"x\", [\"a\", \"b\"], [])\n    assert split_super_sub(\"x__a__b\") == (\"x\", [\"a\", \"b\"], [])\n\n    # Test with only subscripts\n    assert split_super_sub(\"x_a_b\") == (\"x\", [], [\"a\", \"b\"])\n\n    # Test with superscripts and subscripts\n    assert split_super_sub(\"x_a^b_c\") == (\"x\", [\"b\"], [\"a\", \"c\"])\n    assert split_super_sub(\"x_a__b_c\") == (\"x\", [\"b\"], [\"a\", \"c\"])\n\n    # Test with consecutive superscripts and subscripts\n    assert split_super_sub(\"x^a^b_c_d\") == (\"x\", [\"a\", \"b\"], [\"c\", \"d\"])\n    assert split_super_sub(\"x__a__b_c_d\") == (\"x\", [\"a\", \"b\"], [\"c\", \"d\"])\n\n    # Test with non-ASCII characters\n    assert split_super_sub(\"x_\u00fc^\u00e4\") == (\"x\", [\"\u00e4\"], [\"\u00fc\"])\n    assert split_super_sub(\"x__\u00fc__\u00e4\") == (\"x\", [\"\u00e4\"], [\"\u00fc\"])\n", "def test_split_super_sub_edge_cases():\n    # Test with non-string input\n    with pytest.raises(TypeError):\n        split_super_sub(123)\n\n    # Test with None input\n    with pytest.raises(TypeError):\n        split_super_sub(None)\n\n    # Test with string containing only digits\n    assert split_super_sub(\"123\") == (\"\", [], [\"123\"])\n\n    # Test with string containing only superscript\n    assert split_super_sub(\"^123\") == (\"\", [\"123\"], [])\n\n    # Test with string containing only subscript\n    assert split_super_sub(\"_123\") == (\"\", [], [\"123\"])\n\n    # Test with string containing multiple consecutive underscores\n    assert split_super_sub(\"x__123\") == (\"x\", [\"123\"], [])\n\n    # Test with string containing multiple consecutive carets\n    assert split_super_sub(\"x^123^456\") == (\"x\", [\"123\", \"456\"], [])\n", "def test_split_super_sub_edge_cases():\n    assert split_super_sub(\"x__a\") == (\"x\", [\"a\"], [])\n    assert split_super_sub(\"x__a__\") == (\"x\", [\"a\"], [])\n    assert split_super_sub(\"x^a^\") == (\"x\", [\"a\"], [])\n    assert split_super_sub(\"x_a_\") == (\"x\", [], [\"a\"])\n    assert split_super_sub(\"_a\") == (\"\", [], [\"a\"])\n    assert split_super_sub(\"__a\") == (\"\", [\"a\"], [])\n    assert split_super_sub(\"^a\") == (\"\", [\"a\"], [])\n    assert split_super_sub(\"x_\") == (\"x\", [], [])\n    assert split_super_sub(\"x__\") == (\"x\", [], [])\n    assert split_super_sub(\"x^\") == (\"x\", [], [])\n"], "sample_1148": ["def test_matrix_derivative():\n    A = MatrixSymbol('A', 2, 2)\n    x = Symbol('x')\n    expr = A*x\n    derivative = _matrix_derivative(expr, x)\n    assert derivative == A\n\n    B = MatrixSymbol('B', 2, 2)\n    expr = A*B\n    derivative = _matrix_derivative(expr, A)\n    assert derivative == B.T\n\n    C = MatrixSymbol('C', 2, 1)\n    expr = A*C\n    derivative = _matrix_derivative(expr, A)\n    assert derivative == C\n\n    expr = C.T*A\n    derivative = _matrix_derivative(expr, A)\n    assert derivative == C\n", "def test_matrix_derivative():\n    from sympy.tensor.array.array_derivatives import ArrayDerivative\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    x = symbols(\"x\")\n    assert _matrix_derivative(A, x) == ZeroMatrix(2, 2)\n    assert _matrix_derivative(A, A) == Identity(2)*Identity(2)\n", "def test_MatrixExpr_from_index_summation():\n    from sympy import MatrixSymbol, Sum, symbols, Identity, KroneckerDelta\n    from sympy.abc import i, j, k, l\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n\n    # Contracting to the right\n    expr = Sum(A[i, j]*B[j, k], (j, 0, 2))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n\n    # Contracting to the left\n    expr = Sum(B[j, k]*A[i, j], (j, 0, 2))\n    assert MatrixExpr.from_index_summation(expr) == A.T*B.T\n\n    # Contracting two indices\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, 2), (k, 0, 2))\n    assert MatrixExpr.from_index_summation(expr) == A*B.T*A.T\n\n    # With more complicated expression\n    expr = Sum(KroneckerDelta(i, k)*KroneckerDelta(j, l)*A[i, j], (i, 0, 2), (j, 0, 2))\n    assert MatrixExpr.from_index_summation(expr) == Identity(3)\n\n    expr = Sum(A[i, j]*(B[j, k] + 2*B[l, k]), (j, 0, 2), (l, 0, 2))\n    assert MatrixExpr.from_index_summation(expr) == A*(3*B)\n\n    n = symbols('n')\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    expr = Sum(A[i, j]*B[j, k], (j, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n", "def test_MatrixElement_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    a = A[0, 0]\n    assert a.as_real_imag() == (a.as_real_imag()[0], a.as_real_imag()[1])\n    assert a.as_real_imag()[0].is_real\n    assert a.as_real_imag()[1].is_real\n    assert a.as_real_imag()[0] == (a + a.conjugate())/2\n    assert a.as_real_imag()[1] == (a - a.conjugate())/(2*S.ImaginaryUnit)\n", "def test_MatrixElement_getattr():\n    A = MatrixSymbol('A', 3, 3)\n    me = MatrixElement(A, 1, 2)\n    assert me.parent == A\n    assert me.i == 1\n    assert me.j == 2\n    assert me.indices == (1, 2)\n    raises(AttributeError, lambda: me.nonexistent_attribute)\n"], "sample_1149": ["def test_singletonrepr():\n    assert repr(S) == \"S\"\n", "def test_singleton_registry_str_repr():\n    # Test the string representation of the SingletonRegistry.\n\n    # The string representation should be 'S'.\n    assert str(S) == 'S'\n    assert repr(S) == 'S'\n", "def test_SingletonRegistry():\n    # Test that the SingletonRegistry class is working correctly\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n\n    # Test that the registry has the class registered\n    assert MySingleton.__name__ in S._classes_to_install\n\n    # Test that accessing the attribute installs it\n    MySingleton()  # force instantiation\n    assert MySingleton.__name__ not in S._classes_to_install\n    assert hasattr(S, MySingleton.__name__)\n\n    # Test that the __getattr__ method raises an AttributeError for unknown classes\n    try:\n        S.UnknownClass\n        assert False, \"AttributeError should have been raised\"\n    except AttributeError:\n        pass\n\n    # Test that the __repr__ method returns the correct string\n    assert repr(S) == \"S\"\n", "def test_SingletonRegistry_call():\n    # Test S() as a shortcut for sympify()\n    assert S(1) == 1\n    assert S(1.5) == 1.5\n    assert S('x') == sympify('x')\n    assert S('x**2') == sympify('x**2')\n", "def test_SingletonRegistry_call():\n    # Test that S can be used as a shortcut for sympify\n    assert S(1) == 1\n    assert isinstance(S(1), Basic)\n    assert S('x') == sympify('x')\n    assert S(Rational(1, 2)) == Rational(1, 2)\n"], "sample_1147": ["def test_printing_latex_NDimArray_expressions():\n    from sympy.tensor.array import NDimArray\n    assert latex(NDimArray([1, 2, 3])) == r\"\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}\"\n    assert latex(NDimArray([[1, 2, 3], [4, 5, 6]])) == \\\n        r\"\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\"\n    assert latex(NDimArray([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == \\\n        r\"\\begin{bmatrix} \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} & \\begin{bmatrix} 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{bmatrix} \\end{bmatrix}\"\n", "def test_printing_latex_NDimArray_expressions():\n    assert latex(NDimArraySymbol(\"A\", (2, 3, 4))) == \"A\"\n    assert latex(NDimArrayElement(\"A\", (2, 1/(1-x), 0))) == \"{{A}_{2, \\\\frac{1}{1 - x}, 0}}\"\n", "def test_printing_latex_array_expressions():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    assert latex(A[1, 2, 3]) == \"{{A}_{1, 2, 3}}\"\n    assert latex(A[1, 2, 3]**2) == r\"\\left({{A}_{1, 2, 3}}\\right)^{2}\"\n    assert latex(A[1, 2, 3] + 2) == r\"{{A}_{1, 2, 3}} + 2\"\n", "def test_print_MatAdd():\n    from sympy import MatrixSymbol\n    from sympy.printing.latex import LatexPrinter\n    A = MatrixSymbol('A', 5, 5)\n    B = MatrixSymbol('B', 5, 5)\n    l = LatexPrinter()\n    assert l._print(A - 2*B) in [r'- 2 B + A', r'A -2 B']\n    assert l._print(A + 2*B) in [r'2 B + A', r'A + 2 B']\n    assert l._print(B - 2*A) in [r'B - 2 A', r'- 2 A + B']\n    assert l._print(B + 2*A) in [r'B + 2 A', r'2 A + B']\n", "def test_printing_latex_NDimArray():\n    from sympy import NDimArray\n\n    # Create a 2D NDimArray\n    array = NDimArray([[1, 2], [3, 4]])\n\n    # Test the latex representation of the NDimArray\n    assert latex(array) == r\"\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\"\n"], "sample_1146": ["def test_printing_latex_NDimArray():\n    assert latex(NDimArray([1, 2, 3], shape=(3,))) == r'\\left[ 1, \\  2, \\  3\\right]'\n    assert latex(NDimArray([[1, 2], [3, 4]], shape=(2, 2))) == r'\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]'\n", "def test_latex_hadamard_power():\n    assert latex(HadamardPower(x, n)) == r'x^{\\circ n}'\n    assert latex(HadamardPower(x, -n)) == r'x^{\\circ \\left(- n\\right)}'\n", "def test_HadamardPower_printing():\n    from sympy.matrices.expressions import MatrixSymbol, HadamardPower\n    A = MatrixSymbol(\"A\", 2, 3)\n    assert latex(HadamardPower(A, 3)) == r\"A^{\\circ {3}}\"\n    assert latex(HadamardPower(A, -4)) == r\"A^{\\circ \\left({-4}\\right)}\"\n    assert latex(HadamardPower(A, 2.5)) == r\"A^{\\circ {2.5}}\"\n", "def test_NDimArray_latex():\n    from sympy.tensor.array import ImmutableNDimArray\n    from sympy.abc import x, y, z, t, w\n\n    # Basic: scalar array\n    M = ImmutableNDimArray([x])\n    assert latex(M) == r\"x\"\n\n    M = ImmutableNDimArray([x, y])\n    assert latex(M) == r\"\\left[x, \\  y\\right]\"\n\n    M = ImmutableNDimArray([[1 / x, y], [z, w]])\n    assert latex(M) == r\"\\left[\\begin{matrix}\\frac{1}{x} & y\\\\z & w\\end{matrix}\\right]\"\n\n    M = ImmutableNDimArray(\n        [[[1 / x, y], [z, w]], [[1 / t, 2], [3, 4]]])\n    assert latex(M) == \\\n        r\"\\left[\\begin{matrix}\\left[\\begin{matrix}\\frac{1}{x} & y\\\\z & w\\end{matrix}\\right] & \\left[\\begin{matrix}\\frac{1}{t} & 2\\\\3 & 4\\end{matrix}\\right]\\end{matrix}\\right]\"\n\n    # Matrix with function\n    M = ImmutableNDimArray(\n        [[sin(x), cos(x)], [tan(x), x]])\n    assert latex(M) == r\"\\left[\\begin{matrix}\\sin{\\left(x \\right)} & \\cos{\\left(x \\right)}\\\\\\tan{\\left(x \\right)} & x\\end{matrix}\\right]\"\n", "def test_latex_decimal_separator_advanced():\n    x, y, z, t = symbols('x y z t')\n    k, m, n = symbols('k m n', integer=True)\n    f, g, h = symbols('f g h', cls=Function)\n\n    assert(latex(1/2, decimal_separator='comma') == r'\\frac{1}{2}')\n    assert(latex(1/2, decimal_separator='period') == r'\\frac{1}{2}')\n    assert(latex(1/2) == r'\\frac{1}{2}')\n\n    assert(latex([1, 2.3, 4.5], decimal_separator='comma') == r'\\left[ 1; \\  2{,}3; \\  4{,}5\\right]')\n    assert(latex([1, 2.3, 4.5], decimal_separator='period') == r'\\left[ 1, \\  2.3, \\  4.5\\right]' )\n    assert(latex([1, 2.3, 4.5]) == r'\\left[ 1, \\  2.3, \\  4.5\\right]')\n\n    assert(latex(FiniteSet(1, 2.3, 4.5), decimal_separator='comma') == r'\\left\\{1; 2{,}3; 4{,}5\\right\\}')\n    assert(latex(FiniteSet(1, 2.3, 4.5), decimal_separator='period') == r'\\left\\{1, 2.3, 4.5\\right\\}')\n    assert(latex(FiniteSet(1, 2.3, 4.5)) == r'\\left\\{1, 2.3, 4.5\\right\\}')\n\n    assert(latex((1, 2.3, 4.6), decimal_separator = 'comma') == r'\\left( 1; \\  2{,}3; \\  4{,}6\\right)')\n    assert(latex((1, 2.3, 4.6), decimal_separator = 'period') == r'\\left( 1, \\  2.3, \\  4.6\\right)')\n    assert(latex((1, 2.3, 4"], "sample_1150": ["def test_issue_19651():\n    from sympy.solvers.diophantine import parametrize\n    raises(NotImplementedError, lambda: ImageSet(Lambda(x, x**2 + 3*x + 2), S.Integers\n        ).intersect(S.Naturals))\n    raises(NotImplementedError, lambda: ImageSet(Lambda(x, x**3 + x + 1), S.Integers\n        ).intersect(S.Naturals))\n", "def test_issue_20382():\n    r1 = Interval(1, 2)\n    r2 = Interval(3, 4)\n    cr = ComplexRegion(r1*r2)\n    assert cr.contains(1 - I) == False\n    assert cr.contains(1 + 3*I) == True\n    assert cr.contains((1, 3)) == True\n    assert cr.contains((1, 4)) == False\n", "def test_issue_19985():\n    from sympy.abc import n\n    s1 = ImageSet(Lambda(n, n**2), S.Integers)\n    s2 = ImageSet(Lambda(n, -n**2), S.Integers)\n    assert s1.intersect(s2) == FiniteSet(0)\n", "def test_issue_19598():\n    x_set = Interval(0, 1)\n    y_set = Interval(-oo, oo)\n    c = ComplexRegion(x_set*y_set)\n    assert c.intersect(S.Reals) == Interval(0, 1)\n", "def test_issue_18999():\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True)\n    assert Range(n, n + 49).is_finite_set is True\n    assert Range(n, 0).is_finite_set is True\n    assert Range(-3, n + 7).is_finite_set is True\n    assert Range(n, m).is_finite_set is True\n    assert Range(n + m, m - n).is_finite_set is True\n    assert Range(n, n + m + n).is_finite_set is True\n    assert Range(n, oo).is_finite_set is False\n    assert Range(-oo, n).is_finite_set is False\n    assert Range(n, -oo).is_finite_set is True\n    assert Range(oo, n).is_finite_set is True\n"], "sample_1152": ["def test_powsimp_on_non_integer_powers():\n    x = symbols('x')\n    assert powsimp((x**Rational(1, 3))**2 * x**Rational(1, 3)) == x\n    assert powsimp((x**Rational(1, 2))**2 * x**Rational(1, 2)) == x**(S(3)/2)\n", "def test_issue_20265():\n    x = symbols('x', real=True)\n    assert powsimp((-x)**Rational(1, 3)) == (-x)**Rational(1, 3)\n    assert powsimp((-x)**Rational(2, 3)) == (-x)**Rational(2, 3)\n", "def test_issue_20774():\n    x, y = symbols('x y', real=True)\n    assert powsimp((-x)**(2*y)) == x**(2*y)\n    assert powsimp((-x)**y) == (-x)**y\n    assert powsimp((-x)**(2*y + 1)) == -x**(2*y + 1)\n    assert powsimp((-x)**(2*y - 1)) == -x**(2*y - 1)\n    assert powsimp((-x)**(2*y - S.Half)) == (-x)**(2*y - S.Half)\n", "def test_issue_19725():\n    x, y = symbols('x y', real=True)\n    assert powsimp(x**(y/2)*x**(y/2)) == x**y\n    assert powsimp(x**(y/2)*x**(y/2), force=True) == x**y\n    assert powsimp(sqrt(x)*sqrt(x)) == x\n    assert powsimp(sqrt(x)*sqrt(x), force=True) == x\n", "def test_issue_20056():\n    x = Symbol('x')\n    assert powsimp(x**(2/3) * x**(S(4)/3)) == x**2\n    assert powsimp(x**(2/3) * x**(S(4)/3), force=True) == x**2\n    assert powsimp(x**(2/3) * x**(S(5)/3)) == x**(7/3)\n    assert powsimp(x**(2/3) * x**(S(5)/3), force=True) == x**(7/3)\n"], "sample_1151": ["def test_Mod_with_zero_p():\n    assert Mod(x, 0) == x % 0\n    assert Mod(0, 0) == 0 % 0\n    assert Mod(1, 0) == 1 % 0\n", "def test_Mod_fuzz():\n    from random import randint\n    for _ in range(1000):\n        a = randint(-1000, 1000)\n        b = randint(-1000, 1000)\n        if b == 0:\n            continue\n        assert Mod(a, b) == a % b\n", "def test_Mul_is_extended_positive():\n    p = Symbol('p', extended_positive=True)\n    n = Symbol('n', extended_negative=True)\n    z = Symbol('z', zero=True)\n    u = Symbol('u', extended_nonpositive=True)\n    v = Symbol('v', extended_nonnegative=True)\n    w = Symbol('w', finite=True)\n    x = Symbol('x', real=True)\n    y = Symbol('y', extended_real=False)\n\n    assert (p*x).is_extended_positive is None\n    assert (n*x).is_extended_positive is None\n    assert (x*y).is_extended_positive is None\n\n    assert (p*p).is_extended_positive is True\n    assert (n*n).is_extended_positive is None\n    assert (v*v).is_extended_positive is True\n    assert (u*u).is_extended_positive is None\n\n    assert (p*z).is_extended_positive is False\n    assert (p*n).is_extended_positive is False\n    assert (p*u).is_extended_positive is False\n    assert (n*z).is_extended_positive is None\n    assert (n*p).is_extended_positive is False\n    assert (n*v).is_extended_positive is False\n    assert (z*p).is_extended_positive is False\n    assert (z*n).is_extended_positive is None\n    assert (z*v).is_extended_positive is None\n    assert (w*p).is_extended_positive is None\n", "def test_issue_22269():\n    e = Mod((x + 1)**3, 2)\n    assert e.simplify() == Mod(x + 1, 2)\n", "def test_division_with_float_denominator():\n    assert (x / 1.0).is_Float is None\n    assert (x / -1.0).is_Float is None\n    assert (1.0 / x).is_Float is None\n    assert (-1.0 / x).is_Float is None\n    assert (x / 2.0).is_Float is None\n    assert (x / -2.0).is_Float is None\n    assert (2.0 / x).is_Float is None\n    assert (-2.0 / x).is_Float is None\n    assert (x / 1.0).is_commutative is True\n    assert (x / -1.0).is_commutative is True\n    assert (1.0 / x).is_commutative is True\n    assert (-1.0 / x).is_commutative is True\n    assert (x / 2.0).is_commutative is True\n    assert (x / -2.0).is_commutative is True\n    assert (2.0 / x).is_commutative is True\n    assert (-2.0 / x).is_commutative is True\n"], "sample_1153": ["def test_transpose_adjoint():\n    from sympy import transpose, adjoint\n    A = Matrix([[1, 2], [3, 4]])\n    assert transpose(adjoint(A)) == adjoint(transpose(A))\n    B = Matrix([[1 + 1j, 2 - 2j], [3 - 3j, 4 + 4j]])\n    assert transpose(adjoint(B)) == adjoint(transpose(B))\n    x = Symbol('x')\n    expr = x + 1/x\n    assert transpose(adjoint(expr)) == adjoint(transpose(expr))\n    assert transpose(adjoint(adjoint(expr))) == adjoint(transpose(adjoint(expr)))\n", "def test_issue_19627():\n    from sympy import Function, Symbol\n    x = Symbol('x', real=True)\n    f = Function('f', real=True, positive=True)\n    g = Function('g', real=True, negative=True)\n    assert Abs(f(x)).simplify() == f(x)\n    assert Abs(g(x)).simplify() == -g(x)\n", "def test_issue_20370():\n    a, b = symbols('a b')\n    eq = Abs(a)*conjugate(b) + conjugate(Abs(a))*b\n    assert eq.simplify() == 2*Abs(a)*re(b)\n", "def test_eval_derivative():\n    x = Symbol('x')\n    f = Function('f')\n    assert re(f(x)).diff(x) == re(f(x).diff(x))\n    assert im(f(x)).diff(x) == im(f(x).diff(x))\n    assert sign(f(x)).diff(x) == 2*f(x).diff(x)*DiracDelta(f(x))\n    assert Abs(f(x)).diff(x) == f(x).diff(x)*sign(conjugate(f(x)))\n", "def test_issue_19627():\n    f = Function('f', real=True)\n    x = Symbol('x', real=True)\n    assert sqrt(f(x)**2) == Abs(f(x))\n    assert sqrt((f(x) + 1)**2) == Abs(f(x) + 1)\n    assert sqrt(f(x)**2 + 2*f(x) + 1) == Abs(f(x) + 1)\n"], "sample_1154": ["def test__linsolve_empty_equations():\n    assert _linsolve([], [x, y, z]) == {x:x, y:y, z:z}\n    assert _linsolve([S.Zero, S.Zero], [x, y, z]) == {x:x, y:y, z:z}\n    assert _linsolve([x-x, y-y], [x, y, z]) == {x:x, y:y, z:z}\n", "def test__linsolve_empty_equations():\n    assert _linsolve([], []) == {}\n    assert _linsolve([S.Zero], []) == {}\n    assert _linsolve([], [x, y, z]) == {x:x, y:y, z:z}\n    assert _linsolve([S.Zero], [x, y, z]) == {x:x, y:y, z:z}\n", "def test__linear_eq_to_dict():\n    eqs = [Eq(x + 2*y, 3), Eq(4*x - 5*y, 2)]\n    syms = [x, y]\n    eqsdict, eqs_rhs = _linear_eq_to_dict(eqs, syms)\n    assert eqsdict == [{x: 1, y: 2}, {x: 4, y: -5}]\n    assert eqs_rhs == [3, 2]\n\n    # Test with a nonlinear equation\n    eqs = [Eq(x**2 + 2*y, 3), Eq(4*x - 5*y, 2)]\n    raises(PolyNonlinearError, lambda: _linear_eq_to_dict(eqs, syms))\n\n    # Test with an equation that is not an instance of Eq\n    eqs = [x + 2*y - 3, Eq(4*x - 5*y, 2)]\n    eqsdict, eqs_rhs = _linear_eq_to_dict(eqs, syms)\n    assert eqsdict == [{x: 1, y: 2}, {x: 4, y: -5}]\n    assert eqs_rhs == [3, 2]\n", "def test__lin_eq2dict():\n    assert _lin_eq2dict(x, {x}) == (S.Zero, {x: S.One})\n    assert _lin_eq2dict(x + 2*y, {x, y}) == (S.Zero, {x: S.One, y: S(2)})\n    assert _lin_eq2dict(x - 2*y, {x, y}) == (S.Zero, {x: S.One, y: S(-2)})\n    assert _lin_eq2dict(2*x + 3*y, {x, y}) == (S.Zero, {x: S(2), y: S(3)})\n    assert _lin_eq2dict(x + y - 2, {x, y}) == (S(-2), {x: S.One, y: S.One})\n    assert _lin_eq2dict(Eq(x + y, 2), {x, y}) == (S(-2), {x: S.One, y: S.One})\n    raises(PolyNonlinearError, lambda: _lin_eq2dict(x**2 + y, {x, y}))\n    raises(PolyNonlinearError, lambda: _lin_eq2dict(x*y + y, {x, y}))\n", "def test__linear_eq_to_dict():\n    # Test simple equation\n    eq = Eq(x + y, 1)\n    syms = (x, y)\n    eqsdict, eqs_rhs = _linear_eq_to_dict([eq], syms)\n    assert eqsdict == [{x: 1, y: 1}]\n    assert eqs_rhs == [1]\n\n    # Test equation with no variables\n    eq = Eq(2, 3)\n    syms = (x, y)\n    eqsdict, eqs_rhs = _linear_eq_to_dict([eq], syms)\n    assert eqsdict == [{}]\n    assert eqs_rhs == [-1]\n\n    # Test equation with multiple variables and coefficients\n    eq = Eq(2*x + 3*y - 4*z, 0)\n    syms = (x, y, z)\n    eqsdict, eqs_rhs = _linear_eq_to_dict([eq], syms)\n    assert eqsdict == [{x: 2, y: 3, z: -4}]\n    assert eqs_rhs == [0]\n\n    # Test equation with non-linear term (should raise PolyNonlinearError)\n    eq = Eq(x**2 + y, 1)\n    syms = (x, y)\n    raises(PolyNonlinearError, lambda: _linear_eq_to_dict([eq], syms))\n"], "sample_1155": ["def test_construct_domain_with_dict():\n    # Test construct_domain with a dictionary input\n    d = {(1,): 2, (2,): 3}\n    domain, result = construct_domain(d)\n    assert domain == ZZ\n    assert result == {(1,): ZZ(2), (2,): ZZ(3)}\n\n    d = {(1, 2): x/2, (3, 4): 3.5}\n    domain, result = construct_domain(d)\n    assert domain == RR[x]\n    assert result == {(1, 2): domain.convert(x/2), (3, 4): domain.convert(3.5)}\n\n    d = {(1, 2): I*x/2, (3, 4): 3.5}\n    domain, result = construct_domain(d)\n    assert domain == CC[x]\n    assert result == {(1, 2): domain.convert(I*x/2), (3, 4): domain.convert(3.5)}\n", "def test_construct_domain_with_dict_keys():\n    # test constructing domain with dict keys\n    result = construct_domain({x: 1, y: 2})\n    assert result[0] == ZZ[x, y]\n    assert result[1] == {x: 1, y: 2}\n\n    result = construct_domain({x**2: 1, y**2: 2})\n    assert result[0] == ZZ[x, y]\n    assert result[1] == {x**2: 1, y**2: 2}\n\n    result = construct_domain({x: S.Half, y: 2})\n    assert result[0] == QQ[x, y]\n    assert result[1] == {x: S.Half, y: 2}\n\n    result = construct_domain({x: pi, y: 2})\n    assert result[0] == ZZ[x, y, pi]\n    assert result[1] == {x: pi, y: 2}\n", "def test_dict_with_rational_coefficients():\n    result = construct_domain({(1,): S.Half, (2,): S(3)/4})\n    assert result == (QQ, {(1,): QQ(1, 2), (2,): QQ(3, 4)})\n\n    result = construct_domain({(1,): S.Half, (2,): S(3)/4}, field=False)\n    assert result == (QQ, {(1,): QQ(1, 2), (2,): QQ(3, 4)})\n\n    result = construct_domain({(1,): S.Half, (2,): 2})\n    assert result == (QQ, {(1,): QQ(1, 2), (2,): QQ(2)})\n\n    result = construct_domain({(1,): S.Half, (2,): 2}, field=False)\n    assert result == (QQ, {(1,): QQ(1, 2), (2,): QQ(2)})\n", "def test_construct_domain_with_mixed_types():\n    assert construct_domain([1, x, 2.0, I]) == (EX, [EX(1), EX(x), EX(2.0), EX(I)])\n    assert construct_domain([S(1)/2, x, 3.14, I]) == (EX, [EX(S(1)/2), EX(x), EX(3.14), EX(I)])\n    assert construct_domain([S.One, x, S(2), 3.0, I]) == (EX, [EX(1), EX(x), EX(2), EX(3.0), EX(I)])\n    assert construct_domain([1, sqrt(2), 3.14, I]) == (EX, [EX(1), EX(sqrt(2)), EX(3.14), EX(I)])\n    assert construct_domain([1, x, sqrt(2), 3.14, I]) == (EX, [EX(1), EX(x), EX(sqrt(2)), EX(3.14), EX(I)])\n", "def test_construct_domain_with_dict_and_extension():\n    result = construct_domain({x: sqrt(2), y: 3}, extension=True)\n    alg = QQ.algebraic_field(sqrt(2))\n    assert result == (alg.frac_field(x, y), {x: alg.convert(sqrt(2)), y: alg.convert(3)})\n\n    result = construct_domain({x: sqrt(2), y: 3}, extension=False)\n    assert result == (EX, {x: EX(sqrt(2)), y: EX(3)})\n"], "sample_1157": ["def test_implicit_multiplication():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    f = Function('f')\n    assert parse_expr(\"3 x\", transformations=transformations) == 3*x\n    assert parse_expr(\" 3x\", transformations=transformations) == 3*x\n    assert parse_expr(\"x 3\", transformations=transformations) == x*3\n    assert parse_expr(\"x3\", transformations=transformations) == x*3\n    assert parse_expr(\"(x + 1)(x - 1)\", transformations=transformations) == (x+1)*(x-1)\n    assert parse_expr(\"(x+1)(x-1)\", transformations=transformations) == (x+1)*(x-1)\n    assert parse_expr(\"x(x+1)\", transformations=transformations) == x*(x+1)\n    assert parse_expr(\"(x+1)x\", transformations=transformations) == x*(x+1)\n    assert parse_expr(\"f(x) x\", transformations=transformations) == f(x)*x\n    assert parse_expr(\"x f(x)\", transformations=transformations) == x*f(x)\n    assert parse_expr(\"x (1 + y)\", transformations=transformations) == x*(1+y)\n    assert parse_expr(\"(1 + y) x\", transformations=transformations) == x*(1+y)\n    assert parse_expr(\"x ( y + 1)\", transformations=transformations) == x*(y+1)\n    assert parse_expr(\"( y + 1) x\", transformations=transformations) == x*(y+1)\n    assert parse_expr(\"(y + 1)(x + 1)\", transformations=transformations) == (y+1)*(x+1)\n    assert parse_expr(\"(x + 1)(y + 1)\", transformations=transformations) == (x+1)*(y+1)\n    assert parse_expr(\"(x + 1)(y + 1)(z + 1)\", transformations=transformations) == (x+1)*(y+1)*(z+1)\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n    assert parse_expr(\"lambda x: x\", transformations=transformations) == Lambda(x, x)\n    assert parse_expr(\"(lambda x: x)(y)\", transformations=transformations) == Lambda(x, x).subs(x, y)\n    assert parse_expr(\"lambda x, y: x + y\", transformations=transformations) == Lambda((x, y), x + y)\n    assert parse_expr(\"(lambda x, y: x + y)(x, y)\", transformations=transformations) == Lambda((x, y), x + y).subs({x: x, y: y})\n", "def test_parse_expr_evaluate():\n    # Test evaluate option in parse_expr\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr(\"2*x\", transformations=transformations, evaluate=True) == 2*x\n    assert parse_expr(\"2*x\", transformations=transformations, evaluate=False) == 2*x\n    assert parse_expr(\"2*x\", transformations=transformations, evaluate=None) == 2*x\n\n    assert parse_expr(\"x + x\", transformations=transformations, evaluate=True) == 2*x\n    assert parse_expr(\"x + x\", transformations=transformations, evaluate=False).args == (x, x)\n    assert parse_expr(\"x + x\", transformations=transformations, evaluate=None).args == (x, x)\n\n    assert parse_expr(\"x - x\", transformations=transformations, evaluate=True) == 0\n    assert parse_expr(\"x - x\", transformations=transformations, evaluate=False).args == (x, -x)\n    assert parse_expr(\"x - x\", transformations=transformations, evaluate=None).args == (x, -x)\n\n    assert parse_expr(\"x * x\", transformations=transformations, evaluate=True) == x**2\n    assert parse_expr(\"x * x\", transformations=transformations, evaluate=False).args == (x, x)\n    assert parse_expr(\"x * x\", transformations=transformations, evaluate=None).args == (x, x)\n\n    assert parse_expr(\"x / x\", transformations=transformations, evaluate=True) == 1\n    assert parse_expr(\"x / x\", transformations=transformations, evaluate=False).args == (x, 1/x)\n    assert parse_expr(\"x / x\", transformations=transformations, evaluate=None).args == (x, 1/x)\n\n    assert parse_expr(\"x**2 * x**3\", transformations=transformations, evaluate=True) == x**5\n    assert parse_expr(\"x**2 * x**3\", transformations=transformations, evaluate=False) == x**2 * x**3\n    assert parse_expr(\"x**2 * x**3\", transformations=transformations, evaluate=None) == x**2 * x**3\n\n    assert parse_expr(\"x**2 + x**2\", transformations=transformations, evaluate=True) == 2*x**2\n    assert parse_expr(\"x", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    assert parse_expr('lambda x: x**2', transformations=transformations) == Lambda(x, x**2)\n    assert parse_expr('lambda x, y: x + y', transformations=transformations) == Lambda((x, Symbol('y')), x + Symbol('y'))\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"lambda x: x\", transformations=transformations) == Lambda(x, x)\n    assert parse_expr(\"lambda x,y: x+y\", transformations=transformations) == Lambda((x, y), x+y)\n"], "sample_1156": ["def test_finite():\n    x = Symbol('x', real=True)\n    assert sinh(x).is_finite is True\n    assert cosh(x).is_finite is True\n    assert tanh(x).is_finite is True\n    assert coth(x).is_finite is True\n    assert sech(x).is_finite is True\n    assert csch(x).is_finite is True\n\n    assert asinh(x).is_finite is True\n    assert acosh(x).is_finite is False\n    assert atanh(x).is_finite is False\n    assert acoth(x).is_finite is False\n    assert asech(x).is_finite is False\n    assert acsch(x).is_finite is False\n", "def test_csch_as_real_imag():\n    x, y = symbols('x,y')\n    assert csch(x + y*I).as_real_imag() == (-sinh(x)*sin(y)/(-cos(y)**2 + sinh(x)**2),\n            cos(y)*cosh(x)/(-cos(y)**2 + sinh(x)**2))\n", "def test_issue_12732():\n    n = Symbol('n', integer=True)\n    assert sinh(n*pi*I).is_zero\n    assert cosh(n*pi*I).is_real\n    assert sin(n*pi).is_zero\n    assert cos(n*pi).is_real\n", "def test_is_finite():\n    x = Symbol('x')\n    assert sinh(x).is_finite is None\n    assert cosh(x).is_finite is None\n    assert tanh(x).is_finite is None\n    assert sech(x).is_finite is None\n    assert csch(x).is_finite is None\n    assert coth(x).is_finite is None\n\n    assert asinh(x).is_finite is None\n    assert acosh(x).is_finite is None\n    assert atanh(x).is_finite is None\n    assert asech(x).is_finite is None\n    assert acsch(x).is_finite is None\n    assert acoth(x).is_finite is None\n\n    assert sinh(1).is_finite is True\n    assert cosh(1).is_finite is True\n    assert tanh(1).is_finite is True\n    assert sech(1).is_finite is True\n    assert csch(1).is_finite is True\n    assert coth(1).is_finite is True\n\n    assert asinh(1).is_finite is True\n    assert acosh(2).is_finite is True\n    assert atanh(0.5).is_finite is True\n    assert asech(0.5).is_finite is True\n    assert acsch(1).is_finite is True\n    assert acoth(2).is_finite is True\n", "def test_hyperbolic_is_zero():\n    x = Symbol('x')\n    assert sinh(0).is_zero is True\n    assert cosh(0).is_zero is False\n    assert tanh(0).is_zero is True\n    assert coth(0).is_zero is False\n    assert csch(0).is_zero is False\n    assert sech(0).is_zero is False\n    assert asinh(0).is_zero is True\n    assert acosh(1).is_zero is True\n    assert atanh(0).is_zero is True\n    assert acoth(1).is_zero is False\n    assert asech(1).is_zero is True\n    assert acsch(0).is_zero is False\n"], "sample_1158": ["def test_sympify_convert_xor():\n    assert sympify('x^y', convert_xor=True) == x**y\n    assert sympify('x^y', convert_xor=False) == Xor(x, y)\n    assert sympify('x^y') == x**y  # default is True\n", "def test_issue_22081():\n    # test to check if sympify correctly handles nested lists\n    assert sympify([[1, 2], [3, 4]]) == [[1, 2], [3, 4]]\n    assert sympify([[1, 2], [3, '4']]) == [[1, 2], [3, Symbol('4')]]\n    # test to check if sympify correctly handles nested dictionaries\n    assert sympify({'a': 1, 'b': 2}) == {'a': 1, 'b': 2}\n    assert sympify({'a': 1, 'b': '2'}) == {'a': 1, 'b': Symbol('2')}\n    assert sympify({'a': [1, 2], 'b': [3, 4]}) == {'a': [1, 2], 'b': [3, 4]}\n    assert sympify({'a': [1, 2], 'b': [3, '4']}) == {'a': [1, 2], 'b': [3, Symbol('4')]}\n", "def test_issue_22139():\n    assert sympify('1:x') == Slice(1, Symbol('x'), None)\n    assert sympify(':x') == Slice(None, Symbol('x'), None)\n    assert sympify('1:4:2') == Slice(1, 4, 2)\n    assert sympify('4:1:-2') == Slice(4, 1, -2)\n    assert sympify('1::2') == Slice(1, None, 2)\n    assert sympify('::2') == Slice(None, None, 2)\n    assert sympify('1:4:') == Slice(1, 4, None)\n    assert sympify(':4') == Slice(None, 4, None)\n    assert sympify('1::') == Slice(1, None, None)\n    assert sympify('::') == Slice(None, None, None)\n", "def test_sympify_strict():\n    assert sympify(x, strict=True) == x\n    assert sympify(1, strict=True) == Integer(1)\n    assert sympify(1.0, strict=True) == Float(1.0)\n    raises(SympifyError, lambda: sympify('x', strict=True))\n    raises(SympifyError, lambda: sympify([1, 2], strict=True))\n    raises(SympifyError, lambda: sympify((1, 2), strict=True))\n    raises(SympifyError, lambda: sympify({'a': 1}, strict=True))\n", "def test_issue_21621():\n    # Test case for evaluating the sympify function with None value and evaluate=True.\n    assert sympify(None, evaluate=True) is None\n"], "sample_1161": ["def test_printing_str_trigonometric():\n    assert sstr(sin(x)) == \"sin(x)\"\n    assert sstr(cos(x)) == \"cos(x)\"\n    assert sstr(sin(x + y)) == \"sin(x + y)\"\n    assert sstr(cos(x + y)) == \"cos(x + y)\"\n    assert sstr(sin(x)**2 + cos(x)**2) == \"sin**2(x) + cos**2(x)\"\n", "def test_printing_str_transfer_function():\n    from sympy import TransferFunction\n    tf1 = TransferFunction(x, x + 1)\n    assert sstr(tf1) == \"TransferFunction(x, x + 1)\"\n", "def test_ArraySymbol():\n    assert sstr(ArraySymbol(\"A\", 2, 3, 4)) == \"A\"\n    assert sstr(ArraySymbol(\"A\", 2, 3, 4), order=None) == \"A\"\n    assert sstr(ArraySymbol(\"A\", 2, 3, 4), full_prec=True) == \"A\"\n    assert sstr(ArraySymbol(\"A\", 2, 3, 4), full_prec=False) == \"A\"\n", "def test_issue_21460():\n    ss = lambda x: str(S(x, evaluate=False))\n    assert ss('2*3/2') == '2*3/2'\n    assert ss('2*3/-2') == '2*3/(-2)'\n    assert ss('2*3/(-2*3)') == '2*3/(-2*3)'\n    assert ss('2*3/(-2)/3') == '2*3/(-2*3)'\n", "def test_printing_str_float_rational():\n    assert sstr(Rational(1, 2).evalf()) == '0.5'\n    assert sstr(Float(0.5)) == '0.5'\n    assert sstr(Float('0.5')) == '0.5'\n"], "sample_1159": ["def test_issue_16847():\n    from sympy import Poly, Symbol\n    x = Symbol('x', algebraic=True)\n    p = Poly(x**2 + 2*x + 1, x)\n    assert p.is_algebraic\n    assert p.as_expr().is_algebraic\n    x = Symbol('x', algebraic=False)\n    p = Poly(x**2 + 2*x + 1, x)\n    assert p.is_algebraic is None\n    assert p.as_expr().is_algebraic is None\n", "def test_extended_real():\n    x = Symbol('x', extended_real=True)\n    assert x.is_real is None\n    assert x.is_imaginary is None\n    assert x.is_complex is None\n    assert x.is_extended_real is True\n    assert x.is_extended_positive is None\n    assert x.is_extended_negative is None\n    assert x.is_finite is None\n    assert x.is_infinite is None\n    y = Symbol('y', real=True)\n    assert y.is_extended_real is True\n    z = Symbol('z', extended_real=False)\n    assert z.is_real is False\n    assert z.is_imaginary is None\n    assert z.is_complex is None\n    assert z.is_extended_real is False\n    assert z.is_extended_positive is False\n    assert z.is_extended_negative is False\n    assert z.is_finite is None\n    assert z.is_infinite is None\n", "def test_Mul_is_composite():\n    x = Symbol('x', positive=True, integer=True)\n    y = Symbol('y', positive=True, integer=True)\n    assert (x*y).is_prime is None\n    assert ( (x+1)*(y+1) ).is_prime is False\n    assert ( (x+1)*(y+1) ).is_composite is True\n\n    x = Symbol('x', positive=True)\n    assert ( (x+1)*(y+1) ).is_prime is None\n    assert ( (x+1)*(y+1) ).is_composite is None\n\n    x = Symbol('x', positive=True, even=True)\n    y = Symbol('y', positive=True, even=True)\n    assert (x*y).is_prime is False\n    assert (x*y).is_composite is True\n\n    x = Symbol('x', positive=True, even=False)\n    y = Symbol('y', positive=True, even=False)\n    assert (x*y).is_prime is None\n    assert (x*y).is_composite is None\n", "def test_issue_19182():\n    x = Symbol('x', real=True, finite=True)\n    y = Symbol('y', real=True, finite=False)\n    assert (x*y).is_finite is None\n    assert (y*y).is_finite is False\n    assert (y + y).is_finite is False\n", "def test_issue_18555():\n    x = Symbol('x', extended_real=True, nonnegative=True)\n    assert x.is_extended_nonnegative is True\n    assert x.is_extended_positive is None\n    assert x.is_nonnegative is None\n    assert x.is_positive is None\n"], "sample_1160": ["def test_issue_18999():\n    n = Symbol('n', integer=True)\n    assert Range(n, n + 100).is_finite_set is True\n    assert Range(n - 100, n).is_finite_set is True\n    assert Range(n, -3, -1).is_finite_set is True\n    assert Range(n, 3, 1).is_finite_set is True\n    assert Range(n, n + n).is_finite_set is True\n    assert Range(n - n, n).is_finite_set is True\n", "def test_issue_20857():\n    f = ImageSet(Lambda(x, x**2), S.Naturals)\n    g = ImageSet(Lambda(x, x**2), S.Integers)\n    assert f.is_subset(g)\n    assert f.issubset(g)\n    assert f.is_subset(S.Integers)\n    assert f.issubset(S.Integers)\n    assert f.is_subset(S.Naturals)\n    assert f.issubset(S.Naturals)\n    assert not f.is_subset(S.Rationals)\n    assert not f.issubset(S.Rationals)\n", "def test_issue_18195():\n    # check that non-integer step size is handled\n    assert Range(-3, 3, S.Half) == FiniteSet(-3, -Rational(5, 2), -2, -Rational(3, 2),\n                                             -1, -S.Half, 0, S.Half, 1, Rational(3, 2), 2, Rational(5, 2), 3)\n    raises(TypeError, lambda: Range(1, 10, x))\n", "def test_interval_step():\n    interval = Interval(1, 2, True, False)\n    raises(TypeError, lambda: interval.step)\n    interval = Interval(1, 2, False, False)\n    raises(TypeError, lambda: interval.step)\n    interval = Interval(1, 2, True, True)\n    raises(TypeError, lambda: interval.step)\n    interval = Interval(1, 2, False, True)\n    raises(TypeError, lambda: interval.step)\n    interval = Interval(0, 4, False, True, step=2)\n    assert interval.step == 2\n    raises(ValueError, lambda: Interval(0, 4, False, True, step=-1))\n    raises(ValueError, lambda: Interval(0, 4, False, True, step=0))\n    raises(ValueError, lambda: Interval(0, 4, False, True, step=5))\n    raises(ValueError, lambda: Interval(0, 4, False, True, step=-2))\n    interval = Interval(0, 4, False, False, step=2)\n    assert interval.step == 2\n    raises(ValueError, lambda: Interval(0, 4, False, False, step=-1))\n    raises(ValueError, lambda: Interval(0, 4, False, False, step=0))\n    raises(ValueError, lambda: Interval(0, 4, False, False, step=5))\n    raises(ValueError, lambda: Interval(0, 4, False, False, step=-2))\n    interval = Interval(0, 4, True, True, step=2)\n    assert interval.step == 2\n    raises(ValueError, lambda: Interval(0, 4, True, True, step=-1))\n    raises(ValueError, lambda: Interval(0, 4, True, True, step=0))\n    raises(ValueError, lambda: Interval(0, 4, True, True, step=5))\n    raises(ValueError, lambda: Interval(0, 4, True, True, step=-2))\n    interval = Interval(0, 4, True, False, step=2)\n    assert interval.step == 2\n    raises(ValueError, lambda: Interval(0, 4, True, False, step=-1))\n    raises(ValueError, lambda: Interval(0, 4, True, False, step=0))\n    raises(ValueError, lambda: Interval(0, 4, True", "def test_Range_inheritance():\n    assert issubclass(Range, Set)\n    assert isinstance(Range(0, 10), Set)\n    assert Range(0, 10).is_subset(S.Reals)\n    assert Range(0, 10).is_subset(S.Integers)\n    assert not Range(0, 10).is_subset(S.Naturals)\n"], "sample_1162": ["def test_Func_kind():\n    from sympy.core.function import Function\n    f = Function('f')\n    assert f(comm_x).kind is NumberKind\n    assert f(noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    from sympy import Function, sin\n    f = Function('f')\n    assert f(comm_x).kind is UndefinedKind\n    assert sin(comm_x).kind is NumberKind\n    assert sin(noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    from sympy.functions import sin\n    assert sin(comm_x).kind is NumberKind\n    assert sin(noncomm_x).kind is UndefinedKind\n", "def test_FunctionKind():\n    from sympy.core.function import Function, UndefinedFunction, Lambda\n    from sympy.core.function import WildFunction, Subs\n    from sympy.core import symbols\n    f = Function('f')\n    x, y = symbols('x y')\n    assert f(x).kind is NumberKind\n    assert f(x, y).kind is NumberKind\n    g = UndefinedFunction('g')\n    assert g(x).kind is UndefinedKind\n    assert g(x, y).kind is UndefinedKind\n    l = Lambda((x, y), x + y)\n    assert l.kind is NumberKind\n    w = WildFunction('w')\n    assert w.kind is NumberKind\n    s = Subs(x, x, y)\n    assert s.kind is NumberKind\n", "def test_MatrixSymbol_kind():\n    assert MatrixSymbol('A', 2,2).kind is MatrixKind(UndefinedKind)\n    assert MatrixSymbol('A', 2,2, commutative=False).kind is MatrixKind(UndefinedKind)\n    assert MatrixSymbol('A', 2,2, commutative=True).kind is MatrixKind(NumberKind)\n"], "sample_1163": ["def test_polar_lift():\n    from sympy import polar_lift, I, pi, exp_polar, oo\n    x = Symbol('x')\n    assert polar_lift(x).is_polar is True\n    assert polar_lift(x).is_comparable is False\n    assert polar_lift(1).doit() == exp_polar(0)\n    assert polar_lift(-1).doit() == exp_polar(I*pi)\n    assert polar_lift(I).doit() == exp_polar(I*pi/2)\n    assert polar_lift(-I).doit() == exp_polar(-I*pi/2)\n    assert polar_lift(exp_polar(I*pi/2)).doit() == exp_polar(I*pi/2)\n    assert polar_lift(x*exp_polar(I*pi/2)).doit() == x*exp_polar(I*pi/2)\n    assert polar_lift(oo*exp_polar(I*pi/4)).doit() == oo*exp_polar(I*pi/4)\n", "def test_polar_lift():\n    from sympy import polar_lift, exp_polar, pi, I\n    x = Symbol('x')\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == 1\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(x).func == polar_lift\n    assert polar_lift(polar_lift(x)) == polar_lift(x)\n\n    p = Symbol('p', positive=True)\n    assert polar_lift(p) == p\n", "def test_issue_18871():\n    x = Symbol('x')\n    assert unpolarify(exp_polar(x)).simplify() == exp(x)\n    assert unpolarify(exp_polar(x)).func != exp_polar\n    p = Symbol('p', extended_positive=True)\n    assert unpolarify(exp_polar(x), exponents_only=True).simplify() == exp_polar(p*x/x)\n    assert unpolarify(exp_polar(x), exponents_only=True).func == exp_polar\n", "def test_polar_lift():\n    from sympy import polar_lift, I, pi, exp_polar\n    x = Symbol('x')\n    assert polar_lift(5) == 5*exp_polar(0)\n    assert polar_lift(2 + 3*I) == polar_lift(2 + 3*I)\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(exp_polar(2*pi*I)*x) == polar_lift(x)\n    assert polar_lift(exp_polar(pi*I)*x) == exp_polar(pi*I)*polar_lift(x)\n    assert polar_lift(pi) == pi*exp_polar(0)\n    assert polar_lift(-pi) == pi*exp_polar(I*pi)\n", "def test_issue_16343():\n    from sympy import Symbol, Abs, re, im, Q\n    x = Symbol('x')\n    assert Abs(x).is_extended_real\n    assert Abs(x).is_real is None\n    assert re(Abs(x)).is_real\n    assert im(Abs(x)).is_real\n    assert Abs(x).is_nonnegative\n    assert Abs(x).is_positive is None\n    x = Symbol('x', real=True)\n    assert re(x).is_real\n    assert im(x).is_real\n    assert re(x).is_zero is None\n    assert im(x).is_zero\n    assert re(x).is_positive is None\n    assert im(x).is_positive is False\n    assert re(x).is_nonnegative is None\n    assert im(x).is_nonnegative\n    x = Symbol('x', real=True, zero=False)\n    assert re(x).is_real\n    assert im(x).is_real\n    assert re(x).is_zero is False\n    assert im(x).is_zero\n    assert re(x).is_positive is None\n    assert im(x).is_positive is False\n    assert re(x).is_nonnegative is None\n    assert im(x).is_nonnegative\n    assert Q.positive(re(x)) == Q.positive(x)\n    assert Q.nonnegative(re(x)) == Q.nonnegative(x)\n    assert Q.zero(re(x)) == Q.zero(x)\n    assert Q.negative(re(x)) == Q.negative(x)\n    assert Q.nonpositive(re(x)) == Q.nonpositive(x)\n    assert Q.positive(im(x)) is False\n    assert Q.nonnegative(im(x)) is True\n    assert Q.zero(im(x)) is True\n    assert Q.negative(im(x)) is False\n    assert Q.nonpositive(im(x)) is True\n"], "sample_1165": ["def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    assert 2 / q1 == 2 * q1.inverse()\n\n    q0 = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q1 / q0)\n    raises(ValueError, lambda: 2 / q0)\n", "def test_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    raises(ZeroDivisionError, lambda: q1 / 0)\n    q0 = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q1 / q0)\n", "def test_quaternion_non_numeric_input():\n    w, x, y, z = symbols('w:z')\n    q = Quaternion(w, x, y, z)\n\n    assert q.diff(w) == Quaternion(1, 0, 0, 0)\n    assert q.diff(x) == Quaternion(0, 1, 0, 0)\n    assert q.diff(y) == Quaternion(0, 0, 1, 0)\n    assert q.diff(z) == Quaternion(0, 0, 0, 1)\n\n    assert integrate(q, w) == Quaternion(w**2 / 2, w*x, w*y, w*z)\n    assert integrate(q, x) == Quaternion(w*x, x**2 / 2, x*y, x*z)\n    assert integrate(q, y) == Quaternion(w*y, x*y, y**2 / 2, y*z)\n    assert integrate(q, z) == Quaternion(w*z, x*z, y*z, z**2 / 2)\n\n    assert q.norm() == sqrt(w**2 + x**2 + y**2 + z**2)\n    assert conjugate(q) == Quaternion(w, -x, -y, -z)\n", "def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n\n    q0 = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q1 / q0)\n    raises(ValueError, lambda: q1 / (0 + 0*I))\n", "def test_quaternion_issue_with_non_integer_power():\n    q = Quaternion(1, 2, 3, 4)\n    n = Symbol('n', real=True)\n    raises(NotImplemented, lambda: q.pow(n))\n    assert q.pow_cos_sin(4) == Quaternion(900*cos(4*acos(sqrt(30)/30)),\n                                          1800*sqrt(29)*sin(4*acos(sqrt(30)/30))/29,\n                                          2700*sqrt(29)*sin(4*acos(sqrt(30)/30))/29,\n                                          3600*sqrt(29)*sin(4*acos(sqrt(30)/30))/29)\n"], "sample_1164": ["def test_cg_simp():\n    a, alpha, b, beta, c, gamma = symbols('a alpha b beta c gamma')\n    # test Varshalovich 8.7.1 eq 1\n    cg1 = CG(a, alpha, b, 0, a, alpha)\n    cg2 = CG(a, alpha, b, 0, a, alpha).doit()\n    assert cg_simp(cg1**2) == (2*a + 1)*KroneckerDelta(b, 0)\n    assert cg_simp(cg2**2) == (2*a + 1)*KroneckerDelta(b, 0)\n    # test Varshalovich 8.7.1 eq 2\n    cg1 = (-1)**(a - alpha)*CG(a, alpha, a, -alpha, 0, 0)\n    cg2 = (-1)**(a - alpha)*CG(a, alpha, a, -alpha, 0, 0).doit()\n    assert cg_simp(cg1) == sqrt(2*a + 1)\n    assert cg_simp(cg2) == sqrt(2*a + 1)\n    # test Varshalovich 8.7.2 eq 9\n    cg1 = CG(a, alpha, b, beta, c, gamma)*CG(a, alpha, b, beta, c, gamma)\n    cg2 = CG(a, alpha, b, beta, c, gamma).doit()*CG(a, alpha, b, beta, c, gamma).doit()\n    assert cg_simp(cg1) == 1\n    assert cg_simp(cg2) == 1\n", "def test_cg_simp():\n    # Test the simplification of Clebsch-Gordan coefficients\n    j1, j2, j3, m1, m2, m3 = symbols('j1 j2 j3 m1 m2 m3')\n    cg = CG(j1, m1, j2, m2, j3, m3)\n    assert cg_simp(cg**2) == 1\n    assert cg_simp(cg*cg) == 1\n    assert cg_simp(cg + cg) == 2*cg\n", "compilation error", "def test_cg_simp_add():\n    cg1 = CG(1, 1, 0, 0, 1, 1)\n    cg2 = CG(1, 0, 0, 0, 1, 0)\n    cg3 = CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(cg1 + cg2 + cg3) == 3\n    assert cg_simp(cg1 + cg2) == cg1 + cg2\n    assert cg_simp(cg1 + cg1) == 2*cg1\n", "def test_cg_simp():\n    j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3')\n    # Test sum identity\n    a = CG(j1, m1, j2, m2, j3, m3)\n    b = CG(j1, m1, j2, m2, j3, m3)\n    result = cg_simp(a + b)\n    assert result == 2*a\n\n    # Test product identity\n    a = CG(j1, m1, j2, m2, j3, m3)\n    b = CG(j1, m1, j2, m2, j3, m3)\n    result = cg_simp(a * b)\n    assert result == a**2\n\n    # Test sum identity with different terms\n    a = CG(j1, m1, j2, m2, j3, m3)\n    b = CG(j2, m2, j1, m1, j3, m3)\n    result = cg_simp(a + b)\n    assert result == a + b\n\n    # Test product identity with different terms\n    a = CG(j1, m1, j2, m2, j3, m3)\n    b = CG(j2, m2, j1, m1, j3, m3)\n    result = cg_simp(a * b)\n    assert result == a*b\n\n    # Test Varshalovich 8.7.1 Equation 1\n    result = cg_simp(CG(j1, m1, 0, 0, j1, m1))\n    assert result == 1\n\n    # Test Varshalovich 8.7.1 Equation 2\n    result = cg_simp((-1)**(j1-m1)*CG(j1, m1, j1, -m1, 0, 0))\n    assert result == sqrt(2*j1+1)\n\n    # Test Varshalovich 8.7.2 Equation 4\n    result = cg_simp(CG(j1, m1, j2, m2, j3, m3)*CG(j1, m1, j2, m2, j3, m3))\n    assert result == 1\n\n    # Test Varshalovich 8.7.2 Equation "], "sample_1166": ["def test_MonomialOps():\n    m = MonomialOps(3)\n\n    assert m.mul()( (1, 2, 3), (4, 5, 6) ) == (5, 7, 9)\n    assert m.pow()( (1, 2, 3), 2 ) == (2, 4, 6)\n    assert m.mulpow()( (1, 2, 3), (4, 5, 6), 2 ) == (9, 12, 15)\n    assert m.ldiv()( (4, 5, 6), (1, 2, 3) ) == (3, 3, 3)\n    assert m.div()( (4, 5, 6), (1, 2, 3) ) == (3, 3, 3)\n    assert m.lcm()( (1, 2, 3), (4, 5, 6) ) == (4, 5, 6)\n    assert m.gcd()( (1, 2, 3), (4, 5, 6) ) == (1, 2, 3)\n\n    raises(ExactQuotientFailed, lambda: m.div()( (1, 2, 3), (4, 5, 6) ))\n", "def test_MonomialOps():\n    ops = MonomialOps(3)\n\n    assert ops.mul()( (3, 4, 1), (1, 2, 0) ) == (4, 6, 1)\n    assert ops.pow()( (3, 4, 1), 2 ) == (6, 8, 2)\n    assert ops.mulpow()( (3, 4, 1), (1, 2, 0), 2 ) == (5, 8, 1)\n    assert ops.ldiv()( (3, 4, 1), (1, 2, 0) ) == (2, 2, 1)\n    assert ops.div()( (3, 4, 1), (1, 2, 0) ) == (2, 2, 1)\n    assert ops.lcm()( (3, 4, 1), (1, 2, 0) ) == (3, 4, 1)\n    assert ops.gcd()( (3, 4, 1), (1, 2, 0) ) == (1, 2, 0)\n\n    raises(ExactQuotientFailed, lambda: ops.div()( (3, 4, 1), (5, 2, 0) ))\n", "def test_MonomialOps():\n    ops = MonomialOps(3)\n    m = (3, 4, 1)\n    n = (1, 2, 0)\n\n    assert ops.mul()(m, n) == (4, 6, 1)\n    assert ops.pow()(m, 3) == (9, 12, 3)\n    assert ops.mulpow()(m, n, 3) == (6, 10, 1)\n    assert ops.ldiv()(m, n) == (2, 2, 1)\n    assert ops.div()(m, n) == (2, 2, 1)\n    assert ops.lcm()(m, n) == (3, 4, 1)\n    assert ops.gcd()(m, n) == (1, 2, 0)\n", "def test_MonomialOps():\n    ops = MonomialOps(3)\n    assert ops.mul()(A=(1, 2, 3), B=(4, 5, 6)) == (5, 7, 9)\n    assert ops.pow()(A=(1, 2, 3), k=2) == (2, 4, 6)\n    assert ops.mulpow()(A=(1, 2, 3), B=(4, 5, 6), k=2) == (9, 12, 15)\n    assert ops.ldiv()(A=(4, 5, 6), B=(1, 2, 3)) == (3, 3, 3)\n    assert ops.div()(A=(4, 5, 6), B=(1, 2, 3)) == (3, 3, 3)\n    assert ops.lcm()(A=(1, 2, 3), B=(4, 5, 6)) == (4, 5, 6)\n    assert ops.gcd()(A=(4, 5, 6), B=(1, 2, 3)) == (1, 2, 3)\n", "def test_term_div():\n    from sympy.polys.domains import ZZ\n\n    assert term_div(((3, 4, 1), 2), ((1, 2, 0), 1), ZZ) == ((2, 2, 1), 2)\n    assert term_div(((3, 4, 1), 2), ((1, 2, 3), 1), ZZ) is None\n"], "sample_1167": ["def test_latex_TensorIndexedType():\n    from sympy.tensor.tensor import TensorIndexType\n    L = TensorIndexType(\"L\")\n    assert latex(L) == r\"L\"\n", "def test_latex_SparseMatrix():\n    from sympy import SparseMatrix\n    M = SparseMatrix(3, 3, {(0, 0): x, (1, 1): y, (2, 2): z})\n    assert latex(M) == \\\n        r'\\left[\\begin{matrix}x & 0 & 0\\\\0 & y & 0\\\\0 & 0 & z\\end{matrix}\\right]'\n", "def test_issue_22048():\n    # issue 22048\n    from sympy import limit\n    from sympy.abc import z\n    expr = limit((z**2 - 4)/(z - 2), z, 2)\n    assert latex(expr) == r\"4\"\n", "def test_emptyPrinter_str_conversion():\n    # Check that `str` conversion of unknown types is handled gracefully\n    class MyObject:\n            return \"MyObject\"\n\n    assert latex(MyObject()) == r'\\mathtt{\\text{MyObject}}'\n\n    # Check that `str` conversion is escaped correctly\n    class MyObject:\n            return \"My~Object\"\n\n    assert latex(MyObject()) == r'\\mathtt{\\text{My\\textasciitilde Object}}'\n", "def test_latex_MatPow():\n    A = MatrixSymbol('A', 3, 3)\n    assert latex(MatPow(A, 3)) == r\"A^{3}\"\n    assert latex(MatPow(A, 0)) == r\"A^{0}\"\n    assert latex(MatPow(A, -3)) == r\"A^{-3}\"\n    assert latex(MatPow(A, Symbol('n'))) == r\"A^{n}\"\n"], "sample_1168": ["def test_roundrobin():\n    assert list(roundrobin([1, 2, 3], 'abc')) == [1, 'a', 2, 'b', 3, 'c']\n    assert list(roundrobin([1, 2, 3, 4], 'abc')) == [1, 'a', 2, 'b', 3, 'c', 4]\n    assert list(roundrobin([1, 2, 3], 'abcd')) == [1, 'a', 2, 'b', 3, 'c', 'd']\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([0, 1], [2, 3, 4])) == [0, 2, 1, 3, 4]\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin('ABC', 'D', 'EF', 'G')) == [\n        'A', 'D', 'E', 'G', 'B', 'F', 'C']\n    assert list(roundrobin('ABC', '', 'EF')) == ['A', 'E', 'B', 'F', 'C']\n", "def test_roundrobin():\n    letters = ['A', 'B', 'C']\n    numbers = [1, 2, 3]\n    assert list(roundrobin(letters, numbers)) == ['A', 1, 'B', 2, 'C', 3]\n    assert list(roundrobin(letters, numbers, [True, False])) == [\n        'A', 1, True, 'B', 2, False, 'C', 3]\n    assert list(roundrobin('ABC', '123', 'abc')) == [\n        'A', '1', 'a', 'B', '2', 'b', 'C', '3', 'c']\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n"], "sample_1169": ["def test_canonical_ordering_AntiSymmetricTensor_with_equivalent_internal_lines():\n    v = symbols(\"v\")\n\n    c, d = symbols(('c','d'), above_fermi=True,\n                                   cls=Dummy)\n    k, l = symbols(('k','l'), below_fermi=True,\n                                   cls=Dummy)\n\n    # any set of internal lines always gives the same result, \n    # regardless of their order after canonical ordering\n    assert AntiSymmetricTensor(v, (k, l), (c, d)) == AntiSymmetricTensor(v, (l, k), (d, c))\n    assert AntiSymmetricTensor(v, (l, k), (d, c)) == AntiSymmetricTensor(v, (k, l), (c, d))\n    assert AntiSymmetricTensor(v, (k, l), (d, c)) == AntiSymmetricTensor(v, (l, k), (c, d))\n    assert AntiSymmetricTensor(v, (l, k), (c, d)) == AntiSymmetricTensor(v, (k, l), (d, c))\n", "def test_canonical_ordering_AntiSymmetricTensor2():\n    v = symbols(\"v\")\n\n    c, d = symbols(('c','d'), above_fermi=True,\n                                   cls=Dummy)\n    k, l = symbols(('k','l'), below_fermi=True,\n                                   cls=Dummy)\n\n    # formerly, the left gave either the left or the right\n    assert AntiSymmetricTensor(v, (c, d), (k, l)\n        ) == -AntiSymmetricTensor(v, (d, c), (k, l))\n", "def test_substitute_dummies_with_pretty_indices():\n    i, j = symbols('i,j', below_fermi=True, cls=Dummy)\n    a, b = symbols('a,b', above_fermi=True, cls=Dummy)\n    p, q = symbols('p,q', cls=Dummy)\n    f = Function('f')\n    my_dummies = { 'above':'st', 'below':'uv', 'general':'xy' }\n    assert substitute_dummies(f(i, a, p) - f(j, b, q), new_indices=True, pretty_indices=my_dummies) == 0\n", "def test_canonical_ordering_AntiSymmetricTensor_with_same_type():\n    v = symbols(\"v\")\n\n    c, d = symbols(('c','d'), above_fermi=True,\n                                   cls=Dummy)\n    k, l = symbols(('k','l'), below_fermi=True,\n                                   cls=Dummy)\n\n    # formerly, the left gave either the left or the right\n    assert AntiSymmetricTensor(v, (c, d), (k, l)\n        ) == -AntiSymmetricTensor(v, (d, c), (k, l))\n", "def test_BosonState():\n    assert str(FockStateBosonKet([1, 2])) == '|(1, 2)>'\n    assert str(FockStateBosonBra([1, 2])) == '<(1, 2)|'\n    assert FockStateBosonKet([1, 2])._labels() == (1, 2)\n    assert FockStateBosonBra([1, 2])._labels() == (1, 2)\n    assert FockStateBosonKet([1, 2]).lbracket == '|'\n    assert FockStateBosonKet([1, 2]).rbracket == '>'\n    assert FockStateBosonBra([1, 2]).lbracket == '<'\n    assert FockStateBosonBra([1, 2]).rbracket == '|'\n    assert FockStateBosonBra([1, 2]).__len__() == 2\n    assert FockStateBosonKet([1, 2]).__len__() == 2\n"], "sample_1170": ["def test_printing_str_array_expressions2():\n    X = ArraySymbol(\"X\", 2, 3, 4)\n    assert sstr(Subs(X, (x,), (1,))) == 'Subs(X, x, 1)'\n    assert sstr(Subs(X, (x, y), (1, 2))) == 'Subs(X, (x, y), (1, 2))'\n    assert sstr(X.applyfunc(sin)) == 'Lambda(_d, sin(_d)).(X)'\n    assert sstr(X.applyfunc(lambda x: x**2)) == 'Lambda(_d, _d**2).(X)'\n    lamda = Lambda(x, 1/x)\n    assert sstr(X.applyfunc(lamda)) == 'Lambda(x, 1/x).(X)'\n", "def test_trace():\n    A, B = symbols('A B', commutative=False)\n    assert str(Tr(A*B)) == 'Tr(A*B)'\n    assert str(Tr(A + B)) == 'Tr(A) + Tr(B)'\n    assert str(Tr(A - B)) == 'Tr(A) - Tr(B)'\n", "def test_printing_str_quaternion():\n    assert sstr(Quaternion(1, 2, 3, 4)) == \"1 + 2*i + 3*j + 4*k\"\n    assert sstr(Quaternion(x, y, z, w)) == \"x + y*i + z*j + w*k\"\n    assert sstr(Quaternion(1, 2, 3, 4).conjugate()) == \"1 - 2*i - 3*j - 4*k\"\n", "def test_ElementwiseApplyFunction():\n    A = MatrixSymbol(\"A\", 2, 2)\n    expr = ElementwiseApplyFunction(sin, A)\n    assert str(expr) == \"sin.(A)\"\n", "def test_DMP_str():\n    from sympy.polys.domains import ZZ\n    from sympy.polys.densearith import dmp_add\n    f = ZZ.map([1, 2, 3])\n    g = ZZ.map([4, 5, 6])\n    h = dmp_add(f, g, ZZ)\n    assert sstr(h) == \"Poly([5, 7, 9], x, domain='ZZ')\"\n"], "sample_1171": ["def test_issue_19655():\n    fset1 = FiniteSet(1, 2)\n    fset2 = FiniteSet(3, 4)\n    pset = ProductSet(fset1, fset2)\n    imset = ImageSet(Lambda(x, x), pset)\n    assert len(pset) == 4\n    assert len(imset) == 4\n", "def test_issue_20468():\n    assert Intersection(Range(0, 5), Interval(1, 4)) == Range(1, 5)\n    assert Intersection(Range(0, 5, 2), Interval(1, 4)) == Range(2, 5, 2)\n    assert Intersection(Range(5, 0, -1), Interval(1, 4)) == Range(4, 0, -1)\n    assert Intersection(Range(5, 0, -2), Interval(1, 4)) == Range(4, 0, -2)\n", "def test_issue_20242():\n    from sympy import Rational\n    r = Range(Rational(1, 3), Rational(8, 3))\n    assert len(r) == 7\n    assert list(r) == [Rational(1, 3), Rational(2, 3), 1, Rational(4, 3),\n                      Rational(5, 3), 2, Rational(7, 3)]\n    assert r.contains(Rational(1, 3)) is True\n    assert r.contains(Rational(2, 3)) is True\n    assert r.contains(Rational(8, 3)) is False\n", "def test_issue_19767():\n    R = Range(-oo, 1)\n    assert R.intersect(S.Reals) == R\n    assert R.inf == -oo\n    assert R.sup == 1\n    assert 0 in R\n    assert 2 not in R\n", "def test_Range_slice_with_symbolic_value():\n    x = Symbol('x')\n    r = Range(1, 10)\n    raises(TypeError, lambda: r[x])\n    raises(TypeError, lambda: r[x:5])\n    raises(TypeError, lambda: r[2:x])\n    raises(TypeError, lambda: r[x:5:2])\n    raises(TypeError, lambda: r[2:x:2])\n"], "sample_1173": ["def test_split_symbols_custom():\n        if symbol not in ('unsplittable',):\n            return _token_splittable(symbol)\n        return False\n\n    transformations = standard_transformations + \\\n                      (split_symbols_custom(can_split), implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n\n    assert parse_expr(\"xy\", transformations=transformations) == x*y\n    assert parse_expr(\"unsplittable\", transformations=transformations) == Symbol('unsplittable')\n    assert parse_expr(\"fxy(x+1)\", transformations=transformations) == f*x*y*(x+1)\n", "def test_evaluate_false_doit_issue_19505():\n    x = Symbol('x')\n    inputs = [\n        'Abs(x)', 'im(x)', 're(x)', 'sign(x)', 'arg(x)', 'conjugate(x)',\n        'acos(x)', 'acot(x)', 'acsc(x)', 'asec(x)', 'asin(x)', 'atan(x)',\n        'acosh(x)', 'acoth(x)', 'acsch(x)', 'asech(x)', 'asinh(x)', 'atanh(x)',\n        'cos(x)', 'cot(x)', 'csc(x)', 'sec(x)', 'sin(x)', 'tan(x)',\n        'cosh(x)', 'coth(x)', 'csch(x)', 'sech(x)', 'sinh(x)', 'tanh(x)',\n        'exp(x)', 'log(x)', 'sqrt(x)', 'cbrt(x)', 'ln(x)'\n    ]\n    for case in inputs:\n        expr = parse_expr(case, evaluate=False)\n        assert expr.doit() == parse_expr(case)\n", "def test_issue_implicit_multiplication():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication, )\n\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('2x', transformations=transformations) == 2*x\n    assert parse_expr('2 x', transformations=transformations) == 2*x\n    assert parse_expr('2(x)', transformations=transformations) == 2*x\n    assert parse_expr('2 (x)', transformations=transformations) == 2*x\n    assert parse_expr('(2)x', transformations=transformations) == 2*x\n    assert parse_expr('(2) x', transformations=transformations) == 2*x\n    assert parse_expr('(2) (x)', transformations=transformations) == 2*x\n    assert parse_expr('2x(y)', transformations=transformations) == 2*x*(y)\n    assert parse_expr('2 x(y)', transformations=transformations) == 2*x*(y)\n    assert parse_expr('2 (x) (y)', transformations=transformations) == 2*x*(y)\n    assert parse_expr('(2)x(y)', transformations=transformations) == 2*x*(y)\n    assert parse_expr('(2) x(y)', transformations=transformations) == 2*x*(y)\n    assert parse_expr('(2) (x) (y)', transformations=transformations) == 2*x*(y)\n", "def test_function_as_local_dict():\n    x = Symbol('x')\n    local_dict = {'f': lambda x: x**2}\n    expr = parse_expr('f(x)', local_dict=local_dict)\n    assert expr == x**2\n", "def test_auto_symbol():\n    transformations = standard_transformations + (auto_symbol,)\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr('x', transformations=transformations) == x\n    assert parse_expr('x + y', transformations=transformations) == x + y\n    assert parse_expr('x + 1', transformations=transformations) == x + 1\n    assert parse_expr('sin(x)', transformations=transformations) == sin(x)\n    assert parse_expr('exp(x)', transformations=transformations) == exp(x)\n"], "sample_1172": ["def test_solve_generic():\n    assert solve_generic([x - 1], Options((x,), {'domain': 'ZZ'})) == [(1,)]\n\n    assert solve_generic([y - x, y - x - 1], Options((x, y), {'domain': 'ZZ'})) is None\n\n    assert solve_generic([y - x**2, y + x**2], Options((x, y), {'domain': 'ZZ'})) == [(0, 0)]\n\n    assert solve_generic([2*x - 3, y*Rational(3, 2) - 2*x, z - 5*y], Options((x, y, z), {'domain': 'ZZ'})) == \\\n        [(Rational(3, 2), Integer(2), Integer(10))]\n\n    assert solve_generic([x*y - 2*y, 2*y**2 - x**2], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(0, 0), (2, -2), (2, 2)]\n\n    assert solve_generic([y - x**2, y + x**2 + 1], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(-I*sqrt(S.Half), Rational(-1, 2)), (I*sqrt(S.Half), Rational(-1, 2))]\n\n    raises(NotImplementedError, lambda: solve_generic([x**3 - y**3], Options((x, y), {'domain': 'ZZ'})))\n    raises(NotImplementedError, lambda: solve_generic(\n        [z, -2*x*y**2 + x + y**2*z, y**2*(-z - 4) + 2], Options((x, y, z), {'domain': 'ZZ'})))\n    raises(PolynomialError, lambda: solve_generic([1/x], Options((x,), {'domain': 'ZZ'})))\n\n    raises(NotImplementedError, lambda: solve_generic(\n          [x-1,], Options((x, y), {'domain': 'ZZ'})))\n    raises(NotImplementedError, lambda: solve_generic(\n          [y-1,], Options((x, y), {'domain': 'ZZ'})))\n", "def test_solve_generic():\n    # test case with no solution\n    assert solve_generic([x - 1, x - 2], Options((x,), {'domain': 'ZZ'})) is None\n\n    # test case with one solution\n    assert solve_generic([x - 1, y - 1], Options((x, y), {'domain': 'ZZ'})) == [(1, 1)]\n\n    # test case with two solutions\n    assert solve_generic([x**2 - 1, y - 1], Options((x, y), {'domain': 'ZZ'})) == [(-1, 1), (1, 1)]\n\n    # test case with non-integer solution\n    assert solve_generic([x**2 - 2, y - 1], Options((x, y), {'domain': 'QQ'})) == [\n        (-sqrt(2), 1), (sqrt(2), 1)]\n", "def test_solve_generic():\n    assert solve_generic([Poly(x - 1)], Options((x,), {'domain': 'ZZ'})) == [(1,)]\n\n    assert solve_generic([Poly(y - x), Poly(y - x - 1)], Options((x, y), {'domain': 'ZZ'})) is None\n\n    assert solve_generic([Poly(y - x**2), Poly(y + x**2)], Options((x, y), {'domain': 'ZZ'})) == [(0, 0)]\n\n    assert solve_generic([Poly(2*x - 3), Poly(y*Rational(3, 2) - 2*x), Poly(z - 5*y)], Options((x, y, z), {'domain': 'ZZ'})) == \\\n        [(Rational(3, 2), Integer(2), Integer(10))]\n\n    assert solve_generic([Poly(x*y - 2*y), Poly(2*y**2 - x**2)], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(0, 0), (2, -2), (2, 2)]\n\n    assert solve_generic([Poly(y - x**2), Poly(y + x**2 + 1)], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(-I, -Rational(1, 2)), (I, -Rational(1, 2))]\n", "def test_solve_generic():\n    x, y, z = symbols('x y z')\n\n    # Test for zero-dimensional systems\n    assert solve_generic([x - 1, y - 1, z - 1], Options((x, y, z), {'domain': 'ZZ'})) == [(1, 1, 1)]\n\n    # Test for higher dimensional systems\n    raises(NotImplementedError, lambda: solve_generic([x*y - 1], Options((x, y), {'domain': 'ZZ'})))\n\n    # Test for systems with no solutions\n    assert solve_generic([x - 1, x - 2], Options((x,), {'domain': 'ZZ'})) == None\n\n    # Test for systems with parameters\n    assert solve_generic([x - y, x - 2], Options((x, y), {'domain': 'ZZ'})) == [(2, 2)]\n\n    # Test for systems with multiple solutions\n    assert solve_generic([x**2 - 4, y - 2], Options((x, y), {'domain': 'ZZ'})) == [(-2, 2), (2, 2)]\n", "def test_solve_biquadratic_special_cases():\n    x0, y0 = symbols('x0 y0')\n\n    f_1 = x**2 + y**2\n    f_2 = x**2 + y**2 + 1\n    (f_1, f_2), opt = parallel_poly_from_expr((f_1, f_2), x, y)\n    assert solve_biquadratic(f_1, f_2, opt) is None\n\n    f_1 = (x - x0)**2 + (y - y0)**2\n    f_2 = (x - x0)**2 + (y - y0)**2\n    (f_1, f_2), opt = parallel_poly_from_expr((f_1, f_2), x, y)\n    raises(SolveFailed, lambda: solve_biquadratic(f_1, f_2, opt))\n"], "sample_1174": ["def test_polar_lift():\n    from sympy import polar_lift, exp_polar\n    x = Symbol('x')\n    assert polar_lift(x).is_polar is True\n    assert polar_lift(x).as_real_imag() == (re(x), im(x))\n    assert polar_lift(1 + I).as_real_imag() == (1, 1)\n    assert polar_lift(4).as_real_imag() == (4, 0)\n    assert re(polar_lift(1 + I)) == 1\n    assert im(polar_lift(1 + I)) == 1\n    assert polar_lift(0) == 0\n    assert polar_lift(exp_polar(I*pi)) == -1\n", "def test_issue_19628():\n    from sympy import lambdify\n    x = Symbol('x')\n    f = lambdify(x, re(x))\n    assert f(1 + 2j) == 1\n    f = lambdify(x, im(x))\n    assert f(1 + 2j) == 2\n", "def test_issue_19627_abs_simplify():\n    from sympy import Function, Symbol\n    x = Symbol('x')\n    f = Function('f', positive=True)\n    expr = Abs(f(x)**2)\n    assert expr.simplify() == f(x)**2\n", "def test_issue_21843():\n    # test that sign(1 + 1e-1000*I) returns 1\n    x = 1 + 1e-1000*I\n    assert sign(x) == 1\n    # test that sign(1 + 1e-1000*I) with evaluate=False returns sign\n    assert sign(x, evaluate=False).func == sign\n", "def test_polar_lift():\n    from sympy import polar_lift, pi, I, exp_polar, sqrt\n    from sympy.abc import x\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == 1\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(exp_polar(I*pi/2)) == exp_polar(I*pi/2)\n    assert polar_lift(sqrt(2) + I*sqrt(2)) == sqrt(2) + I*sqrt(2)\n    assert polar_lift(exp_polar(I*pi/4)*sqrt(2)) == exp_polar(I*pi/4)*sqrt(2)\n    assert polar_lift(sqrt(2)*exp_polar(I*pi/4)) == sqrt(2)*exp_polar(I*pi/4)\n"], "sample_1175": ["def test_pretty_Intersection():\n    assert pretty(Intersection(Interval(1, 3), Interval(2, 4))) == '[2, 3]'\n    assert upretty(Intersection(Interval(1, 3), Interval(2, 4))) == '[2, 3]'\n    assert upretty(Intersection({1, 2, 3}, {2, 3, 4})) == '{2, 3}'\n    assert pretty(Intersection({1, 2, 3}, {2, 3, 4})) == '{2, 3}'\n", "def test_pretty_LieDerivative():\n    from sympy.diffgeom import Manifold, Patch, CoordSystem, BaseScalarField, BaseVectorField\n    from sympy.diffgeom.rn import R2, R2_r, R2_p\n    m = Manifold('M', 2)\n    p = Patch('P', m)\n    rect = CoordSystem('rect', p, [x, y])\n    b = BaseScalarField(rect, x*y)\n    v = BaseVectorField(rect, [y, x])\n    assert pretty(LieDerivative(v, b)) == \\\n    'x**2 + y**2'\n", "def test_pretty_TensorProduct():\n    assert upretty(tensorproduct(x, y)) == \"x\u2297y\"\n    assert upretty(tensorproduct(x, y, x)) == \"x\u2297y\u2297x\"\n", "def test_pretty_BasisDependent():\n    from sympy.vector import CoordSys3D, BasisDependent\n\n    # TODO: use more meaningful basis-dependent objects\n    C = CoordSys3D('C')\n    e1 = C.i\n    e2 = C.j\n    e3 = C.k\n\n    v1 = e1\n    v2 = e2\n\n    # Testing printer\n    expr = v1._form_field\n    ascii_str = \\", "def test_pretty_combinatorics():\n    n = Symbol('n', integer=True)\n    r = Symbol('r', integer=True)\n    assert pretty(binomial(n, r)) == \\\n        '/n\\\\\\n| |\\n\\\\r/'\n    assert upretty(binomial(n, r)) == \\\n        '\u239bn\u239e\\n\u239c \u239f\\n\u239dr\u23a0'\n    assert pretty(binomial(5, 2)) == \\\n        '/5\\\\\\n| |\\n\\\\2/'\n    assert upretty(binomial(5, 2)) == \\\n        '\u239b5\u239e\\n\u239c \u239f\\n\u239d2\u23a0'\n"], "sample_1176": ["def test_Float_hash():\n    # Test that two floats with the same value but different precisions have the same hash\n    assert hash(Float('1.0', 15)) == hash(Float('1.0', 30))\n    assert hash(Float('1.0', 15)) == hash(Float('1.0'))\n    assert hash(Float(1.0)) == hash(Float(1.0, 15))\n", "def test_mod_inverse_negative():\n    assert mod_inverse(-3, 11) == 8\n    assert mod_inverse(-5, 11) == 6\n    assert mod_inverse(-21124921, 521512) == 7713\n    assert mod_inverse(-124215421, 5125) == 2981\n    assert mod_inverse(-214, 12515) == 1579\n    assert mod_inverse(-5823991, 3299) == 1442\n    assert mod_inverse(-123, 44) == 37\n    assert mod_inverse(-2, 5) == 3\n    assert mod_inverse(-2, -5) == -3\n    assert mod_inverse(2, -5) == -2\n", "def test_Float_addition():\n    a = Float(0.1, 10)\n    b = Float(0.1, 15)\n    assert a + b == Float(0.2, 15)\n    assert b + a == Float(0.2, 15)\n    assert a + 0.2 == Float(0.3, 10)\n    assert 0.2 + a == Float(0.3, 10)\n", "def test_issue_20655():\n    assert S.One._eval_power(S.Zero) is S.One\n    assert S.NegativeOne._eval_power(S.Zero) is S.One\n", "def test_NegativeOne_power():\n    assert S.NegativeOne**S(4) is S.One\n    assert S.NegativeOne**S(3) is S.NegativeOne\n    assert S.NegativeOne**S(2) is S.One\n    assert S.NegativeOne**S(1) is S.NegativeOne\n    assert S.NegativeOne**S(0) is S.One\n    assert S.NegativeOne**S(-1) is S.NegativeOne\n    assert S.NegativeOne**S(-2) is S.One\n    assert S.NegativeOne**S(-3) is S.NegativeOne\n    assert S.NegativeOne**S(-4) is S.One\n    assert S.NegativeOne**S.NaN is S.NaN\n"], "sample_1177": ["def test_polar_lift_properties():\n    from sympy.core.numbers import pi\n    from sympy.core.singleton import S\n    from sympy.functions.elementary.complexes import polar_lift\n\n    x = Symbol('x')\n    assert polar_lift(x).is_polar is True\n    assert polar_lift(x).is_real is None\n    assert polar_lift(x).is_imaginary is None\n    assert polar_lift(x).is_zero is None\n    assert polar_lift(0).is_zero is True\n    assert polar_lift(1).is_zero is False\n    assert polar_lift(I).is_zero is False\n    assert polar_lift(pi).is_zero is False\n\n    assert polar_lift(x).as_real_imag() == (re(polar_lift(x)), im(polar_lift(x)))\n\n    assert polar_lift(2*x) == 2*polar_lift(x)\n    assert polar_lift(S(2)*x) == 2*polar_lift(x)\n    assert polar_lift(3.0*x) == 3.0*polar_lift(x)\n\n    assert polar_lift(x).diff(x) == polar_lift(1)\n", "def test_issue_19419():\n    x = Symbol('x', real=True)\n    z = Symbol('z', complex=True)\n\n    assert abs(conjugate(x)) == x\n    assert abs(conjugate(z)) == Abs(conjugate(z))\n    assert abs(conjugate(z)) == Abs(z)\n", "def test_issue_24367():\n    x = Symbol('x', real=True)\n    i = Symbol('i', imaginary=True)\n    assert unchanged(sign, x*i)\n    assert unchanged(sign, 2*i*x)\n    assert unchanged(sign, 3 + 4*i)\n", "def test_exp_polar():\n    from sympy.functions.elementary.complexes import exp_polar\n    x = Symbol('x')\n    assert exp_polar(0) == 1\n    assert exp_polar(1) == E\n    assert exp_polar(x).diff(x) == exp_polar(x)\n    assert exp_polar(2 + 3*I).evalf() == exp(2 + 3*I).evalf()\n    assert exp_polar(x).is_complex is None\n    assert exp_polar(x).is_real is None\n    assert exp_polar(2*pi*I) == 1\n    assert exp_polar(pi*I) == -1\n    assert exp_polar(pi/2*I) == I\n    assert exp_polar(-pi/2*I) == -I\n    assert exp_polar(pi/4*I) == sqrt(2)/2 + sqrt(2)/2*I\n    assert exp_polar(-pi/4*I) == sqrt(2)/2 - sqrt(2)/2*I\n", "def test_polar_lift():\n    from sympy.functions.elementary.complexes import polar_lift\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n    assert polar_lift(2 + I) == 2*exp_polar(I*atan2(1, 2))\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(p) == p\n    assert polar_lift(0) == 0\n    assert polar_lift(2) == 2*exp_polar(0)\n    assert polar_lift(-2) == 2*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n"], "sample_1178": ["def test_Element():\n    ei = Element(x, 'ijk')\n    assert ei.symbol == x\n    assert ei.indices == Tuple(i, j, k)\n    assert ei.strides == None\n    assert ei.offset == None\n    assert ei == Element(x, (i, j, k))\n    assert ei.func(*ei.args) == ei\n\n    e2 = Element(x, (i, j), (3, 4), 1)\n    assert e2.symbol == x\n    assert e2.indices == Tuple(i, j)\n    assert e2.strides == Tuple(3, 4)\n    assert e2.offset == 1\n    assert e2.func(*e2.args) == e2\n\n    assert ei != e2\n", "def test_Element():\n    elem = Element('x', 'ijk')\n    assert elem.symbol.name == 'x'\n    assert elem.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert elem.strides is None\n    assert elem.offset is None\n    assert elem == Element('x', (Symbol('i'), Symbol('j'), Symbol('k')))\n    assert elem != Element('x', (Symbol('j'), Symbol('k'), Symbol('i')))\n\n    elem_strided = Element('x', 'ijk', strides='lmn', offset='o')\n    assert elem_strided.symbol.name == 'x'\n    assert elem_strided.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert elem_strided.strides == (Symbol('l'), Symbol('m'), Symbol('n'))\n    assert elem_strided.offset == Symbol('o')\n    assert elem_strided == Element('x', (Symbol('i'), Symbol('j'), Symbol('k')), strides=(Symbol('l'), Symbol('m'), Symbol('n')), offset=Symbol('o'))\n", "def test_Element():\n    ei = Element('x', 'ijk')\n    assert ei.symbol == Symbol('x')\n    assert ei.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert ei.strides == none\n    assert ei.offset == none\n    assert ei.kwargs() == {'symbol': Symbol('x'), 'indices': (Symbol('i'), Symbol('j'), Symbol('k'))}\n\n    ej = Element('x', 'lmn', 'pqr')\n    assert ej.symbol == Symbol('x')\n    assert ej.indices == (Symbol('l'), Symbol('m'), Symbol('n'))\n    assert ej.strides == (Symbol('p'), Symbol('q'), Symbol('r'))\n    assert ej.offset == none\n    assert ej.kwargs() == {'symbol': Symbol('x'), 'indices': (Symbol('l'), Symbol('m'), Symbol('n')), \n                            'strides': (Symbol('p'), Symbol('q'), Symbol('r'))}\n\n    ek = Element('x', 'ijk', 'lmn', 1)\n    assert ek.symbol == Symbol('x')\n    assert ek.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert ek.strides == (Symbol('l'), Symbol('m'), Symbol('n'))\n    assert ek.offset == 1\n    assert ek.kwargs() == {'symbol': Symbol('x'), 'indices': (Symbol('i'), Symbol('j'), Symbol('k')), \n                            'strides': (Symbol('l'), Symbol('m'), Symbol('n')), 'offset': 1}\n", "def test_Element():\n    elem = Element('x', (i, j))\n    assert elem.symbol.name == 'x'\n    assert elem.indices == (i, j)\n    assert elem.args == (Symbol('x'), Tuple(i, j), none, none)\n    assert elem == Element('x', (i, j))\n    assert elem.func(*elem.args) == elem\n    assert elem != Element('x', (j, i))\n\n    elem1 = Element('x', (i, 1))\n    assert elem1.indices == (i, 1)\n    assert elem1.args == (Symbol('x'), Tuple(i, 1), none, none)\n    assert elem1 == Element('x', (i, 1))\n\n    elem2 = Element('x', 'ijk')\n    assert elem2.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert elem2.args == (Symbol('x'), Tuple('i', 'j', 'k'), none, none)\n    assert elem2 == Element('x', 'ijk')\n\n    assert elem.func(*elem.args) == elem\n    assert elem1.func(*elem1.args) == elem1\n    assert elem2.func(*elem2.args) == elem2\n", "def test_Element():\n    elem1 = Element('x', 'ijk')\n    assert elem1.symbol.name == 'x'\n    assert elem1.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert elem1.strides == None\n    assert elem1.offset == None\n\n    elem2 = Element('x', 'ijk', strides='lmn', offset='o')\n    assert elem2.symbol.name == 'x'\n    assert elem2.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert elem2.strides == (Symbol('l'), Symbol('m'), Symbol('n'))\n    assert elem2.offset == Symbol('o')\n"], "sample_1179": ["def test_str_elementwiseapplyfunc():\n    M = MatrixSymbol(\"M\", 2, 2)\n    from sympy.functions.elementary.trigonometric import sin\n    assert sstr(ElementwiseApplyFunction(sin, M)) == \"sin.(M)\"\n", "def test_Dimension():\n    from sympy.tensor.tensor import TensorIndexType\n    assert str(TensorIndexType('Lorentz', dim=4)) == 'Lorentz'\n", "def test_printing_str_dmp():\n    from sympy.polys.domains import ZZ\n    from sympy.polys.polytools import Poly\n\n    f = Poly(x**2 + 2*x + 1, x)\n    dmp = f.to_dict()\n    assert sstr(dmp) == '{0: 1, 1: 2, 2: 1}'\n", "def test_ElementwiseApplyFunction_printing():\n    from sympy.tensor.array.expressions import ElementwiseApplyFunction\n    arr = ArraySymbol(\"X\", (3, 3))\n    f = Lambda(x, x**2)\n    expr = ElementwiseApplyFunction(f, arr)\n    assert str(expr) == \"Lambda(x, x**2).(X)\"\n", "def test_Dimension():\n    from sympy.physics.units.quantity import Quantity\n    assert sstr(Quantity(\"length\").get_dimension()) == \"length\"\n"], "sample_1181": ["def test_scipy_print():\n    prntr = SciPyPrinter()\n    M = MatrixSymbol(\"M\", 2, 2)\n    x = MatrixSymbol(\"x\", 2, 1)\n    assert prntr.doprint(M**(-1) * x + x) == \\\n        'scipy.linalg.solve(M, x) + x'\n    assert prntr.doprint(MatrixSolve(M, x) + x) == \\\n        'scipy.linalg.solve(M, x) + x'\n    assert prntr.doprint(sqrt(x)) == 'scipy.sqrt(x)'\n    assert prntr.doprint(expm1(x)) == 'scipy.special.expm1(x)'\n    assert prntr.doprint(log1p(x)) == 'scipy.special.log1p(x)'\n    assert prntr.doprint(hypot(x, x)) == 'scipy.hypot(x, x)'\n    assert prntr.doprint(log10(x)) == 'scipy.log10(x)'\n    assert prntr.doprint(exp2(x)) == 'scipy.exp2(x)'\n    assert prntr.doprint(log2(x)) == 'scipy.log2(x)'\n", "def test_scipy_functions():\n    if not import_module('scipy'):\n        skip(\"SciPy not installed\")\n\n    from sympy import Symbol\n    x = Symbol('x')\n\n    expr = x * 2\n    f = lambdify(x, expr, 'scipy')\n    assert f(1.0) == 2.0\n\n    expr = x + 1\n    f = lambdify(x, expr, 'scipy')\n    assert f(1.0) == 2.0\n", "def test_scipy_lambertw():\n    if not import_module('scipy'):\n        skip(\"SciPy not installed\")\n    expr = LambertW(a)\n    f = lambdify((a,), expr, 'scipy')\n\n    assert abs(f(1) - 0.56714329) < 1e-5\n", "def test_scipy_matrix_solve():\n    if not import_module('scipy'):\n        skip(\"Scipy not installed\")\n\n    M = MatrixSymbol(\"M\", 3, 3)\n    x = MatrixSymbol(\"x\", 3, 1)\n\n    expr = MatrixSolve(M, x)\n    f = lambdify((M, x), expr, 'scipy')\n\n    m0 = np.array([[1, 2, 3], [3, 2, 5], [5, 6, 7]])\n    assert np.linalg.matrix_rank(m0) == 3\n\n    x0 = np.array([3, 4, 5])\n\n    assert np.allclose(f(m0, x0), np.linalg.solve(m0, x0))\n", "def test_cupy_printer():\n    if not import_module('cupy'):\n        skip(\"CuPy not installed\")\n\n    M = MatrixSymbol(\"M\", 2, 2)\n    N = MatrixSymbol(\"N\", 2, 2)\n    P = MatrixSymbol(\"P\", 2, 2)\n    Q = MatrixSymbol(\"Q\", 2, 2)\n\n    prntr = CuPyPrinter()\n    assert prntr.doprint(M*N) == 'cupy.dot(M, N)'\n    assert prntr.doprint(M**-1) == 'cupy.linalg.inv(M)'\n    assert prntr.doprint(P + Q) == 'cupy.add(P, Q)'\n    assert prntr.doprint(P - Q) == 'cupy.subtract(P, Q)'\n"], "sample_1180": ["def test_affine_rank():\n    p1 = Point(0, 0)\n    p2 = Point(1, 0)\n    p3 = Point(1, 1)\n    p4 = Point(2, 2)\n    assert Point.affine_rank() == -1\n    assert Point.affine_rank(p1) == 0\n    assert Point.affine_rank(p1, p2) == 1\n    assert Point.affine_rank(p1, p2, p3) == 2\n    assert Point.affine_rank(p1, p2, p3, p4) == 2\n    assert Point.affine_rank(p1, p1, p1) == 0\n    assert Point.affine_rank(p1, p2, p2) == 1\n    assert Point.affine_rank(p1, p2, p3, p3) == 2\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, 0]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, 0]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, 0]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, 0]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n"], "sample_1182": ["def test_printing_matrix_nodes():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    prntr = PythonCodePrinter({'allow_unknown_functions': True})\n    assert prntr.doprint(A) == 'A'\n    assert prntr.doprint(A**(-1)) == 'A**(-1)'\n    assert prntr.doprint(A**5) == 'A**5'\n    assert prntr.doprint(A + B) == 'A + B'\n    assert prntr.doprint(A * B) == 'A * B'\n    assert prntr.doprint(A.T) == 'A.T'\n\n    from sympy.codegen.ast import MatrixAdd, MatrixMul\n    from sympy.codegen.cfunctions import outerproduct\n    assert prntr.doprint(MatrixAdd(A, B)) == 'A + B'\n    assert prntr.doprint(MatrixMul(A, B)) == 'A * B'\n    assert prntr.doprint(outerproduct(A[0, :], A[:, 0])) == 'A[0, :].T * A[:, 0]'\n", "def test_PythonCodePrinter_reserved_words_suffix():\n    s1, s2 = symbols('if else')\n    py_str = pycode(s1 + s2, reserved_word_suffix='_custom')\n    assert py_str in ('else_custom + if_custom', 'if_custom + else_custom')\n", "def test_PythonCodePrinter_printmethod():\n    prntr = PythonCodePrinter()\n\n    expr = CustomPrintedObject()\n    assert prntr.doprint(expr) == 'sympy.printing.pycode.CustomPrintedObject()'\n\n    class CustomPrintMethod(CustomPrintedObject):\n            return 'CustomPrintMethod'\n\n    expr = CustomPrintMethod()\n    assert prntr.doprint(expr) == 'CustomPrintMethod'\n", "def test_printmethod_not_supported():\n    class CustomPrintedObject(Expr):\n            return 'Custom'\n\n    obj = CustomPrintedObject()\n    assert SymPyPrinter().doprint(obj) == 'Custom'\n    assert NumPyPrinter().doprint(obj) == 'Not supported in numpy'\n    assert MpmathPrinter().doprint(obj) == 'Not supported in mpmath'\n", "def test_PythonCodePrinter_Pow():\n    prntr = PythonCodePrinter()\n\n    assert prntr._hprint_Pow(x**2, rational=False) == 'x**2'\n    assert prntr._hprint_Pow(x**2, rational=True) == 'x**2'\n\n    assert prntr._hprint_Pow(x**0.5, rational=False) == 'math.sqrt(x)'\n    assert prntr._hprint_Pow(x**0.5, rational=True) == 'x**(1/2)'\n\n    assert prntr._hprint_Pow(1/x**0.5, rational=False) == '1/math.sqrt(x)'\n    assert prntr._hprint_Pow(1/x**0.5, rational=True) == 'x**(-1/2)'\n"], "sample_1184": ["def test_beam_parameter_properties():\n    z, l, w_0, n = symbols('z l w_0 n', positive=True)\n    p = BeamParameter(l, z, w=w_0, n=n)\n    assert p.n == n\n    assert p.wavelen == l\n    assert p.z == z\n    assert p.z_r == waist2rayleigh(w_0, l, n)\n    assert p.q == z + I*p.z_r\n", "def test_beam_parameter_equality():\n    wavelen = 530e-9\n    z = 1\n    w = 1e-3\n    n = 2\n    \n    p1 = BeamParameter(wavelen, z, w=w, n=n)\n    p2 = BeamParameter(wavelen, z, w=w, n=n)\n    p3 = BeamParameter(wavelen, z, w=w)\n    \n    assert p1 == p2\n    assert p1 != p3\n    assert p2 != p3\n", "def test_matrix_exception():\n    try:\n        RayTransferMatrix(1, 2, 3)\n        assert False, \"Expected ValueError for invalid input\"\n    except ValueError as e:\n        assert str(e) == \"Expecting 2x2 Matrix or the 4 elements of the Matrix but got (1, 2, 3)\"\n\n    try:\n        RayTransferMatrix( Matrix( [[1, 2], [3, 4], [5, 6]] ) )\n        assert False, \"Expected ValueError for invalid input\"\n    except ValueError as e:\n        assert str(e) == \"Expecting 2x2 Matrix or the 4 elements of the Matrix but got Matrix([[1, 2], [3, 4], [5, 6]])\"\n\n    try:\n        RayTransferMatrix( Matrix( [[1, 2, 3], [4, 5, 6]] ) )\n        assert False, \"Expected ValueError for invalid input\"\n    except ValueError as e:\n        assert str(e) == \"Expecting 2x2 Matrix or the 4 elements of the Matrix but got Matrix([[1, 2, 3], [4, 5, 6]])\"\n\n    try:\n        GeometricRay(1)\n        assert False, \"Expected ValueError for invalid input\"\n    except ValueError as e:\n        assert str(e) == \"Expecting 2x1 Matrix or the 2 elements of the Matrix but got (1,)\"\n", "def test_curved_mirror():\n    R = symbols('R')\n    curved_mirror = CurvedMirror(R)\n    assert curved_mirror == Matrix([[1, 0], [-2/R, 1]])\n\n    beam_parameter = BeamParameter(530e-9, 1, w=1e-3)\n    new_beam_parameter = curved_mirror * beam_parameter\n    assert new_beam_parameter.wavelen == beam_parameter.wavelen\n    assert new_beam_parameter.n == beam_parameter.n\n", "def test_conjugate_gauss_beams_error_handling():\n    l, w_i, w_o, f = symbols('l w_i w_o f')\n    try:\n        conjugate_gauss_beams(l, w_i, w_o)\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        assert True\n\n    try:\n        conjugate_gauss_beams(l, w_i, w_o, f=f, dist=1)\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        assert True\n\n    try:\n        conjugate_gauss_beams(l, w_i, w_o, s_in=1, f=f)\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        assert True\n"], "sample_1185": ["def test_compogen_poly():\n    assert compogen([x**2 - x - 1, x**2 + x], x) == (x**2 + x)**2 - (x**2 + x) - 1\n    assert compogen([x**2 + 2*x + 1, x**2], x) == x**4 + 2*x**2 + 1\n    assert compogen([x**2 + 1, x**2 - 1], x) == x**4 - 1\n", "def test_compogen_fails():\n    A = lambda x: x**2 + 2*x + 3\n    B = lambda x: 4*x**2 + 5*x + 6\n    assert compogen([x**2 + 2*x + 3, x*exp(x)], x) == A(x*exp(x))\n    assert compogen([x**2 + 2*x + 3, 4*x**2 + 5*x + 6], x) == A(B(x))\n    assert compogen([x**2 + 2*x + 3, 1/x + 1/x**2], x) == A(1/x + 1/x**2)\n    assert compogen([x**2 + 2*x + 3, 1/x + 2/(x + 1)], x) == A(1/x + 2/(x + 1))\n", "def test_decompogen_trigonometric():\n    assert decompogen(sin(x)**3, x) == [x**3, sin(x)]\n    assert decompogen(cos(x)**4, x) == [x**4, cos(x)]\n    assert decompogen(sin(x)**2 * cos(x)**2, x) == [x**2 * x**2, sin(x), cos(x)]\n    assert decompogen(sin(x)**2 * cos(x)**3, x) == [x**2 * x**3, sin(x), cos(x)]\n    assert decompogen(sin(x)**3 * cos(x)**2, x) == [x**3 * x**2, sin(x), cos(x)]\n", "def test_compogen_trigonometric():\n    assert compogen([sin(x), cos(x), x**2 + 1], x) == sin(cos(x**2 + 1))\n    assert compogen([cos(x), sin(x), x**2 + 1], x) == cos(sin(x**2 + 1))\n    assert compogen([sin(x), sin(x), x**2 + 1], x) == sin(sin(x**2 + 1))\n    assert compogen([cos(x), cos(x), x**2 + 1], x) == cos(cos(x**2 + 1))\n", "def test_decompogen_constant():\n    assert decompogen(5, x) == [5]\n    assert decompogen(0, x) == [0]\n    assert decompogen(S.Half, x) == [S.Half]\n    assert decompogen(exp(1), x) == [exp(1)]\n"], "sample_1183": ["def test_FracField_from_expr():\n    K, f = field(\"x\", ZZ)\n    assert K.from_expr(2*x) == 2*K.x\n    assert K.from_expr(x**2/3) == K.x**2 / 3\n    assert K.from_expr(2) == 2\n    assert K.from_expr(0) == 0\n    assert K.from_expr(1) == 1\n    assert K.from_expr(-1) == -1\n    assert K.from_expr(1/x) == 1/K.x\n    assert K.from_expr(x/1) == K.x\n    assert K.from_expr((x+1)/(x-1)) == (K.x + 1) / (K.x - 1)\n", "def test_FracField___getitem__():\n    K = ZZ.frac_field(x)\n    assert K[x] == K.inject(x)\n    assert K[x, y] == K.inject(x, y)\n", "def test_FracField_to_domain():\n    F = ZZ.frac_field(x, y)\n    assert F.to_domain() == F\n    assert isinstance(F.to_domain(), FractionField)\n\n    F = QQ.frac_field(x, y)\n    assert F.to_domain() == F\n    assert isinstance(F.to_domain(), FractionField)\n\n    F = ZZ_I.frac_field(x, y)\n    assert F.to_domain() == F\n    assert isinstance(F.to_domain(), FractionField)\n\n    F = QQ_I.frac_field(x, y)\n    assert F.to_domain() == F\n    assert isinstance(F.to_domain(), FractionField)\n", "def test_FracField_to_domain():\n    F, x, y = field(\"x,y\", ZZ)\n    assert F.to_domain() == FractionField(F)\n\n    F, x, y = field(\"x,y\", QQ)\n    assert F.to_domain() == FractionField(F)\n\n    F, x, y = field(\"x,y\", ZZ_I)\n    assert F.to_domain() == FractionField(F)\n\n    F, x, y = field(\"x,y\", QQ_I)\n    assert F.to_domain() == FractionField(F)\n", "def test_FracField():\n    F, x, y = field('x y', ZZ)\n    assert F == ZZ.frac_field(x, y)\n    assert F == ZZ[x, y].to_field()\n    assert F == ZZ.frac_field(x)[y]\n    assert F == ZZ[x].frac_field(y)\n    assert F == ZZ.frac_field(x, y)\n    assert F == F\n\n    assert F != ZZ\n    assert F != ZZ[x, y]\n    assert F != ZZ.frac_field(x)\n    assert F != QQ.frac_field(x, y)\n\n    assert hash(F) == hash(ZZ.frac_field(x, y))\n\n    assert F.ring == ZZ[x, y]\n\n    assert F.domain == ZZ\n    assert F.domain != QQ\n\n    assert F.symbols == (x, y)\n    assert F.ngens == 2\n\n    assert F.gens == (F(x), F(y))\n\n    assert F.zero == F(0)\n    assert F.one == F(1)\n\n    assert F(x) == F(x)\n    assert F(x) != F(y)\n\n    assert F(x) + F(x) == F(2*x)\n    assert F(x) + F(y) == F(x + y)\n    assert F(x) + 1 == F(x + 1)\n\n    assert -F(x) == F(-x)\n    assert F(x) - F(x) == F(0)\n    assert F(x) - F(y) == F(x - y)\n    assert F(x) - 1 == F(x - 1)\n\n    assert F(x)*F(x) == F(x**2)\n    assert F(x)*F(y) == F(x*y)\n    assert F(x)*2 == F(2*x)\n\n    assert F(x)/F(x) == F(1)\n    assert F(x)/F(y) == F(x/y)\n    assert F(x)/2 == F(x/2)\n"], "sample_1186": ["def test_NDimArray_iter():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2, 3], [4, 5, 6]])\n        result = []\n        for i in test_array:\n            result.append(i)\n        assert result == [ArrayType([1, 2, 3]), ArrayType([4, 5, 6])]\n        test_array = ArrayType([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        result = []\n        for i in test_array:\n            result.append(i)\n        assert result == [ArrayType([[1, 2], [3, 4]]), ArrayType([[5, 6], [7, 8]])]\n        test_array = ArrayType([])\n        result = []\n        for i in test_array:\n            result.append(i)\n        assert result == []\n", "def test_array_equal():\n    for ArrayType in array_types:\n        array1 = ArrayType([1, 2, 3])\n        array2 = ArrayType([1, 2, 3])\n        assert array1 == array2\n\n        array3 = ArrayType([1, 2, 4])\n        assert array1 != array3\n\n        array4 = ArrayType([1, 2, 3, 4])\n        assert array1 != array4\n\n        array5 = ArrayType([[1, 2], [3, 4]])\n        array6 = ArrayType([[1, 2], [3, 4]])\n        assert array5 == array6\n\n        array7 = ArrayType([[1, 2], [3, 5]])\n        assert array5 != array7\n\n        array8 = ArrayType([[[1, 2], [3, 4]]])\n        array9 = ArrayType([[[1, 2], [3, 4]]])\n        assert array8 == array9\n\n        array10 = ArrayType([[[1, 2], [3, 5]]])\n        assert array8 != array10\n", "def test_array_rank():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n        assert test_array.rank() == 2\n        test_array = ArrayType([1, 2, 3, 4, 5])\n        assert test_array.rank() == 1\n        test_array = ArrayType([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n        assert test_array.rank() == 3\n        test_array = ArrayType([])\n        assert test_array.rank() == 1\n        test_array = ArrayType([[]])\n        assert test_array.rank() == 2\n", "def test_NDimArray_diff():\n    A = Array([[x**2, x*y], [y**2, x**2*y]])\n    B = A.diff(x)\n    assert B == Array([[2*x, y], [0, 2*x*y]])\n    C = B.diff(y)\n    assert C == Array([[0, 1], [0, 2*x]])\n", "def test_array_iter():\n    for ArrayType in array_types:\n        test_array = ArrayType([1, 2, 3, 4, 5, 6], (2, 3))\n        expected = [1, 2, 3, 4, 5, 6]\n        result = [i for i in test_array]\n        assert result == expected\n\n        test_array = ArrayType([[1, 2, 3], [4, 5, 6]])\n        expected = [[1, 2, 3], [4, 5, 6]]\n        result = [i for i in test_array]\n        assert result == expected\n\n        test_array = ArrayType([1])\n        expected = [1]\n        result = [i for i in test_array]\n        assert result == expected\n\n        test_array = ArrayType([])\n        expected = []\n        result = [i for i in test_array]\n        assert result == expected\n"], "sample_1188": ["def test_pretty_printing_Mul():\n    # test pretty printing of Mul\n    from sympy.abc import x, y\n    from sympy import Rational\n    assert pretty(x*y) == 'x\u22c5y'\n    assert pretty(x*y, use_unicode=False) == 'x*y'\n    assert pretty(x*Rational(3, 2), use_unicode=False) == '3*x/2'\n    assert pretty(x*Rational(3, 2), use_unicode=True) == '3\u22c5x/2'\n", "def test_pretty_printing_sequence():\n    from sympy import SeqMul\n    from sympy.abc import ak, n\n    seq = SeqMul(ak, (n, 0, 5))\n    assert upretty(seq) == \"\u220f(a_k, (k, 0, 5))\"\n", "def test_pretty_printing_mod():\n    from sympy import symbols, Mod\n\n    a, b = symbols('a b')\n    assert upretty(Mod(a, b)) == 'a mod b'\n    assert upretty(Mod(a, b, evaluate=False)) == 'a mod b'\n    assert upretty(Mod(a, 10, evaluate=False)) == 'a mod 10'\n", "def test_pretty_printing_AssignmentBase():\n    from sympy import symbols\n    a = symbols('a')\n    x = symbols('x')\n    assign_add = a + a\n    assign_sub = a - a\n    assert pretty(assign_add) == '2*a'\n    assert pretty(assign_sub) == '0'\n    assign_pow = a**a\n    assert pretty(assign_pow) == 'a**a'\n    assign_mul = a * a\n    assert pretty(assign_mul) == 'a**2'\n", "def test_pretty_printing_trigonometric():\n    from sympy import sin, cos, tan, cot, sec, csc\n    from sympy import symbols\n\n    x = symbols('x')\n    assert upretty(sin(x)) == 'sin(x)'\n    assert upretty(cos(x)) == 'cos(x)'\n    assert upretty(tan(x)) == 'tan(x)'\n    assert upretty(cot(x)) == 'cot(x)'\n    assert upretty(sec(x)) == 'sec(x)'\n    assert upretty(csc(x)) == 'csc(x)'\n"], "sample_1189": ["def test_lambdify_array_derivative():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    A = Array([x, x**2, x**3])\n    f = lambdify(x, derive_by_array(A, x), 'numpy')\n    assert numpy.array_equal(f(1), numpy.array([1, 2, 3]))\n", "def test_lambdify_array():\n    if not numpy:\n        skip(\"numpy not installed\")\n    M = Matrix([[x, x*y], [sin(z) + 4, x**z]])\n    f = lambdify((x, y, z), M, modules=\"numpy\")\n    assert numpy.allclose(f(1, 2, 3), numpy.array([[1, 2], [sin(3) + 4, 1]]))\n", "def test_lambdify_mpmath_multiple_arg():\n    if not mpmath:\n        skip(\"mpmath not installed\")\n\n    f = lambdify((x, y), x*y, 'mpmath')\n    result = f(2, 3)\n    assert isinstance(result, mpmath.mpf)\n    assert result == 6\n", "def test_non_builtins():\n    from sympy import Function\n    f = Function(\"f\")\n    func = lambdify(x, f(x))\n    raises(NameError, lambda: func(1))\n", "def test_issue_17174():\n    if not numpy:\n        skip(\"numpy not installed\")\n    f = lambdify(x, lowergamma(x, 2), 'scipy')\n    assert f(3) == 2\n    f = lambdify(x, uppergamma(x, 2), 'scipy')\n    assert f(3) == 0\n"], "sample_1187": ["def test_issue_gradient_terms():\n    assert gradient_terms(binomial_power=3, no_of_gens=2) == \\\n        [[0, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [y**3, 0, 3, 0],\n         [x, 1, 0, 0], [x*y, 1, 1, 0], [x*y**2, 1, 2, 0], [x*y**3, 1, 3, 0],\n         [x**2, 2, 0, 0], [x**2*y, 2, 1, 0], [x**2*y**2, 2, 2, 0],\n         [x**2*y**3, 2, 3, 0], [x**3, 3, 0, 0], [x**3*y, 3, 1, 0],\n         [x**3*y**2, 3, 2, 0], [x**3*y**3, 3, 3, 0]]\n\n    assert gradient_terms(binomial_power=3) == \\\n        [[0, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [y**3, 0, 3, 0],\n         [x, 1, 0, 0], [x*y, 1, 1, 0], [x*y**2, 1, 2, 0], [x*y**3, 1, 3, 0],\n         [x**2, 2, 0, 0], [x**2*y, 2, 1, 0], [x**2*y**2, 2, 2, 0],\n         [x**2*y**3, 2, 3, 0], [x**3, 3, 0, 0], [x**3*y, 3, 1, 0],\n         [x**3*y**2, 3, 2, 0], [x**3*y**3, 3, 3", "def test_hyperplane_parameters():\n    poly = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    hp_params = hyperplane_parameters(poly)\n    assert hp_params == [((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)]\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n                (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n                [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n                [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    hp_params = hyperplane_parameters(faces, vertices)\n    assert hp_params == [([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5),\n                         ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)]\n", "def test_hyperplane_parameters():\n    # Test with 2D polygon\n    polygon = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    expected = [((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)]\n    assert hyperplane_parameters(polygon) == expected\n\n    # Test with 3D polytope\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0), (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0], [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    expected = [([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5), ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)]\n    assert hyperplane_parameters(faces, vertices) == expected\n", "def test_left_integral2D():\n    m = 3\n    index = 0\n    facets = Polygon(Point(0, 3), Point(5, 3), Point(1, 1)).sides\n    x0 = facets[0].points[0]\n    expr = 1\n    gens = (x, y)\n    assert left_integral2D(m, index, facets, x0, expr, gens) == 5\n", "def test_hyperplane_parameters():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    assert hyperplane_parameters(triangle) == [\n        ((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)\n    ]\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    assert hyperplane_parameters(faces, vertices) == [\n        ([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5),\n        ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)\n    ]\n"], "sample_1190": ["def test_unit_system_derived_units():\n    us = UnitSystem(base_units=(meter, second), name=\"my_unit_system\")\n    assert not us.derived_units\n\n    us = UnitSystem(base_units=(meter, second), derived_units={length: meter}, name=\"my_unit_system\")\n    assert length in us.derived_units\n    assert us.derived_units[length] == meter\n", "def test_get_units_non_prefixed():\n    assert set(SI.get_units_non_prefixed()) == {\n        meter, second, joule, newton, pascal, watt,\n        ampere, kelvin, mole, candela, volt, ohm,\n        siemens, weber, tesla, henry, coulomb, farad,\n        gram, radian, steradian, hertz, lux, katal,\n        becquerel, sievert, gray\n    }\n", "def test_unit_system():\n    us = UnitSystem(base_units=(meter, second), name=\"Test System\")\n    assert us.name == \"Test System\"\n    assert us.dim == 2\n    assert us.get_dimension_system() is not None\n    assert us.is_consistent is True\n    assert us.get_units_non_prefixed() == {meter, second}\n    assert us.extend(base=(kilogram,), name=\"Extended System\").name == \"Extended System\"\n    assert us.get_unit_system(\"Test System\") is us\n", "def test_unit_system_extend():\n    us = UnitSystem(base_units=(meter, second), name=\"SI_base\")\n    extended_us = us.extend(base=(kilogram,), name=\"SI_extended\")\n    assert extended_us.name == \"SI_extended\"\n    assert len(extended_us._base_units) == 3\n    assert extended_us._base_units[0] == meter\n    assert extended_us._base_units[1] == second\n    assert extended_us._base_units[2] == kilogram\n    assert extended_us._dimension_system is None\n", "def test_get_units_non_prefixed():\n    assert set(SI.get_units_non_prefixed()) == set([meter, second, joule, kilogram, day, bit, byte, coulomb, volt, ohm, elementary_charge])\n    assert meter in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert mebibyte not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n"], "sample_1192": ["def test_symbols_with_properties():\n    x, y, z = symbols('x,y,z', real=True, positive=True)\n    assert all(s.is_real for s in (x, y, z))\n    assert all(s.is_positive for s in (x, y, z))\n    assert symbols('x,y,z', real=True).xreplace({x: y}) == (y, y, z)\n    assert symbols('x,y,z', real=True, cls=Dummy) != symbols('x,y,z', cls=Dummy)\n", "def test_symbols_with_properties():\n    x, y, z = symbols('x,y,z', real=True)\n    assert all(i.is_real for i in (x, y, z))\n\n    a, b, c = symbols('a,b,c', integer=True, real=False)\n    assert all(i.is_integer for i in (a, b, c))\n    assert not any(i.is_real for i in (a, b, c))\n\n    p, q, r = symbols('p,q,r', real=True, integer=True)\n    assert all(i.is_real for i in (p, q, r))\n    assert all(i.is_integer for i in (p, q, r))\n\n    j, k = symbols('j,k', integer=True, real=True, commutative=False)\n    assert all(i.is_integer for i in (j, k))\n    assert all(i.is_real for i in (j, k))\n    assert not any(i.is_commutative for i in (j, k))\n\n    u, v = symbols('u,v', commutative=False)\n    assert not any(i.is_commutative for i in (u, v))\n\n    # Test passing properties as a dictionary\n    props = {'integer': True, 'real': False}\n    d, e, f = symbols('d,e,f', **props)\n    assert all(i.is_integer for i in (d, e, f))\n    assert not any(i.is_real for i in (d, e, f))\n\n    props = {'real': True, 'integer': True, 'commutative': False}\n    g, h, i = symbols('g,h,i', **props)\n    assert all(i.is_real for i in (g, h, i))\n    assert all(i.is_integer for i in (g, h, i))\n    assert not any(i.is_commutative for i in (g, h, i))\n", "def test__filter_assumptions():\n    assumptions = {'real': True, 'positive': False, 'other': None}\n    expected_assumptions = {'real': True, 'positive': False}\n    expected_nonassumptions = {'other': None}\n\n    assumptions_result, nonassumptions_result = _filter_assumptions(assumptions)\n\n    assert assumptions_result == expected_assumptions\n    assert nonassumptions_result == expected_nonassumptions\n", "def test__symbol():\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    z = Symbol('x', real=True)\n    assert _symbol('x', x) == x\n    assert _symbol('x', y, real=True) == z\n    assert _symbol('y', y) == y\n    assert _symbol('y', y, real=False) != y\n    assert _symbol('y', x, real=True) == Symbol('y', real=True)\n    assert _symbol('y', z, real=False) == Symbol('y', real=False)\n", "def test_filter_assumptions():\n    assumptions = {'commutative': True, 'real': False, 'complex': True}\n    assumptions, nonassumptions = _filter_assumptions(assumptions)\n    assert assumptions == {'commutative': True, 'real': False, 'complex': True}\n    assert nonassumptions == {}\n\n    assumptions = {'commutative': True, 'foo': 'bar'}\n    assumptions, nonassumptions = _filter_assumptions(assumptions)\n    assert assumptions == {'commutative': True}\n    assert nonassumptions == {'foo': 'bar'}\n\n    assumptions = {'commutative': None, 'real': False, 'complex': True}\n    raises(ValueError, lambda: _filter_assumptions(assumptions))\n"], "sample_1191": ["def test_smith_normal_form_zero_matrix():\n    zr = DomainMatrix([[0]*3 for _ in range(3)], (3, 3), ZZ)\n    assert smith_normal_form(zr).to_dense() == zr\n    zc = DomainMatrix([[0]*2 for _ in range(3)], (3, 2), ZZ)\n    assert smith_normal_form(zc).to_dense() == zc\n    zr = DomainMatrix([[0]*3 for _ in range(2)], (2, 3), ZZ)\n    assert smith_normal_form(zr).to_dense() == zr\n", "def test_hermite_normal_form_modulo_D():\n    # Test that HNF mod D gives the same result as the ordinary HNF algo\n    m = DM([[2, 7, 17, 29, 41], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    hnf_mod_d = _hermite_normal_form_modulo_D(m, ZZ(2))\n    assert hermite_normal_form(m) == hnf_mod_d\n\n    m = DM([[8, 28, 68, 116, 164], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    hnf_mod_d = _hermite_normal_form_modulo_D(m, ZZ(8))\n    assert hermite_normal_form(m) == hnf_mod_d\n\n    # Test that supplying a D that is not a multiple of det(W) raises no error\n    m = DM([[2, 7, 17, 29, 41], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    _hermite_normal_form_modulo_D(m, ZZ(13))\n\n    # Test that check_rank=False does not raise an error when matrix has rank < m\n    m = DM([[2, 7], [0, 0], [0, 0]], ZZ)\n    hermite_normal_form(m, D=ZZ(2), check_rank=False)\n", "def test_hermite_normal_form_modulo_D():\n    m = DM([[12, 6, 4, 8], [3, 9, 6, 12], [2, 16, 14, 28]], ZZ)\n    hnf = DM([[2, 0, 0], [0, 6, 0], [0, 0, 30]], ZZ)\n    D = ZZ(360)\n    assert _hermite_normal_form_modulo_D(m, D) == hnf\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(m.transpose(), D))\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(m, QQ(1)))\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(m, ZZ(-1)))\n", "def test_hermite_normal_with_zero_rows():\n    m = DM([[1, 2, 3], [0, 0, 0], [0, 0, 0]], ZZ)\n    hnf = DM([[1, 0, 0]], ZZ)\n    assert hermite_normal_form(m) == hnf\n\n    m = DM([[0, 0, 0], [0, 0, 0], [1, 2, 3]], ZZ)\n    hnf = DM([[1, 0, 0]], ZZ)\n    assert hermite_normal_form(m) == hnf\n\n    m = DM([[0, 0, 0], [0, 0, 0], [0, 0, 0]], ZZ)\n    hnf = DM([], (0, 3), ZZ)\n    assert hermite_normal_form(m) == hnf\n", "def test_hermite_normal_form_modulo_D():\n    # Test that passing a non-ZZ value for D raises an error.\n    m = DM([[2, 7, 17, 29, 41], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    raises(DMDomainError, lambda: hermite_normal_form(m, D=QQ(96)))\n\n    # Test that passing a non-positive value for D raises an error.\n    raises(DMDomainError, lambda: hermite_normal_form(m, D=ZZ(-1)))\n    raises(DMDomainError, lambda: hermite_normal_form(m, D=ZZ(0)))\n\n    # Test that passing a value for D that is not a multiple of the HNF\n    # determinant still returns the correct result.\n    m = DM([[8, 28, 68, 116, 164], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    hnf = DM([[4, 0, 0], [0, 2, 1], [0, 0, 1]], ZZ)\n    assert hermite_normal_form(m, D=ZZ(10)) == hnf\n\n    # Test that check_rank=True works as expected.\n    m = DM([[1, 1], [1, 1]], ZZ)\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(m, ZZ(2)))\n    assert hermite_normal_form(m, D=ZZ(2), check_rank=True) == DM([[1, 0], [0, 0]], ZZ)\n"], "sample_1193": ["def test_are_similar():\n    from sympy import Point, Circle, Triangle\n    c1, c2 = Circle(Point(0, 0), 4), Circle(Point(1, 4), 3)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n\n    assert are_similar(t1, t2) == True\n    assert are_similar(t1, t3) == False\n    raises(GeometryError, lambda: are_similar(c1, t1))\n", "def test_are_coplanar_with_points_and_plane():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    p3 = Point3D(7, 8, 9)\n    plane = Plane(p1, p2, p3)\n    assert are_coplanar(p1, p2, plane) == True\n    assert are_coplanar(p1, p2, p3, plane) == True\n    assert are_coplanar(p1, p2, Point3D(10, 11, 12)) == False\n", "def test_are_similar():\n    from sympy import Point, Circle, Triangle\n    c1 = Circle(Point(0, 0), 4)\n    c2 = Circle(Point(1, 4), 3)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    \n    assert are_similar(c1, c2)  # All circles are similar\n    assert are_similar(t1, t2)  # t1 and t2 are similar triangles\n    assert not are_similar(t1, t3)  # t1 and t3 are not similar triangles\n    raises(GeometryError, lambda: are_similar(c1, t1))  # Different geometrical entities\n", "def test_ordered_points():\n    p1 = Point2D(1, 2)\n    p2 = Point2D(2, 1)\n    assert _ordered_points([p1, p2]) == (p1, p2)\n    assert _ordered_points([p2, p1]) == (p1, p2)\n    assert _ordered_points([p1, p1]) == (p1, p1)\n    p3 = Point2D(0, 0)\n    p4 = Point2D(1, 0)\n    assert _ordered_points([p3, p4]) == (p3, p4)\n    assert _ordered_points([p4, p3]) == (p3, p4)\n    assert _ordered_points([p3, p3]) == (p3, p3)\n", "def test_are_similar():\n    c1, c2 = Circle(Point(0, 0), 4), Circle(Point(1, 4), 3)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    raises(GeometryError, lambda: are_similar(c1, t1))\n    assert are_similar(c1, c2) == False\n    assert are_similar(t1, t2) == True\n    assert are_similar(t1, t3) == False\n    assert are_similar(t1, t1) == True\n"], "sample_1194": ["def test_julia_inline_function():\n    f = implemented_function('f', Lambda(x, x**2))\n    assert julia_code(f(x)) == \"x .^ 2\"\n    g = implemented_function('g', Lambda(x, x**2 + 2*x))\n    assert julia_code(g(x)) == \"x .^ 2 + 2 * x\"\n", "def test_MatrixSlice():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert julia_code(A[:2, 1:]) == \"A[1:2,2:3]\"\n    assert julia_code(A[:, 1:]) == \"A[:,2:3]\"\n    assert julia_code(A[:2, :]) == \"A[1:2,:]\"\n    assert julia_code(A[1:, 1:]) == \"A[2:3,2:3]\"\n    assert julia_code(A[1:, :]) == \"A[2:3,:]\"\n    assert julia_code(A[:, 1:]) == \"A[:,2:3]\"\n    assert julia_code(A[:, :2]) == \"A[:,1:2]\"\n", "def test_SparseRepMatrix():\n    from sympy.matrices import SparseMatrix\n    M = SparseMatrix(5, 6, {(2, 2): 10, (1, 2): 20, (1, 3): 22, (0, 3): 30, (3, 0): x*y})\n    expected = (\"sparse([4, 2, 3, 1, 2], [1, 3, 3, 4, 4], [x .* y, 20, 10, 30, 22], 5, 6)\")\n    assert julia_code(M) == expected\n", "def test_custom_functions_for_julia_code():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    custom_functions = {\n        \"ceiling\": \"ceil\",\n        \"Abs\": [(lambda x: not x.is_integer, \"myabs\"), (lambda x: x.is_integer, \"abs\")],\n    }\n\n    assert julia_code(ceiling(x), user_functions=custom_functions) == \"ceil(x)\"\n    assert julia_code(Abs(x), user_functions=custom_functions) == \"myabs(x)\"\n    assert julia_code(Abs(n), user_functions=custom_functions) == \"abs(n)\"\n", "def test_julia_pow_default_one_exp():\n    # Testing that when the exponent is 1, no code is generated for it\n    assert julia_code(x**1) == \"x\"\n    assert julia_code(2**1) == \"2\"\n"], "sample_1196": ["def test_contains_eval_method():\n    # Test the eval method of Contains class\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Contains.eval(2, S.Integers) is S.true\n    assert Contains.eval(-2, S.Naturals) is S.false\n    assert Contains.eval(x, S.Naturals) == Contains(x, S.Naturals)\n    raises(TypeError, lambda: Contains.eval(2, None))\n", "def test_eval_method():\n    # Test Contains.eval method\n    x = Symbol('x')\n    assert Contains.eval(x, S.Integers) == Contains(x, S.Integers)\n    assert Contains.eval(2, S.Integers) is S.true\n    assert Contains.eval(-2, S.Naturals) is S.false\n    raises(TypeError, lambda: Contains.eval(x, None))\n", "def test_contains_eval():\n    x = Symbol('x')\n    s = FiniteSet(1, 2, 3)\n    assert Contains(1, s).eval(1, s) == S.true\n    assert Contains(4, s).eval(4, s) == S.false\n    assert Contains(x, s).eval(x, s) == Contains(x, s)\n", "def test_eval_with_set_returned():\n    x = Symbol('x')\n    y = Symbol('y')\n    # A set is returned when the Contains is not simplified\n    assert isinstance(Contains(x, S.Integers).eval(x, S.Integers), bool) is False\n    # An exception is not raised when a set is passed in\n    Contains(x, S.Integers)\n    # An exception is raised when something other than a set is passed in\n    raises(TypeError, lambda: Contains(x, y).eval(x, y))\n", "def test_contains_eval():\n    x = Symbol('x')\n    s = FiniteSet(1, 2, 3)\n    assert Contains(1, s).eval(1, s) is S.true\n    assert Contains(4, s).eval(4, s) is S.false\n    assert Contains(x, s).eval(x, s) == Contains(x, s)\n"], "sample_1195": ["def test_gamma_matrix_simplify_gpgp():\n    i, j, k = tensor_indices('i,j,k', LorentzIndex)\n    p, q = tensor_heads('p,q', [LorentzIndex])\n    ps = p(i)*G(-i)\n    qs = q(i)*G(-i)\n    t = ps*qs*qs\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(-i)*p(i)*q(j)*q(-j))\n", "def test_simplify_gpgp():\n    i0, i1, i2, i3, i4, i5, i6, i7, i8, i9 = tensor_indices('i0:10', LorentzIndex)\n    p, q, r, s = tensor_heads('p,q,r,s', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n    rs = r(i0)*G(-i0)\n    ss = s(i0)*G(-i0)\n\n    t = ps*qs\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(-i0)*p(i0)*q(-i0))\n\n    t = qs*ps\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(-i0)*p(i0)*q(-i0))\n\n    t = ps*qs*rs\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(-i1)*G(-i0)*p(i0)*q(i1)*r(-i1))\n\n    t = ps*qs*rs*ss\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(-i1)*G(-i0)*p(i0)*q(i1)*r(-i1)*s(-i0))\n\n    t = ps*qs + ps*ps\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(-i0)*p(i0)*q(-i0) + G(-i0)*p(i0)*p(-i0))\n", "def test_simplify_gpgp():\n    i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15 = tensor_indices('i0:16', LorentzIndex)\n    mu, nu, rho, sigma = tensor_indices(\"mu, nu, rho, sigma\", LorentzIndex)\n    D = 4\n    p, q = tensor_heads('p,q', [LorentzIndex])\n\n    # Expressions with free indices:\n    t = (G(mu)*p(-mu)*G(nu)*q(-nu))\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(nu)*q(-nu)*p(mu)*G(-mu))\n\n    t = (G(mu)*p(-mu)*G(nu)*p(-nu))\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, 4*p(-mu)*p(mu))\n\n    t = (G(mu)*p(-mu)*G(nu)*p(-nu)*G(rho)*p(-rho))\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, 4*G(rho)*p(-rho)*p(-mu)*p(mu))\n\n    t = (G(mu)*p(-mu)*G(nu)*q(-nu)*G(rho)*p(-rho))\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, 4*G(nu)*q(-nu)*p(-mu)*p(mu))\n\n    # More complicated examples:\n\n    # Two gamma matrices removed:\n    t = (p(i0)*G(-i0)*q(i1)*G(-i1)*p(i2)*G(-i2)*p(i3)*G(-i3))\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, 4*p(i0)*p(-i0)*p(i1)*p(-i1)*q(i2)*G(-i2))\n\n    # No gamma matrices removed:\n    t = (p(i0)*G(-i0)*q(i1)*G(-i1)*p(i2)*G(-i2))\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(i0)*p(-i0)*G(i1)*q(-i1)*G(i2)*p(-", "def test_simplify_gpgp():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    p, q = tensor_heads('p,q', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n    assert _is_tensor_eq(simplify_gpgp(ps*qs*qs), G(-i0)*p(i0)*q(i1)*q(-i1))\n    assert _is_tensor_eq(simplify_gpgp(ps*qs*qs*qs), G(-i0)*p(i0)*q(i1)*q(i2)*q(-i1)*G(-i2))\n    assert _is_tensor_eq(simplify_gpgp(ps*qs*ps*qs), G(-i0)*p(i0)*q(i1)*p(i2)*q(-i2)*G(-i1))\n", "def test_simplify_gpgp():\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n    assert _is_tensor_eq(simplify_gpgp(ps*qs*qs), G(-i0)*p(i0)*q(i1)*q(-i1))\n    assert _is_tensor_eq(simplify_gpgp(ps*qs), ps*qs)\n    assert _is_tensor_eq(simplify_gpgp(ps*ps*qs*qs), ps*ps*qs*qs)\n    t = ps*qs*ps*qs*ps*qs\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, -12*p(i0)*p(-i0)*q(i1)*q(-i1)*G(-i2)*p(i2)*q(i3)*q(-i3) + 16*p(i0)*q(-i0)*p(i1)*q(-i1)*p(i2)*q(-i2)*G(-i3)*p(i3)*q(i4)*q(-i4))\n"], "sample_1197": ["def test_unit_system():\n    us = UnitSystem(base_units=(meter, second), name=\"Test System\", descr=\"Test description\")\n    assert us.name == \"Test System\"\n    assert us.descr == \"Test description\"\n    assert us._base_units == (meter, second)\n    assert us.get_dimension_system() is None\n\n    derived_units = {area: meter**2}\n    us = UnitSystem(base_units=(meter, second), derived_units=derived_units, name=\"Test System\", descr=\"Test description\")\n    assert us.derived_units == derived_units\n    assert us.get_dimension_system() is None\n\n    # Test extend method\n    us_ext = us.extend(base=(kilogram,), units=(joule,), name=\"Extended System\", description=\"Extended description\")\n    assert us_ext.name == \"Extended System\"\n    assert us_ext.descr == \"Extended description\"\n    assert us_ext._base_units == (meter, second, kilogram)\n    assert us_ext._units == (meter, second, kilogram, joule)\n    assert us_ext.derived_units == derived_units\n\n    # Test get_default_unit_system method\n    assert UnitSystem.get_default_unit_system().name == \"SI\"\n\n    # Test get_unit_system method\n    assert UnitSystem.get_unit_system(\"SI\").name == \"SI\"\n    raises(ValueError, lambda: UnitSystem.get_unit_system(\"Invalid System\"))\n", "def test_get_units_non_prefixed():\n    from sympy.physics.units import (meter, kilometer, centimeter, joule, volt, ohm, day, second,\n                                     kilogram, pebibyte, elementary_charge, amu)\n    assert meter in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n    assert day in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n    assert elementary_charge not in SI.get_units_non_prefixed()\n    assert amu in SI.get_units_non_prefixed()\n", "def test_unit_system_get_default_unit_system():\n    assert UnitSystem.get_default_unit_system() == UnitSystem._unit_systems[\"SI\"]\n", "def test_get_units_non_prefixed():\n    assert meter in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert day in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n", "def test_unit_system_get_default_unit_system():\n    default_unit_system = UnitSystem.get_default_unit_system()\n    assert default_unit_system.name == \"SI\"\n"], "sample_1199": ["def test_tensor_product_simp_Mul():\n    assert tensor_product_simp_Mul(TP(A, B)*TP(C, D)) == TP(A*C, B*D)\n    assert tensor_product_simp_Mul(x*TP(A, B)*TP(C, D)) == x*TP(A*C, B*D)\n    assert tensor_product_simp_Mul(TP(A, B)*x*TP(C, D)) == x*TP(A*C, B*D)\n    assert tensor_product_simp_Mul(TP(A, B)*TP(C, D)*TP(E, F)) == TP(A*C*E, B*D*F)\n    assert tensor_product_simp_Mul(TP(A, B)**x * TP(C, D)) == TP(A**x * C, B**x * D)\n", "def test_tensor_product_trace_with_non_commutative_symbols():\n    A, B = symbols('A B', commutative=False)\n    t = TensorProduct(A, B)\n    tr = Tr(t)\n    assert tr.doit() == Tr(A)*Tr(B)\n    tr = Tr(t, [0])\n    assert tr.doit() == Tr(A)*B\n    tr = Tr(t, [1])\n    assert tr.doit() == A*Tr(B)\n", "def test_tensor_product_simp_with_commutator():\n    assert tensor_product_simp(TP(Comm(A, B), C)*TP(D, E)) == TP((A*B - B*A)*D, C*E)\n    assert tensor_product_simp(TP(A, B)*TP(Comm(C, D), E)) == TP(A*C*D - A*D*C, B*E)\n    assert tensor_product_simp(TP(Comm(A, B), C)*TP(Comm(D, E), F)) == TP((A*B - B*A)*(D*E - E*D), C*F)\n", "def test_tensor_product_simp_Mul_Mul():\n    # Test for simplification of Mul with multiple TensorProducts\n    e = TensorProduct(A, B) * TensorProduct(C, D) * TensorProduct(E, F)\n    assert tensor_product_simp_Mul(e) == TensorProduct(A*C*E, B*D*F)\n", "def test_tensor_product_simp_nested():\n    assert tensor_product_simp(TP(TP(A, B), C)*TP(D, TP(E, F))) == TP(A*D, B*E)*TP(C, F)\n    assert tensor_product_simp(TP(A, TP(B, C))*TP(TP(D, E), F)) == TP(A*D, B*E)*TP(C, F)\n    assert tensor_product_simp(TP(TP(A, B), TP(C, D))) == TP(A, B)*TP(C, D)\n"], "sample_1198": ["def test_parser_mathematica_unsupported():\n    parser = MathematicaParser()\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"a//b\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"a /. b\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"a~b~c\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"a@b\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"a @@ b\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"a @@@ b\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"Set[a, b]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"SetDelayed[a, b]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"Module[{a}, b]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"Block[{a}, b]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"With[{a}, b]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"If[a, b, c]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"Do[a, b]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"While[a, b]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"For[a, b, c]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"Which[a, b]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"Switch[a, b]\"))\n", "def test_parser_mathematica_function_application():\n    parser = MathematicaParser()\n\n    convert_chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    assert convert_chain(\"f[x]\") == [\"f\", \"x\"]\n    assert convert_chain(\"f[x, y]\") == [\"f\", \"x\", \"y\"]\n    assert convert_chain(\"f[x, y, z]\") == [\"f\", \"x\", \"y\", \"z\"]\n    assert convert_chain(\"f[g[x]]\") == [\"f\", [\"g\", \"x\"]]\n    assert convert_chain(\"f[g[x], h[y]]\") == [\"f\", [\"g\", \"x\"], [\"h\", \"y\"]]\n    assert convert_chain(\"f[g[x, y], h[z, w]]\") == [\"f\", [\"g\", \"x\", \"y\"], [\"h\", \"z\", \"w\"]]\n    assert convert_chain(\"f[g[x, y], h[z, w], i[u, v]]\") == [\"f\", [\"g\", \"x\", \"y\"], [\"h\", \"z\", \"w\"], [\"i\", \"u\", \"v\"]]\n\n    # Test invalid function applications\n    raises(SyntaxError, lambda: convert_chain(\"f[\"))\n    raises(SyntaxError, lambda: convert_chain(\"f]\"))\n    raises(SyntaxError, lambda: convert_chain(\"f[x\"))\n    raises(SyntaxError, lambda: convert_chain(\"f[x,]\"))\n    raises(SyntaxError, lambda: convert_chain(\"f[, y]\"))\n    raises(SyntaxError, lambda: convert_chain(\"f[x, y\"))\n    raises(SyntaxError, lambda: convert_chain(\"f[x, y]\"))\n    raises(SyntaxError, lambda: convert_chain(\"f[[x]]\"))\n    raises(SyntaxError, lambda: convert_chain(\"f[[x, y]]\"))\n", "def test_function_form():\n    Sin, Cos = symbols(\"Sin Cos\", cls=Function)\n    assert parse_mathematica(\"f[x_] := x^3 /; x > 0\") == SetDelayed(Function(\"f\")(Pattern(x, Blank())), Condition(x**3, x > 0))\n    assert parse_mathematica(\"f[x_, y_] := Sin[x]^2 Tan[y]\") == SetDelayed(Function(\"f\")(Pattern(x, Blank()), Pattern(y, Blank())), sin(x)**2*tan(y))\n    assert parse_mathematica(\"f[x_, 3] := x^3 /; x > 0\") == SetDelayed(Function(\"f\")(Pattern(x, Blank()), 3), Condition(x**3, x > 0))\n    assert parse_mathematica(\"f[x] := Cos[x]\") == SetDelayed(Function(\"f\")(x), cos(x))\n    assert parse_mathematica(\"f[x, y] := Sin[x] Cos[y]\") == SetDelayed(Function(\"f\")(x, y), sin(x)*cos(y))\n", "def test_parser_mathematica_atan2():\n    assert parse_mathematica(\"ArcTan[1, 1]\") == atan2(1, 1)\n    assert parse_mathematica(\"ArcTan[x, y]\") == atan2(y, x)\n    assert parse_mathematica(\"ArcTan[3, 4]\") == atan2(4, 3)\n    assert parse_mathematica(\"ArcTan[x, -y]\") == atan2(-y, x)\n", "def test_mathematica_to_sympy_lambda():\n    parser = MathematicaParser()\n    convert_chain = lambda expr: parser._from_fullformlist_to_fullformsympy(parser._from_mathematica_to_tokens(expr))\n    convert_chain2 = lambda expr: parser._from_fullformlist_to_fullformsympy(parser._from_fullform_to_fullformlist(expr))\n    x, y, z = symbols(\"x y z\")\n\n    assert convert_chain(\"#1^2 + #2^2&\") == Function(lambda x, y: x**2 + y**2)\n    assert convert_chain(\"#1^2 + #2^2 &\") == Function(lambda x, y: x**2 + y**2)\n    assert convert_chain(\"Function[{x, y}, x^2 + y^2]\") == Function(lambda x, y: x**2 + y**2)\n    assert convert_chain(\"Function[x, x^3]\") == Function(lambda x: x**3)\n\n    # fullform expression tests\n    full_form1 = \"Function[{x, y}, Times[x, y]]\"\n    full_form2 = \"Function[{x, y, z}, Plus[Times[x, y], z]]\"\n    full_form3 = \"Function[{x, y, z, n}, Sin[Times[x, Plus[y, z], Power[w, n]]]]\"\n\n    assert convert_chain2(full_form1)(x, y) == x*y\n    assert convert_chain2(full_form2)(x, y, z) == x*y + z\n    assert convert_chain2(full_form3)(x, y, z, n) == sin(x*(y + z)*w**n)\n\n    raises(ValueError, lambda: convert_chain(\"Function[x, y, x^3]\"))\n"], "sample_1200": ["def test_get_units_non_prefixed():\n    assert meter in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert day in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n", "def test_issue_units_non_prefixed():\n    from sympy.physics.units import get_unit_system\n    si = get_unit_system('SI')\n    assert len(si.get_units_non_prefixed()) == 31\n    us = UnitSystem(base_units=(meter, second), name='minimal')\n    assert len(us.get_units_non_prefixed()) == 2\n", "def test_get_units_non_prefixed():\n    assert meter in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert day in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n", "def test_get_units_non_prefixed():\n    assert meter in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n", "def test_get_units_non_prefixed():\n    units = SI.get_units_non_prefixed()\n    assert meter in units\n    assert joule in units\n    assert day in units\n    assert second in units\n    assert volt in units\n    assert ohm in units\n    assert centimeter not in units\n    assert kilometer not in units\n    assert kilogram not in units\n    assert pebibyte not in units\n"], "sample_1203": ["def test_block_homomorphism():\n    # Create a permutation group and a block system\n    from sympy.combinatorics.named_groups import DihedralGroup\n    G = DihedralGroup(8)\n    blocks = [0, 1, 1, 2, 2, 3, 3, 0]\n\n    # Compute the block homomorphism\n    T = block_homomorphism(G, blocks)\n\n    # Check that the homomorphism is well-defined\n    assert T.is_homomorphism\n\n    # Check that the kernel is correct\n    assert T.kernel().order() == 4\n", "def test_block_homomorphism():\n    from sympy.combinatorics.perm_groups import PermutationGroup\n    from sympy.combinatorics.named_groups import DihedralGroup\n    from sympy.combinatorics.homomorphisms import block_homomorphism\n\n    G = DihedralGroup(8)\n    blocks = [0, 0, 1, 1, 2, 2, 3, 3]\n    T = block_homomorphism(G, blocks)\n    assert T.domain == G\n    assert T.codomain.degree == 4\n\n    # check that the kernel is correct\n    ker = T.kernel()\n    assert ker.order() == 2**3\n    assert ker.is_subgroup(G)\n    assert ker.is_normal(G)\n    assert all(T(k) == T.codomain.identity for k in ker)\n\n    # check that the image is correct\n    im = T.image()\n    assert im.order() == 2**2\n    assert im.is_subgroup(T.codomain)\n", "def test_block_homomorphism():\n    from sympy.combinatorics.named_groups import DihedralGroup\n    from sympy.combinatorics.homomorphisms import block_homomorphism\n\n    # Create a dihedral group of order 16\n    G = DihedralGroup(8)\n\n    # Define a block system for the group\n    blocks = [0, 1, 1, 2, 2, 3, 3, 0]\n\n    # Compute the block homomorphism\n    T = block_homomorphism(G, blocks)\n\n    # Check that the homomorphism is well-defined\n    assert T(G.identity) == T.codomain.identity\n\n    # Check that the homomorphism preserves the group operation\n    for g in G.generators:\n        for h in G.generators:\n            assert T(g*h) == T(g)*T(h)\n\n    # Check that the kernel of the homomorphism is correct\n    kernel = T.kernel()\n    for g in G.elements:\n        if T(g) == T.codomain.identity:\n            assert g in kernel\n\n    # Check that the image of the homomorphism is correct\n    image = T.image()\n    for g in G.elements:\n        assert T(g) in image\n", "def test_compose_and_restrict():\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    A = AlternatingGroup(4)\n    T1 = homomorphism(G, A, G.generators, A.generators[:2])\n    H = A.subgroup(A.generators[:2])\n    T2 = homomorphism(H, A, H.generators, A.generators[:2])\n    T = T2.compose(T1)\n    assert T(a*b**2*a**-1) == T1(a*b**2*a**-1)\n    assert T.domain == G\n    assert T.codomain == A\n\n    R = T.restrict_to(G.subgroup([a]))\n    assert R.domain == G.subgroup([a])\n    assert R.codomain == A\n    assert R(a) == T(a)\n", "def test_block_homomorphism():\n    D = DihedralGroup(6)\n    blocks = [0, 0, 0, 1, 1, 1]\n    T = block_homomorphism(D, blocks)\n    assert T.domain == D\n    assert T.codomain == PermutationGroup([Permutation(0, 1)])\n    assert T.is_surjective()\n    assert not T.is_injective()\n"], "sample_1201": ["def test_cgs_gauss_induced_units():\n    # Test conversions involving induced units\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == erg/statcoulomb\n    assert convert_to(statampere, statcoulomb/second, cgs_gauss) == statcoulomb/second\n    assert convert_to(gauss, sqrt(gram/centimeter)/second, cgs_gauss) == sqrt(gram/centimeter)/second\n    assert convert_to(maxwell, sqrt(centimeter**3*gram)/second, cgs_gauss) == sqrt(centimeter**3*gram)/second\n\n    # Test conversions between induced units and base units\n    assert convert_to(statvolt, centimeter**(S(3)/2)*sqrt(gram)/second**2, cgs_gauss) == centimeter**(S(3)/2)*sqrt(gram)/second**2\n    assert convert_to(statampere, centimeter**(S(3)/2)*sqrt(gram)/second**2, cgs_gauss) == centimeter**(S(3)/2)*sqrt(gram)/second**3\n    assert convert_to(gauss, sqrt(gram/centimeter)/second, cgs_gauss) == sqrt(gram/centimeter)/second\n    assert convert_to(maxwell, sqrt(centimeter**3*gram)/second, cgs_gauss) == sqrt(centimeter**3*gram)/second\n", "def test_cgs_gauss_magnetic_units():\n    assert convert_to(1*gauss, tesla, cgs_gauss) == 10**-4*tesla\n    assert convert_to(1*tesla, gauss, cgs_gauss) == 10**4*gauss\n\n    assert convert_to(1*maxwell, weber, cgs_gauss) == 10**-8*weber\n    assert convert_to(1*weber, maxwell, cgs_gauss) == 10**8*maxwell\n\n    assert convert_to(1*oersted, ampere/meter, cgs_gauss) == (1000/(4*S.Pi))*ampere/meter\n    assert convert_to(1*ampere/meter, oersted, cgs_gauss) == (4*S.Pi/1000)*oersted\n", "def test_cgs_gauss_units():\n    # Test the base units of the cgs_gauss system\n    assert cgs_gauss.get_quantity_dimension(centimeter) == length\n    assert cgs_gauss.get_quantity_dimension(gram) == mass\n    assert cgs_gauss.get_quantity_dimension(second) == cgs_gauss.get_dimension('time')\n\n    # Test the derived units of the cgs_gauss system\n    assert cgs_gauss.get_quantity_dimension(statcoulomb) == charge\n    assert cgs_gauss.get_quantity_dimension(statampere) == current\n    assert cgs_gauss.get_quantity_dimension(statvolt) == voltage\n    assert cgs_gauss.get_quantity_dimension(gauss) == magnetic_density\n    assert cgs_gauss.get_quantity_dimension(maxwell) == magnetic_flux\n\n    # Test the scale factors of the cgs_gauss system\n    assert cgs_gauss.get_quantity_scale_factor(statcoulomb) == centimeter**(S(3)/2)*gram**(S.Half)/second\n    assert cgs_gauss.get_quantity_scale_factor(statampere) == statcoulomb/second\n    assert cgs_gauss.get_quantity_scale_factor(statvolt) == erg/statcoulomb\n    assert cgs_gauss.get_quantity_scale_factor(gauss) == sqrt(gram/centimeter)/second\n    assert cgs_gauss.get_quantity_scale_factor(maxwell) == sqrt(centimeter**3*gram)/second\n", "def test_cgs_gauss_convert_units():\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == erg/statcoulomb\n    assert convert_to(statvolt, volt, cgs_gauss) == volt/(10**6/299792458)\n    assert convert_to(statampere, statcoulomb/second, cgs_gauss) == statcoulomb/second\n    assert convert_to(statampere, ampere, cgs_gauss) == ampere/(10*299792458)\n\n    assert convert_to(gauss, sqrt(gram/centimeter)/second, cgs_gauss) == sqrt(gram/centimeter)/second\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10**4\n    assert convert_to(maxwell, sqrt(centimeter**3*gram)/second, cgs_gauss) == sqrt(centimeter**3*gram)/second\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n", "def test_cgs_gauss_convert_units():\n    assert convert_to(statcoulomb, coulomb, cgs_gauss) == coulomb/2997924580\n    assert convert_to(statampere, ampere, cgs_gauss) == ampere/2997924580\n    assert convert_to(statvolt, volt, cgs_gauss) == 10**6*volt/299792458\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10000\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n    assert convert_to(erg, joule, cgs_gauss) == joule/10**7\n    assert convert_to(oersted, ampere/meter, cgs_gauss) == 79.57747154594767*ampere/meter\n    assert convert_to(debye, coulomb*meter, cgs_gauss) == 3.336850850000001e-30*coulomb*meter\n"], "sample_1202": ["def test_Infinity_as_numer_denom():\n    assert oo.as_numer_denom() == (1, 0)\n", "def test_igcd2_coverage():\n    assert igcd2(0, 10) == 10\n    assert igcd2(0, -10) == 10\n    assert igcd2(10, 0) == 10\n    assert igcd2(-10, 0) == 10\n    assert igcd2(0, 0) == 0\n    assert igcd2(10, 15) == 5\n    assert igcd2(15, 10) == 5\n    assert igcd2(-10, 15) == 5\n    assert igcd2(10, -15) == 5\n    assert igcd2(-10, -15) == 5\n", "def test_issue_20739():\n    raises(TypeError, lambda: igcd(int, 3))\n    raises(TypeError, lambda: igcdex(int, 3))\n    raises(TypeError, lambda: mod_inverse(int, 5))\n", "def test_Float_hash():\n    assert hash(Float('1.23')) == hash(Float('1.23', 30))\n    assert hash(Float('1.23')) != hash(Float('1.23', 20))\n    assert hash(Float('123456789012345678', 30)) == hash(Float('123456789012345678', 20))\n    assert hash(Float('inf')) == hash(oo)\n    assert hash(Float('-inf')) == hash(-oo)\n    assert hash(Float('nan')) == hash(nan)\n", "def test_TribonacciConstant_simplify():\n    assert simplify(TribonacciConstant) == TribonacciConstant\n    assert simplify(TribonacciConstant**2) == TribonacciConstant**2\n    assert simplify(TribonacciConstant + 1) == 1 + TribonacciConstant\n    assert simplify(TribonacciConstant - 1) == -1 + TribonacciConstant\n    assert simplify(TribonacciConstant * 2) == 2 * TribonacciConstant\n    assert simplify(TribonacciConstant / 2) == TribonacciConstant / 2\n"], "sample_1205": ["def test_PolyElement_imul_num():\n    R, x, y = ring(\"x,y\", ZZ)\n\n    f = x*y + 3*x*y\n    g = f.imul_num(2)\n    assert g == 6*x*y\n    assert f == 6*x*y\n\n    f = x\n    g = f.imul_num(2)\n    assert g == 2*x\n    assert f == x\n", "def test_PolyElement_imul_num():\n    R, x,y,z = ring(\"x,y,z\", ZZ)\n\n    assert (x + y).imul_num(2) == 2*x + 2*y\n    assert (2*x + 2*y).imul_num(2) == 4*x + 4*y\n    assert (x + y).imul_num(-2) == -2*x - 2*y\n    assert (2*x + 2*y).imul_num(-2) == -4*x - 4*y\n    assert (x + y).imul_num(0) == 0\n    assert (2*x + 2*y).imul_num(0) == 0\n\n    assert x.imul_num(2) == 2*x\n    assert x.imul_num(-2) == -2*x\n    assert x.imul_num(0) == 0\n", "def test_PolyElement_strip_zero():\n    R, x, y = ring(\"x,y\", ZZ)\n\n    f = x + 2*y + 3\n    assert (f[0, 0] == 3)\n    f.strip_zero()\n    assert f == x + 2*y + 3\n    assert (0, 0) in f\n\n    f = x + 2*y\n    assert (0, 0) not in f\n    f.strip_zero()\n    assert f == x + 2*y\n    assert (0, 0) not in f\n\n    f = x + 2*y + 3\n    g = f.copy()\n    g[(0, 0)] = 0\n    assert g == x + 2*y\n    assert (0, 0) in g\n    g.strip_zero()\n    assert g == x + 2*y\n    assert (0, 0) not in g\n", "def test_PolyElement_to_dict():\n    R, x, y = ring(\"x,y\", QQ, lex)\n    f = x**2/3 + y**3/4 + x*y/5\n\n    assert f.to_dict() == {(2, 0): QQ(1,3), (0, 3): QQ(1,4), (1, 1): QQ(1,5)}\n\n    R, = ring(\"\", ZZ)\n    assert R(3).to_dict() == {(): 3}\n", "def test_PolyElement_is_squarefree():\n    R, x, y = ring(\"x,y\", ZZ)\n\n    f = x**2*y + x*y + x\n    assert f.is_squarefree is True\n\n    f = x**2*y + x*y + x**2\n    assert f.is_squarefree is False\n"], "sample_1204": ["def test_is_transitive():\n    # Test for small groups\n    G = SymmetricGroup(3)\n    assert G.is_transitive() == True\n    assert G.is_transitive(strict=False) == True\n\n    G = AlternatingGroup(3)\n    assert G.is_transitive() == True\n    assert G.is_transitive(strict=False) == True\n\n    G = CyclicGroup(5)\n    assert G.is_transitive() == True\n    assert G.is_transitive(strict=False) == True\n\n    # Test for strict transitive\n    G = PermutationGroup(Permutation(0, 1), Permutation(2, 3))\n    assert G.is_transitive() == False\n    assert G.is_transitive(strict=False) == True\n\n    G = PermutationGroup(Permutation(0, 1, 2), Permutation(3, 4, 5))\n    assert G.is_transitive() == False\n    assert G.is_transitive(strict=False) == False\n", "def test_coset():\n    G = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 1))\n    H = PermutationGroup(Permutation(0, 1))\n    g = Permutation(1, 2)\n    c = Coset(g, H)\n    assert c.is_left_coset\n    assert not c.is_right_coset\n    assert c.as_list() == [Permutation(1, 2), Permutation(0, 2, 1)]\n", "def test_coset():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    c = Permutation(0, 2)\n    C = Coset(c, G, G)\n    assert C.is_right_coset\n    assert G.order() == len(C.as_list())\n", "def test_polycyclic_group():\n    a = Permutation([1, 2, 0])\n    b = Permutation([1, 0, 2])\n    G = PermutationGroup([a, b])\n    PcGroup = G.polycyclic_group()\n    assert PcGroup.pcgs == [a]\n    assert PcGroup.pc_series == [PermutationGroup(a)]\n    assert PcGroup.relative_order == [3]\n", "def test_transitivity_degree():\n    # Test whether the transitivity degree is correctly calculated\n    # Create a permutation group\n    a = Permutation(0, 1, 2, 3, 4)\n    b = Permutation(1, 2, 3, 4, 0)\n    G = PermutationGroup([a, b])\n    # The transitivity degree of G should be 1\n    assert G.transitivity_degree == 1\n    # Create another permutation group\n    c = Permutation(0, 1, 2, 3)\n    d = Permutation(0, 2)(1, 3)\n    H = PermutationGroup([c, d])\n    # The transitivity degree of H should be 2\n    assert H.transitivity_degree == 2\n"], "sample_1206": ["def test_issue_13342():\n    assert same_and_same_prec(Float('.1', 15), Float('.1', 53))\n    assert same_and_same_prec(Float('.1', 1), Float('.1', 53))\n    assert not same_and_same_prec(Float('.1', 1), Float('.1', 10))\n    assert not same_and_same_prec(Float('.1', 10), Float('.1', 1))\n    assert not same_and_same_prec(Float('.1', 1), Float('.2', 1))\n    assert not same_and_same_prec(Float('.1', 1), Float('.11', 2))\n", "def test_powers_real_negative():\n    from sympy.core.power import Pow\n    p = Symbol('p', real=True, positive=False)\n    assert Pow(S(2), p, evaluate=False).is_real is False\n    assert Pow(S(2), p, evaluate=False).is_Pow\n    assert Pow(S(2), p, evaluate=False).is_positive is False\n    n = Symbol('n', real=True, negative=False)\n    assert Pow(S(2), n, evaluate=False).is_real\n    assert Pow(S(2), n, evaluate=False).is_Pow\n    assert Pow(S(2), n, evaluate=False).is_positive\n", "def test_comp2():\n    # exact numbers should compare at the same precision;\n    # all _as_mpf_val routines should be sure to abide\n    # by the request to change the prec if necessary; if\n    # they don't, the equality test will fail since it compares\n    # the mpf tuples\n    assert comp(sqrt(2)*pi.evalf(3), 4.5, '')\n    assert comp(sqrt(2)*pi.evalf(15), 4.5, '')\n    assert not comp(sqrt(2)*pi.evalf(15), 4.5, '1e-5')\n    assert comp(sqrt(2)*pi.evalf(3), Float(4.5, 3), '')\n    assert comp(sqrt(2)*pi.evalf(15), Float(4.5, 3), '')\n    assert not comp(sqrt(2)*pi.evalf(15), Float(4.5, 3), '1e-5')\n", "def test_issue_13037():\n    assert Integer(0) == int(0)\n    assert Integer(7) == int(7)\n    assert Integer(7) != int(8)\n    assert Integer(0) != float(7)\n    assert Integer(7) == float(7)\n    assert Integer(7) != float(8)\n", "def test_comp_ge():\n    assert comp(sqrt(2).n(2), 1.4, tol=None)\n    assert comp(sqrt(2).n(2), 1.4, tol=\"1e-5\")\n    assert comp(sqrt(2).n(2), 2, tol=None) is False\n    assert comp(sqrt(2).n(2), 2, tol=\"1e-5\") is False\n    assert comp(sqrt(2).n(2), 1.41, tol=None) is False\n    assert comp(sqrt(2).n(2), 1.41, tol=\"1e-5\") is False\n    assert comp(sqrt(2) + sqrt(3)*I, 1.4 + 1.7*I, tol=None)\n    assert comp(sqrt(2) + sqrt(3)*I, 1.4 + 1.7*I, tol=\"1e-5\")\n    assert comp(sqrt(2) + sqrt(3)*I, 1.4 + 2*I, tol=None) is False\n    assert comp(sqrt(2) + sqrt(3)*I, 1.4 + 2*I, tol=\"1e-5\") is False\n    assert comp(sqrt(2), 1.4, tol=\"0\") is False\n    assert comp(sqrt(2), 1.4, tol=0) is False\n"], "sample_1207": ["def test_parse_expr_with_dictionary_keys():\n    x = Symbol('x')\n    y = Symbol('y')\n    d = {x: 1, y: 2}\n    assert parse_expr(\"x + y\", local_dict=d) == x + y\n    assert parse_expr(\"x + y\", local_dict=d, evaluate=False) == x + y\n    d = {\"x\": 1, \"y\": 2}\n    assert parse_expr(\"x + y\", local_dict=d) == 3\n    assert parse_expr(\"x + y\", local_dict=d, evaluate=False) == 3\n    assert parse_expr(\"x + 'y'\", local_dict=d) == 1 + 2\n    assert parse_expr(\"x + 'y'\", local_dict=d, evaluate=False) == 1 + 2\n    assert parse_expr(\"x + y\", global_dict=d) == x + y\n    assert parse_expr(\"x + y\", global_dict=d, evaluate=False) == x + y\n    d = {\"some_string_key\": 1, \"another_key\": 2}\n    assert parse_expr(\"some_string_key + another_key\", local_dict=d) == 3\n    assert parse_expr(\"some_string_key + another_key\", local_dict=d, evaluate=False) == 3\n", "def test_lambda_notation():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('lambda x: x**2', transformations=standard_transformations + (lambda_notation,)) == Lambda(x, x**2)\n    assert parse_expr('lambda x, y: x**2 + y**2', transformations=standard_transformations + (lambda_notation,)) == Lambda((x, y), x**2 + y**2)\n", "def test_parse_expr_evaluate_true():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test evaluate=True for binary operations\n    assert parse_expr('2*x + 3*x', evaluate=True) == 5*x\n    assert parse_expr('x + x + 2*y', evaluate=True) == 2*x + 2*y\n\n    # Test evaluate=True for relational operations\n    assert parse_expr('2 < 3', evaluate=True) == True\n    assert parse_expr('x < x', evaluate=True) == False\n", "def test_T_accessor():\n    assert T[0] == lambda_notation\n    assert T[11] == rationalize\n    assert T[:5] == standard_transformations\n    assert len(T[:]) == 12\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    inputs = {\n        'lambda x: x**2': Function('Lambda')(Symbol('x'), Symbol('x')**2),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n    # check lamda without colon does not raise exception\n    assert parse_expr('lambda x x**2', transformations=transformations) == Function('Lambda')(Symbol('x'), Symbol('x')**2)\n    # check lamda without variable definition does not raise exception\n    assert parse_expr('lambda : x**2', transformations=transformations) == Function('Lambda')(Symbol('x'), Symbol('x')**2)\n"], "sample_1209": ["def test_prefix_equality():\n    k1 = Prefix('kilo', 'k', 3)\n    k2 = Prefix('kilo', 'k', 3)\n    k3 = Prefix('kilo', 'k', 3, base=2)\n    assert k1 == k2\n    assert k1 != k3\n    assert k1 != kilo  # because kilo is a different object\n    assert k1.scale_factor == kilo.scale_factor\n", "def test_prefix_latex():\n    assert kilo._latex(None) == r'\\text{k}'\n    assert micro._latex(None) == r'\\mu'\n    assert milli._latex(None) == r'\\text{m}'\n\n    kibi._latex_repr = r'\\text{kibi}'\n    assert kibi._latex(None) == r'\\text{kibi}'\n\n    micro._latex_repr = None\n    assert micro._latex(None) == r'\\text{mu}'\n\n    kilo._latex_repr = None\n    assert kilo._latex(None) == r'\\text{k}'\n", "def test_prefix_equality():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n\n    dodeca = Prefix('dodeca', 'dd', 1, base=12)\n\n    assert m == Prefix('milli', 'm', -3)\n    assert k != dodeca\n    assert M != k\n    assert dodeca == Prefix('dodeca', 'dd', 1, base=12)\n    assert m != k\n    assert k == Prefix('kilo', 'k', 3)\n", "def test_prefix_operations_division():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n\n    assert k / k is S.One\n    assert m / m is S.One\n    assert M / M is S.One\n    assert k / M == S.One / 1000\n    assert M / k == 1000\n    assert 1 / (k / m) == m / k\n    assert (k / m) / (M / m) == k / M\n", "def test_prefixes_str_and_latex_repr():\n    assert str(kilo) == 'k'\n    assert str(kibi) == 'Y'  # Note: kibi's abbrev is 'Y' in the code file\n\n    # Check latex representation\n    assert kilo._latex(None) == r'\\text{k}'\n    assert micro._latex(None) == r'\\mu'\n"], "sample_1208": ["def test_sample_numpy():\n    distribs_numpy = [\n        # MatrixNormal and Wishart are not implemented in sample_numpy\n    ]\n\n    size = 5\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy not installed. Abort tests for _sample_numpy.')\n    else:\n        for X in distribs_numpy:\n            samps = sample(X, size=size, library='numpy')\n            for sam in samps:\n                assert Matrix(sam) in X.pspace.distribution.set\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n        N = MatrixNormal('M', [[5, 6]], [4], [[2, 1], [1, 2]])\n        raises(NotImplementedError, lambda: sample(N, size=3, library='numpy'))\n        W = Wishart('W', 5, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(W, size=3, library='numpy'))\n", "def test_MatrixDistribution_check():\n    alpha, beta = symbols('alpha beta', positive=True)\n    scale_matrix = MatrixSymbol('scale_matrix', 2, 2)\n    location_matrix = MatrixSymbol('location_matrix', 2, 2)\n    scale_matrix_1 = MatrixSymbol('scale_matrix_1', 2, 2)\n    scale_matrix_2 = MatrixSymbol('scale_matrix_2', 2, 2)\n\n    # MatrixGammaDistribution\n    MatrixGammaDistribution.check(alpha, beta, scale_matrix)\n\n    raises(ValueError, lambda: MatrixGammaDistribution.check(-alpha, beta, scale_matrix))\n    raises(ValueError, lambda: MatrixGammaDistribution.check(alpha, -beta, scale_matrix))\n    raises(ValueError, lambda: MatrixGammaDistribution.check(alpha, beta, Matrix([[1, 2], [3]])))\n\n    # WishartDistribution\n    WishartDistribution.check(alpha, scale_matrix)\n\n    raises(ValueError, lambda: WishartDistribution.check(-alpha, scale_matrix))\n    raises(ValueError, lambda: WishartDistribution.check(alpha, Matrix([[1, 2], [3]])))\n\n    # MatrixNormalDistribution\n    MatrixNormalDistribution.check(location_matrix, scale_matrix_1, scale_matrix_2)\n\n    raises(ValueError, lambda: MatrixNormalDistribution.check(location_matrix, Matrix([[1, 2], [3]]), scale_matrix_2))\n    raises(ValueError, lambda: MatrixNormalDistribution.check(location_matrix, scale_matrix_1, Matrix([[1, 2], [3]])))\n\n    raises(ValueError, lambda: MatrixNormalDistribution.check(location_matrix, Matrix([[1, 2], [3, 4]]), Matrix([[5]])))\n    raises(ValueError, lambda: MatrixNormalDistribution.check(location_matrix, Matrix([[1]]), Matrix([[5, 6], [7, 8]])))\n\n    # MatrixStudentTDistribution\n    nu = symbols('nu', positive=True)\n    MatrixStudentTDistribution.check(nu, location_matrix, scale_matrix_1, scale_matrix_2)\n\n    raises(ValueError, lambda: MatrixStudentTDistribution.check(-nu, location_matrix, scale_matrix_1, scale_matrix_2))\n    raises(ValueError, lambda: MatrixStudentTDistribution.check(nu, location_matrix, Matrix([[1, 2], [3]]), scale_matrix_2))\n    raises(ValueError, lambda: MatrixStudentTDistribution.check(nu, location_matrix, scale_matrix_1, Matrix([[1, 2], [3]])))\n\n    raises(ValueError, lambda:", "def test_sample_numpy():\n    # Make sure no errors occur when scipy is not installed\n    distribs_numpy = [\n        MatrixNormal('M', [[5, 6]], [4], [[2, 1], [1, 2]]),\n        Wishart('W', 5, [[1, 0], [0, 1]])\n    ]\n    size = 5\n    for X in distribs_numpy:\n        raises(NotImplementedError, lambda: sample(X, size=size, library='numpy'))\n    M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n    raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n", "def test_MatrixGammaDistribution():\n    alpha, beta = symbols('alpha beta', positive=True)\n    scale_matrix = MatrixSymbol('scale_matrix', 2, 2)\n    X = MatrixSymbol('X', 2, 2)\n    M = MatrixGammaDistribution(alpha, beta, scale_matrix)\n    exprd = (exp(-Trace(scale_matrix**(-1)*X)/beta)*Determinant(X)**(alpha - 3/2)) / \\\n            (beta**(2*alpha)*multigamma(alpha, 2)*Determinant(scale_matrix)**alpha)\n    assert M.pdf(X) == exprd\n", "def test_MatrixDistribution():\n    M = MatrixGammaDistribution(1, 2, [[1, 0], [0, 1]])\n    assert M.check(1, 2, [[1, 0], [0, 1]]) is None\n    raises(ValueError, lambda: M.check(1, -2, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: M.check(-1, 2, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: M.check(1, 2, [[1, 2], [3, 4]]))\n    assert M.dimension == (2, 2)\n    assert M.set == MatrixSet(2, 2, S.Reals)\n    X = MatrixSymbol('X', 2, 2)\n    assert M(X) == M.pdf(X)\n    raises(NotImplementedError, lambda: M.sample(size=3))\n    raises(ValueError, lambda: M.sample(size=3, library='abc'))\n"]}, "raw_scored_generations": {"sample_2": [false, false, false, true, false], "sample_3": [true, true, false, true, true], "sample_5": [false, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_0": [false, false, false, false, false], "sample_1": [false, false, false, false, true], "sample_6": [false, false, false, false, false], "sample_7": [true, false, false, false, false], "sample_8": [false, true, true, false, false], "sample_9": [false, false, false, false, false], "sample_10": [true, false, false, false, false], "sample_11": [false, false, false, false, false], "sample_12": [true, false, false, false, true], "sample_13": [false, false, false, true, false], "sample_14": [true, false, false, false, false], "sample_15": [true, true, true, true, true], "sample_16": [false, true, false, false, false], "sample_17": [false, true, false, true, false], "sample_18": [true, false, false, false, false], "sample_19": [true, false, false, false, true], "sample_20": [false, false, false, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, true, false, false, false], "sample_23": [false, false, true, false, false], "sample_24": [false, false, false, false, false], "sample_25": [false, false, false, false, false], "sample_26": [true, true, false, true, false], "sample_27": [true, true, false, false, false], "sample_28": [false, true, false, false, false], "sample_29": [false, false, false, false, false], "sample_30": [false, true, true, true, true], "sample_31": [false, false, false, false, false], "sample_32": [true, false, true, false, false], "sample_33": [false, false, true, true, true], "sample_34": [true, false, true, true, true], "sample_35": [false, false, false, false, false], "sample_36": [true, true, false, false, true], "sample_37": [true, true, false, false, false], "sample_38": [false, true, false, false, true], "sample_39": [false, true, true, false, true], "sample_40": [true, false, false, false, false], "sample_41": [true, false, true, true, false], "sample_42": [true, false, true, true, true], "sample_43": [false, false, false, true, false], "sample_44": [false, true, true, true, false], "sample_45": [true, true, true, true, true], "sample_46": [false, true, false, false, false], "sample_47": [false, false, false, true, false], "sample_48": [true, true, true, true, true], "sample_49": [true, true, true, true, false], "sample_50": [true, true, true, true, true], "sample_51": [true, true, true, true, true], "sample_52": [false, true, true, true, false], "sample_54": [true, true, true, true, true], "sample_53": [true, true, true, true, true], "sample_55": [false, false, false, false, false], "sample_58": [false, true, false, true, true], "sample_56": [true, true, true, true, true], "sample_57": [false, true, false, false, false], "sample_59": [true, false, true, true, false], "sample_60": [false, false, false, false, false], "sample_61": [false, false, false, false, false], "sample_62": [false, false, false, false, false], "sample_63": [false, true, true, true, false], "sample_64": [false, false, true, false, false], "sample_65": [true, true, true, true, true], "sample_67": [false, false, true, true, false], "sample_66": [false, true, false, true, true], "sample_68": [false, false, false, false, false], "sample_69": [true, true, true, false, false], "sample_70": [true, true, true, false, true], "sample_71": [true, true, true, true, true], "sample_72": [true, true, true, false, true], "sample_73": [false, false, false, false, false], "sample_75": [false, false, false, false, false], "sample_74": [true, true, true, true, true], "sample_76": [true, true, true, true, true], "sample_77": [true, true, true, true, true], "sample_78": [true, true, true, true, true], "sample_79": [false, false, false, false, false], "sample_80": [true, true, true, true, true], "sample_82": [true, true, true, true, true], "sample_81": [false, false, false, false, false], "sample_83": [true, false, false, true, false], "sample_85": [false, false, false, true, false], "sample_84": [false, false, false, false, false], "sample_86": [true, false, true, false, true], "sample_88": [true, true, true, true, true], "sample_87": [true, false, true, false, true], "sample_89": [false, false, false, false, false], "sample_90": [false, true, false, false, false], "sample_91": [true, true, true, true, true], "sample_92": [false, false, false, false, false], "sample_93": [false, true, true, true, true], "sample_94": [true, true, true, true, false], "sample_95": [false, false, false, false, false], "sample_98": [true, false, false, false, false], "sample_96": [true, true, true, true, true], "sample_99": [false, true, true, true, true], "sample_97": [false, false, false, true, false], "sample_100": [true, false, false, false, false], "sample_102": [true, true, true, true, true], "sample_101": [true, false, true, true, true], "sample_103": [true, true, true, true, true], "sample_104": [false, false, false, true, false], "sample_107": [false, false, false, false, false], "sample_106": [true, true, false, false, true], "sample_105": [false, false, false, false, false], "sample_108": [true, false, false, false, false], "sample_109": [true, true, true, true, true], "sample_111": [true, true, true, true, true], "sample_110": [false, false, false, false, false], "sample_112": [true, true, true, true, true], "sample_113": [true, false, false, true, false], "sample_114": [true, true, true, true, true], "sample_115": [false, true, true, false, false], "sample_116": [false, true, true, true, false], "sample_117": [true, true, false, true, true], "sample_118": [true, true, true, true, true], "sample_119": [true, true, true, true, true], "sample_120": [true, false, true, false, true], "sample_121": [true, false, true, false, true], "sample_122": [false, true, true, true, false], "sample_123": [false, false, false, false, false], "sample_124": [true, true, true, false, true], "sample_125": [true, true, true, true, true], "sample_126": [true, true, true, true, true], "sample_127": [true, true, true, true, true], "sample_128": [false, true, false, true, true], "sample_129": [false, false, false, false, false], "sample_130": [true, true, true, true, true], "sample_131": [true, false, false, false, false], "sample_132": [true, true, false, false, false], "sample_133": [true, true, true, true, true], "sample_135": [true, true, true, true, true], "sample_134": [true, true, true, true, true], "sample_136": [true, false, false, true, true], "sample_139": [true, true, true, true, true], "sample_137": [true, true, true, true, true], "sample_138": [false, true, false, false, false], "sample_140": [false, false, false, false, true], "sample_141": [false, false, false, true, false], "sample_142": [true, true, true, true, true], "sample_143": [true, true, true, true, true], "sample_144": [true, true, true, true, true], "sample_145": [true, true, true, true, true], "sample_146": [true, true, true, true, true], "sample_147": [true, true, true, true, true], "sample_148": [true, true, true, true, true], "sample_151": [true, true, true, true, true], "sample_149": [true, true, true, true, true], "sample_152": [true, true, true, true, false], "sample_150": [false, false, false, false, false], "sample_153": [true, false, false, false, false], "sample_154": [true, true, true, true, true], "sample_155": [true, true, true, true, true], "sample_156": [true, true, true, false, true], "sample_157": [true, false, false, false, false], "sample_158": [false, false, true, true, false], "sample_159": [true, true, true, true, true], "sample_160": [true, true, true, true, true], "sample_161": [true, false, true, true, true], "sample_162": [true, true, true, true, true], "sample_163": [false, true, false, false, false], "sample_164": [false, false, false, false, false], "sample_165": [false, false, false, false, false], "sample_166": [true, true, true, true, true], "sample_167": [false, false, false, false, false], "sample_168": [true, true, true, true, true], "sample_169": [false, true, true, false, true], "sample_171": [true, true, true, true, true], "sample_170": [false, false, false, false, false], "sample_172": [true, true, true, true, true], "sample_173": [false, false, true, false, true], "sample_174": [false, false, true, false, false], "sample_175": [true, true, true, true, true], "sample_176": [true, true, true, true, true], "sample_177": [true, true, true, true, true], "sample_178": [true, true, true, true, true], "sample_180": [true, false, true, true, false], "sample_179": [false, false, false, false, false], "sample_182": [true, true, true, true, true], "sample_181": [true, true, true, true, true], "sample_183": [false, false, false, true, false], "sample_184": [true, false, false, false, false], "sample_185": [false, false, false, false, false], "sample_186": [true, false, true, true, true], "sample_187": [true, true, true, true, true], "sample_188": [true, true, true, false, true], "sample_189": [true, false, false, true, false], "sample_190": [true, true, true, true, true], "sample_191": [false, false, false, false, true], "sample_192": [true, true, false, true, true], "sample_193": [false, true, true, true, false], "sample_194": [true, true, true, true, true], "sample_195": [false, false, false, true, false], "sample_196": [false, false, false, false, false], "sample_198": [false, false, false, false, false], "sample_197": [true, true, true, true, true], "sample_199": [true, true, true, true, true], "sample_200": [true, true, true, true, true], "sample_201": [true, true, true, true, true], "sample_202": [true, true, true, true, true], "sample_203": [true, true, false, true, true], "sample_204": [true, true, true, true, true], "sample_205": [true, true, true, true, true], "sample_206": [true, true, true, true, true], "sample_207": [false, true, true, false, true], "sample_208": [true, true, true, true, true], "sample_209": [true, true, true, false, false], "sample_210": [false, false, false, false, true], "sample_211": [false, false, false, false, false], "sample_213": [false, false, false, false, true], "sample_212": [false, false, true, false, true], "sample_214": [false, true, true, true, true], "sample_215": [false, false, false, false, false], "sample_216": [true, true, true, true, true], "sample_217": [true, true, false, true, false], "sample_218": [true, true, true, true, true], "sample_219": [true, false, false, false, true], "sample_220": [true, false, true, false, true], "sample_221": [false, true, true, true, true], "sample_222": [true, true, true, true, true], "sample_223": [true, true, true, false, true], "sample_224": [true, true, true, true, true], "sample_225": [false, false, true, false, false], "sample_226": [true, true, false, false, false], "sample_227": [true, true, true, true, true], "sample_228": [true, true, true, true, true], "sample_229": [true, true, true, true, true], "sample_230": [true, true, true, true, true], "sample_231": [false, false, true, true, false], "sample_232": [false, false, true, true, true], "sample_233": [true, true, true, true, true], "sample_234": [true, true, true, true, true], "sample_235": [false, false, true, false, true], "sample_236": [true, true, true, true, true], "sample_237": [true, true, false, true, true], "sample_238": [true, true, true, true, true], "sample_239": [true, false, true, true, true], "sample_240": [true, true, true, true, true], "sample_241": [false, true, false, false, true], "sample_242": [true, false, false, false, false], "sample_243": [true, true, true, true, true], "sample_244": [true, true, true, true, true], "sample_245": [true, true, true, true, true], "sample_246": [true, true, true, true, true], "sample_247": [true, true, true, true, true], "sample_248": [true, true, true, true, true], "sample_249": [false, true, false, true, true], "sample_250": [true, true, true, true, true], "sample_251": [true, true, true, true, true], "sample_252": [false, false, false, true, true], "sample_253": [false, false, false, true, false], "sample_254": [false, true, false, true, false], "sample_256": [false, false, false, false, false], "sample_255": [false, false, false, false, false], "sample_257": [false, true, false, true, false], "sample_258": [false, true, true, true, true], "sample_259": [true, true, true, true, true], "sample_260": [true, true, true, true, true], "sample_261": [true, true, true, true, true], "sample_262": [true, false, true, false, true], "sample_263": [true, true, true, false, false], "sample_264": [true, true, true, true, true], "sample_265": [true, true, true, true, true], "sample_266": [true, true, true, true, true], "sample_267": [false, false, false, false, false], "sample_268": [false, false, true, false, false], "sample_269": [true, true, true, true, true], "sample_270": [false, false, false, false, true], "sample_271": [false, false, false, true, true], "sample_272": [true, true, true, true, true], "sample_273": [false, false, false, false, true], "sample_274": [false, false, false, false, false], "sample_275": [false, false, false, false, false], "sample_276": [true, false, false, false, false], "sample_277": [true, true, true, true, true], "sample_278": [false, true, false, false, false], "sample_279": [true, true, false, true, true], "sample_280": [true, true, true, true, true], "sample_281": [true, true, true, false, true], "sample_282": [true, true, true, true, true], "sample_283": [true, true, true, true, true], "sample_284": [false, false, false, false, true], "sample_285": [true, true, true, true, true], "sample_286": [true, false, false, true, true], "sample_287": [true, true, true, true, true], "sample_288": [false, true, false, true, true], "sample_289": [false, false, false, false, true], "sample_290": [true, false, true, true, true], "sample_291": [false, false, false, false, false], "sample_292": [true, true, true, true, true], "sample_293": [false, false, false, false, false], "sample_294": [true, true, true, true, true], "sample_295": [true, false, false, true, true], "sample_296": [true, true, true, true, true], "sample_297": [false, false, false, true, false], "sample_298": [true, true, true, true, true], "sample_299": [true, true, false, true, false], "sample_300": [true, true, true, true, true], "sample_301": [false, false, false, false, false], "sample_302": [true, false, true, true, true], "sample_303": [true, true, true, true, true], "sample_304": [false, false, false, true, true], "sample_305": [false, false, false, false, true], "sample_306": [true, true, true, true, true], "sample_307": [true, true, true, true, true], "sample_308": [true, true, true, true, true], "sample_309": [false, false, false, false, false], "sample_310": [false, false, true, false, false], "sample_312": [true, true, true, true, true], "sample_311": [false, false, false, false, false], "sample_313": [false, false, false, true, true], "sample_314": [false, false, true, false, false], "sample_315": [true, true, true, true, true], "sample_316": [true, true, true, true, true], "sample_317": [true, true, true, true, true], "sample_318": [true, false, false, true, false], "sample_319": [true, true, true, true, true], "sample_320": [true, true, false, true, true], "sample_321": [true, true, false, true, true], "sample_322": [true, true, true, true, true], "sample_323": [false, true, true, true, true], "sample_324": [true, true, true, true, true], "sample_325": [true, true, true, false, true], "sample_326": [true, true, true, true, true], "sample_327": [true, true, true, true, true], "sample_328": [true, true, true, true, true], "sample_329": [true, false, true, true, true], "sample_330": [false, false, false, false, false], "sample_331": [true, true, true, true, true], "sample_332": [true, true, true, true, true], "sample_333": [true, true, true, false, true], "sample_334": [true, true, true, true, true], "sample_335": [true, true, true, true, true], "sample_336": [false, false, false, false, false], "sample_337": [true, true, true, true, true], "sample_338": [true, true, true, true, true], "sample_339": [true, true, true, true, true], "sample_340": [true, true, true, true, true], "sample_341": [false, true, true, true, true], "sample_342": [false, false, true, true, true], "sample_343": [true, true, true, true, true], "sample_344": [true, true, true, true, true], "sample_345": [false, false, true, false, false], "sample_346": [false, false, false, false, false], "sample_347": [true, true, true, true, true], "sample_348": [true, false, true, false, false], "sample_349": [true, true, true, true, true], "sample_350": [true, true, true, true, true], "sample_351": [true, false, true, false, true], "sample_352": [false, true, false, false, false], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [false, false, false, false, false], "sample_356": [true, true, true, true, true], "sample_357": [true, true, true, true, true], "sample_358": [false, false, false, false, false], "sample_359": [true, true, true, true, true], "sample_360": [false, false, false, true, false], "sample_361": [true, true, true, true, true], "sample_362": [true, false, true, true, false], "sample_363": [true, true, true, true, true], "sample_364": [true, true, true, true, true], "sample_365": [true, true, false, false, false], "sample_366": [true, true, true, true, true], "sample_367": [false, false, false, false, false], "sample_368": [true, true, true, true, true], "sample_369": [true, true, true, true, true], "sample_370": [false, false, false, false, false], "sample_371": [true, false, false, false, false], "sample_372": [false, false, false, false, false], "sample_373": [false, false, false, false, false], "sample_374": [false, false, false, false, false], "sample_375": [true, true, true, true, true], "sample_376": [true, true, true, true, true], "sample_377": [false, true, false, false, false], "sample_378": [true, true, true, true, true], "sample_379": [true, true, true, true, true], "sample_380": [true, true, true, true, true], "sample_381": [true, true, true, true, true], "sample_382": [true, true, true, true, true], "sample_383": [true, true, false, false, true], "sample_384": [true, true, true, true, true], "sample_385": [true, true, true, true, true], "sample_386": [true, true, true, true, true], "sample_387": [false, true, true, true, true], "sample_388": [true, false, false, false, false], "sample_389": [false, false, false, false, false], "sample_390": [true, true, true, true, true], "sample_391": [true, true, true, true, true], "sample_392": [true, true, true, true, false], "sample_393": [true, true, true, true, true], "sample_394": [false, false, false, false, false], "sample_395": [true, true, true, false, true], "sample_396": [false, true, true, true, false], "sample_397": [true, true, true, true, true], "sample_398": [false, false, false, false, false], "sample_399": [true, false, true, true, true], "sample_400": [true, true, true, true, true], "sample_401": [true, true, true, true, true], "sample_402": [true, true, false, true, true], "sample_403": [true, true, true, true, true], "sample_404": [true, true, true, true, true], "sample_405": [false, true, true, false, true], "sample_406": [false, false, false, false, false], "sample_407": [true, true, true, true, true], "sample_408": [true, true, true, true, true], "sample_409": [false, false, false, false, false], "sample_410": [false, false, true, false, true], "sample_411": [true, true, true, true, true], "sample_412": [true, true, true, true, true], "sample_413": [false, false, false, false, false], "sample_414": [true, true, true, true, true], "sample_415": [true, true, true, true, false], "sample_416": [true, true, true, true, true], "sample_417": [false, false, false, false, true], "sample_418": [false, false, true, false, false], "sample_419": [true, true, true, true, true], "sample_420": [false, true, false, true, false], "sample_421": [false, true, false, true, false], "sample_422": [false, true, false, false, false], "sample_423": [true, true, true, true, true], "sample_424": [true, true, true, true, true], "sample_425": [true, true, false, false, true], "sample_426": [true, true, true, true, true], "sample_427": [true, true, true, true, true], "sample_428": [true, true, true, true, true], "sample_429": [true, false, true, false, false], "sample_430": [true, true, true, true, true], "sample_431": [false, true, false, true, false], "sample_432": [true, true, true, true, true], "sample_433": [true, true, true, true, true], "sample_434": [false, false, true, false, false], "sample_435": [false, false, false, false, true], "sample_436": [false, false, false, false, false], "sample_437": [true, false, false, false, false], "sample_438": [true, true, false, false, true], "sample_439": [true, true, false, false, true], "sample_440": [true, true, true, true, false], "sample_441": [false, false, false, false, false], "sample_442": [false, false, false, true, false], "sample_443": [false, false, false, true, false], "sample_444": [false, true, true, false, false], "sample_445": [true, true, true, true, true], "sample_446": [false, false, true, false, false], "sample_447": [true, true, true, true, true], "sample_448": [true, true, true, true, true], "sample_449": [false, true, false, false, false], "sample_450": [true, true, true, true, true], "sample_451": [false, true, true, true, true], "sample_453": [true, true, true, true, true], "sample_452": [true, true, true, true, true], "sample_454": [false, false, false, false, false], "sample_455": [true, true, true, false, false], "sample_456": [true, true, true, true, true], "sample_457": [true, false, true, true, true], "sample_458": [true, false, false, false, false], "sample_459": [false, true, false, false, false], "sample_460": [false, false, false, false, false], "sample_461": [false, false, false, true, false], "sample_462": [true, true, true, true, true], "sample_463": [true, true, true, true, true], "sample_464": [false, true, true, false, true], "sample_465": [false, true, true, true, false], "sample_466": [true, true, true, true, true], "sample_467": [true, true, true, true, true], "sample_469": [true, false, true, true, true], "sample_468": [true, true, true, true, true], "sample_470": [false, true, true, true, true], "sample_471": [true, true, true, true, true], "sample_472": [true, true, true, true, true], "sample_473": [false, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [true, true, true, true, true], "sample_476": [false, false, false, false, false], "sample_477": [true, true, true, true, true], "sample_478": [true, true, true, true, true], "sample_479": [true, true, true, true, true], "sample_480": [true, false, true, false, true], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [true, true, true, false, true], "sample_484": [false, false, false, false, false], "sample_485": [false, true, true, true, true], "sample_486": [true, true, true, true, true], "sample_487": [true, true, true, true, false], "sample_488": [true, true, false, true, true], "sample_489": [true, true, true, true, true], "sample_490": [true, true, true, true, true], "sample_491": [true, false, true, true, true], "sample_492": [true, true, true, true, false], "sample_493": [true, true, true, true, true], "sample_494": [true, true, true, true, true], "sample_495": [true, true, true, true, true], "sample_496": [false, false, false, true, false], "sample_497": [true, false, false, false, true], "sample_498": [false, true, false, true, false], "sample_499": [false, false, false, false, false], "sample_500": [false, false, false, false, true], "sample_501": [false, true, true, false, true], "sample_502": [false, true, false, true, false], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, true], "sample_505": [false, true, true, true, false], "sample_506": [false, true, false, true, true], "sample_507": [false, false, true, true, true], "sample_508": [false, false, false, false, true], "sample_509": [false, true, false, false, false], "sample_510": [true, true, false, true, false], "sample_511": [true, true, true, false, false], "sample_512": [true, false, true, false, false], "sample_513": [true, false, false, false, true], "sample_514": [false, false, true, false, false], "sample_515": [false, false, false, false, false], "sample_516": [true, false, false, true, false], "sample_517": [false, false, false, false, true], "sample_518": [false, true, false, false, true], "sample_519": [false, true, true, false, false], "sample_520": [false, false, true, false, false], "sample_521": [false, false, true, false, false], "sample_522": [false, false, false, false, false], "sample_523": [false, false, false, true, false], "sample_524": [false, true, false, false, false], "sample_525": [true, false, false, false, false], "sample_526": [true, true, false, false, false], "sample_527": [false, false, true, true, false], "sample_528": [true, true, true, false, true], "sample_529": [true, false, false, false, true], "sample_530": [false, false, false, false, true], "sample_531": [false, false, true, false, false], "sample_532": [false, true, true, false, true], "sample_533": [false, true, false, false, true], "sample_534": [false, true, true, false, true], "sample_535": [false, false, true, false, false], "sample_536": [false, false, false, false, false], "sample_537": [true, false, true, false, false], "sample_538": [true, true, true, false, true], "sample_539": [true, false, true, true, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [false, false, true, true, false], "sample_543": [false, true, false, true, true], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [false, false, false, true, false], "sample_547": [false, true, false, false, false], "sample_548": [false, false, false, false, true], "sample_549": [false, false, false, false, false], "sample_550": [true, false, false, false, true], "sample_551": [true, false, false, false, false], "sample_552": [true, false, true, false, true], "sample_553": [false, false, false, false, false], "sample_554": [true, false, false, true, true], "sample_555": [false, true, false, false, false], "sample_556": [true, false, false, false, false], "sample_557": [false, false, false, false, false], "sample_558": [false, false, false, false, false], "sample_559": [false, false, false, false, false], "sample_560": [false, false, true, true, false], "sample_561": [true, true, true, false, false], "sample_562": [false, false, false, true, false], "sample_563": [false, false, true, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, true, false, false, false], "sample_567": [false, true, true, true, true], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, false], "sample_571": [false, false, false, false, false], "sample_572": [true, false, false, false, false], "sample_573": [false, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, true, true, false, true], "sample_577": [false, false, false, false, true], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [true, true, false, false, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [false, false, false, true, true], "sample_584": [true, false, false, false, false], "sample_585": [false, false, false, true, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, false], "sample_590": [false, false, false, false, false], "sample_591": [false, false, false, false, false], "sample_592": [false, false, true, false, false], "sample_593": [false, false, false, false, true], "sample_594": [false, false, false, false, false], "sample_595": [false, false, true, true, false], "sample_596": [true, true, true, false, false], "sample_597": [false, false, false, false, false], "sample_598": [true, false, false, true, true], "sample_599": [false, false, false, true, false], "sample_600": [true, false, false, false, false], "sample_601": [false, false, true, false, true], "sample_602": [false, true, false, false, false], "sample_603": [false, true, false, false, false], "sample_604": [false, false, false, true, false], "sample_605": [false, false, true, false, false], "sample_606": [true, true, false, false, false], "sample_607": [true, true, false, false, true], "sample_608": [true, true, false, false, false], "sample_609": [false, true, false, false, false], "sample_610": [false, true, true, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, true, false, false, false], "sample_613": [false, false, false, true, false], "sample_614": [false, false, false, true, false], "sample_615": [true, true, true, true, false], "sample_616": [false, true, false, false, false], "sample_617": [false, false, false, true, false], "sample_618": [false, false, false, false, false], "sample_619": [true, true, false, false, false], "sample_620": [false, true, false, true, false], "sample_621": [false, false, false, false, false], "sample_622": [true, false, false, true, false], "sample_623": [true, true, true, false, false], "sample_624": [false, false, false, false, false], "sample_625": [false, true, true, true, true], "sample_626": [false, false, false, false, false], "sample_627": [false, true, true, true, true], "sample_628": [false, false, false, false, false], "sample_629": [true, true, false, false, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, true, false, true, true], "sample_633": [false, false, false, false, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, true, false, false], "sample_639": [false, false, false, false, false], "sample_640": [false, false, false, true, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, true, true, false], "sample_646": [false, false, false, false, false], "sample_647": [false, true, true, false, true], "sample_648": [false, true, true, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, false, false, false, true], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, true], "sample_655": [false, true, false, true, false], "sample_656": [false, false, false, true, false], "sample_657": [false, false, false, false, true], "sample_658": [false, false, false, false, false], "sample_659": [false, false, false, false, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [true, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, true], "sample_666": [false, false, false, false, false], "sample_667": [true, false, true, true, false], "sample_668": [false, false, false, false, false], "sample_669": [false, true, false, false, true], "sample_670": [false, false, false, false, false], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [true, false, true, false, false], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [false, false, false, false, false], "sample_678": [false, false, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, true, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, false, false], "sample_684": [true, false, false, false, false], "sample_685": [false, false, true, false, false], "sample_686": [false, false, false, false, false], "sample_687": [false, false, true, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [true, false, false, false, false], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [false, false, true, false, true], "sample_698": [true, false, false, false, false], "sample_699": [false, false, true, true, false], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [true, false, false, true, true], "sample_703": [false, false, false, false, false], "sample_704": [false, false, false, false, false], "sample_705": [true, true, false, false, false], "sample_706": [false, true, false, true, false], "sample_707": [false, false, false, false, true], "sample_708": [false, false, false, false, false], "sample_709": [false, true, false, false, false], "sample_710": [false, false, false, false, false], "sample_711": [false, false, false, false, false], "sample_712": [false, false, false, false, true], "sample_713": [false, true, true, false, true], "sample_714": [false, false, false, false, false], "sample_715": [false, false, true, true, false], "sample_716": [false, false, false, true, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, true], "sample_719": [true, false, false, false, false], "sample_720": [false, false, true, false, false], "sample_721": [false, true, false, true, false], "sample_722": [false, false, false, false, false], "sample_723": [false, true, false, false, true], "sample_724": [false, true, false, true, false], "sample_725": [false, false, false, true, false], "sample_726": [false, false, true, true, false], "sample_727": [false, false, true, true, true], "sample_728": [false, false, false, false, false], "sample_729": [false, false, false, false, false], "sample_730": [true, false, true, false, false], "sample_731": [true, true, true, true, true], "sample_732": [true, true, true, true, true], "sample_733": [false, false, true, false, false], "sample_734": [true, true, false, true, true], "sample_735": [true, true, true, true, true], "sample_736": [false, false, false, true, true], "sample_737": [false, false, false, false, false], "sample_738": [false, false, false, true, true], "sample_739": [true, true, false, false, false], "sample_740": [false, false, false, false, false], "sample_741": [false, true, false, true, false], "sample_742": [false, false, true, false, false], "sample_743": [false, false, false, false, false], "sample_744": [true, false, false, false, false], "sample_745": [true, true, true, true, true], "sample_746": [false, false, false, false, false], "sample_747": [false, false, false, false, true], "sample_748": [true, true, true, true, true], "sample_749": [false, false, false, false, false], "sample_750": [false, false, false, true, true], "sample_751": [false, false, false, false, false], "sample_752": [false, false, false, false, true], "sample_753": [true, false, false, false, true], "sample_754": [false, false, false, false, false], "sample_755": [false, true, false, false, false], "sample_756": [false, true, false, false, false], "sample_757": [false, true, true, true, false], "sample_758": [false, false, true, false, false], "sample_759": [true, true, true, true, true], "sample_760": [true, true, false, false, false], "sample_761": [true, false, true, false, true], "sample_762": [false, false, false, false, false], "sample_763": [true, false, false, false, true], "sample_764": [false, false, false, true, true], "sample_765": [false, false, true, true, true], "sample_766": [true, true, true, true, true], "sample_767": [true, true, false, false, true], "sample_768": [false, false, false, false, false], "sample_769": [false, true, true, true, true], "sample_770": [false, true, true, false, false], "sample_771": [false, false, false, true, true], "sample_772": [true, true, false, false, true], "sample_773": [false, false, false, false, false], "sample_774": [true, false, false, true, false], "sample_775": [false, false, false, false, false], "sample_776": [false, false, false, false, false], "sample_777": [false, true, true, false, false], "sample_778": [true, false, true, false, false], "sample_779": [false, false, false, false, false], "sample_780": [true, true, true, true, false], "sample_781": [false, false, false, false, false], "sample_782": [false, false, false, false, false], "sample_783": [false, true, true, false, false], "sample_784": [true, false, true, true, false], "sample_785": [false, false, false, false, false], "sample_786": [true, true, false, true, true], "sample_787": [true, false, false, true, false], "sample_788": [false, false, true, true, false], "sample_789": [false, false, true, true, false], "sample_790": [false, false, true, false, false], "sample_791": [false, false, true, true, false], "sample_792": [false, false, false, true, true], "sample_793": [false, true, false, false, false], "sample_794": [false, true, true, false, false], "sample_795": [false, false, true, false, false], "sample_796": [true, true, false, true, false], "sample_797": [true, false, false, false, false], "sample_798": [false, true, false, true, true], "sample_799": [true, false, false, false, false], "sample_800": [false, false, false, false, false], "sample_801": [false, false, false, false, false], "sample_802": [true, true, false, false, false], "sample_803": [false, true, false, true, false], "sample_804": [false, false, false, false, false], "sample_805": [false, false, true, true, true], "sample_806": [true, true, false, false, false], "sample_807": [false, false, true, false, false], "sample_808": [false, true, false, true, false], "sample_809": [true, false, false, false, false], "sample_810": [true, false, false, false, false], "sample_811": [false, false, true, true, false], "sample_812": [false, false, false, false, false], "sample_813": [false, true, false, true, true], "sample_814": [false, false, true, false, false], "sample_815": [false, false, false, false, false], "sample_816": [true, false, false, true, false], "sample_817": [true, false, false, false, false], "sample_818": [false, true, true, true, false], "sample_819": [false, true, true, false, true], "sample_820": [false, true, false, true, true], "sample_821": [true, true, true, false, true], "sample_822": [false, false, true, false, true], "sample_823": [false, true, true, false, false], "sample_824": [false, true, true, false, false], "sample_825": [true, false, false, true, false], "sample_826": [true, true, true, true, false], "sample_827": [true, false, false, false, true], "sample_828": [true, false, true, true, false], "sample_829": [true, false, true, false, false], "sample_830": [false, false, false, false, false], "sample_831": [true, false, true, false, false], "sample_832": [true, true, false, false, false], "sample_833": [true, false, true, false, true], "sample_834": [false, true, false, false, true], "sample_835": [true, true, false, true, true], "sample_836": [false, true, false, true, false], "sample_837": [false, false, false, false, false], "sample_838": [false, false, false, false, false], "sample_839": [false, false, false, false, true], "sample_840": [false, false, false, false, true], "sample_841": [false, true, false, false, true], "sample_842": [false, false, false, false, false], "sample_843": [false, true, false, true, true], "sample_844": [true, true, true, false, false], "sample_845": [true, true, true, false, false], "sample_846": [false, false, false, false, true], "sample_847": [false, false, true, true, true], "sample_848": [true, false, true, false, true], "sample_849": [true, false, true, false, true], "sample_850": [false, false, true, false, true], "sample_851": [false, true, false, false, true], "sample_852": [false, false, false, false, true], "sample_853": [true, true, true, false, false], "sample_854": [true, false, false, true, true], "sample_855": [true, false, true, false, false], "sample_856": [false, false, true, false, false], "sample_857": [true, false, true, false, false], "sample_858": [true, true, true, false, true], "sample_859": [true, true, true, false, false], "sample_860": [true, true, false, true, true], "sample_861": [true, true, false, false, true], "sample_862": [false, false, false, true, true], "sample_863": [false, false, false, true, false], "sample_864": [true, true, true, false, true], "sample_865": [false, true, false, false, true], "sample_866": [true, false, true, true, true], "sample_867": [true, true, false, true, false], "sample_868": [false, false, false, false, false], "sample_869": [false, false, true, true, false], "sample_870": [true, true, false, true, true], "sample_871": [false, true, false, false, true], "sample_872": [false, false, false, false, false], "sample_873": [true, false, false, false, false], "sample_874": [false, false, false, false, true], "sample_875": [true, false, true, true, true], "sample_876": [true, true, false, false, true], "sample_877": [false, true, false, false, true], "sample_878": [true, true, true, true, true], "sample_879": [true, false, true, false, false], "sample_880": [false, false, false, false, false], "sample_881": [false, false, true, false, true], "sample_882": [true, true, false, false, true], "sample_883": [true, false, true, true, true], "sample_884": [true, true, true, true, true], "sample_885": [true, false, false, true, false], "sample_886": [true, false, false, false, false], "sample_887": [false, true, true, true, false], "sample_888": [false, false, false, false, true], "sample_889": [false, true, false, false, true], "sample_890": [true, false, true, false, false], "sample_891": [true, false, false, false, false], "sample_892": [false, true, false, true, true], "sample_893": [false, true, false, false, false], "sample_894": [false, false, true, true, true], "sample_895": [true, false, true, false, false], "sample_896": [false, false, false, true, false], "sample_897": [false, true, false, false, true], "sample_898": [false, false, false, true, false], "sample_899": [true, false, false, false, false], "sample_900": [true, false, true, false, false], "sample_901": [true, true, true, true, true], "sample_902": [true, false, true, false, false], "sample_903": [false, true, false, false, false], "sample_904": [false, false, false, false, false], "sample_905": [true, false, false, false, false], "sample_906": [false, false, false, false, false], "sample_907": [true, true, false, false, false], "sample_908": [false, false, false, false, false], "sample_909": [false, false, false, false, false], "sample_910": [true, true, false, false, false], "sample_911": [false, false, false, false, false], "sample_912": [false, false, true, true, false], "sample_913": [false, false, true, true, true], "sample_914": [false, false, false, false, false], "sample_915": [false, true, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, false, true, true], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [false, false, false, false, true], "sample_923": [false, false, false, true, false], "sample_924": [false, false, false, false, false], "sample_925": [false, false, false, false, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [true, true, false, false, true], "sample_930": [false, false, false, false, false], "sample_931": [false, true, true, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, true, false, true, true], "sample_935": [false, false, false, false, false], "sample_936": [false, false, false, false, false], "sample_937": [false, false, false, false, false], "sample_938": [false, false, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, false, false], "sample_941": [false, false, true, true, true], "sample_942": [false, false, false, false, true], "sample_943": [true, false, true, false, false], "sample_944": [false, false, false, true, false], "sample_945": [true, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [true, false, true, false, false], "sample_951": [false, false, true, false, false], "sample_952": [true, false, false, false, false], "sample_953": [false, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [false, true, false, false, false], "sample_956": [false, false, false, false, false], "sample_957": [false, false, true, true, true], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, false, false, false], "sample_961": [true, false, true, false, false], "sample_962": [false, false, false, false, false], "sample_963": [true, true, true, true, true], "sample_964": [true, false, false, false, true], "sample_965": [false, false, true, false, false], "sample_966": [false, false, false, false, true], "sample_967": [false, false, false, false, false], "sample_968": [true, false, true, true, false], "sample_969": [false, false, false, false, false], "sample_970": [true, false, false, false, false], "sample_971": [false, true, false, false, false], "sample_972": [false, false, false, false, false], "sample_973": [true, true, false, false, false], "sample_974": [false, false, true, true, false], "sample_975": [false, false, true, true, false], "sample_976": [false, false, true, false, true], "sample_977": [false, false, false, false, true], "sample_978": [false, true, false, false, true], "sample_979": [false, false, true, true, true], "sample_980": [false, false, false, false, false], "sample_981": [false, false, false, false, false], "sample_982": [false, false, true, false, false], "sample_983": [true, true, true, false, true], "sample_984": [false, false, false, false, false], "sample_985": [false, false, false, false, false], "sample_986": [false, true, false, false, true], "sample_987": [true, false, true, false, false], "sample_988": [false, false, false, true, true], "sample_989": [false, true, true, true, false], "sample_990": [false, false, false, false, false], "sample_991": [true, false, false, false, false], "sample_992": [true, false, false, false, true], "sample_993": [true, true, false, true, true], "sample_994": [false, false, true, false, false], "sample_995": [true, false, false, false, false], "sample_996": [true, false, false, false, false], "sample_997": [false, false, false, false, false], "sample_998": [true, false, false, true, false], "sample_999": [true, false, false, false, false], "sample_1000": [false, true, true, true, false], "sample_1001": [true, true, false, false, true], "sample_1002": [true, false, false, true, true], "sample_1003": [false, false, false, true, false], "sample_1004": [false, false, true, true, true], "sample_1005": [false, false, true, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, true, true, false], "sample_1009": [false, true, false, true, true], "sample_1010": [false, false, false, false, false], "sample_1011": [false, false, false, false, false], "sample_1012": [true, false, false, false, false], "sample_1013": [false, true, false, true, false], "sample_1014": [true, false, true, true, true], "sample_1015": [false, true, false, false, true], "sample_1016": [false, false, true, false, false], "sample_1017": [false, true, false, true, true], "sample_1018": [false, false, true, false, false], "sample_1019": [true, false, false, false, true], "sample_1020": [false, false, false, false, false], "sample_1021": [false, true, false, false, true], "sample_1022": [false, false, false, false, true], "sample_1023": [false, true, false, false, false], "sample_1024": [true, true, true, false, false], "sample_1025": [true, true, false, false, true], "sample_1026": [false, false, true, true, false], "sample_1027": [false, false, true, false, false], "sample_1028": [false, false, false, false, false], "sample_1029": [false, true, false, false, false], "sample_1030": [false, false, false, false, false], "sample_1031": [false, false, false, true, false], "sample_1032": [false, false, false, true, false], "sample_1033": [true, false, true, true, false], "sample_1034": [false, false, false, true, false], "sample_1035": [false, false, true, false, false], "sample_1036": [false, false, false, false, false], "sample_1037": [true, true, true, true, false], "sample_1038": [false, true, false, false, false], "sample_1039": [false, false, true, false, false], "sample_1040": [true, false, false, false, false], "sample_1041": [false, false, false, false, false], "sample_1042": [false, false, true, false, false], "sample_1043": [false, true, false, false, true], "sample_1044": [false, false, false, true, false], "sample_1045": [false, false, false, true, true], "sample_1046": [false, false, false, false, false], "sample_1047": [true, false, true, false, true], "sample_1048": [false, false, false, false, false], "sample_1049": [true, true, false, false, false], "sample_1050": [true, false, false, true, true], "sample_1051": [false, false, false, false, false], "sample_1052": [false, false, false, false, false], "sample_1053": [false, false, true, false, false], "sample_1054": [false, false, true, false, false], "sample_1055": [true, false, true, true, true], "sample_1056": [true, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, true, false, false, false], "sample_1059": [false, false, false, false, true], "sample_1060": [false, false, false, false, false], "sample_1061": [false, false, false, true, true], "sample_1062": [true, false, false, false, false], "sample_1063": [false, false, true, false, true], "sample_1064": [true, true, true, true, true], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, true, false, false], "sample_1067": [true, false, false, false, true], "sample_1068": [false, false, false, false, false], "sample_1069": [false, true, false, false, false], "sample_1070": [false, false, false, true, false], "sample_1071": [false, true, true, false, true], "sample_1072": [true, true, false, false, true], "sample_1073": [false, false, false, false, false], "sample_1074": [false, true, true, false, false], "sample_1075": [true, true, false, true, true], "sample_1076": [true, true, false, true, false], "sample_1077": [false, true, false, false, true], "sample_1078": [false, true, true, true, true], "sample_1079": [true, true, true, true, false], "sample_1080": [true, false, true, false, true], "sample_1081": [false, false, false, false, false], "sample_1082": [false, false, false, false, true], "sample_1083": [true, true, false, false, false], "sample_1084": [false, false, false, false, false], "sample_1085": [false, false, true, false, false], "sample_1086": [false, false, false, true, false], "sample_1087": [false, false, false, false, false], "sample_1088": [false, false, false, false, false], "sample_1089": [false, false, true, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [true, false, true, false, false], "sample_1092": [true, true, false, false, false], "sample_1093": [true, false, false, false, false], "sample_1094": [false, false, true, false, false], "sample_1095": [true, false, true, false, false], "sample_1096": [true, true, false, false, false], "sample_1097": [false, false, false, false, true], "sample_1098": [false, true, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, false, false, false, false], "sample_1101": [false, true, true, true, false], "sample_1102": [false, false, false, true, false], "sample_1103": [false, true, false, false, false], "sample_1104": [false, true, false, false, false], "sample_1105": [true, false, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [false, false, false, false, false], "sample_1108": [false, false, false, true, false], "sample_1109": [true, false, false, false, true], "sample_1110": [false, false, false, true, false], "sample_1111": [false, false, false, false, false], "sample_1112": [true, true, true, false, false], "sample_1113": [false, false, false, false, false], "sample_1114": [false, true, true, true, false], "sample_1115": [false, false, false, false, false], "sample_1116": [false, true, false, false, true], "sample_1117": [true, false, true, true, true], "sample_1118": [false, true, false, false, false], "sample_1119": [false, false, true, false, false], "sample_1120": [false, false, false, false, true], "sample_1121": [false, true, false, false, true], "sample_1122": [false, true, true, false, false], "sample_1123": [false, true, false, false, false], "sample_1124": [false, false, false, false, false], "sample_1125": [false, false, false, true, false], "sample_1126": [true, true, false, true, true], "sample_1127": [false, false, false, false, true], "sample_1128": [true, false, false, false, true], "sample_1129": [false, false, false, false, false], "sample_1130": [true, true, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, false, false, false], "sample_1133": [false, false, false, false, false], "sample_1134": [false, true, false, false, false], "sample_1135": [false, false, false, false, false], "sample_1136": [true, false, false, false, false], "sample_1137": [true, false, false, false, false], "sample_1138": [false, false, false, false, true], "sample_1139": [false, false, false, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, true, false, false, false], "sample_1142": [false, false, false, false, false], "sample_1143": [true, false, false, false, false], "sample_1144": [false, false, false, false, false], "sample_1145": [false, true, true, true, false], "sample_1146": [false, false, true, false, false], "sample_1147": [false, false, false, false, false], "sample_1148": [false, false, false, false, true], "sample_1149": [true, true, false, false, false], "sample_1150": [false, false, true, true, true], "sample_1151": [false, false, false, false, false], "sample_1152": [true, true, false, true, false], "sample_1153": [true, true, false, false, false], "sample_1154": [true, true, false, false, false], "sample_1155": [true, false, true, false, false], "sample_1156": [false, false, true, false, true], "sample_1157": [false, false, false, false, false], "sample_1158": [true, false, false, false, true], "sample_1159": [false, false, true, false, false], "sample_1160": [true, false, false, false, false], "sample_1161": [false, false, true, true, false], "sample_1162": [false, false, false, false, false], "sample_1163": [false, false, false, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, true, false, false], "sample_1166": [false, false, false, false, false], "sample_1167": [false, true, true, false, false], "sample_1168": [false, false, false, false, false], "sample_1169": [true, true, true, true, false], "sample_1170": [false, true, false, false, false], "sample_1171": [true, false, false, false, true], "sample_1172": [false, false, false, false, true], "sample_1173": [false, true, true, true, false], "sample_1174": [false, true, true, true, false], "sample_1175": [true, false, false, false, false], "sample_1176": [false, false, false, true, true], "sample_1177": [false, false, false, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [false, true, false, false, false], "sample_1180": [true, false, false, false, false], "sample_1181": [false, true, true, true, true], "sample_1182": [false, true, false, false, false], "sample_1183": [true, false, false, false, false], "sample_1184": [true, true, false, false, true], "sample_1185": [false, true, false, true, false], "sample_1186": [true, true, true, true, false], "sample_1187": [false, true, true, false, true], "sample_1188": [false, false, true, false, true], "sample_1189": [true, true, false, true, true], "sample_1190": [false, false, false, false, false], "sample_1191": [true, false, false, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [false, false, false, true, false], "sample_1194": [true, false, true, false, true], "sample_1195": [false, false, false, false, false], "sample_1196": [false, false, false, true, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, true, false, false, false], "sample_1200": [false, false, false, true, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, true, false, false, true], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, true, false, false], "sample_1205": [false, false, false, true, false], "sample_1206": [false, false, false, true, false], "sample_1207": [false, false, false, false, false], "sample_1208": [true, false, false, false, false], "sample_1209": [false, false, true, false, false]}}