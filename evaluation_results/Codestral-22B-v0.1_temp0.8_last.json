{"raw_generations": {"sample_200": ["def test_send_messages_with_empty_message(self):\n    backend = smtp.EmailBackend()\n    backend.connection = True\n    email = EmailMessage()\n    with self.assertRaisesMessage(ValueError, \"Message must have a subject and at least one recipient\"):\n        backend.send_messages([email])\n", "def test_send_messages_with_exception(self):\n    \"\"\"An exception is raised if sending a message fails.\"\"\"\n    backend = smtp.EmailBackend()\n    backend.connection = mock.MagicMock()\n    backend.connection.sendmail.side_effect = SMTPException('Error')\n    email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    with self.assertRaises(SMTPException):\n        backend.send_messages([email])\n", "def test_send_mail_with_connection(self):\n    \"\"\"\n    Test sending mail with a specific connection.\n    \"\"\"\n    connection = mail.get_connection('django.core.mail.backends.locmem.EmailBackend')\n    send_mail('Subject', 'Content', 'sender@example.com', ['recipient@example.com'], connection=connection)\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertEqual(mail.outbox[0].subject, 'Subject')\n", "def test_send_messages_ssl_certfile_keyfile(self):\n    \"\"\"\n    Test sending messages with SSL certificate and key files.\n    \"\"\"\n    with tempfile.NamedTemporaryFile() as certfile, tempfile.NamedTemporaryFile() as keyfile:\n        certfile.write(b'fake certificate data')\n        certfile.flush()\n        keyfile.write(b'fake key data')\n        keyfile.flush()\n\n        backend = smtp.EmailBackend(use_ssl=True, ssl_certfile=certfile.name, ssl_keyfile=keyfile.name)\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        with self.assertRaises(SSLError):\n            backend.send_messages([email])\n", "def test_email_host_user_required_no_password(self):\n    \"\"\"\n    The backend raises an exception if EMAIL_HOST_USER is not provided and a\n    password is provided.\n    \"\"\"\n    with self.assertRaisesMessage(ValueError, \"EMAIL_HOST_USER and EMAIL_HOST_PASSWORD must be set together\"):\n        smtp.EmailBackend(username='', password='password')\n"], "sample_1007": ["def test_subfactorial():\n    n = Symbol('n', integer=True, nonnegative=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(n).is_integer is True\n    assert subfactorial(n).is_nonnegative is True\n    assert subfactorial(2*n).is_even is True\n    assert subfactorial(2*n + 1).is_odd is True\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/E\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_even == fuzzy_and(n.is_odd, n.is_nonnegative)\n    assert subfactorial(n).is_integer == fuzzy_and(n.is_integer, n.is_nonnegative)\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n\n    assert subfactorial(n).is_nonnegative == fuzzy_and(n.is_integer, n.is_nonnegative)\n    assert subfactorial(n).is_odd == fuzzy_and(n.is_even, n.is_nonnegative)\n", "def test_binomial_simplify():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    assert binomial(n, k).simplify() == binomial(n, k)\n    assert binomial(n, n).simplify() == 1\n    assert binomial(n, 0).simplify() == 1\n    assert binomial(n, 1).simplify() == n\n    assert binomial(n, n-1).simplify() == n\n    assert binomial(n, k).simplify(ratio=0.5) == binomial(n, k)\n    assert binomial(n, n).simplify(ratio=0.5) == 1\n    assert binomial(n, 0).simplify(ratio=0.5) == 1\n    assert binomial(n, 1).simplify(ratio=0.5) == n\n    assert binomial(n, n-1).simplify(ratio=0.5) == n\n", "def test_subfactorial():\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    n = Symbol('n', integer=True)\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(n + 1).func == subfactorial\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n + 1).is_even is None\n    assert subfactorial(n + 2).is_even is True\n    assert subfactorial(n + 3).is_even is True\n    assert subfactorial(n + 4).is_even is None\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(n + 1).is_integer is None\n    assert subfactorial(n + 2).is_integer is True\n    assert subfactorial(n + 3).is_integer is True\n    assert subfactorial(n + 4).is_integer is True\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / exp(1)\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(n + 2).is_nonnegative is True\n\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(n + 1).is_odd is True\n    assert subfactorial(n + 3).is_odd is True\n    assert subfactorial(n + 4).is_odd is None\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    nn = Symbol('nn', integer=True, nonnegative=True)\n    np = Symbol('np', integer=True, positive=True)\n    nz = Symbol('nz', integer=True, nonzero=True)\n    nf = Symbol('nf', integer=True, negative=True)\n    nt = Symbol('nt', integer=False)\n    fn = Symbol('fn', noninteger=True)\n\n    assert subfactorial(-1) == 0\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(np).func == subfactorial\n    assert subfactorial(nz).func == subfactorial\n    assert subfactorial(nn).func == subfactorial\n    assert subfactorial(nf).func == subfactorial\n    assert subfactorial(nt).func == subfactorial\n    assert subfactorial(fn).func == subfactorial\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(np).is_even is False\n    assert subfactorial(nz).is_even is None\n    assert subfactorial(nn).is_even is None\n    assert subfactorial(nf).is_even is False\n    assert subfactorial(nt).is_even is None\n    assert subfactorial(fn).is_even is None\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(np).is_integer is True\n    assert subfactorial(nz).is_integer is None\n    assert subfactorial(nn).is_integer is True\n    assert subfactorial(nf).is_integer is True\n    assert subfactorial(nt).is_integer is None\n    assert subfactorial(fn).is_integer is None\n\n    assert subfactorial(n"], "sample_55": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.existing_instance = GetFormsetsArgumentCheckingModel.objects.create(name='existing')\n", "def test_formsets_with_inlines_argument(self):\n    # Test add_view\n    response = self.client.get(reverse('admin:admin_views_getformsetsargumentchecking_add'))\n    self.assertEqual(response.context['adminform'].form.base_fields['name'].initial, None)\n\n    # Test change_view\n    gfa = GetFormsetsArgumentChecking.objects.create(name='Test')\n    response = self.client.get(reverse('admin:admin_views_getformsetsargumentchecking_change', args=(gfa.pk,)))\n    self.assertEqual(response.context['adminform'].form.base_fields['name'].initial, 'Test')\n", "def test_formsets_with_inlines_argument(self):\n    response = self.client.get(reverse('admin:admin_views_getformsetsargumentchecking_add'))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context['adminform'].initial, {'name': ''})\n\n    post_data = {'name': 'Test Object'}\n    response = self.client.post(reverse('admin:admin_views_getformsetsargumentchecking_add'), post_data)\n    self.assertEqual(response.status_code, 302)\n\n    obj = GetFormsetsArgumentChecking.objects.get(name='Test Object')\n    response = self.client.get(reverse('admin:admin_views_getformsetsargumentchecking_change', args=(obj.pk,)))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context['adminform'].initial, {'name': 'Test Object'})\n\n    post_data = {'name': 'Updated Test Object'}\n    response = self.client.post(reverse('admin:admin_views_getformsetsargumentchecking_change', args=(obj.pk,)), post_data)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(GetFormsetsArgumentChecking.objects.get(pk=obj.pk).name, 'Updated Test Object')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.test_obj = GetFormsetsArgumentChecking.objects.create(name='Test Object')\n", "def test_queryset_with_inlines(self):\n    \"\"\"\n    #23934 - When adding a new model instance in the admin, the 'obj' argument\n    of get_queryset() of inline model admin should be None. When changing, it\n    should be equal to the existing model instance.\n    The GetQuerysetArgumentCheckingAdmin ModelAdmin throws an exception if obj\n    is not None during add_view or obj is None during change_view.\n    \"\"\"\n    post_data = {'name': '1', 'inlines_set-TOTAL_FORMS': '0', 'inlines_set-INITIAL_FORMS': '0',\n                 'inlines_set-MAX_NUM_FORMS': '0'}\n    response = self.client.post(reverse('admin:admin_views_querysetwithinlines_add'), post_data)\n    self.assertEqual(response.status_code, 302)\n\n    inline_instance = InlineModel.objects.create(name='2', parent=ExplicitlyProvidedPK.objects.create(name='1'))\n    post_data = {'name': '2', 'inlines_set-TOTAL_FORMS': '1', 'inlines_set-INITIAL_FORMS': '1',\n                 'inlines_set-MAX_NUM_FORMS': '0', 'inlines_set-0-id': str(inline_instance.id),\n                 'inlines_set-0-name': '3'}\n    response = self.client.post(reverse('admin:admin_views_querysetwithinlines_change', args=(inline_instance.parent.id,)), post_data)\n    self.assertEqual(response.status_code, 302)\n"], "sample_744": ["def test_power_transformer_yeo_johnson_2d():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_yeo_johnson():\n    X = np.abs(X_2d)\n    X[:, 0] = 0  # introduce a zero value\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_y_johson_valid_axis():\n    X = np.array([[0, 25, 50, 75, 100],\n                  [2, 4, 6, 8, 10],\n                  [2.6, 4.1, 2.3, 9.5, 0.1]])\n\n    pt = PowerTransformer(method='yeo-johnson')\n\n    assert_raises_regex(ValueError, \"axis should be equal to 0 or 1\"\n                        \". Got axis=2\", pt.fit_transform, X.T, axis=2)\n", "def test_power_transformer_invalid_method_exception():\n    pt = PowerTransformer(method='invalid-method')\n    X = np.abs(X_2d)\n\n    # An exception should be raised if PowerTransformer.method isn't valid\n    invalid_method_message = \"Unknown method specified. Valid choices are\"\n    assert_raise_message(ValueError, invalid_method_message,\n                         pt.fit, X)\n", "def test_power_transformer_yellow_square_root():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n"], "sample_908": ["def test_unparse_arg():\n    code = \"def f(x: int): pass\"\n    module = ast.parse(code)\n    assert ast.unparse(module.body[0].args.args[0]) == \"x: int\"\n", "def test_unparse_arg(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c=3, **kwargs): ...\"\n    module = ast.parse(source)\n    func_def = module.body[0]\n    expected = \"a, b=2, *args, c=3, **kwargs\"\n    assert ast.unparse_arguments(func_def.args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].value.args\n    assert ast.unparse_arguments(node) == expected\n"], "sample_1060": ["def test_Pow_with_fraction_exp():\n    p = NumPyPrinter()\n    expr = x**Rational(1, 2)\n    assert p.doprint(expr) == 'numpy.sqrt(x)'\n", "def test_MatMul_shape_compatibility():\n    p = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 3)\n    B = MatrixSymbol(\"B\", 3, 2)\n    assert p.doprint(A*B) == \"A.dot(B)\"\n\n    A = MatrixSymbol(\"A\", 3, 2)\n    B = MatrixSymbol(\"B\", 2, 3)\n    assert p.doprint(A*B) == \"A.T.dot(B.T)\"\n", "def test_additional_functionality():\n    p = NumPyPrinter()\n    expr = sign(x) + acos(y)\n    assert p.doprint(expr) == 'numpy.sign(x) + numpy.arccos(y)'\n\n    expr = Piecewise((1, x < 0), (0, x == 0), (1, x > 0))\n    assert p.doprint(expr) == \"numpy.select([x < 0, x == 0, x > 0], [1, 0, 1], default=numpy.nan)\"\n\n    expr = And(x > 0, y < 0)\n    assert p.doprint(expr) == \"numpy.logical_and(x > 0, y < 0)\"\n\n    expr = Or(x > 0, y < 0)\n    assert p.doprint(expr) == \"numpy.logical_or(x > 0, y < 0)\"\n\n    expr = Mod(x, y)\n    assert p.doprint(expr) == \"numpy.mod(x, y)\"\n", "def test_PythonCodePrinter_complex_numbers():\n    prntr = PythonCodePrinter()\n    z = x + 1j*y\n    assert prntr.doprint(z.real) == \"x\"\n    assert prntr.doprint(z.imag) == \"y\"\n", "def test_additional_functions():\n    p = NumPyPrinter()\n    assert p.doprint(x**0.5) == 'numpy.sqrt(x)'\n    assert p.doprint(x.arg()) == 'numpy.angle(x)'\n    assert p.doprint(x.im) == 'numpy.imag(x)'\n    assert p.doprint(x.re) == 'numpy.real(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == 'numpy.select([x == 0, x > 6], [1, 2], default=numpy.nan)'\n    assert p.doprint(And(x, y)) == 'numpy.logical_and(x, y)'\n    assert p.doprint(Or(x, y)) == 'numpy.logical_or(x, y)'\n    assert p.doprint(~x) == 'numpy.logical_not(x)'\n"], "sample_693": ["def test_do_cleanups_on_test_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n                pass\n            assert MyTestCase.values == [1, 1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 2\n", "def test_do_cleanups_on_test_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n            assert MyTestCase.values == [1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "def test_teardown_class_on_error(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n            @classmethod\n                cls.values.append(2)\n                assert False\n                pass\n            assert MyTestCase.values == [2]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 2\n", "def test_teardown_success(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                self.values.append(None)\n                pass\n        class Second(unittest.TestCase):\n                self.assertEqual(MyTestCase.values, [None])\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 0, failed\n    assert passed == 2\n", "def test_setup_teardown_with_setup_class_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n            @classmethod\n                assert False\n                pass\n            assert MyTestCase.values == []\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n"], "sample_488": ["    def test_models_import_before_apps_ready(self):\n        \"\"\"\n        apps.get_model() with require_ready=False should import the models\n        before apps.apps_ready is True.\n        \"\"\"\n        apps.apps_ready = False\n        apps.models_ready = False\n        try:\n            # The cache must be cleared to trigger the import.\n            apps.get_models.cache_clear()\n            model = apps.get_model(\"apps\", \"TotallyNormal\", require_ready=False)\n            self.assertEqual(model, TotallyNormal)\n        finally:\n            apps.apps_ready = True\n            apps.models_ready = True\n", "    def test_get_registered_model(self):\n        \"\"\"\n        Test for the apps.get_registered_model() method.\n        \"\"\"\n        with self.settings(INSTALLED_APPS=[\"apps\"]):\n            # Test getting a registered model\n            model = apps.get_registered_model(\"apps\", \"TotallyNormal\")\n            self.assertEqual(model, TotallyNormal)\n\n            # Test getting a model that is not registered\n            with self.assertRaises(LookupError):\n                apps.get_registered_model(\"apps\", \"NonExistentModel\")\n", "    def test_get_models(self):\n        \"\"\"\n        Tests apps.get_models().\n        \"\"\"\n        models = apps.get_models()\n        self.assertIn(LogEntry, models)\n        self.assertNotIn(SoAlternative, models)\n\n        models_include_auto_created = apps.get_models(include_auto_created=True)\n        self.assertNotEqual(models, models_include_auto_created)\n\n        models_include_swapped = apps.get_models(include_swapped=True)\n        self.assertNotEqual(models, models_include_swapped)\n\n        models_include_all = apps.get_models(include_auto_created=True, include_swapped=True)\n        self.assertNotEqual(models, models_include_all)\n        self.assertNotEqual(models_include_auto_created, models_include_all)\n        self.assertNotEqual(models_include_swapped, models_include_all)\n", "def test_get_registered_model(self):\n    \"\"\"\n    Tests apps.get_registered_model().\n    \"\"\"\n    model = apps.get_registered_model(\"apps\", \"ModelPK\")\n    self.assertEqual(model, ModelPK)\n\n    with self.assertRaises(LookupError):\n        apps.get_registered_model(\"apps\", \"NonExistentModel\")\n", "    def test_app_config_create(self):\n        \"\"\"\n        Tests the AppConfig.create() method.\n        \"\"\"\n        # Test creating an AppConfig from a string\n        app_config = AppConfig.create(\"apps.apps.MyAdmin\")\n        self.assertIsInstance(app_config, AppConfig)\n        self.assertEqual(app_config.label, \"myadmin\")\n\n        # Test creating an AppConfig from a dotted path\n        app_config = AppConfig.create(\"django.contrib.admin\")\n        self.assertIsInstance(app_config, AppConfig)\n        self.assertEqual(app_config.label, \"admin\")\n\n        # Test creating an AppConfig from an AppConfig instance\n        original_app_config = AppConfig(\"label\", Stub())\n        new_app_config = AppConfig.create(original_app_config)\n        self.assertIs(new_app_config, original_app_config)\n\n        # Test creating an AppConfig with an invalid type\n        with self.assertRaises(TypeError):\n            AppConfig.create(123)\n"], "sample_572": ["def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1, abs=1e-6)\n", "def test_bivariate_probability_stat_with_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1, abs=1e-6)\n", "def test_weighted_ecdf(self, x, weights):\n\n    ecdf = ECDF()\n    stat, vals = ecdf(x, weights=weights)\n    expected_stats = weights[x.argsort()].cumsum() / weights.sum()\n    assert_array_equal(vals[1:], np.sort(x))\n    assert_array_almost_equal(stat[1:], expected_stats)\n    assert stat[0] == 0\n", "def test_custom_estimator_with_errorbars(self, long_df):\n\n        return np.percentile(x, 75) - np.percentile(x, 25)\n\n    agg = EstimateAggregator(custom_estimator, \"sd\")\n    out = agg(long_df, \"z\")\n    assert out[\"z\"] == custom_estimator(long_df[\"z\"])\n    assert out[\"zmin\"] == (custom_estimator(long_df[\"z\"]) - long_df[\"z\"].std())\n    assert out[\"zmax\"] == (custom_estimator(long_df[\"z\"]) + long_df[\"z\"].std())\n", "def test_custom_estimator_errorbars(self, long_df):\n\n    custom_est = lambda x: np.percentile(x, 90)  # noqa: E731\n    agg = EstimateAggregator(custom_est, \"ci\", n_boot=10000, seed=0)\n    out = agg(long_df, \"y\")\n\n    custom_boots = bootstrap(long_df[\"y\"], n_boot=10000, seed=0, func=custom_est)\n    expected_min, expected_max = _percentile_interval(custom_boots, 95)\n\n    assert out[\"ymin\"] == pytest.approx(expected_min, abs=1e-2)\n    assert out[\"ymax\"] == pytest.approx(expected_max, abs=1e-2)\n"], "sample_416": ["def test_no_dbname_and_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"USER\": \"someuser\"}),\n        ([\"psql\", \"-U\", \"someuser\", \"postgres\"], None),\n    )\n", "def test_no_dbname_no_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n            None,\n        ),\n    )\n", "def test_missing_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"PASSWORD\": \"somepassword\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n", "def test_no_name_with_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n                \"OPTIONS\": {\n                    \"service\": \"django_test\",\n                },\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\"],\n            {\"PGSERVICE\": \"django_test\"},\n        ),\n    )\n", "    def test_no_dbname_and_service(self):\n        \"\"\"Test connection to the default 'postgres' db when dbname and service are not provided.\"\"\"\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                None,\n            ),\n        )\n"], "sample_1114": ["def test_issue_18146():\n    e = Symbol('e', integer=True, even=True)\n    o = Symbol('o', integer=True, odd=True)\n    assert Range(5).contains(i) == And(i >= 0, i <= 4)\n    assert Range(1, 9, 2).contains(e) == False\n    assert Range(1, 9, 2).contains(o) == And(o >= 1, o <= 7)\n    assert Range(8, 0, -2).contains(o) == False\n    assert Range(9, 1, -2).contains(o) == And(o >= 3, o <= 9)\n", "def test_issue_18081():\n    assert ImageSet(Lambda(n, n*log(2)), S.Integers).intersection(S.Integers) == \\\n        Intersection(ImageSet(Lambda(n, n*log(2)), S.Integers), S.Integers)\n", "def test_imageset_intersection_nonlinear():\n    n = Dummy()\n    s = ImageSet(Lambda(n, n**2 - 2), S.Integers)\n    assert s.intersect(S.Integers) == FiniteSet(2, -2)\n", "def test_issue_18224():\n    assert imageset(Lambda(x, 1/x), S.Integers).intersect(S.Reals) is S.EmptySet\n", "def test_issue_18139():\n    r = Range(1, 10, 2)\n    assert r.intersection(S.Integers) == r\n    assert r.intersection(S.Reals) == r\n    assert r.intersection(S.Complexes) == r\n"], "sample_5": ["def test_models_evaluate_with_quantities():\n    m = Gaussian1D(amplitude=3 * u.Jy, mean=2 * u.m, stddev=30 * u.cm)\n    x = 2600 * u.mm\n    y = 3 * u.Jy * np.exp(-2)\n    assert_quantity_allclose(m(x), y)\n", "def test_models_quantity_output(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    for args in model['evaluation']:\n        result = m(*args[:-1])\n        assert isinstance(result, u.Quantity)\n        assert result.unit.is_equivalent(args[-1].unit)\n", "def test_models_evaluate_with_quantity_array(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n\n    for args in model['evaluation']:\n        if len(args) == 2:\n            x, y = args\n            x_arr = u.Quantity([x, x], subok=True)\n            y_arr = u.Quantity([y, y], subok=True)\n            result = m(x_arr)\n            assert_quantity_allclose(result, y_arr)\n        else:\n            x, y, z = args\n            x_arr = u.Quantity([x, x])\n            y_arr = u.Quantity([y, y])\n            z_arr = u.Quantity([z, z])\n            result = m(x_arr, y_arr)\n            assert_quantity_allclose(result, z_arr)\n", "def test_models_evaluate_with_array_parameters(model):\n\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    params = {}\n    for key, value in model['parameters'].items():\n        if value is None or key == 'degree':\n            params[key] = value\n        else:\n            params[key] = np.repeat(value, 2)\n\n    m = model['class'](**params)\n\n    for args in model['evaluation']:\n        if len(args) == 2:\n            x, y = args\n            x_arr = u.Quantity([x, x], subok=True)\n            result = m(x_arr)\n            assert_quantity_allclose(result, u.Quantity([y, y], subok=True))\n        else:\n            x, y, z = args\n            x_arr = u.Quantity([x, x])\n            y_arr = u.Quantity([y, y])\n            result = m(x_arr, y_arr)\n            assert_quantity_allclose(result, u.Quantity([z, z]))\n", "def test_models_evaluate_with_magnitude_units(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n    m = model['class'](**model['parameters'])\n    for args in model['evaluation']:\n        if isinstance(args[0], u.Quantity) and isinstance(args[0].value, u.Magnitude):\n            args = list(args)\n            args[0] = args[0].to(u.dimensionless_unscaled)\n        assert_quantity_allclose(m(*args[:-1]), args[-1])\n"], "sample_1029": ["def test_Cycle():\n    from sympy import Cycle\n    sT(Cycle(1, 2, 3), \"Cycle(1, 2, 3)\")\n", "def test_Cycle():\n    from sympy import Cycle\n    sT(Cycle([1, 2, 3]), \"Cycle([1, 2, 3])\")\n", "def test_Cycle():\n    from sympy import Cycle\n    sT(Cycle(1, 2, 3), \"Cycle([1, 2, 3])\")\n", "def test_BooleanAtom_issue_11151():\n    from sympy import S\n    assert srepr(S.true) == \"true\"\n    assert srepr(S.false) == \"false\"\n", "def test_Cycle():\n    from sympy import Cycle\n    c = Cycle([1, 2, 3])\n    sT(c, \"Cycle((1, 2, 3))\")\n"], "sample_738": ["def test_vectorizer_invalid_ngram_range():\n    # Test for invalid ngram_range parameter\n    with pytest.raises(ValueError):\n        vectorizer = CountVectorizer(ngram_range=(2, 1))\n        vectorizer.fit(ALL_FOOD_DOCS)\n", "def test_vectorizer_inverse_transform_sparse():\n    # raw documents\n    data = ALL_FOOD_DOCS\n    for vectorizer in (TfidfVectorizer(), CountVectorizer()):\n        transformed_data = vectorizer.fit_transform(data)\n        inversed_data = vectorizer.inverse_transform(transformed_data)\n        analyze = vectorizer.build_analyzer()\n        for doc, inversed_terms in zip(data, inversed_data):\n            terms = np.sort(np.unique(analyze(doc)))\n            inversed_terms = np.sort(np.unique(inversed_terms))\n            assert_array_equal(terms, inversed_terms)\n\n        # Test that inverse_transform works with sparse matrices\n        inversed_data_sparse = vectorizer.inverse_transform(transformed_data.tocsr())\n        for terms, terms_sparse in zip(inversed_data, inversed_data_sparse):\n            assert_array_equal(np.sort(terms), np.sort(terms_sparse))\n", "def test_vectorizer_custom_preprocessor():\n    # Test that a custom preprocessor works correctly\n        return s.replace(\" \", \"_\")\n\n    vect = CountVectorizer(preprocessor=custom_preprocessor)\n    X = vect.fit_transform([\"hello world\", \"goodbye world\"])\n    assert_array_equal(vect.get_feature_names(), [\"goodbye\", \"hello\", \"world\"])\n    assert_array_equal(X.toarray(), [[1, 1, 1], [1, 0, 1]])\n", "def test_vectorizer_invalid_analyzer():\n    invalid_analyzer = 123\n    message = \"{} is not a valid tokenization scheme/analyzer\".format(invalid_analyzer)\n    exception = ValueError\n    vect = CountVectorizer(analyzer=invalid_analyzer)\n    assert_raise_message(exception, message, vect.fit, ['hello world!'])\n", "def test_countvectorizer_min_df_with_large_dataset():\n    # Test CountVectorizer with min_df parameter on a large dataset\n    # This test is to ensure that CountVectorizer handles large datasets correctly when min_df is set\n    large_dataset = [\" \".join(JUNK_FOOD_DOCS) for _ in range(10000)]\n    vect = CountVectorizer(min_df=0.9)\n    vect.fit(large_dataset)\n    assert_equal(len(vect.vocabulary_), 0)  # No terms should be kept as min_df is too high\n\n    vect.min_df = 0.1\n    vect.fit(large_dataset)\n    assert_greater(len(vect.vocabulary_), 0)  # Some terms should be kept as min_df is lower\n"], "sample_272": ["    def test_atomic_operation_in_atomic_migration(self):\n        \"\"\"\n        An atomic operation is properly rolled back inside an atomic migration.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        with self.assertRaisesMessage(RuntimeError, \"Abort migration\"):\n            executor.migrate([(\"migrations\", \"0001_initial\")])\n        migrations_apps = executor.loader.project_state((\"migrations\", \"0001_initial\")).apps\n        Editor = migrations_apps.get_model(\"migrations\", \"Editor\")\n        self.assertFalse(Editor.objects.exists())\n        # Record previous migration as successful.\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n        # Rebuild the graph to reflect the new DB state.\n        executor.loader.build_graph()\n        # Migrating backwards is also atomic.\n        with self.assertRaisesMessage(RuntimeError, \"Abort migration\"):\n            executor.migrate([(\"migrations\", None)])\n        self.assertFalse(Editor.objects.exists())\n", "def test_minimize_rollbacks_partial_target(self):\n    \"\"\"\n    Minimize rollbacks when target has multiple in-app children and some are already applied.\n\n    a: 1 <---- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n    b:      \\- 1 <--- 2\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    a4_impl = FakeMigration('a4')\n    a4 = ('a', '4')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    b2_impl = FakeMigration('b2')\n    b2 = ('b', '2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(a4, a4_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(b2, b2_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a1)\n    graph.add_dependency(None, a4, a2)\n    graph.add_dependency(None, a4, a3)\n    graph.add_dependency(None, b2, b1)\n    graph.add_dependency(None, b1, a1)\n    graph.add_dependency(None, b2, a2)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        b1: b1_impl,\n        a2", "def test_unapply_migration_atomic(self):\n    \"\"\"\n    Unapplying a migration is atomic (#26527).\n    \"\"\"\n    class Migration(migrations.Migration):\n        atomic = False\n\n            schema_editor.execute(\"CREATE TABLE unapplied (id INTEGER)\")\n            raise RuntimeError('Unapplying migration failed.')\n\n    executor = MigrationExecutor(connection)\n    with self.assertRaisesMessage(RuntimeError, 'Unapplying migration failed.'):\n        executor.unapply_migration(\n            ProjectState(),\n            Migration('0001_initial', 'unapply_migration'),\n        )\n    self.assertTableNotExists('unapplied')\n", "def test_minimize_rollbacks_unapplied_child(self):\n    \"\"\"\n    Do not roll back unapplied children when migrating to an applied migration.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_dependency(None, a2, a1)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n    })\n\n    plan = executor.migration_plan({a1})\n\n    self.assertEqual(plan, [])\n", "    def test_minimize_rollbacks_unapplied_migration(self):\n        r\"\"\"\n        Minimize rollbacks when target migration is unapplied.\n\n        a: 1 -------- 3\n              \\ \\\n               \\  \\\n                \\  2\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        should_be_rolled_back = [a2_impl]\n        exp = [(m, True) for m in should_be_rolled_back]\n        self.assertEqual(plan, exp)\n"], "sample_234": ["def test_union_with_duplicate_values(self):\n    Number.objects.create(num=5, other_num=5)\n    qs1 = Number.objects.filter(num=5).values('num')\n    qs2 = Number.objects.filter(other_num=5).values('num')\n    self.assertNumbersEqual(qs1.union(qs2), [5], ordered=False)\n", "def test_intersection_with_annotated_values(self):\n    Number.objects.create(num=5, other_num=5)\n    qs1 = Number.objects.all().annotate(double_num=F('num') * 2)\n    qs2 = Number.objects.all().annotate(double_num=F('num') * 2)\n    number = qs1.intersection(qs2).values('num', 'double_num').get()\n    self.assertEqual(number['num'], 5)\n    self.assertEqual(number['double_num'], 10)\n", "def test_union_with_values_list_and_ordering(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=5),\n        ReservedName(name='rn0', order=6),\n        ReservedName(name='rn9', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=6)\n    qs2 = ReservedName.objects.filter(order__lte=5)\n    union_qs = qs1.union(qs2)\n    for qs, expected_result in (\n        # Order by a single column.\n        (union_qs.values_list('order', flat=True).order_by('order'), [-1, 5, 6, 7]),\n        (union_qs.values_list('order', flat=True).order_by('-order'), [7, 6, 5, -1]),\n        # Order by multiple columns.\n        (union_qs.values_list('order', flat=True).order_by('name', 'order'), [-1, 5, 6, 7]),\n        (union_qs.values_list('order', flat=True).order_by('-name', '-order'), [7, 6, 5, -1]),\n    ):\n        with self.subTest(qs=qs):\n            self.assertEqual(list(qs), expected_result)\n", "def test_union_with_mixed_data_types(self):\n    Number.objects.create(num=100, other_num='100')\n    qs1 = Number.objects.filter(num=100).values_list('num', flat=True)\n    qs2 = Number.objects.filter(other_num='100').values_list('other_num', flat=True)\n    self.assertEqual(list(qs1.union(qs2).order_by('num')), [100, '100'])\n", "def test_union_with_values_list_and_nulls_last(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=None),\n        ReservedName(name='rn0', order=6),\n        ReservedName(name='rn9', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=6)\n    qs2 = ReservedName.objects.filter(order__isnull=True)\n    union_qs = qs1.union(qs2)\n    self.assertQuerysetEqual(\n        union_qs.values_list('order', flat=True).order_by('order'),\n        [6, 7, None],\n        ordered=False,\n    )\n"], "sample_312": ["def test_add_squash(self):\n    node = Node(['a'], 'AND')\n    self.assertEqual(node.add(Node(['b'], 'AND'), 'AND'), node)\n    self.assertEqual(node, Node(['a', 'b'], 'AND'))\n", "def test_add_squash(self):\n    node = Node(['a'], 'OR')\n    self.assertEqual(node.add(Node(['b'], 'OR'), 'OR'), node)\n    self.assertEqual(node, Node(['a', 'b'], 'OR'))\n", "def test_add_eq_child_same_connector(self):\n    node = Node(['a', 'b'], 'OR')\n    self.assertEqual(node.add('b', 'OR'), 'b')\n    self.assertEqual(node, Node(['a', 'b'], 'OR'))\n", "def test_add_squash(self):\n    node = Node(['a', 'b'], 'AND')\n    other = Node(['c'], 'AND')\n    node.add(other, 'AND', squash=True)\n    self.assertEqual(node, Node(['a', 'b', 'c'], 'AND'))\n", "def test_add_squash_node(self):\n    # Test squashing of a node into the existing children of the node\n    node = Node([('a', 1), ('b', 2)], 'OR')\n    data = Node([('c', 3), ('d', 4)], 'OR')\n    node.add(data, 'OR')\n    self.assertEqual(node, Node([('a', 1), ('b', 2), ('c', 3), ('d', 4)], 'OR'))\n"], "sample_584": ["def test_auto_combine_order_by_coords(self):\n    objs = [Dataset({'foo': ('x', [0])}, coords={'x': ('x', [1])}),\n            Dataset({'foo': ('x', [1])}, coords={'x': ('x', [0])})]\n    actual = auto_combine(objs, concat_dim='x')\n    expected = Dataset({'foo': ('x', [1, 0])},\n                       coords={'x': ('x', [1, 0])})\n    assert_identical(expected, actual)\n", "def test_auto_combine_with_custom_dim(self):\n    objs = [Dataset({'x': [0]}), Dataset({'x': [1]})]\n    with pytest.warns(FutureWarning, match=\"`concat_dim`\"):\n        auto_combine(objs, concat_dim='custom_dim')\n", "def test_auto_combine_with_dimension_coords_and_no_concat(self):\n    objs = [Dataset({'foo': ('x', [0])}, coords={'x': ('x', [0])}),\n            Dataset({'bar': ('x', [1])}, coords={'x': ('x', [1])})]\n    with pytest.warns(FutureWarning, match=\"continue concatenating based\"):\n        auto_combine(objs)\n", "def test_auto_combine_with_uneven_length_coords(self):\n    ds0 = Dataset({'x': [0, 1, 2]})\n    ds1 = Dataset({'x': [3, 4]})\n    with raises_regex(ValueError, \"Coordinate variable x is neither \"\n                                  \"monotonically increasing nor\"):\n        auto_combine([ds1, ds0])\n", "def test_auto_combine_with_different_dimensions(self):\n    objs = [Dataset({'x': [0, 1]}), Dataset({'y': [2, 3]})]\n    with raises_regex(ValueError, 'cannot infer dimension'):\n        auto_combine(objs)\n"], "sample_1138": ["def test_as_f_sign_1():\n    assert as_f_sign_1(x + 1) == (1, x, 1)\n    assert as_f_sign_1(x - 1) == (1, x, -1)\n    assert as_f_sign_1(-x + 1) == (-1, x, -1)\n    assert as_f_sign_1(-x - 1) == (-1, x, 1)\n    assert as_f_sign_1(2*x + 2) == (2, x, 1)\n", "def test_as_f_sign_1():\n    assert as_f_sign_1(x + 1) == (1, x, 1)\n    assert as_f_sign_1(x - 1) == (1, x, -1)\n    assert as_f_sign_1(-x + 1) == (-1, x, -1)\n    assert as_f_sign_1(-x - 1) == (-1, x, 1)\n    assert as_f_sign_1(2*x + 2) == (2, x, 1)\n", "def test_as_f_sign_1():\n    assert as_f_sign_1(x + 1) == (1, x, 1)\n    assert as_f_sign_1(x - 1) == (1, x, -1)\n    assert as_f_sign_1(-x + 1) == (-1, x, -1)\n    assert as_f_sign_1(-x - 1) == (-1, x, 1)\n    assert as_f_sign_1(2*x + 2) == (2, x, 1)\n", "def test_TR15():\n    assert TR15(sin(x)**-2) == cot(x)**2\n", "def test_TR17():\n    assert TR15(1 - 1/sin(x)**4) == (1 - cot(x)**4)\n    assert TR16(1 - 1/cos(x)**4) == (1 - tan(x)**4)\n    assert TR111(1 - 1/tan(x)**4) == (1 - cot(x)**4)\n"], "sample_329": ["def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex((1+2j))\", set())\n    )\n", "def test_serialize_custom_classes(self):\n    class CustomClass:\n            self.value = value\n\n            return self.value == other.value\n\n            return (\n                '%s.%s' % (self.__class__.__module__, self.__class__.__name__),\n                [self.value],\n                {}\n            )\n\n    obj = CustomClass('test')\n    self.assertSerializedEqual(obj)\n    self.assertSerializedResultEqual(\n        obj,\n        (\"migrations.test_writer.CustomClass('test')\", {'import migrations.test_writer'})\n    )\n", "def test_serialize_deconstructed_class(self):\n    class CustomClass:\n            self.a = a\n            self.b = b\n\n            return ('migrations.test_writer.CustomClass', [self.a, self.b], {})\n\n    obj = CustomClass('foo', 42)\n    string, imports = MigrationWriter.serialize(obj)\n    self.assertEqual(string, \"migrations.test_writer.CustomClass('foo', 42)\")\n    self.assertIn('import migrations.test_writer', imports)\n", "def test_serialize_bytes(self):\n    self.assertSerializedEqual(b\"foo\\nbar\")\n    string, imports = MigrationWriter.serialize(b\"foo\\nbar\")\n    self.assertEqual(string, \"b'foo\\\\nbar'\")\n", "def test_serialize_deconstructible(self):\n    self.assertSerializedEqual(DeconstructibleInstances())\n    self.assertSerializedResultEqual(\n        DeconstructibleInstances(),\n        (\"migrations.test_writer.DeconstructibleInstances()\", {'import migrations.test_writer'})\n    )\n"], "sample_1170": ["def test_Differential():\n    from sympy.diffgeom import Differential\n    assert sstr(Differential(b)) == 'd(x)'\n", "def test_Differential():\n    from sympy.diffgeom import CoordSystem, Differential\n    x, y = symbols('x y', real=True)\n    rect = CoordSystem('rect', [x, y])\n    dx = Differential(rect.base_scalars[0])\n    dy = Differential(rect.base_scalars[1])\n    assert str(dx) == \"d(x)\"\n    assert str(dy) == \"d(y)\"\n", "def test_Transpose():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert str(Transpose(A)) == \"A.T\"\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert str(Transpose(B)) == \"B.T\"\n", "def test_issue_21460_expand():\n    ss = lambda x: str(expand(S(x, evaluate=False)))\n    assert ss('4/2') == '2'\n    assert ss('4/-2') == '-2'\n    assert ss('-4/2') == '-2'\n    assert ss('-4/-2') == '2'\n    assert ss('-2*3/-1') == '6'\n    assert ss('-2*3/-1/2') == '3'\n    assert ss('4/2/1') == '2'\n    assert ss('-2/-1/2') == '1'\n    assert ss('2*3*4**(-2*3)') == '2*3/256'\n    assert ss('2*3*1*4**(-2*3)') == '2*3/256'\n", "def test_Boolean():\n    assert str(true) == \"True\"\n    assert str(false) == \"False\"\n    assert str(And(true, false)) == \"True & False\"\n    assert str(Or(true, false)) == \"True | False\"\n    assert str(Not(true)) == \"~True\"\n    assert str(Xor(true, false)) == \"True ^ False\"\n"], "sample_18": ["def test_binary_op_structured(self):\n    q = self.q + self.q\n    assert_info_equal(q, self.q)\n", "def test_binary_op_structured(self):\n    q2 = u.Quantity(np.array([(2.0, 3.0), (4.0, 5.0)], dtype=[(\"p\", \"f8\"), (\"v\", \"f8\")]), \"m, m/s\")\n    q = self.q + q2\n    assert_no_info(q[\"p\"])\n    assert_no_info(q[\"v\"])\n", "def test_arithmetic_operations(self):\n    q1 = self.q * 2\n    assert_info_equal(q1, self.q)\n    q2 = self.q / 2\n    assert_info_equal(q2, self.q)\n    q3 = self.q + 2 * u.m\n    assert_info_equal(q3, self.q)\n    q4 = self.q - 2 * u.m\n    assert_info_equal(q4, self.q)\n", "def test_binary_op_with_quantity(self):\n    q2 = u.Quantity([10.0, 20.0], \"m/s\")\n    q = self.q + q2\n    assert_no_info(q)\n", "def test_binary_op_with_structured_quantity(self):\n    other = u.Quantity([(5.0, 6.0)], \"m, m/s\")\n    result = self.q + other\n    expected_value = np.array([(6.0, 8.0), (8.0, 10.0)], dtype=[(\"p\", \"f8\"), (\"v\", \"f8\")])\n    expected_unit = \"m, m/s\"\n    np.testing.assert_equal(result.value, expected_value)\n    assert result.unit == expected_unit\n    assert_info_equal(result, self.q)\n"], "sample_184": ["def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n"], "sample_39": ["def test_get_axis_types():\n    \"\"\"\n    Test the get_axis_types method.\n    \"\"\"\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'FREQ']\n    result = w.get_axis_types()\n    expected = [{'coordinate_type': 'celestial', 'scale': 'non-linear celestial', 'group': 0, 'number': 0},\n                {'coordinate_type': 'celestial', 'scale': 'non-linear celestial', 'group': 0, 'number': 1},\n                {'coordinate_type': 'spectral', 'scale': 'linear', 'group': 0, 'number': 0}]\n    assert result == expected\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---CAR\", \"DEC--CAR\"]\n    assert w.axis_type_names == [\"RA\", \"DEC\"]\n    w.wcs.ctype = [\"WAVE\", \"FREQ\"]\n    assert w.axis_type_names == [\"WAVE\", \"FREQ\"]\n    w.wcs.cname = [\"X-axis\", \"Y-axis\"]\n    assert w.axis_type_names == [\"X-axis\", \"Y-axis\"]\n", "def test_sip_with_altkey_and_relax():\n    \"\"\"\n    Test that when creating a WCS object using a key, CTYPE with\n    that key is looked at and not the primary CTYPE.\n    Test that relax=False does not include \"-SIP\" suffix in CTYPE.\n    fix for #5443.\n    \"\"\"\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n        w = wcs.WCS(f[0].header)\n    # create a header with two WCSs.\n    h1 = w.to_header(relax=True, key='A')\n    h2 = w.to_header(relax=False)\n    h1['CTYPE1A'] = \"RA---SIN-SIP\"\n    h1['CTYPE2A'] = \"DEC--SIN-SIP\"\n    h1.update(h2)\n    w = wcs.WCS(h1, key='A', relax=False)\n    assert (w.wcs.ctype == np.array(['RA---SIN', 'DEC--SIN'])).all()\n", "def test_all_world2pix_with_divergence():\n    \"\"\"Test all_world2pix with divergent solutions\"\"\"\n    fname = get_pkg_data_filename('data/j94f05bgq_flt.fits')\n    ext = ('SCI', 1)\n    tolerance = 1.0e-4\n    origin = 0\n    random_npts = 25000\n    adaptive = False\n    maxiter = 20\n    detect_divergence = True\n\n    # Open test FITS file and create WCS object\n    h = fits.open(fname)\n    w = wcs.WCS(h[ext].header, h)\n    h.close()\n    del h\n\n    # Generate diverging data\n    divradec = w.all_pix2world([[1.0, 1.0],\n                                [10000.0, 50000.0],\n                                [3.0, 1.0]], 1)\n\n    # Test all_world2pix with divergent solutions\n    try:\n        xy = w.all_world2pix(divradec, 1, maxiter=maxiter,\n                             tolerance=tolerance, adaptive=adaptive,\n                             detect_divergence=detect_divergence,\n                             quiet=False)\n    except wcs.wcs.NoConvergence as e:\n        assert e.divergent is not None\n        assert e.slow_conv is None\n        assert e.best_solution is not None\n        assert e.accuracy is not None\n", "def test_wcs_world2pix_numpy_order():\n    \"\"\"\n    Test that WCS.wcs_world2pix returns Numpy-ordered output\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.crpix = [1, 2]\n    w.wcs.cdelt = [0.1, 0.2]\n    w.wcs.crval = [3, 4]\n\n    x, y = w.wcs_world2pix(3.1, 4.2, 0)\n    assert_allclose(x, 1.1)\n    assert_allclose(y, 1.1)\n"], "sample_45": ["    def test_extract_func_with_dst(self):\n        start_datetime = datetime(2015, 11, 1, 1, 30, 1, 321)\n        end_datetime = datetime(2015, 3, 8, 13, 11, 27, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=True)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        qs = DTModel.objects.annotate(\n            day=Extract('start_datetime', 'day'),\n            week=Extract('start_datetime', 'week'),\n            isoyear=ExtractIsoYear('start_datetime'),\n            weekday=ExtractWeekDay('start_datetime'),\n            quarter=ExtractQuarter('start_datetime'),\n            hour=ExtractHour('start_datetime'),\n        ).order_by('start_datetime')\n\n        model = qs.get()\n        self.assertEqual(model.day, 1)\n        self.assertEqual(model.week, 44)\n        self.assertEqual(model.isoyear, 2015)\n        self.assertEqual(model.weekday, 7)\n        self.assertEqual(model.quarter, 4)\n        self.assertEqual(model.hour, 1)\n", "def test_trunc_func_with_timezone_and_none(self):\n    \"\"\"\n    Test that the Trunc function handles None values correctly when timezone is involved.\n    \"\"\"\n    self.create_model(None, None)\n    melb = pytz.timezone('Australia/Melbourne')\n\n    self.assertIsNone(\n        DTModel.objects.annotate(\n            truncated=Trunc('start_datetime', 'year', output_field=DateTimeField(), tzinfo=melb)\n        ).first().truncated\n    )\n", "def test_trunc_with_timezone_preservation(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, kind, melb)),\n                (end_datetime, truncate_to(end_datetime, kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('year')\n    test_datetime_kind('quarter')\n    test_datetime_kind('month')\n    test_datetime_kind('week')\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n    test_datetime_kind('minute')\n    test_datetime_kind('second')\n", "def test_trunc_tz_applied_after_truncation(self):\n    start_datetime = datetime(2016, 3, 13, 1, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    model = DTModel.objects.annotate(\n        melb_day=TruncDay('start_datetime', tzinfo=melb),\n    ).order_by('start_datetime').get()\n\n    self.assertEqual(model.start_datetime, start_datetime)\n    self.assertEqual(model.melb_day, truncate_to(start_datetime, 'day').astimezone(melb))\n    self.assertEqual(model.start_datetime.day, 13)\n    self.assertEqual(model.melb_day.day, 14)\n", "def test_trunc_func_with_duration_field(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    if settings.USE_TZ:\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    duration = end_datetime - start_datetime\n    truncated_duration = timedelta(days=duration.days)\n\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            truncated=Trunc('duration', 'day', output_field=DurationField())\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, truncated_duration),\n        ],\n        lambda m: (m.start_datetime, m.truncated)\n    )\n"], "sample_686": ["def test_funcargnames_is_deprecated() -> None:\n    class MockFunction:\n        funcargnames = []\n\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"The `funcargnames` attribute was an alias for `fixturenames`\",\n    ):\n        MockFunction.funcargnames = [\"fixture1\", \"fixture2\"]\n", "def test_funcargnames_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"The `funcargnames` attribute was an alias for `fixturenames`\",\n    ):\n        func = mock.Mock()\n        func.funcargnames = [\"arg1\", \"arg2\"]\n        _ = func.funcargnames\n", "def test_warning_captured_hook(pytestconfig: Config) -> None:\n    \"\"\"Check that pytest_warning_captured is deprecated and pytest_warning_recorded is suggested.\"\"\"\n    pluginmanager = pytestconfig.pluginmanager\n\n    # Mock a plugin that uses the deprecated hook\n    pluginmanager.register(\n        WarningDeprecatedPlugin(), name=\"warning_deprecated_plugin\"\n    )\n\n    class WarningDeprecatedPlugin:\n            pass\n\n    with pytest.warns(pytest.PytestDeprecationWarning) as captured:\n        pluginmanager.hook.pytest_warning_captured(None, None, None, None)\n\n    assert len(captured) == 1\n    assert \"The pytest_warning_captured is deprecated\" in str(captured[0].message)\n    assert \"Please use pytest_warning_recorded instead\" in str(captured[0].message)\n", "def test_warning_captured_hook_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"The pytest_warning_captured is deprecated\",\n    ):\n        # Mock the deprecated hook function\n            pass\n\n        # Create a mock plugin with the deprecated hook\n        mock_plugin = mock.Mock()\n        mock_plugin.pytest_warning_captured = pytest_warning_captured\n\n        # Use the mock plugin in the pytest run\n        with pytest.plugin.register(mock_plugin):\n            pytest.main([\"-q\"])\n", "def test_pytest_collect_directory_hook_deprecated(pytestconfig: Config) -> None:\n    \"\"\"Check that pytest_collect_directory hook is deprecated and shows a warning (#XXXX)\"\"\"\n    class MockSession:\n            pass\n            pass\n\n    class MockCollector:\n            self.session = MockSession()\n\n    collector = MockCollector()\n    hook = pytestconfig.pluginmanager.hook.pytest_collect_directory(collector=collector)\n    with pytest.warns(pytest.PytestDeprecationWarning) as cw:\n        hook()\n    assert len(cw) == 1\n    assert cw[0].filename == __file__\n"], "sample_391": ["def test_create_model_add_index(self):\n    \"\"\"\n    AddIndex should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n            ),\n            migrations.AddIndex(\n                \"Foo\",\n                migrations.Index(fields=[\"name\"], name=\"foo_name_idx\"),\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n                options={\n                    \"indexes\": [\n                        migrations.Index(fields=[\"name\"], name=\"foo_name_idx\"),\n                    ],\n                },\n            ),\n        ],\n    )\n", "def test_create_model_alter_table(self):\n    \"\"\"\n    AlterModelTable should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterModelTable(\"Foo\", \"bar\"),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\", \"db_table\": \"bar\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_create_model_rename_model_with_fk_reference(self):\n    \"\"\"\n    RenameModel should optimize into CreateModel if it's a model with a FK reference.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))]\n            ),\n            migrations.CreateModel(\n                \"Bar\",\n                [(\"foo\", models.ForeignKey(\"migrations.Foo\", models.CASCADE))],\n            ),\n            migrations.RenameModel(\"Foo\", \"Baz\"),\n        ],\n        [\n            migrations.CreateModel(\n                \"Baz\", [(\"name\", models.CharField(max_length=255))]\n            ),\n            migrations.CreateModel(\n                \"Bar\",\n                [(\"foo\", models.ForeignKey(\"migrations.Baz\", models.CASCADE))],\n            ),\n        ],\n    )\n", "def test_create_model_with_default_manager(self):\n    \"\"\"\n    CreateModel should preserve the default_manager_name option.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"default_manager_name\": \"custom_manager\"},\n                managers=[(\"custom_manager\", models.Manager())],\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"default_manager_name\": \"custom_manager\"},\n                managers=[(\"custom_manager\", models.Manager())],\n            ),\n        ],\n    )\n", "def test_create_alter_order_with_respect_to_field(self):\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [\n                    (\"a\", models.IntegerField()),\n                    (\"b\", models.IntegerField()),\n                    (\"c\", models.IntegerField()),\n                ],\n            ),\n            migrations.AlterOrderWithRespectTo(\"Foo\", \"b\"),\n            migrations.AlterField(\"Foo\", \"c\", models.CharField(max_length=255)),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [\n                    (\"a\", models.IntegerField()),\n                    (\"b\", models.IntegerField()),\n                    (\"c\", models.CharField(max_length=255)),\n                ],\n                options={\"order_with_respect_to\": \"b\"},\n            ),\n        ],\n    )\n"], "sample_688": ["def test_get_lock_path(tmpdir):\n    path = Path(tmpdir) / \"test_folder\"\n    lock_path = get_lock_path(path)\n    assert lock_path == path / \".lock\"\n", "def test_fixture_scope_different_conftests(testdir):\n    \"\"\"Fixtures should not be shared between modules with different conftest files.\"\"\"\n    foo_path = testdir.mkdir(\"foo\")\n    foo_path.join(\"conftest.py\").write(\n        textwrap.dedent(\n            \"\"\"\\\n            import pytest\n            @pytest.fixture\n                return 1\n            \"\"\"\n        )\n    )\n    foo_path.join(\"test_foo.py\").write(\"def test_foo(fix): assert fix == 1\")\n\n    bar_path = testdir.mkdir(\"bar\")\n    bar_path.join(\"conftest.py\").write(\n        textwrap.dedent(\n            \"\"\"\\\n            import pytest\n            @pytest.fixture\n                return 2\n            \"\"\"\n        )\n    )\n    bar_path.join(\"test_bar.py\").write(\"def test_bar(fix): assert fix == 2\")\n\n    res = testdir.runpytest()\n    assert res.ret == 0\n\n    res.stdout.fnmatch_lines(\n        [\n            \"*2 passed*\",\n        ]\n    )\n", "def test_collect_with_conftest_import_error(testdir):\n    \"\"\"Collection handles conftest.py files with import errors.\"\"\"\n    testdir.makepyfile(\n        **{\n            \"tests/conftest.py\": \"import nonexistentmodule\",\n            \"tests/test_foo.py\": \"def test_foo(): pass\",\n        }\n    )\n    result = testdir.runpytest(\"-v\", \"--import-mode=importlib\")\n    result.stdout.fnmatch_lines([\"*ImportError: No module named 'nonexistentmodule'*\"])\n    result.stdout.fnmatch_lines([\"*1 collected, 1 error*\"])\n", "def test_import_path_with_init(testdir):\n    \"\"\"Test that import_path can import modules within a package.\"\"\"\n    testdir.makepyfile(\n        **{\n            \"mypackage/__init__.py\": \"\",\n            \"mypackage/mymodule.py\": \"def myfunc(): return 42\",\n            \"tests/test_mypackage.py\": \"\"\"\n                from mypackage import mymodule\n                    assert mymodule.myfunc() == 42\n            \"\"\",\n        }\n    )\n    result = testdir.runpytest(\"tests\", \"--import-mode=importlib\")\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n", "def test_fscollector_from_parent_with_no_x(tmpdir, request):\n    \"\"\"Ensure File.from_parent raises TypeError if x is not provided.\"\"\"\n    with pytest.raises(TypeError):\n        MyCollector.from_parent(parent=request.session, fspath=tmpdir / \"foo\")\n"], "sample_888": ["def test_iforest_feature_importances_raises_error(global_random_seed):\n    \"\"\"Test that calling feature_importances_ raises an error.\"\"\"\n    X = iris.data\n    clf = IsolationForest(random_state=global_random_seed).fit(X)\n    with pytest.raises(AttributeError):\n        clf.feature_importances_\n", "def test_iforest_feature_importances(global_random_seed):\n    \"\"\"Test that feature importances are not implemented.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    model = IsolationForest(random_state=global_random_seed)\n    model.fit(X)\n    with pytest.raises(AttributeError):\n        model.feature_importances_\n", "def test_iforest_feature_names_in():\n    X = np.array([[1, 2], [3, 4]])\n    feature_names = [\"feature1\", \"feature2\"]\n    X = pd.DataFrame(X, columns=feature_names)\n\n    model = IsolationForest()\n    model.fit(X)\n\n    assert_array_equal(model.feature_names_in_, feature_names)\n", "def test_iforest_contamination_range():\n    \"\"\"Test that contamination is within the valid range.\"\"\"\n    X = iris.data\n\n    with pytest.raises(ValueError):\n        IsolationForest(contamination=-0.1).fit(X)\n\n    with pytest.raises(ValueError):\n        IsolationForest(contamination=0).fit(X)\n\n    with pytest.raises(ValueError):\n        IsolationForest(contamination=0.51).fit(X)\n\n    # Test valid cases\n    IsolationForest(contamination='auto').fit(X)\n    IsolationForest(contamination=0.1).fit(X)\n    IsolationForest(contamination=0.5).fit(X)\n", "def test_iforest_contamination_boundaries():\n    \"\"\"Test that IsolationForest raises error for invalid contamination values\"\"\"\n\n    with pytest.raises(ValueError, match=\"contamination must be in \\\\(0, 0.5\\\\]\"):\n        IsolationForest(contamination=0).fit(np.random.rand(10, 2))\n\n    with pytest.raises(ValueError, match=\"contamination must be in \\\\(0, 0.5\\\\]\"):\n        IsolationForest(contamination=0.51).fit(np.random.rand(10, 2))\n\n    # Test that it works with valid values\n    IsolationForest(contamination=0.1).fit(np.random.rand(10, 2))\n    IsolationForest(contamination='auto').fit(np.random.rand(10, 2))\n"], "sample_1148": ["def test_MatrixElement_properties():\n    A = MatrixSymbol('A', n, m)\n    assert A[0, 0].parent == A\n    assert A[0, 0].i == 0\n    assert A[0, 0].j == 0\n    assert A[0, 0].indices == (0, 0)\n", "def test_matrix_element_as_explicit():\n    A = MatrixSymbol('A', n, m)\n    assert A[0, 0].as_explicit() == A.as_explicit()[0, 0]\n", "def test_MatrixElement_diff_with_MatrixSymbol():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    M = A * B\n    ME = M[0, 0]\n    assert ME.diff(A[0, 0]) == B[0, 0]\n    assert ME.diff(B[0, 0]) == A[0, 0]\n", "def test_matrix_symbol_valid_index():\n    A = MatrixSymbol('A', n, m)\n    assert A.valid_index(0, 0) == True\n    assert A.valid_index(n-1, m-1) == True\n    assert A.valid_index(n, m-1) == False\n    assert A.valid_index(n-1, m) == False\n    assert A.valid_index(n+1, m-1) == False\n    assert A.valid_index(n-1, m+1) == False\n    assert A.valid_index(n-1, -1) == False\n    assert A.valid_index(-1, m-1) == False\n    assert A.valid_index(x, m-1) == True\n    assert A.valid_index(n-1, x) == True\n    assert A.valid_index(n, x) == False\n    assert A.valid_index(x, m) == False\n    assert A.valid_index(x, y) == True\n", "def test_matrix_symbol_creation_errors():\n    raises(ValueError, lambda: MatrixSymbol('A', 'n', 2))\n    raises(ValueError, lambda: MatrixSymbol('A', 2, 'm'))\n    raises(ValueError, lambda: MatrixSymbol('A', -1, -1))\n    raises(ValueError, lambda: MatrixSymbol('A', 2.0, 2.0))\n    raises(ValueError, lambda: MatrixSymbol('A', 2j, 2j))\n\n    n = symbols('n', real=False)\n    raises(ValueError, lambda: MatrixSymbol('A', n, n))\n    n = symbols('n', negative=True)\n    raises(ValueError, lambda: MatrixSymbol('A', n, n))\n\n    raises(ValueError, lambda: MatrixSymbol('A', 2, 2, 3))\n    raises(ValueError, lambda: MatrixSymbol('A'))\n"], "sample_802": ["def test_pipeline_with_disabled_cache():\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())], memory=None)\n    pipe.fit(X, y=None)\n    # Check that the transformer was not cached\n    assert not hasattr(pipe.named_steps['transf'], 'timestamp_')\n", "def test_pipeline_with_none_estimator():\n    # Test that a pipeline with None estimator works\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', None)])\n    pipe.fit(X, y=None)\n    assert_array_equal(pipe.transform(X), X)\n    assert_array_equal(pipe.fit_transform(X), X)\n    assert_array_equal(pipe.inverse_transform(X), X)\n    assert_raises(AttributeError, getattr, pipe, \"predict\")\n    assert_raises(AttributeError, getattr, pipe, \"score\")\n", "def test_pipeline_memory_disabled():\n    # Test that the pipeline behaves as expected when the memory is disabled\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    # Test with Transformer + SVC\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', transf), ('svc', clf)])\n    cached_pipe = Pipeline([('transf', transf), ('svc', clf)], memory=None)\n\n    # Fit both pipelines\n    pipe.fit(X, y)\n    cached_pipe.fit(X, y)\n\n    # Check that both pipelines yield identical results\n    assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n    assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n    assert_array_equal(pipe.predict_log_proba(X), cached_pipe.predict_log_proba(X))\n    assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n    assert_array_equal(pipe.named_steps['transf'].means_, cached_pipe.named_steps['transf'].means_)\n\n    # Check that the transformer was not cloned\n    assert pipe.named_steps['transf'] is transf\n", "def test_pipeline_with_no_cache():\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())], memory=None)\n    pipe.fit(X, y=None)\n    assert not hasattr(pipe.named_steps['transf'], 'timestamp_')\n", "def test_pipeline_with_disabled_cache():\n    # Test that the pipeline works correctly with disabled cache\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', clf)], memory=None)\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)], memory=cachedir)\n\n        # Fit both pipelines\n        pipe.fit(X, y)\n        cached_pipe.fit(X, y)\n\n        # Check that both pipelines yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X), cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_, cached_pipe.named_steps['transf'].means_)\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_1089": ["def test_issue_8263_with_zero():\n    F, G = symbols('F, G', commutative=False, cls=Function)\n    x, y = symbols('x, y')\n    expr, dummies, _ = _mask_nc(F(x)*G(y) - F(x)*G(y))\n    for v in dummies.values():\n        assert not v.is_commutative\n    assert expr.is_zero\n", "def test_issue_8888():\n    x, y = symbols('x, y', real=True)\n    eq = x + y + sqrt(x**2 + y**2)\n    assert factor_terms(eq) == (x + y)*(1 + sqrt(x**2/y**2 + 1))\n", "def test_issue_9477():\n    # Test for correct handling of symbols with both positive and negative signs\n    x = Dummy(positive=True, negative=True)\n    assert F(x) is None\n    assert F(-x) is None\n\n    # Test for correct handling of symbols with only positive sign\n    x = Dummy(positive=True)\n    assert F(x) is None\n    assert F(-x).is_negative\n\n    # Test for correct handling of symbols with only negative sign\n    x = Dummy(negative=True)\n    assert F(x).is_negative\n    assert F(-x) is None\n", "def test_issue_9000():\n    x = symbols('x')\n    assert _monotonic_sign(sin(x)) is None\n    assert _monotonic_sign(cos(x)) is None\n    assert _monotonic_sign(tan(x)) is None\n    assert _monotonic_sign(exp(x)) is None\n    assert _monotonic_sign(log(x)) is None\n\n    assert F(x**2 + 2*x + 1).is_positive\n    assert F(x**2 - 2*x + 1).is_positive\n    assert F(x**2 - 1).is_nonnegative\n\n    assert F(x**3).is_nonpositive\n    assert F(x**4).is_nonnegative\n\n    assert F(1/(x**2 - 1)).is_nonpositive\n    assert F(1/(x**2 + 1)).is_positive\n", "def test_issue_9124():\n    x, y = symbols('x y')\n    expr = x**2 * y**3 + x**3 * y**2\n    factored = factor_terms(expr)\n    expected = x**2 * y**2 * (x + y)\n    assert factored == expected\n"], "sample_647": ["def test_unformatted_warning_format(pytester: Pytester) -> None:\n    \"\"\"Test the formatting of unformatted warnings.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        from _pytest.warning_types import UnformattedWarning, PytestWarning\n\n            warning = UnformattedWarning(PytestWarning, \"value is {value}\")\n            formatted_warning = warning.format(value=42)\n            assert str(formatted_warning) == \"value is 42\"\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_unformatted_warning(pytester: Pytester) -> None:\n    \"\"\"Test the UnformattedWarning class.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import warning_types\n\n            warning = warning_types.UnformattedWarning(\n                category=pytest.PytestWarning,\n                template=\"Test warning with value: {value}\",\n            )\n            formatted_warning = warning.format(value=42)\n            assert str(formatted_warning) == \"Test warning with value: 42\"\n            assert isinstance(formatted_warning, pytest.PytestWarning)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_unformatted_warning_format() -> None:\n    \"\"\"Make sure UnformattedWarning.format() returns a warning with the correct message.\"\"\"\n    unformatted_warning = UnformattedWarning(PytestExperimentalApiWarning, \"Test warning: {value}\")\n    formatted_warning = unformatted_warning.format(value=\"example\")\n    assert str(formatted_warning) == \"Test warning: example\"\n", "def test_warn_explicit_for(pytester: Pytester) -> None:\n    \"\"\"Test the warn_explicit_for function to ensure it raises the warning correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import warning_types\n\n            pass\n\n        warning_types.warn_explicit_for(some_function, pytest.PytestWarning(\"some warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Werror\")\n    result.stdout.fnmatch_lines([\"pytest.PytestWarning: some warning*\"])\n", "def test_warning_messages(warning_class: UserWarning, message: str) -> None:\n    \"\"\"Make sure all warnings declared in _pytest.warning_types have the correct message\n    when formatted.\"\"\"\n    assert str(warning_class(message)) == message\n"], "sample_359": ["def test_references_field_by_through_field(self):\n    operation = FieldOperation('Model', 'field', models.ManyToManyField('Other', through_fields=('from', 'to')))\n    self.assertIs(operation.references_field('Model', 'from', 'migrations'), True)\n    self.assertIs(operation.references_field('Model', 'to', 'migrations'), True)\n    self.assertIs(operation.references_field('Other', 'from', 'migrations'), False)\n    self.assertIs(operation.references_field('Other', 'to', 'migrations'), False)\n    self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "    def test_references_field_by_name_alter(self):\n        operation = migrations.AlterField('Model', 'field', models.BooleanField(default=True))\n        self.assertIs(operation.references_field('model', 'field', 'migrations'), True)\n", "    def test_alter_order_with_respect_to(self):\n        operation = migrations.AlterOrderWithRespectTo(\"Model\", \"other_model\")\n        self.assertIs(operation.references_model('other_model', 'migrations'), True)\n        self.assertIs(operation.references_model('model', 'migrations'), False)\n        self.assertIs(operation.references_model('missing', 'migrations'), False)\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        'Model', 'field', models.ForeignKey(\n            'Other', models.CASCADE, limit_choices_to={'field': 'value'}\n        )\n    )\n    self.assertIs(operation.references_field('Other', 'field', 'migrations'), True)\n    self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n    self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "def test_references_field_by_through_attr(self):\n    operation = FieldOperation('Model', 'field', models.ManyToManyField('Other', through='Through'))\n    self.assertIs(operation.references_field('Through', 'model', 'migrations'), True)\n    self.assertIs(operation.references_field('Through', 'other', 'migrations'), True)\n    self.assertIs(operation.references_field('Through', 'whatever', 'migrations'), False)\n    self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n"], "sample_14": ["def test_latitude_lower_limit():\n    \"\"\"\n    Test that the validation of the Latitude lower limit works in both float32 and float64.\n    \"\"\"\n    value = -np.pi / 2 - 0.00001\n    with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n        Latitude(value, u.rad)\n\n    value = np.float32(-np.pi / 2 - 0.00001)\n    with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n        Latitude(value, u.rad, dtype=np.float32)\n", "def test_angle_from_quantity():\n    \"\"\"\n    Tests creating Angle objects from Quantity objects\n    \"\"\"\n    q = 3.60827466667 * u.hour\n    a = Angle(q)\n    assert isinstance(a, Angle)\n    assert a.unit == u.hourangle\n    assert_allclose(a.value, q.value)\n\n    q = 54.12412 * u.degree\n    a = Angle(q)\n    assert isinstance(a, Angle)\n    assert a.unit == u.degree\n    assert_allclose(a.value, q.value)\n\n    q = 0.944644098745 * u.radian\n    a = Angle(q)\n    assert isinstance(a, Angle)\n    assert a.unit == u.radian\n    assert_allclose(a.value, q.value)\n", "def test_angle_constructor_with_invalid_units():\n    \"\"\"\n    Tests that the constructor raises an error when given an invalid unit.\n    \"\"\"\n    with pytest.raises(u.UnitsError):\n        angle = Angle(54.12412, unit=u.meter)\n", "def test_angle_unicode():\n    \"\"\"\n    Test unicode string representation of Angle objects\n    \"\"\"\n\n    angle = Angle(\"54.12412\", unit=u.degree)\n\n    res = 'Angle as unicode: 54\u00b007\u203226.832\u2033'\n    assert f\"Angle as unicode: {angle.to_string(format='unicode')}\" == res\n\n    res = 'Angle as unicode: 3h36m29.7888s'\n    assert f\"Angle as unicode: {angle.to_string(unit=u.hour, format='unicode')}\" == res\n\n    angle = Angle(\"3 36 29.78880\", unit=u.degree)\n\n    res = 'Angle as unicode: 3\u00b036\u203229.7888\u2033'\n    assert f\"Angle as unicode: {angle.to_string(unit=u.degree, format='unicode')}\" == res\n\n    res = 'Angle as unicode: 3:36:29.7888'\n    assert f\"Angle as unicode: {angle.to_string(unit=u.degree, sep=':', format='unicode')}\" == res\n\n    res = 'Angle as unicode: 3-36-29.7888'\n    assert f\"Angle as unicode: {angle.to_string(unit=u.degree, sep='-', format='unicode')}\" == res\n\n    angle = Angle(\"-54.12412\", unit=u.degree)\n\n    res = 'Angle as unicode: \u221254\u00b007\u203226.832\u2033'\n    assert f\"Angle as unicode: {angle.to_string(format='unicode')}\" == res\n\n    angle = Angle(\"-1.23456789\", unit=u.degree)\n\n    res = 'Angle as unicode: \u22121\u00b014\u203204.4444\u2033'\n    assert f\"Angle as unicode: {angle.to_string(format='unicode')}\"", "def test_angle_from_other_units():\n    \"\"\"\n    Test creating angles from other units\n    \"\"\"\n    angle = Angle(5.0, unit=u.arcmin)\n    assert_allclose(angle.degree, 5.0 / 60.0)\n\n    angle = Angle(5.0, unit=u.arcsec)\n    assert_allclose(angle.degree, 5.0 / 3600.0)\n\n    # Test that an error is raised when creating an angle from a unit that is not an angle unit\n    with pytest.raises(u.UnitsError):\n        angle = Angle(5.0, unit=u.m)\n"], "sample_465": ["def test_inline_has_change_permission_uses_obj(self):\n    class ConcertInline(TabularInline):\n        model = Concert\n\n            return bool(obj)\n\n    class BandAdmin(ModelAdmin):\n        inlines = [ConcertInline]\n\n    ma = BandAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockChangeUser()\n    self.assertEqual(ma.get_inline_instances(request), [])\n    band = Band(name=\"The Doors\", bio=\"\", sign_date=date(1965, 1, 1))\n    inline_instances = ma.get_inline_instances(request, band)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(inline_instances[0], ConcertInline)\n", "def test_has_module_permission(self):\n    \"\"\"\n    has_module_permission returns True for users who have permission for the app.\n    \"\"\"\n    ma = ModelAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockViewUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockAddUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockChangeUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockDeleteUser()\n    self.assertTrue(ma.has_module_permission(request))\n", "def test_has_view_or_change_permission(self):\n    \"\"\"\n    has_view_or_change_permission returns True for users who can view or change objects\n    and False for users who can't.\n    \"\"\"\n    ma = ModelAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockViewUser()\n    self.assertIs(ma.has_view_or_change_permission(request), True)\n    request.user = self.MockAddUser()\n    self.assertFalse(ma.has_view_or_change_permission(request))\n    request.user = self.MockChangeUser()\n    self.assertTrue(ma.has_view_or_change_permission(request))\n    request.user = self.MockDeleteUser()\n    self.assertFalse(ma.has_view_or_change_permission(request))\n", "def test_inline_has_change_permission_uses_obj(self):\n    class ConcertInline(TabularInline):\n        model = Concert\n\n            return bool(obj)\n\n    class BandAdmin(ModelAdmin):\n        inlines = [ConcertInline]\n\n    ma = BandAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockChangeUser()\n    self.assertEqual(ma.get_inline_instances(request), [])\n    band = Band(name=\"The Doors\", bio=\"\", sign_date=date(1965, 1, 1))\n    inline_instances = ma.get_inline_instances(request, band)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(inline_instances[0], ConcertInline)\n", "def test_inline_has_change_permission_uses_obj(self):\n    class ConcertInline(TabularInline):\n        model = Concert\n\n            return bool(obj)\n\n    class BandAdmin(ModelAdmin):\n        inlines = [ConcertInline]\n\n    ma = BandAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockChangeUser()\n    self.assertEqual(ma.get_inline_instances(request), [])\n    band = Band(name=\"The Doors\", bio=\"\", sign_date=date(1965, 1, 1))\n    inline_instances = ma.get_inline_instances(request, band)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(inline_instances[0], ConcertInline)\n"], "sample_273": ["def test_default_auto_field_setting_in_app_config(self):\n    class Model(models.Model):\n        pass\n\n    self.apps.get_app_config('check_framework').default_auto_field = 'django.db.models.BigAutoField'\n    self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_auto_created_pk_with_override(self):\n        class Model(models.Model):\n            pass\n\n        with override_settings(DEFAULT_AUTO_FIELD='django.db.models.BigAutoField'):\n            self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_proxy_model_with_fields(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=20)\n\n        class ProxyModel(Model):\n            class Meta:\n                proxy = True\n            proxy_field = models.IntegerField()\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"Proxy model 'ProxyModel' contains model fields.\",\n                id='models.E017',\n            ),\n        ])\n", "    def test_index_column_name_too_long(self):\n        class Model(models.Model):\n            long_field_name = models.CharField(max_length=50)\n\n            class Meta:\n                indexes = [models.Index(fields=['long_field_name'])]\n\n        max_length = models.Index.max_name_length\n        Model._meta.local_fields[0].db_column = 'a' * (max_length + 1)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                f'Autogenerated column name too long for field \"{\"a\" * (max_length + 1)}\". '\n                f'Maximum length is \"{max_length}\" for database \"default\".',\n                hint=\"Set the column name manually using 'db_column'.\",\n                obj=Model,\n                id='models.E018',\n            ),\n        ])\n", "    def test_db_table_interpolation_with_app_label(self):\n        class Model(models.Model):\n            class Meta:\n                app_label = 'check_framework'\n                db_table = '%(app_label)s_model'\n\n        self.assertEqual(Model._meta.db_table, 'check_framework_model')\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n"], "sample_1050": ["def test_NumPyPrinter_MinMax():\n    p = NumPyPrinter()\n    expr = Min(x, y, z)\n    assert p.doprint(expr) == 'numpy.amin((x, y, z))'\n    expr = Max(x, y, z)\n    assert p.doprint(expr) == 'numpy.amax((x, y, z))'\n", "def test_issue_18577():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(oo) == \"float('inf')\"\n", "def test_custom_functions():\n    prntr = PythonCodePrinter({'user_functions': {'CustomFunc': 'custom_module.custom_func'}})\n    x = symbols('x')\n    expr = CustomFunc(x)\n    assert prntr.doprint(expr) == 'custom_module.custom_func(x)'\n", "def test_PythonCodePrinter_complex_infinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(zoo * 1j) == \"float('nan')\"\n", "def test_issue_18293():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(oo) == \"float('inf')\"\n"], "sample_793": ["def test_iforest_predict_proba():\n    \"\"\"Check predict_proba raises NotImplementedError\"\"\"\n    X = iris.data\n    clf = IsolationForest().fit(X)\n    with pytest.raises(NotImplementedError):\n        clf.predict_proba(X)\n", "def test_iforest_sample_weight():\n    # Test that sample_weight is correctly used in the fit method\n    X = iris.data\n    y = iris.target\n    sample_weight = np.ones(X.shape[0])\n    sample_weight[:50] = 2\n    clf = IsolationForest().fit(X, sample_weight=sample_weight)\n    assert_equal(clf.max_samples_, X.shape[0] * np.mean(sample_weight))\n", "def test_iforest_different_contamination():\n    # Test IsolationForest with different contamination parameters\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n\n    clf_auto = IsolationForest(behaviour=\"new\", random_state=rng, contamination=\"auto\").fit(X)\n    clf_0_25 = IsolationForest(behaviour=\"new\", random_state=rng, contamination=0.25).fit(X)\n\n    # Check that offset_ is not equal for different contamination parameters\n    assert clf_auto.offset_ != clf_0_25.offset_\n\n    # Check that decision_function returns different values for different contamination parameters\n    assert_array_almost_equal(clf_auto.decision_function(X), clf_0_25.decision_function(X), decimal=5) is False\n", "def test_predict_proba():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest(contamination=0.1).fit(X_train)\n    proba = clf.predict_proba([[2., 2.]])\n    assert proba.shape == (1, 2)\n    assert_array_equal(clf.predict([[2., 2.]]), np.argmax(proba, axis=1))\n", "def test_iforest_contamination_auto():\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9]])\n    y = np.array([1, 1, 1, 1, 1, -1, -1, -1, -1, -1])\n    clf = IsolationForest(contamination='auto', behaviour='new')\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert np.array_equal(y, y_pred)\n"], "sample_52": ["def test_modelchoicefield_to_python(self):\n    f = forms.ModelChoiceField(Category.objects.all())\n    self.assertEqual(f.to_python(self.c1.pk), self.c1)\n    self.assertEqual(f.to_python(str(self.c2.pk)), self.c2)\n    with self.assertRaises(ValidationError):\n        f.to_python(0)\n    with self.assertRaises(ValidationError):\n        f.to_python('invalid')\n", "def test_model_choice_field_with_limit_choices_to(self):\n    qs = Category.objects.filter(name__contains='test')\n    f = forms.ModelChoiceField(queryset=qs, limit_choices_to={'name__contains': 'test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name': 'A test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n    # Check that limit_choices_to is applied when choices are accessed.\n    Category.objects.create(name='Limited test', slug='limited-test', url='limited')\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n", "def test_queryset_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'slug__contains': 'test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_queryset_filtering(self):\n    # Test that the queryset is filtered correctly based on limit_choices_to\n    queryset = Author.objects.all()\n    f = forms.ModelChoiceField(queryset, limit_choices_to={'name__startswith': 'A'})\n    self.assertEqual(f.queryset.count(), 1)\n    self.assertEqual(f.queryset.get().name, 'A test')\n"], "sample_726": ["def test_label_binarize_empty_input():\n    y = []\n    classes = []\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([])\n    assert_raises(ValueError, label_binarize, y, classes, neg_label=neg_label, pos_label=pos_label)\n", "def test_label_binarize_multilabel_different_classes():\n    y = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [1, 2, 3]\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n\n    yield check_binarized_results, y, classes, pos_label, neg_label, expected\n", "def test_label_binarize_sequence_of_lists():\n    y = [[0, 1], [2], [0, 2]]\n    classes = [0, 1, 2]\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([[1, 1, 0], [0, 0, 1], [1, 0, 1]])\n\n    yield check_binarized_results, y, classes, pos_label, neg_label, expected\n", "def test_label_binarize_multilabel_mismatch():\n    y = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1]\n    pos_label = 2\n    neg_label = 0\n    assert_raises(ValueError, label_binarize, y, classes, neg_label=neg_label,\n                  pos_label=pos_label)\n", "def test_label_binarize_invalid_input():\n    y = [0, 1, 2, 3]\n    classes = [0, 1, 2]\n    # Missing class in the data\n    assert_raises(ValueError, label_binarize, y, classes, pos_label=2, neg_label=0)\n\n    y = [0, 1, 2]\n    classes = [0, 1, 2, 3]\n    # Extra class in classes\n    assert_raises(ValueError, label_binarize, y, classes, pos_label=2, neg_label=0)\n\n    y = [[0, 1], [1, 2]]\n    classes = [0, 1, 2]\n    # Multioutput data\n    assert_raises(ValueError, label_binarize, y, classes, pos_label=2, neg_label=0)\n"], "sample_1028": ["def test_issue_14392_complex():\n    assert (I*zoo).as_real_imag() == (nan, nan)\n", "def test_issue_14392_2():\n    assert (cos(zoo)**2).as_real_imag() == (0, nan)\n", "def test_Mul_does_not_distribute_zero():\n    a, b = symbols('a b')\n    assert ((a + b)*0).is_zero\n    assert ((a + 1)*0).is_zero\n    assert (0*(b - a)).is_zero\n    assert (0*(1 - b)).is_zero\n", "def test_Add_is_finite():\n    x, y = symbols('x y', finite=False)\n    assert (x + y).is_finite is False\n    x = symbols('x', finite=True)\n    assert (x + y).is_finite is None\n", "def test_Mod_does_not_distribute_over_Add():\n    a, b, c = symbols('a b c')\n    assert Mod(a + b, c) != Mod(a, c) + Mod(b, c)\n    assert Mod(a + b, c) != Mod(b, c) + Mod(a, c)\n"], "sample_441": ["    def test_unicode_username_normalization(self):\n        # The normalization happens in AbstractBaseUser.clean() and ModelForm\n        # validation calls Model.clean().\n        ohm_username = \"test\u2126\"  # U+2126 OHM SIGN\n        data = {\n            \"username\": ohm_username,\n            \"password1\": \"pwd2\",\n            \"password2\": \"pwd2\",\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertNotEqual(user.username, ohm_username)\n        self.assertEqual(user.username, \"test\u03a9\")  # U+03A9 GREEK CAPITAL LETTER OMEGA\n", "    def test_field_order(self):\n        # Regression test - check the order of fields:\n        user = User.objects.get(username=\"testclient\")\n        form = UserChangeForm(instance=user)\n        expected_fields = [\n            \"username\",\n            \"password\",\n            \"first_name\",\n            \"last_name\",\n            \"email\",\n            \"is_active\",\n            \"is_staff\",\n            \"is_superuser\",\n            \"last_login\",\n            \"date_joined\",\n            \"groups\",\n            \"user_permissions\",\n        ]\n        self.assertEqual(list(form.fields), expected_fields)\n", "    def test_invalid_email_format(self):\n        data = {\n            \"username\": \"testclient\",\n            \"password1\": \"test123\",\n            \"password2\": \"test123\",\n            \"email\": \"invalid_email_format\",\n        }\n        form = UserCreationForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\"email\", form.errors)\n        self.assertEqual(form.errors[\"email\"], [\"Enter a valid email address.\"])\n", "def test_no_password_fields(self):\n    user = User.objects.get(username=\"testclient\")\n    data = {}\n    form = AdminPasswordChangeForm(user, data)\n    required_error = [Field.default_error_messages[\"required\"]]\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors[\"password1\"], required_error)\n    self.assertEqual(form.errors[\"password2\"], required_error)\n    self.assertEqual(form.changed_data, [])\n", "    def test_integer_username(self):\n        user = IntegerUsernameUser.objects.create_user(username=123, password='old_password')\n        data = {'old_password': 'old_password', 'new_password1': 'new_password', 'new_password2': 'new_password'}\n        form = PasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertTrue(user.check_password('new_password'))\n"], "sample_521": ["def test_pathpatch_3d_normal():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    path = Path.unit_rectangle()\n    patch = PathPatch(path)\n    art3d.pathpatch_2d_to_3d(patch, z=(0, 0.5, 0.7, 1, 0), zdir='y', normal=(0, 1, 0))\n    ax.add_artist(patch)\n", "def test_scatter_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    xs = np.arange(10)\n    ys = np.random.rand(10)\n    zs = np.random.rand(10)\n    sc = ax.scatter(xs, ys, zs)\n    art3d.scatter_2d_to_3d(sc, zs=0.5, zdir='x')\n", "def test_scatter_spiral_depthshade():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    th = np.linspace(0, 2 * np.pi * 6, 256)\n    sc = ax.scatter(np.sin(th), np.cos(th), th, s=(1 + th * 5), c=th ** 2, depthshade=True)\n\n    # force at least 1 draw!\n    fig.canvas.draw()\n", "def test_quiver3D_scale_units():\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    x = np.linspace(0, 1, 5)\n    y = np.linspace(0, 1, 5)\n    z = np.linspace(0, 1, 5)\n    u = np.ones_like(x)\n    v = np.ones_like(y)\n    w = np.ones_like(z)\n\n    ax.quiver(x, y, z, u, v, w, scale_units='xyz')\n    fig.canvas.draw()\n\n    # Check that all arrows have the same length\n    arrow_lengths = np.sqrt(u**2 + v**2 + w**2)\n    assert np.all(arrow_lengths == arrow_lengths[0])\n", "def test_pathpatch_3d_custom_colors():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    path = Path.unit_rectangle()\n    patch = PathPatch(path, facecolor='none', edgecolor='r')\n    art3d.pathpatch_2d_to_3d(patch, z=(0, 0.5, 0.7, 1, 0), zdir='y')\n\n    # Test custom facecolor and edgecolor\n    patch.set_facecolor('green')\n    patch.set_edgecolor('blue')\n    ax.add_artist(patch)\n"], "sample_490": ["def test_validate_expression_custom_error(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_uniq\",\n        violation_error_code=\"custom_code\",\n        violation_error_message=\"Custom message\",\n    )\n    msg = \"Custom message\"\n    with self.assertRaisesMessage(ValidationError, msg) as cm:\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    self.assertEqual(cm.exception.code, \"custom_code\")\n", "def test_contains_expressions(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    self.assertTrue(constraint.contains_expressions)\n\n    constraint = models.UniqueConstraint(fields=[\"name\"], name=\"name_uniq\")\n    self.assertFalse(constraint.contains_expressions)\n", "def test_validate_custom_error_message(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"],\n        name=\"unique_name\",\n        violation_error_message=\"Custom unique error message\",\n    )\n    product = UniqueConstraintProduct(name=self.p1.name)\n    msg = \"Custom unique error message\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, product)\n", "def test_validate_nulls_distinct(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"], name=\"name_nulls_distinct_uniq\", nulls_distinct=True\n    )\n    UniqueConstraintProduct.objects.create(name=None)\n    # Null values are considered different.\n    constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=None))\n\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"], name=\"name_nulls_not_distinct_uniq\", nulls_distinct=False\n    )\n    UniqueConstraintProduct.objects.create(name=None)\n    msg = \"Constraint \u201cname_nulls_not_distinct_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=None))\n", "def test_validate_expression_f_obj(self):\n    constraint = models.UniqueConstraint(\n        Lower(models.F(\"name\")), name=\"name_lower_uniq_f_obj\"\n    )\n    msg = \"Constraint \u201cname_lower_uniq_f_obj\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper()),\n        exclude={\"name\"},\n    )\n"], "sample_141": ["def test_helpful_error_message_for_invalid_boolean_field(self):\n    \"\"\"\n    Invalid boolean field values should throw a helpful error message.\n    \"\"\"\n    test_string = \"\"\"[{\n        \"pk\": 1,\n        \"model\": \"serializers.booleanmodel\",\n        \"fields\": {\n            \"bool_field\": \"invalidboolean\"\n        }\n    }]\"\"\"\n    expected = \"(serializers.booleanmodel:pk=1) field_value was 'invalidboolean'\"\n    with self.assertRaisesMessage(DeserializationError, expected):\n        list(serializers.deserialize('json', test_string))\n", "def test_helpful_error_message_for_many2many_invalid_model(self):\n    \"\"\"\n    Invalid many-to-many model should throw a helpful error message.\n    \"\"\"\n    test_string = \"\"\"[{\n        \"pk\": 1,\n        \"model\": \"serializers.article\",\n        \"fields\": {\n            \"author\": 1,\n            \"headline\": \"Unknown many to many model\",\n            \"pub_date\": \"2014-09-15T10:35:00\",\n            \"categories\": [1, 2],\n            \"invalid_model\": [1, \"doesnotexist\"]\n        }\n    }, {\n        \"pk\": 1,\n        \"model\": \"serializers.author\",\n        \"fields\": {\n            \"name\": \"Agnes\"\n        }\n    }, {\n        \"pk\": 1,\n        \"model\": \"serializers.category\",\n        \"fields\": {\n            \"name\": \"Reference\"\n        }\n    }]\"\"\"\n    expected = \"Could not find field 'invalid_model' on serializer 'article'\"\n    with self.assertRaisesMessage(DeserializationError, expected):\n        list(serializers.deserialize('json', test_string))\n", "def test_custom_encoder_with_lazy_string(self):\n    class ScoreLazyString(models.Model):\n        score = models.CharField(max_length=100)\n\n    class CustomJSONEncoder(json.JSONEncoder):\n            if isinstance(o, str) and hasattr(o, '_proxy____args'):\n                return str(o)\n            return super().default(o)\n\n    s = serializers.json.Serializer()\n    json_data = s.serialize(\n        [ScoreLazyString(score=gettext_lazy(\"Test Score\"))], cls=CustomJSONEncoder\n    )\n    self.assertIn('\"fields\": {\"score\": \"Test Score\"}', json_data)\n", "def test_natural_primary_keys(self):\n    \"\"\"\n    Test that the serializer can handle models with natural primary keys.\n    \"\"\"\n    author = Author.objects.create(name='John Doe')\n    category = Category.objects.create(name='Technology')\n    article = Article.objects.create(headline='Test Article', pub_date=datetime.datetime.now(), author=author)\n    article.categories.add(category)\n\n    s = serializers.json.Serializer()\n    json_data = s.serialize([article], use_natural_primary_keys=True)\n    data = json.loads(json_data)\n\n    self.assertEqual(data[0]['model'], 'serializers.article')\n    self.assertEqual(data[0]['fields']['headline'], 'Test Article')\n    self.assertEqual(data[0]['fields']['author'], ['John Doe'])\n    self.assertEqual(data[0]['fields']['categories'], [['Technology']])\n", "def test_deserialize_fk_value(self):\n    \"\"\"\n    Test deserialize_fk_value function with a ForeignKey field\n    \"\"\"\n    field = self.author\n    field_value = 1\n    using = 'default'\n    handle_forward_references = True\n    result = deserialize_fk_value(field, field_value, using, handle_forward_references)\n    self.assertEqual(result, 1)\n"], "sample_626": ["def test_safe_cast_to_index_mixed_types():\n    mixed_types = pd.to_datetime([\"2000-01-01\", \"2001-01-01\", np.datetime64(\"2002-01-01\")])\n    expected = pd.Index(mixed_types)\n    actual = safe_cast_to_index(mixed_types)\n    assert_array_equal(expected, actual)\n    assert isinstance(actual, pd.Index)\n", "def test_pandas_index_getitem(self):\n    pd_idx = pd.Index([1, 2, 3, 4])\n    expected = PandasIndex(pd_idx[[0, 2]], \"x\", coord_dtype=np.int32)\n    actual = PandasIndex(pd_idx, \"x\", coord_dtype=np.int32)[[0, 2]]\n\n    assert actual.index.equals(expected.index)\n    assert actual.dim == expected.dim\n    assert actual.coord_dtype == expected.coord_dtype\n", "def test_safe_cast_to_index_datetime64_with_tz():\n    dates = pd.date_range(\"2000-01-01\", periods=10, tz=\"US/Eastern\")\n    expected = pd.Index(dates.tz_convert(None))\n    actual = safe_cast_to_index(dates)\n    assert_array_equal(expected, actual)\n    assert expected.dtype == actual.dtype\n", "def test_safe_cast_to_index_non_numpy_array():\n    # Test that non-numpy arrays are converted to numpy arrays before casting to index\n    class NonNumpyArray:\n            self.data = data\n\n            return np.array(self.data)\n\n    dates = pd.date_range(\"2000-01-01\", periods=10)\n    non_numpy_dates = NonNumpyArray(dates)\n\n    expected = pd.Index(dates)\n    actual = safe_cast_to_index(non_numpy_dates)\n    assert_array_equal(expected, actual)\n    assert expected.dtype == actual.dtype\n", "def test_safe_cast_to_index_preserve_multiindex():\n    arr = np.array([[1, 2], [3, 4]])\n    mi = pd.MultiIndex.from_arrays(arr, names=('a', 'b'))\n    actual = safe_cast_to_index(mi)\n    assert_array_equal(mi, actual)\n    assert isinstance(actual, pd.MultiIndex)\n"], "sample_204": ["def test_load_no_migration_class(self):\n    \"\"\"\n    If a migration is missing a Migration class, BadMigrationError is raised.\n    \"\"\"\n    with self.assertRaises(BadMigrationError):\n        MigrationLoader(connection)\n", "def test_load_unapplied_dependency(self):\n    \"\"\"\n    Makes sure the loader raises InconsistentMigrationHistory if a migration has an unapplied dependency.\n    \"\"\"\n    migration_loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    msg = (\n        \"Migration migrations.0002_second is applied before its dependency \"\n        \"migrations.0003_third on database 'default'.\"\n    )\n    with self.assertRaisesMessage(InconsistentMigrationHistory, msg):\n        migration_loader.check_consistent_history(connection)\n", "def test_atomic_migrations(self):\n    \"Tests that atomic migrations are handled correctly\"\n    migration_loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n    plan = migration_loader.graph.forwards_plan((\"migrations\", \"0002_atomic\"))\n    self.assertTrue(plan[1][1].atomic)\n    plan = migration_loader.graph.forwards_plan((\"migrations\", \"0003_non_atomic\"))\n    self.assertFalse(plan[2][1].atomic)\n", "def test_loading_with_replace_migrations_disabled(self):\n    \"\"\"\n    Tests loading migrations with replace_migrations set to False.\n    \"\"\"\n    loader = MigrationLoader(connection, replace_migrations=False)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Loading with nothing applied should include all nodes\n    self.assertEqual(\n        len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n        3,\n    )\n\n    # Apply the replacing migration and check that it's still included\n    recorder.record_applied(\"migrations\", \"0003_replaces_0001_0002\")\n    loader.build_graph()\n    self.assertEqual(\n        len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n        3,\n    )\n", "def test_run_before_unapplied(self):\n    \"\"\"\n    Makes sure the loader doesn't apply migrations before their run_before dependencies if they're not applied.\n    \"\"\"\n    # Load and test the plan\n    migration_loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Only 0001_initial and 0003_third should be applied, 0002_second should not be in the plan\n    plan = migration_loader.graph.forwards_plan((\"migrations\", \"0003_third\"))\n    self.assertEqual(\n        plan,\n        [\n            (\"migrations\", \"0001_initial\"),\n            (\"migrations\", \"0003_third\"),\n        ],\n    )\n\n    # Apply 0001_initial and check that 0002_second is now in the plan\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    migration_loader.build_graph()\n    plan = migration_loader.graph.forwards_plan((\"migrations\", \"0003_third\"))\n    self.assertEqual(\n        plan,\n        [\n            (\"migrations\", \"0002_second\"),\n            (\"migrations\", \"0003_third\"),\n        ],\n    )\n"], "sample_984": ["def test_HadamardProduct():\n    from sympy import MatrixSymbol\n    assert str(MatrixSymbol(\"X\", 2, 2) .* MatrixSymbol(\"Y\", 2, 2)) == \"X.*Y\"\n", "def test_MatPow():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert str(A**2) == \"A**2\"\n", "def test_MatrixSymbol_str():\n    X = MatrixSymbol('X', 2, 2)\n    assert str(X) == \"X\"\n", "def test_HadamardProduct():\n    from sympy import HadamardProduct, MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    H = HadamardProduct(A, B)\n    assert str(H) == 'A.*B'\n", "def test_AlgebraicNumber():\n    from sympy import AlgebraicNumber\n    a = AlgebraicNumber(x**2 - 2, x**2 - 2)\n    assert str(a) == \"x\"\n    b = AlgebraicNumber(x**2 - 3, x**2 - 3)\n    assert str(b) == \"AlgebraicNumber(x**2 - 3, x**2 - 3)\"\n"], "sample_422": ["def test_foreignkey_forward(self):\n    authors = Author.objects.order_by(\"name\")\n    with self.assertNumQueries(3):\n        books = list(\n            Book.objects.prefetch_related(\n                Prefetch(\"first_book__authors\", authors),\n                Prefetch(\"first_book__authors\", authors[1:], to_attr=\"authors_sliced\"),\n            )\n        )\n    for book in books:\n        with self.subTest(book=book):\n            self.assertEqual(book.authors_sliced, list(book.first_book.authors.all())[1:])\n", "def test_custom_queryset_limit(self):\n    bookmark = Bookmark.objects.create(url=\"http://www.djangoproject.com/\")\n    django_tag = TaggedItem.objects.create(content_object=bookmark, tag=\"django\")\n    TaggedItem.objects.create(content_object=bookmark, tag=\"python\")\n\n    with self.assertNumQueries(2):\n        bookmark = Bookmark.objects.prefetch_related(\n            Prefetch(\"tags\", TaggedItem.objects.filter(tag=\"django\")[:1]),\n        ).get()\n\n    with self.assertNumQueries(0):\n        self.assertEqual(list(bookmark.tags.all()), [django_tag])\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title=\"Poems\")\n        cls.book2 = Book.objects.create(title=\"Jane Eyre\")\n        cls.author1 = Author.objects.create(name=\"Charlotte\", first_book=cls.book1)\n        cls.author2 = Author.objects.create(name=\"Anne\", first_book=cls.book1)\n        cls.author3 = Author.objects.create(name=\"Emily\", first_book=cls.book2)\n        cls.book1.authors.add(cls.author1, cls.author2)\n        cls.book2.authors.add(cls.author3)\n", "def test_m2m_reverse_with_custom_queryset(self):\n    books = Book.objects.filter(title__startswith=\"P\").order_by(\"title\")\n    with self.assertNumQueries(3):\n        authors = list(\n            Author.objects.prefetch_related(\n                Prefetch(\"books\", books),\n                Prefetch(\"books\", books[1:], to_attr=\"books_sliced\"),\n            )\n        )\n    for author in authors:\n        with self.subTest(author=author):\n            self.assertEqual(author.books_sliced, list(author.books.all())[1:])\n", "def test_reverse_one_to_one(self):\n    authorwithage = AuthorWithAge.objects.get(pk=self.author1.pk)\n    with self.assertNumQueries(1):\n        self.assertEqual(authorwithage.author.name, self.author1.name)\n"], "sample_1100": ["def test_Mul_does_not_distribute_zero():\n    a, b = symbols('a b')\n    assert ((1 + I)*0).is_Mul\n    assert ((a + b)*0).is_Mul\n    assert ((a + 1)*0).is_Mul\n    assert ((1 + I)*0).is_finite is True\n    assert ((1 + I)*0).is_zero\n    assert ((a + b)*0).is_zero\n    assert ((a + 1)*0).is_zero\n    assert ((1 + I)*0).expand() is 0\n    assert ((a + b)*0).expand() is 0\n    assert ((a + 1)*0).expand() is 0\n", "def test_Mul_is_complex():\n    x = Symbol('x', real=True)\n    y = Symbol('y', complex=True)\n\n    assert (x*y).is_complex is True\n    assert (x*y).is_real is False\n\n    assert (x*I).is_complex is True\n    assert (x*I).is_real is False\n\n    assert (I*x).is_complex is True\n    assert (I*x).is_real is False\n\n    assert (I*y).is_complex is True\n    assert (I*y).is_real is False\n\n    assert (I*I).is_complex is False\n    assert (I*I).is_real is True\n", "def test_divmod_with_constants():\n    assert divmod(5, 3) == (1, 2)\n    assert divmod(10, 3) == (3, 1)\n    assert divmod(5.0, 3) == (1.0, 2.0)\n    assert divmod(5, 3.0) == (1.0, 2.0)\n    assert divmod(5.0, 3.0) == (1.0, 2.0)\n    assert divmod(5, -3) == (-2, -1)\n    assert divmod(-5, 3) == (-2, 1)\n    assert divmod(-5, -3) == (2, -1)\n    with raises(TypeError):\n        divmod(x, 3.5)\n    with raises(TypeError):\n        divmod(5, y)\n", "def test_divmod_sympy_types():\n    assert divmod(Integer(10), Integer(3)) == (Integer(3), Integer(1))\n    assert divmod(Rational(10, 3), Integer(2)) == (Rational(1, 2), Rational(1, 3))\n    assert divmod(Integer(10), Rational(3, 2)) == (Integer(3), Rational(1, 2))\n    assert divmod(Rational(10, 3), Rational(2, 3)) == (Integer(1), Rational(1, 3))\n", "def test_issue_15715():\n    # Test for correct substitution with Pow and Mul\n    x, y, z = symbols('x y z')\n    expr = (x + y)**z\n    subs_dict = {x: 2, y: 3}\n    assert expr.subs(subs_dict) == (2 + 3)**z\n\n    expr = x*y*z\n    subs_dict = {x: 2, y: 3}\n    assert expr.subs(subs_dict) == 2*3*z\n"], "sample_226": ["    def test_set_as_test_mirror(self):\n        # set_as_test_mirror() sets the test database name to the primary database name.\n        primary_settings_dict = {\n            'NAME': 'primary_database',\n        }\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        creation.set_as_test_mirror(primary_settings_dict)\n        self.assertEqual(test_connection.settings_dict['NAME'], 'primary_database')\n", "    def test_keepdb_setting_true(self, mocked_sync_apps, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['KEEPDB'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            # _create_test_db is called with keepdb=True\n            mocked_create_test_db.assert_called_with(0, True, True)\n            # App is synced.\n            mocked_sync_apps.assert_called()\n            mocked_args, _ = mocked_sync_apps.call_args\n            self.assertEqual(mocked_args[1], {'app_unmigrated'})\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db') as mocked_destroy_test_db:\n                creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n            # _destroy_test_db is not called when keepdb is True\n", "    def test_multiple_apps(self):\n        # serialize_db_to_string() serializes only migrated apps.\n        # Create objects in an unmigrated app.\n        CircularA.objects.create()\n        CircularB.objects.create()\n        # Create objects in the migrated backends app.\n        Object.objects.create()\n        ObjectReference.objects.create()\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            loader_instance = loader.return_value\n            # Mark only the backends app as migrated.\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        # The unmigrated app objects are not serialized.\n        self.assertNotIn('backends.circulara', data)\n        self.assertNotIn('backends.circularb', data)\n        # The migrated app objects are serialized.\n        self.assertIn('backends.object', data)\n        self.assertIn('backends.objectreference', data)\n", "    def test_serialize_db_to_string(self, mocked_has_table, *mocked_objects):\n        # serialize_db_to_string() serializes all data in the database into a JSON string.\n        Object.objects.create(obj_ref=None)\n        data = connection.creation.serialize_db_to_string()\n        expected_data = '[{\"model\": \"backends.object\", \"pk\": 1, \"fields\": {\"obj_ref\": null, \"related_objects\": []}}]'\n        self.assertEqual(data, expected_data)\n", "    def test_create_test_db_keepdb(self):\n        # create_test_db() with keepdb=True reuses existing database\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        with mock.patch.object(creation, '_create_test_db') as mock_create_test_db:\n            test_database_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n        mock_create_test_db.assert_called_once_with(0, True, True)\n        self.assertEqual(test_database_name, creation._get_test_db_name())\n        self.assertEqual(test_connection.settings_dict['NAME'], test_database_name)\n        self.assertEqual(settings.DATABASES[test_connection.alias]['NAME'], test_database_name)\n"], "sample_727": ["def test_imputation_invalid_strategy():\n    # Test imputation with invalid strategy\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    imputer = Imputer(missing_values=np.nan, strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_different_dtypes():\n    # Test imputation with different data types\n    X = np.array([[1, 2, np.nan],\n                  [4, np.nan, 6],\n                  [7, 8, 9]], dtype=float)\n\n    X_true = np.array([[1, 2, 6],\n                       [4, 6, 6],\n                       [7, 8, 9]], dtype=float)\n\n    _check_statistics(X, X_true, \"mean\", [1, 6, 9], np.nan)\n\n    X = np.array([[1, 2, np.nan],\n                  [4, np.nan, 6],\n                  [7, 8, 9]], dtype=int)\n\n    X_true = np.array([[1, 2, 6],\n                       [4, 6, 6],\n                       [7, 8, 9]], dtype=int)\n\n    _check_statistics(X, X_true, \"mean\", [1, 6, 9], np.nan)\n", "def test_imputation_with_invalid_axis():\n    # Test imputation with an invalid axis value\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    imputer = Imputer(missing_values=0, strategy=\"mean\", axis=2)\n    with assert_raises(ValueError):\n        imputer.fit(X)\n", "def test_imputation_with_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [1, np.nan, 3],\n        [4, 5, np.nan],\n        [7, 8, 9],\n    ])\n    imputer = Imputer(missing_values=np.nan, strategy=\"invalid_strategy\")\n    with assert_raises(ValueError):\n        imputer.fit(X)\n", "def test_imputation_invalid_input():\n    # Test imputation with invalid input\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    # Invalid strategy\n    imputer = Imputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Invalid axis\n    imputer = Imputer(axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Invalid shape during transform\n    imputer = Imputer()\n    imputer.fit(X)\n    X_invalid_shape = np.zeros((X.shape[0], X.shape[1] + 1))\n    assert_raises(ValueError, imputer.transform, X_invalid_shape)\n"], "sample_855": ["def test_dummy_classifier_on_empty_target():\n    X = [[0]] * 3\n    y = []\n    cls = DummyClassifier()\n    with pytest.raises(ValueError, match=\"y must not be empty.\"):\n        cls.fit(X, y)\n", "def test_dummy_classifier_sparse_output_shape():\n    X = [[0]] * 5\n    y = sp.csr_matrix(np.array([[1, 0],\n                                [1, 3],\n                                [4, 0],\n                                [0, 1],\n                                [1, 0]]))\n\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n\n    assert sp.issparse(y_pred)\n    assert y_pred.shape == y.shape\n", "def test_dummy_regressor_sparse_target():\n    X = [[0]] * 5\n    y = sp.csc_matrix(np.array([[2.0], [1.5], [2.0], [1.0], [1.0]]))\n\n    n_samples = len(X)\n    y_expected = np.full((n_samples, 1), np.mean(y.toarray()))\n\n    reg = DummyRegressor(strategy=\"mean\")\n    reg.fit(X, y)\n\n    y_pred = reg.predict(X)\n    assert sp.issparse(y_pred)\n    assert_array_equal(y_pred.toarray(), y_expected)\n", "def test_constant_strategy_sparse_target_exception():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[1, 0],\n                                [1, 3],\n                                [4, 0],\n                                [0, 1],\n                                [1, 0]]))\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=5)\n\n    err_msg = \"constant=5.*Possible values.*\\\\[0, 1, 3, 4]\"\n    with pytest.raises(ValueError, match=err_msg):\n        clf.fit(X, y)\n", "def test_constant_strategy_multioutput_sparse_target():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[2, 1],\n                                [2, 2],\n                                [1, 1],\n                                [1, 1],\n                                [1, 1]]))\n\n    n_samples = len(X)\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=[1, 0])\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert sp.issparse(y_pred)\n    assert_array_equal(y_pred.toarray(), np.hstack([np.ones((n_samples, 1)),\n                                                    np.zeros((n_samples, 1))]))\n"], "sample_953": ["def test_quickstart_epub(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n        'Do you want to use the epub builder': 'yes',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n    assert 'epub' in ns['extensions']\n", "def test_valid_dir_existing_reserved_names(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n        'Name of your master document': 'index',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    # Manually create a reserved name to test the function\n    (tempdir / 'conf.py').write_text('This is a test')\n\n    assert not qs.valid_dir(d)\n", "def test_quickstart_with_empty_input(tempdir):\n    answers = {\n        'Root path': '',\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    with pytest.raises(qs.ValidationError):\n        qs.ask_user(d)\n", "def test_quickstart_existing_masterfile(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n\n    # Create an existing masterfile\n    masterfile = tempdir / f'{d[\"master\"]}{d[\"suffix\"]}'\n    masterfile.write_text(\"Existing content\")\n\n    # Mock user input for existing masterfile\n    new_answers = {\n        'Please enter a new file name, or rename the existing file and press Enter': 'new_master',\n    }\n    qs.term_input = mock_input(new_answers)\n\n    # Generate files again\n    qs.generate(d)\n\n    # Check that the new masterfile was created and the existing one was not modified\n    new_masterfile = tempdir / f'new_master{d[\"suffix\"]}'\n    assert new_masterfile.isfile()\n    assert masterfile.read_text() == \"Existing content\"\n", "def test_quickstart_custom_template(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Custom Template Test',\n        'Author name': 'Test Author',\n        'Project version': '1.0',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    custom_template_dir = tempdir / 'custom_templates'\n    custom_template_dir.mkdir()\n    custom_template_content = 'This is a custom template for root_doc.'\n    with open(custom_template_dir / 'root_doc.rst_t', 'w') as f:\n        f.write(custom_template_content)\n    qs.generate(d, templatedir=str(custom_template_dir))\n\n    rootdocfile = tempdir / 'index.rst'\n    assert rootdocfile.isfile()\n    assert rootdocfile.read_text() == custom_template_content\n"], "sample_1062": ["def test_TR15_16_17():\n    assert TR15(1 - 1/sin(x)**2) == -cot(x)**2\n    assert TR16(1 - 1/cos(x)**2) == -tan(x)**2\n    assert TR111(1 - 1/tan(x)**2) == 1 - cot(x)**2\n", "def test_TR17():\n    assert TR17(1 - 1/sin(x)**2) == 1 - cot(x)**2\n    assert TR17(1 - 1/cos(x)**2) == 1 - tan(x)**2\n", "def test_TR15():\n    assert TR15(1/sin(x)**2) == cot(x)**2\n    assert TR15(1/sin(x)**4) == (cot(x)**2)**2\n    assert TR15(1/sin(x)**6) == (cot(x)**2)**3\n    assert TR15(1/sin(x)**8) == (cot(x)**2)**4\n    assert TR15(1/sin(x)**10) == 1/sin(x)**10\n", "def test_TR15_16_111():\n    assert TR15(1 + 1/sin(x)**2) == sec(x)**2\n    assert TR16(1 + 1/cos(x)**2) == csc(x)**2\n    assert TR111(1 + tan(x)**2) == sec(x)**2\n", "def test_TR14_complex_expression():\n    eq = (cos(x) - 1)**2 * (sin(y) + 1)**3 * (cos(z) + 1)**4\n    ans = -sin(x)**2 * (sin(y) + 1)**3 * (cos(z) + 1)**4\n    assert TR14(eq) == ans\n"], "sample_300": ["def test_transform_with_lookup(self):\n    query = Query(Author, alias_cols=False)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower__exact='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "def test_annotate_expression(self):\n    query = Query(Author)\n    query.add_annotation(F('num') + 1, 'num_plus_one', False)\n    annotation = query.annotations['num_plus_one']\n    self.assertIsInstance(annotation, Add)\n    self.assertEqual(annotation.children[0].target, Author._meta.get_field('num'))\n    self.assertEqual(annotation.children[1], 1)\n", "def test_annotation_in_filter(self):\n    query = Query(Item)\n    query.add_annotation(Count('note'), 'note_count')\n    where = query.build_where(Q(note_count__gt=2))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertIsInstance(lookup.lhs, Ref)\n    self.assertEqual(lookup.lhs.refs, 'note_count')\n    self.assertEqual(lookup.rhs, 2)\n", "def test_transform_with_lookup(self):\n    query = Query(Author, alias_cols=False)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower__icontains='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Contains)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "def test_transform_with_lookup(self):\n    query = Query(Author, alias_cols=False)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower__contains='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Contains)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n"], "sample_1045": ["def test_Float_floor_ceiling():\n    a = Float('1.5')\n    b = Float('-1.5')\n\n    assert(a.floor() == Float('1.0'))\n    assert(a.ceiling() == Float('2.0'))\n    assert(b.floor() == Float('-2.0'))\n    assert(b.ceiling() == Float('-1.0'))\n", "def test_Rational_floor_ceiling():\n    a = Rational(1, 2)\n    b = Rational(3, 2)\n\n    assert(a.floor() == 0)\n    assert(a.ceiling() == 1)\n    assert(b.floor() == 1)\n    assert(b.ceiling() == 2)\n", "def test_Float_floor_ceiling():\n    a = Float('2.3')\n    b = Float('-2.3')\n\n    assert(a.floor() == 2.0)\n    assert(a.ceiling() == 3.0)\n    assert(b.floor() == -3.0)\n    assert(b.ceiling() == -2.0)\n\n    assert((Float('nan')).floor() == nan)\n    assert((Float('nan')).ceiling() == nan)\n", "def test_Rational_floor_ceiling():\n    assert(Rational(7, 2).floor() == 3)\n    assert(Rational(7, 2).ceiling() == 4)\n    assert(Rational(-7, 2).floor() == -4)\n    assert(Rational(-7, 2).ceiling() == -3)\n", "def test_Integer_trunc():\n    a = Integer(4)\n    assert a.trunc() == a\n\n    b = Integer(-4)\n    assert b.trunc() == b\n\n    c = Rational(4, 3)\n    assert c.trunc() == Integer(1)\n\n    d = Rational(-4, 3)\n    assert d.trunc() == Integer(-1)\n\n    e = Float('4.3')\n    assert e.trunc() == Integer(4)\n\n    f = Float('-4.3')\n    assert f.trunc() == Integer(-4)\n\n    g = zoo\n    assert g.trunc() == g\n"], "sample_1071": ["def test_convert_to_incompatible_units():\n    assert convert_to(speed_of_light, joule) == speed_of_light\n    assert convert_to(joule, speed_of_light) == joule\n", "def test_check_dimensions():\n    from sympy import symbols\n\n    x, y = symbols('x y')\n\n    assert check_dimensions(x*meter + y*second) == x*meter + y*second\n    raises(ValueError, lambda: check_dimensions(x*meter + y))\n    raises(ValueError, lambda: check_dimensions(x*meter + y*newton))\n\n    assert check_dimensions(meter + kilometer) == meter + kilometer\n    raises(ValueError, lambda: check_dimensions(meter + second))\n    raises(ValueError, lambda: check_dimensions(meter + newton))\n", "def test_convert_to_incompatible_units():\n    assert convert_to(speed_of_light, [meter, kelvin]) == speed_of_light\n", "def test_convert_to_incompatible_units():\n    assert convert_to(meter, second) == meter\n    assert convert_to(second, meter) == second\n", "def test_convert_to_incompatible_units():\n    assert convert_to(meter, second) == meter\n    assert convert_to(newton, meter) == newton\n"], "sample_467": ["def test_custom_id_for_label(self):\n    widget = SelectDateWidget(years=(\"2014\",))\n    self.assertEqual(widget.id_for_label(\"test_id\"), \"test_id_month\")\n", "def test_selectdate_with_custom_attrs(self):\n    widget = SelectDateWidget(years=(\"2014\",), attrs={\"class\": \"custom-class\", \"data-test\": \"test-value\"})\n    self.assertInHTML(\n        '<select name=\"mydate_year\" id=\"id_mydate_year\" class=\"custom-class\" data-test=\"test-value\">',\n        widget.render(\"mydate\", \"\"),\n    )\n", "def test_selectdate_invalid_year(self):\n    \"\"\"\n    Invalid year (not an integer) should return None.\n    \"\"\"\n    w = SelectDateWidget(years=(\"2014\",))\n    data = {\"date_year\": \"invalid\", \"date_month\": \"8\", \"date_day\": \"13\"}\n    self.assertIsNone(w.value_from_datadict(data, {}, \"date\"))\n", "def test_selectdate_empty_input(self):\n    widget = SelectDateWidget(years=(\"2014\",))\n\n    # Empty data should return None.\n    self.assertIsNone(widget.value_from_datadict({}, {}, \"mydate\"))\n\n    # Partial data should return None.\n    self.assertIsNone(widget.value_from_datadict({\"mydate_year\": \"2014\"}, {}, \"mydate\"))\n    self.assertIsNone(widget.value_from_datadict({\"mydate_month\": \"01\"}, {}, \"mydate\"))\n    self.assertIsNone(widget.value_from_datadict({\"mydate_day\": \"01\"}, {}, \"mydate\"))\n\n    # Partial data with empty strings should return None.\n    self.assertIsNone(widget.value_from_datadict({\"mydate_year\": \"\"}, {}, \"mydate\"))\n    self.assertIsNone(widget.value_from_datadict({\"mydate_month\": \"\"}, {}, \"mydate\"))\n    self.assertIsNone(widget.value_from_datadict({\"mydate_day\": \"\"}, {}, \"mydate\"))\n", "def test_custom_date_format(self):\n    w = SelectDateWidget(years=(\"2007\", \"2008\", \"2009\", \"2010\", \"2011\"))\n    self.assertEqual(\n        w.value_from_datadict(\n            {\"date_year\": \"2010\", \"date_month\": \"8\", \"date_day\": \"13\"}, {}, \"date\"\n        ),\n        \"13-8-2010\",\n    )\n\n    self.assertHTMLEqual(\n        w.render(\"date\", \"13-8-2010\"),\n        \"\"\"\n        <select name=\"date_day\" id=\"id_date_day\">\n            <option value=\"\">---</option>\n            ...\n            <option value=\"13\" selected>13</option>\n            ...\n        </select>\n\n        <select name=\"date_month\" id=\"id_date_month\">\n            <option value=\"\">---</option>\n            ...\n            <option value=\"8\" selected>August</option>\n            ...\n        </select>\n\n        <select name=\"date_year\" id=\"id_date_year\">\n            <option value=\"\">---</option>\n            ...\n            <option value=\"2010\" selected>2010</option>\n            ...\n        </select>\n        \"\"\",\n    )\n"], "sample_593": ["def test_repr_of_dataarray_with_attrs(dataarray):\n    dataarray.attrs = {\"description\": \"Test data.\"}\n    formatted = fh.array_repr(dataarray)\n    # has an expanded data section\n    assert formatted.count(\"class='xr-array-in' type='checkbox' checked>\") == 1\n    # attrs has an item so it'll be enabled and collapsed\n    assert formatted.count(\"class='xr-section-summary-in' type='checkbox' >\") == 1\n    assert \"description\" in formatted\n    assert \"Test data.\" in formatted\n", "def test_coord_section_disabled_for_empty_coords():\n    ds = xr.Dataset()\n    section = fh.coord_section(ds.coords)\n    assert \"disabled\" in section\n", "def test_repr_of_empty_dataarray():\n    empty_dataarray = xr.DataArray([])\n    formatted = fh.array_repr(empty_dataarray)\n    assert \"dim_0\" not in formatted\n    assert \"()\" in formatted\n    assert \"float64\" in formatted\n    assert \"class='xr-array-in' type='checkbox' checked>\" in formatted\n    assert \"class='xr-section-summary-in' type='checkbox' disabled >\" in formatted\n", "def test_repr_of_dataarray_with_attrs_and_coords(dataarray):\n    dataarray.attrs[\"test_attr\"] = \"test_value\"\n    dataarray.coords[\"test_coord\"] = (\"dim_0\", np.arange(4))\n    formatted = fh.array_repr(dataarray)\n    assert \"dim_0\" in formatted\n    # has an expanded data section\n    assert formatted.count(\"class='xr-array-in' type='checkbox' checked>\") == 1\n    # coords and attrs have an items so they'll be enabled and collapsed\n    assert formatted.count(\"class='xr-section-summary-in' type='checkbox' >\") == 2\n    assert \"test_attr\" in formatted\n    assert \"test_coord\" in formatted\n    assert \"test_value\" in formatted\n", "def test_summarize_variable_with_unsafe_name_and_preview():\n    class Variable:\n        dims = (\"x\", \"y\")\n        dtype = np.float64\n        attrs = {}\n        _data = np.random.rand(2, 3)\n\n    name = \"<variable>\"\n    var = Variable()\n    preview = \"<unsafe>\"\n    formatted = fh.summarize_variable(name, var, preview=preview)\n    assert \"&lt;variable&gt;\" in formatted\n    assert \"(x, y)\" in formatted\n    assert \"float64\" in formatted\n    assert \"&lt;unsafe&gt;\" in formatted\n"], "sample_712": ["def test_ordinal_encoder_specified_categories():\n    X = np.array([['a', 'b'], ['c', 'a']], dtype=object)\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], ['a', 'b']])\n    exp = np.array([[0, 1], [2, 0]], dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp.astype('float64'))\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], ['a', 'b']], dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp)\n", "def test_ordinal_encoder_specified_categories():\n    X = np.array([['a', 'b'], [0, 2]], dtype=object).T\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], [0, 1, 2]])\n    exp = np.array([[0, 1], [2, 0]])\n    assert_array_equal(enc.fit_transform(X), exp.astype('float64'))\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], [0, 1, 2]], dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp)\n", "def test_one_hot_encoder_specified_categories_unsorted():\n    X = np.array([[1, 2]], dtype='int64').T\n    enc = OneHotEncoder(categories=[[2, 1]])\n    exp = np.array([[0., 1.],\n                    [1., 0.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n    assert enc.categories_[0].tolist() == [2, 1]\n    assert np.issubdtype(enc.categories_[0].dtype, np.int64)\n", "def test_one_hot_encoder_missing_values():\n    X = [['Male', 1], ['Female', np.nan]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    np.testing.assert_no_warnings(enc.fit_transform, X)\n    X_tr = enc.transform([['Male', 1], ['Female', np.nan]])\n    exp = np.array([[1., 0., 1.], [0., 0., 0.]])\n    np.testing.assert_array_equal(X_tr.toarray(), exp)\n", "def test_one_hot_encoder_categories_fit_transform(X, cat):\n    enc = OneHotEncoder(categories=cat)\n    X_tr = enc.fit_transform(X)\n    enc2 = OneHotEncoder(categories=cat)\n    enc2.fit(X)\n    assert_array_equal(X_tr.toarray(), enc2.transform(X).toarray())\n"], "sample_108": ["    def test_resolve_type_error_means_no_match(self):\n        @DynamicConverter.register_to_python\n            raise TypeError()\n        with self.assertRaises(Resolver404):\n            resolve('/dynamic/abc/')\n", "def test_resolve_invalid_path(self):\n    with self.assertRaises(ImproperlyConfigured):\n        path(r'invalid/path/with$/', empty_view)\n", "def test_path_lookup_with_unicode_characters(self):\n    match = resolve('/articles/\u00e9cole/')\n    self.assertEqual(match.url_name, 'articles-unicode')\n    self.assertEqual(match.args, ())\n    self.assertEqual(match.kwargs, {'title': '\u00e9cole'})\n    self.assertEqual(match.route, 'articles/<str:title>/')\n", "def test_path_lookup_with_optional_parameter(self):\n    match = resolve('/articles/2015/')\n    self.assertEqual(match.url_name, 'articles-year-optional')\n    self.assertEqual(match.args, ())\n    self.assertEqual(match.kwargs, {'year': 2015})\n    self.assertEqual(match.route, 'articles/<int:year>/<int:month>/?')\n\n    match = resolve('/articles/2015/04/')\n    self.assertEqual(match.url_name, 'articles-year-optional')\n    self.assertEqual(match.args, ())\n    self.assertEqual(match.kwargs, {'year': 2015, 'month': 4})\n    self.assertEqual(match.route, 'articles/<int:year>/<int:month>/?')\n", "def test_dynamic_converter_type_error_in_to_url(self):\n    @DynamicConverter.register_to_url\n        raise TypeError('This type error should propagate.')\n    with self.assertRaises(TypeError):\n        reverse('dynamic-converter', kwargs={'value': 'abc'})\n"], "sample_531": ["def test_unpickle_without_device_pixel_ratio():\n    fig = Figure(dpi=42)\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert fig2.dpi == 42\n", "def test_subplots_adjust(fig_test, fig_ref):\n    fig_ref.subplots_adjust(left=0.2, right=0.8, bottom=0.2, top=0.8)\n    fig_test.subplots_adjust(left=0.2, right=0.8, bottom=0.2, top=0.8)\n", "def test_set_constrained_layout_pads():\n    fig = plt.figure()\n    fig.set_constrained_layout_pads(w_pad=2.0, h_pad=1.5)\n    info = fig.get_layout_engine().get_info()\n    assert info['w_pad'] == 2.0\n    assert info['h_pad'] == 1.5\n", "def test_suptitle_with_kwargs(fig_test, fig_ref):\n    fig_ref.suptitle(\"Title\", color='r', size=20)\n    with mpl.rc_context({'figure.titlesize': 20}):\n        fig_test.suptitle(\"Title\", color='r')\n", "def test_figure_repr_with_dpi():\n    fig = plt.figure(figsize=(10, 20), dpi=100)\n    assert repr(fig) == \"<Figure size 1000x2000 with 0 Axes>\"\n"], "sample_928": ["def test_heading(app):\n    env = Environment()\n    env.language = 'en'\n    assert heading(env, 'Test Heading', 1) == 'Test Heading\\n-----------'\n    assert heading(env, 'Test Heading', 2) == 'Test Heading\\n~~~~~~~~~~~'\n    assert heading(env, 'Test Heading', 3) == 'Test Heading\\n", "def test_heading(app):\n    env = Environment()\n    env.language = 'en'\n    assert heading(env, \"Test Heading\", 1) == \"Test Heading\\n----------\"\n    assert heading(env, \"Test Heading\", 2) == \"Test Heading\\n~~~~~~~~~~~~~~\"\n    assert heading(env, \"Test Heading\", 3) == \"Test Heading\\n^^^^^^^^^^^^^^\"\n    env.language = 'ja'\n    assert heading(env, \"\u30c6\u30b9\u30c8\u898b\u51fa\u3057\", 1) == \"\u30c6\u30b9\u30c8\u898b\u51fa\u3057\\n--------------\"\n", "def test_heading(app):\n    env = Environment()\n    env.language = 'en'\n    assert heading(env, 'Test Heading', 1) == 'Test Heading\\n----------'\n    assert heading(env, 'Subheading', 2) == 'Subheading\\n~~~~~~~~~~~~'\n    assert heading(env, 'Subsubheading', 3) == 'Subsubheading\\n^^^^^^^^^^^^^^^^'\n", "def test_heading(app):\n    env = Environment()\n    env.language = 'en'\n    assert heading(env, 'Test Heading', 1) == 'Test Heading\\n----------'\n    assert heading(env, 'Test Heading', 2) == 'Test Heading\\n~~~~~~~~~~~~~~'\n    assert heading(env, 'Test Heading', 3) == 'Test Heading\\n^^^^^^^^^^^^^^'\n\n    env.language = 'ja'\n    assert heading(env, 'Test Heading', 1) == 'Test Heading\\n--------------'\n    assert heading(env, 'Test Heading', 2) == 'Test Heading\\n~~~~~~~~~~~~~~~'\n    assert heading(env, 'Test Heading', 3) == 'Test Heading\\n^^^^^^^^^^^^^^^^'\n", "def test_heading(app):\n    env = Environment(loader=app.builder.templates.loader)\n    env.language = 'en'\n    assert heading(env, 'Heading Level 1', 1) == 'Heading Level 1\\n==============='\n    assert heading(env, 'Heading Level 2', 2) == 'Heading Level 2\\n--------------'\n    assert heading(env, 'Heading Level 3', 3) == 'Heading Level 3\\n~~~~~~~~~~~~~~'\n    env.language = 'ja'\n    assert heading(env, 'Heading Level 1', 1) == 'Heading Level 1\\n=================='\n"], "sample_590": ["def test_concat_different_variables(self):\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2]), \"bar\": (\"x\", [3, 4])}, {\"x\": [0, 1]})\n    ds2 = Dataset({\"foo\": (\"x\", [5, 6]), \"baz\": (\"x\", [7, 8])}, {\"x\": [2, 3]})\n    expected = Dataset(\n        {\n            \"foo\": (\"x\", [1, 2, 5, 6]),\n            \"bar\": (\"x\", [3, 4, np.nan, np.nan]),\n            \"baz\": (\"x\", [np.nan, np.nan, 7, 8]),\n        },\n        {\"x\": [0, 1, 2, 3]},\n    )\n    actual = concat([ds1, ds2], dim=\"x\", data_vars=\"different\")\n    assert_identical(actual, expected)\n", "def test_concat_coords_override(self):\n    # Test case when compat is 'override'\n    ds1 = Dataset({\"foo\": (\"x\", [0, 1])}, {\"x\": [1, 2]})\n    ds2 = Dataset({\"foo\": (\"x\", [2, 3])}, {\"x\": [1, 2]})\n    expected = Dataset({\"foo\": (\"x\", [0, 1, 2, 3])}, {\"x\": [1, 2, 1, 2]})\n    actual = concat([ds1, ds2], dim=\"x\", compat=\"override\")\n    assert_identical(expected, actual)\n", "def test_concat_different_dimensions(self):\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.random.rand(5, 5))}, {\"x\": np.arange(5), \"y\": np.arange(5)})\n    ds2 = Dataset({\"a\": ((\"z\", \"y\"), np.random.rand(3, 5))}, {\"z\": np.arange(3), \"y\": np.arange(5)})\n\n    with pytest.raises(ValueError, match=\"Variables {'a'} are coordinates in some datasets but not others.\"):\n        concat([ds1, ds2], dim=\"w\")\n", "def test_concat_with_different_data_vars(self):\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6])}, {\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"a\": (\"x\", [7, 8, 9]), \"c\": (\"x\", [10, 11, 12])}, {\"x\": [3, 4, 5]})\n\n    expected = Dataset(\n        {\"a\": (\"x\", [1, 2, 3, 7, 8, 9]), \"b\": (\"x\", [4, 5, 6, np.nan, np.nan, np.nan]), \"c\": (\"x\", [np.nan, np.nan, np.nan, 10, 11, 12])},\n        {\"x\": [0, 1, 2, 3, 4, 5]}\n    )\n\n    actual = concat([ds1, ds2], dim=\"x\", data_vars='different')\n    assert_identical(actual, expected)\n", "def test_concat_compat_override(self):\n    ds1 = Dataset(\n        {\"has_x_y\": ((\"y\", \"x\"), [[1, 2]]), \"has_x\": (\"x\", [1, 2])},\n        coords={\"x\": [0, 1], \"y\": [0]},\n    )\n    ds2 = Dataset(\n        {\"has_x_y\": ((\"y\", \"x\"), [[3, 4]]), \"has_x\": (\"x\", [3, 4])},\n        coords={\"x\": [0, 1], \"y\": [1]},\n    )\n\n    result = concat([ds1, ds2], dim=\"y\", data_vars=\"minimal\", compat=\"override\")\n    assert_equal(ds1.has_x, result.has_x)\n"], "sample_550": ["def test_toolmanager_active_toggle():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    tool = fig.canvas.manager.toolmanager.get_tool('pan')\n    assert not tool.active\n    fig.canvas.manager.toolmanager.trigger_tool('pan')\n    assert tool.active\n    fig.canvas.manager.toolmanager.trigger_tool('pan')\n    assert not tool.active\n", "def test_toggle_grid():\n    fig, ax = plt.subplots()\n    assert not ax.xaxis._gridOnMajor\n    assert not ax.yaxis._gridOnMajor\n\n    # Toggle grid on\n    ax.grid(True)\n    assert ax.xaxis._gridOnMajor\n    assert ax.yaxis._gridOnMajor\n\n    # Toggle grid off\n    ax.grid(False)\n    assert not ax.xaxis._gridOnMajor\n    assert not ax.yaxis._gridOnMajor\n", "def test_toolbar_home_after_zoom_pan():\n    fig, ax = plt.subplots()\n    ax.plot(range(11), range(11))\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n\n    # Zoom in from (x, y) = (2, 2) to (5, 5).\n    start, stop = ax.transData.transform([(2, 2), (5, 5)])\n    MouseEvent(\"button_press_event\", fig.canvas, *start, MouseButton.LEFT)._process()\n    MouseEvent(\"button_release_event\", fig.canvas, *stop, MouseButton.LEFT)._process()\n\n    # Pan to (x, y) = (1, 1).\n    start, stop = ax.transData.transform([(2, 2), (1, 1)])\n    tb.pan()\n    MouseEvent(\"button_press_event\", fig.canvas, *start, MouseButton.LEFT)._process()\n    MouseEvent(\"button_release_event\", fig.canvas, *stop, MouseButton.LEFT)._process()\n\n    # Go back to home.\n    KeyEvent(\"key_press_event\", fig.canvas, \"h\")._process()\n\n    assert ax.get_xlim() == ax.get_ylim() == (0, 10)\n", "def test_toolbar_save():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    toolmanager = fig.canvas.manager.toolmanager\n\n    # Mock the save_figure function to check if it's called\n    toolmanager.save_figure = lambda *args, **kwargs: None\n    with unittest.mock.patch.object(toolmanager, 'save_figure') as mock_save_figure:\n        toolmanager.trigger_tool('save')\n        mock_save_figure.assert_called_once()\n\n    # Clean up\n    plt.close(fig)\n", "def test_interactive_colorbar_toolbar(plot_func, orientation, tool):\n    fig, ax = plt.subplots()\n    data = np.arange(12).reshape((4, 3))\n    vmin0, vmax0 = 0, 10\n    coll = getattr(ax, plot_func)(data, vmin=vmin0, vmax=vmax0)\n\n    cb = fig.colorbar(coll, ax=ax, orientation=orientation)\n    if plot_func == \"contourf\":\n        # Just determine we can't navigate and exit out of the test\n        assert not cb.ax.get_navigate()\n        return\n\n    assert cb.ax.get_navigate()\n\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig.canvas.manager.toolmanager.trigger_tool(tool)\n    assert cb.ax.get_navigate_mode() == tool.upper()\n\n    # Reset the toolbar\n    plt.rcParams['toolbar'] = 'None'\n"], "sample_1151": ["def test_div_zero():\n    assert S.Zero/S.Zero is nan\n    assert S.Zero/S.One is S.Zero\n    assert S.One/S.Zero is zoo\n", "def test_issue_18825():\n    assert (x + I*y + z).as_real_imag() == (x + z, y)\n", "def test_issue_18861():\n    assert (x + I*x).conjugate() == x - I*x\n    assert (x + I*y).conjugate() == x - I*y\n", "def test_issue_18623():\n    n = Symbol('n', integer=True)\n    assert Mul(n, (n-2), evaluate=False).is_commutative is None\n", "def test_Add_is_even_odd():\n    x = Symbol('x', integer=True)\n\n    k = Symbol('k', even=True)\n    n = Symbol('n', odd=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n\n    assert (k + n).is_odd is None\n    assert (k + m).is_odd is None\n    assert (n + m).is_odd is True\n\n    assert (k + n).is_even is None\n    assert (k + m).is_even is True\n    assert (n + m).is_even is False\n\n    assert (x + k).is_odd is None\n    assert (x + n).is_odd is None\n\n    assert (x + k).is_even is None\n    assert (x + n).is_even is None\n"], "sample_1099": ["def test_eval_partial_derivative_expr2():\n\n    tau, alpha, beta = symbols(\"tau alpha beta\")\n\n    # this is another special expression\n    # tested: tensor derivative with respect to a symbol\n    # tested: tensor derivative with respect to a tensor\n    base_expr2 = tau**alpha*H(i, j) + beta*A(i)*A(-j)\n\n    tensor_derivative_symbol = PartialDerivative(base_expr2, tau)._perform_derivative()\n    tensor_derivative_tensor = PartialDerivative(base_expr2, A(k))._perform_derivative()\n\n    assert tensor_derivative_symbol - alpha*1/tau*tau**alpha*H(i, j) == 0\n\n    assert (tensor_derivative_tensor - (beta*L.delta(i, -k)*A(-j) +\n        beta*A(i)*L.delta(-j, -k))).expand() == 0\n", "def test_eval_partial_derivative_expr2():\n\n    tau, alpha = symbols(\"tau alpha\")\n\n    # this is a new special expression\n    # tested: vector derivative\n    # tested: tensor derivative\n    base_expr2 = tau*A(i)*H(-i, j) + A(i)*A(-i)*A(j)*tau**alpha\n\n    tensor_derivative = PartialDerivative(base_expr2, H(k, m))._perform_derivative()\n    vector_derivative = PartialDerivative(base_expr2, A(k))._perform_derivative()\n\n    assert (tensor_derivative - tau*A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*L.delta(j, -m)) == 0\n\n    assert (vector_derivative - (tau**alpha*A(-k)*A(j)*alpha*tau**(alpha-1) +\n        tau*A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*A(j) +\n        tau*A(L_0)*A(-L_0)*L.delta(j, -k) +\n        A(L_0)*A(-L_0)*tau**alpha*L.delta(j, -k) +\n        tau*L.delta(L_0, -k)*H(-L_0, j))).expand() == 0\n\n    assert (vector_derivative.contract_metric(L.metric).contract_delta(L.delta) -\n        (tau**alpha*A(-k)*A(j)*alpha*tau**(alpha-1) + 2*tau*A(-k)*A(j) + tau*H(-k, j))).expand() == 0\n", "def test_eval_partial_derivative_mixed_derivatives():\n\n    tau, alpha = symbols(\"tau alpha\")\n\n    # this is a test for mixed derivatives\n    base_expr1 = A(i)*H(-i, j) + A(i)*A(-i)*A(j) + tau**alpha*A(j)\n\n    mixed_derivative = PartialDerivative(PartialDerivative(base_expr1, A(k)), H(m, n))._perform_derivative()\n\n    # the expected result is manually calculated based on the rules of partial derivatives\n    expected_result = PartialDerivative(A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*A(j) +\n                                       A(L_0)*A(-L_0)*A(j) + tau**alpha*A(j), H(m, n))._perform_derivative()\n\n    assert (mixed_derivative - expected_result).expand() == 0\n", "def test_eval_partial_derivative_expr2():\n    # Test for the derivative of a more complex expression\n    # This tests the combination of scalar, vector and tensor derivatives\n    tau, alpha, beta = symbols(\"tau alpha beta\")\n    base_expr2 = A(i) * tau**alpha + B(j) * tau**beta * H(-j, k) + C(k) * tau**(alpha + beta) * A(-k)\n\n    tensor_derivative = PartialDerivative(base_expr2, H(m, n))._perform_derivative()\n    vector_derivative = PartialDerivative(base_expr2, A(m))._perform_derivative()\n    scalar_derivative = PartialDerivative(base_expr2, tau)._perform_derivative()\n\n    # Expected results for the derivatives\n    expected_tensor_derivative = B(L_0) * tau**beta * L.delta(-L_0, -m) * L.delta(k, -n)\n    expected_vector_derivative = tau**alpha + C(L_0) * (alpha + beta) * tau**(alpha + beta - 1) * L.delta(-L_0, -m)\n    expected_scalar_derivative = alpha * tau**(alpha - 1) * A(i) + beta * tau**(beta - 1) * B(j) * H(-j, k) + (alpha + beta) * tau**(alpha + beta - 1) * C(k) * A(-k)\n\n    assert tensor_derivative == expected_tensor_derivative\n    assert vector_derivative == expected_vector_derivative\n    assert scalar_derivative == expected_scalar_derivative\n", "def test_eval_partial_derivative_expr2():\n\n    # this is another special expression\n    # tested: multiple derivatives with respect to same tensor\n    base_expr2 = H(i, j) * A(k) * H(-j, k) + A(m) * A(-m)\n\n    derivative_1 = PartialDerivative(base_expr2, A(i))._perform_derivative()\n    derivative_2 = PartialDerivative(base_expr2, A(i), A(m))._perform_derivative()\n\n    assert derivative_1 == 0\n    assert derivative_2 == -2 * L.metric(-i, L_0) * L.delta(L_0, -m) * A(m)\n"], "sample_863": ["def test_pipeline_fit_params_with_underscores():\n    # tests that Pipeline passes fit_params with underscores to the final estimator\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    pipe.fit(X=None, y=None, clf__should_succeed_with_underscore=True)\n    assert pipe.named_steps['clf'].successful\n    assert not hasattr(pipe.named_steps['clf'], 'should_succeed_with_underscore')\n", "def test_pipeline_init_invalid_params():\n    # Test the pipeline constructor with invalid parameters\n    assert_raises(TypeError, Pipeline, invalid_param=0)\n", "def test_feature_union_fit_transform_params():\n    # Test that fit_transform accepts fit_params\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([0, 1])\n    mult1 = Mult(1)\n    mult2 = Mult(2)\n    fu = FeatureUnion([(\"mult1\", mult1), (\"mult2\", mult2)])\n    fu.fit_transform(X, y, mult1__mult=10)\n    assert_array_equal(np.array([[10, 20], [20, 40]]), mult1.transform(X))\n    assert_array_equal(np.array([[2, 4, 6], [8, 10, 12]]), mult2.transform(X))\n", "def test_pipeline_verbose_transform():\n    # Test verbose output for pipeline's transform method\n    X = np.array([[1, 2], [3, 4]])\n    transf = Transf()\n    pipeline = Pipeline([('mock', transf)])\n\n    # test transform with verbose=False\n    pipeline.set_params(verbose=False)\n    pipeline.fit_transform(X)\n    out, err = capsys.readouterr()\n    assert out == ''\n\n    # test transform with verbose=True\n    pipeline.set_params(verbose=True)\n    pipeline.fit_transform(X)\n    out, err = capsys.readouterr()\n    assert \"[Pipeline](step 1 of 1) Processing mock\" in out\n", "def test_pipeline_fit_params_propagation():\n    # Test that pipeline correctly propagates fit_params to transformers\n    # and estimators\n    X = np.array([[1, 2]])\n    y = np.array([0])\n\n    class DummyTransformer(BaseEstimator, TransformerMixin):\n            self.param = param\n            self.fit_params = None\n\n            self.fit_params = fit_params\n            return self\n\n            return X\n\n    dummy_transformer = DummyTransformer()\n    pipeline = Pipeline([('dummy_transformer', dummy_transformer), ('clf', SVC())])\n\n    pipeline.fit(X, y, dummy_transformer__param=1, clf__C=1)\n    assert dummy_transformer.param == 1\n    assert pipeline.named_steps['clf'].C == 1\n    assert dummy_transformer.fit_params == {}\n\n    pipeline.fit(X, y, dummy_transformer__param=2, clf__C=2, clf__gamma='scale')\n    assert dummy_transformer.param == 2\n    assert pipeline.named_steps['clf'].C == 2\n    assert pipeline.named_steps['clf'].gamma == 'scale'\n    assert dummy_transformer.fit_params == {}\n"], "sample_206": ["    def test_default_value(self):\n        \"\"\"\n        FileField should have a default value of None.\n        \"\"\"\n        d = Document()\n        self.assertIsNone(d.myfile)\n", "def test_file_descriptor_get_with_instance(self):\n    \"\"\"\n    FileDescriptor.__get__() returns the attribute value when an instance is provided.\n    \"\"\"\n    d = Document(myfile='something.txt')\n    field = d._meta.get_field('myfile')\n    descriptor = field.descriptor_class(field)\n    self.assertEqual(descriptor.__get__(d), 'something.txt')\n", "    def test_file_equality(self):\n        \"\"\"\n        FileField's __eq__ method should handle comparison with other File objects and strings.\n        \"\"\"\n        d = Document.objects.create(myfile='something.txt')\n        file = File(open('something.txt', 'rb'))\n        self.assertTrue(d.myfile == file)\n        self.assertTrue(d.myfile == 'something.txt')\n", "def test_non_ascii_filename(self):\n    \"\"\"\n    Non-ASCII filenames should be handled correctly.\n    \"\"\"\n    non_ascii_filename = \"\u975eASCII_filename.txt\"\n    with TemporaryUploadedFile(non_ascii_filename, 'text/plain', 0, 'UTF-8') as tmp_file:\n        Document.objects.create(myfile=tmp_file)\n        self.assertTrue(Document.objects.filter(myfile=non_ascii_filename).exists())\n", "def test_delete_with_save(self):\n    \"\"\"\n    Calling delete on a FileField with save=True should save the model instance.\n    \"\"\"\n    d = Document.objects.create(myfile='something.txt')\n    d.myfile.save('something.txt', ContentFile(b'', name='bla'))\n    d.myfile.delete(save=True)\n    d.refresh_from_db()\n    self.assertFalse(bool(d.myfile))\n"], "sample_532": ["def test_contour_single_level():\n    # Test contour with a single level\n    z = np.array([[1.0, 2.0], [3.0, 4.0]])\n    cs = plt.contour(z, levels=[2.5])\n    assert len(cs.levels) == 1\n    assert cs.levels[0] == 2.5\n", "def test_contour_invalid_levels():\n    fig, ax = plt.subplots()\n    data = np.arange(16).reshape((4, 4))\n    with pytest.raises(ValueError, match=\"Contour levels must be increasing\"):\n        ax.contour(data, levels=[2, 1, 3])\n", "def test_contour_linestyles(style):\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    # Change linestyles using linestyles kwarg\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z, 6, colors='k', linestyles=style)\n    ax.clabel(CS, fontsize=9, inline=True)\n    ax.set_title(f'Single color - positive contours {style}')\n    assert CS.linestyles == [style] * 6\n", "def test_contour_extent():\n    fig, ax = plt.subplots()\n    data = np.arange(16).reshape((4, 4))\n    ax.contour(data, extent=(-1, 1, -1, 1))\n    assert ax.get_xlim() == (-1, 1)\n    assert ax.get_ylim() == (-1, 1)\n", "def test_contour_with_specified_levels():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    levels = [2, 4, 6, 8]\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, levels=levels)\n\n    # Check if the levels in the contour set match the specified levels\n    assert_array_almost_equal(cs.levels, levels)\n"], "sample_566": ["def test_gridspec_no_mutate_input_subplotspec():\n    gs = {'left': .1}\n    gs_orig = dict(gs)\n    fig = plt.figure()\n    spec = gridspec.GridSpec(ncols=2, nrows=1, width_ratios=[1, 2], figure=fig)\n    fig.add_subplot(spec[0], gridspec_kw=gs)\n    assert gs == gs_orig\n", "def test_savefig_pil_kwargs():\n    fig = plt.figure()\n    with io.BytesIO() as buf:\n        fig.savefig(buf, format='png', pil_kwargs={'optimize': True})\n        img = Image.open(buf)\n        img.load()\n\n    # Check that the optimize flag was applied.\n    assert img.info.get('optimize') == True\n", "def test_gridspec_kwarg_propagation():\n    fig, axs = plt.subplots(2, 2, gridspec_kw={'hspace': 0.5, 'wspace': 0.5})\n    for ax in axs.flat:\n        assert ax.get_subplotspec().get_gridspec().get_hspace() == 0.5\n        assert ax.get_subplotspec().get_gridspec().get_wspace() == 0.5\n", "def test_tight_layout_kwargs():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot([0, 1], [0, 1])\n    fig.tight_layout(pad=2.0, h_pad=1.0, w_pad=1.0, rect=[0, 0, 0.5, 0.5])\n    fig.draw_without_rendering()\n    bbox = ax.get_tightbbox(fig.canvas.get_renderer())\n    assert np.allclose(bbox.p0, np.array([0.1, 0.1]))\n    assert np.allclose(bbox.p1, np.array([0.4, 0.4]))\n", "def test_add_subplot_with_label(fig_test, fig_ref):\n    fig_test.add_subplot(111, label='Test Axes')\n    assert fig_test.axes[0].get_label() == 'Test Axes'\n\n    fig_ref.add_subplot(111, label='Ref Axes')\n    assert fig_ref.axes[0].get_label() == 'Ref Axes'\n"], "sample_990": ["def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x) / (1 + tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - tanh(x)**3) / (1 - 3*tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True).expand() == (2*tanh(x)) / (1 + tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == \\\n        (3*tanh(x) - tanh(x)**3) / (1 + 3*tanh(x)**2 - tanh(x)**4)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y))/(1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True).expand() == 2*tanh(x)/(1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - tanh(x)**3)/(1 - 3*tanh(x)**2)\n"], "sample_831": ["def test_plot_tree_max_depth(pyplot):\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    nodes = plot_tree(clf, max_depth=1)\n    assert len(nodes) == 2\n    assert nodes[0].get_text() == \"feature_1 <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    assert nodes[1].get_text() == \"(...)\"\n", "def test_plot_tree_custom_fontsize(pyplot):\n    # Check plot_tree with custom fontsize\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    nodes = plot_tree(clf, fontsize=12)\n    assert len(nodes) == 3\n    assert nodes[0].get_fontsize() == 12\n    assert nodes[1].get_fontsize() == 12\n    assert nodes[2].get_fontsize() == 12\n", "def test_plot_tree_gini_impurity(pyplot):\n    # Check correctness of export_graphviz for criterion = gini impurity\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 6\\nvalue = [3, 3]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n", "def test_plot_tree_gini(pyplot):\n    # Check correctness of plot_tree for criterion = gini\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['feature_1', 'feature_2']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"feature_1 <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 6\\nvalue = [3, 3]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n", "def test_plot_tree_gini(pyplot):\n    # Test export code for criterion = gini\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 6\\nvalue = [3, 3]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n"], "sample_8": ["def test_masked_array_from_masked_with_masked_values():\n    \"\"\"Check that we can initialize a MaskedArray with masked values.\"\"\"\n    np_ma = np.ma.MaskedArray(self.ma, mask=[[False, True, False], [True, False, True]])\n    assert type(np_ma) is np.ma.MaskedArray\n    assert type(np_ma.data) is self._data_cls\n    assert type(np_ma.mask) is np.ndarray\n    assert_array_equal(np_ma.data, self.a)\n    assert_array_equal(np_ma.mask, [[False, True, False], [True, False, True]])\n", "def test_masked_array_from_masked_array():\n    \"\"\"Check that we can initialize a MaskedArray from a MaskedArray.\"\"\"\n    ma = Masked(self.a, mask=self.mask_a)\n    np_ma = np.ma.MaskedArray(ma)\n    assert type(np_ma) is np.ma.MaskedArray\n    assert type(np_ma.data) is self._data_cls\n    assert type(np_ma.mask) is np.ndarray\n    assert_array_equal(np_ma.data, self.a)\n    assert_array_equal(np_ma.mask, self.mask_a)\n", "def test_masked_array_interaction_with_numpy_ma():\n    \"\"\"Check interaction with numpy.ma.MaskedArray.\"\"\"\n    np_ma = np.ma.MaskedArray(self.ma)\n    ma_from_np_ma = Masked(np_ma)\n    assert_masked_equal(ma_from_np_ma, self.ma)\n", "def test_masked_array_from_masked_with_fill_value():\n    \"\"\"Check that we can initialize a MaskedArray with fill_value.\"\"\"\n    fill_value = 999\n    np_ma = np.ma.MaskedArray(self.ma, fill_value=fill_value)\n    assert type(np_ma) is np.ma.MaskedArray\n    assert type(np_ma.data) is self._data_cls\n    assert type(np_ma.mask) is np.ndarray\n    assert_array_equal(np_ma.filled(), np.where(self.mask_a, fill_value, self.a))\n", "def test_masked_array_interaction_with_masked_array(self):\n    np_ma = np.ma.MaskedArray(self.a, mask=self.mask_a)\n    ma = Masked(np_ma)\n    assert type(ma) is Masked\n    assert type(ma.unmasked) is self._data_cls\n    assert type(ma.mask) is np.ndarray\n    assert_array_equal(ma.unmasked, self.a)\n    assert_array_equal(ma.mask, self.mask_a)\n"], "sample_914": ["def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c=3, **kwargs): pass\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == \"a=2, *args, c=3, **kwargs\"\n", "def test_unparse_slice(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_argument_with_annotation():\n    source = \"def func(arg: int = 0): pass\"\n    module = ast.parse(source)\n    arg = module.body[0].args.args[0]\n    expected = \"arg: int = 0\"\n    assert ast.unparse(arg) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c=3, **kwargs): pass\"\n    module = ast.parse(source)\n    expected = \"a, b=2, *args, c=3, **kwargs\"\n    assert ast.unparse_arguments(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def f(a, b=1, c:int=2, *args, d:str, e=3, **kwargs): pass\"\n    module = ast.parse(source)\n    func = module.body[0]\n    assert ast.unparse(func.args) == \"a=1, c: int = 2, *args, d: str, e=3, **kwargs\"\n"], "sample_161": ["def test_m2m_through_fields_validation(self):\n    class Fan(models.Model):\n        pass\n\n    class Event(models.Model):\n        invitees = models.ManyToManyField(\n            Fan,\n            through='Invitation',\n            through_fields=('event', 'invitee'),\n        )\n\n    class Invitation(models.Model):\n        event = models.ForeignKey(Event, models.CASCADE)\n        invitee = models.ForeignKey(Fan, models.CASCADE)\n        inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n    field = Event._meta.get_field('invitees')\n    self.assertEqual(field.check(from_model=Event), [\n        Error(\n            \"'Invitation.invitee' is not a foreign key to 'Event'.\",\n            hint=\"Did you mean one of the following foreign keys to 'Event': event?\",\n            obj=field,\n            id='fields.E339',\n        ),\n    ])\n", "def test_foreign_object_to_unique_field_with_meta_constraint_and_subset(self):\n    class Person(models.Model):\n        country_id = models.IntegerField()\n        city_id = models.IntegerField()\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['country_id', 'city_id'],\n                    name='tfotpuf_unique',\n                ),\n            ]\n\n    class MMembership(models.Model):\n        person_country_id = models.IntegerField()\n        person_city_id = models.IntegerField()\n        person = models.ForeignObject(\n            Person,\n            on_delete=models.CASCADE,\n            from_fields=['person_country_id'],\n            to_fields=['country_id'],\n        )\n\n    field = MMembership._meta.get_field('person')\n    self.assertEqual(field.check(), [])\n", "def test_foreign_object_to_partially_unique_fields(self):\n    class Person(models.Model):\n        country_id = models.IntegerField()\n        city_id = models.IntegerField()\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['country_id'],\n                    name='tfotpuf_partial_unique',\n                    condition=models.Q(city_id__gt=2),\n                ),\n            ]\n\n    class MMembership(models.Model):\n        person_country_id = models.IntegerField()\n        person_city_id = models.IntegerField()\n        person = models.ForeignObject(\n            Person,\n            on_delete=models.CASCADE,\n            from_fields=['person_country_id', 'person_city_id'],\n            to_fields=['country_id', 'city_id'],\n        )\n\n    field = MMembership._meta.get_field('person')\n    self.assertEqual(field.check(), [\n        Error(\n            \"No subset of the fields 'country_id', 'city_id' on model \"\n            \"'Person' is unique.\",\n            hint=(\n                'Mark a single field as unique=True or add a set of '\n                'fields to a unique constraint (via unique_together or a '\n                'UniqueConstraint (without condition) in the model '\n                'Meta.constraints).'\n            ),\n            obj=field,\n            id='fields.E310',\n        ),\n    ])\n", "def test_foreign_object_to_non_unique_fields_with_multiple_to_fields(self):\n    class Person(models.Model):\n        country_id = models.IntegerField()\n        city_id = models.IntegerField()\n        zip_code = models.IntegerField()\n\n    class MMembership(models.Model):\n        person_country_id = models.IntegerField()\n        person_city_id = models.IntegerField()\n        person_zip_code = models.IntegerField()\n\n        person = models.ForeignObject(\n            Person,\n            on_delete=models.CASCADE,\n            from_fields=['person_country_id', 'person_city_id', 'person_zip_code'],\n            to_fields=['country_id', 'city_id', 'zip_code'],\n        )\n\n    field = MMembership._meta.get_field('person')\n    self.assertEqual(field.check(), [\n        Error(\n            \"No subset of the fields 'country_id', 'city_id', 'zip_code' on model 'Person' is unique.\",\n            hint=(\n                'Mark a single field as unique=True or add a set of '\n                'fields to a unique constraint (via unique_together or a '\n                'UniqueConstraint (without condition) in the model '\n                'Meta.constraints).'\n            ),\n            obj=field,\n            id='fields.E310',\n        )\n    ])\n", "def test_multiple_foreign_keys_with_symmetrical_m2m(self):\n    class Person(models.Model):\n        name = models.CharField(max_length=50)\n\n    class Friendship(models.Model):\n        person1 = models.ForeignKey(Person, models.CASCADE, related_name='friends1')\n        person2 = models.ForeignKey(Person, models.CASCADE, related_name='friends2')\n        m2m = models.ManyToManyField(Person, related_name='m2m_friends', symmetrical=False)\n\n        class Meta:\n            unique_together = ('person1', 'person2')\n\n    field = Friendship._meta.get_field('m2m')\n    self.assertEqual(field.check(from_model=Friendship), [\n        Error(\n            \"Reverse accessor for 'Friendship.person1' clashes with reverse accessor for 'Friendship.m2m'.\",\n            hint=(\"Add or change a related_name argument to the definition for \"\n                  \"'Friendship.person1' or 'Friendship.m2m'.\"),\n            obj=Friendship._meta.get_field('person1'),\n            id='fields.E304',\n        ),\n        Error(\n            \"Reverse accessor for 'Friendship.person2' clashes with reverse accessor for 'Friendship.m2m'.\",\n            hint=(\"Add or change a related_name argument to the definition for \"\n                  \"'Friendship.person2' or 'Friendship.m2m'.\"),\n            obj=Friendship._meta.get_field('person2'),\n            id='fields.E304',\n        ),\n        Error(\n            \"Reverse accessor for 'Friendship.m2m' clashes with reverse accessor for 'Friendship.person1'.\",\n            hint=(\"Add or change a related_name argument to the definition for \"\n                  \"'Friendship.m2m' or 'Friendship.person1'.\"),\n            obj=Friendship._meta.get_field('m2m'),\n            id='fields.E"], "sample_504": ["def test_twoslope_colorbar_label():\n    # Test that setting a label on the colorbar works\n    fig, ax = plt.subplots()\n\n    norm = mcolors.TwoSlopeNorm(20, 5, 95)\n    pc = ax.pcolormesh(np.arange(1, 11), np.arange(1, 11),\n                       np.arange(100).reshape(10, 10),\n                       norm=norm, cmap='RdBu_r')\n    cb = fig.colorbar(pc, label='Units')\n    assert cb.ax.get_ylabel() == 'Units'\n", "def test_colorbar_labelpad():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, label='cbar', labelpad=20)\n    assert cbar.ax.get_ylabel().get_position()[1] == 20\n\n    cbar.ax.set_label_position('right')\n    assert cbar.ax.get_ylabel().get_position()[0] > 1.0\n", "def test_colorbar_custom_ticklocator():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cbar = fig.colorbar(pc, ax=ax, ticks=FixedLocator([20, 50, 80]))\n    fig.canvas.draw()\n    ticklabels = [l.get_text() for l in cbar.ax.yaxis.get_ticklabels()]\n    assert ticklabels == ['20', '50', '80']\n", "def test_colorbar_ticklabels():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    pc = ax.contourf(data, levels=levels)\n    cbar = fig.colorbar(pc, ax=ax, orientation='vertical', ticks=levels,\n                        ticklabels=['0', '200', '400', '600', '800', '1000', '1200'])\n    assert [label.get_text() for label in cbar.ax.get_yticklabels()] == ['0', '200', '400', '600', '800', '1000', '1200']\n", "def test_colorbar_set_ticks_labels():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im)\n\n    labels = ['Label 1', 'Label 2', 'Label 3', 'Label 4']\n    cbar.set_ticklabels(labels)\n    fig.canvas.draw()\n    ticklabels = [l.get_text() for l in cbar.ax.yaxis.get_ticklabels()]\n    assert ticklabels == labels\n"], "sample_1171": ["def test_issue_17858():\n    assert 1 in Range(-oo, oo)\n    assert 0 in Range(oo, -oo, -1)\n    assert oo not in Range(-oo, oo)\n    assert -oo not in Range(-oo, oo)\n", "def test_issue_18081():\n    assert ImageSet(Lambda(n, n*log(2)), S.Integers).intersection(\n        S.Integers).dummy_eq(Intersection(ImageSet(\n        Lambda(n, n*log(2)), S.Integers), S.Integers))\n", "def test_imageset_intersection_with_rationals():\n    n = Dummy()\n    s = ImageSet(Lambda(n, -I*(I*(2*pi*n - pi/4) + log(Abs(sqrt(-I))))), S.Rationals)\n    assert s.intersect(S.Reals) == ImageSet(Lambda(n, 2*pi*n + pi*Rational(7, 4)), S.Rationals)\n", "def test_issue_17858_additional():\n    assert 0 in Range(oo) == False\n    assert oo in Range(oo) == False\n    assert -oo in Range(oo) == False\n    assert 0 in Range(-oo, 0, -1) == False\n    assert oo in Range(-oo, 0, -1) == False\n    assert -oo in Range(-oo, 0, -1) == True\n", "def test_issue_18386():\n    A = Interval(0, 1)\n    B = Interval(2, 3)\n    C = ComplexRegion(A*B)\n    assert C.measure == 2\n"], "sample_472": ["def test_paginator_with_custom_error_messages(self):\n    custom_messages = {\n        \"invalid_page\": \"Custom invalid page message\",\n        \"min_page\": \"Custom min page message\",\n        \"no_results\": \"Custom no results message\",\n    }\n    paginator = Paginator([1, 2, 3], 2, error_messages=custom_messages)\n    with self.assertRaisesMessage(PageNotAnInteger, \"Custom invalid page message\"):\n        paginator.validate_number(\"x\")\n    with self.assertRaisesMessage(EmptyPage, \"Custom min page message\"):\n        paginator.validate_number(-1)\n    with self.assertRaisesMessage(EmptyPage, \"Custom no results message\"):\n        paginator.validate_number(3)\n", "def test_paginator_with_queryset(self):\n    \"\"\"\n    Tests the paginator with a Django QuerySet object_list.\n    \"\"\"\n    paginator = Paginator(Article.objects.all().order_by('id'), 5)\n    self.assertEqual(paginator.count, 9)\n    self.assertEqual(paginator.num_pages, 2)\n\n    # Test first page\n    first_page = paginator.page(1)\n    self.assertEqual(first_page.number, 1)\n    self.assertEqual(len(first_page), 5)\n    self.assertFalse(first_page.has_previous())\n    self.assertTrue(first_page.has_next())\n    self.assertEqual(first_page.start_index(), 1)\n    self.assertEqual(first_page.end_index(), 5)\n\n    # Test last page\n    last_page = paginator.page(2)\n    self.assertEqual(last_page.number, 2)\n    self.assertEqual(len(last_page), 4)\n    self.assertTrue(last_page.has_previous())\n    self.assertFalse(last_page.has_next())\n    self.assertEqual(last_page.start_index(), 6)\n    self.assertEqual(last_page.end_index(), 9)\n", "def test_paginator_with_non_integer_per_page(self):\n    \"\"\"\n    Test that the Paginator handles non-integer per_page values correctly.\n    \"\"\"\n    with self.assertRaisesMessage(ValueError, \"invalid literal for int() with base 10: 'a'\"):\n        Paginator([1, 2, 3, 4, 5], \"a\")\n", "def test_page_has_next_previous(self):\n    \"\"\"\n    Test the has_next() and has_previous() methods of the Page class.\n    \"\"\"\n    paginator = Paginator([1, 2, 3, 4, 5], 2)\n\n    # Test the first page\n    page = paginator.page(1)\n    self.assertTrue(page.has_next())\n    self.assertFalse(page.has_previous())\n\n    # Test the second page\n    page = paginator.page(2)\n    self.assertTrue(page.has_next())\n    self.assertTrue(page.has_previous())\n\n    # Test the third page\n    page = paginator.page(3)\n    self.assertFalse(page.has_next())\n    self.assertTrue(page.has_previous())\n", "def test_paginate_large_object_list(self):\n    \"\"\"\n    Paginator can handle large object lists.\n    \"\"\"\n    # Create a large object list\n    large_object_list = list(range(10000))\n    paginator = Paginator(large_object_list, per_page=100)\n\n    # Test the first page\n    first_page = paginator.page(1)\n    self.assertEqual(first_page.number, 1)\n    self.assertEqual(first_page.start_index(), 1)\n    self.assertEqual(first_page.end_index(), 100)\n    self.assertSequenceEqual(first_page.object_list, large_object_list[:100])\n\n    # Test the last page\n    last_page = paginator.page(100)\n    self.assertEqual(last_page.number, 100)\n    self.assertEqual(last_page.start_index(), 9901)\n    self.assertEqual(last_page.end_index(), 10000)\n    self.assertSequenceEqual(last_page.object_list, large_object_list[9900:])\n"], "sample_898": ["def test_label_ranking_average_precision_score_value():\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    expected_score = 0.4166666666666667\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), expected_score)\n", "def test_undefined_binary_multiclass_metrics():\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 2, size=(20, ))\n    y_pred = random_state.randint(0, 2, size=(20, ))\n    y_score = random_state.random_sample(size=(20,))\n\n    for name in METRIC_UNDEFINED_BINARY_MULTICLASS:\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            assert_raises(ValueError, metric, y_true, y_score)\n        else:\n            assert_raises(ValueError, metric, y_true, y_pred)\n", "def test_sample_weight_invariance_multiclass_multilabel():\n    # multiclass multilabel\n    _, y_true = make_multilabel_classification(n_features=1, n_classes=5, random_state=0, n_samples=50)\n    _, y_pred = make_multilabel_classification(n_features=1, n_classes=5, random_state=1, n_samples=50)\n    y_score = check_random_state(0).random_sample(size=(50, 5))\n\n    for name in ALL_METRICS:\n        if name in REGRESSION_METRICS:\n            continue\n        if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or\n                name in METRIC_UNDEFINED_BINARY_MULTICLASS):\n            continue\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_MULTILABEL_METRICS:\n            yield check_sample_weight_invariance, name, metric, y_true, y_score\n        else:\n            yield check_sample_weight_invariance, name, metric, y_true, y_pred\n", "def test_pos_label_option():\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 2, size=(20, ))\n    y_score = random_state.random_sample(size=(20,))\n\n    for name in METRICS_WITH_POS_LABEL:\n        metrics = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            measure_default = metrics(y_true, y_score)\n            measure_pos_label = metrics(y_true, y_score, pos_label=1)\n            assert_almost_equal(measure_default, measure_pos_label)\n            measure_neg_label = metrics(y_true, y_score, pos_label=0)\n            assert_almost_equal(measure_default, 1 - measure_neg_label)\n        else:\n            assert_raises(ValueError, metrics, y_true, y_true, pos_label=1)\n\n", "def test_threshold_based_metrics_multilabel_classification():\n    # Test in the multilabel case\n    n_classes = 4\n    n_samples = 100\n\n    # for both random_state 0 and 1, y_true and y_score has at least one\n    # unlabelled entry\n    _, y_true = make_multilabel_classification(n_features=1,\n                                               n_classes=n_classes,\n                                               random_state=0,\n                                               allow_unlabeled=True,\n                                               n_samples=n_samples)\n    y_score = check_random_state(1).normal(size=(n_samples, n_classes))\n\n    # To make sure at least one empty label is present\n    y_true += [0]*n_classes\n\n    for name in THRESHOLDED_MULTILABEL_METRICS:\n        metric = ALL_METRICS[name]\n        measure = metric(y_true, y_score)\n\n        # Test with normalization\n        measure_normalized = metric(y_true, y_score, normalize=True)\n        assert_greater(measure_normalized, 0,\n                       msg=\"We failed to test correctly the normalize option\")\n        assert_greater_equal(measure_normalized, measure,\n                             msg=\"Normalized score should be greater or equal \"\n                                 \"to the non-normalized score\")\n\n        # Test with sample_weight\n        sample_weight = np.random.randint(1, 10, size=n_samples)\n        measure_weighted = metric(y_true, y_score, sample_weight=sample_weight)\n        assert_not_equal(measure, measure_weighted,\n                         msg=\"Scores with and without sample_weight should be \"\n                             \"different\")\n"], "sample_985": ["def test_assumptions():\n    x, y, z = symbols('x y z', real=True)\n    assert (x + y + z).is_real is True\n    assert (x + I*y + z).is_real is None\n\n    assert (x*y*z).is_positive is None\n    assert (x*y*z).is_negative is None\n    assert (x*y*z).is_nonnegative is None\n    assert (x*y*z).is_nonpositive is None\n\n    x, y, z = symbols('x y z', positive=True)\n    assert (x*y*z).is_positive is True\n    assert (x*y*z).is_negative is False\n    assert (x*y*z).is_nonnegative is True\n    assert (x*y*z).is_nonpositive is False\n\n    x, y, z = symbols('x y z', negative=True)\n    assert (x*y*z).is_positive is False\n    assert (x*y*z).is_negative is True\n    assert (x*y*z).is_nonnegative is False\n    assert (x*y*z).is_nonpositive is True\n", "def test_subclasses():\n    # Test subclassing of Basic\n    class MyBasicSubclass(Basic):\n        pass\n\n    subclass_instance = MyBasicSubclass(b2, b1)\n    assert isinstance(subclass_instance, MyBasicSubclass)\n    assert isinstance(subclass_instance, Basic)\n    assert subclass_instance.args == (b2, b1)\n    assert subclass_instance.func(*subclass_instance.args) == subclass_instance\n", "def test_subs_with_lambdas():\n    x, y = symbols('x y')\n    l = Lambda(x, 2*x)\n    expr = l + x\n    assert expr.subs(l, Lambda(x, 3*x)) == 3*x + x\n    assert expr.subs(l, Lambda(y, 3*y)) == l + x  # No substitution as l depends on x\n", "def test_eval_subs():\n    x, y = symbols('x y')\n    expr = x + y\n    new_expr = expr.subs({x: 2, y: 3})\n    assert new_expr == 5\n\n    expr = sin(x) + cos(x)\n    new_expr = expr.subs({sin(x): 0})\n    assert new_expr == cos(x)\n\n    expr = Min(x, y)\n    new_expr = expr.subs({x: 5, y: 3})\n    assert new_expr == 3\n\n    expr = Max(x, y)\n    new_expr = expr.subs({x: 5, y: 3})\n    assert new_expr == 5\n", "def test_is_commutative():\n    x, y = symbols('x y')\n    assert (x + y).is_commutative\n    assert (x * y).is_commutative\n    assert (sin(x) + cos(y)).is_commutative\n    assert not (sin(x) * cos(y)).is_commutative\n"], "sample_942": ["def test_pyexception_module_option(app):\n    text = (\".. py:exception:: IOError\\n\"\n            \"   :module: exceptions\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert 'exceptions.IOError' in domain.objects\n    assert domain.objects['exceptions.IOError'] == ('index', 'exceptions.IOError', 'exception', False)\n", "def test_pyfunction_signature_with_annotations(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"str\"])])\n", "def test_pycurrentmodule(app):\n    text = (\".. py:currentmodule:: sphinx\\n\"\n            \".. py:function:: func\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[1], desc, desctype=\"function\", domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0], desc_signature, ([desc_addname, \"sphinx.\"],\n                                                [desc_name, \"func\"],\n                                                [desc_parameterlist, ()]))\n    assert 'sphinx.func' in domain.objects\n    assert domain.objects['sphinx.func'] == ('index', 'sphinx.func', 'function', False)\n", "def test_pyfunction_signature_no_args(app):\n    text = \".. py:function:: hello() -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1], desc_parameterlist, '')\n", "def test_pyexception_subclass(app):\n    text = \".. py:exception:: exceptions.MyError\\n   :base-exception: IOError\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"MyError\"])],\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n\n    assert_node(doctree[1][0][2][0][0], nodes.field_name,\n                astext='Base exception')\n    assert_node(doctree[1][0][2][0][1][0][0], pending_xref,\n                refdomain=\"py\", reftype=\"exc\", reftarget=\"IOError\")\n\n    assert 'exceptions.MyError' in domain.objects\n    assert domain.objects['exceptions.MyError'] == ('index', 'exceptions.MyError', 'exception', False)\n"], "sample_818": ["def test_spectral_clustering_with_amg_eigen_tol():\n    # Test that spectral_clustering converges with AMG and eigen_tol\n    X, y = make_blobs(n_samples=200, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.5)\n    labels = spectral_clustering(X, n_clusters=2, random_state=0,\n                                eigen_solver='amg', eigen_tol=1e-5)\n    assert adjusted_rand_score(y, labels) == 1\n", "def test_spectral_clustering_with_different_solvers():\n    # Test that spectral_clustering gives the same results for different solvers\n    # (if they are available)\n    X, y = make_blobs(n_samples=100, random_state=42)\n\n    labels_arpack = spectral_clustering(X, n_clusters=3, eigen_solver='arpack', random_state=0)\n\n    if amg_loaded:\n        labels_amg = spectral_clustering(X, n_clusters=3, eigen_solver='amg', random_state=0)\n        assert adjusted_rand_score(labels_arpack, labels_amg) == 1\n    else:\n        assert_raises(ValueError, spectral_clustering, X, n_clusters=3, eigen_solver='amg', random_state=0)\n\n    try:\n        from sklearn.cluster import spectral_clustering as scikit_spectral_clustering\n        labels_lobpcg = scikit_spectral_clustering(X, n_clusters=3, eigen_solver='lobpcg', random_state=0)\n        assert adjusted_rand_score(labels_arpack, labels_lobpcg) == 1\n    except ImportError:\n        pass  # lobpcg is not available in the current environment\n", "def test_spectral_clustering_with_different_n_components():\n    # Test that spectral_clustering gives same results with different n_components\n    centers = np.array([\n        [0., 0., 0.],\n        [10., 10., 10.],\n        [20., 20., 20.],\n    ])\n    X, true_labels = make_blobs(n_samples=100, centers=centers,\n                                cluster_std=1., random_state=42)\n    D = pairwise_distances(X)  # Distance matrix\n    S = np.max(D) - D  # Similarity matrix\n    S = sparse.coo_matrix(S)\n\n    labels1 = spectral_clustering(S, n_clusters=3, n_components=2,\n                                  random_state=0)\n    labels2 = spectral_clustering(S, n_clusters=3, n_components=3,\n                                  random_state=0)\n\n    assert adjusted_rand_score(labels1, labels2) == 1\n", "def test_spectral_clustering_with_precomputed_affinity():\n    # Test that spectral_clustering works with precomputed affinity\n    # Create a precomputed affinity matrix\n    affinity = np.array([[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]])\n    labels = spectral_clustering(affinity, n_clusters=2, affinity='precomputed')\n    # Check that the labels are either 0 or 1\n    assert set(labels) == {0, 1}\n", "def test_spectral_clustering_sparse_matrix_input(eigen_solver):\n    S = sparse.coo_matrix([[1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                           [1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                           [1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                           [0.2, 0.2, 0.2, 1.0, 1.0, 1.0, 1.0],\n                           [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],\n                           [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],\n                           [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]])\n\n    model = SpectralClustering(random_state=0, n_clusters=2,\n                               affinity='precomputed',\n                               eigen_solver=eigen_solver).fit(S)\n    labels = model.labels_\n    if labels[0] == 0:\n        labels = 1 - labels\n\n    assert adjusted_rand_score(labels, [1, 1, 1, 0, 0, 0, 0]) == 1\n"], "sample_435": ["    def test_validates_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\n            \"password1\": \"testclient\",\n            \"password2\": \"testclient\",\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form[\"password2\"].errors), 2)\n        self.assertIn(\n            \"The password is too similar to the username.\", form[\"password2\"].errors\n        )\n        self.assertIn(\n            \"This password is too short. It must contain at least 12 characters.\",\n            form[\"password2\"].errors,\n        )\n", "def test_custom_email_field_email_case_sensitivity(self):\n    email = 'Test@Mail.com'\n    CustomEmailField.objects.create_user('test name', 'test password', email.lower())\n    form = PasswordResetForm({'email': email})\n    self.assertTrue(form.is_valid())\n    form.save()\n    self.assertEqual(form.cleaned_data['email'], email.lower())\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertEqual(mail.outbox[0].to, [email.lower()])\n", "    def test_cannot_change_password(self):\n        user = User.objects.get(username=\"testclient\")\n        user.set_unusable_password()\n        user.save()\n        form = UserChangeForm(instance=user)\n        self.assertNotIn('password', form.fields)\n", "    def test_custom_form_with_custom_username_field(self):\n        class CustomUserCreationForm(UserCreationForm):\n            class Meta(UserCreationForm.Meta):\n                model = CustomUser\n                fields = (\"email\", \"date_of_birth\")\n                field_classes = {\"email\": UsernameField}\n\n        data = {\n            \"email\": \"test@client222.com\",\n            \"password1\": \"testclient\",\n            \"password2\": \"testclient\",\n            \"date_of_birth\": \"1988-02-24\",\n        }\n        form = CustomUserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data[\"email\"], \"test@client222.com\")\n        self.assertEqual(form.cleaned_data[\"date_of_birth\"], datetime.date(1988, 2, 24))\n", "    def test_custom_user_model(self):\n        data = {\n            \"username\": \"testclient\",\n            \"password1\": \"testclient\",\n            \"password2\": \"testclient\",\n            \"email\": \"testclient@example.com\",\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.email, data[\"email\"])\n"], "sample_1136": ["def test_poly_as_coefficients_dict():\n    p = Poly(x**2 + 2*x*y + y**2, x, y)\n    assert p.as_coefficients_dict() == {(0, 0): 1, (1, 1): 1, (2, 0): 1}\n", "def test_poly_subs():\n    f = Poly(x**2 - y, x, y)\n    g = f.subs({x: z, y: z**2})\n    h = Poly(z**2 - z**2, z)\n    assert g == h\n", "def test_poly_copy_after_modification():\n    poly = Poly(x + y, x, y, z)\n    copy = poly.copy()\n    copy *= x\n    assert poly != copy, (\n        \"Modifying copy should not modify original.\")\n", "def test_poly_subs_with_matrix():\n    p = poly(x + y, x, y)\n    M = Matrix([[1, 2], [3, 4]])\n    result = p.subs({x: M[0, 0], y: M[1, 1]})\n    expected = poly(5, x, y)\n    assert result == expected, f\"Expected {expected}, but got {result}\"\n", "def test_poly_evaluate():\n    f = Poly(x**2 - 2*x + 1, x)\n    assert f.evaluate(3) == 2\n    assert f.evaluate(x=3) == 2\n    assert f.evaluate(x, 3) == 2\n    assert f.evaluate(2, x) == 1\n    assert f.evaluate(2, x=x) == 1\n    assert f.evaluate(x=x, y=2) == x**2 - 2*x + 1\n    assert f.evaluate(3, y=2) == 2\n    assert f.evaluate(3, y=x) == 2\n"], "sample_705": ["def test_linematcher_get_lines_after(pytester: Pytester) -> None:\n    lm = LineMatcher([\"foo\", \"bar\", \"baz\", \"qux\"])\n    assert lm.get_lines_after(\"bar\") == [\"baz\", \"qux\"]\n    with pytest.raises(ValueError, match=\"line 'not_present' not found in output\"):\n        lm.get_lines_after(\"not_present\")\n", "def test_pytester_assert_outcomes_deselected(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(True, reason=\"skipping\")\n            pass\n\n        @pytest.mark.xfail(reason=\"xfailing\")\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(xfailed=1, deselected=1)\n", "def test_pytester_parseconfig_plugins(pytester: Pytester) -> None:\n    pytester.plugins = [\"pytester\"]\n    config = pytester.parseconfig()\n    assert \"pytester\" in config.pluginmanager.list_name_plugins()\n", "def test_pytester_collect_by_name(pytester: Pytester) -> None:\n    modcol = pytester.getmodulecol(\"def test_func(): pass\\ndef test_other_func(): pass\")\n    item = pytester.collect_by_name(modcol, \"test_func\")\n    assert item is not None\n    assert item.name == \"test_func\"\n    nonexistent_item = pytester.collect_by_name(modcol, \"test_nonexistent\")\n    assert nonexistent_item is None\n", "def test_pytester_parseoutcomes_no_summary_line(pytester: Pytester) -> None:\n    \"\"\"Test parseoutcomes with no summary line in the output\"\"\"\n    outlines = [\"some\", \"normal\", \"output\"]\n    errlines = [\"some\", \"nasty\", \"errors\", \"happened\"]\n\n    with pytest.raises(ValueError, match=\"Pytest terminal summary report not found\"):\n        pytester_mod.RunResult.parse_summary_nouns(outlines)\n"], "sample_1047": ["def test_complex_reciprocal_real():\n    assert (1 / (4 - 3*I)).is_real is False\n", "def test_complex_reciprocal_zero():\n    assert (1 / (0 + 0*I)).is_zero is True\n", "def test_issue_9583():\n    x = Symbol('x', zero=True)\n    assert x.is_nonzero is None\n", "def test_issue_10500():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert (x + y*I).is_imaginary is None\n    assert (x + z*I).is_imaginary is None\n    assert (x + y).is_imaginary is False\n    assert (x + z).is_imaginary is None\n", "def test_issue_10302_simplified():\n    u = -(3*2**S.Pi)**(1/S.Pi) + 2*3**(1/S.Pi)\n    i = u + u*S.I\n    assert i.is_real is None\n    assert (u + i).is_zero is False\n    assert (1 + i).is_zero is False\n    a = Dummy('a', zero=True)\n    assert (a + S.I).is_zero is True\n    assert (a + S.I).is_imaginary is True\n"], "sample_1193": ["def test_are_coplanar():\n    from sympy.geometry import Plane\n\n    # Test with coplanar points\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    p3 = Point3D(7, 8, 9)\n    p4 = Point3D(10, 11, 12)\n    assert are_coplanar(p1, p2, p3, p4) is True\n\n    # Test with non-coplanar points\n    p5 = Point3D(0, 0, 0)\n    assert are_coplanar(p1, p2, p3, p5) is False\n\n    # Test with a plane and coplanar points\n    plane = Plane(p1, p2, p3)\n    assert are_coplanar(plane, p4) is True\n\n    # Test with a plane and non-coplanar points\n    assert are_coplanar(plane, p5) is False\n", "def test_are_coplanar():\n    from sympy.geometry import Plane\n\n    # Testing with coplanar points\n    p1, p2, p3, p4 = Point3D(1, 2, 3), Point3D(4, 5, 6), Point3D(7, 8, 9), Point3D(10, 11, 12)\n    plane = Plane(p1, p2, p3)\n    assert are_coplanar(p1, p2, p3, p4, plane)\n\n    # Testing with non-coplanar points\n    p5 = Point3D(13, 14, 15)\n    assert not are_coplanar(p1, p2, p3, p5, plane)\n\n    # Testing with Line3D entities\n    l1 = Line3D(p1, p2)\n    l2 = Line3D(p3, p4)\n    assert are_coplanar(l1, l2, plane)\n\n    l3 = Line3D(p1, p5)\n    assert not are_coplanar(l1, l3, plane)\n", "def test_are_coplanar():\n    from sympy.geometry import Plane\n\n    # Test coplanarity of 3D points\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    p3 = Point3D(7, 8, 9)\n    p4 = Point3D(10, 11, 12)\n\n    assert are_coplanar(p1, p2, p3) == False\n    assert are_coplanar(p1, p2, p3, p4) == True\n\n    # Test coplanarity with a Plane\n    plane = Plane(p1, p2, p3)\n    assert are_coplanar(p1, p2, plane) == True\n    assert are_coplanar(p1, p4, plane) == False\n\n    # Test coplanarity with 2D objects\n    p5 = Point2D(13, 14)\n    assert are_coplanar(p1, p2, p5) == True\n\n    # Test coplanarity with 3D objects\n    line1 = Line3D(p1, p2)\n    line2 = Line3D(p3, p4)\n    assert are_coplanar(line1, line2, plane) == True\n", "def test_are_coplanar():\n    # Test with collinear points\n    assert not are_coplanar(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2))\n\n    # Test with non-collinear points in the same plane\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0))\n\n    # Test with non-collinear points not in the same plane\n    assert not are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 0, 1))\n\n    # Test with a Plane and points in the same plane\n    p = Plane(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0))\n    assert are_coplanar(p, Point3D(0, 0, 0), Point3D(1, 1, 0))\n\n    # Test with a Plane and points not in the same plane\n    assert not are_coplanar(p, Point3D(0, 0, 0), Point3D(0, 0, 1))\n\n    # Test with LinearEntity3D objects\n    l1 = Line3D(Point3D(0, 0, 0), Point3D(1, 0, 0))\n    l2 = Line3D(Point3D(0, 0, 0), Point3D(0, 1, 0))\n    assert are_coplanar(l1, l2)\n\n    # Test with a mix of 2D and 3D objects\n    assert are_coplanar(Point2D(0, 0), Line3D(Point3D(1, 0, 0), Point3D(2, 0, 0)))\n\n    # Test with a mix of different types of objects\n    assert are_coplan", "def test_are_coplanar():\n    # Testing with 3D points\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 0, 0)\n    p3 = Point3D(0, 1, 0)\n    p4 = Point3D(0, 0, 1)\n    assert are_coplanar(p1, p2, p3) is True\n    assert are_coplanar(p1, p2, p4) is False\n\n    # Testing with 3D lines\n    line1 = Line3D(p1, p2)\n    line2 = Line3D(p1, p3)\n    line3 = Line3D(p1, p4)\n    assert are_coplanar(line1, line2) is True\n    assert are_coplanar(line1, line3) is False\n\n    # Testing with mix of 2D and 3D entities\n    p5 = Point(2, 2)\n    line4 = Line(p1, p5)\n    assert are_coplanar(line1, line4) is True\n"], "sample_666": ["def test_encodedfile_readline(tmpfile: BinaryIO) -> None:\n    tmpfile.write(b\"line1\\nline2\\n\")\n    tmpfile.seek(0)\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert ef.readline() == \"line1\\n\"\n    assert ef.readline() == \"line2\\n\"\n    assert ef.readline() == \"\"\n    tmpfile.close()\n", "def test_encodedfile_buffer_write(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.buffer.write(b\"line5\")\n    ef.flush()\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"line5\"\n    tmpfile.close()\n", "def test_capfd_sys_stdout_flush(capfd):\n    sys.stdout.write(\"test\")\n    sys.stdout.flush()\n    out, err = capfd.readouterr()\n    assert out == \"test\"\n", "def test_encodedfile_flush(capsys):\n    \"\"\"Flush on Encoded files should not raise an error.\"\"\"\n    sys.stdout.flush()\n    sys.stderr.flush()\n", "def test_encodedfile_flush(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.write(\"line1\")\n    ef.flush()\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"line1\"\n    ef.close()\n    with pytest.raises(ValueError):\n        ef.flush()\n    tmpfile.close()\n"], "sample_1115": ["def test_tensor_transpose():\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    H = TensorHead(\"H\", [L, L])\n\n    expr = H(i, j)\n    assert expr.transpose(i, j) == expr\n    assert expr.transpose(j, i) == H(j, i)\n\n    expr = H(i, j) + H(j, i)\n    assert expr.transpose(i, j) == expr\n    assert expr.transpose(j, i) == expr\n\n    expr = H(i, j) - H(j, i)\n    assert expr.transpose(i, j) == expr\n    assert expr.transpose(j, i) == -expr\n", "def test_tensor_index_type_dim_validation():\n    L = TensorIndexType(\"L\")\n    raises(ValueError, lambda: TensorIndexType(\"L\", dim=-2))\n    raises(ValueError, lambda: TensorIndexType(\"L\", dim=0))\n\n    L = TensorIndexType(\"L\", dim=2)\n    assert L.dim == 2\n\n    raises(ValueError, lambda: TensorIndexType(\"L\", dim=3))\n", "def test_tensorhead_kwargs():\n    with warns_deprecated_sympy():\n        A = tensorhead('A', [], commuting=1)\n        assert A.comm == 1\n        B = tensorhead('B', [], symmetric=[1, 2])\n        assert B.symmetry == TensorSymmetry.fully_symmetric(2)\n        C = tensorhead('C', [], antisymmetric=[0, 1])\n        assert C.symmetry == TensorSymmetry.fully_symmetric(-2)\n        D = tensorhead('D', [], commuting=2, symmetric=[0], antisymmetric=[1])\n        assert D.comm == 2\n        assert D.symmetry == TensorSymmetry.direct_product(1, -1)\n", "def test_tensor_simplify():\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A, B, C, D = tensor_heads(\"A B C D\", [L])\n    H = TensorHead(\"H\", [L, L])\n\n    expr = A(i)*B(i) + A(i)*B(-i)\n    simplified = expr.simplify()\n    assert simplified == A(i)*(B(i) + B(-i))\n\n    expr = A(i)*B(j) + A(i)*C(j)\n    simplified = expr.simplify()\n    assert simplified == A(i)*(B(j) + C(j))\n\n    expr = A(i)*B(j) - A(i)*C(j)\n    simplified = expr.simplify()\n    assert simplified == A(i)*(B(j) - C(j))\n\n    expr = A(i)*B(j) + A(j)*B(i)\n    simplified = expr.simplify()\n    assert simplified == A(i)*B(j) + A(j)*B(i)\n\n    expr = H(i, j) + H(j, i)\n    simplified = expr.simplify()\n    assert simplified == H(i, j) + H(j, i)\n\n    expr = H(i, j) - H(j, i)\n    simplified = expr.simplify()\n    assert simplified == H(i, j) - H(j, i)\n", "def test_tensor_rank_change():\n    L = TensorIndexType(\"L\")\n    i0, i1, i2, i3 = tensor_indices('i0:4', L)\n    A = TensorHead(\"A\", [L])\n    B = TensorHead(\"B\", [L]*2)\n\n    expr = B(i0, i1)*A(-i0)\n    assert expr.rank_change() == (1, 1)\n\n    expr = B(i0, i1)*A(i0)\n    assert expr.rank_change() == (0, 1)\n\n    expr = B(i0, i1)*A(i2)\n    assert expr.rank_change() == (0, 2)\n"], "sample_466": ["def test_serialize_date(self):\n    self.assertSerializedEqual(datetime.date(2014, 1, 1))\n    self.assertSerializedResultEqual(\n        datetime.date(2014, 1, 1),\n        (\"datetime.date(2014, 1, 1)\", {\"import datetime\"}),\n    )\n", "def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(3, 4),\n        (\"complex((3+4j))\", {\"import cmath\"}),\n    )\n", "def test_serialize_complex(self):\n    \"\"\"Test serialization of complex numbers.\"\"\"\n    complex_number = 1 + 2j\n    self.assertSerializedEqual(complex_number)\n    self.assertSerializedResultEqual(\n        complex_number,\n        (\"complex('1+2j')\", {\"import cmath\"}),\n    )\n", "def test_serialize_deconstructible(self):\n    \"\"\"\n    Test serialization of deconstructible objects.\n    \"\"\"\n    deconstructible_obj = DeconstructibleInstances()\n    string, imports = MigrationWriter.serialize(deconstructible_obj)\n    self.assertEqual(string, \"migrations.test_writer.DeconstructibleInstances()\")\n    self.assertEqual(imports, {\"import migrations.test_writer\"})\n", "def test_serialize_time(self):\n    \"\"\"\n    Make sure time objects can be serialized.\n    \"\"\"\n    t = time.localtime()\n    self.assertSerializedEqual(t)\n\n    # Make sure the serialized time can be constructed back into a time object.\n    serialized_t = self.serialize_round_trip(t)\n    self.assertEqual(time.struct_time, type(serialized_t))\n    self.assertEqual(t, serialized_t)\n\n    # Timezone-aware datetime objects are serialized to UTC.\n    t = datetime.datetime(2012, 1, 1, 1, 1, tzinfo=datetime.timezone.utc)\n    self.assertSerializedResultEqual(\n        t,\n        (\"datetime.datetime(2012, 1, 1, 1, 1, tzinfo=datetime.timezone.utc)\", {\"import datetime\"}),\n    )\n    self.assertSerializedEqual(t)\n"], "sample_486": ["def test_inlineformset_factory_nulls_default_pks_auto_parent_auto_child(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n    the case of a parent object with an AutoField primary key and a child\n    object with an AutoField primary key.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        AutoPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet()\n    self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n", "def test_inlineformset_factory_ignores_default_pks_on_submit_uuid_parent_auto_child(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_ignores_default_pks_on_submit\n    for the case of a parent object with a UUID primary key and a child object\n    with an AutoField primary key.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        UUIDPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet(\n        {\n            \"autopkchildofuuidpkparent_set-TOTAL_FORMS\": 3,\n            \"autopkchildofuuidpkparent_set-INITIAL_FORMS\": 0,\n            \"autopkchildofuuidpkparent_set-MAX_NUM_FORMS\": \"\",\n            \"autopkchildofuuidpkparent_set-0-name\": \"Foo\",\n            \"autopkchildofuuidpkparent_set-1-name\": \"\",\n            \"autopkchildofuuidpkparent_set-2-name\": \"\",\n        }\n    )\n    self.assertTrue(formset.is_valid())\n    self.assertIsNone(formset.instance.uuid)\n    self.assertIsNone(formset.forms[0].instance.parent_id)\n", "def test_inlineformset_factory_ignores_default_pks_on_submit_uuid_parent_auto_child(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_ignores_default_pks_on_submit for\n    the case of a parent object with a UUID primary key and a child object with\n    an AutoField primary key.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        UUIDPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet(\n        {\n            \"autopkchildofuuidpkparent_set-TOTAL_FORMS\": 3,\n            \"autopkchildofuuidpkparent_set-INITIAL_FORMS\": 0,\n            \"autopkchildofuuidpkparent_set-MAX_NUM_FORMS\": \"\",\n            \"autopkchildofuuidpkparent_set-0-name\": \"Foo\",\n            \"autopkchildofuuidpkparent_set-1-name\": \"\",\n            \"autopkchildofuuidpkparent_set-2-name\": \"\",\n        }\n    )\n    self.assertTrue(formset.is_valid())\n    self.assertIsNone(formset.instance.uuid)\n    self.assertIsNone(formset.forms[0].instance.parent_id)\n", "def test_inlineformset_factory_nulls_default_pks_auto_parent_auto_child(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n    the case of a parent object with an AutoField primary key and a child\n    object with an AutoField primary key.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        AutoPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet()\n    self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n", "def test_inlineformset_factory_nulls_default_pks_auto_parent_auto_child(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n    the case of a parent object with an AutoField primary key and a child\n    object with an AutoField primary key.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        AutoPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet()\n    self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n"], "sample_403": ["def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, limit_choices_to={\"field\": \"value\"}),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "    def test_references_field_by_foreign_key(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n        )\n        self.assertIs(operation.references_field(\"Other\", \"id\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"missing\", \"migrations\"), False)\n        self.assertIs(operation.references_field(\"Missing\", \"id\", \"migrations\"), False)\n", "def test_references_field_by_through_model(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\"Other\", through=models.ForeignKey(\"Through\", models.CASCADE)),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Through\", \"whatever\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "def test_references_field_by_to_fields_with_db_constraint(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, to_field=\"field\", db_constraint=False),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), False)\n", "def test_references_field_by_through_from_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\n            \"Other\", through=models.fields.related.ForeignObject(\"Through\", models.CASCADE, [\"from\"], [\"to\"])\n        ),\n    )\n    self.assertIs(operation.references_field(\"Model\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"from\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"to\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Through\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Through\", \"to\", \"migrations\"), True)\n"], "sample_1140": ["def test_Str_length():\n    assert pretty(Str('x').length()) == 'StrLength(x)'\n    assert upretty(Str('x').length()) == '|x|'\n", "def test_issue_18273():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    expr = A + B*C - D\n    assert upretty(expr) == \\\n    '\ud835\udc00 + \ud835\udc01\u22c5\ud835\udc02 - \ud835\udc03'\n", "def test_issue_18592():\n    x, y = symbols('x y')\n    expr = Sum(exp(x + y), (x, 0, 1), (y, 0, 1))\n    ascii_str = \\", "def test_issue_18414():\n    assert pretty(Sum(Sum(x*y, (y, 1, 2)), (x, 1, 2))) == \\\n    '  2    2   \\n'\\\n    ' ___   __  \\n'\\\n    ' \\\\  `  \\\\ ` \\n'\\\n    '  \\\\    y x \\n'\\\n    '  /__   /__\\n'\\\n    'x = 1 y = 1 \\n'\\\n    '           '\n\n    assert upretty(Sum(Sum(x*y, (y, 1, 2)), (x, 1, 2))) == \\\n    '  2    2   \\n'\\\n    ' ___   __  \\n'\\\n    ' \u2572     \u2572    \\n'\\\n    '  \u2572    y x \\n'\\\n    '  \u2571__  \u2571__\\n'\\\n    'x = 1 y = 1 '\n", "def test_issue_18670():\n    # Test for issue #18670\n    f = Function('f')\n    g = Function('g')\n    h = Function('h')\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    expr = Integral(f(x)*g(x)*h(x), (x, y, z))\n    ascii_str = \\"], "sample_682": ["def test_xfail_imperative_in_teardown_function(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(\"hello\")\n\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n    result = testdir.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_this*\", \"*reason:*hello*\"])\n    result = testdir.runpytest(p, \"--runxfail\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *def test_this*\n        *1 fail*\n        \"\"\"\n    )\n", "def test_marked_skipif_with_boolean(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(True, reason=\"boolean skip\")\n            pass\n        \"\"\"\n    )\n    skipped = evaluate_skip_marks(item)\n    assert skipped\n    assert skipped.reason == \"boolean skip\"\n", "def test_skip_exception_message(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.skip(\"Custom skip message\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*Custom skip message*\", \"*1 skipped*\"])\n", "def test_marked_skipif_with_invalid_syntax(testdir) -> None:\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n        \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"invalid syntax\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_skipif_class_with_unconditional_skip(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"True\", reason=\"unconditional skip\")\n        class TestClass(object):\n                assert 0\n                assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rs\")\n    result.stdout.fnmatch_lines([\"*SKIPPED*unconditional skip*\", \"*2 skipped*\"])\n"], "sample_679": ["def test_pytest_param_marks_requires_iterable() -> None:\n    with pytest.raises(TypeError) as excinfo:\n        pytest.param(marks=\"hello\")  # type: ignore[arg-type] # noqa: F821\n    (msg,) = excinfo.value.args\n    assert msg == \"Expected marks to be an iterable, got <class 'str'>: hello\"\n", "def test_pytest_mark_invalid_decorator(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.invalid_decorator(1, 2, 3)\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*TypeError: MarkDecorator() takes 0 or 1 positional argument but 3 were given*\"])\n", "def test_keyword_option_wrong_arguments_in_pytest_param(expr: str, expected_error: str, testdir, capsys) -> None:\n    testdir.makepyfile(\"\"\"def test_func(arg): pass\"\"\")\n    testdir.inline_run(\"-k\", pytest.param(expr))\n    err = capsys.readouterr().err\n    assert expected_error in err\n", "def test_addmarker_order_with_arguments():\n    session = mock.Mock()\n    session.own_markers = []\n    session.parent = None\n    session.nodeid = \"\"\n    node = Node.from_parent(session, name=\"Test\")\n    node.add_marker(pytest.mark.foo(arg1=\"value1\"))\n    node.add_marker(pytest.mark.bar(arg2=\"value2\"))\n    node.add_marker(pytest.mark.baz(arg3=\"value3\"), append=False)\n    extracted = [(x.name, x.kwargs) for x in node.iter_markers()]\n    assert extracted == [(\"baz\", {\"arg3\": \"value3\"}), (\"foo\", {\"arg1\": \"value1\"}), (\"bar\", {\"arg2\": \"value2\"})]\n", "def test_mark_evaluator_istrue(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.nodes import Item\n        from _pytest.mark import MarkEvaluator\n\n        @pytest.fixture\n            return Item.from_parent(parent=None, name=request.function.__name__)\n\n        @pytest.mark.truthy(True)\n            evaluator = MarkEvaluator(item, \"truthy\")\n            assert evaluator.istrue()\n\n        @pytest.mark.falsey(False)\n            evaluator = MarkEvaluator(item, \"falsey\")\n            assert not evaluator.istrue()\n\n        @pytest.mark.condition(\"1 == 1\")\n            evaluator = MarkEvaluator(item, \"condition\")\n            assert evaluator.istrue()\n\n        @pytest.mark.invalidcondition(\"1 == '1'\")\n            evaluator = MarkEvaluator(item, \"invalidcondition\")\n            assert not evaluator.istrue()\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=4)\n"], "sample_343": ["def test_get_content_type_with_obj(self):\n    question = Question.objects.create(text='What?')\n    ct = Answer.question.get_content_type(obj=question)\n    self.assertEqual(ct.model_class(), Question)\n", "def test_get_object_cache_respects_changed_object_id(self):\n    question = Question.objects.create(text='Who?')\n    post = Post.objects.create(title='Answer', parent=question)\n\n    question2 = Question.objects.create(text='What?')\n    post.object_id = question2.pk\n    post.save()\n\n    with self.assertNumQueries(1):\n        self.assertEqual(post.parent, question2)\n        self.assertEqual(post.parent, question2)\n", "    def test_generic_foreign_key_get_filter_kwargs_for_object(self):\n        question = Question.objects.create(text='Who?')\n        kwargs = Answer.question.get_filter_kwargs_for_object(question)\n        self.assertEqual(kwargs, {'content_type_id': question.content_type_id, 'object_id': question.id})\n", "def test_get_object_cache_respects_changed_content_type(self):\n    question = Question.objects.create(text='Who?')\n    post = Post.objects.create(title='Answer', parent=question)\n\n    question.content_type = ContentType.objects.get_for_model(Answer)\n    question.save()\n\n    post = Post.objects.get(pk=post.pk)\n    with self.assertNumQueries(1):\n        self.assertIsNotNone(post.parent)\n        self.assertIsInstance(post.parent, Question)\n        self.assertIsNone(post.parent)\n", "def test_get_object_cache_respects_modified_objects(self):\n    question1 = Question.objects.create(text='Who?')\n    question2 = Question.objects.create(text='Why?')\n    post = Post.objects.create(title='Answer', parent=question1)\n\n    question2.pk = question1.pk\n    question2.save()\n\n    post = Post.objects.get(pk=post.pk)\n    with self.assertNumQueries(1):\n        self.assertEqual(post.object_id, question1.pk)\n        self.assertIsInstance(post.parent, Question)\n        self.assertEqual(post.parent.text, 'Why?')\n"], "sample_1059": ["def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    alpha = Symbol(\"alpha\")\n\n    # Generalized Laguerre polynomials:\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(1, alpha, x) == alpha - x + 1\n    assert assoc_laguerre(2, alpha, x) == alpha**2/2 + x**2/2 - (alpha + 2)*x + 1\n\n    X = assoc_laguerre(n, alpha, x)\n    assert isinstance(X, assoc_laguerre)\n\n    assert assoc_laguerre(n, alpha, 0) == binomial(n + alpha, alpha)\n    assert assoc_laguerre(n, alpha, oo) == (-1)**n*oo\n    assert assoc_laguerre(n, alpha, -oo) == oo\n\n    assert conjugate(assoc_laguerre(n, alpha, x)) == assoc_laguerre(n, conjugate(alpha), conjugate(x))\n\n    _k = Dummy('k')\n\n    assert assoc_laguerre(n, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        gamma(n + alpha + 1)/factorial(n)*Sum(x**_k*RisingFactorial(-n, _k)/(gamma(_k + alpha + 1)*factorial(_k)), (_k, 0, n)))\n\n    assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - 1, alpha + 1, x)\n    assert diff(assoc_laguerre(n, alpha, x), alpha) == Sum(assoc_laguerre(_k, alpha, x)/(-alpha + n), (_k, 0, n - 1))\n\n    raises(ValueError, lambda: assoc_laguerre(-2.1, alpha, x))\n    raises(ValueError, lambda: assoc_laguerre(Rational(5, 2), alpha, x", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n\n    assert assoc_laguerre(0, a, x) == 1\n    assert assoc_laguerre(1, a, x) == a - x + 1\n    assert assoc_laguerre(2, a, x) == a**2/2 + 3*a/2 + x**2/2 + x*(-a - 2) + 1\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, a, 0) == binomial(n + a, a)\n\n    assert diff(assoc_laguerre(n, a, x), x) == -assoc_laguerre(n - 1, a + 1, x)\n    assert diff(assoc_laguerre(n, a, x), a) == Sum(assoc_laguerre(_k, a, x)/(-a + n), (_k, 0, n - 1))\n\n    _k = Dummy('k')\n    assert assoc_laguerre(n, a, x).rewrite(\"polynomial\").dummy_eq(\n        gamma(n + a + 1)/factorial(n)*Sum(x**_k*RisingFactorial(-n, _k)/(gamma(_k + a + 1)*factorial(_k)), (_k, 0, n)))\n\n    assert conjugate(assoc_laguerre(n, a, x)) == assoc_laguerre(n, a.conjugate(), x.conjugate())\n\n    raises(ValueError, lambda: assoc_laguerre(-1, a, x))\n    raises(ValueError, lambda: assoc_laguerre(Rational(5, 2), a, x))\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, a, x).fdiff(4))\n", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    alpha = Symbol(\"alpha\")\n\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(1, alpha, x) == alpha - x + 1\n    assert assoc_laguerre(2, alpha, x) == (alpha**2/2 + 3*alpha/2 + x**2/2 + x*(-alpha - 2) + 1)\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, alpha, 0) == binomial(alpha + n, alpha)\n    assert assoc_laguerre(n, alpha, oo) == (-1)**n * oo\n    assert assoc_laguerre(n, alpha, -oo) == oo\n\n    assert conjugate(assoc_laguerre(n, alpha, x)) == assoc_laguerre(n, alpha.conjugate(), x.conjugate())\n\n    _k = Dummy('k')\n\n    assert assoc_laguerre(n, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        gamma(n + alpha + 1) / factorial(n) * Sum(x**_k * RisingFactorial(-n, _k) / (gamma(_k + alpha + 1) * factorial(_k)), (_k, 0, n)))\n\n    assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - 1, alpha + 1, x)\n    assert diff(assoc_laguerre(n, alpha, x), alpha).dummy_eq(\n        Sum(assoc_laguerre(_k, alpha, x)/(-alpha + n), (_k, 0, n - 1)))\n\n    raises(ValueError, lambda: assoc_laguerre(-2.1, alpha, x))\n    raises(ValueError, lambda: assoc_laguerre(Rational(5, ", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    alpha = Symbol(\"alpha\")\n\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(1, alpha, x) == alpha - x + 1\n    assert assoc_laguerre(2, alpha, x) == alpha**2/2 + alpha*x - x**2/2 + 1\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, alpha, 0) == binomial(n + alpha, alpha)\n    assert assoc_laguerre(n, alpha, oo) == (-1)**n*oo\n    assert assoc_laguerre(n, alpha, -oo) == oo\n\n    assert conjugate(assoc_laguerre(n, alpha, x)) == assoc_laguerre(n, conjugate(alpha), conjugate(x))\n\n    assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - 1, alpha + 1, x)\n    assert diff(assoc_laguerre(n, alpha, x), alpha) == Sum(assoc_laguerre(_k, alpha, x)/(-alpha + n), (_k, 0, n - 1))\n\n    k = Symbol('k')\n    assert assoc_laguerre(-n, alpha, x) == exp(x)*assoc_laguerre(n - 1, alpha - n, -x)\n    assert unchanged(assoc_laguerre, -n + k, alpha, x)\n\n    raises(ValueError, lambda: assoc_laguerre(-2.1, alpha, x))\n    raises(ValueError, lambda: assoc_laguerre(Rational(5, 2), alpha, x))\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, alpha, x).fdiff(1))\n    raises(ArgumentIndexError, lambda: assoc_laguerre(", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    alpha = Symbol(\"alpha\")\n\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(1, alpha, x) == alpha - x + 1\n    assert assoc_laguerre(2, alpha, x) == alpha**2/2 + (3*alpha/2 - 2*x)*x + 1\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, alpha, 0) == binomial(alpha + n, alpha)\n    assert assoc_laguerre(n, alpha, oo) == (-1)**n * oo\n\n    assert conjugate(assoc_laguerre(n, alpha, x)) == assoc_laguerre(n, conjugate(alpha), conjugate(x))\n\n    assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - 1, alpha + 1, x)\n    assert diff(assoc_laguerre(n, alpha, x), alpha) == Sum(assoc_laguerre(_k, alpha, x)/(-alpha + n), (_k, 0, n - 1))\n\n    _k = Dummy('k')\n    assert assoc_laguerre(n, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        gamma(n + alpha + 1) / factorial(n) * Sum(x**_k * RisingFactorial(-n, _k) / (gamma(_k + alpha + 1) * factorial(_k)), (_k, 0, n)))\n\n    raises(ValueError, lambda: assoc_laguerre(-1, alpha, x))\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, alpha, x).fdiff(1))\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, alpha, x).fdiff(4))\n"], "sample_142": ["def test_duplicate_fields_in_fieldsets(self):\n    \"\"\"\n    Duplicate fields in fieldsets raise an error.\n    \"\"\"\n    class MyModelAdmin(admin.ModelAdmin):\n        fieldsets = [\n            (None, {\n                'fields': ['title', 'title']\n            }),\n        ]\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"There are duplicate field(s) in 'fieldsets[0][1]'.\",\n            obj=MyModelAdmin,\n            id='admin.E012'\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_display_with_missing_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = [\"pk\", \"nonexistent\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_display[1]' refers to 'nonexistent', which is \"\n            \"not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E107',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_readonly_fields_and_editable(self):\n    class SongAdmin(admin.ModelAdmin):\n        readonly_fields = (\"title\",)\n        list_display = [\"pk\", \"title\"]\n        list_editable = [\"title\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_editable[0]' refers to 'title', \"\n            \"which is not editable through the admin.\",\n            obj=SongAdmin,\n            id='admin.E125',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_editable_readonly_fields_together(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = [\"pk\", \"title\", \"original_release\"]\n        readonly_fields = [\"original_release\"]\n        fields = [\"title\", \"original_release\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'fields[1]' refers to 'original_release', which is not editable through the admin.\",\n            obj=SongAdmin,\n            id='admin.E125',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_custom_get_form_without_fieldsets(self):\n    \"\"\"\n    The fieldsets checks are not skipped when the ModelAdmin.get_form() method\n    is overridden and there are no fieldsets.\n    \"\"\"\n    class NoFieldsetsAdmin(admin.ModelAdmin):\n            class ExtraFieldForm(SongForm):\n                name = forms.CharField(max_length=50)\n            return ExtraFieldForm\n\n    errors = NoFieldsetsAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The model admin 'admin_checks.NoFieldsetsAdmin' has no fieldsets.\",\n            obj=NoFieldsetsAdmin,\n            id='admin.E009',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_124": ["def test_field_deep_copy_widget(self):\n    class CustomTextInput(TextInput):\n            kwargs['attrs'] = {'class': 'custom-input'}\n            super().__init__(**kwargs)\n\n    field = CharField(widget=CustomTextInput())\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsNot(field_copy.widget, field.widget)\n    self.assertIsInstance(field_copy.widget, CustomTextInput)\n    self.assertIsNot(field_copy.widget.attrs, field.widget.attrs)\n", "def test_attribute_instance_change(self):\n    custom = CustomRenderer()\n    form = Form(renderer=CustomRenderer())\n    form.renderer = custom\n    self.assertEqual(form.renderer, custom)\n", "def test_field_deep_copy_widget_attrs(self):\n    class CustomWidget(TextInput):\n            kwargs['attrs'] = {'class': 'custom-widget'}\n            super().__init__(**kwargs)\n\n    class CustomCharField(CharField):\n            kwargs['widget'] = CustomWidget()\n            super().__init__(**kwargs)\n\n    field = CustomCharField()\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CustomCharField)\n    self.assertIsNot(field_copy.widget, field.widget)\n    self.assertIsNot(field_copy.widget.attrs, field.widget.attrs)\n", "def test_field_deep_copy_widget_attrs(self):\n    class CustomCharField(CharField):\n            kwargs['widget'] = TextInput(attrs={'class': 'custom'})\n            super().__init__(**kwargs)\n\n    field = CustomCharField()\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CustomCharField)\n    self.assertIsNot(field_copy.widget.attrs, field.widget.attrs)\n", "def test_field_deep_copy_widget(self):\n    class CustomTextInput(TextInput):\n        template_name = 'custom_template.html'\n\n    class CustomCharField(CharField):\n        widget = CustomTextInput\n\n    field = CustomCharField()\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CustomCharField)\n    self.assertIsNot(field_copy.widget, field.widget)\n"], "sample_1011": ["def test_octave_user_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_g_matrix\"),\n              (lambda x: not x.is_Matrix, \"custom_g\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n           \"custom_f(x) + custom_g(x) + custom_g_matrix([1 x])\"\n", "def test_octave_gamma():\n    assert mcode(gamma(x)) == 'gamma(x)'\n    assert mcode(gamma(x, y)) == 'gamma(x, y)'\n    assert mcode(gamma(x, y, z)) == (\n        \"% Not supported in Octave:\\n\"\n        \"% gamma\\n\"\n        \"gamma(x, y, z)\"\n    )\n    assert mcode(gamma(z, x, y)) == (\n        \"% Not supported in Octave:\\n\"\n        \"% gamma\\n\"\n        \"gamma(z, x, y)\"\n    )\n", "def test_LambertW_printing():\n    assert mcode(LambertW(x, 0)) == 'lambertw(x, 0)'\n    assert mcode(LambertW(x, 1)) == 'lambertw(1, x)'\n    assert mcode(LambertW(x, -1)) == 'lambertw(-1, x)'\n    assert mcode(LambertW(x, 2)) == (\n        \"% Not supported in Octave:\\n\"\n        \"% LambertW\\n\"\n        \"LambertW(x, 2)\"\n    )\n", "def test_MatrixSlicing():\n    A = MatrixSymbol('A', 4, 4)\n    assert mcode(A[0:2, 1:3]) == 'A(1:2, 2:3)'\n    assert mcode(A[1:3, :]) == 'A(2:3, :)'\n    assert mcode(A[:, 0:2]) == 'A(:, 1:2)'\n    assert mcode(A[0:4:2, 0:4:2]) == 'A(1:2:end, 1:2:end)'\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert mcode(A[0:2, 0:2]) == \"A(1:2, 1:2)\"\n    assert mcode(A[:, 1]) == \"A(:, 2)\"\n    assert mcode(A[::2, ::2]) == \"A(1:2:end, 1:2:end)\"\n    assert mcode(A[:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[2:, :]) == \"A(3:end, :)\"\n"], "sample_186": ["def test_autocomplete_fields_missing_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ('test',)\n\n    self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' refers to 'test', which is not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E037',\n        )\n    ])\n", "def test_check_relation_invalid_fk_name(self):\n    class SongInline(admin.TabularInline):\n        model = Song\n        fk_name = 'invalid_fk'\n\n    class AlbumAdmin(admin.ModelAdmin):\n        model = Album\n        inlines = [SongInline]\n\n    errors = AlbumAdmin(Album, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"'admin_checks.Song' has no ForeignKey to 'admin_checks.Album'.\",\n            obj=SongInline,\n            id='admin.E202',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_autocomplete_fields(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ('title',)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' refers to 'title', which is not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E037',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_display_links_not_in_list_display(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = [\"pk\", \"title\"]\n        list_display_links = [\"album\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_display_links[0]' refers to 'album', which is not defined in 'list_display'.\",\n            obj=SongAdmin,\n            id='admin.E111',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_date_hierarchy_invalid_field(self):\n    \"\"\"\n    Test date_hierarchy with an invalid field\n    \"\"\"\n    class SongAdmin(admin.ModelAdmin):\n        date_hierarchy = \"invalid_field\"\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'date_hierarchy' refers to 'invalid_field', which does not refer to a Field.\",\n            obj=SongAdmin,\n            id='admin.E127',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_409": ["def test_trimmed(self, tag_name):\n    output = self.engine.render_to_string(\"template\")\n    self.assertEqual(output, \"Text with leading and trailing spaces\")\n", "def test_i18n42(self, tag_name):\n    output = self.engine.render_to_string(\"template\")\n    self.assertEqual(output, \"1 + 2\")\n", "def test_i18n_asvar_with_context(self):\n    context = {\"name\": \"World\"}\n    output = self.engine.render_to_string(\"i18n_asvar_with_context\", context=context)\n    self.assertEqual(output, \"Bonjour, World\")\n", "def test_context_var_not_string(self, tag_name):\n    msg = \"Invalid argument '{}' provided to the '{}' tag for the context option\".format(123, tag_name)\n    with self.assertRaisesMessage(TemplateSyntaxError, msg):\n        self.engine.render_to_string(\"template\", {\"context_var\": 123})\n", "def test_i18n42(self):\n    with translation.override(\"en\"):\n        output = self.engine.render_to_string(\"i18n42\")\n    self.assertEqual(output, \"1 item\")\n"], "sample_709": ["def test_pytester_assert_outcomes_deselected(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(True, reason=\"skipping this test\")\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1, deselected=1)\n    # If deselected is not passed, it is not checked at all.\n    result.assert_outcomes(skipped=1)\n", "def test_linematcher_consecutive_re_match() -> None:\n    lm = LineMatcher([\"1\", \"\", \"2\"])\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        lm.re_match_lines([\"1\", r\"\\d?\", \"2\"], consecutive=True)\n    assert str(excinfo.value).splitlines() == [\n        \"exact match: '1'\",\n        \"re.match: '\\\\d?'\",\n        \"    with: ''\",\n        \"no consecutive match: '2'\",\n    ]\n", "def test_pytester_runpython(pytester: Pytester) -> None:\n    script_content = \"import sys; print('Hello, pytester!')\"\n    script = pytester.makepyfile(script_content)\n    result = pytester.runpython(script)\n    assert result.stdout.lines == ['Hello, pytester!']\n    assert result.stderr.lines == []\n", "def test_makefile_creates_file_with_extension(pytester: Pytester) -> None:\n    p1 = pytester.makefile(\".txt\", \"test_content\")\n    assert p1.read_text() == \"test_content\"\n", "def test_pytester_inline_run_plugins(pytester: Pytester) -> None:\n    \"\"\"Test that plugins can be passed to `pytester.inline_run`.\"\"\"\n    pytester.makepyfile(\"def test_func(): pass\")\n\n    class TestPlugin:\n            session.custom_attr = \"plugin_value\"\n\n    reprec = pytester.inline_run(\"--collect-only\", plugins=[TestPlugin()])\n    session = reprec.getcalls(\"pytest_sessionstart\")[0].session\n    assert session.custom_attr == \"plugin_value\"\n"], "sample_362": ["def test_add_model_with_field_added_to_base_model(self):\n    \"\"\"\n    Adding a base field takes place after removing an inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'AddField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='book')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='title', model_name='readable')\n", "def test_alter_order_with_respect_to_removed(self):\n    \"\"\"\n    Removing order_with_respect_to sets it to None.\n    \"\"\"\n    changes = self.get_changes([self.book, self.author_with_book_order_wrt], [self.book, self.author_with_book])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=None)\n", "    def test_operation_with_invalid_suggested_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RunSQL('SELECT 1 FROM person;', name='invalid name'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertIs(migration.suggest_name().startswith('auto_'), True)\n", "def test_rename_field_with_default(self):\n    \"\"\"\n    #23609 - Tests autodetection of nullable to non-nullable alterations with a default value.\n    \"\"\"\n    changes = self.get_changes([self.author_name_null_default], [self.author_name], MigrationQuestioner({\"ask_rename\": True}))\n    self.assertEqual(mocked_ask_method.call_count, 0)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=models.NOT_PROVIDED)\n", "    def test_rename_model_to_existing_model(self):\n        \"\"\"\n        Test that renaming a model to an existing model does not create\n        a migration.\n        \"\"\"\n        before = [\n            ModelState('app', 'OldModel', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'NewModel', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'RenamedModel', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'NewModel', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename_model': True}))\n        self.assertNumberMigrations(changes, 'app', 0)\n"], "sample_659": ["def test_raises_with_raising_dunder_getattr(self):\n    \"\"\"Test current behavior with regard to exceptions via __getattr__ (#4284).\"\"\"\n\n    class CrappyGetAttr(Exception):\n            assert False, \"via __getattr__\"\n\n    with pytest.raises(\n        Failed,\n        match=r\"DID NOT RAISE <class 'raises(\\..*)*CrappyGetAttr'>\",\n    ):\n        pytest.raises(CrappyGetAttr, lambda: None)\n", "def test_raises_with_raising_dunder_module(self):\n    \"\"\"Test current behavior with regard to exceptions via __module__ (#4284).\"\"\"\n\n    class CrappyModuleException(Exception):\n        @property\n            assert False, \"via __module__\"\n\n    with pytest.raises(\n        Failed,\n        match=r\"DID NOT RAISE <class 'raises(\\..*)*CrappyModuleException'>\",\n    ):\n        pytest.raises(CrappyModuleException, lambda: None)\n", "def test_raises_exception_subclass(self):\n    class CustomValueError(ValueError):\n        pass\n\n    with pytest.raises(ValueError):\n        raise CustomValueError(\"CustomValueError\")\n", "def test_raises_with_raising_dunder_dict(self):\n    \"\"\"Test current behavior with regard to exceptions via __dict__ (#4284).\"\"\"\n\n    class CrappyDict(Exception):\n        @property\n            assert False, \"via __dict__\"\n\n    with pytest.raises(\n        Failed,\n        match=r\"DID NOT RAISE <class 'raises(\\..*)*CrappyDict'>\",\n    ):\n        pytest.raises(CrappyDict, lambda: None)\n", "def test_raises_match_exception_with_unicode_message(self):\n    msg = u\"Unicode: \u52a0\"\n    with pytest.raises(ValueError, match=msg):\n        raise ValueError(msg)\n\n    msg = u\"Unicode: \u52a0\"\n    with pytest.raises(ValueError, match=msg.encode(\"utf-8\")):\n        raise ValueError(msg)\n\n    msg = u\"Unicode: \u52a0\"\n    with pytest.raises(ValueError, match=msg):\n        raise ValueError(msg.encode(\"utf-8\"))\n"], "sample_74": ["def test_no_host_port(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n        }), (\n            ['psql', '-U', 'someuser', 'dbname'],\n            {},\n        )\n    )\n", "def test_no_ssl(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'dbname'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_no_ssl(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'dbname'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_no_ssl(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n            'sslmode': '',\n            'sslrootcert': '',\n            'sslcert': '',\n            'sslkey': '',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'dbname'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_ssl_without_certificate(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n            'sslmode': 'verify-ca',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'dbname'],\n            {\n                'PGPASSWORD': 'somepassword',\n                'PGSSLMODE': 'verify-ca',\n            },\n        )\n    )\n"], "sample_1180": ["def test_direction_cosine():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(2, 3, 5)\n    assert p1.direction_cosine(p2) == [sqrt(6)/6, sqrt(6)/6, sqrt(6)/3]\n", "def test_direction_cosine_direction_ratio():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(2, 3, 5)\n    assert p1.direction_cosine(p2) == [sqrt(6)/6, sqrt(6)/6, sqrt(6)/3]\n    assert p1.direction_ratio(p2) == [1, 1, 2]\n", "def test_point_orthogonal_direction():\n    assert Point(1, 0).orthogonal_direction == Point(0, 1)\n    assert Point(0, 1).orthogonal_direction == Point(-1, 0)\n    assert Point(1, 1).orthogonal_direction == Point(-1, 1)\n    assert Point(1, 1, 1).orthogonal_direction == Point(-1, 1, 0)\n", "def test_canberra_distance():\n    p1 = Point(1, 1)\n    p2 = Point(3, 3)\n    p3 = Point(0, 0)\n    p4 = Point(3, 3)\n\n    assert p1.canberra_distance(p2) == 1\n    assert p3.canberra_distance(p4) == 2\n    raises(ValueError, lambda: Point.canberra_distance(p3, p3))\n", "def test_point_properties():\n    p = Point(1, 2, 3)\n    assert p.ambient_dimension == 3\n    assert p.origin == Point(0, 0, 0)\n    assert p.length == 0\n    assert abs(p) == sqrt(14)\n"], "sample_385": ["def test_build_attrs_with_non_english_language(self):\n    form = AlbumForm()\n    attrs = form[\"band\"].field.widget.get_context(\n        name=\"my_field\", value=None, attrs={}\n    )[\"widget\"][\"attrs\"]\n    self.assertEqual(attrs[\"lang\"], \"fr\")\n", "def test_build_attrs_french_language(self):\n    form = AlbumForm()\n    attrs = form[\"band\"].field.widget.get_context(\n        name=\"my_field\", value=None, attrs={}\n    )[\"widget\"][\"attrs\"]\n    self.assertEqual(attrs[\"lang\"], \"fr\")\n", "def test_render_options_i18n(self):\n    beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n    with translation.override(\"de\"):\n        form = AlbumForm(initial={\"band\": beatles.uuid})\n        output = form.as_table()\n        attrs = form[\"band\"].field.widget.build_attrs({})\n        self.assertEqual(attrs[\"lang\"], \"de\")\n", "def test_i18n_build_attrs(self):\n    form = AlbumForm()\n    attrs = form[\"band\"].field.widget.build_attrs({})\n    self.assertEqual(attrs[\"lang\"], \"fr\")\n", "    def test_build_attrs_with_french_language(self):\n        translation.activate(\"fr\")\n        form = AlbumForm()\n        attrs = form[\"band\"].field.widget.get_context(\n            name=\"my_field\", value=None, attrs={}\n        )[\"widget\"][\"attrs\"]\n        self.assertEqual(attrs[\"lang\"], \"fr\")\n"], "sample_631": ["def test_allowed_redefined_builtin(self):\n    \"\"\"Test the --allowed-redefined-builtins option works.\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        open = 'redefined'\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_module(node.root())\n        self.checker.visit_functiondef(node)\n", "def test_possibly_unused_variable(self):\n    \"\"\"Ensure variables are marked as possibly unused when locals() is called.\"\"\"\n    node = astroid.parse(\n        \"\"\"\n        x = 1\n        print(locals())\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"possibly-unused-variable\", node=node.body[0].body[0], args=\"x\")\n    ):\n        self.walk(node)\n", "def test_undefined_variable_in_lambda(self):\n    \"\"\"Make sure undefined variables in lambda\n    raises Undefined-variable message\"\"\"\n    node = astroid.parse(\n        \"\"\"\n    lambda x: y + 1\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"undefined-variable\", node=node.body[0].value, args=\"y\")\n    ):\n        self.walk(node)\n", "def test_ignored_argument_names_function_annotation(self):\n    \"\"\"Make sure is_ignored_argument_names properly ignores\n    function argument annotations\"\"\"\n    node = astroid.parse(\n        \"\"\"", "def test_no_name_in_module_without_modname(self):\n    \"\"\"Make sure that 'from ... import ...' emits a\n    'no-name-in-module' without a modname.\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    from . import THIS_does_not_EXIST\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"no-name-in-module\", node=node, args=(\"THIS_does_not_EXIST\", \"\"))\n    ):\n        self.checker.visit_importfrom(node)\n"], "sample_919": ["def test_pointer_to_member():\n    check('member', 'int MyClass::* p', {1: 'p__MyClass::*i', 2: '1p'})\n    check('member', 'int MyClass::* const p', {1: 'p__MyClass::*iC', 2: '1p'}, output='int MyClass::* const p')\n    check('member', 'int MyClass::* volatile p', {1: 'p__MyClass::*iV', 2: '1p'}, output='int MyClass::* volatile p')\n    check('member', 'int MyClass::* const volatile p', {1: 'p__MyClass::*iVC', 2: '1p'}, output='int MyClass::* volatile const p')\n", "def test_build_domain_cpp_qualified_enum_in_type(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"qualified-enum-in-type\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:type reference target not found: Namespace::Enum\" in ws[0]\n", "def test_anon_class_in_template_arg():\n    check('class', 'template<typename T = std::shared_ptr<struct {}>> {key}A',\n          {2: 'I0E1A'}, output='template<typename T = std::shared_ptr<struct [anonymous]>> {key}A')\n", "def test_cpp_expr_role():\n        parser = DefinitionParser(text, location=None, config=None)\n        ast = parser.parse_expression()\n        res = ast.get_display_string()\n        if res != output:\n            print(\"\")\n            print(\"Input:    \", text)\n            print(\"Result:   \", res)\n            print(\"Expected: \", output)\n            assert False\n        signode = nodes.TextElement()\n        ast.describe_signature(signode, 'markType', None, None)\n        res = signode.astext()\n        if res != output:\n            print(\"\")\n            print(\"Input:    \", text)\n            print(\"Result:   \", res)\n            print(\"Expected: \", output)\n            assert False\n\n    check('cpp:expr', '42', '42')\n    check('cpp:expr', 'nullptr', 'nullptr')\n    check('cpp:expr', 'true', 'true')\n    check('cpp:expr', 'false', 'false')\n    check('cpp:expr', 'this', 'this')\n    check('cpp:expr', 'A::B::C', 'A::B::C')\n    check('cpp:expr', 'A::B<C, D>::E', 'A::B<C, D>::E')\n    check('cpp:expr', 'A::template B<C, D>::template E<F>', 'A::template B<C, D>::template E<F>')\n    check('cpp:expr', 'A::template B<C, D>::template operator()<E>', 'A::template B<C, D>::template operator()<E>')\n    check('cpp:expr', 'A::template operator()<B>', 'A::template operator()<B>')\n    check('cpp:expr', 'A::operator()', 'A::operator()')\n    check('cpp:expr', 'A::operator bool', 'A::operator bool')\n    check('cpp:expr', 'A::operator int', 'A::operator int')\n    check('cpp:expr', 'A::operator int*', 'A::operator int*')\n    check('cpp:", "def test_global_namespaces():\n    # test global namespaces\n    check('class', '{key}::A', {1: \"A\", 2: \"1A\"})\n    check('class', '{key}::A::B', {1: \"A::B\", 2: \"N1A1BE\"})\n    check('function', 'void f(::A a)', {1: \"f__A\", 2: \"1f1A\"})\n    check('function', 'void f(::A::B a)', {1: \"f__A::B\", 2: \"1fN1A1BE\"})\n"], "sample_967": ["def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'http://example.com/mathjax.js' in content\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'http://example.com/mathjax.js' in content\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://example.com/mathjax.js' in content\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'http://example.com/mathjax.js' in content\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'http://example.com/mathjax.js' in content\n"], "sample_318": ["    def test_valid_reverse_lookahead_positive(self):\n        self.assertEqual(reverse('lookahead-positive', kwargs={'city': 'a-city'}), '/lookahead+/a-city/')\n", "    def test_include_app_name_conflict(self):\n        msg = 'App namespaces must be unique.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include((self.url_patterns, 'app_name'), namespace='app_name')\n", "    def test_optional_parameters(self):\n        test_urls = [\n            ('optional-params', [], {}, '/optional-params/'),\n            ('optional-params', ['arg1'], {}, '/optional-params/arg1/'),\n            ('optional-params', [], {'param1': 'arg1'}, '/optional-params/arg1/'),\n            ('optional-params', ['arg1', 'arg2'], {}, '/optional-params/arg1/arg2/'),\n            ('optional-params', [], {'param1': 'arg1', 'param2': 'arg2'}, '/optional-params/arg1/arg2/'),\n        ]\n        for name, args, kwargs, expected in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs):\n                self.assertEqual(reverse(name, args=args, kwargs=kwargs), expected)\n", "def test_invalid_view_import(self):\n    msg = \"View does not exist in module urlpatterns_reverse.invalid_views.\"\n    with self.assertRaisesMessage(ViewDoesNotExist, msg):\n        get_callable('urlpatterns_reverse.invalid_views.nonexistent_view')\n", "    def test_include_trailing_dollar(self):\n        test_urls = [\n            (r'^test_include_trailing_dollar/$', False),\n            (r'^test_include_trailing_dollar$', True),\n        ]\n        for url_pattern, should_raise in test_urls:\n            with self.subTest(url_pattern=url_pattern):\n                p = RegexPattern(url_pattern)\n                if should_raise:\n                    with self.assertRaises(Warning):\n                        p._check_include_trailing_dollar()\n                else:\n                    self.assertEqual(p._check_include_trailing_dollar(), [])\n"], "sample_555": ["def test_annulus_large_angles():\n    fig, ax = plt.subplots()\n    ann = Annulus((0.5, 0.5), 0.2, 0.05, theta1=0, theta2=360)\n    ax.add_patch(ann)\n    ax.set_aspect('equal')\n", "def test_arc_setters():\n    arc = Arc((0, 0), 1, 1, theta1=0, theta2=90, angle=45)\n    assert arc.center == (0, 0)\n    assert arc.width == 1\n    assert arc.height == 1\n    assert arc.angle == 45\n    assert arc.theta1 == 0\n    assert arc.theta2 == 90\n\n    arc.center = (1, 1)\n    arc.width = 2\n    arc.height = 2\n    arc.angle = 90\n    arc.theta1 = 30\n    arc.theta2 = 120\n\n    assert arc.center == (1, 1)\n    assert arc.width == 2\n    assert arc.height == 2\n    assert arc.angle == 90\n    assert arc.theta1 == 30\n    assert arc.theta2 == 120\n", "def test_arc_in_collection_transform():\n    arc1 = Arc([.5, .5], .5, 1, theta1=0, theta2=60, angle=20, transform=mtransforms.Affine2D().rotate_deg(30))\n    arc2 = Arc([.5, .5], .5, 1, theta1=0, theta2=60, angle=20)\n    col = mcollections.PatchCollection(patches=[arc2], facecolors='none', edgecolors='k', transform=mtransforms.Affine2D().rotate_deg(30))\n    fig_ref = plt.figure()\n    fig_ref.subplots().add_patch(arc1)\n    fig_test = plt.figure()\n    fig_test.subplots().add_collection(col)\n    assert_array_almost_equal(fig_ref.canvas.renderer.tostring_rgba(), fig_test.canvas.renderer.tostring_rgba())\n", "def test_arc_default_params():\n    arc = Arc([0, 0], 1, 1)\n    assert arc.get_theta1() == 0\n    assert arc.get_theta2() == 360\n    assert arc.get_center() == (0, 0)\n    assert arc.get_width() == 1\n    assert arc.get_height() == 1\n    assert arc.get_angle() == 0\n", "def test_arc_get_transform():\n    arc = Arc([.5, .5], .5, 1, theta1=0, theta2=60, angle=20)\n    transform = arc.get_transform()\n    assert isinstance(transform, mtransforms.Affine2D)\n    assert np.allclose(transform.get_matrix(), np.eye(3))\n"], "sample_975": ["def test_nsolve_with_precision():\n    x = Symbol('x')\n    e1 = x**2 - pi\n    q = nsolve(e1, x, 3.0, prec=50)\n\n    assert abs(sqrt(pi).evalf(50) - q) < 1e-49\n", "def test_nsolve_with_precision():\n    x = Symbol('x')\n    eq = Eq(x**2, 2)\n    sol = nsolve(eq, x, 1, prec=50)\n    assert abs(sol - sqrt(2).evalf(50)) < 1e-50\n", "def test_nsolve_with_precision():\n    x = Symbol('x')\n    eq = Eq(sin(x), 0)\n    sol = nsolve(eq, x, 3, prec=50)\n    assert abs(sol - pi) < 1e-49\n", "def test_nsolve_precision():\n    # Test that nsolve can solve with higher precision\n    x = Symbol('x')\n    f = sin(x) - x\n    sol = nsolve(f, x, 3, prec=50)\n    assert abs(sol - pi.evalf(50)) < 1e-49\n", "def test_nsolve_overdetermined():\n    # test nsolve with overdetermined system\n    x, y = symbols('x y')\n    f1 = 2*x + y - 3\n    f2 = x - y - 1\n    f3 = x + y - 5  # this equation is redundant\n    f = Matrix((f1, f2, f3)).T\n    F = lambdify((x, y), f.T, modules='mpmath')\n    x0 = (1, 1)\n    x = nsolve(f, (x, y), x0)\n    assert mnorm(F(*x), 1) <= 1.e-10\n"], "sample_194": ["def test_opclasses_database_constraint(self):\n    UniqueConstraintProduct.objects.create(name='resum\u00e9', color='red')\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name='resume', color='red')\n", "def test_tablespace_database_constraint(self):\n    UniqueConstraintProduct.objects.using('other').create(name='p3', color='green')\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.using('other').create(name='p3', color='green')\n", "def test_opclasses_database_constraint(self):\n    UniqueConstraintProduct.objects.create(name='p1', color='red')\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name='p1', color='Red')  # 'Red' is a case variation of 'red'\n", "def test_opclasses_database_constraint(self):\n    UniqueConstraintProduct.objects.create(name='p1', color='red')\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name='p1', color='red')\n\n    constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\n    expected_name = 'name_color_uniq'\n    constraint = constraints[expected_name]\n    self.assertEqual(constraint['opclasses'], ['varchar_pattern_ops', 'varchar_pattern_ops'])\n", "def test_opclasses_database_constraint(self):\n    UniqueConstraintProduct.objects.create(name='p1', color='red')\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name='P1', color='red')\n"], "sample_236": ["def test_fast_delete_combined_relationships_multiple_objects(self):\n    # The cascading fast-delete of SecondReferrer should be combined\n    # in a single DELETE WHERE referrer_id OR unique_field, even if\n    # multiple objects are deleted.\n    origin = Origin.objects.create()\n    referer1 = Referrer.objects.create(origin=origin, unique_field=42)\n    referer2 = Referrer.objects.create(origin=origin, unique_field=43)\n    with self.assertNumQueries(2):\n        Referrer.objects.filter(unique_field__in=[42, 43]).delete()\n    self.assertEqual(Referrer.objects.count(), 0)\n", "def test_fast_delete_combined_relationships_with_keep_parents(self):\n    # The cascading fast-delete of SecondReferrer should be combined\n    # in a single DELETE WHERE referrer_id OR unique_field, and parent\n    # relationships should be kept.\n    origin = Origin.objects.create()\n    referer = Referrer.objects.create(origin=origin, unique_field=42)\n    SecondReferrer.objects.create(referrer=referer)\n    with self.assertNumQueries(2):\n        referer.delete(keep_parents=True)\n    self.assertTrue(Origin.objects.filter(id=origin.id).exists())\n    self.assertFalse(Referrer.objects.filter(id=referer.id).exists())\n    self.assertFalse(SecondReferrer.objects.exists())\n", "def test_fast_delete_with_do_nothing(self):\n    u = User.objects.create(avatar=Avatar.objects.create())\n    a = Avatar.objects.get(pk=u.avatar_id)\n    a.do_nothing_set.add(R.objects.create())\n    # 1 query to fast-delete the user\n    # 1 query to delete the avatar\n    # No query for R objects as they have DO_NOTHING behavior\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    self.assertTrue(R.objects.exists())\n", "def test_fast_delete_multiple_models(self):\n    # Test fast delete for multiple models in a single call\n    a = Avatar.objects.create(desc='a')\n    u1 = User.objects.create(avatar=a)\n    u2 = User.objects.create(avatar=a)\n    m2m_t = M2MTo.objects.create()\n    m2m_f = M2MFrom.objects.create()\n    m2m_f.m2m.add(m2m_t)\n\n    # 1 to delete a, 1 for fast-delete users, 1 for fast-delete m2m for m2m_f\n    self.assertNumQueries(3, Avatar.objects.filter(desc='a').delete)\n\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    self.assertFalse(M2MTo.objects.exists())\n    self.assertFalse(M2MFrom.m2m.through.objects.exists())\n", "def test_fast_delete_with_signals(self):\n        pass\n\n    models.signals.pre_delete.connect(noop, sender=User)\n\n    u = User.objects.create(avatar=Avatar.objects.create())\n    a = Avatar.objects.get(pk=u.avatar_id)\n\n    # Even if signals are connected, User can still be fast-deleted.\n    collector = Collector(using='default')\n    self.assertTrue(collector.can_fast_delete(u))\n\n    # However, the avatar deletion will not be fast-deleted due to the signal.\n    self.assertNumQueries(3, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n\n    models.signals.pre_delete.disconnect(noop, sender=User)\n"], "sample_443": ["def test_decr_version_non_existent(self):\n    with self.assertRaises(ValueError):\n        cache.decr_version(\"does_not_exist\")\n", "def test_incr_non_existent_key(self):\n    key = 'non_existent_key'\n    with self.assertRaises(ValueError):\n        cache.incr(key)\n", "def test_cache_with_same_alias_different_backends(self):\n    test_caches = CacheHandler(\n        {\n            \"default\": {\n                \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n            },\n            \"alternate\": {\n                \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n                \"LOCATION\": \"alternate\",\n            },\n        }\n    )\n    cache1 = test_caches[\"default\"]\n    cache2 = test_caches[\"alternate\"]\n\n    self.assertIsNot(cache1, cache2)\n    self.assertIsInstance(cache1, LocMemCache)\n    self.assertIsInstance(cache2, LocMemCache)\n", "def test_get_or_set_no_version(self):\n    msg = \"get_or_set() missing 1 required positional argument: 'default'\"\n    self.assertEqual(cache.get_or_set(\"brian\", 1979), 1979)\n    with self.assertRaisesMessage(TypeError, msg):\n        cache.get_or_set(\"brian\")\n    self.assertEqual(cache.get(\"brian\"), 1979)\n", "    def setUp(self):\n        self.original_caches = CACHES\n        CACHES = {\n            \"default\": {\n                \"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\",\n            },\n            \"alternate\": {\n                \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n                \"LOCATION\": \"alternate\",\n            },\n        }\n"], "sample_212": ["    def test_session_middleware(self):\n            return HttpResponse()\n\n        middleware = SessionMiddleware(get_response)\n        request = HttpRequest()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'session_key'\n\n        middleware.process_request(request)\n        self.assertIsNotNone(request.session)\n\n        response = middleware.process_response(request, HttpResponse())\n        self.assertIsNotNone(response.cookies[settings.SESSION_COOKIE_NAME])\n", "    def test_sync_to_async_calls_process_request_and_response(self):\n        \"\"\"\n        Test that the sync_to_async() function correctly calls the\n        process_request() and process_response() hooks.\n        \"\"\"\n        middleware = SimpleMiddleWare(get_response)\n        request = HttpRequest()\n        response = HttpResponse()\n\n        async_middleware = async_to_sync(middleware)(request)\n        self.assertTrue(hasattr(request, 'thread_and_connection'))\n\n        async_middleware(response)\n        self.assertTrue(hasattr(response, 'thread_and_connection'))\n", "    def test_process_response(self):\n        middleware = SimpleMiddleWare(get_response=async_to_sync(get_response))\n        request = HttpRequest()\n        request.session = self.client.session\n        request.session.save()\n        response = HttpResponse()\n        middleware.process_response(request, response)\n        self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n        self.assertEqual(request.session.session_key, response.cookies[settings.SESSION_COOKIE_NAME].value)\n", "    def test_sync_to_async_uses_base_thread_and_connection_for_middleware(self):\n        \"\"\"\n        The process_request() and process_response() hooks of the SessionMiddleware\n        must be called with the sync_to_async thread_sensitive flag enabled, so that\n        database operations use the correct thread and connection.\n        \"\"\"\n            \"\"\"Fake request_started/request_finished.\"\"\"\n            return (threading.get_ident(), id(connection))\n\n        async def get_response(request):\n            return HttpResponse()\n\n        request = HttpRequest()\n        middleware = SessionMiddleware(get_response)\n        middleware_instance = async_to_sync(middleware)(request)\n        self.assertIsInstance(middleware_instance, HttpResponse)\n        self.assertEqual(request.thread_and_connection, request_lifecycle())\n", "def test_session_middleware(self):\n    \"\"\"\n    Test that SessionMiddleware processes request and response correctly.\n    \"\"\"\n    request = HttpRequest()\n    request.COOKIES = {'sessionid': 'test_session_key'}\n\n    # Mocking the SessionStore\n    class MockSessionStore:\n            self.session_key = session_key or 'test_session_key'\n            self.accessed = True\n            self.modified = False\n\n            return False\n\n            return True\n\n            return 1209600  # 2 weeks\n\n            pass\n\n    middleware = SessionMiddleware(lambda r: HttpResponse())\n    middleware.SessionStore = MockSessionStore\n\n    middleware.process_request(request)\n    self.assertTrue(hasattr(request, 'session'))\n    self.assertIsInstance(request.session, MockSessionStore)\n\n    response = middleware.process_response(request, HttpResponse())\n    self.assertIsInstance(response, HttpResponse)\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n"], "sample_297": ["def test_ticket_24605_filter_with_values(self):\n    \"\"\"\n    Subquery table names should be quoted when using values().\n    \"\"\"\n    i1 = Individual.objects.create(alive=True)\n    RelatedIndividual.objects.create(related=i1)\n    i2 = Individual.objects.create(alive=False)\n    RelatedIndividual.objects.create(related=i2)\n    i3 = Individual.objects.create(alive=True)\n    i4 = Individual.objects.create(alive=False)\n\n    self.assertSequenceEqual(\n        Individual.objects.filter(Q(alive=False), Q(related_individual__isnull=True)).values('pk'),\n        [{'pk': i4.pk}]\n    )\n    self.assertSequenceEqual(\n        Individual.objects.exclude(Q(alive=False), Q(related_individual__isnull=True)).values('pk'),\n        [{'pk': i1.pk}, {'pk': i2.pk}, {'pk': i3.pk}]\n    )\n", "def test_ticket_24605_additional_scenario(self):\n    \"\"\"\n    Additional test scenario for subquery table names quoting.\n    \"\"\"\n    i5 = Individual.objects.create(alive=True)\n    RelatedIndividual.objects.create(related=i5)\n    i6 = Individual.objects.create(alive=True)\n\n    self.assertSequenceEqual(\n        Individual.objects.filter(Q(alive=True), Q(related_individual__isnull=False)).order_by('pk'),\n        [i5]\n    )\n    self.assertSequenceEqual(\n        Individual.objects.exclude(Q(alive=True), Q(related_individual__isnull=False)).order_by('pk'),\n        [i6]\n    )\n", "def test_ticket_24605_exclude_subquery(self):\n    \"\"\"\n    Subquery table names should be quoted when using exclude() with a subquery.\n    \"\"\"\n    i1 = Individual.objects.create(alive=True)\n    RelatedIndividual.objects.create(related=i1)\n    i2 = Individual.objects.create(alive=False)\n    RelatedIndividual.objects.create(related=i2)\n    i3 = Individual.objects.create(alive=True)\n    i4 = Individual.objects.create(alive=False)\n\n    self.assertSequenceEqual(\n        Individual.objects.exclude(related_individual__isnull=False, alive=True),\n        [i2, i4]\n    )\n", "def test_related_fk_with_complex_query(self):\n    # Test filtering on a related fk with a complex query\n    a1 = Ticket23605A.objects.create()\n    a2 = Ticket23605A.objects.create()\n    c1 = Ticket23605C.objects.create(field_c0=20000.0)\n    Ticket23605B.objects.create(\n        field_b0=20000.0, field_b1=True,\n        modelc_fk=c1, modela_fk=a1)\n    complex_q = Q(ticket23605b__field_b0__gte=2000000 / F(\"ticket23605b__modelc_fk__field_c0\"))\n    qs1 = Ticket23605A.objects.filter(complex_q)\n    self.assertSequenceEqual(qs1, [a1])\n    qs2 = Ticket23605A.objects.exclude(complex_q)\n    self.assertSequenceEqual(qs2, [a2])\n", "def test_annotation_tag_filter_with_queryset(self):\n    tag1 = Tag.objects.create(name='tag1')\n    tag2 = Tag.objects.create(name='tag2')\n    ann1 = Annotation.objects.create(name='ann1', tag=tag1)\n    ann2 = Annotation.objects.create(name='ann2', tag=tag2)\n    tag_qs = Tag.objects.filter(name='tag1')\n    self.assertSequenceEqual(Annotation.objects.filter(tag=tag_qs), [ann1])\n"], "sample_156": ["def test_custom_renderer_class(self):\n    class CustomForm(Form):\n        renderer = CustomRenderer\n\n    form = CustomForm()\n    self.assertIsInstance(form.renderer, CustomForm.renderer)\n", "    def test_attribute_instance_kwarg(self):\n        custom = CustomRenderer()\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        form = CustomForm(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n", "def test_attribute_subclass(self):\n    class CustomForm(Form):\n        default_renderer = DjangoTemplates\n\n    class SubForm(CustomForm):\n        default_renderer = CustomRenderer\n\n    form = SubForm()\n    self.assertIsInstance(form.renderer, SubForm.default_renderer)\n", "def test_field_deep_copy_widget(self):\n    class CustomTextInput(TextInput):\n            super().__init__(attrs={'class': 'custom-class'}, **kwargs)\n\n    field = CharField(widget=CustomTextInput)\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsInstance(field_copy.widget, CustomTextInput)\n    self.assertIsNot(field_copy.widget, field.widget)\n    self.assertIsNot(field_copy.widget.attrs, field.widget.attrs)\n", "def test_field_deep_copy_widget(self):\n    class CustomTextInput(TextInput):\n        custom_attr = 'Custom attribute'\n\n    field = CharField(widget=CustomTextInput())\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsInstance(field_copy.widget, CustomTextInput)\n    self.assertIsNot(field_copy.widget, field.widget)\n    self.assertEqual(field_copy.widget.custom_attr, 'Custom attribute')\n"], "sample_452": ["def test_references_field_by_through_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\"Other\", through_fields=(\"from\", \"to\")),\n    )\n    self.assertIs(operation.references_field(\"Model\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"from\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), False)\n", "def test_references_field_by_many_to_many_field(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ManyToManyField(\"Other\")\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_db_constraint(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.OneToOneField(\"Other\", models.CASCADE, db_constraint=False),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "    def test_add_field_references_model(self):\n        operation = migrations.AddField(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE),\n        )\n        self.assertIs(operation.references_model(\"Other\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"Model\", \"migrations\"), False)\n        self.assertIs(operation.references_model(\"Missing\", \"migrations\"), False)\n", "    def test_references_field_by_base_field(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.OneToOneField(\"Other\", models.CASCADE)\n        )\n        self.assertIs(operation.references_field(\"Other\", \"id\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n"], "sample_1120": ["def test_matrix_element_invalid_index():\n    A = MatrixSymbol('A', 2, 3)\n    raises(IndexError, lambda: A[2, 3])\n    raises(IndexError, lambda: A[2, -1])\n    raises(IndexError, lambda: A[-1, 3])\n    raises(IndexError, lambda: A[-1, -1])\n", "def test_MatrixElement_shape():\n    A = MatrixSymbol('A', n, m)\n    A_elem = A[i, j]\n    assert A_elem.shape == ()\n    assert A_elem.rows == 1\n    assert A_elem.cols == 1\n", "def test_MatrixSymbol_invalid_shape():\n    raises(ValueError, lambda: MatrixSymbol('A', n, -m))\n    raises(ValueError, lambda: MatrixSymbol('A', -n, m))\n    raises(ValueError, lambda: MatrixSymbol('A', n, m*n))\n    raises(ValueError, lambda: MatrixSymbol('A', n*m, m))\n", "def test_matrix_inverse_mul():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    assert Inverse(A)*A == Identity(n)\n    assert A*Inverse(A) == Identity(n)\n    assert Inverse(A)*B*A == A.inv()*B*A\n    assert A*Inverse(B)*B == A*Inverse(B)*B\n", "def test_MatrixElement_simplify():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert simplify(A[0, 0] * B[0, 0]) == (A * B)[0, 0]\n    assert simplify(A[0, 0] * B[1, 0]) != (A * B)[0, 0]\n"], "sample_34": ["def test_unit_operations_with_quantities():\n    \"\"\"Test operations between units and quantities.\"\"\"\n    q1 = 5 * u.m\n    q2 = 10 * u.cm\n    assert q1 + q2 == 5.1 * u.m\n    assert q1 - q2 == 4.9 * u.m\n    assert q1 * q2 == 0.05 * u.m ** 2\n    assert q1 / q2 == 50 * u.dimensionless_unscaled\n    assert q1 ** 2 == 25 * u.m ** 2\n", "def test_unit_division_with_string():\n    \"\"\"Check that division with strings produces the correct unit.\"\"\"\n    u1 = u.cm\n    us = 'kg'\n    assert u1 / us == u1 / u.Unit(us)\n", "def test_equivalencies():\n    equivalencies = [(u.m, u.foot, 3.28084, lambda x: x / 3.28084)]\n    assert (u.foot.to(u.m, equivalencies=equivalencies) == 1.0 * u.m)\n    assert (u.m.to(u.foot, equivalencies=equivalencies) == 3.28084 * u.foot)\n\n    # Test invalid equivalency\n    with pytest.raises(ValueError):\n        u.m.to(u.s, equivalencies=equivalencies)\n", "def test_compose_with_string_input():\n    # Test that the compose method works correctly with string input\n    composed = u.m.compose(units=[\"cm\", \"mm\"])\n    assert any(unit.is_equivalent(\"mm\") for unit in composed)\n    assert any(unit.is_equivalent(\"cm\") for unit in composed)\n", "def test_compose_with_equivalencies():\n    # Issue #2578\n    from astropy.units import equivalencies\n\n    # Define a custom equivalency\n        if u1 == u.m and u2 == u.km:\n            return u1 / 1000.0\n        elif u1 == u.km and u2 == u.m:\n            return u1 * 1000.0\n\n    # Create a custom unit\n    custom_unit = u.def_unit(\"custom_unit\", u.m)\n\n    # Test compose with the custom equivalency\n    result = custom_unit.compose(equivalencies=[custom_equivalency], units=[u.km])\n    assert result[0].bases[0] is u.km\n    assert result[0].scale == 0.001\n\n    # Test compose with the custom equivalency and a predefined equivalency\n    result = custom_unit.compose(equivalencies=[custom_equivalency, u.spectral()], units=[u.Hz])\n    assert len(result) > 0  # There should be at least one valid composition\n"], "sample_368": ["def test_atomic_migration_rollback_on_error(self):\n    \"\"\"\n    An atomic migration is properly rolled back if it raises an error.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    with self.assertRaisesMessage(RuntimeError, \"Abort migration\"):\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n    migrations_apps = executor.loader.project_state((\"migrations\", \"0001_initial\")).apps\n    Publisher = migrations_apps.get_model(\"migrations\", \"Publisher\")\n    self.assertFalse(Publisher.objects.exists())\n    self.assertTableNotExists(\"migrations_book\")\n", "def test_detect_soft_applied_add_field_foreignkey(self):\n    \"\"\"\n    executor.detect_soft_applied() detects ForeignKey columns from an AddField operation.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    executor.migrate([(\"migrations\", None)], fake=True)\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableExists(\"migrations_book\")\n    self.assertColumnExists(\"migrations_book\", \"author_id\")\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    # Leave the table for author but remove the foreign key column.\n    with connection.schema_editor() as editor:\n        editor.execute(editor.sql_delete_column % {\"table\": \"migrations_book\", \"name\": \"author_id\"})\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n    # Cleanup by removing the remaining table.\n    with connection.schema_editor() as editor:\n        editor.execute(editor.sql_delete_table % {\"table\": \"migrations_author\"})\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n", "def test_atomic_operation_in_atomic_migration(self):\n    \"\"\"\n    An atomic operation is properly rolled back inside an atomic migration.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    with self.assertRaisesMessage(RuntimeError, \"Abort migration\"):\n        executor.migrate([(\"migrations\", \"0001_atomic\")])\n    migrations_apps = executor.loader.project_state((\"migrations\", \"0001_atomic\")).apps\n    Editor = migrations_apps.get_model(\"migrations\", \"Editor\")\n    self.assertFalse(Editor.objects.exists())\n    # Record previous migration as successful.\n    executor.migrate([(\"migrations\", \"0001_atomic\")], fake=True)\n    # Rebuild the graph to reflect the new DB state.\n    executor.loader.build_graph()\n    # Migrating backwards is also atomic.\n    with self.assertRaisesMessage(RuntimeError, \"Abort migration\"):\n        executor.migrate([(\"migrations\", None)])\n    self.assertFalse(Editor.objects.exists())\n", "def test_deferred_sql(self):\n    \"\"\"\n    Tests deferred SQL is properly applied and rolled back.\n    \"\"\"\n    class MigrationWithDeferredSQL(migrations.Migration):\n        operations = [\n            migrations.RunSQL(\n                \"CREATE TABLE deferred_test (id INT);\",\n                reverse_sql=migrations.RunSQL.noop,\n            ),\n        ]\n\n    executor = MigrationExecutor(connection)\n    self.assertTableNotExists(\"deferred_test\")\n    # Apply the migration with deferred SQL\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n    executor.apply_migration(\n        ProjectState(),\n        MigrationWithDeferredSQL(\"0001_deferred_sql\", \"migrations\"),\n    )\n    self.assertTableExists(\"deferred_test\")\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Migrate backwards, deferred SQL should be rolled back\n    executor.migrate([(\"migrations\", None)])\n    self.assertTableNotExists(\"deferred_test\")\n", "def test_migrate_backwards_with_missing_replacement_migration(self):\n    \"\"\"\n    Migrate backwards to a replacement migration that is not in the graph.\n\n    This can happen if a squash migration is pushed to a deployment that already\n    has all its replaced migrations applied.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    executor.loader.graph.add_node(('migrations', '0001_squashed_0002'), None)\n    executor.recorder.record_applied('migrations', '0001_squashed_0002')\n    executor.recorder.record_applied('migrations', '0001_initial')\n    executor.recorder.record_applied('migrations', '0002_second')\n    try:\n        self.assertTableExists('migrations_author')\n        self.assertTableExists('migrations_book')\n        executor.migrate([('migrations', '0001_initial')])\n        self.assertTableExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n    finally:\n        # Cleanup\n        executor.migrate([('migrations', None)])\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n"], "sample_994": ["def test_issue_10368():\n    a = S(32442016954)/78058255275\n    assert type(int(a)) is type(int(-a)) is int\n", "def test_Float_precision():\n    # Make sure Float inputs for keyword args work\n    assert Float('1.0', dps=Float(15))._prec == 53\n    assert Float('1.0', precision=Float(15))._prec == 15\n    assert type(Float('1.0', precision=Float(15))._prec) == int\n    assert sympify(srepr(Float('1.0', precision=15.0))) == Float('1.0', precision=15)\n", "def test_Float_imaginary():\n    assert Float(1).is_imaginary is False\n    assert Float(1j).is_imaginary is True\n    assert Float('1').is_imaginary is False\n    assert Float('1j').is_imaginary is True\n", "def test_Float_subclass():\n    class CustomFloat(Float):\n        pass\n\n    cf = CustomFloat('1.23', 3)\n    assert isinstance(cf, Float)\n    assert isinstance(cf, CustomFloat)\n    assert cf == Float('1.23', 3)\n", "def test_Number_sympify():\n    assert sympify(1) is S.One\n    assert sympify(1.5) == Float(1.5)\n    assert sympify('1') is S.One\n    assert sympify('1.5') == Float('1.5')\n    assert sympify('1/2') == S.Half\n    assert sympify(Rational(1, 2)) == S.Half\n    assert sympify(Float(1.5)) == Float(1.5)\n    assert sympify(Integer(1)) is S.One\n    assert sympify(nan) is S.NaN\n    assert sympify(oo) is S.Infinity\n    assert sympify(-oo) is S.NegativeInfinity\n    assert sympify(I) is S.ImaginaryUnit\n    assert sympify(pi) is S.Pi\n    assert sympify(E) is S.Exp1\n    assert sympify(GoldenRatio) is S.GoldenRatio\n    assert sympify(EulerGamma) is S.EulerGamma\n    assert sympify(Catalan) is S.Catalan\n"], "sample_339": ["def test_inlineformset_factory_with_custom_queryset(self):\n    author = Author.objects.create(name='Charles Baudelaire')\n    book1 = Book.objects.create(author=author, title='Les Paradis Artificiels')\n    book2 = Book.objects.create(author=author, title='Les Fleurs du Mal')\n    BookFormSet = inlineformset_factory(\n        Author,\n        Book,\n        fields='__all__',\n        queryset=Book.objects.filter(title='Les Paradis Artificiels'),\n    )\n    formset = BookFormSet(instance=author)\n    self.assertEqual(len(formset), 1)\n    self.assertEqual(formset.forms[0].initial['title'], 'Les Paradis Artificiels')\n", "def test_modelformset_factory_with_custom_save(self):\n    class CustomSaveBook(Book):\n            self.title = self.title.upper()\n            super().save(*args, **kwargs)\n\n    BookFormSet = modelformset_factory(CustomSaveBook, fields=\"__all__\", extra=1)\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '',\n        'form-0-title': 'test_book',\n    }\n    formset = BookFormSet(data)\n    self.assertTrue(formset.is_valid())\n    saved = formset.save()\n    self.assertEqual(len(saved), 1)\n    book1, = saved\n    self.assertEqual(book1.title, 'TEST_BOOK')\n", "def test_model_formset_factory_can_order(self):\n    AuthorFormSet = modelformset_factory(Author, fields='__all__', can_order=True)\n    formset = AuthorFormSet()\n    self.assertIn('ORDER', formset.forms[0].fields)\n    self.assertEqual(formset.forms[0].fields['ORDER'].widget.attrs['class'], 'form-control')\n", "def test_modelformset_factory_with_non_model_form(self):\n    class CustomForm(forms.Form):\n        title = forms.CharField()\n\n    with self.assertRaisesMessage(ValueError, \"ModelFormSet view can only be used with ModelForm.\"):\n        modelformset_factory(Book, form=CustomForm)\n", "def test_inlineformset_factory_disabled_forms(self):\n    author = Author.objects.create(name='Charles Baudelaire')\n    BookFormSet = inlineformset_factory(Author, Book, fields=\"__all__\", extra=1, can_delete=True)\n    formset = BookFormSet(instance=author, initial=[{'title': 'Les Fleurs du Mal', 'author': author, 'DELETE': True}])\n    self.assertEqual(len(formset.forms), 2)\n    self.assertTrue(formset.forms[0].fields['DELETE'].disabled)\n    self.assertFalse(formset.forms[1].fields['DELETE'].disabled)\n"], "sample_598": ["def test_array_repr_with_name():\n    ds = xr.Dataset(coords={\"foo\": [1, 2, 3], \"bar\": [1, 2, 3]})\n    ds[(1, 2)] = xr.DataArray([0], dims=\"test\", name=\"test_variable\")\n    actual = formatting.array_repr(ds[(1, 2)])\n    expected = dedent(\n        \"\"\"\\\n        <xarray.DataArray 'test_variable' (1, 2) (test: 1)>\n        array([0])\n        Dimensions without coordinates: test\"\"\"\n    )\n\n    assert actual == expected\n", "def test_inline_variable_array_repr_large_array():\n    large_array = np.random.randn(1000000)\n    variable = xr.Variable(\"large\", large_array)\n    max_width = 50\n    expected_output = f\"[{large_array.size} values with dtype={large_array.dtype}]\"\n    actual_output = formatting.inline_variable_array_repr(variable, max_width=max_width)\n    assert actual_output == expected_output\n", "def test_inline_variable_array_repr_dask_array():\n    import dask.array as da\n\n    value = da.from_array(np.random.randn(10, 10), chunks=(5, 5))\n    variable = xr.Variable(\"x\", value)\n\n    max_width = 30\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    expected = \"dask.array<chunksize=(5, 5)>\".ljust(max_width)[:max_width]\n    assert actual == expected\n", "def test_format_array_flat_large_array():\n    array = np.arange(10000)\n    max_width = 20\n    actual = formatting.format_array_flat(array, max_width)\n    expected = \"0 ... 9999\"\n    assert expected == actual\n", "def test_inline_variable_array_repr_large_data():\n    data = np.random.rand(1000000)\n    da = xr.DataArray(data)\n\n    max_width = 20\n    result = formatting.inline_variable_array_repr(da, max_width)\n\n    expected = \"[1000000 values with dtype=float64]\"\n    assert result == expected\n"], "sample_396": ["    def test_ticket_24605_additional_cases(self):\n        \"\"\"\n        Additional test cases for ticket 24605 to ensure robustness.\n        \"\"\"\n        i5 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i5)\n        i6 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=True), Q(related_individual__isnull=False)\n            ),\n            [i5],\n        )\n        self.assertSequenceEqual(\n            Individual.objects.exclude(\n                Q(alive=True), Q(related_individual__isnull=False)\n            ).order_by(\"pk\"),\n            [i1, i2, i3, i4, i6],\n        )\n", "def test_filter_with_fk_lookup(self):\n    school = School.objects.create()\n    teacher = Teacher.objects.create(school=school)\n    student = Student.objects.create(school=school)\n\n    qs = Student.objects.filter(school__teacher=teacher)\n    self.assertSequenceEqual(qs, [student])\n", "    def test_ticket_24722(self):\n        \"\"\"\n        QuerySet.exclude() should not apply a default ordering.\n        \"\"\"\n        NamedCategory.objects.create(name=\"B\")\n        NamedCategory.objects.create(name=\"A\")\n        qs = NamedCategory.objects.exclude(name=\"B\")\n        self.assertEqual(qs[0].name, \"A\")\n", "def test_filter_with_pk_in_subquery(self):\n    tag1 = Tag.objects.create(name=\"tag1\")\n    tag2 = Tag.objects.create(name=\"tag2\")\n    note1 = Note.objects.create(note=\"note1\", misc=\"misc1\", tag=tag1)\n    note2 = Note.objects.create(note=\"note2\", misc=\"misc2\", tag=tag2)\n    subquery = Note.objects.filter(tag__name=\"tag1\").values_list(\"pk\", flat=True)\n    qs = Note.objects.filter(pk__in=subquery)\n    self.assertSequenceEqual(qs, [note1])\n", "    def test_ticket_24820(self):\n        \"\"\"\n        When an empty QuerySet is passed to the rhs of an __in lookup,\n        the rhs should be wrapped in parentheses to avoid an\n        EmptyResultSet being raised.\n        \"\"\"\n        self.assertSequenceEqual(\n            Annotation.objects.filter(tag__in=Annotation.objects.none()),\n            []\n        )\n"], "sample_998": ["def test_Quaternion_latex_printing_zero():\n    q = Quaternion(0, 0, 0, 0)\n    assert latex(q) == \"0\"\n", "def test_Quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    result = q1 * q2\n    expected = Quaternion(-60, 12, 30, 28)\n    assert result == expected\n", "def test_issue_13651_with_evaluate():\n    expr = c + Mul(-1, a + b)\n    assert latex(expr) == r\"c - a - b\"\n", "def test_issue_13651():\n    expr = c + Mul(-1, a + b, evaluate=False)\n    assert latex(expr) == r\"c - \\left(a + b\\right)\"\n", "def test_Quaternion_multiplication_printing():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    assert latex(q1*q2) == \"1 i 5 + 1 j 7 + 1 k 8 - 41 + 6 i + 8 j - 12 k\"\n"], "sample_1195": ["def test_gamma_trace_function():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n\n    result = gamma_trace(G(i0)*G(i1))\n    assert _is_tensor_eq(result, 4*LorentzIndex.metric(i0, i1))\n\n    result = gamma_trace(ps*ps)\n    assert _is_tensor_eq(result, 4*p(i0)*p(-i0))\n\n    result = gamma_trace(ps*qs + ps*ps)\n    assert _is_tensor_eq(result, 4*p(i0)*p(-i0) + 4*p(i0)*q(-i0))\n", "def test_gamma_trace_with_scalar_coefficients():\n    i0, i1 = tensor_indices('i0:2', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    ps = p(i0) * G(-i0)\n    qs = q(i0) * G(-i0)\n\n    # Test with scalar coefficients\n    t = 2 * ps * qs\n    ts = gamma_trace(t)\n    assert _is_tensor_eq(ts, 8 * p(i0) * q(-i0))\n\n    t = 3 * ps * qs + 2 * ps * ps\n    ts = gamma_trace(t)\n    assert _is_tensor_eq(ts, 8 * p(i0) * q(-i0) + 16 * p(i0) * p(-i0))\n", "def test_gamma_trace():\n    mu, nu, rho, sigma = tensor_indices(\"mu, nu, rho, sigma\", LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n\n    t = gamma_trace(G(mu)*G(nu))\n    assert _is_tensor_eq(t, 4*LorentzIndex.metric(mu, nu))\n\n    t = gamma_trace(p(mu)*G(-mu)*G(nu)*q(-nu))\n    assert _is_tensor_eq(t, 4*p(mu)*q(-mu))\n\n    t = gamma_trace(p(mu)*G(-mu)*q(nu)*G(-nu))\n    assert _is_tensor_eq(t, 4*p(mu)*q(-mu))\n\n    t = gamma_trace(p(mu)*G(-mu)*q(nu)*G(-nu)*G(rho)*G(-rho))\n    assert _is_tensor_eq(t, 4*p(mu)*q(-mu))\n\n    t = gamma_trace(p(mu)*G(-mu)*q(nu)*G(-nu)*G(rho)*G(-rho)*G(sigma)*G(-sigma))\n    assert _is_tensor_eq(t, -4*p(mu)*q(-mu))\n", "def test_gamma_trace():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n\n    t = gamma_trace(G(i0)*G(i1))\n    assert _is_tensor_eq(t, 4*LorentzIndex.metric(i0, i1))\n\n    t = gamma_trace(ps*ps)\n    assert _is_tensor_eq(t, 4*p(i0)*p(-i0))\n\n    t = gamma_trace(ps*qs + ps*ps)\n    assert _is_tensor_eq(t, 4*p(i0)*p(-i0) + 4*p(i0)*q(-i0))\n", "def test_gamma_trace_function():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n    assert _is_tensor_eq(gamma_trace(G(i0)*G(i1)), 4*LorentzIndex.metric(i0, i1))\n    assert _is_tensor_eq(gamma_trace(ps*ps), 4*p(i0)*p(-i0))\n    assert _is_tensor_eq(gamma_trace(ps*qs + ps*ps), 4*p(i0)*p(-i0) + 4*p(i0)*q(-i0))\n"], "sample_49": ["def test_media_addition(self):\n    # Media objects can be added together to combine their media resources.\n    m1 = Media(css={'all': ('path/to/css1', '/path/to/css2')}, js=('/path/to/js1',))\n    m2 = Media(css={'all': ('/path/to/css2', '/path/to/css3')}, js=('/path/to/js1', '/path/to/js4'))\n    combined_media = m1 + m2\n    self.assertEqual(\n        str(combined_media),\n        \"\"\"<link href=\"path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_order_conflict_warning(self):\n    # Media merging should raise a warning if the same media file appears in\n    # different relative orders.\n    with self.assertWarns(RuntimeWarning):\n        Media.merge(['foo.js', 'bar.js'], ['bar.js', 'foo.js'])\n", "def test_media_property_inheritance_order(self):\n    # Media properties can inherit from multiple parents, and the order of\n    # inheritance matters.\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css1',)\n            }\n            js = ('/path/to/js1',)\n\n    class MyWidget2(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css2',)\n            }\n            js = ('/path/to/js2',)\n\n    class MyWidget12(MyWidget1, MyWidget2):\n            return super().media\n        media = property(_media)\n\n    w12 = MyWidget12()\n    # MyWidget12 inherits from MyWidget1 first, then MyWidget2.\n    self.assertEqual(\n        str(w12.media),\n        \"\"\"<link href=\"/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_multi_widget_subwidgets(self):\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n    class MyWidget2(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css2', '/path/to/css3')\n            }\n            js = ('/path/to/js1', '/path/to/js4')\n\n    class MyMultiWidget(MultiWidget):\n            widgets = [MyWidget1, MyWidget2]\n            super().__init__(widgets, attrs)\n\n    mymulti = MyMultiWidget()\n    subwidgets = list(mymulti.subwidgets(name='test', value=''))\n    self.assertEqual(len(subwidgets), 2)\n    self.assertIsInstance(subwidgets[0], MyWidget1)\n    self.assertIsInstance(subwidgets[1], MyWidget2)\n", "def test_media_definition(self):\n    # A widget can define media if it needs to.\n    # Any absolute path will be preserved; relative paths are combined\n    # with the value of settings.MEDIA_URL\n    class MyWidget(TextInput):\n        media = Media(\n            css={'all': ('path/to/css1', '/path/to/css2')},\n            js=('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3'),\n        )\n\n    w = MyWidget()\n    self.assertEqual(\n        str(w.media),\n        \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_987": ["def test_evalf_atan():\n    assert NS(atan(1), 15) == '0.785398163397448'\n    assert NS(atan(0), 15) == '0.000000000000000'\n    assert NS(atan(-1), 15) == '-0.785398163397448'\n    assert NS(atan(oo), 15) == '1.57079632679490'\n    assert NS(atan(-oo), 15) == '-1.57079632679490'\n", "def test_evalf_integers():\n    assert NS('123456789', 10) == '123456789.0000000000'\n    assert NS('-123456789', 10) == '-123456789.0000000000'\n    assert NS('0', 10) == '0.000000000000000'\n", "def test_evalf_complex_absolute_value():\n    assert NS(Abs(1 + 1j), 10) == '1.4142135624'\n    assert NS(Abs(-1 - 1j), 10) == '1.4142135624'\n    assert NS(Abs(1 - 1j), 10) == '1.4142135624'\n    assert NS(Abs(-1 + 1j), 10) == '1.4142135624'\n    assert NS(Abs(0 + 1j), 10) == '1.0'\n    assert NS(Abs(0 - 1j), 10) == '1.0'\n    assert NS(Abs(1 + 0j), 10) == '1.0'\n    assert NS(Abs(-1 + 0j), 10) == '1.0'\n    assert NS(Abs(0 + 0j), 10) == '0.0'\n", "def test_issue_11569():\n    from sympy import re, im\n    assert re(sqrt(2) + I*sqrt(3)) == sqrt(2)\n    assert im(sqrt(2) + I*sqrt(3)) == sqrt(3)\n", "def test_evalf_hypergeometric():\n    from sympy import hyper, symbols\n    n, z = symbols('n z')\n    h = hyper([1, 1], [2], z)\n    assert NS(h.evalf(10), subs={z: 0.5}) == '1.644934067'\n    assert NS(h.evalf(10), subs={z: 2}) == '3.375'\n    assert NS(h.evalf(10), subs={z: -2}) == '-0.145631068'\n    h = hyper([1, 1], [2, 3], z)\n    assert NS(h.evalf(10), subs={z: 0.5}) == '1.500000000'\n    assert NS(h.evalf(10), subs={z: 2}) == '1.512658228'\n    assert NS(h.evalf(10), subs={z: -2}) == '0.121969587'\n    h = hyper([1, 1, 1], [2, 2], z)\n    assert NS(h.evalf(10), subs={z: 0.5}) == '1.500000000'\n    assert NS(h.evalf(10), subs={z: 2}) == '1.524979167'\n    assert NS(h.evalf(10), subs={z: -2}) == '0.119688529'\n    h = hyper([1, 1, 1], [2, 2, 3], z)\n    assert NS(h.evalf(10), subs={z: 0.5}) == '1.500000000'\n    assert NS(h.evalf(10), subs={z: 2}) == '1.524224786'\n    assert NS(h.evalf(10), subs={z: -2}) =="], "sample_542": ["def test_text_path_effects():\n    fig, ax = plt.subplots()\n    t = ax.text(0.5, 0.5, \"test\", path_effects=[mpl.patheffects.withStroke(linewidth=3, foreground=\"w\")])\n    fig.canvas.draw()\n    assert len(t.get_path_effects()) == 1\n", "def test_latex_kerning():\n    plt.figure()\n    plt.figtext(0.1, 0.5, r\"$ATAVATAVATAVATAVATA$\", size=30)\n", "def test_set_math_fontfamily():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, r\"$\\alpha$\", math_fontfamily=\"STIX\")\n    fig.canvas.draw()\n\n    assert ax.texts[0].get_math_fontfamily() == \"STIX\"\n", "def test_bbox_padding():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Hello, World!\", bbox=dict(facecolor=\"yellow\", pad=10))\n    fig.canvas.draw()\n    bbox_extent = text.get_window_extent()\n    text_extent = text._text_transform.transform(text.get_window_extent())\n    assert np.allclose(bbox_extent.extents, text_extent.extents + 2 * 10)\n", "def test_text_bbox_padding():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test', bbox=dict(pad=10.0))\n    fig.canvas.draw()\n    bbox = text.get_window_extent()\n    pad = 10.0 / 72.0 * fig.dpi\n    assert bbox.width > pad * 2\n    assert bbox.height > pad * 2\n"], "sample_334": ["def test_form_prefix(self):\n    class TestForm(Form):\n        field1 = CharField()\n\n    class TestFormPrefix(TestForm):\n        prefix = 'prefix'\n\n    form = TestFormPrefix()\n    self.assertEqual(form.prefix, 'prefix')\n", "    def test_hidden_field_label_rendering(self):\n        class HiddenForm(Form):\n            hidden_field = CharField(widget=HiddenInput)\n\n        form = HiddenForm()\n        self.assertHTMLEqual(\n            form.as_ul(),\n            '<input type=\"hidden\" name=\"hidden_field\" id=\"id_hidden_field\">'\n        )\n", "    def test_custom_renderer(self):\n        class CustomForm(Form):\n            field = CharField()\n            default_renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertIsInstance(form.renderer, CustomForm.default_renderer)\n", "def test_field_deep_copy_validators(self):\n        if value == 'invalid':\n            raise ValidationError('Custom error message.')\n\n    field = CharField(validators=[custom_validator])\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsNot(field_copy.validators, field.validators)\n    self.assertIsNot(field_copy.validators[0], field.validators[0])\n", "    def test_default_renderer(self):\n        # Test that the default renderer is used when none is specified.\n        class TestForm(Form):\n            test_field = CharField()\n\n        form = TestForm()\n        renderer = get_default_renderer()\n        self.assertIsInstance(form.renderer, type(renderer))\n"], "sample_835": ["def test_adaboost_regressor_loss():\n    # Test different loss functions for AdaBoostRegressor\n\n    # Linear loss\n    reg = AdaBoostRegressor(random_state=0, loss='linear')\n    reg.fit(boston.data, boston.target)\n    score = reg.score(boston.data, boston.target)\n    assert score > 0.85\n\n    # Square loss\n    reg = AdaBoostRegressor(random_state=0, loss='square')\n    reg.fit(boston.data, boston.target)\n    score = reg.score(boston.data, boston.target)\n    assert score > 0.85\n\n    # Exponential loss\n    reg = AdaBoostRegressor(random_state=0, loss='exponential')\n    reg.fit(boston.data, boston.target)\n    score = reg.score(boston.data, boston.target)\n    assert score > 0.85\n\n    # Invalid loss function\n    with pytest.raises(ValueError):\n        reg = AdaBoostRegressor(random_state=0, loss='invalid')\n", "def test_adaboost_regressor_loss():\n    # Check the effect of the loss parameter in AdaBoostRegressor.\n    X, y = datasets.make_regression(n_samples=100, n_features=10, random_state=0)\n    losses = ['linear', 'square', 'exponential']\n    for loss in losses:\n        reg = AdaBoostRegressor(n_estimators=50, loss=loss, random_state=0)\n        reg.fit(X, y)\n        score = reg.score(X, y)\n        assert score > 0.5, f\"Failed with loss={loss} and score={score}\"\n", "def test_weight_boosting_base():\n    # Check that weight boosting base class raises an error when called directly.\n    with pytest.raises(TypeError):\n        weight_boosting.BaseWeightBoosting()\n", "def test_adaboost_with_zero_error():\n    # Check that AdaBoost terminates early when estimator_error is 0\n    X = np.array([[1], [2], [3], [4], [5]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    class ZeroErrorEstimator:\n            pass\n            return np.ones(X.shape[0])\n\n    clf = AdaBoostClassifier(base_estimator=ZeroErrorEstimator(), n_estimators=10, algorithm='SAMME.R')\n    clf.fit(X, y)\n    assert len(clf.estimators_) == 1, \"AdaBoost should have terminated early when estimator_error is 0\"\n", "def test_adaboost_with_bad_loss():\n    # Test that AdaBoostRegressor raises a ValueError when an invalid loss is provided.\n    X = np.random.rand(10, 2)\n    y = np.random.rand(10)\n\n    ada = AdaBoostRegressor(loss='invalid')\n    with pytest.raises(ValueError, match=\"loss must be 'linear', 'square', or 'exponential'\"):\n        ada.fit(X, y)\n"], "sample_305": ["def test_annotate_with_expression(self):\n    qs = Book.objects.annotate(discounted_price=ExpressionWrapper(F('price') * 0.9, output_field=DecimalField()))\n    book = qs.get(pk=self.b1.pk)\n    self.assertEqual(book.discounted_price, Decimal('27.00'))\n", "def test_aggregate_on_foreign_key(self):\n    # A query with an aggregation on a foreign key should succeed.\n    qs = Book.objects.aggregate(total_books=Count('contact'))\n    self.assertEqual(qs['total_books'], 6)\n", "def test_annotate_filter_combination(self):\n    # Test combination of annotate and filter operations\n    books = Book.objects.annotate(num_authors=Count('authors')).filter(num_authors__gt=1)\n    self.assertEqual(books.count(), 2)\n    self.assertQuerysetEqual(books, [\n        \"Artificial Intelligence: A Modern Approach\",\n        \"Python Web Development with Django\",\n    ], attrgetter(\"name\"))\n", "def test_annotation_with_distinct_and_filter(self):\n    # Test that an annotation with distinct works with a filter\n    qs = Author.objects.annotate(num_books=Count('book', distinct=True)).filter(num_books__gt=1)\n    self.assertQuerysetEqual(\n        qs,\n        ['Peter Norvig'],\n        lambda b: b.name\n    )\n", "def test_annotate_with_filter_on_same_table(self):\n    \"\"\"\n    Regression test for #10511 -- Annotating on a filtered queryset on the same table.\n    \"\"\"\n    qs = Author.objects.filter(book_contact_set__price=Decimal('23.09')).annotate(book_cnt=Count('book_contact_set'))\n    self.assertQuerysetEqual(qs, ['Brad Dayley'], lambda a: a.name)\n    self.assertEqual(qs[0].book_cnt, 1)\n"], "sample_964": ["def test_pyclass_without_annotation(app):\n    text = \".. py:class:: Class\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"Class\"])],\n                                  [desc_content, ()])]))\n\n    assert 'Class' in domain.objects\n    assert domain.objects['Class'] == ('index', 'Class', 'class', False)\n", "def test_pyfunction_with_optional_positional_only_args(app):\n    text = \".. py:function:: hello(a, /, b=1)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc_sig_operator, \"/\"],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"1\"])])])\n", "def test_pyfunction_with_no_parameters(app):\n    text = \".. py:function:: hello() -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1], desc_parameterlist, children=[])\n", "def test_pyfunction_signature_with_nested_types(app):\n    text = \".. py:function:: hello(name: List[Dict[str, int]]) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameter, ([desc_sig_name, \"name\"],\n                                  [desc_sig_punctuation, \":\"],\n                                  desc_sig_space,\n                                  [desc_sig_name, ([pending_xref, \"List\"],\n                                                   [desc_sig_punctuation, \"[\"],\n                                                   [pending_xref, \"Dict\"],\n                                                   [desc_sig_punctuation, \"[\"],\n                                                   [pending_xref, \"str\"],\n                                                   [desc_sig_punctuation, \",\"],\n                                                   desc_sig_space,\n                                                   [pending_xref, \"int\"],\n                                                   [desc_sig_punctuation, \"]\"],\n                                                   [desc_sig_punctuation, \"]\"])])])\n", "def test_pyfunction_with_default_value(app):\n    text = \".. py:function:: hello(name='World')\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"'World'\"])])\n"], "sample_774": ["def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['ghi', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [1, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 2, 55, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['def', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [0, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 12, 2, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['def', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 0, 1],\n           [1, 0, 1],\n           [0, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 2, 1, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['abc', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 0],\n           [1, 0, 1],\n           [1, 0, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 12, 2, 55])\n    expected_inverse = np.array(X, dtype=object)\n    expected_inverse[:, [0, 1, 3]] = None\n    assert_array_equal(expected_inverse, enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['ghi', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [1, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 12, 2, 55])\n    inv_trans = enc.inverse_transform(trans)\n    exp_inv = np.array([['def', 12, 1, 55],\n                        ['ghi', 12, 3, 56],\n                        ['ghi', 12, 1, 56]], dtype=object)\n    assert_array_equal(inv_trans, exp_inv)\n"], "sample_946": ["def test_pyexception_with_union_type_operator(app):\n    text = \".. py:exception:: exceptions.IOError | OSError\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"IOError\"],\n                                                    \" \",\n                                                    [desc_sig_punctuation, \"|\"],\n                                                    \" \",\n                                                    [desc_name, \"OSError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pyfunction_with_optional_argument_and_default_value(app):\n    text = \".. py:function:: hello(name: str = 'world') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"str\"],\n                                                      \" \",\n                                                      [desc_sig_operator, \"=\"],\n                                                      \" \",\n                                                      [nodes.inline, \"'world'\"])])\n", "def test_domain_py_get_objects(app, status, warning):\n    app.builder.build_all()\n\n    domain = app.env.domains['py']\n    objects = list(domain.get_objects())\n\n    assert ('module_a.submodule.ModTopLevel', 'module_a.submodule.ModTopLevel', 'class',\n            'module', 'module-module_a.submodule.ModTopLevel', 0) in objects\n    assert ('module_a.submodule.ModTopLevel.mod_child_1',\n            'module_a.submodule.ModTopLevel.mod_child_1', 'method', 'module',\n            'module-module_a.submodule.ModTopLevel.mod_child_1', 1) in objects\n    assert ('module_a.submodule.ModTopLevel.mod_child_2',\n            'module_a.submodule.ModTopLevel.mod_child_2', 'method', 'module',\n            'module-module_a.submodule.ModTopLevel.mod_child_2', 1) in objects\n    assert ('ModNoModule', 'ModNoModule', 'class', 'module', 'module-ModNoModule', 1) in objects\n    assert ('module_b.submodule.ModTopLevel', 'module_b.submodule.ModTopLevel', 'class',\n            'module', 'module-module_b.submodule.ModTopLevel', 0) in objects\n\n    assert ('TopLevel', 'TopLevel', 'class', 'roles', 'NestedParentA', 1) in objects\n    assert ('top_level', 'top_level', 'method', 'roles', 'top_level', 1) in objects\n    assert ('NestedParentA', 'NestedParentA', 'class', 'roles', 'NestedParentA', 1) in objects\n    assert ('NestedParentA.child_1', 'NestedParentA.child_1', 'method',\n            'roles', 'NestedParentA.child_1', 1) in objects\n    assert ('NestedParentA", "def test_pyfunction_signature_with_module(app):\n    text = (\".. py:function:: sphinx.util.docutils.nodes.make_id\\n\"\n            \"   (name, docname=None, prefix='', sep='-') -> str\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"sphinx.util.docutils.nodes.\"],\n                                                    [desc_name, \"make_id\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1], pending_xref, **{\"py:module\": \"sphinx.util.docutils.nodes\"})\n", "def test_pyproperty_no_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:property:: prop\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'prop (Class property)', 'Class.prop', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"property \"],\n                                                     [desc_name, \"prop\"])],\n                                   [desc_content, ()]))\n    assert 'Class.prop' in domain.objects\n    assert domain.objects['Class.prop'] == ('index', 'Class.prop', 'property', False)\n"], "sample_962": ["def test_restify_mock_decorator():\n    with mock(['decorator']):\n        import decorator\n        @decorator.decorator\n            return f(*args, **kwargs)\n\n        assert restify(func) == ':py:func:`decorator.decorator<tests.test_util_typing.func>`'\n", "def test_restify_mock_decorator():\n    with mock(['decorator']):\n        import decorator\n        @decorator.decorator\n            return func\n\n        assert restify(dec_func) == ':py:func:`decorator.dec_func`'\n", "def test_restify_mock_decorator():\n    with mock(['decorator']):\n        import decorator\n        @decorator.decorator\n            pass\n        assert restify(func) == ':py:func:`func`'\n", "def test_restify_mock_undecorate():\n    with mock(['undecorated']):\n        import undecorated\n        from sphinx.ext.autodoc.mock import ismock, undecorate\n\n        @undecorated.decorator\n            pass\n\n        decorated_func = func()\n        assert ismock(decorated_func)\n\n        undecorated_func = undecorate(decorated_func)\n        assert not ismock(undecorated_func)\n        assert undecorated_func.__name__ == 'func'\n", "def test_restify_mock_functions():\n    with mock(['unknown']):\n        import unknown\n        assert restify(unknown.secret.function) == ':py:func:`unknown.secret.function`'\n"], "sample_1013": ["def test_numpy_floor():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    f = lambdify(x, floor(x), modules=\"numpy\")\n    assert f(2.7) == 2.0\n    assert f(-2.7) == -3.0\n", "def test_tensorflow_array_arg_with_numpy():\n    # Test for issue 14655 with numpy arrays as input\n    if not numpy:\n        skip(\"numpy not installed.\")\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    f = lambdify([[x, y]], x*x + y, 'tensorflow')\n\n    fcall = f(numpy.array([2.0, 1.0]))\n\n    s = tensorflow.Session()\n    assert s.run(fcall) == 5\n", "def test_lambdify_with_custom_printer():\n    class CustomPrinter(LambdaPrinter):\n            return f\"custom_{super()._print_Symbol(expr)}\"\n\n    expr = x + y + z\n    func = lambdify((x, y, z), expr, printer=CustomPrinter())\n    assert func(1, 2, 3) == 6\n    assert str(func) == \"<function _lambdifygenerated at 0x...\"\n    assert func.__doc__.startswith(\"Created with lambdify. Signature:\\n\\n\")\n    assert \"func(custom_x, custom_y, custom_z)\" in func.__doc__\n    assert \"Expression:\\n\\ncustom_x + custom_y + custom_z\" in func.__doc__\n", "def test_issue_14655():\n    # Test for issue 14655\n    if not numpy:\n        skip(\"numpy not installed\")\n    if not tensorflow:\n        skip(\"tensorflow not installed\")\n\n    f = lambdify([[x, y]], x*x + y, ['numpy', 'tensorflow'])\n\n    assert f(numpy.array([2.0, 1.0])) == 5\n\n    fcall = f(tensorflow.constant([2.0, 1.0]))\n    s = tensorflow.Session()\n    assert s.run(fcall) == 5\n", "def test_tensorflow_dotproduct():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n    A = Matrix([x, y, z])\n    f1 = lambdify([x, y, z], DotProduct(A, A), modules='tensorflow')\n    f2 = lambdify([x, y, z], DotProduct(A, A.T), modules='tensorflow')\n    f3 = lambdify([x, y, z], DotProduct(A.T, A), modules='tensorflow')\n    f4 = lambdify([x, y, z], DotProduct(A, A.T), modules='tensorflow')\n    sess = tensorflow.Session()\n    assert sess.run(f1(1, 2, 3)) == sess.run(f2(1, 2, 3)) == sess.run(f3(1, 2, 3)) == sess.run(f4(1, 2, 3)) == 14\n"], "sample_459": ["def test_integerfield_handles_float_input_correctly(self):\n    f = models.IntegerField()\n    self.assertEqual(f.clean(\"2.7\", None), 3)\n", "def test_integerfield_float_rounding(self):\n    obj = self.model.objects.create(value=1.9)\n    instance = self.model.objects.get(value=2)\n    self.assertEqual(instance.pk, obj.pk)\n\n    # Test the reverse lookup.\n    instance = self.model.objects.get(value__gt=1.9)\n    self.assertEqual(instance.pk, obj.pk)\n\n    instance = self.model.objects.get(value__gte=1.9)\n    self.assertEqual(instance.pk, obj.pk)\n\n    with self.assertRaises(self.model.DoesNotExist):\n        self.model.objects.get(value__lt=1.9)\n\n    with self.assertRaises(self.model.DoesNotExist):\n        self.model.objects.get(value__lte=1.9)\n", "    def test_float_rounding_gt_lookup(self):\n        obj = self.model.objects.create(value=10)\n        # A float value with decimal part should be rounded up to the nearest integer\n        self.assertEqual(self.model.objects.get(value__gt=9.5), obj)\n", "def test_integerfield_exact_lookup_with_float_rounds_up(self):\n    obj = self.model.objects.create(value=10)\n    qs = self.model.objects.filter(value__exact=9.9)\n    self.assertEqual(qs.count(), 1)\n    self.assertEqual(qs[0].value, 10)\n", "    def test_float_rounding(self):\n        obj = self.model.objects.create(value=1.5)\n        self.assertEqual(obj.value, 2)\n        obj = self.model.objects.get(value__gt=1.4)\n        self.assertEqual(obj.value, 2)\n        obj = self.model.objects.get(value__gte=1.5)\n        self.assertEqual(obj.value, 2)\n        obj = self.model.objects.get(value__lt=2.1)\n        self.assertEqual(obj.value, 2)\n        obj = self.model.objects.get(value__lte=1.6)\n        self.assertEqual(obj.value, 2)\n"], "sample_527": ["def test_toolbar_home():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    ax = plt.gca()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.figure.canvas.manager.toolmanager.trigger_tool('home')\n    assert ax.get_xlim() == (0, 1)\n    assert ax.get_ylim() == (0, 1)\n", "def test_interactive_zoom_with_data_limits():\n    fig, ax = plt.subplots()\n    ax.set_xlim(1, 10)\n    ax.set_ylim(0, 1)\n    assert ax.get_navigate_mode() is None\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    assert ax.get_navigate_mode() == 'ZOOM'\n\n    # Zoom from x=2, y=0.2 to x=8, y=0.8 (data coordinates, \"d\").\n    d0 = (2, 0.2)\n    d1 = (8, 0.8)\n    # Convert to screen coordinates (\"s\").  Events are defined only with pixel\n    # precision, so round the pixel values, and below, check against the\n    # corresponding xdata/ydata, which are close but not equal to d0/d1.\n    s0 = ax.transData.transform(d0).astype(int)\n    s1 = ax.transData.transform(d1).astype(int)\n\n    # Zoom in.\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, *s0, MouseButton.LEFT)\n    fig.canvas.callbacks.process(start_event.name, start_event)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas, *s1, MouseButton.LEFT)\n    fig.canvas.callbacks.process(stop_event.name, stop_event)\n    assert ax.get_xlim() == (start_event.xdata, stop_event.xdata)\n    assert ax.get_ylim() == (start_event.ydata, stop_event.ydata)\n\n    # Zoom out.\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, *s1, MouseButton.RIGHT)\n    fig.canvas.callbacks.process(start_event.name, start_event)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas,", "def test_mouse_key_events():\n    fig, ax = plt.subplots()\n    assert fig.canvas.manager.key_pressed is None\n    assert fig.canvas.manager.key_released is None\n    assert fig.canvas.manager.key_pressed_time is None\n    assert fig.canvas.manager.key_released_time is None\n\n    # Simulate a key press event\n    event = KeyEvent(\"key_press_event\", fig.canvas, 'a')\n    fig.canvas.callbacks.process(event.name, event)\n    assert fig.canvas.manager.key_pressed == 'a'\n    assert fig.canvas.manager.key_released is None\n    assert fig.canvas.manager.key_pressed_time is not None\n    assert fig.canvas.manager.key_released_time is None\n\n    # Simulate a key release event\n    event = KeyEvent(\"key_release_event\", fig.canvas, 'a')\n    fig.canvas.callbacks.process(event.name, event)\n    assert fig.canvas.manager.key_pressed is None\n    assert fig.canvas.manager.key_released == 'a'\n    assert fig.canvas.manager.key_pressed_time is None\n    assert fig.canvas.manager.key_released_time is not None\n\n    # Simulate a mouse press event\n    event = MouseEvent(\"button_press_event\", fig.canvas, 100, 100, MouseButton.LEFT)\n    fig.canvas.callbacks.process(event.name, event)\n    assert fig.canvas.manager.key_pressed is None\n    assert fig.canvas.manager.key_released is None\n    assert fig.canvas.manager.key_pressed_time is None\n    assert fig.canvas.manager.key_released_time is None\n", "def test_widgetlock_zoompan_disconnect():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    fig.canvas.widgetlock(ax)\n    tb = NavigationToolbar2(fig.canvas)\n\n    # Connect the callback\n    cid = tb.zoom()\n    assert ax.get_navigate_mode() is None\n\n    # Disconnect the callback\n    fig.canvas.mpl_disconnect(cid)\n    tb.zoom()\n    assert ax.get_navigate_mode() == \"ZOOM\"\n", "def test_pick_event_with_artist_and_ind():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [4, 5, 6])\n    fig.canvas.draw()\n\n    picks = []\n        assert event.artist == line\n        assert event.ind == [0]\n        picks.append(event)\n    fig.canvas.mpl_connect(\"pick_event\", handle_pick)\n\n    KeyEvent(\"key_press_event\", fig.canvas, \"a\")._process()\n    MouseEvent(\"pick_event\", fig.canvas,\n               *fig.transFigure.transform((1.5, 4.5)),\n               MouseButton.LEFT, xdata=1, ydata=4, inaxes=ax,\n               artist=line, ind=[0])._process()\n    KeyEvent(\"key_release_event\", fig.canvas, \"a\")._process()\n    assert len(picks) == 1\n"], "sample_786": ["def test_constant_feature_transform():\n    X = np.array([[1, 1],\n                  [1, 1],\n                  [1, 1],\n                  [1, 1]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    assert_warns_message(UserWarning,\n                         \"Feature 1 is constant and will be replaced \"\n                         \"with 0.\", est.fit, X)\n    assert est.n_bins_[1] == 1\n    # replace the feature with zeros\n    Xt = est.transform(X)\n    assert_array_equal(Xt[:, 1], np.zeros(X.shape[0]))\n", "def test_constant_feature_transform():\n    X = np.array([[1, 1, 1],\n                  [2, 2, 2],\n                  [3, 3, 3]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    est.fit(X)\n    Xt = est.transform(X)\n    assert_array_equal(Xt, np.zeros((3, 3)))\n", "def test_invalid_n_samples():\n    est = KBinsDiscretizer(n_bins=3).fit(X)\n    bad_X = np.arange(12).reshape(-1, 1)\n    assert_raise_message(ValueError,\n                         \"Incorrect number of samples. Expecting 4, \"\n                         \"received 6\", est.transform, bad_X)\n", "def test_transform_zero_variance(n_bins, encode):\n    X = np.zeros((100, 3))\n    kbd = KBinsDiscretizer(n_bins=n_bins, strategy='uniform', encode=encode)\n    Xt = kbd.fit_transform(X)\n    assert_array_equal(Xt, np.zeros_like(Xt))\n", "def test_constant_feature_inverse_transform():\n    X = np.array([[1, -2, 5],\n                  [1, -1, 5],\n                  [1, 0, 5],\n                  [1, 1, 5]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    warnings.simplefilter(\"always\")\n    assert_warns_message(UserWarning,\n                         \"Feature 0 is constant and will be replaced \"\n                         \"with 0.\", est.fit, X)\n    Xt = est.transform(X)\n    Xinv = est.inverse_transform(Xt)\n    assert_array_equal(X[:, 1:], est.inverse_transform(Xt[:, 1:]))\n"], "sample_387": ["def test_ForeignKey_to_field(self):\n    from selenium.webdriver.common.by import By\n\n    self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_band_change\", args=(42,))\n    )\n    main_window = self.selenium.current_window_handle\n\n    # Open the popup window and click on a profile\n    self.selenium.find_element(By.ID, \"lookup_id_profile\").click()\n    self.wait_for_and_switch_to_popup()\n    link = self.selenium.find_element(By.LINK_TEXT, \"Test Profile\")\n    link.click()\n\n    # The field now contains the selected profile's id\n    self.selenium.switch_to.window(main_window)\n    self.wait_for_value(\"#id_profile\", \"1\")\n", "    def test_change_list_ForeignKey(self):\n        self.client.force_login(self.superuser)\n        response = self.client.get(reverse(\"admin:admin_widgets_cartire_change\", args=(self.tire.id,)))\n        self.assertContains(response, \"/auth/user/add/\")\n", "def test_ForeignKey_limit_choices_to(self):\n    from selenium.webdriver.common.by import By\n\n    # Create a band that doesn't match the limit_choices_to filter\n    Band.objects.create(id=100, name=\"Limited Band\", style=\"rock\")\n\n    self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n    )\n    main_window = self.selenium.current_window_handle\n\n    # Open the popup window\n    self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n    self.wait_for_and_switch_to_popup()\n\n    # Check that the limited band is not in the list\n    with self.assertRaises(NoSuchElementException):\n        self.selenium.find_element(By.LINK_TEXT, \"Limited Band\")\n\n    # Close the popup window\n    self.selenium.switch_to.window(main_window)\n", "def test_initial_data(self):\n    from selenium.webdriver.common.by import By\n\n    self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n    # Create an event with initial data\n    event = Event.objects.create(name=\"Test Event\", main_band=Band.objects.get(id=42))\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_event_change\", args=(event.id,))\n    )\n    main_window = self.selenium.current_window_handle\n\n    # The field should be pre-filled with the initial data\n    self.assertEqual(\n        self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"), \"42\"\n    )\n\n    # Open the popup window and check that the initial data is selected\n    self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n    self.wait_for_and_switch_to_popup()\n    self.wait_for_element_present(\".selector-selected\")\n    self.assertEqual(\n        self.selenium.find_element(By.CSS_SELECTOR, \".selector-selected\").text, \"Bogey Blues\"\n    )\n", "def test_custom_foreignkey_widget(self):\n    from selenium.webdriver.common.by import By\n\n    self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n    )\n    main_window = self.selenium.current_window_handle\n\n    # No value has been selected yet for the custom_foreignkey_field\n    self.assertEqual(\n        self.selenium.find_element(By.ID, \"id_custom_foreignkey_field\").get_attribute(\"value\"), \"\"\n    )\n\n    # Open the popup window and click on a custom foreignkey object\n    self.selenium.find_element(By.ID, \"lookup_id_custom_foreignkey_field\").click()\n    self.wait_for_and_switch_to_popup()\n    link = self.selenium.find_element(By.LINK_TEXT, \"Custom Object\")\n    self.assertIn(\"/custommodel/1/\", link.get_attribute(\"href\"))\n    link.click()\n\n    # The field now contains the selected custom foreignkey object's id\n    self.selenium.switch_to.window(main_window)\n    self.wait_for_value(\"#id_custom_foreignkey_field\", \"1\")\n"], "sample_669": ["def test_encodedfile_mode(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert ef.mode == \"w\"\n    tmpfile.close()\n    with pytest.raises(ValueError):\n        ef.mode\n", "def test_encodedfile_name(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert ef.name == \"<io.BufferedRandom name={}>\".format(tmpfile.name)\n    tmpfile.close()\n", "def test_stdout_write_returns_len(capsys):\n    \"\"\"Write on Encoded files, namely captured stdout, should return number of characters written.\"\"\"\n    assert sys.stdout.write(\"Bar\") == 3\n", "def test_encodedfile_writelines_unicode_error(capsys):\n    \"\"\"Writelines with unencodable characters should use 'replace' error handler and return the number of characters written.\"\"\"\n    assert sys.stderr.writelines([\"Foo\", \"B\\u00e4r\"]) == 7\n", "def test_encodedfile_read_with_errors(tmpfile: BinaryIO) -> None:\n    data = b\"f\\xc3\\x96o b\\xc3\\xa4r\"\n    tmpfile.write(data)\n    tmpfile.flush()\n    tmpfile.seek(0)\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\", errors=\"replace\")\n    assert ef.read() == \"f\u00c3\u00b6o b\u00c3\u00a4r\"\n    tmpfile.close()\n"], "sample_27": ["def test_fitsdiff_with_different_ver(tmp_path):\n    \"\"\"Make sure diff report reports HDU name and ver if different in files\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\", ver=1)])\n    hdulist.writeto(path1)\n    hdulist[1].ver = 2\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension HDU 1 (SCI, 2):\" in diff.report()\n", "def test_fitsdiff_with_different_vers(tmp_path):\n    \"\"\"Make sure diff report reports HDU ver if not same in files\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\", ver=1)])\n    hdulist.writeto(path1)\n    hdulist[1].ver = 2\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension HDU 1 (SCI, 2):\" in diff.report()\n", "def test_fitsdiff_with_ver(tmp_path):\n    \"\"\"Make sure diff report doesn't report HDU ver if not same in files\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\", ver=1)])\n    hdulist.writeto(path1)\n    hdulist[1].ver = 2\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension HDU 1 (SCI, 2):\" in diff.report()\n", "def test_identical_files_ignore_keywords(self):\n    \"\"\"Test identicality of two files with ignored keywords.\"\"\"\n    a = np.arange(100).reshape(10, 10)\n    hdu = PrimaryHDU(data=a)\n    hdu.header['IGNORE'] = 'test'\n    hdu.writeto(self.temp(\"testa.fits\"))\n    hdu.writeto(self.temp(\"testb.fits\"))\n    diff = FITSDiff(self.temp(\"testa.fits\"), self.temp(\"testb.fits\"), ignore_keywords=['IGNORE'])\n    assert diff.identical\n    report = diff.report()\n    assert \"Keyword IGNORE\" not in report\n    assert \"No differences found.\" in report\n", "def test_fitsdiff_with_ver(tmp_path):\n    \"\"\"Make sure diff report reports HDU ver if not same in files\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\", ver=1)])\n    hdulist.writeto(path1)\n    hdulist[1].ver = 2\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension HDU 1 (SCI, 2):\" in diff.report()\n"], "sample_673": ["def test_doctest_unexpected_exception_continue_on_failure(self, testdir):\n    testdir.maketxtfile(\n        \"\"\"\n        >>> i = 0\n        >>> 0 / i\n        2\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\")\n    result.stdout.fnmatch_lines(\n        [\n            \"test_doctest_unexpected_exception_continue_on_failure.txt F *\",\n            \"\",\n            \"*= FAILURES =*\",\n            \"*_ [[]doctest[]] test_doctest_unexpected_exception_continue_on_failure.txt _*\",\n            \"002 >>> 0 / i\",\n            \"UNEXPECTED EXCEPTION: ZeroDivisionError*\",\n            \"Traceback (most recent call last):\",\n            '  File \"*/doctest.py\", line *, in __run',\n            \"    *\",\n            '  File \"<doctest test_doctest_unexpected_exception_continue_on_failure.txt[1]>\", line 1, in <module>',\n            \"ZeroDivisionError: division by zero\",\n            \"*/test_doctest_unexpected_exception_continue_on_failure.txt:2: UnexpectedException\",\n        ],\n        consecutive=True,\n    )\n", "def test_doctest_linedata_missing_property(self, testdir):\n    testdir.tmpdir.join(\"hello.py\").write(\n        textwrap.dedent(\n            \"\"\"\\\n            class Fun(object):\n                    self._property = 42\n\n                @property\n                    '''\n                    >>> a = 1\n                    >>> Fun()._property\n                    42\n                    '''\n            \"\"\"\n        )\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\"*hello*\", \"006*>>> Fun()._property*\", \"Expected: 42\", \"Got: 0\", \"*1 failed*\"]\n    )\n", "def test_doctest_unicode_literals_and_number_precision(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n            '''\n            >>> import math\n            >>> u'\u00e5\u00e9'  # doctest: +ALLOW_UNICODE, +NUMBER\n            u'\u00e5\u00e9'\n            >>> math.pi  # doctest: +NUMBER\n            3.1416\n            '''\n        \"\"\"\n    )\n    reprec = testdir.inline_run(\"--doctest-modules\")\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_setup_py_skip(self, testdir):\n    p = testdir.makepyfile(\n        setup=\"\"\"\n        from setuptools import setup\n        setup(\n            name='sample',\n            version='0.0',\n            description='description',\n            py_modules=['test_doctest_setup_py_skip'],\n            setup_requires=['pytest-runner'],\n        )\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"*collected 0 items*\"])\n", "def test_doctest_report_choices(self, testdir):\n    result = testdir.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"--doctest-report={udiff,cdiff,ndiff,only_first_failure,none}\",\n            \"choose another output format for diffs on doctest failure\",\n        ]\n    )\n"], "sample_710": ["def test_do_cleanups_on_test_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n                pass\n            assert MyTestCase.values == [1, 1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 2\n", "def test_do_cleanups_on_test_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n                pass\n            assert MyTestCase.values == [1, 1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 2\n", "def test_do_cleanups_on_test_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n            assert MyTestCase.values == [1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "def test_do_class_cleanups_on_multiple_setupclass_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n            @classmethod\n                    cls.values.append(1)\n                cls.addClassCleanup(cleanup)\n                assert False\n            @classmethod\n                    cls.values.append(2)\n                cls.addClassCleanup(cleanup)\n                assert False\n                pass\n            assert MyTestCase.values == [1, 2]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "def test_testcase_custom_exception_info_with_unhandled_error(pytester: Pytester, type: str) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        from typing import Generic, TypeVar\n        from unittest import TestCase\n        import pytest, _pytest._code\n\n        class MyTestCase(TestCase):\n                excinfo = pytest.raises(ZeroDivisionError, lambda: 0/0)\n                # We fake an incompatible exception info that raises an error.\n                class FakeExceptionInfo(Generic[TypeVar(\"E\")]):\n                        mp.undo()\n                        raise UnhandledException(\"This should not be caught\")\n                    @classmethod\n                        return cls()\n                    @classmethod\n                        return cls()\n                class UnhandledException(Exception):\n                    pass\n                mp = pytest.MonkeyPatch()\n                mp.setattr(_pytest._code, 'ExceptionInfo', FakeExceptionInfo)\n                try:\n                    excinfo = excinfo._excinfo\n                    result.add%(type)s(self, excinfo)\n                finally:\n                    mp.undo()\n\n                pass\n    \"\"\"\n        % locals()\n    )\n    result = pytester.runpytest()\n    assert \"UnhandledException: This should not be caught\" in result.stdout.str()\n"], "sample_834": ["def test_input_output_shapes():\n    \"\"\"Test that the output shape of transform is as expected.\"\"\"\n    X = iris_data\n    y = iris_target\n    n_components = 2\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, max_iter=5)\n    nca.fit(X, y)\n    X_transformed = nca.transform(X)\n    assert_equal(X_transformed.shape, (X.shape[0], n_components))\n", "def test_transform_without_fit():\n    # Test transform method without fitting the model\n    nca = NeighborhoodComponentsAnalysis()\n    X = np.arange(12).reshape(4, 3)\n    with pytest.raises(NotFittedError, match=\"This NeighborhoodComponentsAnalysis instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"):\n        nca.transform(X)\n", "def test_nca_transform_shape():\n    \"\"\"Test that transform method returns an array of expected shape.\"\"\"\n    X = iris_data\n    y = iris_target\n    n_components = 2\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, max_iter=5)\n    nca.fit(X, y)\n    X_transformed = nca.transform(X)\n    assert X_transformed.shape == (X.shape[0], n_components)\n", "def test_nca_with_custom_random_state():\n    # Test NCA with a custom random state\n    custom_random_state = 42\n    nca_custom_rs = NeighborhoodComponentsAnalysis(random_state=custom_random_state)\n    nca_custom_rs.fit(iris_data, iris_target)\n\n    nca_same_rs = NeighborhoodComponentsAnalysis(random_state=custom_random_state)\n    nca_same_rs.fit(iris_data, iris_target)\n\n    assert_array_almost_equal(nca_custom_rs.components_, nca_same_rs.components_)\n\n    # Test NCA with a different random state\n    nca_diff_rs = NeighborhoodComponentsAnalysis(random_state=custom_random_state + 1)\n    nca_diff_rs.fit(iris_data, iris_target)\n\n    assert not np.array_equal(nca_custom_rs.components_, nca_diff_rs.components_)\n", "def test_input_validation():\n    # Test that invalid inputs raise appropriate errors\n    X = np.arange(12).reshape(4, 3)\n    y = [1, 1, 2, 2]\n    nca = NeighborhoodComponentsAnalysis()\n\n    # X is not 2D array-like\n    assert_raises(ValueError, nca.fit, np.array([1, 2, 3]), y)\n\n    # X and y have different lengths\n    assert_raises(ValueError, nca.fit, X, y[:-1])\n\n    # y is not 1D array-like\n    assert_raises(ValueError, nca.fit, X, np.array([[1, 2], [3, 4]]))\n\n    # y contains invalid labels\n    assert_raises(ValueError, nca.fit, X, [1, 'a', 2, 2])\n\n    # y contains only one label\n    assert_raises(ValueError, nca.fit, X, [1, 1, 1, 1])\n"], "sample_678": ["def test_ensure_deletable(tmp_path):\n    \"\"\"Ensure that ensure_deletable function works correctly.\"\"\"\n    path = tmp_path / \"temp-1\"\n    path.mkdir()\n\n    assert ensure_deletable(path, float(\"inf\")) is True\n\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n\n    assert ensure_deletable(path, float(\"inf\")) is False\n    assert ensure_deletable(path, 0) is True\n    assert not lock_path.exists()\n", "def test_ensure_deletable(tmp_path):\n    \"\"\"Ensure that ensure_deletable returns True for a deletable directory and False for a locked directory.\"\"\"\n    path = tmp_path / \"temp-1\"\n    path.mkdir()\n    assert ensure_deletable(path, 0)\n\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n    assert not ensure_deletable(path, float('inf'))\n\n    # Ensure lock is removed if it's old\n    assert ensure_deletable(path, 0)\n    assert not lock_path.exists()\n", "def test_ensure_deletable(tmp_path, monkeypatch):\n    \"\"\"Test ensure_deletable function when the lock exists and is considered dead.\"\"\"\n    path = tmp_path / \"test_dir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n\n    # Mock the time to make the lock seem dead\n    mock_time = 0\n    monkeypatch.setattr(\"os.path.getmtime\", lambda _: mock_time)\n\n    assert ensure_deletable(path, mock_time - 1)\n    assert not lock_path.exists()\n", "def test_ensure_deletable(tmp_path):\n    \"\"\"Ensure that ensure_deletable behaves as expected.\"\"\"\n    path = tmp_path / \"temp-1\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n\n    # Case 1: Lock doesn't exist\n    assert ensure_deletable(path, 0) == True\n\n    # Case 2: Lock exists but is old\n    with open(lock_path, 'w') as f:\n        f.write('test')\n    os.utime(lock_path, (0, 0))\n    assert ensure_deletable(path, 1) == True\n    assert not lock_path.exists()\n\n    # Case 3: Lock exists and is recent\n    with open(lock_path, 'w') as f:\n        f.write('test')\n    assert ensure_deletable(path, 10**9) == False\n    assert lock_path.exists()\n", "def test_ensure_deletable(tmp_path, monkeypatch):\n    \"\"\"Test the ensure_deletable function.\"\"\"\n    path = tmp_path / \"test_dir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n\n    # When lock does not exist, it should return True\n    assert ensure_deletable(path, 0)\n\n    # Create lock file\n    lock_path.touch()\n\n    # When lock exists but is old, it should return True and delete the lock\n    assert ensure_deletable(path, 100000)\n    assert not lock_path.exists()\n\n    # Create lock file again\n    lock_path.touch()\n\n    # When lock exists and is new, it should return False\n    assert not ensure_deletable(path, 0)\n    assert lock_path.exists()\n"], "sample_635": ["def test_finds_missing_yield_type_numpy(self) -> None:\n    \"\"\"Example of a function having missing yield documentation in\n    a numpy style docstring\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''Generate numbers up to n.\n\n        Parameters\n        ----------\n        n : int\n            The maximum number to generate.\n\n        Yields\n        ------\n            int\n                A number between 0 and n.\n        '''\n        i = 0\n        while i < n:\n            yield i\n            i += 1\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"missing-yield-type-doc\", node=node)\n    ):\n        self.checker.visit_functiondef(node)\n", "def test_finds_multiple_raises_sphinx(self) -> None:\n    \"\"\"Example of a function having multiple raises documentation in\n    a Sphinx style docstring\n    \"\"\"\n    node = astroid.extract_node(\n        '''\n        \"\"\"The docstring\n\n        :param named_arg: Returned\n        :type named_arg: object\n        :raises ValueError: When named_arg is invalid\n        :raises TypeError: When named_arg is of the wrong type\n        :returns: Maybe named_arg\n        :rtype: object or None\n        \"\"\"\n        if not named_arg:\n            raise ValueError(\"named_arg is invalid\")\n        if not isinstance(named_arg, object):\n            raise TypeError(\"named_arg is of the wrong type\")\n        return named_arg\n    '''\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "    def test_finds_missing_params_in_epytext_docstring(self) -> None:\n        \"\"\"Example of a function with missing Epytext parameter documentation in the docstring\"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''docstring ...\n\n            @param x: bla\n            @type x: int\n\n            missing parameter documentation\n            '''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-param-doc\", node=node, args=(\"y, z\")),\n            MessageTest(msg_id=\"missing-type-doc\", node=node, args=(\"y, z\",)),\n        ):\n            self.checker.visit_functiondef(node)\n", "def test_docparams_check_init_and_private(self) -> None:\n    \"\"\"Check that both __init__ and other private methods are checked correctly\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    #pylint disable=missing-module-docstring, too-few-public-methods,\n    class MyClass:\n            '''\n            My init docstring\n            '''\n\n            '''\n            My private method\n            '''\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        MessageTest(\n            msg_id=\"missing-param-doc\", node=node.body[0], args=(\"my_param\",)\n        ),\n        MessageTest(\n            msg_id=\"missing-param-doc\", node=node.body[1], args=(\"my_param\",)\n        ),\n    ):\n        self.checker.visit_functiondef(node.body[0])\n        self.checker.visit_functiondef(node.body[1])\n", "def test_handles_multiple_type_declarations_sphinx(self) -> None:\n    \"\"\"Example of a function with multiple type declarations in a Sphinx style docstring.\n\n    The first declaration should take precedence.\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''docstring ...\n\n        :param int arg: Integer argument.\n        :type arg: float\n        '''\n        pass\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n"], "sample_1156": ["def test_sinh_nonnegative():\n    k = symbols('k', real=True)\n    n = symbols('n', integer=True)\n\n    assert sinh(k, evaluate=False).is_nonnegative is None\n    assert sinh(k + 2*n*pi*I, evaluate=False).is_nonnegative is None\n    assert sinh(I*pi/2, evaluate=False).is_nonnegative is True\n    assert sinh(3*I*pi/2, evaluate=False).is_nonnegative is True\n    assert sinh(S.Zero, evaluate=False).is_nonnegative is True\n", "def test_asinh_real():\n    # asinh(x) is real for real values of x\n    k = symbols('k', real=True)\n    n = symbols('n', integer=True)\n\n    assert asinh(k, evaluate=False).is_real is True\n    assert asinh(k + 2*n*pi*I, evaluate=False).is_real is False\n    assert asinh(I, evaluate=False).is_real is False\n    assert asinh(-I, evaluate=False).is_real is False\n", "def test_tanh_real_imag():\n    x = Symbol('x', real=True)\n    re, im = tanh(x).as_real_imag(deep=False)\n    assert re == sinh(x)*cosh(x)\n    assert im == 0\n\n    x = Symbol('x', real=False)\n    re, im = tanh(x).as_real_imag(deep=False)\n    assert re == sinh(re(x))*cosh(re(x))/(cos(im(x))**2 + sinh(re(x))**2)\n    assert im == sin(im(x))*cos(im(x))/(cos(im(x))**2 + sinh(re(x))**2)\n", "def test_asinh_real():\n    # Test asinh with real arguments\n    assert asinh(1).is_real is True\n    assert asinh(-1).is_real is True\n    assert asinh(0).is_real is True\n    assert asinh(sqrt(2)).is_real is None\n    assert asinh(-sqrt(2)).is_real is None\n", "def test_asinh_is_real():\n    x = Symbol('x')\n\n    assert asinh(x).is_real is None\n\n    assert asinh(1).is_real is True\n    assert asinh(-1).is_real is True\n    assert asinh(I).is_real is False\n    assert asinh(-I).is_real is False\n    assert asinh(2 + 3*I).is_real is False\n    assert asinh(x*I).is_real is True\n    assert asinh(k*pi*I).is_real is True\n"], "sample_741": ["def test_grid_search_cv_splits_consistency_with_groups():\n    # Check if a one time iterable is accepted as a cv parameter with groups.\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=n_samples, random_state=0)\n    groups = np.arange(n_samples) // 10\n\n    gs = GridSearchCV(LinearSVC(random_state=0),\n                      param_grid={'C': [0.1, 0.2, 0.3]},\n                      cv=LeaveOneGroupOut())\n    gs.fit(X, y, groups=groups)\n\n    gs2 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=GroupKFold(n_splits=n_splits))\n    gs2.fit(X, y, groups=groups)\n\n        for key in ('mean_fit_time', 'std_fit_time',\n                    'mean_score_time', 'std_score_time'):\n            cv_results.pop(key)\n        return cv_results\n\n    # Check if the splits are consistent\n    np.testing.assert_equal(_pop_time_keys(gs.cv_results_),\n                            _pop_time_keys(gs2.cv_results_))\n", "def test_grid_search_custom_scorer():\n    # Test grid-search with custom scorer\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n        return np.mean(y_true == y_pred)\n\n    search = GridSearchCV(SVC(gamma='scale'), cv=3, iid=False,\n                          param_grid={'C': [1, 10], 'gamma': [0.1, 1]},\n                          scoring=make_scorer(custom_scorer))\n    search.fit(X, y)\n    assert_true(search.best_score_ >= 0)\n    assert_true(search.best_score_ <= 1)\n", "def test_grid_search_custom_scoring():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n        return -np.sum(y_true == y_pred)\n\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    grid_search = GridSearchCV(SVC(gamma='scale'), param_grid=params, scoring=custom_scoring)\n    grid_search.fit(X, y)\n    assert_equal(grid_search.best_score_, -np.sum(y == grid_search.predict(X)))\n", "def test_grid_search_iid_param_with_warnings():\n    # Test the IID parameter with warnings\n    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,\n                      cluster_std=0.1, shuffle=False, n_samples=80)\n    cv = [[np.ones(X.shape[0], dtype=bool), ~np.ones(X.shape[0], dtype=bool)],\n          [~np.ones(X.shape[0], dtype=bool), np.ones(X.shape[0], dtype=bool)]]\n\n    # once with iid=None\n    grid_search = GridSearchCV(SVC(), param_grid={'C': [1, 10]},\n                               cv=cv, iid=None)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        grid_search.fit(X, y)\n        assert_true(len(w) > 0)\n        assert_true(issubclass(w[-1].category, DeprecationWarning))\n        assert_true(\"The default of the `iid` parameter will change\" in str(w[-1].message))\n    assert_equal(grid_search.iid, True)\n", "def test_search_over_iterable():\n    # Check if a one time iterable is accepted as a cv parameter.\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=n_samples, random_state=0)\n\n    gs = GridSearchCV(LinearSVC(random_state=0),\n                      param_grid={'C': [0.1, 0.2, 0.3]},\n                      cv=StratifiedKFold(n_splits=n_splits).split(X, y))\n    gs.fit(X, y)\n\n    # Check if the number of splits is correct\n    assert_equal(gs.n_splits_, n_splits)\n\n    # Check if the best score is not None\n    assert_true(gs.best_score_ is not None)\n"], "sample_434": ["def test_dispatch_method_calls_correct_handler(self):\n    request_factory = RequestFactory()\n    tests = [\n        (SyncView, False),\n        (AsyncView, True),\n    ]\n    for view_cls, is_coroutine in tests:\n        with self.subTest(view_cls=view_cls, is_coroutine=is_coroutine):\n            instance = view_cls()\n            request = request_factory.get(\"/\")\n            instance.setup(request)\n\n            response = instance.dispatch(request)\n            self.assertIs(\n                asyncio.iscoroutine(response),\n                is_coroutine,\n            )\n            if is_coroutine:\n                response = asyncio.run(response)\n\n            self.assertEqual(response.content, b\"Hello (sync) world!\") if not is_coroutine else self.assertEqual(response.content, b\"Hello (async) world!\")\n", "    def test_setup_method(self):\n        request = RequestFactory().get(\"/\")\n        view = SyncView()\n        view.setup(request, \"arg1\", \"arg2\", kwarg1=\"value1\")\n        self.assertIs(view.request, request)\n        self.assertEqual(view.args, (\"arg1\", \"arg2\"))\n        self.assertEqual(view.kwargs, {\"kwarg1\": \"value1\"})\n", "    def test_view_dispatch_correctly_routes(self):\n        request_factory = RequestFactory()\n        tests = [\n            (SyncView, False),\n            (AsyncView, True),\n        ]\n        for view_cls, is_coroutine in tests:\n            with self.subTest(view_cls=view_cls, is_coroutine=is_coroutine):\n                instance = view_cls()\n                request = request_factory.get(\"/\")\n                instance.setup(request)\n                response = instance.dispatch(request)\n                self.assertIs(\n                    asyncio.iscoroutine(response),\n                    is_coroutine,\n                )\n                if is_coroutine:\n                    response = asyncio.run(response)\n\n                self.assertIsInstance(response, HttpResponse)\n                self.assertEqual(response.content, b\"Hello (sync) world!\" if not is_coroutine else b\"Hello (async) world!\")\n", "compilation error", "    def test_setup(self):\n        request = RequestFactory().get(\"/\")\n        args = (\"arg1\", \"arg2\")\n        kwargs = {\"kwarg1\": \"value1\", \"kwarg2\": \"value2\"}\n        view = SyncView()\n        view.setup(request, *args, **kwargs)\n        self.assertEqual(view.request, request)\n        self.assertEqual(view.args, args)\n        self.assertEqual(view.kwargs, kwargs)\n        self.assertEqual(view.head, view.get)\n"], "sample_529": ["def test_shadow():\n    x = np.linspace(1, 100, 100)\n    y = x\n    plt.plot(x, y, label='mylabel', lw=10)\n    plt.legend(shadow=True)\n", "def test_legend_markers_from_patch():\n    # Test that markers can be copied for legend patches\n    _markers = ['o', 's', '^']\n    fig, ax = plt.subplots()\n    patches = [mpatches.Patch(facecolor='white', edgecolor='black', marker=mark)\n               for mark in _markers]\n    labels = [\"circle\", \"square\", \"triangle\"]\n    markers = [patch.get_marker() for patch in patches]\n    legend = ax.legend(patches, labels, handler_map={mpatches.Patch: HandlerPatch(fill=False)})\n\n    new_markers = [line.get_marker() for line in legend.get_lines()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert markers == new_markers == _markers\n    assert labels == new_labels\n", "def test_legend_location_validation():\n    # Test that legend location validation works correctly (#20819)\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n\n    with pytest.raises(ValueError, match=\"'best' is not a valid location\"):\n        ax.legend(loc='best')\n\n    with pytest.raises(ValueError, match=\"10 is not a valid location\"):\n        ax.legend(loc=10)\n\n    with pytest.raises(ValueError, match=\"'invalid' is not a valid location\"):\n        ax.legend(loc='invalid')\n", "def test_legend_markers_from_pathcollection():\n    # Test that markers can be copied for legend scatter points (#17960)\n    _markers = ['.', '*', 'v']\n    fig, ax = plt.subplots()\n    scatters = [ax.scatter([0], [0], marker=mark) for mark in _markers]\n    labels = [\"foo\", \"bar\", \"xyzzy\"]\n    markers = [scatter.get_paths()[0].get_marker() for scatter in scatters]\n    legend = ax.legend([scatter for scatter in scatters], labels)\n\n    new_markers = [line.get_marker() for line in legend.get_lines()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert markers == new_markers == _markers\n    assert labels == new_labels\n", "def test_legend_update_from_first_child():\n    # Test that legend can be updated from first child for BarContainer (#21188)\n    fig, ax = plt.subplots()\n    bar_container = ax.bar([0], [1], color='blue', label='Bar')\n    ax.legend(handler_map={mcollections.BarContainer: HandlerPatch(update_func=legend_handler.update_from_first_child)})\n    assert ax.get_legend().get_patches()[0].get_facecolor() == 'blue'\n"], "sample_1145": ["def test_symmetric():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[0, 0], Q.symmetric(X)) == X[0, 0]\n", "def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[0, 0], Q.symmetric(X)) == X[0, 0]\n    assert refine(X[0, 1], Q.antisymmetric(X)) == -X[1, 0]\n", "def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.positive(re(x))) == x / Abs(x)\n    assert refine(sign(x), Q.negative(re(x))) == x / Abs(x)\n    assert refine(sign(x), Q.zero(re(x)) & Q.positive(im(x))) == S.ImaginaryUnit\n    assert refine(sign(x), Q.zero(re(x)) & Q.negative(im(x))) == -S.ImaginaryUnit\n", "def test_symmetric():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    Y = MatrixSymbol('Y', 2, 2, symmetric=True)\n    assert refine(Y[0, 1], Q.symmetric(Y)) == Y[1, 0]\n    assert refine(Y[1, 0], Q.symmetric(Y)) == Y[0, 1]\n", "def test_refine_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[0, 0], Q.symmetric(X)) == X[0, 0]\n    assert refine(X[2, 2], Q.symmetric(X)) == X[2, 2]\n"], "sample_602": ["def test_to_zarr():\n    data = xr.DataArray(np.random.rand(4, 3), dims=(\"x\", \"y\"))\n    ds = xr.Dataset({\"data\": data})\n    store = {}\n    ds.to_zarr(store)\n    ds_zarr = xr.open_zarr(store)\n    xr.testing.assert_equal(ds, ds_zarr)\n", "def test_open_dataset_with_custom_engine_and_parameters():\n    expected = xr.Dataset(\n        dict(a=3 * np.arange(5)), coords=dict(x=(\"x\", np.arange(5), dict(units=\"m\")))\n    )\n\n    class CustomBackend(xr.backends.BackendEntrypoint):\n        open_dataset_parameters = [\"custom_parameter\"]\n\n            filename_or_obj,\n            drop_variables=None,\n            custom_parameter=None,\n            **kwargs,\n        ):\n            if custom_parameter:\n                return expected.copy(deep=True)\n            else:\n                return xr.Dataset()\n\n    actual = xr.open_dataset(\"fake_filename\", engine=CustomBackend, custom_parameter=True)\n    assert_identical(expected, actual)\n", "def test_open_dataset_with_custom_engine_and_decode_coords():\n    expected = xr.Dataset(\n        data_vars={'var': ('x', np.arange(5))},\n        coords={'coord': ('x', np.arange(5))}\n    )\n\n    class CustomBackend(xr.backends.BackendEntrypoint):\n            filename_or_obj,\n            drop_variables=None,\n            decode_coords=None,\n            **kwargs,\n        ):\n            if decode_coords:\n                return expected.set_coords('coord')\n            else:\n                return expected.copy(deep=True)\n\n    actual = xr.open_dataset(\"fake_filename\", engine=CustomBackend, decode_coords=True)\n    assert_identical(expected.set_coords('coord'), actual)\n", "def test_open_dataset_with_decode_coords():\n    data = xr.Dataset(\n        {\"var1\": (\"x\", [1, 2, 3]), \"coord1\": (\"x\", [\"a\", \"b\", \"c\"])},\n        coords={\"x\": [0, 1, 2]},\n    )\n    data.to_netcdf(\"test.nc\")\n\n    ds = xr.open_dataset(\"test.nc\", decode_coords=\"coordinates\")\n    assert \"coord1\" in ds.coords\n", "def test_to_netcdf_custom_engine():\n    expected = xr.Dataset(\n        dict(a=2 * np.arange(5)), coords=dict(x=(\"x\", np.arange(5), dict(units=\"s\")))\n    )\n\n    class CustomBackend(xr.backends.BackendEntrypoint):\n            filename_or_obj,\n            drop_variables=None,\n            **kwargs,\n        ):\n            return expected.copy(deep=True)\n\n            dataset,\n            path_or_file=None,\n            mode=\"w\",\n            format=None,\n            group=None,\n            encoding=None,\n            unlimited_dims=None,\n            compute=True,\n            multifile=False,\n            invalid_netcdf=False,\n        ):\n            return \"custom_netcdf_output\"\n\n    actual = xr.to_netcdf(expected, engine=CustomBackend)\n    assert actual == \"custom_netcdf_output\"\n"], "sample_1161": ["def test_issue_21537():\n    assert str(Mul(x, Pow(1/y, -1, evaluate=False), evaluate=False)) == 'x/(1/y)'\n", "def test_ComplexRootOf():\n    assert str(ComplexRootOf(x**5 + 2*x - 1, 0)) == \"ComplexRootOf(x**5 + 2*x - 1, 0)\"\n", "def test_Transpose():\n    A = MatrixSymbol('A', 2, 2)\n    assert str(A.T) == \"A.T\"\n", "def test_ImmutableMatrix():\n    from sympy.matrices.immutable import ImmutableMatrix\n    A = ImmutableMatrix([[1, 2], [3, 4]])\n    assert str(A) == \"Matrix([\\n[1, 2],\\n[3, 4]])\"\n", "def test_Dimension():\n    from sympy import Dimension\n    d = Dimension('d')\n    assert str(d) == 'd'\n"], "sample_70": ["    def test_fast_delete_hidden_relationships(self):\n        p = Parent.objects.create()\n        Child.objects.create(parent_ptr=p)\n        # 1 for self delete, 1 for fast delete of hidden child qs.\n        self.assertNumQueries(2, p.delete)\n        self.assertFalse(Parent.objects.exists())\n        self.assertFalse(Child.objects.exists())\n", "def test_fast_delete_with_signal_listeners(self):\n    # Test fast delete when there are signal listeners\n        pass\n\n    models.signals.pre_delete.connect(noop, sender=User)\n    models.signals.post_delete.connect(noop, sender=User)\n\n    u = User.objects.create(\n        avatar=Avatar.objects.create()\n    )\n    a = Avatar.objects.get(pk=u.avatar_id)\n\n    # 1 query to find the users for the avatar (fast delete is not possible)\n    # 1 query to delete the user\n    # 1 query to delete the avatar\n    self.assertNumQueries(3, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n\n    models.signals.pre_delete.disconnect(noop, sender=User)\n    models.signals.post_delete.disconnect(noop, sender=User)\n", "def test_fast_delete_signals(self):\n    calls = []\n\n        calls.append('')\n\n    models.signals.post_delete.connect(noop, sender=User)\n\n    u = User.objects.create(avatar=Avatar.objects.create())\n    a = Avatar.objects.get(pk=u.avatar_id)\n    # User can be fast-deleted, but post_delete signal is connected.\n    # So, we expect 1 query for User and 1 query for Avatar.\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    self.assertEqual(len(calls), 1)\n\n    models.signals.post_delete.disconnect(noop, sender=User)\n", "    def test_fast_delete_o2o(self):\n        parent = Parent.objects.create()\n        child = Child.objects.create(parent_ptr=parent)\n        # 1 for self delete, 1 for fast delete of empty \"child\" qs.\n        self.assertNumQueries(2, parent.delete)\n        self.assertFalse(Parent.objects.exists())\n        self.assertFalse(Child.objects.exists())\n", "def test_fast_delete_with_signals(self):\n    from django.db import models\n\n    # Define a simple signal to check if it's called\n        pre_delete_signal.called = True\n    pre_delete_signal.called = False\n\n    models.signals.pre_delete.connect(pre_delete_signal, sender=User)\n\n    u = User.objects.create()\n    u.delete()\n\n    # If signals are present, fast delete should not be used\n    self.assertTrue(pre_delete_signal.called)\n    self.assertEqual(User.objects.count(), 0)\n\n    models.signals.pre_delete.disconnect(pre_delete_signal, sender=User)\n"], "sample_811": ["def test_check_paired_arrays():\n    # Ensure that paired array check works for dense matrices.\n    XA = np.resize(np.arange(40), (5, 8))\n    XB = np.resize(np.arange(40), (5, 8))\n    XA_checked, XB_checked = check_paired_arrays(XA, XB)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n", "def test_check_tuple_input_different_dimensions():\n    # Ensures that an error is raised if the dimensions are different.\n    XA = tuplify(np.resize(np.arange(45), (5, 9)))\n    XB = tuplify(np.resize(np.arange(32), (4, 8)))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n", "def test_check_different_number_of_samples():\n    # Ensure an error is raised if the number of samples is different.\n    XA = np.resize(np.arange(40), (5, 8))\n    XB = np.resize(np.arange(64), (8, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, XB)\n", "def test_manhattan_distances_sparse():\n    # Check the pairwise Manhattan distances computation for sparse matrices\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    X[X < 0.8] = 0  # make it sparse\n    Y = rng.random_sample((10, 4))\n    Y[Y < 0.8] = 0  # make it sparse\n\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n\n    D_dense = manhattan_distances(X, Y)\n    D_sparse = manhattan_distances(X_sparse, Y_sparse)\n\n    assert_array_almost_equal(D_dense, D_sparse)\n", "def test_check_float64_conversion():\n    # Ensures that float32 input is converted to float64.\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n"], "sample_483": ["def test_check_autocomplete_fields(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = \"title\"\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields' must be a list or tuple.\",\n            obj=SongAdmin,\n            id=\"admin.E036\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_model_form_subclasses_base_model_form(self):\n    class SongAdmin(admin.ModelAdmin):\n        form = forms.Form  # Not a subclass of BaseModelForm\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'form' must inherit from 'BaseModelForm'.\",\n            obj=SongAdmin,\n            id=\"admin.E016\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_raw_id_fields_type(self):\n    class RawIdFieldsTypeAdmin(admin.ModelAdmin):\n        raw_id_fields = \"test\"\n\n    self.assertEqual(\n        RawIdFieldsTypeAdmin(Album, AdminSite()).check(),\n        [\n            checks.Error(\n                \"The value of 'raw_id_fields' must be a list or tuple.\",\n                obj=RawIdFieldsTypeAdmin,\n                id=\"admin.E001\",\n            )\n        ],\n    )\n", "def test_check_field_spec_item_with_through_field(self):\n    \"\"\"\n    Test _check_field_spec_item with a field that specifies a through model.\n    \"\"\"\n\n    class BookAdmin(admin.ModelAdmin):\n        fields = [\"authors\"]\n\n    errors = BookAdmin(Book, AdminSite())._check_field_spec_item(BookAdmin, \"authors\", \"fields\")\n    expected = [\n        checks.Error(\n            \"The value of 'fields' cannot include the ManyToManyField 'authors', \"\n            \"because that field manually specifies a relationship model.\",\n            obj=BookAdmin,\n            id=\"admin.E013\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_non_editable_fields(self):\n    \"\"\"\n    Regression test for #12691 - if a field is not editable, it can't be included in\n    list_editable.\n    \"\"\"\n\n    class SongAdmin(admin.ModelAdmin):\n        list_display = [\"pk\", \"title\", \"original_release\"]\n        list_editable = [\"title\", \"original_release\"]\n        readonly_fields = [\"original_release\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_editable[1]' refers to 'original_release', \"\n            \"which is not editable through the admin.\",\n            obj=SongAdmin,\n            id=\"admin.E125\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_10": ["def test_rename_columns_invalid_names_messages():\n    t = table.table_helpers.simple_table(1)\n    with pytest.raises(KeyError, match='column \"d\" does not exist'):\n        t.rename_columns(['c', 'd'], ['c', 'd2'])\n    with pytest.raises(KeyError,\n                       match='columns {\\'[de]\\', \\'[de]\\'} do not exist'):\n        t.rename_columns(['c', 'd', 'e'], ['c', 'd2', 'e2'])\n", "def test_rename_columns_invalid_names_messages():\n    t = table.table_helpers.simple_table(1)\n    with pytest.raises(KeyError, match='column \"d\" does not exist'):\n        t.rename_columns(['c', 'd'], ['x', 'y'])\n    with pytest.raises(KeyError,\n                       match='columns {\\'[de]\\', \\'[de]\\'} do not exist'):\n        t.rename_columns(['c', 'd', 'e'], ['x', 'y', 'z'])\n", "def test_remove_columns_with_empty_list():\n    t = table.table_helpers.simple_table(1)\n    t.remove_columns([])\n    assert t.colnames == ['a', 'b', 'c']\n", "def test_init_with_list_of_masked_arrays():\n    \"\"\"Test the fix for #8977\"\"\"\n    m0 = np.ma.array([0, 1, 2], mask=[True, False, True])\n    m1 = np.ma.array([3, 4, 5], mask=[False, True, False])\n    mc = [m0, m1]\n\n    # Test _init_from_list\n    t = table.Table([mc], names=['a'])\n\n    # Test add_column\n    t['b'] = [m1, m0]\n\n    assert t['a'].shape == (2, 3)\n    assert np.all(t['a'][0] == m0)\n    assert np.all(t['a'][1] == m1)\n    assert np.all(t['a'][0].mask == m0.mask)\n    assert np.all(t['a'][1].mask == m1.mask)\n\n    assert t['b'].shape == (2, 3)\n    assert np.all(t['b'][0] == m1)\n    assert np.all(t['b'][1] == m0)\n    assert np.all(t['b'][0].mask == m1.mask)\n    assert np.all(t['b'][1].mask == m0.mask)\n", "def test_add_columns_from_other_table():\n    # Test for #12558 to add columns from another table\n    t1 = table.Table({'a': [1, 2], 'b': [3, 4]})\n    t2 = table.Table({'c': [5, 6], 'd': [7, 8]})\n    t1.add_columns([t2['c'], t2['d']])\n    assert t1.colnames == ['a', 'b', 'c', 'd']\n    assert np.all(t1['c'] == [5, 6])\n    assert np.all(t1['d'] == [7, 8])\n"], "sample_717": ["def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                download_if_missing=False)\n\n    # The data is cropped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 2 * 2914))\n\n    # the target is array of binary values indicating whether the pairs are of the same person or not\n    assert_array_equal(lfw_pairs.target, [1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n\n    # target names are 'Different persons' and 'Same person'\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n\n    # It is possible to ask for the original data without any cropping or color conversion\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA, resize=None,\n                                slice_=None, color=True, download_if_missing=False)\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 250, 250, 3))\n\n    # the target and target names are the same as previously\n    assert_array_equal(lfw_pairs.target, [1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                download_if_missing=False)\n\n    # The data is cropped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 2 * 62 * 47))\n\n    # the target is an array of binary labels indicating if the pair is of the same person or not\n    assert_array_equal(lfw_pairs.target, [0, 0, 1, 1, 0, 1, 0, 1, 0, 1])\n\n    # target names are 'Different persons' and 'Same person'\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n\n    # It is possible to ask for the original data without any cropping or color conversion\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA, resize=None,\n                                slice_=None, color=True, download_if_missing=False)\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 250, 250, 3))\n\n    # the target and target names are the same as previously\n    assert_array_equal(lfw_pairs.target, [0, 0, 1, 1, 0, 1, 0, 1, 0, 1])\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n\n    # Testing with an invalid subset\n    assert_raises(ValueError, fetch_lfw_pairs, subset='invalid', data_", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 13496))\n\n    # the target is array of binary values (0 for different persons, 1 for same person)\n    assert_array_equal(lfw_pairs.target, [1, 0, 0, 0, 0, 1, 1, 0, 0, 1])\n\n    # target_names are the possible outcomes of the target\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n\n    # It is possible to ask for the original data without any croping or color\n    # conversion\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, resize=None,\n                                 slice_=None, color=True,\n                                 download_if_missing=False)\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 250, 250, 3))\n\n    # the target and target_names are the same as previously\n    assert_array_equal(lfw_pairs.target, [1, 0, 0, 0, 0, 1, 1, 0, 0, 1])\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, subset='train', download_if_missing=False)\n\n    # The data is a set of pairs of images, each image is cropped around the center and converted to gray levels\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 12428))\n\n    # the target is array of binary labels indicating whether the pair is from the same person or not\n    assert_array_equal(lfw_pairs.target, [1, 0, 1, 0, 1, 0, 1, 0, 0, 0])\n\n    # target_names indicate the meaning of the labels\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n\n    # It is possible to ask for the original data without any cropping or color conversion\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, subset='train', resize=None, slice_=None, color=True, download_if_missing=False)\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 250, 250, 3))\n\n    # the labels and names are the same as previously\n    assert_array_equal(lfw_pairs.target, [1, 0, 1, 0, 1, 0, 1, 0, 0, 0])\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 5828))\n\n    # the target is an array of binary values (0 for different persons, 1 for same person)\n    assert_array_equal(lfw_pairs.target, [1, 1, 1, 0, 0, 0, 1, 0, 0, 0])\n\n    # the target_names are descriptive\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n\n    # It is possible to ask for the original data without any croping or color\n    # conversion\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                resize=None, slice_=None, color=True,\n                                download_if_missing=False)\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 250, 250, 3))\n"], "sample_140": ["    def test_sensitive_post_parameters_decorator(self):\n        request = self.client.post('/sensitive_view/', {'password': 'secret', 'email': 'user@example.com'})\n        self.assertEqual(request.sensitive_post_parameters, ['password'])\n        response = sensitive_view(request)\n        self.assertNotContains(response, 'secret', status_code=500)\n", "    def test_sensitive_variables_decorator(self):\n        @sensitive_variables('password')\n            return password\n\n        result = test_func('secret')\n        self.assertEqual(result, 'secret')\n        self.assertEqual(test_func.sensitive_variables, ('password',))\n", "def test_sensitive_post_parameters_with_all(self):\n    \"\"\"\n    All POST parameters are treated as sensitive when no arguments are\n    provided to the sensitive_post_parameters decorator.\n    \"\"\"\n    request = self.rf.post('/some_url/', self.breakfast_data)\n    response = sensitive_post_parameters()(sensitive_view)(request)\n    self.verify_paranoid_response(response, check_for_vars=False)\n", "    def test_sensitive_callable(self):\n        \"\"\"\n        Callable settings which contain sensitive information should not be\n        evaluated in the debug page.\n        \"\"\"\n            return \"sensitive_info\"\n\n        with self.settings(DEBUG=True, SENSITIVE_SETTING=callable_setting):\n            response = self.client.get('/raises500/')\n            self.assertNotContains(response, 'sensitive_info', status_code=500)\n", "    def test_sensitive_variables_wrapper(self):\n        @sensitive_variables('password')\n            pass\n\n        self.assertTrue(hasattr(test_func, 'sensitive_variables'))\n        self.assertEqual(test_func.sensitive_variables, ('password',))\n"], "sample_971": ["def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message')\n        assert 'WARNING: prefix: message' in warning.getvalue()\n\n    # It also works during pending_warnings.\n    with logging.pending_warnings():\n        with prefixed_warnings(\"prefix:\"):\n            logger.warning('message')\n            assert 'WARNING: prefix: message' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message')\n    assert 'prefix: WARNING: message' in warning.getvalue()\n\n    # It also works during pending_warnings.\n    with logging.pending_warnings():\n        with prefixed_warnings(\"pending:\"):\n            logger.warning('message')\n    assert 'pending: WARNING: message' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message')\n\n    assert \"prefix: WARNING: message\" in warning.getvalue()\n\n    warning.truncate(0)\n    warning.seek(0)\n\n    with prefixed_warnings(\"another:\"):\n        logger.warning('another message')\n\n    assert \"another: WARNING: another message\" in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message')\n\n    assert 'prefix: WARNING: message' in warning.getvalue()\n\n    # It also works during pending_warnings.\n    warning.truncate(0)\n    warning.seek(0)\n    with logging.pending_warnings():\n        with prefixed_warnings(\"another_prefix:\"):\n            logger.warning('another_message')\n\n    assert 'another_prefix: WARNING: another_message' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message')\n        assert \"WARNING: prefix: message\" in warning.getvalue()\n\n    # Test nested prefixed_warnings\n    with prefixed_warnings(\"outer:\"):\n        logger.warning('message1')\n        with prefixed_warnings(\"inner:\"):\n            logger.warning('message2')\n        logger.warning('message3')\n\n    assert \"WARNING: outer: message1\" in warning.getvalue()\n    assert \"WARNING: outer: inner: message2\" in warning.getvalue()\n    assert \"WARNING: outer: message3\" in warning.getvalue()\n"], "sample_382": ["def test_python_file_changed(self, mock_reset):\n    python_file_path = Path(__file__).parent / 'templates' / 'custom_filter.py'\n    self.assertFalse(autoreload.template_changed(None, python_file_path))\n    mock_reset.assert_not_called()\n", "    def test_template_directories_with_different_paths(self, mock_to_path):\n        # Mock the to_path function to return the same input\n        mock_to_path.side_effect = lambda x: x\n\n        # Override settings with different types of paths\n        with override_settings(\n            TEMPLATES=[\n                {\n                    'DIRS': [\n                        '/absolute/path',\n                        'relative/path',\n                        Path('/mixed/path'),\n                    ],\n                    'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                }\n            ]\n        ):\n            self.assertSetEqual(\n                autoreload.get_template_directories(),\n                {\n                    '/absolute/path',\n                    Path.cwd() / 'relative/path',\n                    '/mixed/path',\n                }\n            )\n\n        mock_to_path.assert_has_calls([\n            mock.call('/absolute/path'),\n            mock.call('relative/path'),\n            mock.call('/mixed/path'),\n        ])\n", "    def test_python_file_changed(self, mock_reset):\n        python_path = Path(__file__).parent / 'templates' / 'custom_filters.py'\n        self.assertTrue(autoreload.template_changed(None, python_path))\n        mock_reset.assert_called_once()\n", "    def test_reset_jinja2_loaders(self, mock_reset):\n        autoreload.reset_loaders()\n        mock_reset.assert_called_once()\n", "def test_custom_loader_template_changed(self, mock_loader_reset, mock_get_dirs):\n    template_path = EXTRA_TEMPLATES_DIR / 'index.html'\n    self.assertTrue(autoreload.template_changed(None, template_path))\n    mock_loader_reset.assert_called_once()\n"], "sample_642": ["def test_verbose_abbreviation_with_value(capsys: CaptureFixture) -> None:\n    \"\"\"Test that we correctly handle an abbreviated pre-processable option with a value.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            with pytest.raises(SystemExit):\n                Run([\"--rcfile=test.rc\"])\n            out = capsys.readouterr()\n            # This output only exists when launched in verbose mode\n            assert \"Using config file: test.rc\" in out.err\n", "def test_init_hook_abbreviation(capsys: CaptureFixture) -> None:\n    \"\"\"Test that we correctly handle an abbreviated pre-processable init-hook option.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            with mock.patch('builtins.exec') as mocked_exec:\n                with pytest.raises(SystemExit):\n                    Run([\"--init-ho\", \"print('test')\"])\n            mocked_exec.assert_called_once_with(\"print('test')\")\n            out = capsys.readouterr()\n            # This output only exists when launched in verbose mode\n            assert \"No config file found, using default configuration\" in out.err\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that options are correctly preprocessed.\"\"\"\n    run = Run([])\n    processed_args = config._preprocess_options(run, [\"--verbose\", \"--rcfile\", \"test.rc\"])\n    assert run.verbose is True\n    assert run._rcfile == \"test.rc\"\n    assert processed_args == []\n", "def test_set_output_abbreviation(capsys: CaptureFixture) -> None:\n    \"\"\"Test that we correctly handle an abbreviated set-output option.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            with mock.patch.object(Run, \"_output\", None):\n                Run([\"--out\", \"test_output.txt\"])\n            out = capsys.readouterr()\n            assert Run._output == \"test_output.txt\"\n", "def test_init_hook_abbreviation() -> None:\n    \"\"\"Test that the init-hook pre-processable option handles an abbreviation.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            init_hook = \"print('Test init-hook')\"\n            with mock.patch('builtins.exec') as mocked_exec:\n                with pytest.raises(SystemExit):\n                    Run([f\"--in={init_hook}\"])\n                mocked_exec.assert_called_once_with(init_hook)\n"], "sample_420": ["    def test_descriptor_raises_validation_error_field_specific(self):\n        form_class = modelform_factory(\n            model=StrictAssignmentDescriptorFieldSpecific, fields=[\"title\"]\n        )\n        form = form_class(data={\"title\": \"testing descriptor\"}, files=None)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors,\n            {\"title\": [\"Cannot set descriptor\", \"This field cannot be blank.\"]},\n        )\n", "def test_setattr_raises_validation_error_multiple_fields(self):\n    \"\"\"\n    A model ValidationError using the dict form with multiple fields should put the error\n    messages into the correct keys of form.errors.\n    \"\"\"\n    form_class = modelform_factory(\n        model=StrictAssignmentMultipleFields, fields=[\"title\", \"description\"]\n    )\n    form = form_class(\n        data={\"title\": \"testing setattr\", \"description\": \"testing setattr\"},\n        files=None,\n    )\n    # This line turns on the ValidationError; it avoids the model erroring\n    # when its own __init__() is called when creating form.instance.\n    form.instance._should_error = True\n    self.assertFalse(form.is_valid())\n    self.assertEqual(\n        form.errors,\n        {\n            \"title\": [\"Cannot set attribute\"],\n            \"description\": [\"Cannot set attribute\", \"This field cannot be blank.\"],\n        },\n    )\n", "def test_setattr_raises_validation_error_custom_message(self):\n    \"\"\"\n    A model ValidationError with a custom message should be passed through to the form.\n    \"\"\"\n    form_class = modelform_factory(model=StrictAssignmentCustomMessage, fields=[\"title\"])\n    form = form_class(data={\"title\": \"testing setattr\"}, files=None)\n    # This line turns on the ValidationError; it avoids the model erroring\n    # when its own __init__() is called when creating form.instance.\n    form.instance._should_error = True\n    self.assertFalse(form.is_valid())\n    self.assertEqual(\n        form.errors,\n        {\"title\": [\"This is a custom error message.\"]},\n    )\n", "    def test_validation_error_on_save(self):\n        \"\"\"\n        A model ValidationError not using the dict form should raise an error\n        when the form is saved.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentAll, fields=[\"title\"])\n        form = form_class(data={\"title\": \"testing setattr\"}, files=None)\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        with self.assertRaisesMessage(ValidationError, \"Cannot set attribute\"):\n            form.save()\n", "def test_model_form_applies_localize_to_no_fields(self):\n    class NoLocalizedTripleForm(forms.ModelForm):\n        class Meta:\n            model = Triple\n            localized_fields = ()\n            fields = \"__all__\"\n\n    f = NoLocalizedTripleForm({\"left\": 10, \"middle\": 10, \"right\": 10})\n    self.assertTrue(f.is_valid())\n    self.assertFalse(f.fields[\"left\"].localize)\n    self.assertFalse(f.fields[\"middle\"].localize)\n    self.assertFalse(f.fields[\"right\"].localize)\n"], "sample_31": ["def test_write_latex_no_latex_names(self, write, tmp_path, format):\n    \"\"\"Test to write a LaTeX file without using LaTeX names\"\"\"\n    fp = tmp_path / \"test_write_latex_no_latex_names.tex\"\n    write(fp, format=format, latex_names=False)\n    tbl = QTable.read(fp)\n    # asserts each column name has not been changed to LaTeX format\n    for column_name in tbl.colnames[2:]:\n        assert column_name not in _FORMAT_TABLE.values()\n", "def test_latex_names(self, write, tmp_path, latex_names):\n    fp = tmp_path / \"test_latex_names.tex\"\n    write(fp, latex_names=latex_names)\n    tbl = QTable.read(fp)\n    # asserts each column name matches the expectation\n    expected_names = [_FORMAT_TABLE.get(k, k) if latex_names else k for k in self.cosmo.__parameters__]\n    for column_name, expected_name in zip(tbl.colnames[2:], expected_names):\n        assert column_name == expected_name\n", "def test_write_latex_file_object(self, write, tmp_path, format):\n    \"\"\"Test writing to a file object\"\"\"\n    fp = tmp_path / \"test_write_latex_file_object.tex\"\n    with open(fp, \"w\") as f:\n        write(f, format=format)\n    # Check the content of the file\n    with open(fp, \"r\") as f:\n        content = f.read()\n        assert \"\\\\begin{table}\" in content\n        assert \"\\\\end{table}\" in content\n", "def test_write_latex_file_like_object(self, write, tmp_path, format):\n    \"\"\"Test writing LaTeX to a file-like object\"\"\"\n    with open(tmp_path / \"test_write_latex_file_like_object.tex\", \"w\") as fp:\n        write(fp, format=format)\n    # Read the file to check if the content is correct\n    with open(tmp_path / \"test_write_latex_file_like_object.tex\", \"r\") as fp:\n        content = fp.read()\n        assert \"\\\\begin{table}\" in content\n        assert \"\\\\end{table}\" in content\n", "def test_write_latex_kwargs(self, write, tmp_path, format):\n    \"\"\"Test passing kwargs to write_latex\"\"\"\n    fp = tmp_path / \"test_write_latex_kwargs.tex\"\n    write(fp, format=format, latex_names=False, overwrite=True, meta_data={'test': 123})\n    # Add assertions here to check the written LaTeX file content and metadata\n"], "sample_64": ["    def test_server_error(self):\n        response = HttpResponseServerError()\n        self.assertEqual(response.status_code, 500)\n\n        # Standard HttpResponse init args can be used\n        response = HttpResponseServerError(\n            content='The server encountered an internal error',\n            content_type='text/html',\n        )\n        self.assertContains(response, 'The server encountered an internal error', status_code=500)\n", "def test_file_response(self):\n    filename = os.path.join(os.path.dirname(__file__), 'abc.txt')\n    with open(filename, 'rb') as file:\n        response = FileResponse(file)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(filename)))\n        self.assertIn('Content-Type', response)\n        self.assertIsInstance(response.file_to_stream, file)\n", "    def test_redirect_with_content(self):\n        response = HttpResponseRedirect('/redirected/', content='Redirecting...', content_type='text/plain')\n        self.assertContains(response, 'Redirecting...', status_code=302)\n        self.assertEqual(response['Content-Type'], 'text/plain')\n", "def test_file_response_content_type(self):\n    # Test the content type of FileResponse\n    file = open(os.path.join(os.path.dirname(__file__), 'test.txt'), 'rb')\n    response = FileResponse(file)\n    self.assertEqual(response['Content-Type'], 'text/plain')\n\n    # Test the content type of FileResponse with filename\n    file = open(os.path.join(os.path.dirname(__file__), 'test.json'), 'rb')\n    response = FileResponse(file, filename='test.json')\n    self.assertEqual(response['Content-Type'], 'application/json')\n\n    # Test the content type of FileResponse with unsupported file extension\n    file = open(os.path.join(os.path.dirname(__file__), 'test.xyz'), 'rb')\n    response = FileResponse(file, filename='test.xyz')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n", "    def test_redirect_with_scheme(self):\n        # Test redirect with scheme 'http'\n        response = HttpResponseRedirect('http://example.com')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, 'http://example.com')\n\n        # Test redirect with scheme 'https'\n        response = HttpResponseRedirect('https://example.com')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, 'https://example.com')\n\n        # Test redirect with scheme 'ftp'\n        response = HttpResponseRedirect('ftp://example.com')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, 'ftp://example.com')\n\n        # Test redirect with unsupported scheme\n        with self.assertRaises(DisallowedRedirect):\n            HttpResponseRedirect('ssh://example.com')\n"], "sample_694": ["def test_argument_type_str_choice(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\n                \"--option\", type=\"str\", choices=['a', 'b'],\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\", \"--option\", \"a\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str', but when supplied should be a type.*\",\n            \"*For choices this is optional and can be omitted, but when supplied should be a type.*\",\n        ]\n    )\n", "def test_argument_type_str_choice(pytester: Pytester) -> None:\n    \"\"\"Test that using a string for type argument with choices is deprecated (#8367).\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            parser.addoption('--foo', type='int', choices=[1, 2])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'int'. \"\n            \"For choices this is optional and can be omitted, \"\n            \"but when supplied should be a type (for example `str` or `int`). \"\n            \"(options: ['--foo'])\",\n        ]\n    )\n    result.assert_outcomes(warnings=1)\n", "def test_argument_type_str_choice(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            group = parser.getgroup(\"test_group\")\n            group.addoption('--test-option', type='str', choices=['a', 'b'])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str', \"\n            \"but when supplied should be a type (for example `str` or `int`). (options: --test-option)\",\n        ]\n    )\n", "def test_argument_type_str_choice(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            group = parser.getgroup('custom options')\n            group.addoption(\n                '--custom-option', type='str', choices=['foo', 'bar'], default='foo',\n                help='Custom option with choices'\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str', \"\n            \"but when supplied should be a type (for example `str` or `int`).*\",\n        ]\n    )\n", "def test_argparse_type_str_choice(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            parser.addoption(\"--str-choice\", type=\"str\", choices=[\"foo\", \"bar\"])\n            parser.addoption(\"--wrong-choice\", type=\"str\", choices=[\"foo\"])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str'.*\",\n            \"*options: --str-choice, --wrong-choice\",\n        ]\n    )\n    result.assert_outcomes(warnings=2)\n"], "sample_159": ["def test_username_unique(self):\n    \"\"\"A unique USERNAME_FIELD shouldn't raise any errors.\"\"\"\n    class CustomUserUniqueUsername(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n    errors = checks.run_checks()\n    self.assertEqual(errors, [])\n", "def test_no_model_backend(self):\n    \"\"\"\n    A non-unique USERNAME_FIELD with a custom authentication backend\n    raises a warning.\n    \"\"\"\n    class CustomUserNoModelBackend(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=False)\n        USERNAME_FIELD = 'username'\n\n    with self.settings(AUTHENTICATION_BACKENDS=['my.custom.backend']):\n        errors = checks.run_checks()\n        self.assertEqual(errors, [\n            checks.Warning(\n                \"'CustomUserNoModelBackend.username' is named as \"\n                \"the 'USERNAME_FIELD', but it is not unique.\",\n                hint='Ensure that your authentication backend(s) can handle non-unique usernames.',\n                obj=CustomUserNoModelBackend,\n                id='auth.W004',\n            ),\n        ])\n", "def test_username_field_exists(self):\n    \"\"\"USERNAME_FIELD should be defined.\"\"\"\n    class CustomUserNoUserNameField(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        REQUIRED_FIELDS = ['username']\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"'CustomUserNoUserNameField' must have a USERNAME_FIELD.\",\n            obj=CustomUserNoUserNameField,\n            id='auth.E004',\n        ),\n    ])\n", "    def test_builtin_permission_name_length(self):\n        \"\"\"\n        The builtin permission name length should not exceed the max length for Permission name.\n        \"\"\"\n        class CustomUserWithBuiltinPermission(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = []\n\n            class Meta:\n                permissions = [('this_permission_name_is_really_really_long' * 5, 'Custom permission')]\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].id, 'auth.E007')\n", "    def test_is_superuser_method(self):\n        \"\"\"\n        <User Model>.is_superuser must not be a method.\n        \"\"\"\n        class CustomUserMethodIsSuperuser(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n                return True\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Critical(\n                '%s.is_superuser must be an attribute or property rather than '\n                'a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as superusers!' % CustomUserMethodIsSuperuser,\n                obj=CustomUserMethodIsSuperuser,\n                id='auth.C009',\n            ),\n        ])\n"], "sample_1082": ["def test_asech_real_assumptions():\n    z = Symbol('z', real=False)\n    assert asech(z).is_real is None\n", "def test_asinh_expansion():\n    x = Symbol('x')\n    assert asinh(x).expand(trig=True) == log(x + sqrt(x**2 + 1))\n    assert asinh(2*x).expand(trig=True).expand() == log(2*x + sqrt(4*x**2 + 1))\n    assert asinh(3*x).expand(trig=True).expand() == log(3*x + sqrt(9*x**2 + 1))\n", "def test_asech_expansion():\n    x = Symbol('x')\n    assert asech(x).expansion_term(2, x) == -x**2/2\n    assert asech(x).expansion_term(4, x) == x**4/8\n    assert asech(x).expansion_term(6, x) == -5*x**6/96\n", "def test_asinh_expansion():\n    x = Symbol('x')\n    assert asinh(x).expand(trig=True) == log(x + sqrt(x**2 + 1))\n    assert asinh(sinh(x)).expand(trig=True) == x\n    assert asinh(tanh(x)).expand(trig=True) == log(tanh(x) + sqrt(1 - tanh(x)**2))\n", "def test_acsch_expansion():\n    x = Symbol('x')\n    assert acsch(x).series(x, 0, 10) == \\\n        1/x - 1/3*x + 1/45*x**3 - 1/945*x**5 + 2/945*x**7 + O(x**10)\n    t5 = acsch(x).taylor_term(5, x)\n    assert t5 == 2/945*x**5\n    assert acsch(x).taylor_term(7, x, t5, 0) == 2/945*x**7\n"], "sample_848": ["def test_multi_target_classification_one_target():\n    # Test multi target classification raises\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n    y = y.reshape((-1, 1))  # Make it a single-column 2D array\n    clf = MultiOutputClassifier(LogisticRegression())\n    assert_raises(ValueError, clf.fit, X, y)\n", "def test_multi_target_regression_n_jobs():\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    # Test multi-output regression with n_jobs=1\n    rgr_single = MultiOutputRegressor(GradientBoostingRegressor(random_state=0), n_jobs=1)\n    rgr_single.fit(X_train, y_train)\n    y_pred_single = rgr_single.predict(X_test)\n\n    # Test multi-output regression with n_jobs=2\n    rgr_multi = MultiOutputRegressor(GradientBoostingRegressor(random_state=0), n_jobs=2)\n    rgr_multi.fit(X_train, y_train)\n    y_pred_multi = rgr_multi.predict(X_test)\n\n    # Assert that the predictions are the same, even with parallel processing\n    assert_almost_equal(y_pred_single, y_pred_multi)\n", "def test_multi_target_classification_predict_proba():\n    # Test predict_proba method of MultiOutputClassifier\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[0, 1], [2, 0], [1, 2]])\n    base_estimator = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n    multi_target_clf = MultiOutputClassifier(base_estimator)\n    multi_target_clf.fit(X, y)\n    y_proba = multi_target_clf.predict_proba(X)\n    assert len(y_proba) == y.shape[1]\n    for proba in y_proba:\n        assert proba.shape == (X.shape[0], len(np.unique(y[:, 0])))\n", "def test_multi_output_classification_score():\n    # Test the score function of the MultiOutputClassifier\n    forest = RandomForestClassifier(n_estimators=10, random_state=1)\n    multi_target_forest = MultiOutputClassifier(forest)\n\n    # Train the classifier\n    multi_target_forest.fit(X, y)\n\n    # Get the score\n    score = multi_target_forest.score(X, y)\n\n    # Check the score\n    assert 0 <= score <= 1\n", "def test_multi_output_regression_score():\n    # test if the score function of MultiOutputRegressor is working as expected\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    rgr.fit(X_train, y_train)\n    score = rgr.score(X_test, y_test)\n\n    # compute the R^2 of each target separately and then take the average\n    individual_scores = [r2_score(y_test[:, i], rgr.estimators_[i].predict(X_test)) for i in range(3)]\n    expected_score = np.mean(individual_scores)\n\n    assert_almost_equal(score, expected_score)\n"], "sample_473": ["    def test_exception_in_view(self):\n        response = self.client.get(\"/exception_in_view/\")\n        self.assertEqual(response.status_code, 500)\n        self.assertIn(b\"Internal Server Error\", response.content)\n", "    def test_suspicious_operation_in_view_returns_400(self):\n        response = self.client.get(\"/suspicious_operation/\")\n        self.assertEqual(response.status_code, 400)\n", "    def test_uncaught_exception(self):\n        response = self.client.get(\"/uncaught_exception/\")\n        self.assertEqual(response.status_code, 500)\n        self.assertIn(b\"Internal Server Error\", response.content)\n", "    def test_view_exception(self):\n        msg = \"View function for url 'handler_error/' raised an exception\"\n        with self.assertRaisesMessage(ValueError, msg):\n            self.client.get(\"/handler_error/\")\n", "compilation error"], "sample_745": ["def test_function_transformer_with_sparse_input():\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    y = np.array([5, 6])\n\n    transformer = FunctionTransformer(func=lambda X, y: X.multiply(y),\n                                      inverse_func=lambda X, y: X.divide(y),\n                                      accept_sparse=True,\n                                      pass_y=True)\n\n    X_transformed = transformer.fit_transform(X, y)\n    expected_transformed = sparse.csr_matrix(np.array([[5, 10], [15, 20]]))\n    assert_allclose_dense_sparse(X_transformed, expected_transformed)\n\n    X_inverse_transformed = transformer.inverse_transform(X_transformed, y)\n    assert_allclose_dense_sparse(X_inverse_transformed, X)\n", "def test_function_transformer_accept_sparse():\n    X_sparse = sparse.csr_matrix([1, 2, 3, 4]).reshape((2, 2))\n\n    # Test that accept_sparse works correctly\n    F = FunctionTransformer(np.sqrt, accept_sparse=True)\n    assert_allclose_dense_sparse(F.transform(X_sparse), np.sqrt(X_sparse.toarray()))\n\n    # Test that accept_sparse=False raises ValueError for sparse input\n    F = FunctionTransformer(np.sqrt, accept_sparse=False)\n    with pytest.raises(ValueError):\n        F.transform(X_sparse)\n", "def test_inverse_transform_with_args():\n    X = np.array([1, 4, 9, 16]).reshape((2, 2))\n    y = np.array([2, 3]).reshape((2, 1))\n\n    # Test that inverse_transform works correctly with additional arguments\n    F = FunctionTransformer(\n        func=lambda X, y: np.sqrt(X) + y,\n        inverse_func=lambda X, y: np.around((X - y) ** 2), inv_kw_args=dict(decimals=3),\n        pass_y=True,\n    )\n    assert_array_equal(\n        F.inverse_transform(F.transform(X, y), y),\n        np.around((np.sqrt(X) + y) ** 2, decimals=3),\n    )\n", "def test_function_transformer_accept_sparse():\n    X_sparse = sparse.csr_matrix(np.array([[1, 0], [0, 1]]))\n    X_dense = np.array([[1, 0], [0, 1]])\n\n    # Test that accept_sparse=False raises an error with sparse input\n    F = FunctionTransformer(np.sqrt, accept_sparse=False)\n    with pytest.raises(TypeError):\n        F.transform(X_sparse)\n\n    # Test that accept_sparse=True works correctly with sparse input\n    F = FunctionTransformer(np.sqrt, accept_sparse=True)\n    assert_array_equal(F.transform(X_sparse).toarray(), np.sqrt(X_dense))\n", "def test_function_transformer_validate_sparse():\n    X_sparse = sparse.csr_matrix([[1, 0], [0, 1]])\n    trans = FunctionTransformer(func=np.sqrt, accept_sparse=False, validate=True)\n    with pytest.raises(ValueError, match=\"sparse input is not allowed\"):\n        trans.fit_transform(X_sparse)\n\n    trans = FunctionTransformer(func=np.sqrt, accept_sparse=True, validate=True)\n    Xt = trans.fit_transform(X_sparse)\n    assert_allclose_dense_sparse(X_sparse, trans.inverse_transform(Xt))\n"], "sample_1184": ["def test_gaussian_conj():\n    s_in, z_r_in, f = symbols('s_in z_r_in f')\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert s_out == 1 / (-1/(s_in + z_r_in**2/(s_in - f)) + 1/f)\n    assert z_r_out == z_r_in / ((1 - (s_in/f)**2) + (z_r_in/f)**2)\n    assert m == 1/sqrt((1 - (s_in/f)**2) + (z_r_in/f)**2)\n", "def test_rayleigh2waist():\n    wavelen, z_r = symbols('wavelen z_r')\n    assert rayleigh2waist(z_r, wavelen) == sqrt(z_r/pi*wavelen)\n", "def test_beam_parameter_divergence():\n    l = symbols('l')\n    w = symbols('w')\n    p = BeamParameter(l, 1, w=w)\n    assert streq(p.divergence, l / (pi * w))\n", "def test_beam_parameter():\n    wavelen = 530e-9\n    z = 1\n    w = 1e-3\n    p = BeamParameter(wavelen, z, w=w)\n    assert N(p.q) == 1 + 5.92753330865999*I\n    assert N(p.w_0) == 0.001\n    assert N(p.z_r) == 5.92753330865999\n    fs = FreeSpace(10)\n    p1 = fs*p\n    assert N(p1.w) == 0.00210803120913829\n", "def test_beam_parameter_initialization():\n    p = BeamParameter(530e-9, 1, w=1e-3)\n    assert streq(p.wavelen, 530e-9)\n    assert streq(p.z, 1)\n    assert streq(p.w_0, 1e-3)\n    assert streq(p.z_r, Float(1.8867924528301886, 15))\n"], "sample_360": ["    def test_invalid_cache_backend(self):\n        test_caches = CacheHandler({\n            'invalid_backend': {\n                'BACKEND': 'django.core.cache.backends.invalid.InvalidBackend',\n            },\n        })\n        msg = \"Could not find backend 'django.core.cache.backends.invalid.InvalidBackend': No module named 'django.core.cache.backends.invalid'\"\n        with self.assertRaisesMessage(InvalidCacheBackendError, msg):\n            test_caches['invalid_backend']\n", "    def test_cache_get_or_set_many(self):\n        cache.set_many({\"key1\": \"spam\", \"key2\": \"eggs\"})\n        self.assertEqual(cache.get_or_set_many({\"key1\": \"new_spam\", \"key3\": \"ham\"}), {\"key1\": \"spam\", \"key3\": \"ham\"})\n        self.assertEqual(cache.get(\"key1\"), \"spam\")\n        self.assertEqual(cache.get(\"key2\"), \"eggs\")\n        self.assertEqual(cache.get(\"key3\"), \"ham\")\n", "def test_cache_versioning_with_custom_key(self):\n    custom_key_func = lambda key, *args: key + '_custom'\n    custom_key_cache = caches['custom_key']\n    custom_key_cache.key_func = custom_key_func\n\n    cache.set('answer1', 42)\n    custom_key_cache.set('answer1', 43)\n\n    self.assertEqual(cache.get('answer1'), 42)\n    self.assertEqual(custom_key_cache.get('answer1'), 43)\n\n    cache.set('answer1', 44, version=2)\n    custom_key_cache.set('answer1', 45, version=2)\n\n    self.assertEqual(cache.get('answer1', version=1), 42)\n    self.assertEqual(cache.get('answer1', version=2), 44)\n    self.assertEqual(custom_key_cache.get('answer1', version=1), 43)\n    self.assertEqual(custom_key_cache.get('answer1', version=2), 45)\n", "def test_custom_backend_with_options(self):\n    # Custom backend that takes options\n    test_caches = CacheHandler({\n        'custom_backend': {\n            'BACKEND': 'custom_backend.CustomBackend',\n            'OPTIONS': {'KEY1': 'VALUE1', 'KEY2': 'VALUE2'},\n        },\n    })\n    backend = test_caches['custom_backend']\n    self.assertEqual(backend.options, {'KEY1': 'VALUE1', 'KEY2': 'VALUE2'})\n", "    def tearDown(self):\n        cache.clear()\n"], "sample_1143": ["def test_Float_floor_ceiling():\n    assert Float(2.3).floor() == 2.0\n    assert Float(2.3).ceiling() == 3.0\n    assert Float(-2.3).floor() == -3.0\n    assert Float(-2.3).ceiling() == -2.0\n", "def test_Rational_floor_ceiling():\n    a = Rational(5, 3)\n\n    assert a.floor() == Rational(1, 1)\n    assert a.ceiling() == Rational(2, 1)\n", "def test_Integer_as_numerator_denominator():\n    a = Integer(3)\n    assert a.numerator() == 3\n    assert a.denominator() == 1\n", "def test_Float_floor_ceiling():\n    a = Float('1.23')\n    b = Float('-1.23')\n\n    assert a.floor() == Float('1.0')\n    assert a.ceiling() == Float('2.0')\n    assert b.floor() == Float('-2.0')\n    assert b.ceiling() == Float('-1.0')\n\n    c = Float('2.0')\n    d = Float('-2.0')\n\n    assert c.floor() == c\n    assert c.ceiling() == c\n    assert d.floor() == d\n    assert d.ceiling() == d\n", "def test_GoldenRatio_expand_with_func():\n    assert GoldenRatio.expand(func=True) == S.Half + sqrt(5) / 2\n"], "sample_1009": ["def test_vector_applyfunc():\n    N = ReferenceFrame('N')\n    v1 = x*N.x + y*N.y + z*N.z\n    v2 = v1.applyfunc(sin)\n    assert v2 == sin(x)*N.x + sin(y)*N.y + sin(z)*N.z\n\n    v3 = v1.applyfunc(lambda x: x**2)\n    assert v3 == x**2*N.x + y**2*N.y + z**2*N.z\n\n    # Test that applyfunc raises a TypeError if the argument is not callable\n    with raises(TypeError):\n        v1.applyfunc(\"sin\")\n", "def test_vector_applyfunc():\n    from sympy import sin, cos\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.y])\n    B = A.orientnew('B', 'Axis', [q2, A.z])\n\n    v = q1 * N.x + q2 * A.y + sin(q1) * B.z\n    v_applied = v.applyfunc(cos)\n\n    assert v_applied == cos(q1) * N.x + cos(q2) * A.y + cos(sin(q1)) * B.z\n", "def test_vector_dot_product():\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n\n    v1 = A.x + A.y\n    v2 = B.x + B.z\n    v3 = A.x - A.y\n\n    assert v1 & v2 == 0  # Dot product in different frames is 0\n    assert v1 & v3 == 0  # Dot product of orthogonal vectors is 0\n    assert v1 & v1 == 2  # Dot product of the same vector is the square of its magnitude\n", "def test_vector_express():\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n    u1, u2, u3, u4 = symbols('u1 u2 u3 u4')\n\n    v = u1 * A.x + u2 * A.y + u3 * A.z\n    v_expr = v.express(N)\n\n    assert isinstance(v_expr, Vector)\n    assert v_expr.args == [(u1 * A.dcm(N).T[0], N), (u2 * A.dcm(N).T[1], N), (u3 * A.dcm(N).T[2], N)]\n\n    v = u1 * A.x + u2 * B.y\n    raises(ValueError, lambda: v.express(N))\n", "def test_vector_magnitude_normalize():\n    q1, q2, q3 = symbols('q1 q2 q3')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.x])\n    B = A.orientnew('B', 'Axis', [q2, A.y])\n    C = B.orientnew('C', 'Axis', [q3, B.z])\n\n    v = q1*N.x + q2*N.y + q3*N.z\n    assert v.magnitude() == sqrt(q1**2 + q2**2 + q3**2)\n\n    nv = v.normalize()\n    assert nv.magnitude() == 1\n    assert nv == v/sqrt(q1**2 + q2**2 + q3**2)\n\n    # Test with a vector in a different reference frame\n    v_diff_frame = q1*A.x + q2*A.y + q3*A.z\n    nv_diff_frame = v_diff_frame.normalize()\n    assert nv_diff_frame.magnitude() == 1\n    assert nv_diff_frame == v_diff_frame/sqrt(q1**2 + q2**2 + q3**2)\n\n    # Test with a vector in a rotated reference frame\n    v_rotated_frame = q1*C.x + q2*C.y + q3*C.z\n    nv_rotated_frame = v_rotated_frame.normalize()\n    assert nv_rotated_frame.magnitude() == 1\n    assert nv_rotated_frame == v_rotated_frame/sqrt(q1**2 + q2**2 + q3**2)\n"], "sample_250": ["def test_datetime_with_utc_tzinfo(self):\n    dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=utc)\n    self.assertEqual(datetime.fromtimestamp(int(format(dt, 'U')), utc), dt)\n", "def test_time_formats_no_leading_zero(self):\n    my_birthday = datetime(1979, 7, 8, 10, 5)\n\n    self.assertEqual(dateformat.format(my_birthday, 'g'), '10')\n    self.assertEqual(dateformat.format(my_birthday, 'G'), '10')\n    self.assertEqual(dateformat.format(my_birthday, 'h'), '10')\n    self.assertEqual(dateformat.format(my_birthday, 'H'), '10')\n    self.assertEqual(dateformat.format(my_birthday, 'i'), '05')\n    self.assertEqual(dateformat.format(my_birthday, 's'), '00')\n", "    def test_invalid_date_format_specifiers(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n\n        for specifier in ['b', 'c', 'd', 'D', 'E', 'F', 'I', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'S', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n            msg = (\n                \"The format for time objects may not contain date-related \"\n                \"format specifiers (found '%s').\" % specifier\n            )\n            with self.assertRaisesMessage(TypeError, msg):\n                dateformat.time_format(dt, specifier)\n", "def test_timezone_naive_datetime(self):\n    dt = datetime(2015, 10, 25, 1, 30, 0)\n\n    # Try all formatters that involve self.timezone.\n    self.assertEqual(format(dt, 'e'), '')\n    self.assertEqual(format(dt, 'O'), '+0200')\n    self.assertEqual(format(dt, 'T'), 'CET')\n    self.assertEqual(format(dt, 'Z'), '7200')\n", "    def test_invalid_date_format_specifiers(self):\n        my_time = datetime.time(12, 30, 45)\n\n        for specifier in ['b', 'c', 'd', 'D', 'E', 'F', 'I', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'S', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n            msg = (\n                \"The format for time objects may not contain date-related \"\n                \"format specifiers (found '%s').\" % specifier\n            )\n            with self.assertRaisesMessage(TypeError, msg):\n                dateformat.time_format(my_time, specifier)\n"], "sample_3": ["def test_masked_vals_in_object_subtypes():\n    \"\"\"Test null values in object subtypes.\"\"\"\n    t = Table()\n    t['o'] = np.ma.array([1, 2], mask=[0, 1], dtype=np.int64)\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    # datatype:\n    # - {name: o, datatype: string, subtype: json}\n    # schema: astropy-2.0\n    o\n    1\n    null\n    \"\"\"\n    hdr = _get_ecsv_header_dict(out.getvalue())\n    hdr_exp = _get_ecsv_header_dict(txt)\n    assert hdr == hdr_exp\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2.colnames == t.colnames\n    for name in t2.colnames:\n        assert t2[name].dtype == t[name].dtype\n        assert type(t2[name]) is type(t[name])\n        for val1, val2 in zip(t2[name], t[name]):\n            if isinstance(val1, np.ma.MaskedArray):\n                assert np.all(val1.mask == val2.mask)\n            assert np.all(val1 == val2)\n", "def test_read_complex_v10():\n    \"\"\"Test an ECSV file with a complex column for version 1.0 and above\n    Note: ECSV Version >=1.0 files should raise ValueError for complex datatype.\n    \"\"\"\n    txt = \"\"\"\\", "def test_read_write_all_dtypes():\n    \"\"\"Test that we can read and write all valid ECSV datatypes.\"\"\"\n    t = Table()\n    for dtype in ECSV_DATATYPES:\n        if dtype == 'bool':\n            data = np.array([False, True, False])\n        elif dtype == 'string':\n            data = np.array(['ab 0', 'ab, 1', 'ab2'])\n        else:\n            data = np.arange(3, dtype=dtype)\n        t[dtype] = Column(data)\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t2.colnames == t.colnames\n    for name in t2.colnames:\n        assert np.all(t2[name] == t[name])\n        assert t2[name].dtype == t[name].dtype\n", "def test_read_write_unicode_chars():\n    \"\"\"Test reading and writing of ECSV files containing Unicode characters.\"\"\"\n    t = Table()\n    t['unicode'] = ['h\u00e9ll\u00f6', 'w\u00f3rld']\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t.colnames == t2.colnames\n    assert np.all(t2['unicode'] == t['unicode'])\n", "def test_masked_vals_in_object_subtypes():\n    \"\"\"Test null values in object subtypes.\"\"\"\n    t = Table()\n    t['o'] = np.ma.array([1, 2], mask=[0, 1], dtype=np.int64)\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    # datatype:\n    # - {name: o, datatype: string, subtype: json}\n    # schema: astropy-2.0\n    o\n    1\n    null\n    \"\"\"\n    hdr = _get_ecsv_header_dict(out.getvalue())\n    hdr_exp = _get_ecsv_header_dict(txt)\n    assert hdr == hdr_exp\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2.colnames == t.colnames\n    for name in t2.colnames:\n        assert t2[name].dtype == t[name].dtype\n        assert type(t2[name]) is type(t[name])\n        for val1, val2 in zip(t2[name], t[name]):\n            if isinstance(val1, np.ma.MaskedArray):\n                assert np.all(val1.mask == val2.mask)\n            assert np.all(val1 == val2)\n"], "sample_570": ["def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1)\n", "def test_invalid_errorbar_arg(self):\n\n    with pytest.raises(TypeError, match=\"`errorbar` must be a callable, string, or (string, number) tuple\"):\n        EstimateAggregator(\"mean\", errorbar=123)\n", "def test_univariate_stats(self, x, stat, expected_value):\n    h = Histogram(stat=stat)\n    heights, _ = h(x)\n    assert heights.sum() == pytest.approx(expected_value)\n", "def test_bivariate_percent_stat(self, x, y):\n\n    h = Histogram(stat=\"percent\")\n    heights, _ = h(x, y)\n    assert heights.sum() == 100\n", "def test_no_errorbars(self, long_df):\n\n    agg = EstimateAggregator(\"mean\")\n    out = agg(long_df, \"x\")\n    assert pd.isna(out[\"xmin\"])\n    assert pd.isna(out[\"xmax\"])\n"], "sample_797": ["def test_power_transformer_sparse_input(method, standardize):\n    # Check that PowerTransformer handles sparse input correctly\n    X_dense = np.abs(X_2d)\n    X_sparse = sparse.csr_matrix(X_dense)\n\n    pt_dense = PowerTransformer(method=method, standardize=standardize)\n    pt_sparse = PowerTransformer(method=method, standardize=standardize)\n\n    X_trans_dense = pt_dense.fit_transform(X_dense)\n    X_trans_sparse = pt_sparse.fit_transform(X_sparse)\n\n    assert_array_almost_equal(X_trans_dense, X_trans_sparse.toarray())\n\n    X_inv_trans_dense = pt_dense.inverse_transform(X_trans_dense)\n    X_inv_trans_sparse = pt_sparse.inverse_transform(X_trans_sparse)\n\n    assert_array_almost_equal(X_inv_trans_dense, X_inv_trans_sparse.toarray())\n", "def test_power_transformer_sparse_input(method, standardize):\n    # Check that PowerTransformer handles sparse input correctly\n    X = sparse.csr_matrix(np.abs(X_2d))\n\n    pt = PowerTransformer(method=method, standardize=standardize)\n\n    X_trans = pt.fit_transform(X)\n    X_trans_func = power_transform(\n        X.toarray(), method=method,\n        standardize=standardize\n    )\n\n    assert_array_almost_equal(X_trans.toarray(), X_trans_func)\n\n    # Test inverse transformation\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv.toarray(), X.toarray())\n", "def test_power_transformer_1d_arrays(method, standardize, X):\n    # Check that PowerTransformer works with 1D arrays\n    if method == 'box-cox':\n        X = np.abs(X)\n\n    pt = PowerTransformer(method, standardize)\n    X_trans = pt.fit_transform(X)\n    X_inv_trans = pt.inverse_transform(X_trans)\n\n    if isinstance(X, list):\n        X = np.array(X)  # cast only after scaling done\n\n    assert_array_almost_equal(X, X_inv_trans)\n", "def test_power_transformer_sparse_input(method):\n    # Test that PowerTransformer can handle sparse input\n    X_sparse = sparse.csr_matrix(X_2d)\n    pt = PowerTransformer(method=method, standardize=True)\n    X_trans_sparse = pt.fit_transform(X_sparse)\n\n    # Check that the output is also a sparse matrix\n    assert sparse.issparse(X_trans_sparse)\n\n    # Check that the transform is equivalent to the dense version\n    pt_dense = PowerTransformer(method=method, standardize=True)\n    X_trans_dense = pt_dense.fit_transform(X_2d)\n    assert_array_almost_equal(X_trans_sparse.toarray(), X_trans_dense)\n", "def test_power_transformer_sparse_input(method, standardize):\n    # check that the transformer works with sparse inputs\n    X = sparse.csr_matrix(X_1col)\n    if method == 'box-cox':\n        X.data = np.abs(X.data)\n\n    pt = PowerTransformer(method, standardize)\n    X_trans = pt.fit_transform(X)\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X.toarray(), X_inv_trans.toarray())\n"], "sample_530": ["def test_textarea_multiline_baseline():\n    fig, ax = plt.subplots()\n\n    text0 = TextArea(\"test\\ntest long text\", textprops={'fontsize': 10})\n    ab0 = AnnotationBbox(text0, (0.5, 0.5), xycoords='axes fraction',\n                         boxcoords=\"offset points\", box_alignment=(0., 0.5))\n    ax.add_artist(ab0)\n\n    text1 = TextArea(\"test\\ntest long text\", textprops={'fontsize': 10},\n                     multilinebaseline=True)\n    ab1 = AnnotationBbox(text1, (0.5, 0.3), xycoords='axes fraction',\n                         boxcoords=\"offset points\", box_alignment=(0., 0.5))\n    ax.add_artist(ab1)\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n", "def test_offsetimage_zoom():\n    fig, ax = plt.subplots()\n\n    im = np.random.rand(10, 10)\n    oi1 = OffsetImage(im, zoom=1)\n    oi2 = OffsetImage(im, zoom=2)\n\n    ab1 = AnnotationBbox(oi1, (0.5, 0.5), xycoords='axes fraction',\n                         frameon=False)\n    ax.add_artist(ab1)\n\n    ab2 = AnnotationBbox(oi2, (0.8, 0.8), xycoords='axes fraction',\n                         frameon=False)\n    ax.add_artist(ab2)\n", "def test_offsetbox_get_extent_offsets(mode):\n    # Check the get_extent_offsets method of OffsetBox\n    # with different packing modes\n    ob = OffsetBox()\n    packer = PackerBase(mode=mode)\n    ob._children = [packer]\n    renderer = mock.Mock()\n    renderer.points_to_pixels.return_value = 1.0\n    packer.get_extent.return_value = (50, 30, 10, 5)\n    width, height, xdescent, ydescent, offsets = ob.get_extent_offsets(renderer)\n    assert isinstance(width, float)\n    assert isinstance(height, float)\n    assert isinstance(xdescent, float)\n    assert isinstance(yd descent, float)\n    assert isinstance(offsets, list)\n", "def test_offsetbox_packing():\n    fig, ax = plt.subplots()\n\n    # Create a HPacker with children\n    hpacker = HPacker(pad=4, sep=5, children=[\n        TextArea(\"Child 1\"),\n        TextArea(\"Child 2\"),\n        TextArea(\"Child 3\"),\n    ])\n\n    # Add the HPacker to the axes\n    ax.add_artist(AnchoredOffsetbox(\"center\", child=hpacker))\n\n    ax.set_xlim((0, 1))\n    ax.set_ylim((0, 1))\n", "def test_offsetbox_alignment():\n    fig, ax = plt.subplots()\n\n    # Test alignment of HPacker\n    hpacker = HPacker(children=[TextArea('left'), TextArea('center'), TextArea('right')],\n                      align='left')\n    ax.add_artist(hpacker)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    # Check if the left edge of the first TextArea is aligned with the left edge of the HPacker\n    assert_allclose(hpacker.get_window_extent(renderer).x0,\n                    hpacker.get_children()[0].get_window_extent(renderer).x0)\n\n    # Test alignment of VPacker\n    vpacker = VPacker(children=[TextArea('bottom'), TextArea('center'), TextArea('top')],\n                      align='bottom')\n    ax.add_artist(vpacker)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    # Check if the bottom edge of the first TextArea is aligned with the bottom edge of the VPacker\n    assert_allclose(vpacker.get_window_extent(renderer).y0,\n                    vpacker.get_children()[0].get_window_extent(renderer).y0)\n"], "sample_996": ["def test_product_with_zero_factor():\n    # Test a product with a zero factor\n    assert product(0, (k, 1, n)) == 0\n", "def test_issue_14567():\n    a, b, k = symbols('a b k', integer=True)\n    p = Product(k, (k, a, b)).doit()\n    assert p.subs({a: 1, b: 5}).doit() == factorial(5)\n", "def test_product_with_limits_swap():\n    a, b, c, d = symbols('a b c d', integer=True)\n    p1 = Product(x, (i, a, b)).doit()\n    p2 = Product(x, (i, b + 1, a - 1)).doit()\n    assert simplify(p1 * p2) == 1\n    p3 = Product(x, (i, c, d)).doit()\n    p4 = Product(x, (i, d + 1, c - 1)).doit()\n    assert simplify(p3 * p4) == 1\n", "def test_issue_14500():\n    # Test product with limits reversed and evaluated\n    n = Symbol('n', integer=True, positive=True)\n    p = Product(n, (n, 5, 1))\n    p_reversed = p.reverse_order(n)\n    assert p.doit() == 120\n    assert p_reversed.doit() == 1 / 120\n", "def test_issue_14454():\n    n = Symbol('n', integer=True, positive=True)\n    p = Product(1 + 1/n**2, (n, 1, oo))\n    assert p.is_convergent() is S.true\n    assert p.doit() == pi**2/6\n"], "sample_901": ["def test_k_means_elkan_int_data():\n    # Issue GH #14314\n    X = np.array([[-1], [1]], dtype=np.int)\n    km = KMeans(n_clusters=2, algorithm='elkan')\n    km.fit(X)\n    assert km.cluster_centers_.dtype.kind == \"f\"\n", "def test_k_means_sparse_init():\n    # Check that the initialization works with sparse input\n    X_sparse = sp.csr_matrix(X)\n    init_sparse = sp.csr_matrix(centers.copy())\n    km = KMeans(init=init_sparse, n_clusters=n_clusters, random_state=42, n_init=1)\n    km.fit(X_sparse)\n    _check_fitted_model(km)\n", "def test_k_means_elkan_algorithm_with_sparse_input():\n    # Check that using the 'elkan' algorithm with sparse input raises an error\n    km = KMeans(algorithm='elkan', random_state=42)\n    assert_raise_message(TypeError, \"algorithm='elkan' not supported for sparse input X\", km.fit, X_csr)\n", "def test_k_means_elkan_sparse_data():\n    # check that elkan algorithm works with sparse data\n    X = sp.csr_matrix(np.random.rand(100, 10))\n    km = KMeans(n_clusters=5, algorithm='elkan', random_state=42)\n    km.fit(X)\n    labels = km.predict(X)\n    assert np.unique(labels).shape[0] == 5\n    assert km.inertia_ > 0.0\n", "def test_minibatch_kmeans_partial_fit_sparse_data():\n    # Issue GH #14314\n    X = sp.csr_matrix([[-1], [1]], dtype=np.int)\n    km = MiniBatchKMeans(n_clusters=2)\n    km.partial_fit(X)\n    assert km.cluster_centers_.dtype.kind == \"f\"\n"], "sample_1137": ["def test_issue_14932_with_units():\n    assert (log(inch) - log(2*meter)).simplify() == log(inch/2) - log(2)\n    assert (log(inch) - log(foot/2)).simplify() == log(12) - log(2)\n    p = symbols('p', positive=True)\n    assert (log(inch) - log(p*meter)).simplify() == log(inch/p) - log(2)\n", "def test_convert_to_multiple_units():\n    expr = speed_of_light * joule\n    target_units = [kilometer, second, joule]\n    converted_expr = convert_to(expr, target_units)\n    expected_expr = 299792458 * kilometer / second * joule\n    assert converted_expr == expected_expr\n", "def test_issue_15000():\n    # Testing the conversion from meter to kilometer\n    assert convert_to(3*meter, kilometer) == 0.003*kilometer\n\n    # Testing the conversion from kilometer to meter\n    assert convert_to(2*kilometer, meter) == 2000*meter\n\n    # Testing the conversion with a more complex expression\n    assert convert_to(5*meter + 2*kilometer, meter) == 2005*meter\n", "def test_issue_15000():\n    # Test case for issue #15000\n    # The issue was that convert_to() was not handling the case where the target_units was a non-iterable quantity\n    # This test checks that the function correctly handles this case\n    speed = Quantity('speed')\n    speed.set_global_relative_scale_factor(1, meter/second)\n\n    assert convert_to(speed_of_light, speed) == speed_of_light / speed\n", "def test_issue_16000():\n    # Testing the conversion of a complex quantity with a dimension not defined in SI\n    non_si_unit = Quantity(\"non_si_unit\")\n    non_si_unit.set_global_relative_scale_factor(10, Dimension(\"non_si_dim\"))\n    si_unit = Quantity(\"si_unit\")\n    si_unit.set_global_relative_scale_factor(1, meter)\n\n    expr = non_si_unit * si_unit\n    conv = convert_to(expr, non_si_unit)\n    assert conv == 10 * si_unit\n"], "sample_285": ["def test_nonexistent_dirs(self):\n    self.assertEqual(check_finders(None), [\n        Warning(\n            \"The directory '/nonexistent/path' in the STATICFILES_DIRS setting \"\n            \"does not exist.\",\n            id='staticfiles.W004',\n        )\n    ])\n", "def test_dirs_contains_missing_path(self):\n    self.assertEqual(check_finders(None), [\n        Warning(\n            \"The directory '/missing/path/' in the STATICFILES_DIRS setting does not exist.\",\n            id='staticfiles.W004',\n        )\n    ])\n", "def test_dirs_does_not_exist(self):\n    static_dir = Path(TEST_ROOT) / 'nonexistent_dir'\n    with self.settings(STATICFILES_DIRS=[static_dir]):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                f\"The directory '{static_dir}' in the STATICFILES_DIRS setting \"\n                f\"does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n", "def test_dirs_not_exist(self):\n    nonexistent_dir = '/nonexistent/directory'\n    with self.settings(STATICFILES_DIRS=[nonexistent_dir]):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                f\"The directory '{nonexistent_dir}' in the STATICFILES_DIRS setting \"\n                f\"does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n", "def test_dirs_do_not_exist(self):\n    with self.settings(STATICFILES_DIRS=[('/nonexistent/path', '/another/nonexistent/path')]):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                \"The directory '/nonexistent/path' in the STATICFILES_DIRS setting does not exist.\",\n                id='staticfiles.W004',\n            ),\n            Warning(\n                \"The directory '/another/nonexistent/path' in the STATICFILES_DIRS setting does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n"], "sample_1150": ["def test_issue_17858_additional():\n    assert -1 in Range(-oo, oo)\n    assert -oo not in Range(oo, -oo, -1)\n    assert oo in Range(oo, -oo, -1)\n", "def test_Range_intersection():\n    assert Range(1, 10).intersect(Range(5, 15)) == Range(5, 10)\n    assert Range(1, 10).intersect(Range(15, 20)) == Range(0)\n    assert Range(1, 10, 2).intersect(Range(2, 10, 3)) == Range(2, 10, 6)\n    assert Range(1, oo).intersect(Range(-oo, 10)) == Range(1, 10)\n", "def test_range_issue_17465():\n    n = Symbol('n', integer=True)\n    assert Range(n + 1, 2*n + 2, 2).size == n + 1\n", "def test_issue_17858_additional():\n    assert 1 in Range(-oo, oo, 2)\n    assert 0 not in Range(oo, -oo, -1)\n    assert 2 in Range(oo, -oo, -2)\n", "def test_issue_18592():\n    from sympy import Lambda, ImageSet, S\n\n    # Test the intersection of an ImageSet with a set of numbers\n    assert ImageSet(Lambda(x, x**2), S.Naturals).intersect(S.Integers) == ImageSet(Lambda(x, x**2), S.Naturals)\n\n    # Test the intersection of two ImageSets\n    set1 = ImageSet(Lambda(x, x**2), S.Naturals)\n    set2 = ImageSet(Lambda(x, x), S.Naturals)\n    assert set1.intersect(set2) == {0, 1, 4, 9, 16, 25, ...}\n"], "sample_492": ["def test_serialize_complex(self):\n    complex_num = complex(1, 2)\n    self.assertSerializedEqual(complex_num)\n    self.assertSerializedResultEqual(complex_num, (\"complex((1+2j))\", set()))\n", "def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(3, 4),\n        (\"complex('(3+4j)')\", set()),\n    )\n", "def test_serialize_registered_serializer(self):\n    class CustomObject:\n            self.value = value\n\n    class CustomSerializer(BaseSerializer):\n            return \"CustomObject(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(CustomObject, CustomSerializer)\n    custom_obj = CustomObject(\"test\")\n    self.assertSerializedResultEqual(custom_obj, (\"CustomObject('test')\", set()))\n    MigrationWriter.unregister_serializer(CustomObject)\n    with self.assertRaises(ValueError):\n        MigrationWriter.serialize(custom_obj)\n", "def test_serialize_complex(self):\n    \"\"\"\n    Test serialization of complex numbers.\n    \"\"\"\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedEqual(complex(3, 4))\n    self.assertSerializedResultEqual(complex(1, 2), (\"complex(1+2j)\", set()))\n    self.assertSerializedResultEqual(complex(3, 4), (\"complex(3+4j)\", set()))\n", "def test_serialize_custom_fields(self):\n    class CustomField(models.Field):\n            self.arg1 = arg1\n            self.arg2 = arg2\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            kwargs.update({\n                'arg1': self.arg1,\n                'arg2': self.arg2,\n            })\n            return name, path, args, kwargs\n\n    field = CustomField(arg1='value1', arg2=42)\n    string, imports = MigrationWriter.serialize(field)\n    self.assertEqual(string, \"migrations.test_writer.CustomField(arg1='value1', arg2=42)\")\n    self.assertIn(\"import migrations.test_writer\", imports)\n"], "sample_940": ["def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @func.register(int)\n        pass\n\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(not_singledispatch_func) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @func.register(int)\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(_) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @func.register(int)\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.register) is False\n    assert inspect.is_singledispatch_function(func.dispatch) is False\n    assert inspect.is_singledispatch_function(_) is False\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig, globals(), locals())\n\n    assert evaluated_sig.parameters['x'].annotation == list\n    assert evaluated_sig.parameters['y'].annotation == types.FunctionType\n    assert evaluated_sig.return_annotation == dict\n", "def test_signature_evaluate():\n    from typing import List, Dict, Tuple, Union, Optional\n\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig, globals(), locals())\n    assert evaluated_sig.parameters['x'].annotation == List[int]\n    assert evaluated_sig.parameters['y'].annotation == Dict[str, Union[int, float]]\n    assert evaluated_sig.parameters['z'].annotation == Tuple[int, ...]\n    assert evaluated_sig.return_annotation == Optional[str]\n"], "sample_1176": ["def test_Number_as_coefficient():\n    a = Number(1)\n    b = Number(2)\n    c = a + b\n    assert c.as_coefficient(a) == (2, 0)\n    assert c.as_coefficient(b) == (1, 1)\n    assert c.as_coefficient(Number(3)) == (1, 2)\n", "def test_Integer_trunc():\n    a = Integer(4)\n    assert a.trunc() == a\n", "def test_Float_floor_ceiling():\n    a = Float(3.7)\n    b = Float(2.3)\n\n    assert a.floor() == 3.0\n    assert a.ceiling() == 4.0\n    assert b.floor() == 2.0\n    assert b.ceiling() == 3.0\n", "def test_issue_11894():\n    assert comp(Integer(12345678901234567890), Integer(12345678901234567890))\n    assert not comp(Integer(12345678901234567890), Integer(12345678901234567891))\n    assert comp(Integer(-12345678901234567890), Integer(-12345678901234567890))\n    assert not comp(Integer(-12345678901234567890), Integer(-12345678901234567889))\n", "def test_issue_12345():\n    # Test for issue #12345\n    # Adding a test to cover the case where the base is a negative integer\n    # and the exponent is a negative rational number\n    base = Integer(-2)\n    exp = Rational(-3, 4)\n    result = base**exp\n    assert isinstance(result, Pow)\n    assert result.base == base\n    assert result.exp == exp\n"], "sample_254": ["def test_inline_nonauto_editable_pk(self):\n    response = self.client.get(reverse('admin:admin_inlines_author_add'))\n    self.assertContains(\n        response,\n        '<input class=\"vIntegerField\" id=\"id_editablepkbookchild_set-0-manual_pk\" '\n        'name=\"editablepkbookchild_set-0-manual_pk\" type=\"number\">',\n        html=True\n    )\n    self.assertContains(\n        response,\n        '<input class=\"vIntegerField\" id=\"id_editablepkbookchild_set-2-manual_pk\" '\n        'name=\"editablepkbookchild_set-2-manual_pk\" type=\"number\">',\n        html=True\n    )\n", "def test_inline_formset_non_field_error(self):\n    self.admin_login(username='super', password='secret')\n    self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_titlecollection_add'))\n    self.selenium.find_element_by_link_text('Add another Title').click()\n    self.selenium.find_element_by_id('id_title_set-0-title1').send_keys('title1')\n    self.selenium.find_element_by_id('id_title_set-0-title2').send_keys('title2')\n    self.selenium.find_element_by_id('id_title_set-1-title1').send_keys('title3')\n    self.selenium.find_element_by_id('id_title_set-1-title2').send_keys('title3')\n    self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n    non_field_error = self.selenium.find_element_by_css_selector('.errorlist.nonfield')\n    self.assertEqual(non_field_error.text, 'The two titles must be the same')\n", "def test_inline_nonauto_noneditable_pk_change(self):\n    author = Author.objects.create(name=\"Test Author\")\n    book = NonAutoPKBook.objects.create(name=\"Test Book\", author=author)\n    response = self.client.get(reverse('admin:admin_inlines_author_change', args=(author.pk,)))\n    self.assertContains(response, '<input type=\"hidden\" name=\"nonautopkbook_set-0-rand_pk\" value=\"%s\"' % book.pk)\n", "    def test_immutable_content_type_with_change(self):\n        \"\"\"Regression for #9362\n        The problem depends only on InlineAdminForm and its \"original\"\n        argument, so we can safely set the other arguments to None/{}. We just\n        need to check that the content_type argument of Child isn't altered by\n        the internals of the inline form when there is a change.\"\"\"\n\n        sally = Teacher.objects.create(name='Sally')\n        john = Parent.objects.create(name='John')\n        joe = Child.objects.create(name='Joe', teacher=sally, parent=john)\n\n        iaf = InlineAdminForm(None, None, {'name': 'Joe Jr.'}, {}, joe)\n        parent_ct = ContentType.objects.get_for_model(Parent)\n        self.assertEqual(iaf.original.content_type, parent_ct)\n", "def test_inline_formset_error_messages(self):\n    self.admin_login(username='super', password='secret')\n    self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n    stacked_inline_error_selector = 'div#inner5stacked_set-group ul.errorlist'\n    tabular_inline_error_selector = 'div#inner5tabular_set-group ul.errorlist'\n\n    # No error messages are shown initially\n    self.assertEqual(len(self.selenium.find_elements_by_css_selector(stacked_inline_error_selector)), 0)\n    self.assertEqual(len(self.selenium.find_elements_by_css_selector(tabular_inline_error_selector)), 0)\n\n    # Inlines with errors, error messages are displayed\n    test_fields = ['#id_inner5stacked_set-0-dummy', '#id_inner5tabular_set-0-dummy']\n    show_links = self.selenium.find_elements_by_link_text('SHOW')\n    for show_index, field_name in enumerate(test_fields):\n        show_links[show_index].click()\n        self.wait_until_visible(field_name)\n        self.selenium.find_element_by_id(field_name[1:]).send_keys(1)\n    self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n    self.assertEqual(len(self.selenium.find_elements_by_css_selector(stacked_inline_error_selector)), 1)\n    self.assertEqual(len(self.selenium.find_elements_by_css_selector(tabular_inline_error_selector)), 1)\n\n    # Error messages are removed after the error is corrected\n    self.selenium.find_element_by_id('id_inner5stacked_set-0-dummy').clear()\n    self.selenium.find_element_by_id('id_inner5stacked_set-0"], "sample_665": ["def test_collect_handles_raising_on_dunder_all(testdir):\n    \"\"\"Handle dunder all classes that might raise on\n    ``hasattr`` (#4266).\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        class ImproperlyConfigured(Exception):\n            pass\n\n        class RaisesOnHasAttr(object):\n                raise ImproperlyConfigured\n\n            __all__ = property(raises)\n\n        raises = RaisesOnHasAttr()\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n    assert result.ret == 0\n", "def test_collect_sub_with_symlinks_init(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\").write(\"def test_init(): pass\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Create a broken symlink to a test in __init__.py.\n    sub.join(\"test_broken_init.py\").mksymlinkto(\"test_init.py\")\n\n    # Symlink that gets collected.\n    sub.join(\"test_symlink_file.py\").mksymlinkto(\"test_file.py\")\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub/__init__.py::test_init PASSED*\",\n            \"sub/test_file.py::test_file PASSED*\",\n            \"sub/test_broken_init.py::test_init PASSED*\",\n            \"sub/test_symlink_file.py::test_file PASSED*\",\n            \"*4 passed in*\",\n        ]\n    )\n", "def test_collect_sub_with_symlinks_out_of_tree(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n    sub.join(\"test_symlink.py\").mksymlinkto(\"test_file.py\")\n\n    out_of_tree = testdir.tmpdir.join(\"out_of_tree\").ensure(dir=True)\n    symlink_to_sub = out_of_tree.join(\"symlink_to_sub\")\n    symlink_to_sub.mksymlinkto(sub)\n\n    sub.chdir()\n    result = testdir.runpytest(\"-vs\", \"--rootdir=%s\" % sub, symlink_to_sub)\n    result.stdout.fnmatch_lines(\n        [\n            \"test_file.py::test_file PASSED\",\n            \"test_symlink.py::test_file PASSED\",\n            \"*2 passed in*\",\n        ]\n    )\n    assert result.ret == 0\n", "def test_collect_with_syntax_error(testdir):\n    \"\"\"Test that a syntax error in a test file is reported correctly (#4520).\"\"\"\n    testdir.makepyfile(syntax_error=\"invalid syntax\")\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.INTERRUPTED\n    result.stdout.fnmatch_lines(\n        [\"*ERROR collecting syntax_error.py*\", \"*SyntaxError: invalid syntax*\"]\n    )\n", "def test_parametrized_scope(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture(scope=\"module\")\n            pass\n        @pytest.fixture(scope=\"class\")\n            pass\n        @pytest.mark.parametrize(\"arg\", [1, 2])\n        class TestClass:\n                pass\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"--setup-show\")\n    result.stdout.fnmatch_lines([\"*SETUP    MODULE module_fixture*\"])\n    result.stdout.fnmatch_lines([\"*SETUP    CLASS class_fixture*\"])\n"], "sample_57": ["    def test_password_help_text_translation(self):\n        user = User.objects.get(username='testclient')\n        form = UserChangeForm(instance=user)\n        with translation.override('fr'):\n            html = form.as_p()\n            self.assertIn('Votre mot de passe ne peut pas \u0102\u0218tre vu.', html)\n", "    def test_unicode_username(self):\n        user = User.objects.get(username='testclient')\n        new_username = '\u00e9\u00e7\u00e0\u00e8\u00ea\u00f4'\n        data = {\n            'username': new_username,\n            'password': user.password,\n        }\n        form = UserChangeForm(data, instance=user)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(form.cleaned_data['username'], new_username)\n        self.assertEqual(User.objects.get(username=new_username), user)\n", "def test_inactive_user_with_custom_login_allowed_policy(self):\n    # The user is inactive, but our custom form policy allows them to log in.\n    data = {\n        'username': 'inactive',\n        'password': 'password',\n    }\n\n    class CustomAuthenticationForm(AuthenticationForm):\n            pass\n\n    form = CustomAuthenticationForm(None, data)\n    self.assertTrue(form.is_valid())\n", "def test_integer_username_change(self):\n    user = IntegerUsernameUser.objects.create_user(username=123, password='oldpassword')\n    data = {\n        'username': 456,\n        'password': 'oldpassword',\n        'new_password1': 'newpassword',\n        'new_password2': 'newpassword',\n    }\n    form = PasswordChangeForm(user, data)\n    self.assertTrue(form.is_valid())\n    user = form.save()\n    self.assertEqual(user.username, 456)\n    self.assertTrue(user.check_password('newpassword'))\n", "    def test_custom_password_label(self):\n        class CustomUserChangeForm(UserChangeForm):\n            password = ReadOnlyPasswordHashField(\n                label=_(\"Secure password\"),\n                help_text=_(\"This is a custom help text for the password field.\"),\n            )\n\n        user = User.objects.get(username='testclient')\n        form = CustomUserChangeForm(instance=user)\n        self.assertIn(\"Secure password\", form.as_table())\n        self.assertIn(\"This is a custom help text for the password field.\", form.as_table())\n"], "sample_569": ["def test_lmplot_legend_out(self):\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend_out=True)\n    assert g._legend_out is True\n", "def test_regplot_line_kws_color(self):\n\n    f, ax = plt.subplots()\n    color = 'g'\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, line_kws={'color': color})\n    npt.assert_array_equal(ax.lines[0].get_color(), color)\n", "def test_regplot_line_kws(self):\n\n    f, ax = plt.subplots()\n    color = np.array([[0.3, 0.8, 0.5, 0.5]])\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df,\n                    line_kws={'color': color})\n    assert ax.lines[0].get_color() == (0.3, 0.8, 0.5, 0.5)\n\n    f, ax = plt.subplots()\n    color = 'r'\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df,\n                    line_kws={'color': color})\n    assert ax.lines[0].get_color() == 'r'\n", "def test_regplot_color(self):\n    color = (0.2, 0.5, 0.8)\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n    scatter = ax.collections[0]\n    npt.assert_array_equal(scatter.get_facecolors()[0, :3], color)\n    line = ax.lines[0]\n    npt.assert_array_equal(line.get_color(), color)\n", "def test_regplot_partial(self):\n\n    x = self.rs.randn(100)\n    y = x + self.rs.randn(100)\n    z = x + self.rs.randn(100)\n\n    f, ax = plt.subplots()\n    ax = lm.regplot(x=y, y=z, y_partial=x, data=None)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 1\n\n    x_resid = ax.collections[0].get_offsets().T[0]\n    y_resid = ax.collections[0].get_offsets().T[1]\n\n    _, r_orig = np.corrcoef(y, z)[0]\n    _, r_semipartial = np.corrcoef(x_resid, y_resid)[0]\n\n    assert r_semipartial < r_orig\n"], "sample_482": ["def test_empty(self):\n    output = self.engine.render_to_string(\"escapeseq_empty\", {\"a\": []})\n    self.assertEqual(output, \"\")\n", "def test_mark_safe(self):\n    output = self.engine.render_to_string(\"escapeseq_mark_safe\", {\"a\": [\"x&y\", \"<p>\"]})\n    self.assertEqual(output, \"x&amp;y, &lt;p&gt;\")\n", "def test_mark_safe(self):\n    output = self.engine.render_to_string(\"escapeseq_mark_safe\", {\"a\": [\"x&y\", \"<p>\"]})\n    self.assertEqual(output, \"x&y, <p>\")\n", "def test_safe_marked(self):\n    output = self.engine.render_to_string(\n        \"escapeseq_safe_marked\",\n        {\"a\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n    )\n    self.assertEqual(output, \"x&y, <p>\")\n", "def test_empty(self):\n    output = self.engine.render_to_string(\n        \"escapeseq_empty\",\n        {\"a\": [], \"b\": []},\n    )\n    self.assertEqual(output, \"-- \")\n"], "sample_852": ["def test_make_moons_noise():\n    X, y = make_moons(3, shuffle=False, noise=0.1)\n    X_no_noise, y_no_noise = make_moons(3, shuffle=False, noise=0.0)\n    assert np.std(X - X_no_noise) > 0.0, \"Noise not added when noise parameter is set\"\n", "def test_make_classification_shuffle():\n    X1, y1 = make_classification(n_samples=100, n_features=20, n_informative=5, n_redundant=1, n_repeated=1, n_classes=3, n_clusters_per_class=1, hypercube=False, shift=None, scale=None, weights=None, shuffle=False, random_state=0)\n    X2, y2 = make_classification(n_samples=100, n_features=20, n_informative=5, n_redundant=1, n_repeated=1, n_classes=3, n_clusters_per_class=1, hypercube=False, shift=None, scale=None, weights=None, shuffle=True, random_state=0)\n    assert not np.array_equal(X1, X2), \"Data should be shuffled when shuffle=True\"\n    assert not np.array_equal(y1, y2), \"Data should be shuffled when shuffle=True\"\n", "def test_make_moons_shuffle():\n    X, y = make_moons(3, shuffle=True, random_state=0)\n    X_no_shuffle, y_no_shuffle = make_moons(3, shuffle=False, random_state=0)\n\n    # Test if shuffle works\n    assert not np.array_equal(X, X_no_shuffle)\n    assert not np.array_equal(y, y_no_shuffle)\n", "def test_make_classification_shuffle():\n    \"\"\"Test the shuffle parameter in make_classification\"\"\"\n    X1, y1 = make_classification(shuffle=True, random_state=0)\n    X2, y2 = make_classification(shuffle=False, random_state=0)\n\n    assert not np.array_equal(X1, X2), \"Data should be shuffled\"\n    assert not np.array_equal(y1, y2), \"Labels should be shuffled\"\n\n    X3, y3 = make_classification(shuffle=False, random_state=0)\n    assert np.array_equal(X2, X3), \"Results should be deterministic\"\n    assert np.array_equal(y2, y3), \"Results should be deterministic\"\n", "def test_make_moons_noise():\n    X, y = make_moons(3, shuffle=False, noise=0.1)\n    for x, label in zip(X, y):\n        center = [0.0, 0.0] if label == 0 else [1.0, 0.5]\n        dist_sqr = ((x - center) ** 2).sum()\n        assert dist_sqr >= 0.9 and dist_sqr <= 1.1, \"Point is too far from expected unit circle\"\n"], "sample_436": ["def test_help_custom_command(self):\n    \"help --commands shows the list of all available custom commands\"\n    args = [\"help\", \"--commands\", \"--settings=test_project.settings\"]\n    out, err = self.run_manage(args)\n    self.assertNotInOutput(out, \"usage:\")\n    self.assertNotInOutput(out, \"Options:\")\n    self.assertNotInOutput(out, \"[django]\")\n    self.assertOutput(out, \"custom_command\")\n    self.assertNotInOutput(out, \"\\n\\n\")\n", "def test_runserver_port_validation(self):\n    with self.assertRaises(CommandError):\n        call_command(\"runserver\", addrport=\"80000\")\n\n    with self.assertRaises(CommandError):\n        call_command(\"runserver\", addrport=\"80:80000\")\n\n    with self.assertRaises(CommandError):\n        call_command(\"runserver\", addrport=\"0.0.0.0:80000\")\n", "def test_manage_py_help_output(self):\n    \"\"\"\n    Test help output when a custom manage.py file is used.\n    \"\"\"\n    out, err = self.run_manage([\"help\"], manage_py=\"custom_manage.py\")\n    self.assertOutput(out, \"Custom help text\")\n    self.assertNoOutput(err)\n", "def test_invalid_project_name_type(self):\n    \"\"\"\n    Passing a non-string argument to the startproject command results in an error message.\n    \"\"\"\n    args = [\"startproject\", 123]\n    out, err = self.run_django_admin(args)\n    self.assertNoOutput(out)\n    self.assertOutput(err, \"usage:\")\n    self.assertOutput(err, \"Error: Project name must be a string.\")\n", "    def test_help_output_no_custom_user_model(self):\n        out, err = self.run_django_admin([\"help\", \"createsuperuser\"])\n        self.assertNoOutput(err)\n        self.assertOutput(\n            out,\n            \"Type:    Management Command\\n\\n\"\n            \"Creates a superuser.\\n\\n\"\n            \"Options:\\n\"\n            \"--database DATABASE\\n\"\n            \"            Specifies the database to use. Defaults to the \"\n            \"\\\"default\\\" database.\\n\"\n            \"--email EMAIL\\n\"\n            \"            The email address for the superuser.\\n\"\n            \"--noinput\\n\"\n            \"            Tells Django to NOT prompt the user for input of any kind.\",\n        )\n"], "sample_15": ["def test_radian_array(self, function):\n    q1 = function(np.array([180.0, 90.0]) * u.degree, 0.0 * u.arcmin, 0.0 * u.arcsec)\n    assert_allclose(q1.value, np.array([np.pi, np.pi / 2]))\n    assert q1.unit == u.radian\n\n    q2 = function(0.0 * u.degree, np.array([30.0, 45.0]) * u.arcmin, 0.0 * u.arcsec)\n    assert_allclose(q2.value, (np.array([30.0, 45.0]) * u.arcmin).to(u.radian).value)\n    assert q2.unit == u.radian\n\n    q3 = function(0.0 * u.degree, 0.0 * u.arcmin, np.array([30.0, 60.0]) * u.arcsec)\n    assert_allclose(q3.value, (np.array([30.0, 60.0]) * u.arcsec).to(u.radian).value)\n    assert q3.unit == u.radian\n", "def test_erf_inplace(self, function):\n    q = np.arange(-1.0, 10.0) * u.m\n    check = q.value.copy()\n    function(q, out=q)\n    function(check, out=check)\n    assert np.all(q.value == check)\n    assert q.unit is u.m\n", "def test_jv_invalid_units(self, function):\n    with pytest.raises(TypeError, match=\"Cannot convert input to dimensionless\"):\n        function(2.0 * u.m, 3.0 * u.s)\n", "def test_radian_inplace(self, function):\n    q = 30.0 * u.degree\n    check = q.value.copy()\n    function(q, 0.0 * u.arcmin, 0.0 * u.arcsec, out=q)\n    function(check, 0.0, 0.0, out=check)\n    assert_allclose(q.value, check)\n    assert q.unit == u.radian\n", "def test_inplace_ufunc(self, function):\n    q = np.arange(10.0) * u.dimensionless_unscaled\n    check = q.value.copy()\n    function(q, out=q)\n    function(check, out=check)\n    assert np.all(q.value == check)\n    assert q.unit is u.dimensionless_unscaled\n"], "sample_534": ["def test_contour_label_inline():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    labels = cs.clabel(inline=True)\n    assert len(labels) > 0\n", "def test_linestyles_cycle():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    # Cycle through linestyles\n    fig1, ax1 = plt.subplots()\n    CS1 = ax1.contour(X, Y, Z, 6, colors='k',\n                      linestyles=['solid', 'dashed', 'dashdot', 'dotted'])\n    ax1.clabel(CS1, fontsize=9, inline=True)\n    ax1.set_title('Single color - cycling linestyles')\n    assert CS1.linestyles == ['solid', 'dashed', 'dashdot', 'dotted']\n", "def test_contour_with_origin_and_extent():\n    x = np.linspace(-1, 1, 10)\n    y = np.linspace(-1, 1, 10)\n    x, y = np.meshgrid(x, y)\n    z = np.sin(np.pi * x) * np.cos(np.pi * y)\n\n    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n\n    origins = [None, 'lower', 'upper', 'image']\n    extents = [None, [-2, 2, -2, 2], [0, 1, 0, 1]]\n\n    for ax, origin in zip(axs.flat, origins):\n        ax.contourf(x, y, z, origin=origin, extent=extents[0])\n        ax.set_title(f'Origin: {origin}, Extent: {extents[0]}')\n\n    for ax, extent in zip(axs.flat, extents):\n        ax.contourf(x, y, z, origin=origins[0], extent=extent)\n        ax.set_title(f'Origin: {origins[0]}, Extent: {extent}')\n", "def test_contour_color_norm():\n    # Test that colors and norm are set correctly for contour and contourf\n    x = np.arange(1, 10)\n    y = x.reshape(-1, 1)\n    z = x * y\n\n    # Test contourf\n    cs_filled = plt.contourf(x, y, z, norm=LogNorm(vmin=z.min(), vmax=z.max()))\n    assert cs_filled.norm is not None\n    assert isinstance(cs_filled.norm, LogNorm)\n\n    # Test contour\n    cs = plt.contour(x, y, z, norm=LogNorm(vmin=z.min(), vmax=z.max()))\n    assert cs.norm is not None\n    assert isinstance(cs.norm, LogNorm)\n", "def test_contour_antialiased():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2)\n    ax1.contour(x, y, z, antialiased=True)\n    ax1.set_title('Antialiased')\n    ax2.contour(x, y, z, antialiased=False)\n    ax2.set_title('Not antialiased')\n"], "sample_271": ["    def test_tick_triggers_for_file_change(self, mock_sleep, mock_notify_file_changed):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            ticker = self.reloader.tick()\n            next(ticker)\n            self.increment_mtime(self.existing_file)\n            next(ticker)\n            self.assertEqual(mock_notify_file_changed.call_count, 1)\n", "    def test_notify_file_changed(self):\n        reloader = autoreload.BaseReloader()\n        path = Path('test.py')\n        with mock.patch.object(autoreload.file_changed, 'send') as mock_send:\n            mock_send.return_value = [(None, True)]\n            with mock.patch('django.utils.autoreload.trigger_reload') as mock_trigger_reload:\n                reloader.notify_file_changed(path)\n                self.assertEqual(mock_send.call_count, 1)\n                self.assertEqual(mock_trigger_reload.call_count, 0)\n\n        mock_send.return_value = [(None, False)]\n        with mock.patch('django.utils.autoreload.trigger_reload') as mock_trigger_reload:\n            reloader.notify_file_changed(path)\n            self.assertEqual(mock_send.call_count, 1)\n            self.assertEqual(mock_trigger_reload.call_count, 1)\n", "def test_watch_dir_multiple_times(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    self.reloader.watch_dir(self.tempdir, '*.py')  # Watch the same directory and glob multiple times\n    with self.tick_twice():\n        self.increment_mtime(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 1)  # Only one notification should be sent\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "def test_tick_triggers_on_file_add(self, mock_notify_file_changed):\n    new_file = self.tempdir / 'new_file.py'\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        self.ensure_file(new_file)\n        next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n        self.assertIn(new_file, mock_notify_file_changed.call_args[0])\n", "def test_nested_directory_glob_recursive(self, mocked_modules, notify_mock):\n    inner_py_file = self.ensure_file(self.tempdir / 'dir' / 'subdir' / 'file.py')\n    self.reloader.watch_dir(self.tempdir, '**/*.py')\n    self.reloader.watch_dir(self.tempdir / 'dir', '**/*.py')\n    with self.tick_twice():\n        self.increment_mtime(inner_py_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [inner_py_file])\n"], "sample_427": ["def test_custom_template_name(self):\n    class CustomFormSet(BaseFormSet):\n        template_name = \"custom/template.html\"\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomFormSet)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"custom/template.html\")\n", "def test_can_delete_formset_forms(self):\n    ChoiceFormFormset = formset_factory(form=Choice, can_delete=True)\n    formset = ChoiceFormFormset(initial=[{\"choice\": \"Zero\", \"votes\": \"1\"}])\n    self.assertEqual(len(formset), 1)\n    self.assertIn(\"DELETE\", formset.forms[0].fields)\n", "def test_formset_prefix_override(self):\n    \"\"\"\n    A custom prefix can be specified when creating a formset, and it will be used\n    for all forms in the formset.\n    \"\"\"\n    custom_prefix = \"custom_prefix\"\n    formset = self.make_choiceformset(prefix=custom_prefix)\n    self.assertHTMLEqual(\n        str(formset),\n        f'<input type=\"hidden\" name=\"{custom_prefix}-TOTAL_FORMS\" value=\"1\">'\n        f'<input type=\"hidden\" name=\"{custom_prefix}-INITIAL_FORMS\" value=\"0\">'\n        f'<input type=\"hidden\" name=\"{custom_prefix}-MIN_NUM_FORMS\" value=\"0\">'\n        f'<input type=\"hidden\" name=\"{custom_prefix}-MAX_NUM_FORMS\" value=\"1000\">'\n        f'<div>Choice:<input type=\"text\" name=\"{custom_prefix}-0-choice\"></div>'\n        f'<div>Votes:<input type=\"number\" name=\"{custom_prefix}-0-votes\"></div>',\n    )\n", "def test_formset_initial_data_with_empty_form(self):\n    \"\"\"\n    A FormSet can be prefilled with existing data by providing a list of\n    dicts to the `initial` argument. If there is room for an extra blank form,\n    it should be rendered as an empty form.\n    \"\"\"\n    formset = self.make_choiceformset(\n        initial=[{\"choice\": \"Calexico\", \"votes\": 100}]\n    )\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-0-votes\" value=\"100\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-1-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-1-votes\"></li>',\n    )\n    # Retrieving an empty form works. Tt shows up in the form list.\n    self.assertTrue(formset.empty_form.empty_permitted)\n    self.assertHTMLEqual(\n        formset.empty_form.as_ul(),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"choices-__prefix__-choice\"></li>", "    def test_file_field(self):\n        \"\"\"\n        Formsets with FileField works.\n        \"\"\"\n\n        class FileForm(Form):\n            file = FileField()\n\n        FileFormSet = formset_factory(FileForm)\n        data = {\n            \"form-TOTAL_FORMS\": \"1\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-0-file\": SimpleUploadedFile(\"test.txt\", b\"file_content\"),\n        }\n        files = {\n            \"form-0-file\": SimpleUploadedFile(\"test.txt\", b\"file_content\"),\n        }\n        formset = FileFormSet(data, files, prefix=\"form\")\n        self.assertTrue(formset.is_valid())\n        self.assertTrue(formset.is_multipart())\n"], "sample_672": ["def test_broken_format():\n    \"\"\"saferepr() can create proper representations of objects with\n    broken __format__ (#issue_number)\n    \"\"\"\n\n    class SomeObject:\n            raise RuntimeError\n\n    assert saferepr(SomeObject())\n", "def test_broken_repr_instance():\n    \"\"\"saferepr() can handle broken __repr__ of custom objects\"\"\"\n    class BrokenRepr:\n            raise RuntimeError(\"Broken __repr__\")\n\n    obj = BrokenRepr()\n    assert \"RuntimeError\" in saferepr(obj)\n", "def test_broken_repr_subclass():\n    \"\"\"saferepr() can create proper representations of classes with\n    broken __repr__ from a subclass (#some_issue_number)\n    \"\"\"\n\n    class Parent:\n            return \"Parent object\"\n\n    class Child(Parent):\n            raise RuntimeError(\"Broken repr\")\n\n    assert \"RuntimeError\" in saferepr(Child())\n", "def test_broken_repr_str():\n    \"\"\"saferepr() can create proper representations of classes with\n    broken __repr__ and __str__ (#12345)\n    \"\"\"\n\n    class BrokenReprStr:\n            raise RuntimeError\n\n            raise RuntimeError\n\n    assert \"RuntimeError\" in saferepr(BrokenReprStr())\n", "def test_broken_repr_on_instance():\n    \"\"\"saferepr() can create proper representations of instances with\n    broken __repr__ methods (#XYZ)\n    \"\"\"\n\n    class BrokenReprClass:\n            raise RuntimeError\n\n    assert \"RuntimeError\" in saferepr(BrokenReprClass())\n"], "sample_1066": ["def test_mathml_matrix_determinant():\n    from sympy.matrices import MatrixSymbol, Determinant\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert mathml(Determinant(X), printer='presentation') == \\\n        '<mfenced close=\"|\" open=\"|\"><mi>X</mi></mfenced>'\n    assert mathml(Determinant(X + Y), printer='presentation') == \\\n        '<mfenced close=\"|\" open=\"|\"><mrow><mi>X</mi><mo>+</mo><mi>Y</mi></mrow></mfenced>'\n    assert mathml(Determinant(X*Y), printer='presentation') == \\\n        '<mfenced close=\"|\" open=\"|\"><mrow><mi>X</mi><mo>&InvisibleTimes;</mo><mi>Y</mi></mrow></mfenced>'\n    assert mathml(Determinant(X**2), printer='presentation') == \\\n        '<mfenced close=\"|\" open=\"|\"><msup><mi>X</mi><mn>2</mn></msup></mfenced>'\n", "def test_print_kronecker_delta():\n    assert mathml(KroneckerDelta(x, y), printer='presentation') == \\\n        '<mrow><mo>&#x3B4;</mo><mfenced close=\"\" open=\"\"><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mathml(KroneckerDelta(x, 0), printer='presentation') == \\\n        '<mrow><mo>&#x3B4;</mo><mfenced close=\"\" open=\"\"><mi>x</mi><mn>0</mn></mfenced></mrow>'\n    assert mathml(KroneckerDelta(0, y), printer='presentation') == \\\n        '<mrow><mo>&#x3B4;</mo><mfenced close=\"\" open=\"\"><mn>0</mn><mi>y</mi></mfenced></mrow>'\n    assert mathml(KroneckerDelta(0, 0), printer='presentation') == \\\n        '<mrow><mo>&#x3B4;</mo><mfenced close=\"\" open=\"\"><mn>0</mn><mn>0</mn></mfenced></mrow>'\n", "def test_mathml_transpose_inverse():\n    from sympy.matrices import MatrixSymbol, Transpose, Inverse\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert mathml(Transpose(Inverse(X)), printer='presentation') == \\\n        '<msup><mfenced><msup><mi>X</mi><mn>-1</mn></msup></mfenced><mo>T</mo></msup>'\n    assert mathml(Inverse(Transpose(X)), printer='presentation') == \\\n        '<msup><mfenced><msup><mi>X</mi><mo>T</mo></msup></mfenced><mn>-1</mn></msup>'\n    assert mathml(Transpose(Inverse(X + Y)), printer='presentation') == \\\n        '<msup><mfenced><msup><mrow><mi>X</mi><mo>+</mo><mi>Y</mi></mrow><mn>-1</mn></msup></mfenced><mo>T</mo></msup>'\n", "def test_print_hermite_simplification():\n    assert mathml(hermite(n, x, simplify=True), printer='presentation') == \\\n        '<msub><mo>H</mo><mi>n</mi></msub><mfenced><mi>x</mi></mfenced>'\n    assert mathml(hermite(n, x, simplify=False), printer='presentation') == \\\n        '<mrow><msub><mo>H</mo><mi>n</mi></msub><mfenced><mi>x</mi></mfenced></mrow>'\n", "def test_print_mixed_symbols():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n    expr = A * x\n    assert mathml(expr, printer='presentation') == \\\n        '<mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>x</mi></mrow>'\n"], "sample_1042": ["def test_Indexed_assumptions():\n    i = Symbol('i', integer=True)\n    a = Symbol('a')\n    A = IndexedBase(a, real=True)\n    for c in (A, A[i]):\n        assert c.is_real\n        assert c.is_complex\n        assert not c.is_imaginary\n        assert not c.is_nonnegative\n        assert not c.is_nonzero\n        assert c.is_commutative\n        assert log(exp(c)) == c\n\n    assert A != IndexedBase(a)\n    assert A == IndexedBase(a, real=True)\n    assert A[i] != Indexed(a, i)\n", "def test_IndexedBase_offset():\n    i, j, m, n = symbols('i j m n', integer=True)\n    a = IndexedBase('a', offset=m)\n    assert a.offset == m\n    assert Indexed(a, i, j).offset == m\n\n    b = IndexedBase('b', offset=n)\n    assert b.offset == n\n    assert Indexed(b, i, j).offset == n\n\n    assert a != b\n    assert a[i, j].subs(m, n) == b[i, j]\n", "def test_Indexed_assumptions():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A', real=True)\n    assert A[i, j].is_real\n    assert A[i, j].is_complex\n    assert not A[i, j].is_imaginary\n    assert log(exp(A[i, j])) == A[i, j]\n\n    B = IndexedBase('B', integer=True)\n    assert B[i, j].is_integer\n    assert not B[i, j].is_real\n    assert not B[i, j].is_complex\n    assert not B[i, j].is_imaginary\n\n    C = IndexedBase('C', imaginary=True)\n    assert not C[i, j].is_integer\n    assert not C[i, j].is_real\n    assert C[i, j].is_complex\n    assert C[i, j].is_imaginary\n", "def test_Indexed_getitem():\n    A = IndexedBase(\"A\")\n    i, j = symbols(\"i j\", integer=True)\n    assert A[i, j][i] == A[i, j][0]\n    assert A[i, j][j] == A[i, j][1]\n    assert A[i, j][i, j] == A[i, j]\n    assert A[i, j][j, i] == Indexed(A, j, i)\n", "def test_Indexed_shape_strides():\n    i, j = symbols('i j', integer=True)\n    o, p = symbols('o p', integer=True)\n    n, m = symbols('n m', integer=True)\n    a = IndexedBase('a', shape=(o, p), strides=(2, 3))\n    assert a.shape == Tuple(o, p)\n    assert a.strides == Tuple(2, 3)\n    assert Indexed(a, Idx(i, m), Idx(j, n)).ranges == [Tuple(0, m - 1), Tuple(0, n - 1)]\n    assert Indexed(a, Idx(i, m), Idx(j, n)).shape == Tuple(o, p)\n"], "sample_1073": ["def test_sqrtdenest_rational_combination():\n    z = sqrt(1+sqrt(3)) + sqrt(3+3*sqrt(3)) - sqrt(10+6*sqrt(3))\n    assert sqrtdenest(z) == 0\n", "def test_sqrtdenest_ratcomb():\n    z = sqrt(1+sqrt(3)) + sqrt(3+3*sqrt(3)) - sqrt(10+6*sqrt(3))\n    assert sqrtdenest(z) == 0\n    z = sqrt(2+sqrt(3)) + sqrt(3+sqrt(6)) - sqrt(7+4*sqrt(3))\n    assert sqrtdenest(z) == sqrt(3)\n", "def test_issue_14133():\n    z = sqrt(4*r6 + 12*r2 + 40)\n    assert sqrtdenest(z) == 2*r2 + 2*r6 + 5\n", "def test_sqrt_ratcomb():\n    z = sqrt(1+sqrt(3)) + sqrt(3+3*sqrt(3)) - sqrt(10+6*sqrt(3))\n    assert sqrtdenest(z) == 0\n", "def test_sqrtdenest_issue_12420():\n    I = S.ImaginaryUnit\n    e = sqrt(2 + I)\n    assert sqrtdenest(e) == e\n"], "sample_1027": ["def test_Poly_integrate_symbol():\n    z = Symbol('z')\n    assert Poly(x**2 + x*y, x, y).integrate(z) == Poly(x**2 + x*y, x, y, z)\n", "def test_Poly_equals():\n    assert Poly(x**2 - 2) == Poly(x**2 - 2)\n    assert Poly(x**2 - 2) != Poly(x**2 - 3)\n    assert Poly(x**2 - 2, x) == Poly(x**2 - 2, x)\n    assert Poly(x**2 - 2, x) != Poly(x**2 - 3, x)\n    assert Poly(x**2 - 2, x) != Poly(x**2 - 2, y)\n", "def test_poly_with_floats():\n    # Test poly function with float coefficients\n    assert poly(1.5*x + 2.5) == Poly(1.5*x + 2.5, x)\n    assert poly(1.5*x + 2.5, x) == Poly(1.5*x + 2.5, x)\n    assert poly(1.5*(x + 2.0)) == Poly(1.5*x + 3.0, x)\n", "def test_poly_subs():\n    f = Poly(x**3 + 2*x**2 + 3*x + 4, x)\n    assert f.subs(x, y) == Poly(y**3 + 2*y**2 + 3*y + 4, y)\n    assert f.subs({x: y}) == Poly(y**3 + 2*y**2 + 3*y + 4, y)\n    assert f.subs([(x, y)]) == Poly(y**3 + 2*y**2 + 3*y + 4, y)\n    assert f.subs(x, 2) == 14\n    assert f.subs({x: 2}) == 14\n    assert f.subs([(x, 2)]) == 14\n    assert f.subs({x: y, y: 2}) == 14\n", "def test_Poly_domain():\n    assert Poly(x + 1, domain='ZZ[x]').domain.is_ZZ\n    assert Poly(x + 1, domain=ZZ[x]).domain.is_ZZ\n    assert Poly(x + 1, domain='QQ[x]').domain.is_QQ\n    assert Poly(x + 1, domain=QQ[x]).domain.is_QQ\n    assert Poly(x + 1, domain='RR[x]').domain.is_RR\n    assert Poly(x + 1, domain=RR[x]).domain.is_RR\n    assert Poly(x + 1, domain='CC[x]').domain.is_CC\n    assert Poly(x + 1, domain=CC[x]).domain.is_CC\n    assert Poly(x + 1, domain='ZZ(x)').domain.is_Field\n    assert Poly(x + 1, domain=ZZ.old_poly_ring()).domain.is_ZZ\n    assert Poly(x + 1, domain=ZZ[y]).domain.is_ZZ\n    assert Poly(x + y, domain=ZZ[x][y]).domain.is_ZZ\n"], "sample_394": ["def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username=\"staff\",\n        password=\"secret\",\n        email=\"staff@example.com\",\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse(\"admin10:admin_views_article_changelist\")\n    response = self.client.get(known_url[:-1], SCRIPT_NAME=\"/prefix/\")\n    self.assertRedirects(\n        response,\n        \"/prefix\" + known_url,\n        status_code=301,\n        fetch_redirect_response=False,\n    )\n", "def test_missing_slash_append_slash_true_unknown_url_with_query_params_without_final_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username=\"staff\",\n        password=\"secret\",\n        email=\"staff@example.com\",\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    unknown_url = \"/test_admin/admin10/unknown/?param=value\"\n    response = self.client.get(unknown_url[:-1])\n    self.assertEqual(response.status_code, 404)\n", "def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n    user = User.objects.create_user(\n        username=\"user\",\n        password=\"secret\",\n        email=\"user@example.com\",\n        is_staff=False,\n    )\n    self.client.force_login(user)\n    known_url = reverse(\"admin10:admin_views_article_changelist\")\n    response = self.client.get(known_url[:-1])\n    self.assertRedirects(\n        response,\n        \"/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article\",\n    )\n", "def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n    user = User.objects.create_user(\n        username=\"user\",\n        password=\"secret\",\n        email=\"user@example.com\",\n        is_staff=False,\n    )\n    self.client.force_login(user)\n    known_url = reverse(\"admin10:admin_views_article_changelist\")\n    response = self.client.get(known_url[:-1])\n    self.assertRedirects(\n        response,\n        \"/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article/\",\n    )\n", "    def test_single_model_append_slash(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin11:admin_views_actor_changelist\")\n        response = self.client.get(known_url)\n        self.assertRedirects(\n            response, known_url + \"/\", status_code=301, fetch_redirect_response=False\n        )\n"], "sample_84": ["    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n", "    def test_no_slashes(self):\n        self.assertEqual(escape_leading_slashes('example.com'), 'example.com')\n", "    def test_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n        self.assertEqual(escape_leading_slashes('/path'), '/path')\n", "    def test_double_slash_urls(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('///example.com'), '/%2F/%2Fexample.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n"], "sample_192": ["def test_extra_forms_invalid(self):\n    data = {\n        'choices-TOTAL_FORMS': '3',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': '',  # <-- this choice is missing but required\n        'choices-1-votes': '1',\n        'choices-2-choice': 'Two',\n        'choices-2-votes': '2',\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=3)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual([{}, {'choice': ['This field is required.']}, {}], formset.errors)\n", "def test_formset_validation_with_extra_initial_forms(self):\n    \"\"\"\n    A formset is valid if extra forms are not changed and initial forms are valid.\n    \"\"\"\n    initial = [\n        {'choice': 'Calexico', 'votes': 100},\n        {'choice': 'Fergie', 'votes': 900},\n    ]\n    data = {\n        'choices-TOTAL_FORMS': '3',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '2',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-2-choice': '',\n        'choices-2-votes': '',\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=1)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices', initial=initial)\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.forms],\n        [\n            {'votes': 100, 'choice': 'Calexico'},\n            {'votes': 900, 'choice': 'Fergie'},\n            {},\n        ]\n    )\n", "    def test_formset_min_num(self):\n        \"\"\"Formsets with min_num should have at least that many forms.\"\"\"\n        MinChoiceFormSet = formset_factory(Choice, min_num=2)\n        formset = MinChoiceFormSet()\n        self.assertEqual(len(formset.forms), 2)\n\n        # Validation should fail if not enough forms are filled\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': '',\n            'choices-1-votes': '',\n        }\n        formset = MinChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Please submit 2 or more forms.'])\n", "    def test_empty_formset_with_initial_data(self):\n        \"\"\"\n        An empty formset with initial data will have that data in the form\n        when accessed as empty_form.\n        \"\"\"\n        initial_data = [{'choice': 'Calexico', 'votes': 100}]\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet(initial=initial_data)\n        empty_form = formset.empty_form\n        self.assertEqual(empty_form.initial, initial_data[0])\n", "def test_extra_form_validation(self):\n    \"\"\"Extra forms should be validated only if they have changed.\"\"\"\n    class ExtraValidationForm(Form):\n        name = CharField(required=False)\n\n    data = {\n        'form-TOTAL_FORMS': '3',\n        'form-INITIAL_FORMS': '1',\n        'form-MIN_NUM_FORMS': '1',\n        'form-MAX_NUM_FORMS': '3',\n        'form-0-name': 'Initial data',\n    }\n    ExtraValidationFormSet = formset_factory(ExtraValidationForm, extra=2)\n    formset = ExtraValidationFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    data['form-1-name'] = 'Changed data'\n    formset = ExtraValidationFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    data['form-2-name'] = ''  # Blank data in extra form\n    formset = ExtraValidationFormSet(data, prefix='form')\n    self.assertFalse(formset.is_valid())\n"], "sample_643": ["def test_template_option_json_format(linter) -> None:\n    \"\"\"Test the msg-template option with JSON format.\"\"\"\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = '{{ \"line\": {line}, \"msg_id\": \"{msg_id}\", \"msg\": \"{msg}\" }}'\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\n        \"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4)\n    )\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1] == '{ \"line\": 1, \"msg_id\": \"C0301\", \"msg\": \"Line too long (1/2)\" }'\n    assert out_lines[2] == '{ \"line\": 2, \"msg_id\": \"C0301\", \"msg\": \"Line too long (3/4)\" }'\n", "def test_colorize_ansi_deprecation(recwarn: WarningsRecorder) -> None:\n    \"\"\"Test the deprecation warning for the old colorize_ansi parameters.\"\"\"\n    with pytest.warns(DeprecationWarning) as records:\n        colorize_ansi(\"Test message\", \"red\", \"bold\")\n\n    assert len(records) == 1\n    assert (\n        \"In pylint 3.0, the colorize_ansi function of Text reporters will only accept a MessageStyle parameter\"\n        in str(records[0].message)\n    )\n", "def test_colorize_ansi_deprecated_parameter(recwarn: WarningsRecorder) -> None:\n    \"\"\"Test the deprecated parameter in colorize_ansi function.\"\"\"\n    with pytest.warns(DeprecationWarning) as records:\n        colorize_ansi(\"Test message\", \"green\", \"bold\")\n        assert len(records) == 1\n        assert \"In pylint 3.0, the colorize_ansi function of Text reporters will only accept a MessageStyle parameter\" in str(records[0].message)\n", "def test_colorized_text_reporter_custom_color_mapping(linter: PyLinter) -> None:\n    \"\"\"Test the ColorizedTextReporter with a custom color mapping.\"\"\"\n    output = StringIO()\n    color_mapping = {\"C\": MessageStyle(\"blue\", (\"bold\",)), \"W\": MessageStyle(\"yellow\")}\n    linter.reporter = ColorizedTextReporter(output, color_mapping)\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"W0301\", line=2, args=(3, 4))\n    linter.close()\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1] == \"\\033[1;34m************* Module my_mod\\033[0m\"\n    assert out_lines[2].startswith(\"my_mod:1:0: \\033[1;34mC0301: Line too long (1/2) (line-too-long)\\033[0m\")\n    assert out_lines[3].startswith(\"my_mod:2:0: \\033[33mW0301: Message (3/4) (message-id)\\033[0m\")\n", "def test_color_mapping_option(linter: PyLinter) -> None:\n    \"\"\"Test the color_mapping option for ColorizedTextReporter.\"\"\"\n    output = StringIO()\n    linter.reporter.out = output\n    linter.set_option(\"output-format\", \"colorized\")\n    linter.config.color_mapping = {\n        \"I\": (\"green\", \"bold\"),\n        \"C\": (\"magenta\", \"underline\"),\n        \"R\": (\"red\", \"blink\"),\n        \"W\": (\"yellow\", \"inverse\"),\n        \"E\": (\"cyan\", \"strike\"),\n        \"F\": (\"blue\", \"italic\"),\n        \"S\": (\"white\", \"reset\"),\n    }\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\n        \"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4)\n    )\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert \"\\033[1;32;49m************* Module my_mod\\033[0m\" in out_lines[1]\n    assert \"\\033[35;4;49mC0301: Line too long (1/2) (line-too-long)\\033[0m\" in out_lines[2]\n    assert \"\\033[35;4;49mC0301: Line too long (3/4) (line-too-long)\\033[0m\" in out_lines[3]\n"], "sample_1040": ["def test_print_random_symbol():\n    X = RandomSymbol('X')\n    assert mpp.doprint(X) == '<mi>X</mi>'\n    assert mp.doprint(X) == '<ci>X</ci>'\n", "def test_print_random_symbol():\n    R = RandomSymbol('R', set=S.Reals)\n    assert mpp.doprint(R) == '<mi>R</mi>'\n    assert mp.doprint(R) == '<ci>R</ci>'\n", "def test_presentation_mathml_power_of_negative():\n    mml = mpp._print(-x**2)\n    assert mml.childNodes[0].childNodes[0].nodeValue == '-'\n    assert mml.childNodes[1].childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mml.childNodes[1].childNodes[1].childNodes[0].nodeValue == '2'\n", "def test_print_matrix_symbol_bold():\n    A = MatrixSymbol('A', 1, 2)\n    assert mathml(A, printer='presentation', mat_symbol_style=\"bold\") == '<mi mathvariant=\"bold\">A</mi>'\n    assert mathml(A, mat_symbol_style=\"bold\") == '<ci>A</ci>' # No effect in content printer\n", "def test_presentation_mathml_matrix_symbol():\n    A = MatrixSymbol('A', 1, 2)\n    assert mpp._print(A).nodeName == 'mi'\n    assert mpp._print(A).childNodes[0].nodeValue == 'A'\n    del A\n\n    A = MatrixSymbol('A', 2, 3)\n    assert mpp._print(A).nodeName == 'mi'\n    assert mpp._print(A).childNodes[0].nodeValue == 'A'\n"], "sample_581": ["def test_blueprint_url_defaults_override(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/foo\", defaults={\"baz\": 42})\n        return f\"{bar}/{baz:d}\"\n\n    app.register_blueprint(bp, url_prefix=\"/1\", url_defaults={\"bar\": 23})\n    app.register_blueprint(bp, name=\"bp2\", url_prefix=\"/2\", url_defaults={\"bar\": 19, \"baz\": 27})\n\n    assert client.get(\"/1/foo\").data == b\"23/42\"\n    assert client.get(\"/2/foo\").data == b\"19/27\"\n", "def test_blueprint_register_once(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n    call_count = 0\n\n    @bp.record_once\n        nonlocal call_count\n        call_count += 1\n\n    app.register_blueprint(bp)\n    app.register_blueprint(bp)\n\n    assert call_count == 1\n", "def test_blueprint_url_defaults_overriding(app, client):\n    bp = flask.Blueprint(\"test\", __name__)\n\n    @bp.route(\"/foo\", defaults={\"baz\": 42})\n        return f\"{bar}/{baz:d}\"\n\n    app.register_blueprint(bp, url_prefix=\"/1\", url_defaults={\"bar\": 23})\n    app.register_blueprint(bp, name=\"test2\", url_prefix=\"/2\", url_defaults={\"baz\": 19})\n\n    assert client.get(\"/1/foo\").data == b\"23/42\"\n    assert client.get(\"/2/foo\").data == b\"23/19\"\n", "def test_blueprint_url_defaults_overriding(app, client):\n    bp = flask.Blueprint(\"bp\", __name__, url_defaults={\"bar\": 42})\n\n    @bp.route(\"/foo\", defaults={\"baz\": 23})\n        return f\"{bar}/{baz:d}\"\n\n    app.register_blueprint(bp, url_prefix=\"/1\", url_defaults={\"bar\": 19})\n\n    assert client.get(\"/1/foo\").data == b\"19/23\"\n", "def test_blueprint_url_defaults_merging(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/foo\", defaults={\"baz\": 42})\n        return f\"{bar}/{baz:d}\"\n\n    @bp.route(\"/bar\")\n        return str(bar)\n\n    app.register_blueprint(bp, url_prefix=\"/1\", url_defaults={\"bar\": 23})\n    app.register_blueprint(bp, name=\"bp2\", url_prefix=\"/2\", url_defaults={\"bar\": 19, \"baz\": 50})\n\n    assert client.get(\"/1/foo\").data == b\"23/42\"\n    assert client.get(\"/2/foo\").data == b\"19/50\"\n    assert client.get(\"/1/bar\").data == b\"23\"\n    assert client.get(\"/2/bar\").data == b\"19\"\n"], "sample_993": ["def test_FreeGroupElm_subword():\n    w = x**5*y*x**2*y**-4*x\n    assert w.subword(2, 6) == x**3*y\n    assert w.subword(6, 8) == y**-4*x\n    assert w.subword(0, 0) == F.identity\n    assert w.subword(0, 5) == x**5*y*x**2\n", "def test_FreeGroup_subword_index():\n    w = x**2*y*x*y**3\n    assert w.subword_index(x*y) == 1\n    assert w.subword_index(y**3) == 3\n    raises(ValueError, lambda: w.subword_index(x**3))\n    raises(ValueError, lambda: w.subword_index(y**4))\n", "def test_FreeGroupElm_subword_index():\n    w = x**2*y**3*x**-1*y\n    assert w.subword_index(y**3) == 3\n    assert w.subword_index(x*y) == 2\n    assert w.subword_index(x**-1*y) == 5\n    assert raises(ValueError, lambda: w.subword_index(y**4))\n    assert raises(ValueError, lambda: w.subword_index(y**-3))\n", "def test_FreeGroupElm_cyclic_conjugates():\n    w = x*y*x*y*x\n    assert w.cyclic_conjugates() == {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x, x*y*x*y*x}\n\n    s = x*y*x**2*y*x\n    assert s.cyclic_conjugates() == {x**2*y*x**2*y, y*x**2*y*x**2, x*y*x**2*y*x}\n", "def test_FreeGroupElm_subword():\n    w = x**5*y*x**2*y**-4*x\n    assert w.subword(2, 6) == x**3*y\n    assert w.subword(0, 13) == w\n    assert w.subword(3, 3) == F.identity\n    assert w.subword(7, 10) == y**-4*x\n    assert w.subword(10, 13) == F.identity\n"], "sample_187": ["def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorldTest'), 'hello world test')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld_Test'), 'hello world_ test')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('HelloWorld')), 'hello world')\n", "    def test_camel_case_to_spaces(self):\n        items = [\n            ('HelloWorld', 'hello world'),\n            ('CamelCaseExample', 'camel case example'),\n            ('singleWord', 'single word'),\n            ('', ''),\n            ('Already lowercase', 'already lowercase'),\n            ('MultipleSpaces', 'multiple spaces'),\n        ]\n        for value, output in items:\n            self.assertEqual(text.camel_case_to_spaces(value), output)\n            self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "    def test_camel_case_to_spaces(self):\n        items = [\n            ('ThisIsCamelCase', 'this is camel case'),\n            ('CamelCaseText', 'camel case text'),\n            ('SomeURL', 'some u r l'),\n            ('LongStringWithoutSpaces', 'long string without spaces'),\n            ('', ''),\n        ]\n        for value, output in items:\n            self.assertEqual(text.camel_case_to_spaces(value), output)\n            self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseTest'), 'camel case test')\n    self.assertEqual(text.camel_case_to_spaces('AnotherTestExample'), 'another test example')\n    self.assertEqual(text.camel_case_to_spaces('NoCamelCase'), 'no camel case')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('LazyCamelCaseTest')), 'lazy camel case test')\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('', ''),\n        ('HelloWorld', 'hello world'),\n        ('CamelCase', 'camel case'),\n        ('MultipleCapitalLetters', 'multiple capital letters'),\n        ('NoSpace', 'no space'),\n        ('WithSpaces', 'with spaces'),\n        ('LeadingCapital', 'leading capital'),\n        ('TrailingCapital', 'trailing capital'),\n        ('SingleCharacter', 'single character'),\n        ('AllCaps', 'all caps'),\n        ('alllowercase', 'alllowercase'),\n        ('Number2Letter', 'number 2 letter'),\n        ('Number22Letter', 'number 22 letter'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n"], "sample_103": ["def test_filter_aggregate_with_distinct(self):\n    vals = Book.objects.filter(rating__lt=4.5).aggregate(distinct_pages=Count('pages', distinct=True))\n    self.assertEqual(vals, {'distinct_pages': 3})\n", "def test_stddev_variance_aggregates(self):\n    # Test StdDev aggregate\n    stddev = Book.objects.aggregate(stddev_rating=StdDev('rating'))\n    self.assertIsInstance(stddev['stddev_rating'], float)\n\n    # Test Var aggregate\n    variance = Book.objects.aggregate(variance_rating=Variance('rating'))\n    self.assertIsInstance(variance['variance_rating'], float)\n\n    # Test StdDev and Var aggregates with sample=True\n    stddev_sample = Book.objects.aggregate(stddev_rating_sample=StdDev('rating', sample=True))\n    variance_sample = Book.objects.aggregate(variance_rating_sample=Variance('rating', sample=True))\n    self.assertIsInstance(stddev_sample['stddev_rating_sample'], float)\n    self.assertIsInstance(variance_sample['variance_rating_sample'], float)\n", "def test_stddev_aggregate(self):\n    ages = Author.objects.aggregate(stddev_age=StdDev('age'))\n    self.assertIsInstance(ages['stddev_age'], float)\n    self.assertAlmostEqual(ages['stddev_age'], 5.5277, places=4)\n\n    # Test sample standard deviation\n    sample_ages = Author.objects.aggregate(stddev_age=StdDev('age', sample=True))\n    self.assertIsInstance(sample_ages['stddev_age'], float)\n    self.assertAlmostEqual(sample_ages['stddev_age'], 5.7156, places=4)\n", "def test_annotate_with_case_when(self):\n    books = Book.objects.annotate(\n        price_category=Case(\n            When(price__lt=Decimal('30.0'), then=Value('Cheap')),\n            When(price__lt=Decimal('50.0'), then=Value('Moderate')),\n            default=Value('Expensive'),\n        )\n    ).order_by('name')\n\n    self.assertQuerysetEqual(\n        books, [\n            ('Artificial Intelligence: A Modern Approach', 'Expensive'),\n            ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 'Expensive'),\n            ('Practical Django Projects', 'Cheap'),\n            ('Python Web Development with Django', 'Cheap'),\n            ('Sams Teach Yourself Django in 24 Hours', 'Cheap'),\n            ('The Definitive Guide to Django: Web Development Done Right', 'Cheap')\n        ],\n        lambda b: (b.name, b.price_category)\n    )\n", "def test_distinct_on_aggregate_with_filter(self):\n    \"\"\"\n    Distinct on aggregate with filter should only aggregate the distinct items\n    that match the filter.\n    \"\"\"\n    books = Book.objects.filter(authors__name__contains=\"Norvig\").distinct()\n    self.assertEqual(books.count(), 2)\n\n    # Only one book (Paradigms of Artificial Intelligence Programming) has rating >= 4.0\n    distinct_books_with_filter = books.filter(rating__gte=4.0).distinct()\n    self.assertEqual(distinct_books_with_filter.count(), 1)\n\n    # Count the distinct ratings with the filter\n    rating_count = distinct_books_with_filter.aggregate(Count('rating', distinct=True))\n    self.assertEqual(rating_count['rating__count'], 1)\n"], "sample_983": ["def test_sparse_cholesky_symbolic():\n    S = SparseMatrix([\n        [1, 0, 3, 2],\n        [0, 0, 1, 0],\n        [4, 0, 0, 5],\n        [0, 6, 7, 0]])\n    assert S.row_structure_symbolic_cholesky() == [[0], [], [0], [1, 2]]\n    assert S.liupc() == ([[0], [], [0], [1, 2]], [4, 3, 4, 4])\n", "def test_sparse_matrix_scalar_multiply():\n    A = SparseMatrix([[1, 2], [3, 4]])\n    B = A.scalar_multiply(2)\n    assert B == SparseMatrix([[2, 4], [6, 8]])\n    C = A.scalar_multiply(0)\n    assert C == SparseMatrix(2, 2, {})\n", "def test_sparse_matrix_multiplication():\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = SparseMatrix(((5, 6), (7, 8)))\n    assert A*B == SparseMatrix(((19, 22), (43, 50)))\n\n    A = SparseMatrix(((1, 0), (0, 1)))\n    B = SparseMatrix(((1, 2), (3, 4)))\n    assert A*B == B\n\n    A = SparseMatrix(((1, 2, 3), (4, 5, 6)))\n    B = SparseMatrix(((7, 8), (9, 10), (11, 12)))\n    assert A*B == SparseMatrix(((58, 64), (139, 154)))\n", "def test_sparse_matmul():\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = SparseMatrix(((5, 6), (7, 8)))\n    assert A * B == SparseMatrix(((19, 22), (43, 50)))\n\n    A = SparseMatrix(((1, 0), (0, 1)))\n    B = SparseMatrix(((1, 2), (3, 4)))\n    assert A * B == B\n", "def test_sparse_scalar_multiply():\n    A = SparseMatrix(((1, 2), (3, 4)))\n    scalar = 2\n    B = A.scalar_multiply(scalar)\n    assert B == SparseMatrix(((2, 4), (6, 8)))\n    scalar = 0\n    B = A.scalar_multiply(scalar)\n    assert B == SparseMatrix(((0, 0), (0, 0)))\n"], "sample_60": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.episode = self._create_object(Episode)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.user = User.objects.create_user(username='normal', password='secret', email='normal@example.com')\n"], "sample_1204": ["def test_symmetric_permutation_group():\n    G = SymmetricPermutationGroup(4)\n    assert G.order() == 24\n    assert G.degree == 4\n    assert G.identity == Permutation([0, 1, 2, 3])\n    assert Permutation([1, 2, 0, 3]) in G\n    assert Permutation([1, 2, 3]) not in G\n", "def test_conjugacy_class_size():\n    S = SymmetricGroup(4)\n    x = Permutation(1, 2, 3)\n    assert S.conjugacy_class_size(x) == 8\n", "def test_polycyclic_group():\n    a = Permutation([0, 1, 2])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    P = G.polycyclic_group()\n    assert P.pc_sequence == [Permutation([2, 1, 0])]\n    assert P.pc_series == [PermutationGroup(Permutation([0, 1, 2])), PermutationGroup(Permutation([0, 1, 2]), Permutation([2, 1, 0]))]\n    assert P.relative_order == [2]\n", "def test_coset_class_exceptions():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    H = PermutationGroup([a])\n    g = Permutation(1, 2, size=3)\n\n    with pytest.raises(ValueError):\n        Coset(g, H, G, dir='+')\n\n    with pytest.raises(ValueError):\n        Coset(a, H, dir='x')\n\n    with pytest.raises(TypeError):\n        Coset(a, H, dir=123)\n", "def test_polycyclic_group():\n    P = PermutationGroup(Permutation(0,1,5,2)(3,7,4,6), Permutation(0,3,5,4)(1,6,2,7))\n    pg = P.polycyclic_group()\n    assert pg.order() == P.order()\n\n    P = DihedralGroup(12)\n    pg = P.polycyclic_group()\n    assert pg.order() == P.order()\n\n    P = PermutationGroup(Permutation(1, 5)(2, 4), Permutation(0, 1, 2, 3, 4, 5), Permutation(0, 2))\n    pg = P.polycyclic_group()\n    assert pg.order() == P.order()\n"], "sample_432": ["def test_save_with_changes_without_pending_action(self):\n    from selenium.webdriver.common.by import By\n\n    Parent.objects.create(name=\"parent\")\n\n    self.admin_login(username=\"super\", password=\"secret\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n    )\n\n    name_input = self.selenium.find_element(By.ID, \"id_form-0-name\")\n    name_input.clear()\n    name_input.send_keys(\"other name\")\n    self.selenium.find_element(By.NAME, \"_save\").click()\n    success_message = self.selenium.find_element(By.CSS_SELECTOR, \".success\").text\n    self.assertEqual(success_message, \"The parent \u201cother name\u201d was changed successfully.\")\n", "def test_deterministic_order_for_model_with_no_ordering(self):\n    \"\"\"\n    The primary key is used in the ordering of the changelist's results to\n    guarantee a deterministic order, even when the model doesn't have any\n    default ordering defined (#17198).\n    \"\"\"\n    superuser = self._create_superuser(\"superuser\")\n\n    for counter in range(1, 51):\n        UnorderedObject.objects.create(id=counter, bool=True)\n\n    class UnorderedObjectAdmin(admin.ModelAdmin):\n        list_per_page = 10\n\n    custom_site.register(UnorderedObject, UnorderedObjectAdmin)\n    model_admin = UnorderedObjectAdmin(UnorderedObject, custom_site)\n    request = self._mocked_authenticated_request(\"/unorderedobject/?p=1\", superuser)\n    response = model_admin.changelist_view(request)\n    self.assertEqual(response.context_data[\"cl\"].result_list[0].id, 1)\n    custom_site.unregister(UnorderedObject)\n", "def test_save_with_no_changes_and_no_action(self):\n    from selenium.webdriver.common.by import By\n\n    Parent.objects.create(name=\"parent\")\n\n    self.admin_login(username=\"super\", password=\"secret\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n    )\n\n    self.selenium.find_element(By.NAME, \"_save\").click()\n    alert = self.selenium.switch_to.alert\n    try:\n        self.assertEqual(\n            alert.text,\n            \"You haven\u2019t made any changes to save.\",\n        )\n    finally:\n        alert.dismiss()\n", "def test_search_fields_with_related_field(self):\n    parent = Parent.objects.create(name=\"Mary\")\n    Child.objects.create(parent=parent, name=\"Danielle\", age=18)\n    Child.objects.create(parent=parent, name=\"Daniel\", age=19)\n\n    m = ParentAdmin(Parent, custom_site)\n    m.search_fields = [\"child__name\"]\n\n    request = self.factory.get(\"/parent/\", data={SEARCH_VAR: \"Danielle\"})\n    request.user = self.superuser\n\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.queryset.count(), 1)\n\n    request = self.factory.get(\"/parent/\", data={SEARCH_VAR: \"Daniel\"})\n    request.user = self.superuser\n\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.queryset.count(), 1)\n\n    request = self.factory.get(\"/parent/\", data={SEARCH_VAR: \"Dan\"})\n    request.user = self.superuser\n\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.queryset.count(), 2)\n", "def test_deterministic_order_for_model_with_meta_ordering(self):\n    \"\"\"\n    The primary key is used in the ordering of the changelist's results to\n    guarantee a deterministic order, even when the model defines a Meta ordering\n    (#17198).\n    \"\"\"\n    superuser = self._create_superuser(\"superuser\")\n\n    for counter in range(1, 51):\n        Band.objects.create(name=\"Band %s\" % counter, nr_of_members=counter)\n\n    class BandAdmin(admin.ModelAdmin):\n        list_per_page = 10\n\n    custom_site.register(Band, BandAdmin)\n    model_admin = BandAdmin(Band, custom_site)\n    counter = 51\n    for page in range(1, 6):\n        request = self._mocked_authenticated_request(\"/band/?p=%s\" % page, superuser)\n        response = model_admin.changelist_view(request)\n        for result in response.context_data[\"cl\"].result_list:\n            counter -= 1\n            self.assertEqual(result.id, counter)\n    custom_site.unregister(Band)\n"], "sample_762": ["def test_get_params_deep_false():\n    test = T(K(), K())\n    params = test.get_params(deep=False)\n    assert 'a__d' not in params\n    assert 'b__d' not in params\n    assert 'a' in params\n    assert 'b' in params\n", "def test_get_param_names():\n    # Test that _get_param_names returns the correct parameter names\n    params = MyEstimator._get_param_names()\n    assert_equal(params, ['empty', 'l1'])\n", "def test_pickling_with_nested_estimators():\n    estimator = Pipeline([\n        ('transformer', TransformerMixin()),\n        ('classifier', DecisionTreeClassifier())\n    ])\n    estimator.fit(iris.data, iris.target)\n\n    serialized = pickle.dumps(estimator)\n    estimator_restored = pickle.loads(serialized)\n\n    assert_array_equal(estimator_restored.predict(iris.data),\n                       estimator.predict(iris.data))\n", "def test_clone_non_estimator_safe_false():\n    # Test clone function with safe=False on an object that is not an estimator\n    no_estimator = NoEstimator()\n    cloned_obj = clone(no_estimator, safe=False)\n    assert cloned_obj is not no_estimator\n    assert_true(isinstance(cloned_obj, NoEstimator))\n", "def test_pickling_when_setstate_is_overwritten_by_mixin():\n    estimator = MultiInheritanceEstimator()\n    estimator._attribute_not_pickled = \"this attribute should not be pickled\"\n\n    serialized = pickle.dumps(estimator)\n    estimator_restored = pickle.loads(serialized)\n    assert_equal(estimator_restored.attribute_pickled, 5)\n    assert_equal(estimator_restored._attribute_not_pickled, None)\n    assert estimator_restored._restored\n\n    estimator_restored.attribute_pickled = 10\n    serialized_again = pickle.dumps(estimator_restored)\n    estimator_restored_again = pickle.loads(serialized_again)\n    assert_equal(estimator_restored_again.attribute_pickled, 10)\n    assert estimator_restored_again._restored\n"], "sample_536": ["def test_polygon_selector_update(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    new_verts = [(75, 75), (175, 75), (75, 175)]\n    tool.verts = new_verts\n    assert tool.verts == new_verts\n    if draw_bounding_box:\n        assert tool._box.extents == (75, 175, 75, 175)\n", "def test_polygon_selector_box_props(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_handle_props=dict(color='r'),\n                                   box_props=dict(edgecolor='g', facecolor='b'))\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check the properties of the bounding box\n    assert tool._box._corner_handles.artists[0].get_color() == 'r'\n    assert tool._box._selection_artist.get_edgecolor() == 'g'\n    assert tool._box._selection_artist.get_facecolor() == 'b'\n", "def test_polygon_selector_clear_key(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        # Polygon completed, now clear using the key\n        ('on_key_press', dict(key='escape')),\n        ('on_key_release', dict(key='escape')),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    # After clearing the polygon, no vertices should be left\n    assert tool.verts == []\n", "def test_polygon_selector_box_props(ax, draw_bounding_box):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with box_props\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box,\n                                   box_props={'edgecolor': 'r', 'facecolor': 'none', 'linewidth': 2})\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the box properties are set correctly\n    assert tool._box._selection_artist.get_edgecolor() == 'r'\n    assert tool._box._selection_artist.get_facecolor() == 'none'\n    assert tool._box._selection_artist.get_linewidth() == 2\n", "def test_polygon_selector_box_remove(ax, draw_bounding_box):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Remove a point from the polygon and check that the box extents update\n    np.testing.assert_allclose(\n        tool._box.extents, (0.0, 40.0, 0.0, 40.0))\n\n    MouseEvent(\n        \"button_press_event\", ax.figure.canvas, *ax.transData.transform((30, 20)), 3)._process()\n    MouseEvent(\n        \"button_release_event\", ax.figure.canvas, *ax.transData.transform((30, 20)), 3)._process()\n    np.testing.assert_allclose(\n        tool.verts, [(0, 20), (20, 40), (40, 20)])\n    np.testing.assert_allclose(\n        tool._box.extents, (0.0, 40.0, 20.0, 40.0))\n"], "sample_619": ["def test_decode_cf_datetime_float():\n    units = \"seconds since 2018-08-22T03:23:03Z\"\n    num_dates = 50.0\n    result = decode_cf_datetime(num_dates, units)\n    expected = np.asarray(np.datetime64(\"2018-08-22T03:23:53\", \"ns\"))\n    np.testing.assert_equal(result, expected)\n", "def test_decode_cf_datetime_out_of_bounds():\n    units = \"days since 1677-01-01\"\n    num_dates = -366\n    with pytest.raises(OutOfBoundsDatetime):\n        decode_cf_datetime(num_dates, units)\n", "def test_decode_non_standard_calendar_single_element_timedelta(calendar, num_time, expected):\n    import cftime\n\n    units = \"days since 0001-01-01\"\n\n    actual = coding.times.decode_cf_timedelta(num_time, units)\n\n    assert actual.dtype == np.dtype(\"timedelta64[ns]\")\n    assert expected == actual\n", "def test_decode_cf_datetime_uint64_with_cftime_out_of_range():\n    units = \"days since 1700-01-01\"\n    calendar = \"360_day\"\n    num_dates = np.uint64(182621)\n    with pytest.raises(OutOfBoundsDatetime):\n        decode_cf_datetime(num_dates, units, calendar)\n", "def test_use_cftime_false_non_standard_calendar_out_of_range(units_year) -> None:\n    calendar = _NON_STANDARD_CALENDARS[0]\n    numerical_dates = [0, 1]\n    units = f\"days since {units_year}-01-01\"\n    with pytest.raises(OutOfBoundsDatetime):\n        decode_cf_datetime(numerical_dates, units, calendar, use_cftime=False)\n"], "sample_819": ["def test_transform_hard_voting():\n    \"\"\"Check transform method of VotingClassifier with hard voting on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    assert_array_equal(eclf.transform(X).shape, (3, 4))\n    assert_array_equal(eclf.transform(X),\n                       np.array([clf1.predict(X), clf2.predict(X), clf3.predict(X)]))\n", "def test_predict_proba_flatten_transform():\n    \"\"\"Check flatten_transform parameter of VotingClassifier\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf1 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=True).fit(X, y)\n    eclf2 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=False).fit(X, y)\n\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n    assert_array_equal(eclf1.predict_proba(X).shape, (4, 2))\n    assert_array_equal(eclf2.predict_proba(X).shape, (3, 4, 2))\n    assert_array_almost_equal(eclf1.predict_proba(X),\n                              eclf2.predict_proba(X).mean(axis=0))\n", "def test_parallel_regressor():\n    \"\"\"Check parallel backend of VotingRegressor on boston dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=50, random_state=1)\n    ereg1 = VotingRegressor([('lr', reg1), ('rf', reg2)], n_jobs=1).fit(X_r, y_r)\n    ereg2 = VotingRegressor([('lr', reg1), ('rf', reg2)], n_jobs=2).fit(X_r, y_r)\n\n    assert_array_almost_equal(ereg1.predict(X_r), ereg2.predict(X_r))\n", "def test_parallel_fit_regressor():\n    \"\"\"Check parallel backend of VotingRegressor on boston dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    reg3 = DummyRegressor(strategy='mean')\n    X_r_train, X_r_test, y_r_train, y_r_test = \\\n        train_test_split(X_r, y_r, test_size=.25)\n\n    ereg1 = VotingRegressor([('lr', reg1), ('rf', reg2), ('dr', reg3)],\n                            n_jobs=1).fit(X_r_train, y_r_train)\n    ereg2 = VotingRegressor([('lr', reg1), ('rf', reg2), ('dr', reg3)],\n                            n_jobs=2).fit(X_r_train, y_r_train)\n\n    assert_array_almost_equal(ereg1.predict(X_r_test), ereg2.predict(X_r_test))\n", "def test_regressor_parallel_fit():\n    \"\"\"Check parallel backend of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    reg3 = DummyRegressor(strategy='median')\n    X_r = np.array([[1.1], [1.2], [3.4], [2.1], [2.2]])\n    y_r = np.array([2.1, 2.2, 4.4, 3.1, 3.2])\n\n    ereg1 = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dummy', reg3)],\n        n_jobs=1).fit(X_r, y_r)\n    ereg2 = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dummy', reg3)],\n        n_jobs=2).fit(X_r, y_r)\n\n    assert_array_almost_equal(ereg1.predict(X_r), ereg2.predict(X_r))\n"], "sample_446": ["def test_large_numbers(self):\n    self.assertEqual(floatformat(12345678901234567890.123456789, 2), \"12345678901234567890.12\")\n", "def test_large_numbers(self):\n    with localcontext() as ctx:\n        ctx.prec = 20\n        self.assertEqual(floatformat(12345678901234567890.12345678901234567890, 2), \"12345678901234567890.12\")\n        self.assertEqual(floatformat(12345678901234567890.12345678901234567890, -2), \"12345678901234567890.12\")\n", "    def test_decimal_large_exponent(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            num = Decimal(\"1E-21234567890123456789\")\n            self.assertEqual(floatformat(num, 2), \"0.00\")\n", "def test_large_decimal_values(self):\n    with localcontext() as ctx:\n        ctx.prec = 50  # Set precision to handle large decimal values\n        self.assertEqual(floatformat(Decimal(\"1234567890123456789012345678901234567890\"), 10), \"123456789012345678901234567890.1234567890\")\n        self.assertEqual(floatformat(Decimal(\"-1234567890123456789012345678901234567890\"), 10), \"-123456789012345678901234567890.1234567890\")\n", "def test_scientific_notation(self):\n    with localcontext() as ctx:\n        # Adjust the precision to avoid converting to scientific notation.\n        ctx.prec = 100\n        self.assertEqual(floatformat(1.23456789e-4, 8), \"0.00012346\")\n        self.assertEqual(floatformat(1.23456789e4, 2), \"12345.68\")\n"], "sample_350": ["def test_values_list_with_empty_qs(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.none()\n    qs3 = Number.objects.filter(pk__in=[])\n    self.assertEqual(len(qs1.values_list('num').union(qs2)), 10)\n    self.assertEqual(len(qs2.values_list('num').union(qs1)), 10)\n    self.assertEqual(len(qs1.values_list('num').union(qs3)), 10)\n    self.assertEqual(len(qs3.values_list('num').union(qs1)), 10)\n    self.assertEqual(len(qs2.values_list('num').union(qs1, qs1, qs1)), 10)\n    self.assertEqual(len(qs2.values_list('num').union(qs1, qs1, all=True)), 20)\n    self.assertEqual(len(qs2.values_list('num').union(qs2)), 0)\n    self.assertEqual(len(qs3.values_list('num').union(qs3)), 0)\n", "def test_union_with_mixed_fields(self):\n    qs1 = Number.objects.filter(num__gte=5).values('num')\n    qs2 = Number.objects.filter(num__lte=5).values('other_num')\n    with self.assertRaisesMessage(TypeError, \"Merging 'QuerySet' classes must involve the same values in each case.\"):\n        qs1.union(qs2)\n", "def test_union_with_values_list_and_filter(self):\n    qs1 = Number.objects.filter(num__lt=5).values_list('num', flat=True)\n    qs2 = Number.objects.filter(num__gt=6).values_list('num', flat=True)\n    self.assertCountEqual(qs1.union(qs2).filter(num__gt=4, num__lt=7), [5])\n", "def test_union_with_mixed_fields(self):\n    qs1 = Number.objects.filter(num=1).values('num')\n    qs2 = Number.objects.filter(num=2).values('other_num')\n    with self.assertRaises(TypeError):\n        qs1.union(qs2)\n", "def test_union_with_exclude(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.exclude(num__lte=4)\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 4, 5], ordered=False)\n"], "sample_845": ["def test_callable_analyzer_returns_iterator(Estimator):\n        return iter(doc.split())\n\n    data = ['this is text, not file or filename']\n    X = Estimator(analyzer=analyzer).fit_transform(data)\n    assert X.shape[0] == len(data)\n", "def test_custom_analyzer_with_decode_error(Estimator):\n    # check if a custom exception from the analyzer is shown to the user\n        return doc.decode('utf-8')\n\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    data = ['\\xc3\\xa9'.encode('utf-8')]\n    with pytest.raises(UnicodeDecodeError):\n        Estimator(analyzer=analyzer, decode_error='strict').fit_transform(data)\n", "def test_custom_analyzer_with_decoder(Estimator):\n        return doc.decode('utf-8').split()\n\n    data = [b'this is a test document']\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    vec = Estimator(analyzer=analyzer, decode_error='strict', input='content')\n    X = vec.fit_transform(data)\n    assert X.shape[1] == 4  # 'this', 'is', 'a', 'test'\n", "def test_callable_analyzer_with_custom_decode_error(Estimator):\n    data = [b'caf\\xc3\\xa9']\n    analyzer = lambda doc: doc.decode(encoding='utf-8')\n    vec = Estimator(analyzer=analyzer, decode_error='ignore')\n    vec.fit_transform(data)\n", "def test_vectorizer_max_df_range(Estimator):\n    vect = Estimator(max_df=0.0)\n    assert_raises(ValueError, vect.fit, [\"text\", \"text\"])\n\n    vect = Estimator(max_df=1.1)\n    assert_raises(ValueError, vect.fit, [\"text\", \"text\"])\n\n    vect = Estimator(min_df=-1.0)\n    assert_raises(ValueError, vect.fit, [\"text\", \"text\"])\n"], "sample_484": ["def test_expression_length(self):\n    Author.objects.create(name=\"Jane Doe\", alias=\"doe\")\n    authors = Author.objects.annotate(name_part=Right(\"name\", Value(2) + Value(3)))\n    self.assertQuerySetEqual(\n        authors.order_by(\"name\"), [\"e\"], lambda a: a.name_part\n    )\n", "def test_none_length(self):\n    Author.objects.create(name=None, alias=\"null\")\n    authors = Author.objects.annotate(name_part=Right(\"name\", Value(5)))\n    self.assertQuerySetEqual(\n        authors.order_by(\"name\"),\n        [None, \"honda\", \"\" if connection.features.interprets_empty_strings_as_nulls else None],\n        lambda a: a.name_part,\n    )\n", "def test_expression_length(self):\n    Author.objects.create(name=\"Mary Jane\", alias=\"jane\")\n    authors = Author.objects.annotate(name_part=Right(\"name\", Value(5) - Length(\"alias\")))\n    self.assertQuerySetEqual(\n        authors.order_by(\"name\"),\n        [\"y Jane\", \"\"],\n        lambda a: a.name_part,\n    )\n", "def test_special_characters(self):\n    Author.objects.create(name=\"J\u00f6hn D\u00f6e\", alias=\"doej\")\n    authors = Author.objects.annotate(name_part=Right(\"name\", 5))\n    self.assertEqual(authors.get(name=\"J\u00f6hn D\u00f6e\").name_part, \"hn D\u00f6e\")\n", "def test_special_characters(self):\n    Author.objects.create(name=\"J\u00f3hn D\u0153\", alias=\"john\")\n    authors = Author.objects.annotate(name_part=Right(\"name\", 4))\n    self.assertQuerySetEqual(\n        authors.order_by(\"name\"), [\" D\u0153\", \"hn\"], lambda a: a.name_part\n    )\n"], "sample_81": ["    def test_match(self):\n        pattern = RoutePattern('article/<int:id>/')\n        match = pattern.match('article/123/')\n        self.assertEqual(match, ('', (), {'id': 123}))\n", "    def test_check(self):\n        pattern = RoutePattern('path/<int:pk>/')\n        warnings = pattern.check()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(warnings[0].id, '2_0.W001')\n", "def dummy_view(request):\n    pass\n", "    def test_converters(self):\n        pattern = RoutePattern('<int:pk>/')\n        self.assertEqual(pattern.converters['pk'], IntConverter())\n", "    def test_route_to_regex_identifier(self):\n        with self.assertRaises(ImproperlyConfigured):\n            _route_to_regex('<int:123>')\n"], "sample_418": ["def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"mylist\": [1, 2, 3]})\n    self.assertEqual(output, \"Length is 3\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"var\": \"django\"})\n    self.assertEqual(output, \"6\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"var\": \"abc\"})\n    self.assertEqual(output, \"True\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"var\": \"\"})\n    self.assertEqual(output, \"True\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"mylist\": []})\n    self.assertEqual(output, \"Length is 0\")\n"], "sample_748": ["def test_grid_search_with_none_param():\n    X, y = [[1], [2], [3], [4], [5]], [0, 0, 0, 0, 1]\n    est = DecisionTreeRegressor(random_state=0)\n    params = {\"random_state\": [0, None]}\n    cv = KFold(random_state=0)\n\n    grid_search = GridSearchCV(est, params, cv=cv).fit(X, y)\n    assert_array_equal(grid_search.cv_results_['param_random_state'],\n                       [0, None])\n", "def test_grid_search_cv_splits_consistency_with_different_params():\n    # Check if the cross-validation splits are consistent for different parameters\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=n_samples, random_state=0)\n\n    gs1 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2]},\n                       cv=KFold(n_splits=n_splits, shuffle=True, random_state=0))\n    gs1.fit(X, y)\n\n    gs2 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.3, 0.4]},\n                       cv=KFold(n_splits=n_splits, shuffle=True, random_state=0))\n    gs2.fit(X, y)\n\n    # Check if the train and test indices are the same for both parameter sets\n    for i in range(n_splits):\n        assert_array_equal(gs1.cv[i]['train'], gs2.cv[i]['train'])\n        assert_array_equal(gs1.cv[i]['test'], gs2.cv[i]['test'])\n", "def test_grid_search_cv_with_predict_proba():\n    # Test that grid search can be used with estimators that have predict_proba method\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=2, random_state=0)\n    clf = SVC(probability=True, random_state=0)\n    grid_search = GridSearchCV(clf, param_grid={'C': [0.1, 1, 10]}, cv=3)\n    grid_search.fit(X, y)\n    assert hasattr(grid_search.best_estimator_, 'predict_proba')\n    proba = grid_search.predict_proba(X)\n    assert proba.shape == (100, 2)\n", "def test_search_with_no_scorer():\n    # Test search with no scorer\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    clf = MockClassifier()\n    search = GridSearchCV(clf, param_grid={'foo_param': [1, 2, 3]}, scoring=None)\n    search.fit(X, y)\n    assert_false(hasattr(search, 'best_score_'))\n    assert_false(hasattr(search, 'cv_results_'))\n    assert_raise_message(NotFittedError, \"This GridSearchCV instance was initialized with `refit=True` and \"\n                         \"either `fit` has not been called yet or `fit` did not complete successfully. \"\n                         \"You can use the `score_samples` method to predict scores before fitting.\",\n                         search.score, X, y)\n", "def test_grid_search_multimetric_refit_string():\n    # Test that GridSearchCV can be used for model selection and refit with a string\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    clf = SVC(gamma='scale')\n    scoring = {'accuracy': make_scorer(accuracy_score), 'recall': make_scorer(recall_score)}\n\n    # Single metric evaluation with refit string\n    for refit in scoring:\n        grid_search = GridSearchCV(clf, param_grid={'C': [0.1, 1, 10]}, scoring=scoring, refit=refit)\n        grid_search.fit(X, y)\n        assert_equal(grid_search.refit, refit)\n        assert_equal(grid_search.best_estimator_.score(X, y), grid_search.best_score_)\n        assert_equal(sorted(grid_search.cv_results_.keys()),\n                     sorted(['mean_fit_time', 'mean_score_time', 'mean_test_accuracy', 'mean_test_recall',\n                             'mean_train_accuracy', 'mean_train_recall', 'param_C', 'params',\n                             'rank_test_accuracy', 'rank_test_recall', 'split0_test_accuracy',\n                             'split0_test_recall', 'split0_train_accuracy', 'split0_train_recall',\n                             'split1_test_accuracy', 'split1_test_recall', 'split1_train_accuracy',\n                             'split1_train_recall', 'split2_test_accuracy', 'split2_test_recall',\n                             'split2_train_accuracy', 'split2_train_recall', 'std_fit_time',\n                             'std_score_time', 'std_test_accuracy', 'std_test_recall',\n                             'std_train_accuracy', 'std_train_recall']))\n"], "sample_753": ["def test_liblinear_decision_function_non_zero():\n    # Test positive prediction when decision_function values are non-zero.\n    # Liblinear predicts the positive class when decision_function values\n    # are non-zero. This is a test to verify that we do the same.\n    X, y = make_classification(n_samples=5, n_features=5, random_state=0)\n    clf = LogisticRegression(fit_intercept=False)\n    clf.fit(X, y)\n\n    # Dummy data such that the decision function becomes positive.\n    X = np.ones((5, 5))\n    assert_array_equal(clf.predict(X), np.ones(5))\n", "def test_solver_options():\n    # Test that the correct error messages are raised for incorrect solver options\n    X, y = iris.data, iris.target\n\n    msg = ('Logistic Regression supports only liblinear, newton-cg, lbfgs, sag and saga solvers, got wrong_solver')\n    lr = LogisticRegression(solver=\"wrong_solver\")\n    assert_raise_message(ValueError, msg, lr.fit, X, y)\n\n    msg = \"multi_class should be either multinomial or ovr, got wrong_multi_class\"\n    lr = LogisticRegression(solver='newton-cg', multi_class=\"wrong_multi_class\")\n    assert_raise_message(ValueError, msg, lr.fit, X, y)\n\n    msg = \"Solver liblinear does not support a multinomial backend.\"\n    lr = LogisticRegression(solver='liblinear', multi_class='multinomial')\n    assert_raise_message(ValueError, msg, lr.fit, X, y)\n\n    for solver in ['newton-cg', 'lbfgs', 'sag']:\n        msg = (\"Solver %s supports only l2 penalties, got l1 penalty.\" % solver)\n        lr = LogisticRegression(solver=solver, penalty='l1')\n        assert_raise_message(ValueError, msg, lr.fit, X, y)\n\n    for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:\n        msg = (\"Solver %s supports only dual=False, got dual=True\" % solver)\n        lr = LogisticRegression(solver=solver, dual=True)\n        assert_raise_message(ValueError, msg, lr.fit, X, y)\n", "def test_logistic_regression_no_features(solver):\n    # Test that LogisticRegression raises an error when fitting with no features\n    X, y = make_classification(n_samples=10, n_features=0, random_state=0)\n    clf = LogisticRegression(solver=solver)\n    msg = \"This solver needs samples of at least 2 features\"\n    assert_raise_message(ValueError, msg, clf.fit, X, y)\n", "def test_logreg_l1_intercept():\n    # Test that L1 penalty with intercept is equivalent to L1 penalty without\n    # intercept and adding a feature vector of ones.\n    rng = np.random.RandomState(42)\n    n_samples = 50\n    X, y = make_classification(n_samples=n_samples, n_features=20,\n                               random_state=0)\n    X_noise = rng.normal(size=(n_samples, 3))\n    X = np.concatenate((X, X_noise), axis=1)\n\n    lr_liblinear_no_intercept = LogisticRegression(penalty=\"l1\", C=1.0,\n                                                   solver='liblinear',\n                                                   fit_intercept=False,\n                                                   tol=1e-10)\n    lr_liblinear_no_intercept.fit(X, y)\n\n    X_with_intercept = np.concatenate((X, np.ones((n_samples, 1))), axis=1)\n    lr_liblinear_with_intercept = LogisticRegression(penalty=\"l1\", C=1.0,\n                                                     solver='liblinear',\n                                                     fit_intercept=False,\n                                                     tol=1e-10)\n    lr_liblinear_with_intercept.fit(X_with_intercept, y)\n\n    assert_array_almost_equal(lr_liblinear_with_intercept.coef_[:, :-1],\n                              lr_liblinear_no_intercept.coef_)\n", "def test_logreg_intercept_scaling_positive():\n    # Test that intercept_scaling is correctly applied when fit_intercept is True\n\n    clf = LogisticRegression(fit_intercept=True, intercept_scaling=2.0)\n    clf.fit(X, Y1)\n    assert_equal(clf.intercept_, 2.0 * clf.coef_[:, -1])\n"], "sample_1207": ["def test_convert_xor():\n    transformations = standard_transformations + (convert_xor,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"x^y\", transformations=transformations) == x**y\n", "def test_implicit_application():\n    transformations = standard_transformations + \\\n                      (implicit_application,)\n    x = Symbol('x')\n    f = Function('f')\n    g = Function('g')\n    assert parse_expr(\"f(g x)\", transformations=transformations) == f(g(x))\n    assert parse_expr(\"f g x\", transformations=transformations) == f(g(x))\n", "def test_lambda_notation():\n    x = Symbol('x')\n    f = parse_expr('lambda x: x**2')\n    assert f(x) == x**2\n    raises(TokenError, lambda: parse_expr('lambda x: x**2*'))\n    raises(TokenError, lambda: parse_expr('lambda *x: x**2'))\n    raises(TokenError, lambda: parse_expr('lambda x*, y: x**2'))\n    raises(TokenError, lambda: parse_expr('lambda x, *y: x**2'))\n", "def test_lambda_notation():\n    x = Symbol('x')\n    f = parse_expr('lambda x: x**2')\n    g = Lambda((x,), x**2)\n    assert f == g\n    assert f(3) == g(3)\n    # Test that lambda notation is not applied if only \"lambda\" is passed\n    assert parse_expr('lambda') == Symbol('lambda')\n    # Test that starred arguments raise a TokenError\n    raises(TokenError, lambda: parse_expr('lambda *args: args'))\n", "def test_lambda_notation():\n    x = Symbol('x')\n    inputs = {\n        'lambda x: x + 1': Lambda((x,), x + 1),\n        'lambda x, y: x + y': Lambda((x, y), x + y),\n    }\n    transformations = standard_transformations + (lambda_notation,)\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n\n    raises(TokenError, lambda: parse_expr('lambda x, *args: x + args[0]', transformations=transformations))\n    raises(TokenError, lambda: parse_expr('lambda x, **kwargs: x + kwargs[\"y\"]', transformations=transformations))\n"], "sample_761": ["def test_iterative_imputer_max_iter_zero():\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n\n    imputer = IterativeImputer(max_iter=0)\n    X_imputed = imputer.fit_transform(X)\n    # with max_iter=0, only initial imputation is performed\n    assert_allclose(X_imputed, imputer.initial_imputer_.transform(X))\n", "def test_iterative_imputer_invalid_imputation_order(imputation_order):\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n\n    imputer = IterativeImputer(missing_values=0,\n                               max_iter=2,\n                               imputation_order=imputation_order,\n                               random_state=rng)\n    with pytest.raises(ValueError, match=\"Got an invalid imputation order: '{}'\".format(imputation_order)):\n        imputer.fit_transform(X)\n", "def test_iterative_imputer_all_values_missing():\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = np.full((n, d), np.nan)\n\n    imputer = IterativeImputer(max_iter=1, random_state=rng)\n    X_imputed = imputer.fit_transform(X)\n    # when all values are missing, imputed values should be zero\n    assert_allclose(X_imputed, np.zeros((n, d)))\n", "def test_imputation_most_frequent_integer(dtype):\n    # Test imputation using the most-frequent strategy on integers\n    X = np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ], dtype=dtype)\n\n    X_true = np.array([\n        [2, 0, 5],\n        [2, 3, 3],\n        [1, 3, 3],\n        [2, 3, 7],\n    ], dtype=dtype)\n\n    imputer = SimpleImputer(missing_values=-1, strategy=\"most_frequent\")\n    X_trans = imputer.fit_transform(X)\n\n    assert_array_equal(X_trans, X_true)\n", "def test_iterative_imputer_copy(array_constructor):\n    # Test iterative imputer with copy\n    X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n\n    # copy=True\n    X = array_constructor(X_orig.copy())\n    imputer = IterativeImputer(missing_values=0, max_iter=1, copy=True)\n    Xt = imputer.fit(X).transform(X)\n    Xt[0, 0] = -1\n    assert not np.all(X == Xt)\n\n    # copy=False\n    X = array_constructor(X_orig.copy())\n    imputer = IterativeImputer(missing_values=0, max_iter=1, copy=False)\n    Xt = imputer.fit(X).transform(X)\n    Xt[0, 0] = -1\n    assert_allclose(X, Xt)\n"], "sample_675": ["def test_logging_emit_error_with_log_level(testdir: Testdir) -> None:\n    \"\"\"\n    An exception raised during emit() should fail the test even when log level is set.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.basicConfig(level=logging.INFO)\n            logging.warning('oops', 'first', 2)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n    result.stdout.fnmatch_lines(\n        [\n            \"====* FAILURES *====\",\n            \"*not all arguments converted during string formatting*\",\n        ]\n    )\n", "def test_logging_capture_exception(testdir: Testdir) -> None:\n    \"\"\"\n    Test that logging.exception() captures the traceback.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            try:\n                raise ValueError(\"Test error\")\n            except ValueError:\n                logging.exception(\"Exception occurred\")\n\n            assert \"Traceback (most recent call last):\" in caplog.text\n            assert \"ValueError: Test error\" in caplog.text\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_file_absolute_path(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import os\n\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n\n            abs_log_file = os.path.join(os.path.dirname(__file__), 'abs_pytest.log')\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            plugin.set_log_path(abs_log_file)\n            logging.getLogger('catchlog').info(\"This log message will be shown in absolute path\")\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"This log message will be shown\" in contents\n\n    abs_log_file = os.path.join(testdir.tmpdir.strpath, 'abs_pytest.log')\n    assert os.path.isfile(abs_log_file)\n    with open(abs_log_file) as rfh:\n        contents = rfh.read()\n        assert \"This log message will be shown in absolute path\" in contents\n", "def test_log_format_options(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.formatter._fmt == \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"\n            assert plugin.formatter.datefmt == \"%H:%M:%S\"\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n        log_date_format = \"%Y-%m-%d %H:%M:%S\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    result.stdout.fnmatch_lines([\"*test_log_format_options*100%*\", \"=* 1 passed in *=\"])\n", "def test_log_cli_handler_set_when(testdir):\n    \"\"\"\n    Test that the log_cli_handler's set_when() method updates the handler's current phase.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=INFO\")\n    plugin = result.parseconfigurehooks()[0].pluginmanager.get_plugin(\"logging-plugin\")\n    assert plugin.log_cli_handler._when is None\n    plugin.log_cli_handler.set_when(\"start\")\n    assert plugin.log_cli_handler._when == \"start\"\n    plugin.log_cli_handler.set_when(\"setup\")\n    assert plugin.log_cli_handler._when == \"setup\"\n    plugin.log_cli_handler.set_when(\"call\")\n    assert plugin.log_cli_handler._when == \"call\"\n    plugin.log_cli_handler.set_when(\"teardown\")\n    assert plugin.log_cli_handler._when == \"teardown\"\n    plugin.log_cli_handler.set_when(\"finish\")\n    assert plugin.log_cli_handler._when == \"finish\"\n"], "sample_701": ["def test_argparse_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption('--foo', type='str', choices=['a', 'b'])\n        \"\"\"\n    )\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"`type` argument to addoption() is the string 'str'\"):\n        pytester.parseconfig()\n", "def test_argument_type_str_choice_is_deprecated():\n    with pytest.warns(PytestDeprecationWarning, match=r\"type argument to addoption\\(\\) is the string 'str'. For choices this is optional\"):\n        parser = pytest.Parser()\n        parser.addoption('--foo', type='str', choices=['a', 'b'])\n", "def test_argument_type_str_choice(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption('--str-choice', type='str', choices=['a', 'b'])\n            parser.addoption('--str-choice-ok', type=str, choices=['a', 'b'])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str'.*\",\n            \"*(options: --str-choice)*\",\n        ]\n    )\n", "def test_addoption_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup('custom options')\n            group.addoption('--custom-choice', type='str', choices=['choice1', 'choice2'], default='choice1')\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: The `type` argument to addoption() is the string 'str'.*\"]\n    )\n", "def test_arg_parse_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--myopt\", default=\"%default\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n        ]\n    )\n"], "sample_1061": ["def test_Float_floor_ceiling_power():\n    assert(Float('1.5').floor() == 1)\n    assert(Float('1.5').ceiling() == 2)\n    assert((Float('1.5'))**2 == 2.25)\n", "def test_Float_conjugate():\n    assert Float(1.0).conjugate() == Float(1.0)\n    assert Float(1.0 + 2.0*I).conjugate() == Float(1.0 - 2.0*I)\n    assert Float('1.0', precision=10).conjugate() == Float('1.0', precision=10)\n    assert Float('1.0 + 2.0*I', precision=10).conjugate() == Float('1.0 - 2.0*I', precision=10)\n", "def test_issue_10368():\n    a = S(32442016954)/78058255275\n    assert type(int(a)) is type(int(-a)) is int\n", "def test_Integer_factorial():\n    assert Integer(5).factorial() == 120\n    assert Integer(0).factorial() == 1\n    assert Integer(1).factorial() == 1\n    assert Integer(-5).factorial() == ValueError\n", "def test_issue_10063_complex():\n    assert (2 + 3j)**Float(3) == Float(21 - 44j)\n"], "sample_1133": ["def test_refraction_angle_total_internal_reflection():\n    m1 = Medium('m1', n=1.5)\n    m2 = Medium('m2', n=1)\n    assert refraction_angle(0.6, m1, m2) is None  # TIR\n    raises(ValueError, lambda: refraction_angle(0.6, m1, m2, normal=[0, 0, 1]))\n", "def test_refraction_angle_exceptions():\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    raises(ValueError, lambda: refraction_angle(1.5, 1, 1, n))\n    raises(ValueError, lambda: refraction_angle(r1, 1.33, 1, plane=None, normal=None))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1.33, normal=n, plane=Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, normal=\"invalid\"))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, plane=\"invalid\"))\n", "def test_refraction_angle_exceptions():\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    raises(ValueError, lambda: refraction_angle(1.2, 1.33, 1, normal=n))\n    raises(ValueError, lambda: refraction_angle(2, 1.33, 1, normal=n))\n    raises(ValueError, lambda: refraction_angle(-0.5, 1.33, 1, normal=n))\n    raises(ValueError, lambda: refraction_angle(0.5, 1.33, 1, plane=n))\n    raises(TypeError, lambda: refraction_angle(r1, 1.33, 1, normal=1.2))\n    raises(TypeError, lambda: refraction_angle(r1, 1.33, 1, plane='plane'))\n", "def test_refraction_angle_exceptions():\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    raises(ValueError, lambda: refraction_angle(-0.5, 1, 2, n))\n    raises(ValueError, lambda: refraction_angle(1.5, 1, 2, n))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 2, n, n))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 2, normal=n, plane=Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 2, normal=[0, 0, 0]))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 2, plane=0))\n", "def test_refraction_angle_exceptions():\n    n1, n2 = symbols('n1, n2')\n    m1 = Medium('m1', n=n1)\n    m2 = Medium('m2', n=n2)\n    normal = Matrix([0, 0, 1])\n    plane = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    incident = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n\n    # Test angle of incidence not in range [0:pi/2)\n    raises(ValueError, lambda: refraction_angle(pi/2, m1, m2, normal))\n    raises(ValueError, lambda: refraction_angle(-0.1, m1, m2, normal))\n\n    # Test normal and plane provided together\n    raises(ValueError, lambda: refraction_angle(incident, m1, m2, normal, plane))\n\n    # Test normal not concurrent with the incident ray\n    normal_non_concurrent = Ray3D(Point3D(1, 0, 0), Point3D(0, 1, 0))\n    raises(ValueError, lambda: refraction_angle(incident, m1, m2, normal_non_concurrent))\n\n    # Test plane not an instance of geometry.plane.Plane\n    raises(TypeError, lambda: refraction_angle(incident, m1, m2, plane=\"Not a plane\"))\n\n    # Test normal not a Matrix, Ray3D, or sequence\n    raises(TypeError, lambda: refraction_angle(incident, m1, m2, normal=\"Not a normal\"))\n"], "sample_252": ["def test_key_transform_with_expression(self):\n    expression = ExpressionWrapper(Value('bar'), output_field=CharField())\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expression)),\n        [self.objs[7]],\n    )\n", "def test_key_transform_exact(self):\n    self.assertIs(NullableJSONModel.objects.filter(value__d__0__exact='e').exists(), True)\n    self.assertIs(NullableJSONModel.objects.filter(value__d__1__exact={'f': 'g'}).exists(), True)\n", "def test_key_transform_exact(self):\n    tests = [\n        (KeyTransform('foo', 'value'), 'bar'),\n        (KeyTransform('baz', 'value'), {'a': 'b', 'c': 'd'}),\n        (KeyTransform('c', KeyTransform('baz', 'value')), 'd'),\n        (KeyTransform('1', KeyTransform('d', 'value')), {'f': 'g'}),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(value__exact=lookup),\n                [obj for obj in self.objs if obj.value == value],\n            )\n", "def test_key_transform_numeric_lookups(self):\n    tests = [\n        ('value__c__gt', 13, self.objs[3:5]),\n        ('value__c__gt', 14, []),\n        ('value__c__gte', 14, [self.objs[3], self.objs[4]]),\n        ('value__c__lt', 15, self.objs[3:5]),\n        ('value__c__lt', 14, []),\n        ('value__c__lte', 14, self.objs[3:5]),\n        ('value__c__lte', 13, []),\n    ]\n    for lookup, value, expected in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(**{lookup: value}),\n                expected,\n            )\n", "def test_key_transform_exact(self):\n    tests = [\n        ({'a': 'b'}, self.objs[3:5]),\n        ({'a': 'b', 'c': 14}, [self.objs[4]]),\n        ({'d': ['e', {'f': 'g'}]}, [self.objs[4]]),\n        ([1, [2]], [self.objs[5]]),\n        ({'n': [None]}, [self.objs[4]]),\n        ({'j': None}, [self.objs[4]]),\n    ]\n    for value, expected in tests:\n        with self.subTest(value=value):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(value__exact=value),\n                expected,\n            )\n"], "sample_357": ["def test_create_model_with_multiple_indexes(self):\n    author = ModelState('otherapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n        ('email', models.EmailField(max_length=254)),\n    ], {'indexes': [\n        models.Index(fields=['name'], name='author_name_idx'),\n        models.Index(fields=['email'], name='author_email_idx'),\n    ]})\n    changes = self.get_changes([], [author])\n    name_index = models.Index(fields=['name'], name='author_name_idx')\n    email_index = models.Index(fields=['email'], name='author_email_idx')\n    # Right number of migrations?\n    self.assertEqual(len(changes['otherapp']), 1)\n    # Right number of actions?\n    migration = changes['otherapp'][0]\n    self.assertEqual(len(migration.operations), 3)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AddIndex', 'AddIndex'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='author', index=name_index)\n    self.assertOperationAttributes(changes, 'otherapp', 0, 2, model_name='author', index=email_index)\n", "    def test_alter_model_options_ordering(self):\n        \"\"\"\n        Changing the model ordering adds a new operation.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_ordering])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, options={\"ordering\": [\"name\"]})\n", "def test_add_model_with_field_removed_from_base_model_before_other_models(self):\n    \"\"\"\n    Removing a base field takes place before adding other models that inherit\n    from the model with the removed field.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [], bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [], bases=('app.readable',)),\n        ModelState('app', 'magazine', [], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='magazine')\n", "def test_many_create_models_with_different_operations(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel('Person1', fields=[]),\n            migrations.CreateModel('Person2', fields=[]),\n            migrations.AddField('Person2', 'parent', models.ForeignKey('Person1', on_delete=models.CASCADE)),\n            migrations.DeleteModel('Person1'),\n        ]\n\n    migration = Migration('some_migration', 'test_app')\n    self.assertEqual(\n        migration.suggest_name(),\n        'person2_delete_person1',\n    )\n", "def test_alter_field_with_custom_deconstructible(self):\n    \"\"\"#24040 - AlterField should handle custom deconstructible instances.\"\"\"\n    class CustomDeconstructible:\n            self.value = value\n\n            return self.value == other.value\n\n            return hash(self.value)\n\n            return ('__main__.CustomDeconstructible', (self.value,), {})\n\n    author_before = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=CustomDeconstructible(\"Ada Lovelace\")))\n        ],\n    )\n    author_after = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=CustomDeconstructible(\"Grace Hopper\")))\n        ],\n    )\n    changes = self.get_changes([author_before], [author_after])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n    self.assertEqual(changes['testapp'][0].operations[0].field.default, CustomDeconstructible(\"Grace Hopper\"))\n"], "sample_266": ["def test_no_migrations_class(self):\n    with self.assertRaisesMessage(\n        BadMigrationError,\n        \"Migration 0001_initial in app migrations has no Migration class\",\n    ):\n        MigrationLoader(connection)\n", "def test_loading_with_replace_migrations_disabled(self):\n    \"\"\"\n    Tests loading with replace_migrations set to False.\n    \"\"\"\n    migration_loader = MigrationLoader(connection, replace_migrations=False)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n    # Loading with nothing applied should just give us the two nodes\n    self.assertEqual(\n        len([x for x in migration_loader.graph.nodes if x[0] == \"migrations\"]),\n        2,\n    )\n    # Even after fake-apply one migration, it should still use the two nodes\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    migration_loader.build_graph()\n    self.assertEqual(\n        len([x for x in migration_loader.graph.nodes if x[0] == \"migrations\"]),\n        2,\n    )\n", "def test_loading_replacement_cycle(self):\n    \"\"\"Tests loading a replacement cycle raises a CycleError\"\"\"\n    with self.assertRaises(MigrationGraph.CycleError):\n        MigrationLoader(connection)\n", "def test_loading_squashed_partial(self):\n    \"Tests loading a partially applied squashed migration\"\n\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Loading with one migration applied should still give us the one node\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    loader.build_graph()\n    self.assertEqual(\n        len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n        1,\n    )\n\n    # However, applying the second migration should replace the squashed node\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    loader.build_graph()\n    self.assertEqual(\n        len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n        2,\n    )\n", "def test_loading_squashed_partial(self):\n    \"\"\"Tests loading a partially applied squashed migration.\"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Empty database: use squashed migration\n    loader.build_graph()\n    self.assertEqual(len(loader.graph.nodes), 3)\n\n    # Partially apply squashed migration\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    loader.build_graph()\n    self.assertEqual(len(loader.graph.nodes), 4)\n\n    # Complete the squashed migration\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    loader.build_graph()\n    self.assertEqual(len(loader.graph.nodes), 3)\n\n    # Apply the remaining migration\n    recorder.record_applied(\"migrations\", \"0003_third\")\n    loader.build_graph()\n    self.assertEqual(len(loader.graph.nodes), 1)\n"], "sample_687": ["def test_caplog_records_clear(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"info message\")\n    assert len(caplog.records) == 1\n    caplog.clear()\n    assert len(caplog.records) == 0\n", "def test_log_level_int_or_str(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n            logger = logging.getLogger('catchlog')\n            logger.info(\"INFO message will be shown\")\n            logger.debug(\"DEBUG message won't be shown\")\n            assert 'INFO' in caplog.text\n            assert 'DEBUG' not in caplog.text\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level='INFO'\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_log_file_handler(testdir, tmpdir):\n    log_file = str(tmpdir.join(\"test.log\"))\n    testdir.makeini(\n        f\"\"\"\n        [pytest]\n        log_file={log_file}\n        log_file_level=DEBUG\n        log_file_format=%(asctime)s %(levelname)s %(message)s\n        log_file_date_format=%Y-%m-%d %H:%M:%S\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logger = logging.getLogger('test_log_file_handler')\n            logger.debug(\"DEBUG message\")\n            logger.info(\"INFO message\")\n            logger.warning(\"WARNING message\")\n            logger.error(\"ERROR message\")\n            logger.critical(\"CRITICAL message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n\n    with open(log_file) as f:\n        log_content = f.read()\n        assert \"DEBUG message\" in log_content\n        assert \"INFO message\" in log_content\n        assert \"WARNING message\" in log_content\n        assert \"ERROR message\" in log_content\n        assert \"CRITICAL message\" in log_content\n", "def test_log_file_enabled(testdir, tmpdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger('test_logger')\n            logger.info('Test message')\n            assert 'Test message' in caplog.text\n    \"\"\"\n    )\n    log_file = tmpdir.join('test.log')\n    testdir.makeini(\n        f\"\"\"\n        [pytest]\n        log_file = {log_file}\n        log_file_level = INFO\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n    assert log_file.read().strip() == 'INFO     test_logger:test_log_file.py:6 Test message'\n", "def test_caplog_captures_with_exception_in_setup(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n        @pytest.fixture\n            logger = logging.getLogger('error_in_setup')\n            caplog.set_level(logging.INFO)\n            logger.info(\"Setup log before error\")\n            raise Exception(\"Setup failed\")\n\n            logger = logging.getLogger('test_exception_in_setup')\n            logger.info(\"This log should not be captured\")\n            assert \"Setup log before error\" in caplog.text\n            assert \"This log should not be captured\" not in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*Setup failed*\"])\n    assert result.ret == 1\n"], "sample_274": ["def test_file_field_error_messages(self):\n    e = {\n        'required': 'REQUIRED',\n        'invalid': 'INVALID FILE',\n        'max_length': 'FILE NAME IS TOO LONG',\n    }\n    f = FileField(max_length=10, error_messages=e)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID FILE'], f.clean, 'abc')\n    self.assertFormErrors(['FILE NAME IS TOO LONG'], f.clean, SimpleUploadedFile('this_file_name_is_too_long.txt', b''))\n", "def test_modelchoicefield_limit_choices_to(self):\n    # Create choices for the model choice field tests below.\n    ChoiceModel.objects.create(pk=1, name='a')\n    ChoiceModel.objects.create(pk=2, name='b')\n    ChoiceModel.objects.create(pk=3, name='c')\n\n    # ModelChoiceField with limit_choices_to\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': 'INVALID CHOICE',\n    }\n    f = ModelChoiceField(queryset=ChoiceModel.objects.all(), limit_choices_to={'pk__in': [1, 2]}, error_messages=e)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID CHOICE'], f.clean, '3')\n", "def test_modelform_factory(self):\n    # Test modelform_factory function\n    from django import forms\n    from .models import ChoiceModel\n\n    class ChoiceModelForm(forms.ModelForm):\n        class Meta:\n            model = ChoiceModel\n            fields = ['name']\n\n    Form = modelform_factory(ChoiceModel, form=ChoiceModelForm)\n    self.assertTrue(issubclass(Form, ChoiceModelForm))\n    self.assertEqual(Form._meta.model, ChoiceModel)\n    self.assertEqual(Form._meta.fields, ['name'])\n\n    # Test with custom widget\n    widgets = {'name': forms.TextInput(attrs={'class': 'custom-input'})}\n    Form = modelform_factory(ChoiceModel, form=ChoiceModelForm, widgets=widgets)\n    self.assertEqual(Form.base_fields['name'].widget.attrs['class'], 'custom-input')\n\n    # Test with custom form field\n    class CustomChoiceField(forms.ChoiceField):\n        pass\n\n    Form = modelform_factory(ChoiceModel, form=ChoiceModelForm, field_classes={'name': CustomChoiceField})\n    self.assertTrue(isinstance(Form.base_fields['name'], CustomChoiceField))\n", "def test_filefield_max_length(self):\n    e = {\n        'max_length': 'FILE NAME TOO LONG',\n    }\n    f = FileField(error_messages=e, max_length=5)\n    self.assertFormErrors(['FILE NAME TOO LONG'], f.clean, SimpleUploadedFile('name_too_long', 'content'))\n", "    def test_invalid_pk_value(self):\n        # Create choices for the test\n        ChoiceModel.objects.create(pk='abc', name='a')\n\n        # ModelChoiceField\n        e = {\n            'required': 'REQUIRED',\n            'invalid_pk_value': '\u201c%(pk)s\u201d is not a valid primary key.',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['\u201c123\u201d is not a valid primary key.'], f.clean, '123')\n\n        # ModelMultipleChoiceField\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['\u201c123\u201d is not a valid primary key.'], f.clean, ['123'])\n"], "sample_616": ["def test_polyval_missing_degree_dim():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"coefficient\")\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs, degree_dim=\"degree\")\n", "def test_polyval_invalid_degree_dim(use_dask):\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"invalid_dim\")\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n        coeffs = coeffs.chunk({\"invalid_dim\": 2})\n        x = x.chunk({\"x\": 2})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs)\n", "def test_polyval_degree_dim_name(use_dask) -> None:\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"mydegree\", coords={\"mydegree\": [0, 1, 2]})\n    expected = xr.DataArray([9, 2 + 6 + 16, 2 + 9 + 36], dims=\"x\")\n    if use_dask:\n        coeffs = coeffs.chunk({\"mydegree\": 2})\n        x = x.chunk({\"x\": 2})\n    with raise_if_dask_computes():\n        actual = xr.polyval(x, coeffs, degree_dim=\"mydegree\")\n    xr.testing.assert_allclose(actual, expected)\n", "def test_polyval_broadcast_dataset(use_dask) -> None:\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.Dataset(\n        {\"a\": (\"degree\", [0, 1]), \"b\": (\"degree\", [1, 0])},\n        coords={\"degree\": [0, 1]},\n    )\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [1, 1, 1])})\n    if use_dask:\n        coeffs = coeffs.chunk({\"degree\": 2})\n        x = x.chunk({\"x\": 2})\n    with raise_if_dask_computes():\n        actual = xr.polyval(x, coeffs)\n    xr.testing.assert_allclose(actual, expected)\n", "def test_polyval_broadcast_coeffs(x, coeffs, expected) -> None:\n    with raise_if_dask_computes():\n        actual = xr.polyval(x, coeffs)\n    xr.testing.assert_allclose(actual, expected)\n"], "sample_758": ["def test_retrieve_samples_from_non_standard_shape_with_num_samples():\n    X = TestNonNumericShape()\n    assert_raises_regex(TypeError, \"Expected sequence or array-like, got <class '.*TestNonNumericShape'>\", _num_samples, X)\n", "def test_retrieve_samples_from_non_standard_shape():\n    shape = (10, 'unknown')\n    X = np.ones(shape)\n    assert_raises_regex(\n        TypeError,\n        \"Expected sequence or array-like, got <class 'str'>\",\n        _num_samples,\n        X\n    )\n", "def test_num_samples():\n    # Test _num_samples function with various inputs\n    assert_equal(_num_samples([1, 2, 3]), 3)\n    assert_equal(_num_samples(np.array([[1, 2], [3, 4]])), 2)\n    assert_equal(_num_samples(sp.csr_matrix([[1, 2], [3, 4]])), 2)\n    assert_raise_message(TypeError, \"Expected sequence or array-like\",\n                         _num_samples, \"string\")\n    assert_raise_message(TypeError, \"Singleton array\",\n                         _num_samples, np.array(42))\n    assert_raise_message(TypeError, \"Expected sequence or array-like\",\n                         _num_samples, KNeighborsClassifier())\n", "def test_check_X_y_multilabel_y_numeric():\n    X = np.ones((2, 2))\n    y = np.array([[0, 1], [1, 0]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True, y_numeric=True)\n    assert_equal(y_checked.dtype, np.float64)\n", "def test_retrieve_samples_from_non_standard_shape_raises_error():\n    X = TestNonNumericShape()\n    assert_raises_regex(TypeError, \"Expected sequence or array-like, got <class 'test_retrieve_samples_from_non_standard_shape.<locals>.TestNonNumericShape'>\", _num_samples, X)\n"], "sample_122": ["def test_cache_key_with_unicode_prefix(self):\n    prefix = 'cach\u00e9_pr\u00e9fix'\n    request = self.factory.get(self.path)\n    response = HttpResponse()\n    learn_cache_key(request, response, key_prefix=prefix)\n    self.assertIn(prefix, get_cache_key(request, key_prefix=prefix))\n", "def test_cache_control_with_max_age(self):\n    view_with_max_age_cache = cache_page(3)(cache_control(max_age=60)(hello_world_view))\n    request = self.factory.get('/view/')\n    response = view_with_max_age_cache(request, '1')\n    self.assertEqual(response.content, b'Hello World 1')\n    self.assertIn('max-age=60', response['Cache-Control'])\n    response = view_with_max_age_cache(request, '2')\n    self.assertEqual(response.content, b'Hello World 1')\n", "def test_get_with_invalid_key_warning(self):\n    with self.assertWarns(CacheKeyWarning):\n        cache.get('key with spaces')\n", "def test_cache_read_for_model_instance_with_deferred_lazy_loading(self):\n    # Don't want fields with callable as default to be called on cache read when lazy loading is enabled\n    expensive_calculation.num_runs = 0\n    Poll.objects.all().delete()\n    my_poll = Poll.objects.create(question=\"Why?\")\n    self.assertEqual(Poll.objects.count(), 1)\n    pub_date = my_poll.pub_date\n    cache.set('question', my_poll)\n\n    # Simulate lazy loading by closing the connection\n    connection.close()\n\n    cached_poll = cache.get('question')\n    self.assertEqual(cached_poll.pub_date, pub_date)\n    # We only want the default expensive calculation run once\n    self.assertEqual(expensive_calculation.num_runs, 1)\n", "def test_custom_key_func_and_key_prefix(self):\n    # Test custom key_func with key_prefix\n    key_prefix = 'test_prefix'\n    cache_params = {\n        'BACKEND': self.base_params['BACKEND'],\n        'LOCATION': '127.0.0.1:11211',\n        'KEY_FUNCTION': custom_key_func,\n        'KEY_PREFIX': key_prefix,\n    }\n    with self.settings(CACHES={'default': cache_params}):\n        cache = caches['default']\n        cache.set('answer', 42)\n        expected_key = custom_key_func('answer', key_prefix, 1)\n        self.assertEqual(cache.get('answer'), 42)\n        self.assertEqual(cache._cache.get(expected_key), 42)\n"], "sample_1012": ["def test_PythonCodePrinter_precision():\n    p = PythonCodePrinter({'precision': 5})\n    assert p.doprint(pi) == '3.14159'\n", "def test_PythonCodePrinter_complex_infinity():\n    p = PythonCodePrinter()\n    assert p.doprint(zoo) == 'float(\\'nan\\')'\n    assert p.doprint(oo) == 'float(\\'inf\\')'\n    assert p.doprint(-oo) == 'float(\\'-inf\\')'\n", "def test_Piecewise_with_None_default():\n    p = PythonCodePrinter()\n    expr = Piecewise((1, x > 0), (None, True))\n    assert p.doprint(expr) == '(1 if (x > 0) else None)'\n", "def test_PythonCodePrinter_custom_functions():\n    prntr = PythonCodePrinter({'user_functions': {'custom_func': 'custom_module.custom_func'}})\n    expr = custom_func(x, y)\n    assert prntr.doprint(expr) == 'custom_module.custom_func(x, y)'\n    assert prntr.module_imports == {'custom_module': {'custom_func'}}\n", "def test_print_sympy_function():\n    from sympy import Function, symbols\n    x = symbols('x')\n    f = Function('f')\n    assert pycode(f(x)) == 'f(x)'\n"], "sample_696": ["def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n        ]\n    )\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    plugin_path = pytester.makepyfile(\n        \"\"\"\n        import pytest\n            parser.addoption(\n                \"--string-choice\",\n                type=\"str\",\n                choices=[\"choice1\", \"choice2\"],\n            )\n        \"\"\"\n    )\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"`type` argument to addoption()\"):\n        pytester.runpytest_subprocess(\"--string-choice=choice1\", plugins=[str(plugin_path)])\n\n    plugin_path = pytester.makepyfile(\n        \"\"\"\n        import pytest\n            parser.addoption(\n                \"--string-only\",\n                type=\"str\",\n            )\n        \"\"\"\n    )\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"`type` argument to addoption()\"):\n        pytester.runpytest_subprocess(\"--string-only=test\", plugins=[str(plugin_path)])\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    plugin_name = \"my_plugin\"\n    pytester.makeconftest(\n        f\"\"\"\n        import pytest\n            parser.addoption('--myopt', type='str', choices=['foo', 'bar'])\n        \"\"\"\n    )\n    with pytest.warns(pytest.PytestDeprecationWarning, match=r\"type argument.*choices.*str\"):\n        pytester.config.getoption(\"myopt\")\n\n    plugin = pytester.importpluginyaml(plugin_name, \"\"\"\n        pytest_addoption:\n            myopt:\n                type: str\n                choices:\n                    - foo\n                    - bar\n        \"\"\")\n    with pytest.warns(pytest.PytestDeprecationWarning, match=r\"type argument.*choices.*str\"):\n        pytester.config.getoption(\"myopt\")\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-W\", \"error::UserWarning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n        ]\n    )\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption(\n                '--my-option',\n                type='str',\n                choices=['choice1', 'choice2'],\n                help='My option'\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str'.*choices*\",\n        ]\n    )\n"], "sample_689": ["def test_strict_option_is_deprecated(pytester: Pytester) -> None:\n    result = pytester.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The --strict option is deprecated, use --strict-markers instead.*\"])\n", "def test_strict_option_is_deprecated(testdir: Pytester) -> None:\n    result = testdir.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The --strict option is deprecated, use --strict-markers instead.*\"])\n", "def test_strict_option_is_deprecated(testdir: Pytester) -> None:\n    result = testdir.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The --strict option is deprecated*\"])\n", "def test_strict_option_is_deprecated(pytester: Pytester) -> None:\n    result = pytester.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The --strict option is deprecated*use --strict-markers*\"])\n", "def test_strict_option_is_deprecated(testdir: Testdir) -> None:\n    result = testdir.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The --strict option is deprecated*\"])\n"], "sample_311": ["def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username='staff',\n        password='secret',\n        email='staff@example.com',\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse('admin10:admin_views_article_changelist')\n    response = self.client.get(known_url[:-1], SCRIPT_NAME='/prefix/')\n    self.assertRedirects(\n        response,\n        '/prefix' + known_url,\n        status_code=301,\n        fetch_redirect_response=False,\n    )\n", "def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username='staff',\n        password='secret',\n        email='staff@example.com',\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse('admin10:admin_views_article_changelist')\n    response = self.client.get(known_url[:-1], SCRIPT_NAME='/prefix/')\n    self.assertRedirects(\n        response,\n        '/prefix' + known_url,\n        status_code=301,\n        fetch_redirect_response=False,\n    )\n", "    def test_unknown_url_redirects_login_if_not_authenticated_with_query_string(self):\n        unknown_url = '/test_admin/admin/unknown/?q=test'\n        response = self.client.get(unknown_url)\n        self.assertRedirects(response, '%s?next=%s' % (reverse('admin:login'), unknown_url))\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n\n        cls.c4 = City.objects.create(state=cls.s1, name='New-York')\n        cls.r7 = Restaurant.objects.create(city=cls.c4, name='New Pizza')\n        cls.w4 = Worker.objects.create(work_at=cls.r7, name='Mario', surname='Rossi-Bianchi')\n", "def test_view_on_site_exception(self):\n    \"\"\"\n    An exception in view_on_site should return a 500 response.\n    \"\"\"\n    class ViewOnSiteException(Exception):\n        pass\n\n        raise ViewOnSiteException(\"Test exception\")\n\n    model_admin = ModelAdmin(Worker, None)\n    model_admin.get_view_on_site_url = view_on_site_raise_exception\n\n    with self.assertRaises(ViewOnSiteException):\n        model_admin.get_view_on_site_url(Worker())\n"], "sample_730": ["def test_enet_max_iter():\n    # Test that an error message is raised if an estimator does not\n    # converge within the maximum number of iterations\n    msg = \"Objective did not converge\"\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    assert_warns_message(ConvergenceWarning, msg, ElasticNet(\n        alpha=0.1, max_iter=1, random_state=42).fit, X, y)\n    assert_warns_message(ConvergenceWarning, msg, MultiTaskElasticNet(\n        alpha=0.1, max_iter=1, random_state=42).fit, X, y[:, None])\n", "def test_enet_negative_alpha():\n    # Test that an error message is raised if an estimator is called with a negative alpha\n    msg = \"Penalty term must be positive; got (alpha=%r)\"\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    assert_raise_message(ValueError, msg % (-1), ElasticNet(alpha=-1).fit, X, y)\n    assert_raise_message(ValueError, msg % (-1), ElasticNetCV(alphas=[-1]).fit, X, y)\n    assert_raise_message(ValueError, msg % (-1), MultiTaskElasticNet(alpha=-1).fit, X, y[:, None])\n    assert_raise_message(ValueError, msg % (-1), MultiTaskElasticNetCV(alphas=[-1]).fit, X, y[:, None])\n\n    # Test that an error message is raised if an estimator is called with a negative l1_ratio\n    msg = \"l1_ratio must be between 0 and 1; got (l1_ratio=%r)\"\n    assert_raise_message(ValueError, msg % (-1), ElasticNet(l1_ratio=-1).fit, X, y)\n    assert_raise_message(ValueError, msg % (-1), ElasticNetCV(l1_ratio=[-1]).fit, X, y)\n    assert_raise_message(ValueError, msg % (-1), MultiTaskElasticNet(l1_ratio=-1).fit, X, y[:, None])\n    assert_raise_message(ValueError, msg % (-1), MultiTaskElasticNetCV(l1_ratio=[-1]).fit, X, y[:, None])\n", "def test_enet_multi_task_input_shape():\n    X, y, _, _ = build_dataset(n_features=10, n_targets=3)\n    clf = MultiTaskElasticNetCV(n_alphas=5, eps=2e-3, l1_ratio=[0.3, 0.5])\n    clf.fit(X, y)\n    assert_equal((3, X.shape[1]), clf.coef_.shape)\n    assert_equal((3, ), clf.intercept_.shape)\n\n    # Test with 1-dimensional y\n    y = y[:, 0]\n    assert_raises(ValueError, clf.fit, X, y)\n", "def test_enet_positive_with_l1_ratio():\n    # Test that positive parameter is allowed only for l1_ratio=1\n\n    X, Y, _, _ = build_dataset(n_samples=50, n_features=50, n_targets=2)\n\n    # For mono output\n    # Test that the coefs returned by positive=True in enet_path are positive\n    enet_positive = ElasticNet(positive=True, l1_ratio=1)\n    enet_positive.fit(X, Y[:, 0])\n    assert_true(np.all(enet_positive.coef_ >= 0))\n\n    # Test that an error is raised for l1_ratio!=1\n    enet_negative = ElasticNet(positive=True, l1_ratio=0.5)\n    assert_raises(ValueError, enet_negative.fit, X, Y[:, 0])\n\n    # For multi output, positive parameter is not allowed\n    # Test that an error is raised\n    multitask_enet_positive = MultiTaskElasticNet(positive=True, l1_ratio=1)\n    assert_raises(ValueError, multitask_enet_positive.fit, X, Y)\n", "def test_enet_precompute_array_shape():\n    # Test that an error message is raised if the shape of the precompute array is incorrect\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n    precompute = np.array([[1, 2, 3], [4, 5, 6]])\n    msg = (\"When precompute='True', the precomputed Gram matrix should be a square matrix\"\n           \" with shape (n_features, n_features). Got shape (2, 3) instead.\")\n    assert_raise_message(ValueError, msg, ElasticNet(precompute=precompute).fit, X, y)\n"], "sample_568": ["def test_surface3d_zsort_nan():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z[x.shape[0] // 2:, x.shape[1] // 2:] = np.nan\n\n    ax.plot_surface(x, y, z, cmap='jet')\n    ax.view_init(elev=45, azim=145)\n", "def test_surface3d_zsort_nan():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z[x.shape[0] // 2:, x.shape[1] // 2:] = np.nan\n\n    ax.plot_surface(x, y, z, cmap='jet')\n    ax.view_init(elev=45, azim=145)\n", "def test_surface3d_zsort_nan():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z[x.shape[0] // 2:, x.shape[1] // 2:] = np.nan\n\n    ax.plot_surface(x, y, z, cmap='jet')\n    ax.view_init(elev=45, azim=145)\n", "def test_scatter_masked_sizes():\n    \"\"\"\n    Test size parameter usage with non-finite coordinate arrays.\n\n    GH#26236\n    \"\"\"\n\n    x = [np.nan, 1, 2,  1]\n    y = [0, np.inf, 2,  1]\n    z = [0, 1, -np.inf, 1]\n    sizes = [10, 20, 30, 40]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    path3d = ax.scatter(x, y, z, s=sizes)\n\n    # Assert sizes' equality\n    assert len(path3d.get_offsets()) == len(path3d.get_sizes())\n", "def test_plot_surface_inf():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z[x.shape[0] // 2:, x.shape[1] // 2:] = np.inf\n\n    ax.plot_surface(x, y, z, cmap='jet')\n    ax.view_init(elev=45, azim=145)\n"], "sample_398": ["def test_logout_redirect_url_overrides_next_page(self):\n    self.login()\n    response = self.client.post(\"/logout/next_page/?next=/test/\")\n    self.assertRedirects(response, \"/test/\", fetch_redirect_response=False)\n    self.confirm_logged_out()\n", "def test_login_redirect_url_setting_overrides_next_page(self):\n    response = self.login(url=\"/login/next_page/\")\n    self.assertRedirects(response, \"/custom/\", fetch_redirect_response=False)\n", "def test_logout_with_get(self):\n    self.login()\n    response = self.client.get(\"/logout/\")\n    # RemovedInDjango50Warning: When the deprecation ends, replace with\n    #   self.assertEqual(response.status_code, 405)\n    self.confirm_logged_out()\n", "    def test_password_change_with_custom_user(self):\n        u = CustomUser.custom_objects.get(email=\"staffmember@example.com\")\n        self.client.login(username=u.username, password=\"password\")\n        response = self.client.post(\n            \"/password_change/\",\n            {\n                \"old_password\": \"password\",\n                \"new_password1\": \"newpassword\",\n                \"new_password2\": \"newpassword\",\n            },\n        )\n        self.assertRedirects(response, \"/password_change/done/\", fetch_redirect_response=False)\n        self.client.logout()\n        self.client.login(username=u.username, password=\"newpassword\")\n", "def test_password_change_mismatched_passwords(self):\n    user_change_url = reverse(\n        \"auth_test_admin:auth_user_change\", args=(self.admin.pk,)\n    )\n    password_change_url = reverse(\n        \"auth_test_admin:auth_user_password_change\", args=(self.admin.pk,)\n    )\n\n    response = self.client.post(\n        password_change_url,\n        {\n            \"password1\": \"password1\",\n            \"password2\": \"password2\",\n        },\n    )\n    self.assertContains(response, \"The two password fields didn't match.\")\n    # Check the password has not been changed\n    u = User.objects.get(username=\"testclient\")\n    self.assertTrue(not u.check_password(\"password1\"))\n"], "sample_439": ["def test_custom_renderer_override(self):\n    class CustomRenderer(DjangoTemplates):\n        form_template_name = \"forms_tests/custom_form_snippet.html\"\n\n    class Person(Form):\n        first_name = CharField()\n\n    t = Template(\"{{ form }}\")\n    html = t.render(Context({\"form\": Person(renderer=CustomRenderer())}))\n    expected = \"\"\"\n    <div class=\"customFieldWrapper\"><label for=\"id_first_name\">First name:</label>\n    <input type=\"text\" name=\"first_name\" required id=\"id_first_name\"></div>\n    \"\"\"\n    self.assertHTMLEqual(html, expected)\n", "def test_form_prefix_and_id_for_label(self):\n    class MyForm(Form):\n        my_field = CharField()\n\n    form = MyForm(auto_id='my_id_%s', prefix='my_prefix')\n    bound_field = form['my_field']\n    self.assertEqual(bound_field.id_for_label, 'my_id_my_prefix-my_field')\n", "def test_custom_error_list_class(self):\n    class CustomErrorList(ErrorList):\n        error_class = \"custom-error-class\"\n\n    class CommentForm(Form):\n        name = CharField(max_length=50, required=False)\n        email = EmailField()\n        comment = CharField()\n\n    data = {\"email\": \"invalid\"}\n    f = CommentForm(data, auto_id=False, error_class=CustomErrorList)\n    self.assertHTMLEqual(\n        f.as_p(),\n        '<p>Name: <input type=\"text\" name=\"name\" maxlength=\"50\"></p>'\n        '<ul class=\"errorlist custom-error-class\">'\n        '<li>Enter a valid email address.</li></ul>'\n        '<p>Email: <input type=\"email\" name=\"email\" value=\"invalid\" required></p>'\n        '<ul class=\"errorlist custom-error-class\">'\n        '<li>This field is required.</li></ul>'\n        '<p>Comment: <input type=\"text\" name=\"comment\" required></p>',\n    )\n", "def test_splitdatetimefield_microseconds(self):\n    class EventForm(Form):\n        happened_at = SplitDateTimeField(widget=SplitHiddenDateTimeWidget)\n\n    now = datetime.datetime.now()\n    now_with_microseconds = now.replace(microsecond=123456)\n    form = EventForm(initial={'happened_at': now_with_microseconds}, auto_id=False)\n    self.assertHTMLEqual(\n        str(form),\n        '<input type=\"hidden\" name=\"happened_at_0\" value=\"{}\" id=\"id_happened_at_0\">'\n        '<input type=\"hidden\" name=\"happened_at_1\" value=\"{}\" id=\"id_happened_at_1\">'.format(\n            now.strftime('%Y-%m-%d'), now.strftime('%H:%M:%S')\n        )\n    )\n", "def test_field_with_custom_widget_template(self):\n    class CustomWidget(TextInput):\n        template_name = \"forms_tests/widget_test.html\"\n\n    class CustomField(CharField):\n        widget = CustomWidget\n\n    class CustomForm(Form):\n        custom_field = CustomField()\n\n    f = CustomForm()\n    self.assertHTMLEqual(\n        str(f[\"custom_field\"]),\n        '<input type=\"text\" name=\"custom_field\" required>'\n        '<span class=\"custom-widget-text\">Custom widget text</span>',\n    )\n"], "sample_690": ["def test_skip_with_allow_module_level_parameter(pytester: Pytester) -> None:\n    \"\"\"Verify that using pytest.skip() with allow_module_level=True is allowed.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.skip(\"skip_module_level\", allow_module_level=True)\n\n            assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines([\"*SKIP*skip_module_level\"])\n", "def test_xfail_mark_decorator(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason=\"Expected Failure\")\n            assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *XFAIL*\n        *Expected Failure*\n        *1 xfailed*\n    \"\"\"\n    )\n", "def test_skipif_class_markeval_namespace(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        class TestClass(object):\n            pytestmark = pytest.mark.skipif(\"config._hackxyz\")\n                pass\n    \"\"\"\n    )\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            return {\"config\": {\"_hackxyz\": True}}\n        \"\"\"\n    )\n    rec = pytester.inline_run()\n    rec.assertoutcome(skipped=1)\n", "def test_marked_xfail_with_boolean_reason(self, pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(True, reason=123)\n            assert 0\n        \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_xfail_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"reason argument must be a string\" in excinfo.value.msg\n", "def test_skipif_markeval_namespace_override(pytester: Pytester) -> None:\n    \"\"\"Keys defined by ``pytest_markeval_namespace()`` in nested plugins override top-level ones.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            return {\"arg\": \"top-level\"}\n        \"\"\"\n    )\n    pytester.syspathinsert(str(pytester.path.joinpath(\"plugin\")))\n    pytester.makeconftest(\n        \"plugin\",\n        \"\"\"\n        import pytest\n\n            return {\"arg\": \"plugin\"}\n        \"\"\",\n    )\n    p = pytester.makepyfile(\n        \"plugin/test_plugin.py\",\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"arg == 'plugin'\")\n            assert False\n        \"\"\",\n    )\n    p = pytester.makepyfile(\n        \"test_top_level.py\",\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"arg == 'top-level'\")\n            assert False\n        \"\"\",\n    )\n\n    reprec = pytester.inline_run(\"-vs\", \"--capture=no\")\n    reprec.assertoutcome(skipped=1)\n"], "sample_96": ["def test_actions_unique_with_different_names(self):\n        pass\n\n        pass\n\n    action1.__name__ = 'custom_action1'\n    action2.__name__ = 'custom_action2'\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_custom_permissions_defined_without_matching_has_method(self):\n        pass\n\n    custom_permission_action.allowed_permissions = ('custom',)\n\n    class BandAdmin(ModelAdmin):\n        actions = (custom_permission_action,)\n            return True\n\n    self.assertIsValid(BandAdmin, Band)\n", "    def test_valid_case(self):\n        class TestModelAdmin(ModelAdmin):\n            save_on_top = True\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n", "def test_actions_unique_with_different_names(self):\n        action1.__name__ = 'action1'\n        pass\n\n        action2.__name__ = 'action2'\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "    def has_custom_permission(self, request):\n        return True\n"], "sample_304": ["def test_email_validator_with_allowlist(self):\n    validator = EmailValidator(allowlist=['example.com'])\n    self.assertIsNone(validator('test@example.com'))\n    with self.assertRaises(ValidationError):\n        validator('test@invalid.com')\n", "    def test_validate_comma_separated_integer_list_with_negative_numbers(self):\n        validator = int_list_validator(allow_negative=True)\n        self.assertIsNone(validator('-1,-2,3'))\n        self.assertIsNone(validator('1,-2,-12'))\n        with self.assertRaises(ValidationError):\n            validator('1,2,3')\n        with self.assertRaises(ValidationError):\n            validator('1.2.3')\n        with self.assertRaises(ValidationError):\n            validator('1.2.3\\n')\n", "compilation error", "compilation error", "compilation error"], "sample_1052": ["def test_c_with_printer():\n    #issue 13586\n    from sympy.printing.ccode import C99CodePrinter\n    class CustomPrinter(C99CodePrinter):\n            return \"fastpow({}, {})\".format(self._print(expr.base),\n                                            self._print(expr.exp))\n\n    x, y = symbols('x y')\n    expr = x**y\n\n    gen = C99CodeGen(printer=CustomPrinter())\n\n    expected = (\n        '#include \"expr.h\"\\n'\n        '#include <math.h>\\n'\n        'double expr(double x, double y) {\\n'\n        '   double expr_result;\\n'\n        '   expr_result = fastpow(x, y);\\n'\n        '   return expr_result;\\n'\n        '}\\n'\n    )\n\n    result = codegen(('expr', expr), header=False, empty=False, code_gen=gen)\n    source = result[0][1]\n    assert source == expected\n", "def test_c_with_printer():\n    #issue 13586\n    from sympy.printing.ccode import C99CodePrinter\n    class CustomPrinter(C99CodePrinter):\n            return \"fastpow({}, {})\".format(self._print(expr.base),\n                                            self._print(expr.exp))\n\n    x, y = symbols('x y')\n    expr = x**y\n\n    # use the custom printer\n    gen = C99CodeGen(printer=CustomPrinter())\n\n    expected = (\n        '#include \"expr.h\"\\n'\n        '#include <math.h>\\n'\n        'double expr(double x, double y) {\\n'\n        '   double expr_result;\\n'\n        '   expr_result = fastpow(x, y);\\n'\n        '   return expr_result;\\n'\n        '}\\n'\n    )\n\n    result = codegen(('expr', expr), header=False, empty=False, code_gen=gen)\n    source = result[0][1]\n    assert source == expected\n", "def test_custom_c_with_printer():\n    from sympy.printing.ccode import C99CodePrinter\n    class CustomPrinter(C99CodePrinter):\n            return \"fastpow({}, {})\".format(self._print(expr.base),\n                                            self._print(expr.exp))\n\n    x, y = symbols('x y')\n    expr = x**y\n\n    gen = C99CodeGen(printer=CustomPrinter())\n\n    expected = (\n        '#include \"expr.h\"\\n'\n        '#include <math.h>\\n'\n        'double expr(double x, double y) {\\n'\n        '   double expr_result;\\n'\n        '   expr_result = fastpow(x, y);\\n'\n        '   return expr_result;\\n'\n        '}\\n'\n    )\n\n    result = codegen(('expr', expr), header=False, empty=False, code_gen=gen)\n    source = result[0][1]\n    assert source == expected\n", "def test_custom_codegen_with_printer():\n    from sympy.printing.ccode import C99CodePrinter\n    class CustomPrinter(C99CodePrinter):\n            return \"fastpow({}, {})\".format(self._print(expr.base),\n                                            self._print(expr.exp))\n\n    x, y = symbols('x y')\n    expr = x**y\n\n    gen = C99CodeGen(printer=CustomPrinter())\n    gen.preprocessor_statements.append('#include \"fastpow.h\"')\n\n    expected = (\n        '#include \"expr.h\"\\n'\n        '#include \"fastpow.h\"\\n'\n        'double expr(double x, double y) {\\n'\n        '   double expr_result;\\n'\n        '   expr_result = fastpow(x, y);\\n'\n        '   return expr_result;\\n'\n        '}\\n'\n    )\n\n    result = codegen(('expr', expr), header=False, empty=False, code_gen=gen)\n    source = result[0][1]\n    assert source == expected\n", "def test_custom_printer_with_codegen():\n    from sympy.printing.ccode import C99CodePrinter\n    from sympy.functions.elementary.exponential import exp\n\n    class CustomPrinter(C99CodePrinter):\n            return \"fastpow({}, {})\".format(self._print(expr.base),\n                                            self._print(expr.exp))\n\n    printer = CustomPrinter()\n\n    x, y = symbols('x y')\n    expr = x**y\n\n    # replace math.h with a different header\n    gen = C99CodeGen(printer=printer,\n                     preprocessor_statements=['#include \"fastpow.h\"'])\n\n    expected = (\n        '#include \"expr.h\"\\n'\n        '#include \"fastpow.h\"\\n'\n        'double expr(double x, double y) {\\n'\n        '   double expr_result;\\n'\n        '   expr_result = fastpow(x, y);\\n'\n        '   return expr_result;\\n'\n        '}\\n'\n    )\n\n    result = codegen(('expr', expr), header=False, empty=False, code_gen=gen)\n    source = result[0][1]\n    assert source == expected\n"], "sample_197": ["def test_depth_zero(self):\n    \"\"\"\n    Depth of zero should raise a ValueError.\n    \"\"\"\n    with self.assertRaises(ValueError):\n        timesince(self.t, self.t + self.onehour, depth=0)\n", "def test_custom_time_strings(self):\n    \"\"\" Custom time strings are correctly used. \"\"\"\n    custom_time_strings = {\n        'year': '%d y',\n        'month': '%d m',\n        'week': '%d w',\n        'day': '%d d',\n        'hour': '%d h',\n        'minute': '%d min',\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), '1 min')\n    self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), '1 h')\n    self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1 d')\n    self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1 w')\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1 m')\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1 y')\n", "    def test_leap_year_boundary(self):\n        leap_year = datetime.date(2020, 2, 29)\n        non_leap_year = datetime.date(2021, 3, 1)\n        self.assertEqual(timesince(leap_year, non_leap_year), '1\\xa0day')\n        self.assertEqual(timesince(non_leap_year, leap_year), '1\\xa0day')\n", "def test_zero_depth(self):\n    \"\"\"\n    Depth cannot be zero.\n    \"\"\"\n    with self.assertRaises(ValueError):\n        timesince(self.t, self.t + self.oneyear, depth=0)\n", "def test_custom_time_strings(self):\n    \"\"\"\n    Custom time strings can be provided.\n    \"\"\"\n    custom_time_strings = {\n        'year': npgettext_lazy('custom', '%d yr', '%d yrs'),\n        'month': npgettext_lazy('custom', '%d mth', '%d mths'),\n        'week': npgettext_lazy('custom', '%d wk', '%d wks'),\n        'day': npgettext_lazy('custom', '%d dy', '%d dys'),\n        'hour': npgettext_lazy('custom', '%d hr', '%d hrs'),\n        'minute': npgettext_lazy('custom', '%d min', '%d mins'),\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), '1\\xa0min')\n    self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), '1\\xa0hr')\n    self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1\\xa0dy')\n    self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1\\xa0wk')\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0mth')\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0yr')\n"], "sample_365": ["def test_lazy_mod(self):\n    \"\"\"\n    % works correctly for Promises.\n    \"\"\"\n    lazy_obj = lazy(lambda: 'Hello, %s!', str)\n    self.assertEqual(lazy_obj() % 'world', 'Hello, world!')\n", "def test_lazy_pickling(self):\n    original_object = 'Lazy pickled text'\n    lazy_obj = lazy(lambda: original_object, str)\n    pickled_obj = pickle.dumps(lazy_obj())\n    unpickled_obj = pickle.loads(pickled_obj)\n    self.assertEqual(original_object, unpickled_obj)\n", "    def test_lazy_string_interpolation(self):\n        \"\"\"\n        String interpolation works correctly for Promises.\n        \"\"\"\n        lazy_a = lazy(lambda: 'World', str)\n        self.assertEqual('Hello, %s!' % lazy_a(), 'Hello, World!')\n", "def test_classproperty_no_getter(self):\n    \"\"\"\n    Test that classproperty raises AttributeError if getter is not defined.\n    \"\"\"\n    class Foo:\n        @classproperty\n            pass\n\n    with self.assertRaises(AttributeError):\n        Foo.foo\n", "    def test_lazy_text_formatting(self):\n        \"\"\"\n        String formatting works correctly for Promises.\n        \"\"\"\n        lazy_obj = lazy(lambda: 'world', str)\n        formatted_string = lazy(\"%s!\" % lazy_obj())\n        self.assertEqual(formatted_string, 'world!')\n"], "sample_183": ["def test_only_when_arguments_with_empty_when(self):\n    msg = 'An empty Q() can\\'t be used as a When() condition.'\n    with self.assertRaisesMessage(ValueError, msg):\n        Case(When(Q()))\n", "def test_invalid_when_constructor_kwargs(self):\n    msg = 'When() does not support keyword arguments other than condition and then.'\n    with self.assertRaisesMessage(TypeError, msg):\n        When(integer=1, then=2)\n", "def test_case_expression_with_empty_q(self):\n    expression = Case(\n        When(Q(pk__in=[]), then=Value('empty')),\n        default=Value('not empty'),\n        output_field=CharField(),\n    )\n    self.assertEqual(expression._resolve_output_field(), CharField())\n", "def test_invalid_when_constructor_kwargs(self):\n    msg = \"An empty Q() can't be used as a When() condition.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        When(Q())\n", "def test_invalid_when_constructor_kwargs(self):\n    msg = \"An empty Q() can't be used as a When() condition.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        When(Q())\n"], "sample_857": ["def test_prune_tree_classifier_monotonic_impurity(criterion, dataset, tree_cls):\n    dataset = DATASETS[dataset]\n    X, y = dataset[\"X\"], dataset[\"y\"]\n    est = tree_cls(max_leaf_nodes=20, random_state=0)\n    info = est.cost_complexity_pruning_path(X, y)\n\n    impurities = info.impurities\n    assert np.all(np.diff(impurities) >= 0)\n", "def test_prune_tree_raises_invalid_ccp_alpha():\n    clf = DecisionTreeClassifier()\n    msg = \"ccp_alpha values must be non-decreasing\"\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(ccp_alpha=[0.0, 1.0, 0.5])\n        clf.fit(X, y)\n\n    clf.set_params(ccp_alpha=0.0)\n    clf.fit(X, y)\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(ccp_alpha=[0.0, 1.0, 0.5])\n        clf._prune_tree()\n", "def test_prune_tree_with_ccp_alpha(name):\n    TreeEstimator = ALL_TREES[name]\n    est = TreeEstimator(random_state=0, max_leaf_nodes=3)\n    est.fit(iris.data, iris.target)\n\n    # prune the tree with ccp_alpha\n    est_pruned = TreeEstimator(random_state=0, ccp_alpha=0.01)\n    est_pruned.fit(iris.data, iris.target)\n\n    # check that the pruned tree is a subtree of the original tree\n    assert_is_subtree(est.tree_, est_pruned.tree_)\n", "def test_prune_tree_classifier_monotonic_impurity(criterion, dataset, tree_cls):\n    dataset = DATASETS[dataset]\n    X, y = dataset[\"X\"], dataset[\"y\"]\n    est = tree_cls(max_leaf_nodes=20, random_state=0)\n    info = est.cost_complexity_pruning_path(X, y)\n\n    impurities = info.impurities\n    assert np.all(np.diff(impurities) >= 0)\n", "def test_prune_tree_with_ccp_alpha_zero(tree_cls):\n    X, y = iris.data, iris.target\n    est = tree_cls(random_state=0)\n    est.fit(X, y)\n\n    est_pruned = tree_cls(ccp_alpha=0.0, random_state=0)\n    est_pruned.fit(X, y)\n\n    assert_tree_equal(est.tree_, est_pruned.tree_, \"Pruned tree differs from original tree with ccp_alpha=0\")\n"], "sample_1201": ["def test_electromagnetic_units_conversion():\n    assert convert_to(statvolt, volt, cgs_gauss) == volt/(2997924580*10**6)\n    assert convert_to(volt, statvolt, cgs_gauss) == 2997924580*10**6*statvolt\n    assert convert_to(statvolt, sqrt(erg*gram/centimeter), cgs_gauss) == sqrt(erg/centimeter)\n    assert convert_to(volt, sqrt(erg*gram/centimeter), cgs_gauss) == 2997924580*10**6*sqrt(erg/centimeter)\n\n    assert convert_to(ohm, statvolt/statampere, cgs_gauss) == 10**5*second/centimeter\n    assert convert_to(statvolt/statampere, ohm, cgs_gauss) == centimeter/10**5/second\n\n    assert convert_to(farad, second/ohm, cgs_gauss) == 10**5*centimeter\n    assert convert_to(second/ohm, farad, cgs_gauss) == centimeter/10**5\n\n    assert convert_to(henry, ohm*second, cgs_gauss) == 10**5*centimeter*second\n    assert convert_to(ohm*second, henry, cgs_gauss) == centimeter*second/10**5\n", "def test_conversion_of_electromagnetic_units():\n    assert convert_to(statvolt, volt, cgs_gauss) == volt*10**6/(2997924580*sqrt(4*pi*coulomb_constant))\n    assert convert_to(volt, statvolt, cgs_gauss) == statvolt*(2997924580*sqrt(4*pi*coulomb_constant))/10**6\n\n    assert convert_to(farad, henry, cgs_gauss) == henry*(2997924580**2*4*pi*coulomb_constant)/(10**5*centimeter)\n    assert convert_to(henry, farad, cgs_gauss) == farad*(10**5*centimeter)/(2997924580**2*4*pi*coulomb_constant)\n\n    assert convert_to(ohm, ohm, cgs_gauss) == ohm\n    assert convert_to(ohm, ohm, SI) == ohm\n\n    assert convert_to(speed_of_light, meter/second, cgs_gauss) == meter/second\n    assert convert_to(speed_of_light, meter/second, SI) == meter/second\n\n    assert convert_to(planck, joule*second, SI) == joule*second\n    assert convert_to(gravitational_constant, newton*meter**2/kg**2, SI) == newton*meter**2/kg**2\n    assert convert_to(elementary_charge, coulomb, SI) == coulomb\n", "def test_electromagnetic_conversions():\n    assert convert_to(statvolt, volt, cgs_gauss) == volt/2997924580*10**6\n    assert convert_to(volt, statvolt, cgs_gauss) == 2997924580*statvolt/10**6\n    assert convert_to(statvolt, sqrt(gram*centimeter)/second, cgs_gauss) == sqrt(erg/gram)\n    assert convert_to(volt, sqrt(gram*centimeter)/second, cgs_gauss) == 2997924580*sqrt(erg/gram)/10**6\n\n    assert convert_to(ohm, henry, cgs_gauss) == henry*2997924580**2*10**5/centimeter\n    assert convert_to(henry, ohm, cgs_gauss) == ohm/2997924580**2/10**5*centimeter\n    assert convert_to(farad, ohm, cgs_gauss) == ohm*centimeter/2997924580**2/10**5\n    assert convert_to(ohm, farad, cgs_gauss) == farad/centimeter*2997924580**2*10**5\n", "def test_conversion_to_from_cgs_gauss():\n    assert convert_to(statvolt, volt, cgs_gauss) == volt/(10**6/speed_of_light)\n    assert convert_to(volt, statvolt, cgs_gauss) == (10**6/speed_of_light)*statvolt\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == erg/statcoulomb\n    assert convert_to(ohm, statvolt/statampere, cgs_gauss) == 10**5/speed_of_light**2*second/centimeter\n    assert convert_to(farad, statcoulomb/statvolt, cgs_gauss) == 10**5*speed_of_light**2*centimeter\n    assert convert_to(henry, statvolt*second/statampere, cgs_gauss) == 10**5/speed_of_light**2/centimeter*second**2\n", "def test_coulomb_constant_conversion():\n    assert convert_to(coulomb_constant, 1, cgs_gauss) == 1\n    assert convert_to(1, coulomb_constant, cgs_gauss) == 1\n\n    # SI units have different base units, no conversion for coulomb_constant:\n    assert convert_to(coulomb_constant, 1, SI) == coulomb_constant\n    assert convert_to(1, coulomb_constant, SI) == 1\n\n    # Test conversion with other units\n    assert convert_to(statvolt*meter/statcoulomb, volt, cgs_gauss) == volt/2997924580\n    assert convert_to(farad, statcoulomb/(volt*second), cgs_gauss) == farad/2997924580**2\n    assert convert_to(henry, statvolt*second/statampere, cgs_gauss) == henry*2997924580**2\n    assert convert_to(ohm, statvolt/statampere, cgs_gauss) == ohm*2997924580\n"], "sample_614": ["def test_format_array_flat_max_width_limit() -> None:\n    array = np.arange(100)\n    max_width = 10\n    actual = formatting.format_array_flat(array, max_width)\n    assert len(actual) <= max_width\n", "def test_unindexed_dims_repr() -> None:\n    dims = [\"x\", \"y\", \"z\"]\n    coords = {\"x\": np.arange(10), \"y\": np.arange(20)}\n    expected = \"Dimensions without coordinates: z\"\n    assert formatting.unindexed_dims_repr(dims, coords) == expected\n\n    coords = {\"x\": np.arange(10), \"y\": np.arange(20), \"z\": np.arange(30)}\n    assert formatting.unindexed_dims_repr(dims, coords) is None\n\n    dims = [\"x\", \"y\", \"z\", \"w\"]\n    coords = {\"x\": np.arange(10), \"y\": np.arange(20)}\n    expected = \"Dimensions without coordinates: z, w\"\n    assert formatting.unindexed_dims_repr(dims, coords) == expected\n", "def test_unindexed_dims_repr() -> None:\n    dims = (\"x\", \"y\", \"z\")\n    coords = {\"x\": [1, 2], \"y\": [3, 4]}\n\n    expected = \"Dimensions without coordinates: z\"\n    actual = formatting.unindexed_dims_repr(dims, coords)\n    assert expected == actual\n\n    coords = {\"x\": [1, 2], \"y\": [3, 4], \"z\": [5, 6]}\n    expected = None\n    actual = formatting.unindexed_dims_repr(dims, coords)\n    assert expected == actual\n\n    coords = {\"x\": [1, 2]}\n    expected = \"Dimensions without coordinates: y, z\"\n    actual = formatting.unindexed_dims_repr(dims, coords)\n    assert expected == actual\n", "def test_inline_dask_repr():\n    import dask.array as da\n    array = da.zeros((10, 10), chunks=(5, 5))\n    actual = formatting.inline_dask_repr(array)\n    expected = \"dask.array<chunksize=(5, 5)>\"\n    assert actual == expected\n\n    array = da.zeros((10, 10), chunks=(5, 5), dtype=np.float64)\n    actual = formatting.inline_dask_repr(array)\n    expected = \"dask.array<chunksize=(5, 5), meta=np.ndarray>\"\n    assert actual == expected\n", "def test_unindexed_dims_repr() -> None:\n    ds = xr.Dataset(coords={\"foo\": [1, 2, 3], \"bar\": [1, 2, 3]})\n    ds[\"unindexed\"] = xr.DataArray([0], dims=\"unindexed_dim\")\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords, max_rows=1)\n    expected = \"Dimensions without coordinates: unindexed_dim\"\n    assert actual == expected\n\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords, max_rows=2)\n    expected = \"Dimensions without coordinates: unindexed_dim\"\n    assert actual == expected\n\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords, max_rows=3)\n    expected = \"Dimensions without coordinates: ... unindexed_dim\"\n    assert actual == expected\n\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    expected = \"Dimensions without coordinates: unindexed_dim\"\n    assert actual == expected\n\n    ds = xr.Dataset()\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    assert actual is None\n"], "sample_630": ["def test_get_annotation_annassign_no_value(assign, label):\n    \"\"\"AnnAssign without value\"\"\"\n    node = astroid.extract_node(assign)\n    got = get_annotation(node.annotation).name\n    assert isinstance(node, astroid.AnnAssign)\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_container_types(assign, label):\n    \"\"\"Test container types\"\"\"\n    node = astroid.extract_node(assign)\n    got = get_annotation(node.value).name\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_string_annotation(assign, expected):\n    \"\"\"Test string annotations\"\"\"\n    node = astroid.extract_node(assign)\n    got = get_annotation(node.value).name\n    assert isinstance(node, astroid.AnnAssign)\n    assert got == expected, f\"got {got} instead of {expected} for value {node}\"\n", "def test_get_annotation_list(assign, label):\n    \"\"\"List\"\"\"\n    node = astroid.extract_node(assign)\n    got = get_annotation(node.value).name\n    assert isinstance(node, astroid.AnnAssign)\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_arg(arg, label):\n    \"\"\"Arg\"\"\"\n    node = astroid.extract_node(f\"def func({arg}): ...\")\n    arg_node = node.args.args[0]\n    got = get_annotation(arg_node).name\n    assert isinstance(arg_node, astroid.Arg)\n    assert got == label, f\"got {got} instead of {label} for value {arg_node}\"\n"], "sample_1113": ["def test_block_index_out_of_bounds():\n    B = BlockMatrix([[A1, A2], [A3, A4]])\n    raises(IndexError, lambda: B[n + m, k + l])\n    raises(IndexError, lambda: B[n + m, k])\n    raises(IndexError, lambda: B[n, k + l])\n    raises(IndexError, lambda: B[n + m, k + l - 1])\n    raises(IndexError, lambda: B[n + m - 1, k + l])\n", "def test_block_index_out_of_range():\n    n = 3\n    A = BlockMatrix([[MatrixSymbol('A1', n, n), MatrixSymbol('A2', n, n)]])\n    raises(IndexError, lambda: A[0, 2*n])\n", "def test_block_matrix_operations():\n    A = BlockMatrix([[MatrixSymbol('A1', 2, 2), MatrixSymbol('A2', 2, 2)], [MatrixSymbol('A3', 2, 2), MatrixSymbol('A4', 2, 2)]])\n    B = BlockMatrix([[MatrixSymbol('B1', 2, 2), MatrixSymbol('B2', 2, 2)], [MatrixSymbol('B3', 2, 2), MatrixSymbol('B4', 2, 2)]])\n\n    # Test multiplication\n    C = A * B\n    assert isinstance(C, BlockMatrix)\n    assert C[0, 0] == A[0, 0] * B[0, 0] + A[0, 1] * B[1, 0]\n\n    # Test addition\n    D = A + B\n    assert isinstance(D, BlockMatrix)\n    assert D[0, 0] == A[0, 0] + B[0, 0]\n\n    # Test transpose\n    E = A.transpose()\n    assert isinstance(E, BlockMatrix)\n    assert E[0, 0] == A[0, 0].transpose()\n\n    # Test trace\n    F = A.trace()\n    assert F == A[0, 0].trace() + A[1, 1].trace()\n", "def test_block_matrix_multiplication():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    block_A = BlockMatrix([[A, B], [C, D]])\n    block_B = BlockMatrix([[A, C], [B, D]])\n\n    # Multiplication of two block matrices\n    result = block_A * block_B\n    expected_result = BlockMatrix([[A*A + B*C, A*C + B*D], [C*A + D*C, C*C + D*D]])\n    assert result.equals(expected_result)\n\n    # Multiplication of a block matrix with a regular matrix\n    result = block_A * A\n    expected_result = BlockMatrix([[A*A + B*C, A*B + B*D], [C*A + D*C, C*C + D*D]])\n    assert result.equals(expected_result)\n\n    # Multiplication of a regular matrix with a block matrix\n    result = A * block_A\n    expected_result = BlockMatrix([[A*A + A*B, A*C + A*D], [A*A + A*B, A*C + A*D]])\n    assert result.equals(expected_result)\n", "def test_block_index_large_symbolic():\n    n, m, k = symbols('n m k', integer=True, positive=True)\n    i, j = symbols('i j', integer=True, nonnegative=True)\n    A1 = MatrixSymbol('A1', n, n)\n    A2 = MatrixSymbol('A2', n, m)\n    A3 = MatrixSymbol('A3', n, k)\n    A4 = MatrixSymbol('A4', m, n)\n    A5 = MatrixSymbol('A5', m, m)\n    A6 = MatrixSymbol('A6', m, k)\n    A7 = MatrixSymbol('A7', k, n)\n    A8 = MatrixSymbol('A8', k, m)\n    A9 = MatrixSymbol('A9', k, k)\n    A = BlockMatrix([[A1, A2, A3], [A4, A5, A6], [A7, A8, A9]])\n    assert A[n + i, m + j] == MatrixElement(A, n + i, m + j)\n"], "sample_175": ["def test_fast_delete_with_generic_foreign_key(self):\n    delete_top = DeleteTop.objects.create()\n    generic_b1 = GenericB1.objects.create(generic_delete_top=delete_top)\n    generic_b2 = GenericB2.objects.create(generic_delete_top=delete_top)\n    GenericDeleteBottom.objects.create(generic_b1=generic_b1, generic_b2=generic_b2)\n    self.assertNumQueries(3, delete_top.delete)\n    self.assertFalse(DeleteTop.objects.exists())\n    self.assertFalse(GenericB1.objects.exists())\n    self.assertFalse(GenericB2.objects.exists())\n    self.assertFalse(GenericDeleteBottom.objects.exists())\n", "def test_fast_delete_cascade(self):\n    a = create_a('fast_delete_cascade')\n    # 1 for self delete, 1 for fast delete of cascaded \"r\" qs.\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(A.objects.exists())\n    self.assertFalse(R.objects.exists())\n", "    def test_fast_delete_with_keeping_parents(self):\n        child = RChild.objects.create()\n        parent_id = child.r_ptr_id\n        child_delete(keep_parents=True)\n        self.assertFalse(RChild.objects.filter(id=child.id).exists())\n        self.assertTrue(R.objects.filter(id=parent_id).exists())\n", "def test_fast_delete_reverse_fk_set(self):\n    a = Avatar.objects.create()\n    u = User.objects.create(avatar=a)\n    # 1 query to fast-delete the user\n    # 1 query to delete the avatar\n    # 1 query to update the avatar_id field on the user\n    self.assertNumQueries(3, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n", "def test_fast_delete_select_related(self):\n    a = Avatar.objects.create(desc='a')\n    User.objects.create(avatar=a)\n    # Avatar can be fast-deleted, but selecting related objects requires a query.\n    self.assertNumQueries(2, User.objects.select_related('avatar').filter(avatar__desc='a').delete)\n    self.assertEqual(User.objects.count(), 0)\n    self.assertEqual(Avatar.objects.count(), 1)\n"], "sample_864": ["def test_mean_shift_with_sparse_matrix():\n    # Test MeanShift algorithm with sparse matrix\n    X = sparse.csr_matrix(X)\n    ms = MeanShift(bandwidth=1.2)\n    labels = ms.fit(X).labels_\n    labels_unique = np.unique(labels)\n    n_clusters_ = len(labels_unique)\n    assert n_clusters_ == 3\n    assert labels_unique[0] == 0\n", "def test_mean_shift_with_sparse_matrix():\n    # Test MeanShift with sparse matrix\n    X = sparse.lil_matrix((1000, 1000))\n    ms = MeanShift(bandwidth=1.2)\n    msg = \"A sparse matrix was passed, but dense data is required.\"\n    assert_raise_message(TypeError, msg, ms.fit, X,)\n", "def test_meanshift_sparse_matrix():\n    # Test MeanShift algorithm with sparse matrix\n    X_sparse = sparse.csr_matrix(X)\n    ms = MeanShift(bandwidth=1.2)\n    labels = ms.fit_predict(X_sparse)\n    labels_sparse = ms.predict(X_sparse)\n    assert_array_equal(labels, labels_sparse)\n", "def test_mean_shift_with_sparse_matrix():\n    # Test MeanShift with sparse matrix\n    X = sparse.lil_matrix((100, 10))\n    ms = MeanShift(bandwidth=1.2)\n    msg = \"A sparse matrix was passed, but dense data is required.\"\n    assert_raise_message(TypeError, msg, ms.fit, X)\n", "def test_meanshift_max_iter():\n    # Test MeanShift with max_iter parameter\n    X = np.array([[1, 1], [2, 1], [1, 0],\n                  [4, 7], [3, 5], [3, 6]])\n    ms = MeanShift(bandwidth=2, max_iter=1)\n    with pytest.warns(UserWarning, match=\"The number of iterations may not have converged.\"):\n        ms.fit(X)\n    assert ms.n_iter_ == 1\n"], "sample_82": ["def test_selectdate_id_for_label(self):\n    widget = SelectDateWidget()\n    self.assertEqual(widget.id_for_label('test'), 'test_month')\n", "def test_id_for_label(self):\n    self.assertEqual(self.widget.id_for_label('mydate'), 'mydate_month')\n    self.widget.month_field = '%s_custom_month'\n    self.assertEqual(self.widget.id_for_label('mydate'), 'mydate_custom_month')\n", "def test_render_with_attrs(self):\n    widget = SelectDateWidget(\n        years=('2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016'),\n        attrs={'class': 'custom-class'}\n    )\n    self.assertIn('class=\"custom-class\"', widget.render('mydate', ''))\n", "def test_id_for_label(self):\n    \"\"\"\n    The first select box's ID for label should be the base ID plus '_month'.\n    \"\"\"\n    self.assertEqual(self.widget.id_for_label('mydate'), 'mydate_month')\n", "def test_value_omitted_from_data_partial_data(self):\n    # Test case when only some fields are present in the data\n    self.assertIs(self.widget.value_omitted_from_data({'field_month': '12'}, {}, 'field'), False)\n    self.assertIs(self.widget.value_omitted_from_data({'field_year': '2000'}, {}, 'field'), False)\n    self.assertIs(self.widget.value_omitted_from_data({'field_day': '1'}, {}, 'field'), False)\n\n    # Test case when some fields are missing in the data\n    self.assertIs(self.widget.value_omitted_from_data({'field_year': '2000', 'field_day': '1'}, {}, 'field'), False)\n    self.assertIs(self.widget.value_omitted_from_data({'field_month': '12', 'field_day': '1'}, {}, 'field'), False)\n    self.assertIs(self.widget.value_omitted_from_data({'field_month': '12', 'field_year': '2000'}, {}, 'field'), False)\n"], "sample_270": ["def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_include_pointing_to_fk(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk_1_id', 'fk_2'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n"], "sample_77": ["def test_urlize_with_nofollow(self):\n    value = 'Visit <a href=\"http://example.com\">example</a> for more info.'\n    output = 'Visit <a href=\"http://example.com\" rel=\"nofollow\">example</a> for more info.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_autoescape(self):\n    value = 'Search for google.com/?q=\"escaped\" and see.'\n    output = 'Search for <a href=\"http://google.com/?q=%22escaped%22\">google.com/?q=\"escaped\"</a> and see.'\n    self.assertEqual(urlize(value, autoescape=True), output)\n", "def test_strip_tags_edge_cases(self):\n    # Test with edge cases that could potentially cause issues\n    items = (\n        ('<script>alert(\"Hello, world!\");</script>', 'alert(\"Hello, world!\");'),\n        ('<![CDATA[This is a CDATA section.]]>', 'This is a CDATA section.'),\n        ('<!-- This is a comment -->', ''),\n        ('<!DOCTYPE html>', ''),\n    )\n    for value, output in items:\n        with self.subTest(value=value, output=output):\n            self.check_output(strip_tags, value, output)\n            self.check_output(strip_tags, lazystr(value), output)\n", "def test_urlize_no_follow(self):\n    value = 'Search for google.com'\n    output = '<a href=\"http://google.com\" rel=\"nofollow\">google.com</a>'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Check out this site: www.example.com.', 'Check out this site: <a href=\"http://www.example.com\">www.example.com</a>.'),\n        ('Read more here: http://example.com!', 'Read more here: <a href=\"http://example.com\">http://example.com</a>!'),\n        ('Follow this link: (http://example.com).', 'Follow this link: (<a href=\"http://example.com\">http://example.com</a>).'),\n        ('Visit this site: www.example.com, it\\'s great!', 'Visit this site: <a href=\"http://www.example.com\">www.example.com</a>, it\\'s great!'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_352": ["    def test_ticket_24605_with_values(self):\n        \"\"\"\n        Subquery table names should be quoted when using values().\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(Individual.objects.filter(Q(alive=False), Q(related_individual__isnull=True)).values('pk'), [{'pk': i4.pk}])\n        self.assertSequenceEqual(\n            Individual.objects.exclude(Q(alive=False), Q(related_individual__isnull=True)).values('pk').order_by('pk'),\n            [{'pk': i1.pk}, {'pk': i2.pk}, {'pk': i3.pk}]\n        )\n", "    def test_ticket_24605_exclude(self):\n        \"\"\"\n        Subquery table names should be quoted in exclude conditions as well.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.exclude(Q(alive=True), Q(related_individual__isnull=False)).order_by('pk'),\n            [i2, i4]\n        )\n", "    def test_ticket_24605_exclusion(self):\n        \"\"\"\n        Subquery table names should be quoted in exclusion cases as well.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(Individual.objects.exclude(Q(alive=False), Q(related_individual__isnull=False)), [i1])\n", "def test_filter_with_custom_lookup(self):\n    # Test filtering with a custom lookup\n    author = Author.objects.create(name='Author1', num=1)\n    AuthorExtraInfo.objects.create(author=author, extra_info='Info1')\n    qs = Author.objects.filter(extra_info__startswith='Info')\n    self.assertSequenceEqual(qs, [author])\n", "    def test_ticket_24605_subquery_table_names(self):\n        \"\"\"\n        Subquery table names should be quoted for both filters and excludes.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(Individual.objects.exclude(Q(alive=False), Q(related_individual__isnull=True)), [i1, i3])\n        self.assertSequenceEqual(Individual.objects.exclude(Q(alive=True), Q(related_individual__isnull=True)), [i2, i4])\n"], "sample_840": ["def test_pls_deflation_mode():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for mode in [\"canonical\", \"regression\"]:\n        pls_reg = pls_.PLSRegression(deflation_mode=mode)\n        pls_reg.fit(X, Y)\n        # Add some assertions to check the results based on the deflation mode\n", "def test_pls_predict():\n    # Test the predict method of PLSRegression\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    clf = pls_.PLSRegression()\n    clf.fit(X, Y)\n    Y_pred = clf.predict(X)\n\n    assert Y_pred.shape == Y.shape\n    assert_array_almost_equal(Y_pred, Y, decimal=2)  # Check that the prediction is close to the actual values\n", "def test_pls_algorithm_compatibility():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls_bynipals = pls_.PLSCanonical(n_components=X.shape[1], algorithm=\"nipals\")\n    pls_bysvd = pls_.PLSCanonical(n_components=X.shape[1], algorithm=\"svd\")\n    pls_bynipals.fit(X, Y)\n    pls_bysvd.fit(X, Y)\n    assert_array_almost_equal(pls_bynipals.x_weights_, pls_bysvd.x_weights_, decimal=5,\n                              err_msg=\"nipals and svd implementations lead to different x weights\")\n    assert_array_almost_equal(pls_bynipals.y_weights_, pls_bysvd.y_weights_, decimal=5,\n                              err_msg=\"nipals and svd implementations lead to different y weights\")\n\n    # Test error when algorithm is not nipals or svd\n    pls_byother = pls_.PLSCanonical(n_components=X.shape[1], algorithm=\"other\")\n    assert_raise_message(ValueError, \"Got algorithm other when only 'svd' and 'nipals' are known\",\n                         pls_byother.fit, X, Y)\n\n    # Test error when algorithm is svd and mode is B\n    pls_bysvd_modeB = pls_.PLSCanonical(n_components=X.shape[1], algorithm=\"svd\", mode=\"B\")\n    assert_raise_message(ValueError, \"Incompatible configuration: mode B is not implemented with svd algorithm\",\n                         pls_bysvd_modeB.fit, X, Y)\n", "def test_pls_regression_with_constant_columns():\n    # Checking behavior when the first column of X or Y is constant\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    X[:, 0] = 1\n    pls_2 = pls_.PLSRegression(n_components=X.shape[1])\n    pls_2.fit(X, Y)\n\n    # Check that the weights and loadings for the constant column are all zeros\n    assert_array_almost_equal(pls_2.x_weights_[0], np.zeros(X.shape[1]))\n    assert_array_almost_equal(pls_2.x_loadings_[0], np.zeros(X.shape[1]))\n\n    # Check that the other weights and loadings are not all zeros\n    assert np.any(pls_2.x_weights_[1:] != 0)\n    assert np.any(pls_2.x_loadings_[1:] != 0)\n", "def test_pls_predict():\n    # Check that predict gives the same result as applying coef_ to X\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    clf = pls_.PLSRegression()\n    clf.fit(X, Y)\n    Y_pred = clf.predict(X)\n    Y_pred_manual = np.dot(X, clf.coef_) + clf.y_mean_\n    assert_array_almost_equal(Y_pred, Y_pred_manual)\n"], "sample_968": ["def test_noindexentry_property(app):\n    text = (\".. py:property:: f\\n\"\n            \".. py:property:: g\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (property)', 'f', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[])\n", "def test_module_index_platform(app):\n    text = (\".. py:module:: sphinx\\n\"\n            \"   :platform: Linux\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', 'Linux', '', '')])],\n        True\n    )\n", "def test_pydecoratormethod_signature_prefix(app):\n    text = (\".. py:class:: Class\\n\"\n            \"   .. py:decoratormethod:: deco\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][1][0][0], desc_signature,\n                ([desc_addname, \"@\"],\n                 [desc_addname, \"Class.\"],\n                 [desc_name, \"deco\"]))\n\n    assert 'Class.deco' in domain.objects\n    assert domain.objects['Class.deco'] == ('index', 'Class.deco', 'method', False)\n", "def test_pyfunction_signature_with_comments(app):\n    text = \".. py:function:: func(arg1, arg2)  # This is a comment\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist,\n                                                    desc_content)],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"arg1\"],\n                                      [desc_parameter, desc_sig_name, \"arg2\"])])\n    assert doctree[1][1].astext().strip() == 'This is a comment'\n", "def test_pyfunction_signature_with_default_value(app):\n    text = \".. py:function:: hello(name='World') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"'World'\"])])\n"], "sample_791": ["def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['ghi', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [1, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 12, 2, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['def', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [1, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 2, 12, 55])\n    expected_inverse = np.array(X, dtype=object)\n    expected_inverse[:, [0, 2]] = 'abc', 2\n    assert_array_equal(expected_inverse,\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_unsorted_categories_numeric():\n    X = np.array([[1, 2]]).T\n\n    enc = OneHotEncoder(categories=[[2, 1, 3]])\n    exp = np.array([[1., 0., 0.],\n                    [0., 1., 0.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n    assert enc.categories_[0].tolist() == [2, 1, 3]\n    assert np.issubdtype(enc.categories_[0].dtype, np.integer)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([['a', 'b']], dtype=object).T\n    enc = OneHotEncoder(categories=[['b', 'a', 'c']])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_drop_with_auto_categories(drop):\n    enc = OneHotEncoder(drop=drop, categories='auto')\n    X = [['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]]\n    trans = enc.fit_transform(X).toarray()\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, drop)\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['ghi', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [1, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[0] for cat in enc.categories_]\n    assert_array_equal(dropped_cats, ['abc', 2, 12])\n    X_inv = enc.inverse_transform(trans)\n    X_inv[:, 1] = 12\n    assert_array_equal(np.array(X, dtype=object), X_inv)\n"], "sample_597": ["def test_merge_override_join(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 3]), \"b\": (\"x\", [4, 4]), \"x\": [1, 2]})\n    assert expected.identical(ds1.merge(ds2, join=\"override\"))\n    assert expected.identical(ds2.merge(ds1, join=\"override\"))\n", "def test_merge_override(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n\n    assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"override\"))\n\n    ds3 = xr.Dataset({\"a\": (\"y\", [2, 3]), \"y\": [1, 2]})\n    with pytest.raises(ValueError):\n        ds1.merge(ds3, compat=\"override\")\n", "def test_merge_override(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [3, 4]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [3, 4]), \"x\": [1, 2]})\n    assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"override\"))\n", "def test_merge_override(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [3, 4]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 4]), \"x\": [0, 1]})\n    assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n\n    expected = xr.Dataset({\"a\": (\"x\", [3, 2]), \"x\": [1, 2]})\n    assert expected.identical(ds2.merge(ds1, compat=\"override\"))\n\n    expected = xr.Dataset({\"a\": (\"x\", [1, 3]), \"x\": [0, 1]})\n    assert expected.identical(ds1.merge(ds2, compat=\"override\", join=\"outer\"))\n\n    expected = xr.Dataset({\"a\": (\"x\", [3, 2]), \"x\": [1, 2]})\n    assert expected.identical(ds2.merge(ds1, compat=\"override\", join=\"outer\"))\n", "def test_merge_exact(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    with pytest.raises(ValueError):\n        ds1.merge(ds2, join=\"exact\")\n\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"b\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [2, 3]), \"x\": [0, 1]})\n    assert expected.identical(ds1.merge(ds2, join=\"exact\"))\n\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    with pytest.raises(ValueError):\n        ds1.merge(ds2, join=\"exact\")\n"], "sample_1010": ["def test_Quaternion_conjugate_printing():\n    q = Quaternion(x, y, z, t)\n    assert latex(q.conjugate()) == r\"\\overline{x} - y i - z j - t k\"\n", "def test_Quaternion_properties():\n    q = Quaternion(x, y, z, t)\n    assert q.scalar == x\n    assert q.vector == [y, z, t]\n    assert q.conjugate() == Quaternion(x, -y, -z, -t)\n    assert q.norm() == x**2 + y**2 + z**2 + t**2\n", "def test_Quaternion_properties():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    assert q1.norm() == 30\n    assert (q1 * q2).conjugate() == (q2 * q1).conjugate()\n    assert q1.inverse() * q1 == Quaternion(1, 0, 0, 0)\n", "def test_Quaternion_multiplication_printing():\n    q1 = Quaternion(x, y, z, t)\n    q2 = Quaternion(a, b, c, d)\n    assert latex(q1 * q2) == \"x a - y b - z c - t d + x b i + y a + t c j + z d k + y c i - z b i - t a j + z t k - x d k\"\n", "def test_latex_MatrixSymbol_dense_shape():\n    # test cases for MatrixSymbol dense shape\n    A = MatrixSymbol(\"A\", 2, 3)\n    B = MatrixSymbol(\"B\", 3, 4)\n\n    assert latex(A.shape) == r\"\\left(2, 3\\right)\"\n    assert latex(B.shape) == r\"\\left(3, 4\\right)\"\n    assert latex(A * B) == r\"A B\"\n"], "sample_812": ["def test_nested_dict():\n    # Render a nested dict\n    nested_dict = {'a': 1, 'b': {'c': 2, 'd': 3, 'e': {'f': 4, 'g': 5}}}\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    expected = \"\"\"{'a': 1, 'b': {'c': 2, 'd': 3, 'e': {'f': 4, 'g': 5}}}\"\"\"\n    assert pp.pformat(nested_dict) == expected\n", "def test_n_max_elements_to_show_pipeline():\n    # Test n_max_elements_to_show with a pipeline\n    n_max_elements_to_show = 1\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression(C=999),\n                             LogisticRegression(C=888))\n    expected = \"\"\"", "def test_bruteforce_ellipsis():\n    # Check that bruteforce ellipsis is used when repr is still too long\n    long_string = 'a' * 1000  # a very long string\n    param_grid = {'very_long_parameter_name': [long_string]}\n    gs = GridSearchCV(SVC(), param_grid)\n    repr_ = gs.__repr__()\n    assert '...' in repr_\n", "def test_compact_representation():\n    # Check that the compact=True parameter of _EstimatorPrettyPrinter is\n    # correctly used\n    pp = _EstimatorPrettyPrinter(compact=True)\n    lr = LogisticRegression(C=99, class_weight=.4, fit_intercept=False,\n                            tol=1234, verbose=True)\n    expected = \"LogisticRegression(C=99, class_weight=0.4, fit_intercept=False, tol=1234, verbose=True)\"\n    assert pp.pformat(lr) == expected\n", "def test_indent_at_name():\n    # Test the indent_at_name param of _EstimatorPrettyPrinter\n    pp = _EstimatorPrettyPrinter(compact=True, indent=2, indent_at_name=True)\n    lr = LogisticRegression(C=99)\n    expected = \"\"\""], "sample_770": ["def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)), [0] * 10))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5)\n    labels = [0] * 10 + [1] * 10\n    pytest.approx(davies_bouldin_score(X, labels), 1.0)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                          [0] * 5 + [1] * 5))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels), 1.3333333333333335)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                          [0] * 10))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 5 + [1] * 5 + [2] * 5 + [3] * 5\n    pytest.approx(davies_bouldin_score(X, labels),\n                  1.125, abs=1e-2)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all the samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                          [0] * 5 + [1] * 5))\n\n    # Assert the value is 0. when all the mean cluster are equal\n    assert_equal(0., davies_bouldin_score([[-1, -1], [1, 1]] * 10,\n                                          [0] * 10 + [1] * 10))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  (1 + 1 + 1 + 1) / 4)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                          [0] * 10))\n\n    # Assert the value is 0. when all samples are in different clusters\n    assert_equal(0., davies_bouldin_score(np.arange(10).reshape(10, 1),\n                                          np.arange(10)))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5)\n    labels = [0] * 10 + [1] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  (2.5 / 2.0 + 2.5 / 2.0) / 2.0)\n"], "sample_413": ["    def test_template_tags_with_different_libraries(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"diff_tags_1\", \"diff_tags_app_1.templatetags.diff_tags\"\n                ),\n                self.get_settings(\n                    \"diff_tags_2\", \"diff_tags_app_2.templatetags.diff_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_template_tags_with_no_conflicts(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"tags1\", \"tags_app_1.templatetags.tags1\"\n                ),\n                self.get_settings(\n                    \"tags2\", \"tags_app_2.templatetags.tags2\"\n                ),\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "def test_template_tags_with_different_module_name(self):\n    with self.settings(\n        TEMPLATES=[\n            self.get_settings(\n                \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n            ),\n            self.get_settings(\n                \"same_tags\", \"same_tags_app_2.templatetags.different_tags\"\n            ),\n        ]\n    ):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "def test_template_tags_with_different_names(self):\n    with self.settings(\n        TEMPLATES=[\n            self.get_settings(\n                \"different_tags_1\",\n                \"different_tags_app_1.templatetags.different_tags\",\n            ),\n            self.get_settings(\n                \"different_tags_2\",\n                \"different_tags_app_2.templatetags.different_tags\",\n            ),\n        ]\n    ):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "def test_template_tags_with_different_library_name_and_module_name(self):\n    with self.settings(\n        TEMPLATES=[\n            self.get_settings(\n                \"different_tags\",\n                \"different_tags_app_1.templatetags.different_tags\",\n            ),\n            self.get_settings(\n                \"different_tags\",\n                \"different_tags_app_2.templatetags.different_tags\",\n            ),\n        ]\n    ):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n"], "sample_1203": ["def test_group_isomorphism_free_group():\n    # Test isomorphism between two free groups\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**2, b**3])\n    H = FpGroup(F, [a**2, b**3])\n    check, T = group_isomorphism(G, H)\n    assert check\n    assert T(a*b**2*a**-1) == a*b**2*a**-1\n", "def test_homomorphism_check():\n    # Test that homomorphism checks the validity of the mapping\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = AlternatingGroup(4)\n    c = Permutation(3)(0, 1, 2)\n    d = Permutation(3)(1, 2, 3)\n    # The following images do not define a homomorphism\n    with raises(ValueError):\n        homomorphism(G, H, [a, b], [c, d**2])\n", "def test_group_isomorphism_fp_to_perm():\n    # FpGroup -> PermutationGroup\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n\n    A = AlternatingGroup(4)\n    check, T = group_isomorphism(G, A)\n    assert check\n    assert T(a*b**2*a**-1).is_even_permutation()\n    assert T(b*a*b**-1*a**-1*b**-1).is_even_permutation()\n    assert T(T.invert(T(a*b))).is_identity\n", "def test_group_isomorphism():\n    # Test isomorphism between two cyclic groups of the same order\n    G = CyclicGroup(7)\n    H = CyclicGroup(7)\n    assert is_isomorphic(G, H)\n\n    # Test isomorphism between two dihedral groups of the same order\n    D1 = DihedralGroup(8)\n    D2 = DihedralGroup(8)\n    assert is_isomorphic(D1, D2)\n\n    # Test isomorphism between two groups that are not isomorphic\n    G = CyclicGroup(7)\n    H = CyclicGroup(8)\n    assert not is_isomorphic(G, H)\n\n    # Test isomorphism between two free groups with the same generators and relators\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = FpGroup(F, [a**3, b**3, (a*b)**2])\n    assert is_isomorphic(G, H)\n\n    # Test isomorphism between two groups that are isomorphic but with different generators\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    E, c, d = free_group(\"c, d\")\n    H = FpGroup(E, [c**3, d**3, (c*d)**2])\n    assert is_isomorphic(G, H)\n", "def test_invert_subgroup():\n    # PermutationGroup -> PermutationGroup\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    P = PermutationGroup(p)\n    T = homomorphism(P, D, [p], [p])\n    H = D.subgroup([D.generators[0]**2])\n    assert T.invert_subgroup(H) == P.subgroup([p**4])\n\n    # FreeGroup -> FpGroup\n    G = FpGroup(F, [a*b*a**-1*b**-1])\n    T = homomorphism(F, G, F.generators, G.generators)\n    H = G.subgroup([G.generators[0]**3])\n    assert T.invert_subgroup(H) == F.subgroup([a**3])\n"], "sample_407": ["def test_fk_instantiation_with_to_field(self):\n    # Test of ForeignKey instantiation with a 'to_field' argument.\n    parent = Parent.objects.create(name=\"Parent\")\n    child = Child(parent_id=parent.name)\n    self.assertIsNot(child.parent, parent)\n    child.save()\n    self.assertEqual(child.parent, parent)\n", "def test_fk_assignment_and_related_object_cache_with_to_field(self):\n    # Tests of ForeignKey assignment and the related-object cache with a to_field.\n\n    p = Parent.objects.create(name=\"Parent\")\n    c = ToFieldChild.objects.create(parent=p)\n\n    # Look up the object again so that we get a \"fresh\" object.\n    c = ToFieldChild.objects.get(parent=p)\n    p = c.parent\n\n    # Accessing the related object again returns the exactly same object.\n    self.assertIs(c.parent, p)\n\n    # But if we kill the cache, we get a new object.\n    del c._state.fields_cache[\"parent\"]\n    self.assertIsNot(c.parent, p)\n\n    # Assigning a new object results in that object getting cached immediately.\n    p2 = Parent.objects.create(name=\"Parent 2\")\n    c.parent = p2\n    self.assertIs(c.parent, p2)\n\n    # Assigning None will not fail: Child.parent is null=False.\n    setattr(c, \"parent\", None)\n\n    # You also can't assign an object of the wrong type here\n    msg = (\n        'Cannot assign \"<First: First object (1)>\": \"ToFieldChild.parent\" must '\n        'be a \"Parent\" instance.'\n    )\n    with self.assertRaisesMessage(ValueError, msg):\n        setattr(c, \"parent\", First(id=1, second=1))\n\n    # Creation using keyword argument and unsaved related instance (#8070).\n    p = Parent()\n    msg = (\n        \"save() prohibited to prevent data loss due to unsaved related object \"\n        \"'parent'.\"\n    )\n    with self.assertRaisesMessage(ValueError, msg):\n        ToFieldChild.objects.create(parent=p)\n", "def test_set_null_on_delete(self):\n    # Create a Reporter and an Article.\n    r = Reporter.objects.create(first_name=\"John\", last_name=\"Smith\", email=\"john@example.com\")\n    a = Article.objects.create(headline=\"This is a test\", pub_date=datetime.date(2005, 7, 27), reporter=r)\n\n    # Delete the Reporter and verify that the Article's reporter field is set to None.\n    r.delete()\n    a.refresh_from_db()\n    self.assertIsNone(a.reporter)\n", "def test_fk_assignment_on_unsaved_instance(self):\n    # Assigning a related object to an unsaved instance should cache the related object.\n    p = Parent()\n    c = Child(parent=p)\n    self.assertIs(c.parent, p)\n\n    # Assigning None should not raise an error.\n    c.parent = None\n    self.assertIsNone(c.parent)\n", "def test_add_multiple_objects(self):\n    new_article1 = self.r.article_set.create(\n        headline=\"John's second story\",\n        pub_date=datetime.date(2005, 7, 29),\n    )\n    new_article2 = Article(\n        headline=\"Fourth article\",\n        pub_date=datetime.date(2005, 7, 27),\n    )\n    self.r.article_set.add(new_article1, new_article2, bulk=False)\n    self.assertSequenceEqual(\n        self.r.article_set.all(),\n        [new_article1, new_article2, self.a],\n    )\n    self.assertEqual(new_article2.reporter.id, self.r.id)\n"], "sample_117": ["    def test_validates_password(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'testclient',\n            'password2': 'testclient',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form[\"password2\"].errors), 2)\n        self.assertIn('The password is too similar to the username.', form[\"password2\"].errors)\n        self.assertIn(\n            'This password is too short. It must contain at least 12 characters.',\n            form[\"password2\"].errors\n        )\n", "def test_password_reset_form_with_extra_email_context(self):\n    (user, username, email) = self.create_dummy_user()\n    extra_email_context = {'site_name': 'Custom Site'}\n    form = PasswordResetForm({\"email\": email})\n    self.assertTrue(form.is_valid())\n    form.save(extra_email_context=extra_email_context)\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertIn('Custom Site', mail.outbox[0].body)\n", "def test_password_reset_with_extra_context(self):\n    (user, username, email) = self.create_dummy_user()\n    data = {\"email\": email}\n    form = PasswordResetForm(data)\n    self.assertTrue(form.is_valid())\n    extra_context = {\"custom_key\": \"custom_value\"}\n    form.save(extra_email_context=extra_context)\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertIn(\"custom_key\", mail.outbox[0].body)\n    self.assertIn(\"custom_value\", mail.outbox[0].body)\n", "    def test_username_field_with_custom_user_model(self):\n        class CustomUser(User):\n            class Meta:\n                proxy = True\n\n        class CustomUserChangeForm(UserChangeForm):\n            class Meta:\n                model = CustomUser\n\n        # Verify that the username field is CharField\n        form = CustomUserChangeForm()\n        self.assertIsInstance(form.fields['username'], CharField)\n", "def test_password_min_length(self):\n    user = User.objects.get(username='testclient')\n    data = {\n        'new_password1': 'abc',\n        'new_password2': 'abc',\n    }\n    form = SetPasswordForm(user, data)\n    self.assertFalse(form.is_valid())\n    self.assertEqual(\n        form[\"new_password2\"].errors,\n        ['This password is too short. It must contain at least 8 characters.']\n    )\n"], "sample_546": ["def test_widgetlock_zoom():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    fig.canvas.widgetlock(ax)\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    assert ax.get_navigate_mode() is None\n", "def test_toolbar_home():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    ax = fig.add_subplot(111)\n    ax.set_xlim(10, 20)\n    ax.set_ylim(0, 10)\n    fig.canvas.manager.toolmanager.trigger_tool('home')\n    assert ax.get_xlim() == (0, 1)\n    assert ax.get_ylim() == (0, 1)\n", "def test_interactive_rubberband():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    tb = fig.canvas.manager.toolmanager.get_tool('rubberband')\n\n    # Mouse move starts from 0, 0 and ends at 1, 1\n    mousestart = (0, 0)\n    mouseend = (1, 1)\n    # Convert to screen coordinates (\"s\").  Events are defined only with pixel\n    # precision, so round the pixel values, and below, check against the\n    # corresponding xdata/ydata, which are close but not equal to d0/d1.\n    sstart = ax.transData.transform(mousestart).astype(int)\n    send = ax.transData.transform(mouseend).astype(int)\n\n    # Set up the mouse movements\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, *sstart, button=MouseButton.LEFT)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas, *send, button=MouseButton.LEFT)\n\n    tb.press(start_event)\n    tb.drag(stop_event)\n    tb.release(stop_event)\n\n    # Should be close, but won't be exact due to screen integer resolution\n    assert tuple(ax.get_xlim()) == pytest.approx((0, 1), abs=0.02)\n    assert tuple(ax.get_ylim()) == pytest.approx((0, 1), abs=0.02)\n", "def test_toolmanager_add_remove_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_len = len(fig.canvas.manager.toolmanager.tools)\n\n    # Add a new tool and check if it's added\n    new_tool = 'new_tool'\n    fig.canvas.manager.toolmanager.add_tool(new_tool, None)\n    assert len(fig.canvas.manager.toolmanager.tools) == initial_len + 1\n    assert new_tool in fig.canvas.manager.toolmanager.tools\n\n    # Remove the new tool and check if it's removed\n    fig.canvas.manager.toolmanager.remove_tool(new_tool)\n    assert len(fig.canvas.manager.toolmanager.tools) == initial_len\n    assert new_tool not in fig.canvas.manager.toolmanager.tools\n", "def test_savefig_bbox_inches(backend, bbox_inches):\n    test_backend = pytest.importorskip(f'matplotlib.backends.backend_{backend}')\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot([0, 1], [0, 1])\n    filename = f'test_{backend}_{bbox_inches}.{backend}'\n    fig.savefig(filename, bbox_inches=bbox_inches)\n    assert os.path.exists(filename)\n    os.remove(filename)\n"], "sample_296": ["def test_decode_empty(self):\n    \"\"\"\n    Decoding an empty string should return None.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    decoded = storage._decode(\"\")\n    self.assertIsNone(decoded)\n", "    def test_decode_empty(self):\n        \"\"\"\n        _decode() returns None when given an empty string.\n        \"\"\"\n        storage = self.get_storage()\n        self.assertIsNone(storage._decode(''))\n", "    def test_no_messages(self):\n        \"\"\"\n        If no messages are added to the storage, the cookie is not set.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        storage.update(response)\n\n        self.assertNotIn(CookieStorage.cookie_name, response.cookies)\n", "def test_large_messages(self):\n    \"\"\"\n    Messages larger than max_cookie_size are rejected and don't cause errors.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Generate a message that is larger than max_cookie_size.\n    msg_size = CookieStorage.max_cookie_size + 100\n    large_msg = get_random_string(msg_size)\n    storage.add(constants.INFO, large_msg)\n\n    with self.assertRaises(Exception) as context:\n        storage.update(response)\n\n    self.assertIsInstance(context.exception, ValueError)\n    self.assertEqual(str(context.exception), \"Message is too large to store in a cookie.\")\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 0)\n", "def test_safe_data_serialization(self):\n    \"\"\"\n    SafeData instances are properly serialized and deserialized by the\n    custom JSON encoder/decoder classes.\n    \"\"\"\n    safe_message = mark_safe('Safe message')\n    message = Message(constants.DEBUG, safe_message)\n    encoded = json.dumps(message, cls=MessageEncoder)\n    decoded = json.loads(encoded, cls=MessageDecoder)\n    self.assertIsInstance(decoded.message, SafeData)\n    self.assertEqual(str(decoded.message), str(safe_message))\n"], "sample_512": ["def test_subplot_mosaic_reuse():\n    # create an Axes\n    ax1 = plt.subplot_mosaic({'a': [1, 1]})['a']\n    # check that it is current\n    assert ax1 is plt.gca()\n    # make sure we get it back if we ask again\n    assert ax1 is plt.subplot_mosaic({'a': [1, 1]})['a']\n    # remove it\n    ax1.remove()\n    # create a polar plot\n    ax2 = plt.subplot_mosaic({'a': [1, 1]}, subplot_kw=dict(projection='polar'))['a']\n    assert ax2 is plt.gca()\n    # this should have deleted the first axes\n    assert ax1 not in plt.gcf().axes\n    # assert we get it back if no extra parameters passed\n    assert ax2 is plt.subplot_mosaic({'a': [1, 1]})['a']\n    ax2.remove()\n    # now check explicitly setting the projection to rectilinear\n    # makes a new axes\n    ax3 = plt.subplot_mosaic({'a': [1, 1]}, subplot_kw=dict(projection='rectilinear'))['a']\n    assert ax3 is plt.gca()\n    assert ax3 is not ax2\n    assert ax2 not in plt.gcf().axes\n", "def test_subplot_mosaic_gridspec_kw():\n    fig, axd = plt.subplot_mosaic({'a': [0, 0], 'b': [0, 1]},\n                                  gridspec_kw={'width_ratios': [1, 2]})\n    assert axd['a'].get_gridspec().width_ratios == [1, 2]\n    plt.close(fig)\n", "def test_subplot_remove_with_overlap():\n    # Test that overlapping axes are removed when a new axes is created.\n    fig = plt.figure()\n    ax1 = plt.subplot(221)\n    ax2 = plt.subplot(222)\n    ax3 = plt.subplot(223)\n    ax4 = plt.subplot(224)\n\n    ax5 = plt.subplot(223, frameon=False)  # overlaps with ax3\n\n    assert ax3 not in fig.axes\n    assert ax5 in fig.axes\n", "def test_subplot_mosaic():\n    fig = plt.figure()\n    ax_dict = fig.subplot_mosaic(\"AB;CD\")\n    assert set(ax_dict.keys()) == {\"A\", \"B\", \"C\", \"D\"}\n    assert all(isinstance(ax, plt.Axes) for ax in ax_dict.values())\n", "def test_subplot_mosaic_empty_sentinel():\n    fig, ax_dict = plt.subplot_mosaic({'a': ['b', 'c'], 'd': ['b', 'c']}, empty_sentinel='x')\n    assert 'a' not in ax_dict\n    assert 'x' not in ax_dict\n    assert 'b' in ax_dict\n    assert 'c' in ax_dict\n    assert 'd' in ax_dict\n"], "sample_1": ["def test_custom_model_separability_matrix():\n    @custom_model\n        return x + y\n\n    model_b.separable = False\n    model_b.n_inputs = 2\n    model_b.n_outputs = 1\n\n    result = separability_matrix(model_b)\n    assert_allclose(result, np.array([[True, True]]))\n", "def test_custom_model_separability_matrix():\n    @custom_model\n        return x + y\n\n    result = separability_matrix(model_b)\n    expected = np.array([[True, True]])\n    assert_allclose(result, expected)\n", "def test_custom_model_separability_matrix():\n    @custom_model\n        return x + y\n\n    result = separability_matrix(model_b)\n    assert_allclose(result, np.array([[True, True]]))\n", "def test_custom_model_separability_matrix():\n    @custom_model\n        return x * y, x + y\n\n    result = separability_matrix(model_b)\n    expected = np.array([[True, True], [True, True]])\n    assert_allclose(result, expected)\n", "def test_custom_model_separability_matrix():\n    # Define a custom separable model\n    @custom_model\n        return x, y\n\n    # Test separability_matrix for the custom model\n    sep_mat = separability_matrix(sep_model)\n    assert_allclose(sep_mat, np.array([[True, False], [False, True]]))\n"], "sample_513": ["def test_legend_fontsize():\n    # Test the fontsize of the legend\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    legend = ax.legend(fontsize=12)\n    assert legend.prop.get_size() == 12\n", "def test_multiline_labels():\n    # test multiline labels\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), '-o', label='one\\ntwo')\n    ax.plot(np.ones(10)*5, ':x', label='three\\nfour\\nfive')\n    ax.plot(np.arange(20, 10, -1), 'd', label='six\\nseven\\neight\\nnine')\n    ax.legend(loc='best')\n", "def test_legend_title_fontprop_fontproperties():\n    # test the title_fontproperties kwarg\n    plt.plot(range(10))\n    with pytest.raises(ValueError):\n        plt.legend(title='Aardvark', title_fontsize=22,\n                   title_fontproperties={'family': 'serif', 'size': 22})\n\n    leg = plt.legend(title='Aardvark', title_fontproperties=FontProperties(\n                                       family='serif', size=22))\n    assert leg.get_title().get_size() == 22\n\n    fig, axes = plt.subplots(2, 3, figsize=(10, 6))\n    axes = axes.flat\n    axes[0].plot(range(10))\n    leg0 = axes[0].legend(title='Aardvark', title_fontproperties=FontProperties(\n                                           family='serif', size=22))\n    assert leg0.get_title().get_fontproperties().get_family() == 'serif'\n    axes[1].plot(range(10))\n    leg1 = axes[1].legend(title='Aardvark',\n                          title_fontproperties={'family': 'serif', 'size': 22})\n    assert leg1.get_title().get_fontproperties().get_family() == 'serif'\n", "def test_legend_empty_labels():\n    # Test that an empty list of labels results in no legend (#20279)\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    leg = ax.legend([])\n    assert len(leg.legendHandles) == 0\n    assert len(leg.texts) == 0\n", "def test_legend_markers_from_collections():\n    # Test that markers can be copied for legend collections (#17960)\n    _markers = ['o', 's', '^']\n    fig, ax = plt.subplots()\n    collections = [plt.scatter([0], [0], marker=mark)\n                   for mark in _markers]\n    labels = [\"circle\", \"square\", \"triangle\"]\n    markers = [col._paths[0].vertices[0][0] for col in collections]\n    legend = ax.legend(collections, labels)\n\n    new_markers = [path._vertices[0][0] for path in legend.get_patches()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert markers == new_markers == _markers\n    assert labels == new_labels\n"], "sample_725": ["def test_check_X_y_complex_data_error():\n    X = np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]])\n    y = np.array([1, 2])\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # list of lists\n    X = [[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]]\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # tuple of tuples\n    X = ((1 + 2j, 3 + 4j, 5 + 7j), (2 + 3j, 4 + 5j, 6 + 7j))\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # list of np arrays\n    X = [np.array([1 + 2j, 3 + 4j, 5 + 7j]),\n         np.array([2 + 3j, 4 + 5j, 6 + 7j])]\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # tuple of np arrays\n    X = (np.array([1 + 2j, 3 + 4j, 5 + 7j]),\n         np.array([2 + 3j, 4 + 5j, 6 + 7j]))\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # dataframe\n    X = MockDataFrame(\n        np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + ", "def test_check_memory():\n    memory = check_memory(None)\n    assert isinstance(memory, Memory)\n\n    memory = check_memory('/tmp/')\n    assert isinstance(memory, Memory)\n\n    memory = check_memory(DummyMemory())\n    assert memory == DummyMemory()\n\n    with pytest.raises(ValueError):\n        check_memory(123)\n", "def test_check_array_complex_data_warning():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    with warnings.catch_warnings(record=True) as w:\n        X_checked = check_array(X, dtype=np.complex)\n        assert_equal(len(w), 0)\n        assert_equal(X_checked.dtype, np.complex)\n\n    X = np.array([[1 + 1j, 2, 3], [4, 5, 6]])\n    with warnings.catch_warnings(record=True) as w:\n        X_checked = check_array(X, dtype=np.complex)\n        assert_equal(len(w), 0)\n        assert_equal(X_checked.dtype, np.complex)\n\n    X = np.array([[1, 2, 3], [4, 5, 6 + 1j]])\n    with warnings.catch_warnings(record=True) as w:\n        X_checked = check_array(X, dtype=np.complex)\n        assert_equal(len(w), 0)\n        assert_equal(X_checked.dtype, np.complex)\n", "def test_check_memory():\n    # Test that check_memory returns the correct instance\n    memory = check_memory(None)\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    memory = check_memory('/tmp')\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n    assert memory.cachedir == '/tmp'\n\n    memory = check_memory(DummyMemory())\n    assert memory == DummyMemory()\n\n    # Test that check_memory raises ValueError for invalid input\n    with pytest.raises(ValueError) as excinfo:\n        check_memory(123)\n    assert \"should be None, a string or have the same interface as sklearn.externals.joblib.Memory\" in str(excinfo.value)\n", "def test_check_X_y():\n    # Test check_X_y function\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test multi-output y\n    y_multi = np.array([[0, 1], [1, 0]])\n    X_checked, y_checked = check_X_y(X, y_multi, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y_multi)\n\n    # Test y_numeric conversion\n    y_obj = np.array(['0', '1'])\n    X_checked, y_checked = check_X_y(X, y_obj, y_numeric=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y_checked.astype(np.float64))\n\n    # Test inconsistent length of X and y\n    y_short = np.array([0])\n    with pytest.raises(ValueError):\n        check_X_y(X, y_short)\n\n    # Test non-finite values in y\n    y_inf = np.array([0, np.inf])\n    with pytest.raises(ValueError):\n        check_X_y(X, y_inf)\n\n    # Test non-finite values in y for multi-output\n    y_inf_multi = np.array([[0, np.inf], [1, 0]])\n    with pytest.raises(ValueError):\n        check_X_y(X, y_inf_multi, multi_output=True)\n"], "sample_181": ["def test_filtered_aggregate_ref_exists_subquery(self):\n    aggs = Author.objects.annotate(\n        has_expensive_books=Exists(\n            Book.objects.filter(\n                contact__pk=OuterRef('pk'),\n                price__gt=Decimal('30.00'),\n            )\n        ),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(has_expensive_books=True)),\n    )\n    self.assertEqual(aggs['cnt'], 2)\n", "def test_filtered_aggregates_with_m2m_related_model(self):\n    agg = Sum('book__pages', filter=Q(book__publisher__name='Apress'))\n    qs = Author.objects.annotate(total_pages=agg).order_by('pk')\n    self.assertSequenceEqual([a.total_pages for a in qs], [447, 528, 1047])\n", "def test_filtered_aggregate_with_complex_filter(self):\n    filter_condition = (\n        Q(friends__book__publisher__name='Apress') &\n        (Q(friends__age__gt=40) | Q(friends__age__lt=60)) &\n        ~Q(friends__name='test3')\n    )\n    agg = Sum('friends__book__pages', filter=filter_condition)\n    result = Author.objects.filter(name='test').aggregate(pages=agg)['pages']\n    self.assertEqual(result, 528)\n", "def test_filtered_aggregate_with_outerref(self):\n    subquery = Book.objects.filter(\n        publisher=OuterRef('publisher'),\n    ).annotate(\n        total_pages=Sum('pages', filter=Q(rating__gt=3)),\n    ).values('total_pages')\n    agg = Sum('publisher__book__total_pages')\n    result = Publisher.objects.annotate(total_pages=agg).values('total_pages').get()\n    self.assertEqual(result['total_pages'], 995)\n", "def test_filtered_aggregate_with_annotation_and_subquery(self):\n    annotated_books = Book.objects.annotate(\n        author_age_times_pages=F('contact__age') * F('pages')\n    )\n    qs = annotated_books.annotate(\n        total_author_age_times_pages=Sum('author_age_times_pages', filter=Q(pages__gt=400)),\n    ).filter(\n        total_author_age_times_pages__isnull=False,\n    ).order_by('pk')\n    self.assertSequenceEqual([b.total_author_age_times_pages for b in qs], [44700, 52800])\n"], "sample_936": ["def test_stringify_type_hints_broken_type():\n    assert stringify(BrokenType) == \"test_util_typing.BrokenType\"\n", "def test_stringify_type_hints_broken_type():\n    assert stringify(BrokenType) == \"test_util_typing.BrokenType\"\n", "def test_stringify_type_hints_broken_type():\n    assert stringify(BrokenType) == \"test_util_typing.BrokenType\"\n", "def test_stringify_type_hints_generic():\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric) == \"test_util_typing.MyGeneric\"\n    assert stringify(MyGeneric[int]) == \"test_util_typing.MyGeneric[int]\"\n    assert stringify(MyGeneric[str]) == \"test_util_typing.MyGeneric[str]\"\n    assert stringify(MyGeneric[T]) == \"test_util_typing.MyGeneric[T]\"\n", "def test_stringify_type_hints_Generic():\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric) == \"test_util_typing.MyGeneric\"\n    assert stringify(MyGeneric[int]) == \"test_util_typing.MyGeneric[int]\"\n    assert stringify(MyGeneric[str]) == \"test_util_typing.MyGeneric[str]\"\n    assert stringify(MyGeneric[Union[int, str]]) == \"test_util_typing.MyGeneric[Union[int, str]]\"\n"], "sample_617": ["def test_polyval_invalid_degree_dim():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"invalid_dim\": [0, 1, 2]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs, degree_dim=\"invalid_dim\")\n", "def test_polyval_no_degree_dim():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"coeffs\")\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs)\n", "def test_polyval_broadcasting():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([[2, 3], [4, 5]], dims=(\"degree\", \"y\"), coords={\"degree\": [0, 1]})\n    expected = xr.DataArray([[2, 3], [8, 13], [18, 29]], dims=(\"x\", \"y\"))\n    actual = xr.polyval(coord=x, coeffs=coeffs)\n    xr.testing.assert_allclose(actual, expected)\n", "def test_polyval_custom_degree_dim(\n    use_dask: bool,\n    x: xr.DataArray,\n    coeffs: xr.DataArray,\n    degree_dim: str,\n    expected: xr.DataArray,", "def test_polyval_degree_dim_checks_with_missing_degree(use_dask):\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]}).isel(degree=[0, 2])\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs)\n"], "sample_425": ["def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex((1+2j))\", set()),\n    )\n", "def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(complex(1, 2), (\"complex((1+2j))\", {}))\n", "def test_serialize_complex(self):\n    complex_num = complex(3, 4)\n    self.assertSerializedEqual(complex_num)\n    self.assertSerializedResultEqual(complex_num, (\"complex((3+4j))\", set()))\n", "def test_serialize_complex(self):\n    \"\"\"\n    Test serialization of complex numbers.\n    \"\"\"\n    complex_num = 1 + 2j\n    self.assertSerializedEqual(complex_num)\n    self.assertSerializedResultEqual(complex_num, (\"complex('1+2j')\", {}))\n\n    field = models.CharField(default=complex_num)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(string, \"models.CharField(default=complex('1+2j'))\")\n", "def test_serialize_registered_serializer(self):\n    class CustomObject:\n            self.value = value\n\n    class CustomSerializer(BaseSerializer):\n            return f\"CustomObject({self.value})\", {}\n\n    BaseSerializer.register(CustomObject, CustomSerializer)\n\n    obj = CustomObject(42)\n    string, imports = MigrationWriter.serialize(obj)\n    self.assertEqual(string, \"CustomObject(42)\")\n    self.assertEqual(imports, set())\n\n    BaseSerializer.unregister(CustomObject)\n"], "sample_655": ["def test_capture_with_live_logging_and_disabled(testdir, capture_fixture):\n    # Test if capturing works correctly with live logging and if the disabled context manager works\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"before\")\n            with {0}.disabled():\n                print(\"inside disabled\")\n            print(\"after\")\n\n            captured = {0}.readouterr()\n            assert captured.out == \"before\\\\nafter\\\\n\"\n            assert \"inside disabled\" not in captured.out\n\n            logging.info(\"something\")\n            with {0}.disabled():\n                logging.info(\"inside disabled\")\n            logging.info(\"something else\")\n\n            captured = {0}.readouterr()\n            assert \"inside disabled\" not in captured.out\n        \"\"\".format(\n            capture_fixture\n        )\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n", "def test_logging_after_capture_disabled(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            with capsys.disabled():\n                logging.info(\"should not be captured\")\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n    assert \"should not be captured\" not in result.stdout.str()\n", "def test_suspend_resume_with_live_logging(testdir, caplog):\n    # Issue 3819\n    # capturing should work with live cli logging\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n            print(\"begin test\")\n            logging.info(\"something in test\")\n            caplog.clear()\n\n            capmanager = pytest.config.pluginmanager.getplugin(\"capturemanager\")\n            with capmanager.global_and_fixture_disabled():\n                print(\"disabled capture\")\n\n            assert \"disabled capture\" not in caplog.text\n            logging.info(\"another log message\")\n            assert \"another log message\" in caplog.text\n\n            print(\"end test\")\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n\n    assert \"something in test\" in result.outlines\n    assert \"another log message\" in result.outlines\n    assert \"disabled capture\" not in result.outlines\n", "def test_capture_manager_resume_global_capture(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            capfd.suspend()\n            capman = CaptureManager(\"fd\")\n            capman.resume_global_capture()\n            assert capman._global_capturing is not None\n            capman.stop_global_capturing()\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.assert_outcomes(passed=1)\n", "def test_capture_with_live_logging_no_capture(testdir):\n    # Test case when capturing is disabled\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n\n            logging.info(\"something\")\n            print(\"next\")\n            logging.info(\"something\")\n\n            # Try to read from sys.stdout and sys.stderr directly, as they should not be captured\n            # This may raise an error or print unexpected output if capture is not properly disabled\n            sys.stdout.read()\n            sys.stderr.read()\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\", \"--capture=no\")\n    assert result.ret == 0\n    # Assert that the output is not captured\n    assert \"hello\" in result.stdout.str()\n    assert \"world\" in result.stdout.str()\n    assert \"next\" in result.stdout.str()\n"], "sample_400": ["def test_multiple_operations_with_no_suggested_name(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.RunSQL(\"SELECT 1 FROM person;\"),\n            migrations.RunSQL(\"SELECT 1 FROM animal;\"),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    suggest_name = migration.suggest_name()\n    self.assertIs(suggest_name.startswith(\"auto_\"), True)\n", "def test_add_model_with_field_and_relation_removed_from_base_model(self):\n    \"\"\"\n    Removing a base field and a base relation takes place before adding a new\n    inherited model that has a field and a relation with the same name.\n    \"\"\"\n    before = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n                (\"parent\", models.ForeignKey(\"app.readable\", models.CASCADE)),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [\n                (\"title\", models.CharField(max_length=200)),\n                (\"parent\", models.ForeignKey(\"app.readable\", models.CASCADE)),\n            ],\n            bases=(\"app.readable\",),\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RemoveField\", \"RemoveField\", \"CreateModel\"])\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, name=\"title\", model_name=\"readable\"\n    )\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 1, name=\"parent\", model_name=\"readable\"\n    )\n    self.assertOperationAttributes(changes, \"app\", 0, 2, name=\"book\")\n", "def test_alter_model_options_proxy_with_custom_name(self):\n    \"\"\"Changing a proxy model's options with a custom name should also make a change.\"\"\"\n    changes = self.get_changes(\n        [self.author_proxy, self.author_empty],\n        [self.author_proxy_options_custom_name, self.author_empty],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"authorproxy\",\n        options={\"verbose_name\": \"Super Author Custom Name\"},\n    )\n", "def test_add_index_with_custom_fields(self):\n    author_with_custom_index = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\", models.F(\"age\")], name=\"author_name_age_idx\")\n            ],\n        },\n    )\n    changes = self.get_changes([self.author_empty], [author_with_custom_index])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"AddIndex\"])\n    added_index = models.Index(\n        fields=[\"name\", models.F(\"age\")], name=\"author_name_age_idx\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, model_name=\"author\", index=added_index\n    )\n", "def test_model_field_custom_default_now(self):\n    class MyNow(datetime.datetime):\n            super().__init__(**kwargs)\n\n            path, args, kwargs = super().deconstruct()\n            return path, args, kwargs\n\n    author_with_custom_default_now = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"created_at\", models.DateTimeField(default=MyNow)),\n        ],\n    )\n    changes = self.get_changes([], [author_with_custom_default_now])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\")\n    self.assertEqual(\n        changes[\"testapp\"][0].operations[0].fields[1][1].deconstruct(),\n        (\"django.db.models.DateTimeField\", [], {\"default\": MyNow()}),\n    )\n"], "sample_816": ["def test_countvectorizer_custom_vocabulary_duplicate_indices():\n    vocab = {\"pizza\": 0, \"beer\": 0}\n    try:\n        CountVectorizer(vocabulary=vocab)\n    except ValueError as e:\n        assert_in(\"duplicate term in vocabulary: 'beer'\", str(e).lower())\n", "def test_vectorizer_max_df_min_df():\n    test_data = ['abc', 'dea', 'eat']\n    vect = CountVectorizer(analyzer='char', max_df=0.6, min_df=2)\n    vect.fit(test_data)\n    assert 'a' not in vect.vocabulary_.keys()  # {ae} ignored\n    assert 'c' not in vect.vocabulary_.keys()  # {bdt} ignored\n    assert_equal(len(vect.vocabulary_.keys()), 2)    # {ae} remain\n    assert 'a' in vect.stop_words_\n    assert 'c' in vect.stop_words_\n    assert_equal(len(vect.stop_words_), 3)\n", "def test_tfidfvectorizer_custom_analyzer():\n        return doc.lower().split()\n\n    X = [\"This is a sample document.\", \"Another sample document.\"]\n    vec = TfidfVectorizer(analyzer=custom_analyzer)\n    X_tfidf = vec.fit_transform(X)\n    expected_features = [\"this\", \"is\", \"a\", \"sample\", \"document\", \"another\"]\n    assert set(vec.get_feature_names()) == set(expected_features)\n", "def test_custom_vocabulary_from_file(Estimator):\n    # Test if custom vocabulary can be loaded from a file\n    vocab_file = StringIO(\"apple\\norange\\nbanana\")\n    vec = Estimator(vocabulary=vocab_file)\n    vec.fit_transform([\"apple orange\", \"banana apple\", \"orange banana\"])\n    assert vec.vocabulary_ == {\"apple\": 0, \"orange\": 1, \"banana\": 2}\n", "def test_vectorizer_with_custom_tokenizer(Estimator):\n    data = [\"This is a test sentence.\"]\n\n        return doc.split()\n\n    vec = Estimator(tokenizer=custom_tokenizer)\n    X = vec.fit_transform(data)\n\n    assert X.shape[1] == 5  # 5 unique words in the sentence\n    assert isinstance(vec.vocabulary_, dict)\n    assert \"test\" in vec.vocabulary_\n"], "sample_111": ["def test_get_ordering_with_empty_string(self):\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get('/child/', {'o': ''})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering(request, cl.queryset), ['id'])\n", "def test_dynamic_ordering(self):\n    \"\"\"\n    Regression tests for #16796: dynamic ordering support.\n    \"\"\"\n    parent = Parent.objects.create(name='parent')\n    for i in range(1, 10):\n        Child.objects.create(id=i, name='child %s' % i, parent=parent, age=i)\n\n    user_noparents = self._create_superuser('noparents')\n    user_parents = self._create_superuser('parents')\n\n    # Test with user 'noparents'\n    m = DynamicListDisplayLinksChildAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request('/child/', user_noparents)\n    response = m.changelist_view(request)\n    self.assertContains(response, 'Parent object')\n    self.assertContains(response, 'Age')\n\n    ordering = m.get_ordering(request)\n    self.assertEqual(ordering, ['name'])\n\n    # Test with user 'parents'\n    m = DynamicListDisplayLinksChildAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request('/child/', user_parents)\n    response = m.changelist_view(request)\n    self.assertContains(response, 'Parent object')\n    self.assertContains(response, 'Age')\n\n    ordering = m.get_ordering(request)\n    self.assertEqual(ordering, ['parent__name', 'age'])\n", "def test_distinct_for_non_unique_related_object_in_list_filter_with_multiple_values(self):\n    \"\"\"\n    Regressions tests for #15819: If a field listed in list_filters\n    is a non-unique related object, distinct() must be called with multiple values.\n    \"\"\"\n    parent1 = Parent.objects.create(name='Mary')\n    parent2 = Parent.objects.create(name='John')\n    Child.objects.create(parent=parent1, name='Daniel')\n    Child.objects.create(parent=parent2, name='Daniel')\n\n    m = ParentAdmin(Parent, custom_site)\n    request = self.factory.get('/parent/', data={'child__name': 'Daniel'})\n    request.user = self.superuser\n\n    cl = m.get_changelist_instance(request)\n    # Make sure distinct() was called\n    self.assertEqual(cl.queryset.count(), 2)\n", "def test_changelist_search_fields_with_escaped_characters(self):\n    band = Group.objects.create(name='The Hype & Band')\n    concert = Concert.objects.create(name='Woodstock', group=band)\n\n    m = ConcertAdmin(Concert, custom_site)\n    m.search_fields = ['group__name']\n\n    request = self.factory.get('/', data={SEARCH_VAR: 'Hype\\\\&'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertCountEqual(cl.queryset, [concert])\n\n    request = self.factory.get('/', data={SEARCH_VAR: 'Wood'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertCountEqual(cl.queryset, [])\n", "def test_list_display_tuple_with_admin_order_field(self):\n    swallow = Swallow.objects.create(origin='Africa', load='12.34', speed='22.2')\n    swallow2 = Swallow.objects.create(origin='Africa', load='12.34', speed='22.2')\n    swallow_o2o = SwallowOneToOne.objects.create(swallow=swallow2)\n\n    model_admin = SwallowAdmin(Swallow, custom_site)\n    model_admin.list_display = (\n        ('origin', 'get_origin_display'),\n        'load',\n        ('speed', 'get_speed_display'),\n    )\n    superuser = self._create_superuser('superuser')\n    request = self._mocked_authenticated_request('/swallow/', superuser)\n    response = model_admin.changelist_view(request)\n    # Ensure the methods defined in list_display are called during rendering\n    self.assertContains(response, model_admin.get_origin_display(swallow))\n    self.assertContains(response, str(swallow.load))\n    self.assertContains(response, model_admin.get_speed_display(swallow))\n    self.assertContains(response, model_admin.get_origin_display(swallow2))\n    self.assertContains(response, str(swallow2.load))\n    self.assertContains(response, model_admin.get_speed_display(swallow2))\n"], "sample_952": ["def test_evaluate_signature():\n    from typing import List\n\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig)\n\n    assert evaluated_sig.parameters['x'].annotation == list\n    assert evaluated_sig.return_annotation == list\n", "def test_is_builtin_class_method_with_subclass():\n    class MyDict(dict):\n            super().__init__(*args, **kwargs)\n\n            pass\n\n    assert inspect.is_builtin_class_method(MyDict, '__init__') is True\n    assert inspect.is_builtin_class_method(MyDict, 'my_method') is False\n", "def test_isenumclass(app):\n    from target.enums import Color, NonEnum\n\n    assert inspect.isenumclass(Color) is True\n    assert inspect.isenumclass(NonEnum) is False\n", "def test_getdoc_partial_method():\n        \"\"\"\n        docstring\n            indented text\n        \"\"\"\n        pass\n\n    p = functools.partial(func, 10, c=11)\n\n    assert inspect.getdoc(p) == func.__doc__\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    sig = inspect.evaluate_signature(sig, globals(), locals())\n    assert sig.parameters['x'].annotation == int\n    assert sig.parameters['y'].annotation == float\n    assert sig.return_annotation == str\n"], "sample_788": ["def test_transform_and_inverse_transform(encode):\n    kbd = KBinsDiscretizer(n_bins=3, strategy='uniform', encode=encode)\n    Xt = kbd.fit_transform(X)\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_almost_equal(X, Xinv)\n", "def test_invalid_input_shape():\n    est = KBinsDiscretizer(n_bins=3)\n    X = np.array([1, 2, 3])  # input should be a 2D array\n    assert_raises(ValueError, est.fit, X)\n", "def test_transform_invalid_input():\n    est = KBinsDiscretizer(n_bins=3).fit(X)\n    bad_X = np.array([[1, 2, 'a', 4]])\n    assert_raise_message(ValueError,\n                         \"Input X must be a numeric array-like, got 'a'.\",\n                         est.transform, bad_X)\n", "def test_invalid_input_shape():\n    est = KBinsDiscretizer(n_bins=3)\n    X_invalid = [[-2, 1, -4],\n                 [-1, 2, -3],\n                 [0, 3, -2]]\n    assert_raises(ValueError, est.fit, X_invalid)\n", "def test_transform_invalid_shape():\n    est = KBinsDiscretizer(n_bins=3).fit(X)\n    bad_X = np.arange(25).reshape(5, -1)\n    assert_raise_message(ValueError,\n                         \"Incorrect number of features. Expecting 4, received 5\",\n                         est.transform, bad_X)\n"], "sample_1081": ["def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(220, 220) is False\n    assert is_amicable(220, 221) is False\n", "def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(10744, 10856) is True\n    assert is_amicable(12285, 14595) is True\n    assert is_amicable(17296, 18416) is True\n    assert is_amicable(220, 220) is False\n    assert is_amicable(1184, 1184) is False\n    assert is_amicable(2620, 2620) is False\n", "def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(10744, 10856) is True\n    assert is_amicable(12285, 14595) is True\n    assert is_amicable(220, 285) is False\n    assert is_amicable(220, 220) is False\n", "def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(10744, 10856) is True\n    assert is_amicable(12285, 14595) is True\n    assert is_amicable(220, 284) is True\n    assert is_amicable(220, 220) is False\n    assert is_amicable(220, 285) is False\n", "def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(10744, 10856) is True\n    assert is_amicable(12285, 14595) is True\n    assert is_amicable(220, 284) is True\n    assert is_amicable(220, 220) is False\n    assert is_amicable(220, 123) is False\n"], "sample_773": ["def test_logistic_regression_path_multi_class_auto():\n    # check multi_class='auto' in logistic_regression_path\n\n    X = iris.data[::10]\n    y_multi = iris.target[::10]\n    y_bin = y_multi == 0\n\n    coefs_auto_bin, _, _ = _logistic_regression_path(X, y_bin, multi_class='auto')\n    coefs_ovr_bin, _, _ = _logistic_regression_path(X, y_bin, multi_class='ovr')\n    assert np.allclose(coefs_auto_bin, coefs_ovr_bin)\n\n    coefs_auto_multi, _, _ = _logistic_regression_path(X, y_multi, multi_class='auto')\n    coefs_multi_multi, _, _ = _logistic_regression_path(X, y_multi, multi_class='multinomial')\n    assert np.allclose(coefs_auto_multi, coefs_multi_multi)\n\n    # Make sure multi_class='ovr' is distinct from ='multinomial'\n    assert not np.allclose(coefs_auto_bin,\n                           _logistic_regression_path(X, y_bin, multi_class='multinomial')[0])\n    assert not np.allclose(coefs_auto_bin,\n                           _logistic_regression_path(X, y_multi, multi_class='multinomial')[0])\n", "def test_logistic_regression_penalties(est, penalty):\n    # check that different penalties lead to different results\n\n        return clone(est).set_params(**kw).fit(X, y)\n\n    X = iris.data[::10]\n    X2 = iris.data[1::10]\n    y_multi = iris.target[::10]\n    est_penalty = fit(X, y_multi, penalty=penalty, solver='saga', multi_class='multinomial')\n    est_no_penalty = fit(X, y_multi, penalty='none', solver='saga', multi_class='multinomial')\n\n    assert not np.allclose(est_penalty.coef_, est_no_penalty.coef_)\n    assert not np.allclose(est_penalty.predict_proba(X2), est_no_penalty.predict_proba(X2))\n", "def test_logistic_regression_path_liblinear_multiclass():\n    # Test that logistic_regression_path raises a ValueError for\n    # multiclass data with liblinear solver.\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,\n                               n_redundant=0, n_clusters_per_class=1,\n                               random_state=0, n_features=2)\n    Cs = [.00001, 1, 10000]\n    with pytest.raises(ValueError, match=\"Solver liblinear does not support\"\n                                         \" a multinomial backend.\"):\n        _logistic_regression_path(X, y, penalty='l1', Cs=Cs,\n                                  solver='liblinear', random_state=0,\n                                  multi_class='multinomial')\n", "def test_logistic_regression_path_zero_penalty():\n    # Test that logistic_regression_path returns the correct coefs when\n    # penalty is set to zero. In this case, the coefs should be the same as\n    # the coefs obtained from LinearRegression.\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    coefs, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=[0], solver='saga')\n    lr = LinearRegression().fit(X, y)\n    assert_array_almost_equal(coefs[0], lr.coef_)\n", "def test_logistic_regression_path_intercept_scaling(multi_class):\n    # Test the effect of intercept_scaling on logistic_regression_path\n\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    Cs = np.logspace(-3, 2, 5)\n    coefs_default, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=Cs,\n                                                    solver='liblinear', random_state=0,\n                                                    multi_class=multi_class)\n    coefs_scaled, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=Cs,\n                                                    solver='liblinear', random_state=0,\n                                                    multi_class=multi_class, intercept_scaling=2)\n\n    # The coefficients should be different because of the intercept_scaling\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(coefs_default, coefs_scaled)\n"], "sample_823": ["def test_check_preserve_type_sparse():\n    # Ensures that type float32 is preserved for sparse matrices.\n    XA = csr_matrix(np.resize(np.arange(40), (5, 8)).astype(np.float32))\n    XB = csr_matrix(np.resize(np.arange(40), (5, 8)).astype(np.float32))\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, None)\n    assert_equal(XA_checked.dtype, np.float32)\n\n    # both float32\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked.dtype, np.float32)\n    assert_equal(XB_checked.dtype, np.float32)\n", "def test_check_invalid_dtype():\n    # Ensure an error is raised on invalid dtype input arrays.\n    XA = np.arange(45).reshape(9, 5).astype(np.complex128)\n    XB = np.arange(32).reshape(4, 8).astype(np.complex128)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n", "def test_check_invalid_input():\n    # Ensure an error is raised on invalid input types.\n    XA = \"invalid input\"\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    XA = np.resize(np.arange(45), (5, 9))\n    XB = \"invalid input\"\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    XA = np.resize(np.arange(45), (5, 9))\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, XB)\n", "def test_polynomial_kernel():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n    K = polynomial_kernel(X, Y, degree=2, gamma=0.5, coef0=1)\n    assert K.shape == (5, 3)\n    assert_array_almost_equal(K[0, 0], np.dot(X[0], Y[0]) ** 2 * 0.5 + 1)\n", "def test_polynomial_kernel():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    K = polynomial_kernel(X, X, degree=2, gamma=0.5, coef0=1)\n    # the diagonal elements of a polynomial kernel are (gamma * x.dot(x) + coef0) ** degree\n    diag = [(0.5 * np.dot(x, x) + 1) ** 2 for x in X]\n    assert_array_almost_equal(K.flat[::6], diag)\n"], "sample_202": ["def test_empty_messages(self):\n    \"\"\"\n    An empty list of messages is properly handled.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    set_cookie_data(storage, [])\n    self.assertEqual(list(storage), [])\n\n    storage = self.storage_class(self.get_request())\n    set_cookie_data(storage, [], encode_empty=True)\n    self.assertEqual(list(storage), [])\n", "def test_hash_consistency(self):\n    \"\"\"\n    Test that the hash function is consistent for a given value.\n    \"\"\"\n    storage = self.get_storage()\n    value = 'test_value'\n    hash1 = storage._hash(value)\n    hash2 = storage._hash(value)\n    self.assertEqual(hash1, hash2)\n", "    def test_large_message(self):\n        \"\"\"\n        A message larger than the maximum cookie size is not stored.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        msg_size = CookieStorage.max_cookie_size + 1\n        storage.add(constants.INFO, 'a' * msg_size)\n        unstored_messages = storage.update(response)\n\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(unstored_messages[0].message, 'a' * msg_size)\n", "def test_empty_cookie(self):\n    \"\"\"\n    The storage should handle empty cookies correctly.\n    \"\"\"\n    request = self.get_request()\n    storage = self.storage_class(request)\n\n    # Empty cookie\n    request.COOKIES[CookieStorage.cookie_name] = ''\n    self.assertEqual(list(storage), [])\n\n    # None cookie\n    del request.COOKIES[CookieStorage.cookie_name]\n    self.assertEqual(list(storage), [])\n", "def test_message_ordering(self):\n    \"\"\"\n    Messages are stored and retrieved in the same order.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Add messages in a specific order\n    messages = [Message(constants.INFO, str(i)) for i in range(5)]\n    for msg in messages:\n        storage.add(msg.level, msg.message)\n    storage.update(response)\n\n    # Check that messages are retrieved in the same order\n    new_storage = self.storage_class(self.get_request())\n    new_storage._request = response.wsgi_request\n    new_messages = list(new_storage)\n\n    self.assertEqual(new_messages, messages)\n"], "sample_815": ["def test_balanced_accuracy_score_numeric(y_true, y_pred):\n    macro_recall = recall_score(y_true, y_pred, average='macro',\n                                labels=np.unique(y_true))\n    with ignore_warnings():\n        # Warnings are tested in test_balanced_accuracy_score_unseen\n        balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == pytest.approx(macro_recall)\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    chance = balanced_accuracy_score(y_true, np.full_like(y_true, y_true[0]))\n    assert adjusted == (balanced - chance) / (1 - chance)\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = ['a', 'b', 'a', 'b']\n    y_pred = ['a', 'a', 'a', 'b']\n    sample_weight = [1, 1, 2, 1]\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    assert balanced == pytest.approx(0.75)\n", "def test_balanced_accuracy_score_adjusted_unseen():\n    assert_warns_message(UserWarning, 'y_pred contains classes not in y_true',\n                         balanced_accuracy_score, [0, 0, 0], [0, 0, 1], adjusted=True)\n", "def test_balanced_accuracy_score_multilabel_indicator():\n    y_true = np.array([[0, 1, 1], [1, 0, 0], [0, 0, 1]])\n    y_pred = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n    balanced = balanced_accuracy_score(y_true, y_pred, average='samples')\n    expected_balanced = (1 + 1 + 1) / 3\n    assert balanced == pytest.approx(expected_balanced)\n", "def test_balanced_accuracy_score_with_sample_weight(y_type):\n    if y_type == 'binary':\n        y_true = [0, 0, 1, 1]\n        y_pred = [0, 0, 1, 1]\n    elif y_type == 'multiclass':\n        y_true = [0, 1, 2, 2]\n        y_pred = [0, 1, 2, 3]\n    else:  # y_type == 'multilabel-indicator'\n        y_true = np.array([[0, 1], [1, 1], [0, 0], [1, 0]])\n        y_pred = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n\n    sample_weight = [0.2, 0.3, 0.4, 0.1]\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n\n    # Manually calculate balanced accuracy with sample weights\n    recalls = recall_score(y_true, y_pred, average=None, sample_weight=sample_weight)\n    balanced_manual = np.mean(recalls)\n\n    assert balanced == pytest.approx(balanced_manual)\n"], "sample_65": ["def test_i18n_language_english_default_with_new_app(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    when a new app is added to the installed apps.\n    \"\"\"\n    app6_trans_string = 'This string from app6 must be translated'\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, app6_trans_string)\n", "def test_setlang_with_invalid_language_code(self):\n    \"\"\"\n    The set_language view should not set the language in the session or the\n    cookie if the language code is invalid.\n    \"\"\"\n    invalid_lang_code = 'xx'\n    post_data = {'language': invalid_lang_code, 'next': '/'}\n    response = self.client.post('/i18n/setlang/', post_data, HTTP_REFERER='/i_should_not_be_used/')\n    with ignore_warnings(category=RemovedInDjango40Warning):\n        self.assertNotIn(LANGUAGE_SESSION_KEY, self.client.session)\n    self.assertNotIn(settings.LANGUAGE_COOKIE_NAME, self.client.cookies)\n", "def test_i18n_language_different_plural_forms(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a correct language catalog\n    if the requested language has a different number of plural forms than\n    the default language.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('ru'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, '\"{count} plural3\": [\"{count} plural3 p3\", \"{count} plural3 p3s\", \"{count} plural3 p3t\"]')\n", "def test_i18n_language_english_default_multiple_packages(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    if the default language is en-us, the selected language has a\n    translation available and a catalog composed by djangojs domain\n    translations of multiple Python packages is requested. Ensure that the\n    catalog can handle plural forms correctly. See #13388, #3594 and #13514\n    for more details.\n    \"\"\"\n    base_trans_string = 'il faut traduire cette cha\\\\u00eene de caract\\\\u00e8res de '\n    app1_trans_string = base_trans_string + 'app1'\n    app6_trans_string = base_trans_string + 'app6'\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n_multi_packages3/')\n        self.assertContains(response, app1_trans_string)\n        self.assertContains(response, app6_trans_string)\n\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, app6_trans_string)\n        self.assertNotContains(response, app1_trans_string)\n\n        response = self.client.get('/jsi18n/app1+app6/')\n        self.assertContains(response, app1_trans_string)\n        self.assertContains(response, app6_trans_string)\n\n        # Check plural forms\n        self.assertEqual(\n            response.context['catalog']['{count} plural3'],\n            ['{count} plural3 p3', '{count} plural3 p3s', '{count} plural3 p3t']\n        )\n        self.assertEqual(\n            response.context['catalog']['{count} plural2'],\n            ['{count} plural2', '{count} plural2s']\n        )\n", "    def test_setlang_https_redirect(self):\n        \"\"\"The set_language view redirects to HTTPS if the request is sent over HTTPS.\"\"\"\n        lang_code = self._get_inactive_language_code()\n        post_data = {'language': lang_code, 'next': '/'}\n        response = self.client.post('/i18n/setlang/', post_data, secure=True)\n        self.assertRedirects(response, 'https://testserver/')\n"], "sample_806": ["def test_gradient_boosting_with_custom_loss_function():\n    # Check that custom loss functions work.\n    # Define a custom loss function\n        return np.mean((y_true - y_pred) ** 2)\n\n    X, y = make_regression(random_state=0)\n    gb = GradientBoostingRegressor(loss=custom_loss)\n    gb.fit(X, y)\n    assert hasattr(gb, 'train_score_')\n", "def test_gradient_boosting_with_custom_init_estimator():\n    # Test that the init parameter can be a custom estimator\n\n    class CustomInitEstimator(BaseEstimator):\n            return self\n\n            return np.zeros(X.shape[0])\n\n    X, y = make_regression(random_state=0)\n    gb = GradientBoostingRegressor(init=CustomInitEstimator())\n    gb.fit(X, y)  # custom init estimator works fine\n\n    with pytest.raises(\n            ValueError,\n            match='The initial estimator CustomInitEstimator does not support '\n                  'sample weights'):\n        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))\n", "def test_n_iter_no_change_validation_fraction(GBEstimator):\n    # Test if n_iter_no_change and validation_fraction are properly handled.\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n\n    # validation_fraction must be a float in (0, 1).\n    est = GBEstimator(n_iter_no_change=10, validation_fraction='invalid')\n    assert_raises(ValueError, est.fit, X, y)\n    est = GBEstimator(n_iter_no_change=10, validation_fraction=0)\n    assert_raises(ValueError, est.fit, X, y)\n    est = GBEstimator(n_iter_no_change=10, validation_fraction=1.5)\n    assert_raises(ValueError, est.fit, X, y)\n\n    # n_iter_no_change must be an integer or None.\n    est = GBEstimator(n_iter_no_change=10.5, validation_fraction=0.2)\n    assert_raises(ValueError, est.fit, X, y)\n", "def test_regressor_with_alpha_loss(presort, loss):\n    # Test GradientBoostingRegressor with 'huber' and 'quantile' loss functions with alpha parameter\n    X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    for alpha in [0.1, 0.5, 0.9]:\n        reg = GradientBoostingRegressor(loss=loss, alpha=alpha, random_state=42, presort=presort)\n        reg.fit(X_train, y_train)\n        y_pred = reg.predict(X_test)\n\n        # Check if predictions are within a reasonable range\n        assert np.all(np.abs(y_pred) <= np.max(np.abs(y_train)) + 1.0)\n", "def test_gradient_boosting_init_consistency():\n    # Check that the init estimator provides consistent predictions for\n    # different datasets with the same shape (see issue #13970)\n\n    X1, y1 = make_regression(n_samples=100, n_features=5, random_state=0)\n    X2, y2 = make_regression(n_samples=200, n_features=5, random_state=0)\n\n    init = DummyRegressor(strategy=\"median\")\n    gb1 = GradientBoostingRegressor(init=init, random_state=0).fit(X1, y1)\n    gb2 = GradientBoostingRegressor(init=init, random_state=0).fit(X2, y2)\n\n    assert_array_almost_equal(gb1.init_.predict(X1), gb2.init_.predict(X2))\n"], "sample_547": ["def test_annotationbbox_fontsize():\n    fig, ax = plt.subplots()\n    ab = AnnotationBbox(DrawingArea(20, 20, 0, 0, clip=True), (0.5, 0.5),\n                        xycoords='data', fontsize=12)\n    assert ab.get_fontsize() == 12\n\n    ab.set_fontsize(14)\n    assert ab.get_fontsize() == 14\n", "def test_offsetbox_get_offset():\n    fig, ax = plt.subplots()\n    ob = OffsetBox()\n    bbox = Bbox.from_bounds(0, 0, 100, 50)\n    renderer = fig.canvas.get_renderer()\n\n    # Test with fixed offset\n    ob.set_offset((20, 10))\n    assert ob.get_offset(bbox, renderer) == (20, 10)\n\n    # Test with callable offset\n    ob.set_offset(lambda width, height, xdescent, ydescent, renderer: (width / 2, height / 2))\n    assert ob.get_offset(bbox, renderer) == (50, 25)\n", "def test_anchoredtext_vertical_alignment():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test\\ntest long text\", loc=\"lower center\",\n                         pad=0.2, prop={\"va\": \"bottom\"})\n    ax.add_artist(text0)\n    text1 = AnchoredText(\"test\\ntest long text\", loc=\"center\",\n                         pad=0.2, prop={\"va\": \"center\"})\n    ax.add_artist(text1)\n    text2 = AnchoredText(\"test\\ntest long text\", loc=\"upper center\",\n                         pad=0.2, prop={\"va\": \"top\"})\n    ax.add_artist(text2)\n", "def test_offsetimage():\n    fig, ax = plt.subplots()\n\n    arr = np.random.rand(10, 10)\n    im = OffsetImage(arr, zoom=2)\n    ab = AnchoredOffsetbox('center', child=im, frameon=False)\n    ax.add_artist(ab)\n", "def test_anchoredtext_vertical_alignment():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test long text\\ntest\", loc=\"lower left\",\n                         pad=0.2, prop={\"va\": \"top\"})\n    ax.add_artist(text0)\n    text1 = AnchoredText(\"test long text\\ntest\", loc=\"center left\",\n                         pad=0.2, prop={\"va\": \"center\"})\n    ax.add_artist(text1)\n    text2 = AnchoredText(\"test long text\\ntest\", loc=\"upper left\",\n                         pad=0.2, prop={\"va\": \"bottom\"})\n    ax.add_artist(text2)\n"], "sample_275": ["    def test_proxy_model_delete(self):\n        \"\"\"\n        Test deletion of instances of a proxy model.\n        \"\"\"\n        foo_file_proxy = FooFileProxy.objects.create(my_file=File.objects.create())\n        self.assertEqual(FooFileProxy.objects.count(), 1)\n        self.assertEqual(File.objects.count(), 1)\n        foo_file_proxy.delete()\n        self.assertEqual(FooFileProxy.objects.count(), 0)\n        self.assertEqual(File.objects.count(), 0)\n", "def test_delete_proxy_of_proxy_of_proxy(self):\n    \"\"\"\n    Deleting a proxy-of-proxy-of-proxy instance should bubble through to its proxy\n    and non-proxy parents, deleting *all* referring objects.\n    \"\"\"\n    test_image = self.create_image()\n\n    # Get the Image as a Photo and then as a FooPhotoProxy\n    test_photo = Photo.objects.get(pk=test_image.pk)\n    foo_photo = FooPhoto(my_photo=test_photo)\n    foo_photo.save()\n    foo_photo_proxy = FooPhotoProxy.objects.get(pk=foo_photo.pk)\n\n    FooPhotoProxy.objects.all().delete()\n\n    # A FooPhotoProxy deletion == FooPhoto deletion == Photo deletion == Image deletion == File deletion\n    self.assertEqual(len(FooPhotoProxy.objects.all()), 0)\n    self.assertEqual(len(FooPhoto.objects.all()), 0)\n    self.assertEqual(len(Photo.objects.all()), 0)\n    self.assertEqual(len(Image.objects.all()), 0)\n    self.assertEqual(len(File.objects.all()), 0)\n\n    # The FooPhotoProxy deletion should have cascaded and deleted *all*\n    # references to it.\n    self.assertEqual(len(FooFile.objects.all()), 0)\n    self.assertEqual(len(FooImage.objects.all()), 0)\n", "    def test_select_related_delete(self):\n        # Test deleting objects that have been prefetched using select_related()\n        foo = FooFile.objects.create(my_file=File.objects.create())\n        FooFile.objects.select_related('my_file').get(id=foo.id).delete()\n        self.assertFalse(FooFile.objects.filter(id=foo.id).exists())\n        self.assertFalse(File.objects.filter(id=foo.my_file.id).exists())\n", "    def test_on_delete_set_null(self):\n        \"\"\"\n        When using on_delete=models.SET_NULL, deleting an object sets the\n        foreign key field to NULL.\n        \"\"\"\n        contact1 = Contact.objects.create(label='Contact 1')\n        researcher = Researcher.objects.create(\n            primary_contact=contact1,\n            secondary_contact=None,\n        )\n        contact1.delete()\n        researcher.refresh_from_db()\n        self.assertIsNone(researcher.primary_contact)\n        self.assertIsNone(researcher.secondary_contact)\n", "def test_filtered_relation_error_with_prefetch(self):\n    \"\"\"\n    Ensure that a ValueError is raised when a FilteredRelation is used with\n    prefetch_related.\n    \"\"\"\n    person = Person.objects.create(name='John Doe')\n    email = Email.objects.create(label=\"home-email\", email_address=\"john@example.com\")\n    person.contacts.add(email)\n\n    with self.assertRaises(ValueError):\n        Person.objects.prefetch_related('contacts__email')\n"], "sample_1049": ["def test_plane_parameter_value():\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    t = symbols('t', real=True)\n    on_circle = p.arbitrary_point(t).subs(t, pi/4)\n    assert p.parameter_value(on_circle, t) == {t: pi/4}\n    off_circle = p.p1 + (on_circle - p.p1)*2\n    assert p.parameter_value(off_circle, t) == {t: pi/4}\n    u, v = symbols('u v', real=True)\n    assert p.parameter_value(on_circle, u, v) == {u: sqrt(10)/10, v: sqrt(10)/30}\n    assert p.parameter_value(off_circle, u, v) == {u: sqrt(10)/5, v: sqrt(10)/15}\n    raises(ValueError, lambda: p.parameter_value(Point3D(1, 2, 3), t))\n    raises(ValueError, lambda: p.parameter_value(Point3D(1, 2, 3), u, v))\n", "def test_plane_parameter_value():\n    from sympy import Plane, Point, pi\n    from sympy.abc import t, u, v\n\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    on_circle = p.arbitrary_point(t).subs(t, pi/4)\n\n    assert p.parameter_value(on_circle, t) == {t: pi/4}\n\n    off_circle = p.p1 + (on_circle - p.p1)*2\n    assert p.parameter_value(off_circle, t) == {t: pi/4}\n\n    assert p.parameter_value(on_circle, u, v) == {u: sqrt(10)/10, v: sqrt(10)/30}\n    assert p.parameter_value(off_circle, u, v) == {u: sqrt(10)/5, v: sqrt(10)/15}\n\n    raises(ValueError, lambda: p.parameter_value(Point(1, 2, 3), t))\n    raises(ValueError, lambda: p.parameter_value(Point(1, 2, 3), u, v))\n", "def test_parameter_value():\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    on_circle = p.arbitrary_point(pi/4)\n    assert p.parameter_value(on_circle, t) == {t: pi/4}\n    off_circle = p.p1 + (on_circle - p.p1)*2\n    assert p.parameter_value(off_circle, t) == {t: pi/4}\n    assert p.parameter_value(on_circle, u, v) == {u: sqrt(10)/10, v: sqrt(10)/30}\n    assert p.parameter_value(off_circle, u, v) == {u: sqrt(10)/5, v: sqrt(10)/15}\n    # Test that a point not on the plane raises a ValueError\n    raises(ValueError, lambda: p.parameter_value(Point3D(0, 0, 2), t))\n", "def test_parameter_value():\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    pt = Point3D(1, 2, 3)\n    params = p.parameter_value(pt, Dummy('u'), Dummy('v'))\n    assert p.arbitrary_point(**params) == pt\n", "def test_plane_parameter_value():\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    on_circle = p.arbitrary_point(pi/4)\n    assert p.parameter_value(on_circle, t) == {t: pi/4}\n\n    off_circle = p.p1 + (on_circle - p.p1)*2\n    assert p.parameter_value(off_circle, t) == {t: pi/4}\n\n    assert p.parameter_value(on_circle, u, v) == {u: sqrt(10)/10, v: sqrt(10)/30}\n    assert p.parameter_value(off_circle, u, v) == {u: sqrt(10)/5, v: sqrt(10)/15}\n\n    raises(ValueError, lambda: p.parameter_value(Point(0, 0), t))\n"], "sample_165": ["def test_modelform_defines_fields(self):\n    class TestForm(ModelForm):\n        class Meta:\n            model = ChoiceModel\n            fields = ['name']\n\n    self.assertTrue(modelform_defines_fields(TestForm))\n\n    class TestForm2(ModelForm):\n        class Meta:\n            model = ChoiceModel\n            exclude = ['name']\n\n    self.assertTrue(modelform_defines_fields(TestForm2))\n\n    class TestForm3(ModelForm):\n        pass\n\n    self.assertFalse(modelform_defines_fields(TestForm3))\n", "def test_modelchoicefield_limit_choices_to(self):\n    # Create choices for the model choice field test.\n    ChoiceModel.objects.create(pk=1, name='a', is_valid=True)\n    ChoiceModel.objects.create(pk=2, name='b', is_valid=False)\n    ChoiceModel.objects.create(pk=3, name='c', is_valid=True)\n\n    # ModelChoiceField with limit_choices_to\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': 'INVALID CHOICE',\n    }\n    f = ModelChoiceField(queryset=ChoiceModel.objects.all(), limit_choices_to={'is_valid': True}, error_messages=e)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID CHOICE'], f.clean, '2')\n\n    # ModelMultipleChoiceField with limit_choices_to\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': '%(value)s IS INVALID CHOICE',\n        'invalid_list': 'NOT A LIST OF VALUES',\n    }\n    f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), limit_choices_to={'is_valid': True}, error_messages=e)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['NOT A LIST OF VALUES'], f.clean, '3')\n    self.assertFormErrors(['2 IS INVALID CHOICE'], f.clean, ['2'])\n", "def test_limit_choices_to(self):\n    # Create choices for the model choice field tests below.\n    ChoiceModel.objects.create(pk=1, name='a', is_published=True)\n    ChoiceModel.objects.create(pk=2, name='b', is_published=False)\n    ChoiceModel.objects.create(pk=3, name='c', is_published=True)\n\n    # ModelChoiceField with limit_choices_to\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': 'INVALID CHOICE',\n    }\n    f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e, limit_choices_to={'is_published': True})\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID CHOICE'], f.clean, '2')  # choice not published\n    self.assertFormErrors([], f.clean, '1')  # choice published\n\n    # ModelMultipleChoiceField with limit_choices_to\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': '%(value)s IS INVALID CHOICE',\n        'invalid_list': 'NOT A LIST OF VALUES',\n    }\n    f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e, limit_choices_to={'is_published': True})\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['NOT A LIST OF VALUES'], f.clean, '3')\n    self.assertFormErrors(['2 IS INVALID CHOICE'], f.clean, ['2'])  # choice not published\n    self.assertFormErrors([], f.clean, ['1'])  # choice published\n", "def test_invalid_pk_value_error_message(self):\n    f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), error_messages={'invalid_pk_value': '\u201c%(pk)s\u201d is not a valid choice.'})\n    self.assertFormErrors(['\u201cinvalid_pk\u201d is not a valid choice.'], f.clean, ['invalid_pk'])\n", "    def test_limit_choices_to_callable(self):\n            return {'name__startswith': 'a'}\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), limit_choices_to=limit_choices_to)\n        self.assertEqual(list(f.choices), [('', '---------'), (1, 'a')])\n\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), limit_choices_to=limit_choices_to)\n        self.assertEqual(list(f.choices), [(1, 'a')])\n"], "sample_759": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['b', 'a']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[2, 1]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_specified_categories_mismatch():\n    # Test error when the number of categories does not match the number of features\n    X = np.array([['a', 'b']], dtype=object).T\n    enc = OneHotEncoder(categories=[['a', 'b', 'c']])\n    msg = \"Shape mismatch: if n_values is an array, it has to be of shape \\\\(n_features,\\\\).\"\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_unseen_categories():\n    X = np.array([['a', 1], ['b', 2]])\n    enc = OneHotEncoder(categories=[['a', 'b', 'c'], [1, 2, 3]])\n    enc.fit(X)\n\n    # test transforming with unseen categories\n    X_test = np.array([['d', 4]])\n    enc.handle_unknown = 'ignore'\n    exp = np.array([[0., 0., 0., 0., 0., 0.]])\n    assert_array_equal(enc.transform(X_test).toarray(), exp)\n\n    # test transforming with unseen categories and error\n    enc.handle_unknown = 'error'\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n"], "sample_859": ["def test_lassoCV_copy_X_True():\n    X, y, _, _ = build_dataset()\n    X = X.copy(order='F')\n\n    original_X = X.copy()\n    lasso_cv = LassoCV(copy_X=True)\n    lasso_cv.fit(X, y)\n\n    assert_array_equal(original_X, X)\n", "def test_lassoCV_does_not_set_precompute_with_array(monkeypatch, precompute,\n                                                     inner_precompute):\n    X, y, _, _ = build_dataset()\n    calls = 0\n\n    class LassoMock(Lasso):\n            super().fit(X, y)\n            nonlocal calls\n            calls += 1\n            assert self.precompute == inner_precompute\n\n    monkeypatch.setattr('sklearn.linear_model.coordinate_descent.Lasso', LassoMock)\n    LassoCV(precompute=precompute, cv=2).fit(X, y)\n    assert calls == 2\n", "def test_enetCV_does_not_set_precompute(monkeypatch, precompute,\n                                        inner_precompute):\n    X, y, _, _ = build_dataset()\n    calls = 0\n\n    class ElasticNetCVMock(ElasticNetCV):\n            super().fit(X, y)\n            nonlocal calls\n            calls += 1\n            assert self.precompute == inner_precompute\n\n    monkeypatch.setattr(\"sklearn.linear_model.coordinate_descent.ElasticNetCV\", ElasticNetCVMock)\n    model = ElasticNetCV(precompute=precompute, cv=3)\n    model.fit(X, y)\n    assert calls == 1\n", "def test_lassoCV_max_iter(monkeypatch):\n    X, y, _, _ = build_dataset()\n    max_iter = 10\n    calls = 0\n\n    class LassoMock(Lasso):\n            super().fit(X, y)\n            nonlocal calls\n            calls += 1\n            assert self.max_iter == max_iter\n\n    monkeypatch.setattr(\"sklearn.linear_model.coordinate_descent.Lasso\", LassoMock)\n\n    LassoCV(max_iter=max_iter).fit(X, y)\n    assert calls == 10\n", "def test_lassoCV_does_not_overwrite_precompute(monkeypatch, precompute):\n    X, y, _, _ = build_dataset()\n    calls = 0\n\n    class LassoMock(Lasso):\n            super().fit(X, y)\n            nonlocal calls\n            calls += 1\n            assert self.precompute == precompute\n\n    monkeypatch.setattr(\"sklearn.linear_model.coordinate_descent.Lasso\", LassoMock)\n\n    clf = LassoCV(precompute=precompute, cv=3)\n    clf.fit(X, y)\n    assert calls > 0\n"], "sample_522": ["def test_colorbar_set_formatter_locator_horizontal():\n    # check that the locator properties echo what is on the axis:\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, orientation='horizontal')\n    cb.ax.xaxis.set_major_locator(FixedLocator(np.arange(10)))\n    cb.ax.xaxis.set_minor_locator(FixedLocator(np.arange(0, 10, 0.2)))\n    assert cb.locator is cb.ax.xaxis.get_major_locator()\n    assert cb.minorlocator is cb.ax.xaxis.get_minor_locator()\n    cb.ax.xaxis.set_major_formatter(LogFormatter())\n    cb.ax.xaxis.set_minor_formatter(LogFormatter())\n    assert cb.formatter is cb.ax.xaxis.get_major_formatter()\n    assert cb.minorformatter is cb.ax.xaxis.get_minor_formatter()\n\n    # check that the setter works as expected:\n    loc = FixedLocator(np.arange(7))\n    cb.locator = loc\n    assert cb.ax.xaxis.get_major_locator() is loc\n    loc = FixedLocator(np.arange(0, 7, 0.1))\n    cb.minorlocator = loc\n    assert cb.ax.xaxis.get_minor_locator() is loc\n    fmt = LogFormatter()\n    cb.formatter = fmt\n    assert cb.ax.xaxis.get_major_formatter() is fmt\n    fmt = LogFormatter()\n    cb.minorformatter = fmt\n    assert cb.ax.xaxis.get_minor_formatter() is fmt\n", "def test_colorbar_set_ticks_locator():\n    # check that the locator properties echo what is on the axis:\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_ticks(np.arange(10))\n    assert isinstance(cb.locator, FixedLocator)\n    assert np.allclose(cb.locator.locs, np.arange(10))\n", "def test_colorbar_set_alpha():\n    # Test setting the alpha value of the colorbar\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n", "def test_colorbar_with_text():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, label='Random Data')\n    fig.canvas.draw()\n    assert 'Random Data' in cb.ax.get_ylabel()\n", "def test_vertical_colorbar_with_fraction():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.7)\n    fig.draw_without_rendering()\n    assert cb.ax.get_position().height == 0.7 * ax.get_position().height\n"], "sample_814": ["def test_gradient_boosting_with_n_iter_no_change_zero():\n    # Check that n_iter_no_change=0 is equivalent to not using early stopping\n\n    X, y = make_classification(n_samples=100, random_state=0)\n    gb_no_early_stopping = GradientBoostingClassifier(n_estimators=100, random_state=0)\n    gb_early_stopping = GradientBoostingClassifier(n_estimators=100, n_iter_no_change=0, random_state=0)\n\n    gb_no_early_stopping.fit(X, y)\n    gb_early_stopping.fit(X, y)\n\n    assert_array_almost_equal(gb_no_early_stopping.predict(X), gb_early_stopping.predict(X))\n", "def test_gradient_boosting_n_estimators_increment():\n    # Check that n_estimators increases monotonically with n_iter_no_change\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    gbc1 = GradientBoostingClassifier(n_estimators=100, n_iter_no_change=10,\n                                      learning_rate=0.1, max_depth=3,\n                                      random_state=42)\n    gbc2 = GradientBoostingClassifier(n_estimators=100, n_iter_no_change=20,\n                                      learning_rate=0.1, max_depth=3,\n                                      random_state=42)\n\n    gbc1.fit(X, y)\n    gbc2.fit(X, y)\n\n    assert gbc1.n_estimators_ < gbc2.n_estimators_\n", "def test_gradient_boosting_with_one_class():\n    # Check that an error is raised if trying to fit with one class\n\n    X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n    y = [0, 0, 0, 0]\n\n    gbc = GradientBoostingClassifier()\n    with pytest.raises(\n            ValueError,\n            match='The training data contains only 1 class. Please provide '\n                  'the true target values.'):\n        gbc.fit(X, y)\n", "def test_gradient_boosting_apply():\n    # Test the apply method which returns the index of the leaf that each sample falls on\n    # for each tree in the ensemble\n\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                               n_redundant=0, random_state=0)\n    gb = GradientBoostingClassifier(n_estimators=10, random_state=0)\n    gb.fit(X, y)\n\n    leaves = gb.apply(X)\n    assert leaves.shape == (100, 10)\n    assert np.all(leaves >= 0)\n    assert np.all(leaves < gb.estimators_.shape[0])\n", "def test_gradient_boosting_with_init_class_labels():\n    # Check that GradientBoostingClassifier works when init predicts class\n    # labels and y contains class labels.\n\n    X, y = make_classification(n_classes=3)\n    init_est = LinearSVC()\n    gb = GradientBoostingClassifier(init=init_est)\n    gb.fit(X, y)\n    assert gb.score(X, y) >= 0.9  # check that model is trained\n"], "sample_903": ["def test_non_square_distance_matrix():\n    # Distance matrix should be square.\n    tsne = TSNE(metric=\"precomputed\")\n    assert_raises_regexp(ValueError, \".* square distance matrix\",\n                         tsne.fit_transform, np.array([[0.0, 1.0], [2.0, 3.0], [4.0, 5.0]]))\n", "def test_fit_transform_idempotence():\n    # Fit and transform should yield the same result when called twice.\n    random_state = check_random_state(0)\n    X = random_state.randn(50, 2)\n    tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,\n                random_state=0, method='exact')\n    X_embedded1 = tsne.fit_transform(X)\n    X_embedded2 = tsne.fit_transform(X)\n    assert_array_almost_equal(X_embedded1, X_embedded2)\n", "def test_tsne_manhattan_metric():\n    # Test t-SNE with Manhattan distance metric.\n    random_state = check_random_state(0)\n    n_components = 2\n    X = random_state.randn(50, n_components).astype(np.float32)\n    tsne = TSNE(n_components=n_components, metric='manhattan', random_state=0)\n    X_embedded = tsne.fit_transform(X)\n\n    # Compute Manhattan distances between original and embedded data points\n    dist_orig = manhattan_distances(X)\n    dist_embedded = manhattan_distances(X_embedded)\n\n    # Check that the mean and standard deviation of the distances are similar\n    assert_almost_equal(np.mean(dist_orig), np.mean(dist_embedded), decimal=1)\n    assert_almost_equal(np.std(dist_orig), np.std(dist_embedded), decimal=1)\n", "def test_different_metrics():\n    # Test trustworthiness with different metrics\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n    metrics = ['euclidean', 'manhattan', 'cosine']\n    trustworthiness_values = []\n    for metric in metrics:\n        tsne = TSNE(n_components=2, metric=metric)\n        X_embedded = tsne.fit_transform(X)\n        t = trustworthiness(X, X_embedded, metric=metric)\n        trustworthiness_values.append(t)\n    assert_less_equal(np.diff(trustworthiness_values), 0,\n                      msg='Trustworthiness values are not monotonically decreasing with distance metrics')\n", "def test_answer_gradient_cosine_distance():\n    # Test the gradient calculation with cosine distance\n    # These tests & answers have been checked against the reference implementation by LvdM.\n    pos_input = np.array([[1.0, 0.0], [0.0, 1.0], [5.0, 2.0], [7.3, 2.2]])\n    pos_output = np.array([[-4.733518e-06, -4.733518e-06],\n                           [4.733518e-06, 4.733518e-06],\n                           [-1.260696e-05, 1.991044e-05],\n                           [1.764946e-05, -3.388266e-05]])\n    neighbors = np.array([[1, 2, 3],\n                          [0, 2, 3],\n                          [1, 0, 3],\n                          [1, 2, 0]])\n    grad_output = np.array([[2.20867401e-05, -2.20867401e-05],\n                            [-2.20867401e-05, 2.20867401e-05],\n                            [-2.48611850e-05, 2.48611850e-05],\n                            [-3.38826605e-05, -1.76494602e-05]])\n    _run_answer_test(pos_input, pos_output, neighbors, grad_output,\n                     False, 0.1, 2, cosine_distances)\n"], "sample_1084": ["def test_union_Integers_Rationals():\n    assert Union(S.Integers, S.Rationals) == S.Rationals\n", "def test_ComplexRegion_subtract():\n    # Polar form\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n\n    p1 = Interval(0, 1)*Interval(S.Pi, 2*S.Pi)\n\n    assert c1.subtract(c2) == ComplexRegion(p1, polar=True)\n\n    # Rectangular form\n    c3 = ComplexRegion(Interval(2, 5)*Interval(6, 9))\n    c4 = ComplexRegion(Interval(4, 6)*Interval(7, 9))\n\n    p2 = Interval(2, 4)*Interval(6, 9)\n\n    assert c3.subtract(c4) == ComplexRegion(p2)\n", "def test_Rationals_intersection():\n    assert S.Rationals.intersect(S.Reals) == S.Rationals\n    assert S.Rationals.intersect(S.Naturals) == S.Naturals\n    assert S.Rationals.intersect(S.Naturals0) == S.Naturals0\n    assert S.Rationals.intersect(S.Integers) == S.Integers\n    assert S.Rationals.intersect(S.Rationals) == S.Rationals\n    assert S.Rationals.intersect(S.Complexes) == S.Rationals\n    assert S.Rationals.intersect(S.Integers).is_subset(S.Rationals)\n    assert S.Rationals.intersect(S.Integers).is_subset(S.Naturals) is False\n    assert S.Rationals.intersect(S.Integers).is_subset(S.Naturals0) is False\n", "def test_range_rational_intersection():\n    assert Range(0, 10, 2).intersect(S.Rationals) == Range(0, 10, 2)\n    assert Range(1, 10, 2).intersect(S.Rationals) == Range(1, 10, 2)\n    assert Range(0, 10, 3).intersect(S.Rationals) == Range(0, 9, 3)\n    assert Range(1, 10, 3).intersect(S.Rationals) == Range(1, 8, 3)\n    assert Range(0, 10, S.Half).intersect(S.Rationals) == Range(0, 10, 2)\n    assert Range(1, 10, S.Half).intersect(S.Rationals) == Range(1, 9, 2)\n    assert Range(0, 10, Rational(1, 3)).intersect(S.Rationals) == Range(0, 10, Rational(1, 3))\n    assert Range(1, 10, Rational(1, 3)).intersect(S.Rationals) == Range(1, 10, Rational(1, 3))\n    assert Range(0, 10, pi).intersect(S.Rationals) is S.EmptySet\n    assert Range(0, 10, -2).intersect(S.Rationals) == Range(0, 10, 2)\n    assert Range(10, 0, -2).intersect(S.Rationals) == Range(10, 2, -2)\n    assert Range(0, 10, -3).intersect(S.Rationals) == Range(0, 9, 3)\n    assert Range(10, 0, -3).intersect(S.Rationals) == Range(10, 4, -3)\n", "def test_Rationals_intersect_RealSet():\n    assert S.Rationals.intersect(S.Integers) == S.Integers\n    assert S.Rationals.intersect(S.Naturals) == S.Naturals\n    assert S.Rationals.intersect(S.Naturals0) == S.Naturals0\n    assert S.Rationals.intersect(S.Reals) == S.Rationals\n    assert S.Rationals.intersect(Interval(-1, 1)) == Intersection(S.Rationals, Interval(-1, 1))\n    assert S.Rationals.intersect(Interval(0, 1, left_open=True)) == Intersection(S.Rationals, Interval(0, 1, left_open=True))\n"], "sample_1132": ["def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3), (1, -2, 3), (1, 2, -3), (1, -2, -3), (-1, 2, 3), (-1, -2, 3), (-1, 2, -3), (-1, -2, -3)]\n", "def test_permute_signs():\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3), (1, -2, 3), (1, 2, -3), (1, -2, -3)]\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n", "def test_ibin():\n    assert ibin(16, 8) == 2\n    assert ibin(16, 16) == 4\n    assert ibin(16, 32) == 5\n    assert ibin(16, 1) == 0\n    assert ibin(0, 1) == 0\n    assert ibin(1, 1) == 1\n    assert ibin(1, 2) == 1\n    assert ibin(2, 1) == 0\n"], "sample_554": ["def test_annotation_antialiased_update():\n    annot = Annotation(\"foo\\nbar\", (.5, .5), antialiased=True)\n    assert annot._antialiased is True\n\n    annot.update({'antialiased': False})\n    assert annot._antialiased is False\n", "def test_set_transform_rotates_text():\n    txt = Text(0.5, 0.5, \"foo\\nbar\", transform_rotates_text=True)\n    assert txt._transform_rotates_text is True\n\n    txt.set_transform_rotates_text(False)\n    assert txt._transform_rotates_text is False\n\n    txt.set_transform_rotates_text(True)\n    assert txt._transform_rotates_text is True\n", "def test_get_antialiased_after_set_antialiased():\n    txt = Text(.5, .5, \"foo\\nbar\")\n    assert txt.get_antialiased() == mpl.rcParams['text.antialiased']\n\n    txt.set_antialiased(True)\n    assert txt.get_antialiased() is True\n\n    txt.set_antialiased(False)\n    assert txt.get_antialiased() is False\n", "def test_annotation_with_offset_points():\n    fig, ax = plt.subplots()\n    annot = Annotation(\"offset points\", xy=(0.5, 0.5), xytext=(10, 10),\n                       textcoords=\"offset points\")\n    ax.add_artist(annot)\n    fig.canvas.draw()\n    # Add assertions to verify the correct behavior of the annotation\n", "def test_annotation_antialiased_manual():\n    annot = Annotation(\"foo\\nbar\", (.5, .5), antialiased=True)\n    annot.set_antialiased(False)\n    assert annot.get_antialiased() is False\n    assert annot._antialiased is False\n"], "sample_188": ["def test_expression_wrapper_group_by_cols(self):\n    expr = ExpressionWrapper(F('id'), output_field=IntegerField())\n    self.assertEqual(expr.get_group_by_cols(alias=None), [Ref(None, expr)])\n", "def test_wrapper_with_aggregate(self):\n    Number.objects.create(integer=10, float=123.45)\n    qs = Number.objects.annotate(\n        integer_sum=ExpressionWrapper(Sum('integer'), output_field=IntegerField())\n    )\n    self.assertEqual(qs.get().integer_sum, 10)\n", "def test_output_field_type(self):\n    expr = ExpressionWrapper(Value('test'), output_field=CharField(max_length=10))\n    self.assertEqual(expr.output_field, CharField(max_length=10))\n", "def test_annotation_with_filtered_aggregate(self):\n    Company.objects.filter(num_employees__lt=50).update(ceo=Employee.objects.get(firstname='Frank'))\n    inner = Company.objects.filter(\n        ceo=OuterRef('pk')\n    ).values('ceo').annotate(total_employees=Sum('num_employees')).filter(total_employees__gt=100).values('total_employees')\n    outer = Employee.objects.annotate(total_employees=Subquery(inner, output_field=IntegerField())).filter(total_employees__isnull=False)\n    self.assertSequenceEqual(\n        outer.values_list('firstname', flat=True),\n        ['Frank'],\n    )\n", "def test_expression_wrapper_group_by_cols(self):\n    expr = ExpressionWrapper(F('col'), output_field=CharField())\n    self.assertEqual(expr.get_group_by_cols(alias=None), [Ref(None, Col('col'))])\n"], "sample_478": ["def test_actions_unique_with_different_names(self):\n    @admin.action(description=\"Action 1\")\n        pass\n\n    @admin.action(description=\"Action 2\")\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_actions_unique_different_names(self):\n    @admin.action(description=\"Action 1\")\n        pass\n\n    @admin.action(description=\"Action 2\")\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_actions_unique_with_different_names(self):\n    @admin.action\n        pass\n\n    @admin.action(name=\"custom_name\")\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_actions_unique_different_names(self):\n    @admin.action(description=\"Action 1\")\n        pass\n\n    @admin.action(description=\"Action 2\")\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_actions_unique_different_names(self):\n    @admin.action\n        pass\n\n    @admin.action\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n"], "sample_1102": ["def test_issue_18205_explicit_domain():\n    assert cancel((2 + I)*(3 - I), domain='EX') == 7 + I\n    assert cancel((2 + I)*(2 - I), domain='EX') == 5\n", "def test_issue_18205_rational():\n    assert cancel(Rational(2, 3)*Rational(3, 2)) == 1\n    assert cancel(Rational(2, 3)*Rational(6, 4)) == 1\n", "def test_issue_18375():\n    x = Symbol('x')\n    p = poly(x - 1)\n    M = Matrix([[poly(x + 1), poly(x + 1)]])\n    assert p * M == M * p == Matrix([[poly(x**2 - 1), poly(x**2 - 1)]])\n", "def test_issue_18205_cancel_complex_number():\n    # Test with complex numbers\n    assert cancel((2 + I)*(3 - I)) == 7 + I\n    assert cancel((2 + I)*(2 - I)) == 5\n", "def test_issue_18205_different_expression():\n    assert cancel((2 + I)*(3 + I)) == 5 + 5*I\n    assert cancel((2 - I)*(2 + I)) == 4\n"], "sample_462": ["def test_choicefield_initial(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], initial=\"1\")\n    self.assertEqual(\"1\", f.clean(\"\"))\n    self.assertEqual(\"1\", f.clean(None))\n    self.assertEqual(\"2\", f.clean(\"2\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_initial(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], initial=\"1\")\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    self.assertEqual(\"1\", f.clean(\"\"))\n    self.assertEqual(\"1\", f.clean(None))\n", "def test_choicefield_with_integer_choices(self):\n    f = ChoiceField(choices=[(1, \"One\"), (2, \"Two\")])\n    self.assertEqual(\"1\", f.clean(1))\n    self.assertEqual(\"2\", f.clean(2))\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    self.assertEqual(\"2\", f.clean(\"2\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_initial_value(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], initial=\"1\")\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    self.assertEqual(\"1\", f.clean(\"\"))\n    self.assertEqual(\"1\", f.clean(None))\n", "def test_choicefield_callable_choices_change(self):\n    choices = []\n\n        return choices\n\n    class ChoiceFieldForm(Form):\n        choicefield = ChoiceField(choices=choices_as_callable)\n\n    choices = [(\"J\", \"John\"), (\"P\", \"Paul\")]\n    form = ChoiceFieldForm()\n    self.assertEqual([(\"J\", \"John\"), (\"P\", \"Paul\")], list(form.fields[\"choicefield\"].choices))\n\n    choices = [(\"G\", \"George\"), (\"R\", \"Ringo\")]\n    form = ChoiceFieldForm()\n    self.assertEqual([(\"G\", \"George\"), (\"R\", \"Ringo\")], list(form.fields[\"choicefield\"].choices))\n"], "sample_633": ["def test_ignore_docstrings_and_imports() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-docstrings\", \"--ignore-imports\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\"", "def test_hide_code_with_signatures() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\"] + 2 * [HIDE_CODE_WITH_SIGNATURES])\n    assert ex.value.code == 0\n    assert \"TOTAL lines=32 duplicates=16 percent=50.00\" in output.getvalue()\n", "def test_ignore_signatures_with_imports() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", \"--ignore-imports\", SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert \"TOTAL lines=35 duplicates=0 percent=0.00\" in output.getvalue()\n", "def test_ignore_signatures_with_imports_fail() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''", "def test_ignore_signatures_class_methods_different_order_fail() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR_CLS_A, SIMILAR_CLS_B])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''"], "sample_930": ["def test_create_index_with_category(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :category: software\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :category: language\\n\"\n            \".. index:: Sphinx\\n\"\n            \"   :category: documentation\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('documentation', [('Sphinx', [[('', '#index-0')], [], 'documentation'])])\n    assert index[1] == ('language', [('Python', [[('', '#index-1')], [], 'language'])])\n    assert index[2] == ('software', [('docutils', [[('', '#index-2')], [], 'software'])])\n", "def test_create_index_with_category_key(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :name: ref1\\n\"\n            \"   :category: Library\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :name: ref2\\n\"\n            \"   :category: Language\\n\"\n            \".. index:: Sphinx\\n\"\n            \"   :category: Tool\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('Library', [('docutils', [[('', '#ref1')], [], None])])\n    assert index[1] == ('Language', [('Python', [[('', '#ref2')], [], None])])\n    assert index[2] == ('Tool', [('Sphinx', [[('', '#index-0')], [], None])])\n\n    # check the category key is used for sorting\n    assert index[0][0] < index[1][0] < index[2][0]\n", "def test_create_index_with_category(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :category: refcat1\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :category: refcat2\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('refcat1', [('docutils', [[('', '#index-0')], [], 'refcat1'])])\n    assert index[1] == ('refcat2', [('Python', [[('', '#index-1')], [], 'refcat2'])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n", "def test_create_index_with_category(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :category: tools\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :category: languages\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    assert len(index) == 3\n    assert index[0] == ('languages', [('Python', [[('', '#index-1')], [], 'languages'])])\n    assert index[1] == ('tools', [('docutils', [[('', '#index-0')], [], 'tools'])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n", "def test_create_index_with_categories(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :category: Markup Languages\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :category: Programming Languages\\n\"\n            \".. index:: single: Sphinx\\n\"\n            \"   :category: Documentation Tools\\n\"\n            \".. index:: single: \u00eanigma\\n\"\n            \"   :category: Programming Languages\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('Documentation Tools',\n                        [('Sphinx', [[('', '#index-0')], [], 'Documentation Tools'])])\n    assert index[1] == ('Markup Languages',\n                        [('docutils', [[('', '#index-1')], [], 'Markup Languages'])])\n    assert index[2] == ('Programming Languages',\n                        [('Python', [[('', '#index-2')], [], 'Programming Languages']),\n                         ('\u00eanigma', [[('', '#index-3')], [], 'Programming Languages'])])\n"], "sample_317": ["def test_atom_feed_item_author(self):\n    \"\"\"\n    Test if author information is present in the Atom feed item.\n    \"\"\"\n    response = self.client.get('/syndication/atom/')\n    feed = minidom.parseString(response.content).firstChild\n    items = feed.getElementsByTagName('entry')\n    for item in items:\n        author_elements = item.getElementsByTagName('author')\n        self.assertEqual(len(author_elements), 1)\n        author = author_elements[0]\n        self.assertChildNodes(author, ['name', 'email', 'uri'])\n        self.assertChildNodeContent(author, {\n            'name': 'Sally Smith',\n            'email': 'test@example.com',\n            'uri': 'http://example.com/~sally/',\n        })\n", "def test_item_copyright(self):\n    \"\"\"\n    Test that the item_copyright is included in the feed for RSS and Atom.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/item_copyright/')\n    doc = minidom.parseString(response.content)\n    items = doc.getElementsByTagName('rss')[0].getElementsByTagName('channel')[0].getElementsByTagName('item')\n    self.assertChildNodeContent(items[0], {'copyright': 'Copyright 2022'})\n\n    response = self.client.get('/syndication/atom/item_copyright/')\n    doc = minidom.parseString(response.content)\n    items = doc.getElementsByTagName('feed')[0].getElementsByTagName('entry')\n    self.assertChildNodeContent(items[0], {'rights': 'Copyright 2022'})\n", "def test_atom_feed_author(self):\n    \"\"\"\n    Test the author information in the Atom feed.\n    \"\"\"\n    response = self.client.get('/syndication/atom/')\n    feed = minidom.parseString(response.content).firstChild\n\n    author = feed.getElementsByTagName('author')[0]\n    self.assertChildNodes(author, ['name', 'email', 'uri'])\n    self.assertChildNodeContent(author, {\n        'name': 'Sally Smith',\n        'email': 'test@example.com',\n        'uri': 'http://www.example.com/',\n    })\n", "def test_feed_last_modified_time_updated_date(self):\n    \"\"\"\n    Tests the Last-Modified header with updated dates when published dates are absent.\n    \"\"\"\n    response = self.client.get('/syndication/updated-dates/')\n    self.assertEqual(response.headers['Last-Modified'], 'Mon, 02 Jan 2006 18:00:00 GMT')\n", "def test_author_information(self):\n    \"\"\"\n    Test the author information in RSS and Atom feeds.\n    \"\"\"\n    # Test RSS feed\n    response = self.client.get('/syndication/rss2/')\n    doc = minidom.parseString(response.content)\n    chan = doc.getElementsByTagName('rss')[0].getElementsByTagName('channel')[0]\n    items = chan.getElementsByTagName('item')\n    self.assertEqual(\n        items[0].getElementsByTagName('author')[0].firstChild.wholeText,\n        'test@example.com (Sally Smith)'\n    )\n    self.assertEqual(\n        items[1].getElementsByTagName('dc:creator')[0].firstChild.wholeText,\n        'Sally Smith'\n    )\n\n    # Test Atom feed\n    response = self.client.get('/syndication/atom/')\n    doc = minidom.parseString(response.content)\n    chan = doc.getElementsByTagName('feed')[0]\n    items = chan.getElementsByTagName('entry')\n    author = items[0].getElementsByTagName('author')[0]\n    self.assertEqual(\n        author.getElementsByTagName('name')[0].firstChild.wholeText,\n        'Sally Smith'\n    )\n    self.assertEqual(\n        author.getElementsByTagName('email')[0].firstChild.wholeText,\n        'test@example.com'\n    )\n"], "sample_216": ["def test_add_model_with_field_added_to_base_model(self):\n    \"\"\"\n    Adding a base field takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AddField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_add_date_fields_without_auto_now_not_asking_for_default(self, mocked_ask_method):\n    changes = self.get_changes([self.author_empty], [self.author_dates_of_birth_without_auto_now])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\", \"AddField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now=False)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 1, auto_now=False)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 2, auto_now=False)\n", "def test_alter_field_with_default(self):\n    \"\"\"#22030 - Altering a field with a default should work.\"\"\"\n    changes = self.get_changes([self.author_name], [self.author_name_default_changed])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default='New Default')\n", "def test_add_model_with_options(self):\n    model_state = ModelState('app', 'model', [\n        ('id', models.AutoField(primary_key=True)),\n    ], options={'db_table': 'custom_table'})\n    changes = self.get_changes([], [model_state])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, name='model',\n        options={'db_table': 'custom_table'},\n    )\n    altered_model_state = ModelState('app', 'Model', [\n        ('id', models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([model_state], [altered_model_state])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='model', table=None)\n", "def test_alter_field_with_long_name(self):\n    \"\"\"Tests autodetection of altered fields with long names.\"\"\"\n    long_name = 'a' * 64\n    before = [\n        ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            (long_name, models.CharField(max_length=100)),\n        ])\n    ]\n    after = [\n        ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            (long_name, models.CharField(max_length=200)),\n        ])\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, name=long_name, preserve_default=True)\n"], "sample_1110": ["def test_airyaiprime():\n    from sympy import airyaiprime, airybiprime\n\n    expr1 = airyaiprime(x)\n    expr2 = airybiprime(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.airy(x)[1]'\n    assert prntr.doprint(expr2) == 'scipy.special.airy(x)[3]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with NumPy:\\n  # airyaiprime\\nairyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python with NumPy:\\n  # airybiprime\\nairybiprime(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python:\\n  # airyaiprime\\nairyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python:\\n  # airybiprime\\nairybiprime(x)'\n", "def test_airy_derivatives():\n    from sympy import airyaiprime, airybiprime\n\n    expr1 = airyaiprime(x)\n    expr2 = airybiprime(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.airy(x)[1]'\n    assert prntr.doprint(expr2) == 'scipy.special.airy(x)[3]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with NumPy:\\n  # airyaiprime\\nairyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python with NumPy:\\n  # airybiprime\\nairybiprime(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python:\\n  # airyaiprime\\nairyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python:\\n  # airybiprime\\nairybiprime(x)'\n", "def test_print_log1p():\n    from sympy import log1p\n\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(x + 1)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x + 1)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log1p(x)'\n", "def test_airyaiprime():\n    from sympy import airyaiprime, airybiprime\n\n    expr1 = airyaiprime(x)\n    expr2 = airybiprime(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.airy(x)[1]'\n    assert prntr.doprint(expr2) == 'scipy.special.airy(x)[3]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with NumPy:\\n  # airyaiprime\\nairyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python with NumPy:\\n  # airybiprime\\nairybiprime(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python:\\n  # airyaiprime\\nairyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python:\\n  # airybiprime\\nairybiprime(x)'\n", "def test_airyaiprime_airybiprime():\n    from sympy import airyaiprime, airybiprime\n\n    expr1 = airyaiprime(x)\n    expr2 = airybiprime(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.airy(x)[1]'\n    assert prntr.doprint(expr2) == 'scipy.special.airy(x)[3]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with NumPy:\\n  # airyaiprime\\nairyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python with NumPy:\\n  # airybiprime\\nairybiprime(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python:\\n  # airyaiprime\\nairyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python:\\n  # airybiprime\\nairybiprime(x)'\n"], "sample_1032": ["def test_rewrite_Max_as_Abs():\n    from sympy.functions.elementary.complexes import Abs\n    from sympy.abc import x, y, z\n    assert Max(x, y)._eval_rewrite_as_Abs(x, y) == (x + y + Abs(x - y)) / 2\n    assert Max(x, y, z)._eval_rewrite_as_Abs(x, y, z) == (x + y + z + Abs(x - y) + Abs(x - z) - Abs(y - z)) / 2\n", "def test_rewrite_as_Abs_minmax_symmetry():\n    from sympy.functions.elementary.complexes import Abs\n    from sympy.abc import x, y\n    assert Max(x, y).rewrite(Abs) == Min(-x, -y).rewrite(Abs)\n    assert Min(x, y).rewrite(Abs) == Max(-x, -y).rewrite(Abs)\n", "def test_rewrite_MinMax_as_Abs():\n    from sympy.functions.elementary.complexes import Abs\n    from sympy.abc import x, y, z\n\n    assert Min(x, y)._eval_rewrite_as_Abs(x, y) == Abs(x - y) / 2 + Min(x, y)\n    assert Max(x, y)._eval_rewrite_as_Abs(x, y) == Abs(x - y) / 2 + Max(x, y)\n    assert Min(x, y, z)._eval_rewrite_as_Abs(x, y, z) == Abs(x - y) / 2 + Min(x, y, z)\n    assert Max(x, y, z)._eval_rewrite_as_Abs(x, y, z) == Abs(x - y) / 2 + Max(x, y, z)\n", "def test_rewrite_Max_as_Abs():\n    from sympy.functions.elementary.complexes import Abs\n    from sympy.abc import x, y\n    assert Max(x, y).rewrite(Abs) == (x + y + Abs(x - y))/2\n    assert Max(x, y, z).rewrite(Abs) == (x + y + z + Abs(x - y) - Abs(x - z) + Abs(y - z))/2\n    assert Max(x, y, z, w).rewrite(Abs) == (x + y + z + w + Abs(x - y) - Abs(x - z) + Abs(x - w) - Abs(y - z) + Abs(y - w) + Abs(z - w))/2\n", "def test_MinMax_with_complex_numbers():\n    from sympy import I, sqrt\n    assert Min(2 + 3*I, 4 + I) == 2 + 3*I\n    assert Max(2 + 3*I, 4 + I) == 4 + I\n    assert Min(2 + sqrt(3)*I, 4 + sqrt(2)*I) == Min(2 + sqrt(3)*I, 4 + sqrt(2)*I)\n    assert Max(2 + sqrt(3)*I, 4 + sqrt(2)*I) == Max(2 + sqrt(3)*I, 4 + sqrt(2)*I)\n"], "sample_363": ["def test_AdminURLWidget_render_idn(self):\n    w = widgets.AdminURLFieldWidget()\n    self.assertHTMLEqual(\n        w.render('test', 'http://example-\u00e4\u00fc\u00f6.com'),\n        '<p class=\"url\">Currently: <a href=\"http://xn--example--7za4pnc.com\">'\n        'http://example-\u00e4\u00fc\u00f6.com</a><br>'\n        'Change:<input class=\"vURLField\" name=\"test\" type=\"url\" '\n        'value=\"http://example-\u00e4\u00fc\u00f6.com\"></p>'\n    )\n", "    def test_attrs(self):\n        w = widgets.AdminIntegerFieldWidget()\n        self.assertHTMLEqual(\n            w.render('test', 123),\n            '<input value=\"123\" type=\"text\" class=\"vIntegerField\" name=\"test\">',\n        )\n        w = widgets.AdminIntegerFieldWidget(attrs={'class': 'myIntegerField'})\n        self.assertHTMLEqual(\n            w.render('test', 123),\n            '<input value=\"123\" type=\"text\" class=\"myIntegerField\" name=\"test\">',\n        )\n", "    def test_clear_file_input(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.common.keys import Keys\n\n        band = Band.objects.create(name='Linkin Park')\n        album = band.album_set.create(\n            name='Hybrid Theory', cover_art=r'albums\\hybrid_theory.jpg'\n        )\n\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_album_change', args=(album.id,)))\n\n        # Check the clear checkbox\n        self.selenium.find_element(By.ID, 'id_cover_art-clear').click()\n\n        # Save the changes\n        self.selenium.find_element(By.NAME, '_save').click()\n\n        # Reload the page and check the file input is empty\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_album_change', args=(album.id,)))\n        self.assertFalse(self.selenium.find_element(By.ID, 'id_cover_art').get_attribute('value'))\n", "    def setUp(self):\n        super().setUp()\n        Band.objects.create(id=42, name='Bogey Blues', uuid='550e8400-e29b-41d4-a716-446655440000')\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.band = Band.objects.create(name='Linkin Park')\n        cls.album = cls.band.album_set.create(\n            name='Hybrid Theory', cover_art=SimpleUploadedFile('test.jpg', b'content')\n        )\n"], "sample_979": ["def test_matrix_multiplication_associativity():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, p)\n\n    assert (A*B)*C == A*(B*C)\n", "def test_matrix_multiplication_associativity():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, k)\n    assert (A * B) * C == A * (B * C)\n", "def test_MatrixElement_as_real_imag():\n    A = MatrixSymbol('A', n, m)\n    real_A, imag_A = A.as_real_imag()\n    assert real_A == (S(1)/2) * (A + A.conjugate())\n    assert imag_A == (S(1)/(2*I)) * (A - A.conjugate())\n", "def test_matrixelement_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, l)\n\n    assert A[k, l].subs(A, B) == B[k, l]\n    assert (A*B)[k, l].subs(A, C) == (C*B)[k, l]\n    assert (A*B)[k, l].subs(B, C) == (A*C)[k, l]\n    assert (A*B)[k, l].subs({A: C, B: A}) == (C*A)[k, l]\n", "def test_matrix_addition_subtraction():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', n, m)\n\n    assert A + (B - C) == (A + B) - C\n    assert (A + B) - A == B\n    assert (A - B) + B == A\n"], "sample_263": ["def test_loading_with_multiple_exclude_models(self):\n    Site.objects.all().delete()\n    management.call_command('loaddata', 'fixture1', exclude=['fixtures.Article', 'fixtures.Category'], verbosity=0)\n    self.assertFalse(Article.objects.exists())\n    self.assertFalse(Category.objects.exists())\n    self.assertEqual(Site.objects.get().domain, 'example.com')\n", "def test_dumpdata_with_empty_database(self):\n    # Delete all objects from the database\n    Article.objects.all().delete()\n    Category.objects.all().delete()\n    Site.objects.all().delete()\n    # Dump the current contents of the database\n    self._dumpdata_assert(['fixtures'], '[]')\n", "def test_circular_reference_natural_key(self):\n    management.call_command('loaddata', 'circular_reference_natural_key.json', verbosity=0)\n    obj_a = CircularA.objects.get()\n    obj_b = CircularB.objects.get()\n    self.assertEqual(obj_a.obj, obj_b)\n    self.assertEqual(obj_b.obj, obj_a)\n    self._dumpdata_assert(\n        ['fixtures'],\n        '[{\"model\": \"fixtures.circulara\", '\n        '\"fields\": {\"key\": \"x\", \"obj\": [\"y\"]}}, '\n        '{\"model\": \"fixtures.circularb\", '\n        '\"fields\": {\"key\": \"y\", \"obj\": [\"x\"]}}]',\n        natural_primary_keys=True,\n        natural_foreign_keys=True,\n    )\n", "def test_dumpdata_with_exclude_models(self):\n    # Load fixture1 which has a site, two articles, and a category\n    Site.objects.all().delete()\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n\n    # Excluding fixtures.Article and fixtures.Category should only leave sites\n    self._dumpdata_assert(\n        ['sites', 'fixtures'],\n        '[{\"pk\": 1, \"model\": \"sites.site\", \"fields\": {\"domain\": \"example.com\", \"name\": \"example.com\"}}]',\n        exclude_list=['fixtures.Article', 'fixtures.Category'],\n    )\n\n    # Excluding fixtures.Article and fixtures.Category, and sites should be a no-op\n    self._dumpdata_assert(\n        ['sites', 'fixtures'],\n        '[{\"pk\": 1, \"model\": \"sites.site\", \"fields\": {\"domain\": \"example.com\", \"name\": \"example.com\"}}, '\n        '{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": \"News Stories\"}}, '\n        '{\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, '\n        '{\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]',\n        exclude_list=['fixtures.Article', 'fixtures.Category', 'sites'],\n    )\n", "def test_dumpdata_multiple_models(self):\n    # Load fixture1 which has a site, two articles, and a category\n    Site.objects.all().delete()\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n\n    # Dumping multiple models should return all of them\n    self._dumpdata_assert(\n        ['fixtures.Article', 'fixtures.Category'],\n        '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": '\n        '\"News Stories\"}}, {\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place '\n        'on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, {\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": '\n        '{\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]'\n    )\n"], "sample_19": ["def test_axis_type_names():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.set()\n    assert w.axis_type_names == [\"RA\", \"DEC\", \"FREQ\"]\n", "def test_reorient_celestial_first():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"TIME\", \"DEC--TAN\", \"RA---TAN\"]\n    w.wcs.set()\n    w_reoriented = w.reorient_celestial_first()\n    assert w_reoriented.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\", \"TIME\"]\n", "def test_time_axis_reorient():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"TIME\", \"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.set()\n    reoriented = w.reorient_celestial_first()\n    assert list(reoriented.wcs.ctype) == [\"RA---TAN\", \"DEC--TAN\", \"TIME\"]\n", "def test_temporal_multiple_time_axes():\n    w = wcs.WCS(naxis=4)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"TIME\", \"TIME\"]\n    w.wcs.set()\n    assert w.has_temporal\n    assert w.sub([wcs.WCSSUB_TIME]).naxis == 2\n    assert w.sub([wcs.WCSSUB_TIME]).is_temporal is False\n    assert w.temporal.naxis == 1\n    assert w.temporal.is_temporal\n", "def test_reorient_celestial_first():\n    w = wcs.WCS(naxis=4)\n    w.wcs.ctype = [\"TIME\", \"DEC--TAN\", \"RA---TAN\", \"WAVE\"]\n    w.wcs.set()\n    w_reorient = w.reorient_celestial_first()\n    assert w_reorient.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\", \"TIME\", \"WAVE\"]\n    assert w_reorient.wcs.crpix == [3, 2, 1, 4]\n"], "sample_30": ["def test_empty_table():\n    votable = parse(get_pkg_data_filename(\"data/empty_table.xml\"))\n    table = votable.get_first_table()\n    assert len(table.array) == 0\n    assert len(table.array.dtype.names) == 2\n", "def test_select_columns_by_name_invalid():\n    columns = [\"invalid_column\"]\n    with pytest.raises(ValueError):\n        parse(get_pkg_data_filename(\"data/regression.xml\"), columns=columns)\n", "def test_parse_single_table_with_invalid_column():\n    with pytest.raises(ValueError):\n        parse_single_table(get_pkg_data_filename(\"data/regression.xml\"), columns=[\"invalid_column\"])\n", "def test_precision1():\n    assert issubclass(self.array[\"precision1\"].dtype.type, np.float64)\n    assert_array_equal(\n        self.array[\"precision1\"],\n        [8.9990234375, 0.0, np.inf, np.nan, -np.inf]\n    )\n    assert_array_equal(self.mask[\"precision1\"], [False, False, False, True, False])\n", "def test_timesys_versions():\n    output = io.StringIO()\n\n    # Test that timesys is allowed only in 1.3 and later\n    for version in (\"1.1\", \"1.2\"):\n        votable = parse(get_pkg_data_filename(\"data/timesys.xml\"), version=version)\n        validate(votable, output, xmllint=False)\n        assert \"W26: TIMESYS is not supported before VOTable 1.3\" in output.getvalue()\n        output.seek(0)\n        output.truncate(0)\n"], "sample_458": ["    def test_large_numbers(self):\n        self.assertEqual(floatformat(123456789.123456789, 9), \"123456789.123456789\")\n        self.assertEqual(floatformat(123456789.123456789, \"9\"), \"123456789.123456789\")\n        self.assertEqual(floatformat(123456789.123456789, \"9g\"), \"123,456,789.123456789\")\n        self.assertEqual(floatformat(123456789.123456789, \"9ug\"), \"123456789.123456789\")\n        self.assertEqual(floatformat(123456789.123456789, \"9u\"), \"123456789.123456789\")\n", "def test_large_numbers(self):\n    with localcontext() as ctx:\n        ctx.prec = 50\n        self.assertEqual(floatformat(Decimal(\"1.2345678901234567890123456789\"), 30), \"1.234567890123456789012345678900\")\n        self.assertEqual(floatformat(Decimal(\"1234567890.12345678901234567890\"), 20), \"1234567890.12345678901234567890\")\n", "    def test_large_values(self):\n        self.assertEqual(floatformat(123456789.123456789, 10), \"123456789.1234567890\")\n        self.assertEqual(floatformat(\"123456789.123456789\", 10), \"123456789.1234567890\")\n        self.assertEqual(floatformat(Decimal(\"123456789.123456789\"), 10), \"123456789.1234567890\")\n        self.assertEqual(floatformat(123456789.123456789, \"10\"), \"123456789.1234567890\")\n        self.assertEqual(floatformat(123456789.123456789, \"10g\"), \"123,456,789.1234567890\")\n        self.assertEqual(floatformat(123456789.123456789, \"10u\"), \"123456789.1234567890\")\n        self.assertEqual(floatformat(123456789.123456789, \"10ug\"), \"123,456,789.1234567890\")\n        self.assertEqual(floatformat(123456789.123456789, \"10gu\"), \"123456789.1234567890\")\n", "def test_non_numeric_argument(self):\n    self.assertEqual(floatformat(123.456, \"abc\"), \"123.456\")\n", "def test_large_numbers(self):\n    with localcontext() as ctx:\n        ctx.prec = 100\n        self.assertEqual(floatformat(1e100, 0), \"10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\")\n        self.assertEqual(floatformat(1e-100, 0), \"0\")\n        self.assertEqual(floatformat(1e100, -100), \"1e+100\")\n        self.assertEqual(floatformat(1e-100, -100), \"0e+0\")\n"], "sample_925": ["def test_mock_object_with_attributes():\n    attributes = {'attr1': 'value1', 'attr2': 'value2'}\n    mock = _MockObject(attributes=attributes)\n\n    assert mock.attr1 == 'value1'\n    assert mock.attr2 == 'value2'\n    assert repr(mock) == '_MockObject'\n", "def test_mock_attributes():\n    attributes = {'attribute1': 'value1', 'attribute2': 'value2'}\n    mock = _MockObject(attributes=attributes)\n    assert mock.attribute1 == 'value1'\n    assert mock.attribute2 == 'value2'\n", "def test_MockObject_getitem():\n    mock = _MockObject()\n    assert isinstance(mock['key'], _MockObject)\n    assert repr(mock['key']) == '_MockObject.key'\n", "def test_mock_module_magic_attributes():\n    mock = _MockModule('mocked_module')\n    assert mock.__file__ == os.devnull\n    assert mock.__sphinx_mock__ is True\n    assert mock.__all__ == []\n    assert mock.__path__ == []\n", "def test_mock_subclass_with_attributes():\n    class Parent:\n        parent_attr = 'parent_value'\n\n    mock = _MockObject()\n    SubClass = _make_subclass('SubClass', 'mocked_module', mock, {'sub_attr': 'sub_value'})\n    obj = SubClass()\n    assert isinstance(obj, SubClass)\n    assert isinstance(obj, _MockObject)\n    assert obj.parent_attr == 'parent_value'\n    assert obj.sub_attr == 'sub_value'\n"], "sample_506": ["def test_spines_color():\n    fig, ax = plt.subplots()\n    ax.spines['left'].set_color('red')\n    ax.spines['right'].set_color('green')\n    ax.spines['top'].set_color('blue')\n    ax.spines['bottom'].set_color('magenta')\n", "def test_spines_bounds():\n    fig, ax = plt.subplots()\n    ax.spines['left'].set_bounds(-5, 5)\n    ax.spines['bottom'].set_bounds(-5, 5)\n    ax.set_xlim(-10, 10)\n    ax.set_ylim(-10, 10)\n", "def test_spines_bounds():\n    fig, ax = plt.subplots()\n    ax.spines['left'].set_bounds(-1, 1)\n    ax.spines['bottom'].set_bounds(-2, 2)\n    ax.set_xlim([-3, 3])\n    ax.set_ylim([-3, 3])\n", "def test_spines_color():\n    # test setting the edgecolor of spines\n    fig, ax = plt.subplots()\n    ax.spines['left'].set_color('red')\n    ax.spines['right'].set_color('green')\n    ax.spines['top'].set_color('blue')\n    ax.spines['bottom'].set_color('yellow')\n", "def test_spines_visibility():\n    fig, ax = plt.subplots()\n    ax.spines[:].set_visible(False)\n    ax.plot([1, 2, 3], [1, 2, 3])\n"], "sample_255": ["def test_handle_one_request(self):\n    \"\"\"WSGIRequestHandler.handle_one_request() should handle a request.\"\"\"\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    server = Stub(get_app=lambda: lambda env, start_response: [b'OK'])\n    handler = WSGIRequestHandler(request, '192.168.0.2', server)\n    handler.handle_one_request()\n    self.assertEqual(b'OK', handler.wfile.getvalue())\n", "def test_handle_one_request(self):\n    \"\"\"Test the handling of one request in the WSGIRequestHandler.\"\"\"\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n    # Mock the server and app for the request\n    handler.server = Stub(get_app=lambda: lambda environ, start_response: [])\n    handler.server.base_environ = {}\n\n    # Call handle_one_request\n    handler.handle_one_request()\n\n    # Assert that the request was handled correctly\n    self.assertEqual(handler.close_connection, True)\n", "def test_server_handler_close(self):\n    \"\"\"ServerHandler.close() calls _read_limited() on the input stream.\"\"\"\n    class LimitedStreamStub:\n            self.called = False\n\n            self.called = True\n\n    limited_stream = LimitedStreamStub()\n    handler = ServerHandler(\n        limited_stream, BytesIO(), BytesIO(), self.request_factory.get('/').environ\n    )\n    handler.close()\n    self.assertTrue(limited_stream.called)\n", "def test_connection_header(self):\n    \"\"\"Test the 'Connection' header handling in WSGIRequestHandler.\"\"\"\n    # Test with a known content length\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    handler = WSGIRequestHandler(request, '192.168.0.2', Stub(app=lambda e, sr: []))\n    handler.handle_one_request()\n    self.assertIn('Content-Length', handler.headers)\n    self.assertEqual(handler.headers['Connection'], 'close')\n\n    # Test without a content length\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    handler = WSGIRequestHandler(request, '192.168.0.2', Stub(app=lambda e, sr: [b'test']))\n    handler.handle_one_request()\n    self.assertNotIn('Content-Length', handler.headers)\n    self.assertEqual(handler.headers['Connection'], 'close')\n\n    # Test with a threaded server\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    handler = WSGIRequestHandler(request, '192.168.0.2', Stub(app=lambda e, sr: []))\n    handler.server = Stub(ThreadingMixIn=True)\n    handler.handle_one_request()\n    self.assertNotIn('Connection', handler.headers)\n", "def test_strips_underscore_headers_in_app(self):\n    \"\"\"Ensure underscore headers are stripped in the WSGI application.\n\n    This is a more comprehensive test that exercises the code path of\n    the WSGI application to ensure underscore headers are stripped there\n    as well.\n    \"\"\"\n    rfile = BytesIO()\n    rfile.write(b\"GET / HTTP/1.0\\r\\n\")\n    rfile.write(b\"Some-Header: good\\r\\n\")\n    rfile.write(b\"Some_Header: bad\\r\\n\")\n    rfile.write(b\"Other_Header: bad\\r\\n\")\n    rfile.seek(0)\n\n    wfile = BytesIO()\n\n    request = WSGIRequest(self.request_factory.get('/',\n                                                   HTTP_SOME_HEADER='bad',\n                                                   HTTP_OTHER_HEADER='bad',\n                                                   HTTP_SOME_DASH_HEADER='good').environ)\n    request.rfile = rfile\n    request.wfile = Stub(sendall=wfile.write)\n\n    # Use the real WSGI server and handler to process the request\n    server = WSGIServer(('localhost', 8000), WSGIRequestHandler)\n    server.set_app(test_app)\n    server.get_request = lambda: request\n    server.handle_request()\n\n    wfile.seek(0)\n    response = wfile.read().decode()\n\n    # The underscore header should be stripped, and the dash header\n    # should be present\n    self.assertIn('HTTP_SOME_DASH_HEADER:good', response)\n    self.assertNotIn('HTTP_SOME_HEADER', response)\n    self.assertNotIn('HTTP_OTHER_HEADER', response)\n"], "sample_480": ["def test_key_transform_with_integer_index(self):\n    obj = NullableJSONModel.objects.create(value={\"array\": [\"first\", \"second\"]})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__array__0=\"first\"),\n        [obj],\n    )\n", "def test_key_transform_in_with_integer(self):\n    obj = NullableJSONModel.objects.create(value={\"array\": [1, 2, 3]})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__array__in=[1]),\n        [obj],\n    )\n", "def test_key_transform_exact(self):\n    obj = NullableJSONModel.objects.create(value={\"a\": \"b\"})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__exact=\"b\"),\n        [obj],\n    )\n\n    obj = NullableJSONModel.objects.create(value={\"a\": 1})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__exact=1),\n        [obj],\n    )\n\n    obj = NullableJSONModel.objects.create(value={\"a\": None})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__exact=None),\n        [obj],\n    )\n", "def test_key_exact_numeric(self):\n    tests = [\n        (\"value__p__exact\", 4.2),\n        (\"value__p__exact\", F(\"value__p\")),\n        (\"value__p__exact\", KeyTransform(\"p\", \"value\")),\n        (\"value__p__exact\", KeyTextTransform(\"p\", \"value\")),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(**{lookup: value}),\n                [self.objs[4]],\n            )\n", "def test_key_transform_expression_in_query(self):\n    expression = KeyTransform(\"d\", \"value\")\n    annotated_query = NullableJSONModel.objects.annotate(key=expression).filter(key__0__isnull=False)\n    direct_query = NullableJSONModel.objects.filter(value__d__0__isnull=False)\n    self.assertSequenceEqual(annotated_query, direct_query)\n"], "sample_661": ["def test_record_testsuite_property_empty_value(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p1_node.assert_attr(name=\"stats\", value=\"\")\n", "def test_escaped_skipreason_issue3533_with_multiline(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason='1 <> 2\\\\nThis is a multiline reason')\n            pass\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    snode = node.find_first_by_tag(\"skipped\")\n    assert \"1 <> 2\" in snode.text\n    assert \"This is a multiline reason\" in snode.text\n    snode.assert_attr(message=\"1 <> 2\\\\nThis is a multiline reason\")\n", "def test_skipped_with_custom_reason(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.skip(\"custom reason\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(skipped=1)\n    tnode = node.find_first_by_tag(\"testcase\")\n    tnode.assert_attr(classname=\"test_skipped_with_custom_reason\", name=\"test_skip\")\n    snode = tnode.find_first_by_tag(\"skipped\")\n    snode.assert_attr(type=\"pytest.skip\", message=\"custom reason\")\n", "def test_error_with_skipped_and_failed_tests(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip(reason=\"Skipped test\")\n            pass\n\n            assert 0\n\n        @pytest.fixture\n            raise ValueError(\"Setup error\")\n\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(errors=1, failures=1, skipped=1, tests=3)\n    skip_node = node.find_nth_by_tag(\"testcase\", 0)\n    skip_node.find_first_by_tag(\"skipped\").assert_attr(type=\"pytest.skip\", message=\"Skipped test\")\n    fail_node = node.find_nth_by_tag(\"testcase\", 1)\n    fail_node.find_first_by_tag(\"failure\")\n    error_node = node.find_nth_by_tag(\"testcase\", 2)\n    error_node.find_first_by_tag(\"error\").assert_attr(message=\"test setup failure\")\n", "def test_teardown_error_without_setup_error(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            yield\n            raise ValueError(\"Teardown Exception\")\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(errors=1, tests=1)\n    tnode = node.find_first_by_tag(\"testcase\")\n    fnode = tnode.find_first_by_tag(\"error\")\n    fnode.assert_attr(message=\"test teardown failure\")\n    assert \"Teardown Exception\" in fnode.toxml()\n"], "sample_837": ["def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    # Check if all the necessary sections are present\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n\n    # Check if all the necessary information is present in each section\n    sys_info = _get_sys_info()\n    deps_info = _get_deps_info()\n    blas_info = _get_blas_info()\n\n    for k, stat in sys_info.items():\n        assert '{k:>10}: {stat}'.format(k=k, stat=stat) in captured.out\n\n    for k, stat in blas_info.items():\n        assert '{k:>10}: {stat}'.format(k=k, stat=stat) in captured.out\n\n    for k, stat in deps_info.items():\n        assert '{k:>10}: {stat}'.format(k=k, stat=stat) in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n\n    assert 'System:' in out\n    assert 'python' in out\n    assert 'executable' in out\n    assert 'machine' in out\n\n    assert 'BLAS:' in out\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out\n\n    assert 'Python deps:' in out\n    assert 'pip' in out\n    assert 'setuptools' in out\n    assert 'sklearn' in out\n    assert 'numpy' in out\n    assert 'scipy' in out\n    assert 'Cython' in out\n    assert 'pandas' in out\n    assert 'matplotlib' in out\n    assert 'joblib' in out\n", "def test_show_versions():\n    # Redirect stdout to a buffer\n    import io\n    import contextlib\n    buffer = io.StringIO()\n    with contextlib.redirect_stdout(buffer):\n        show_versions()\n\n    output = buffer.getvalue()\n\n    # Check if system, BLAS and Python deps sections are printed\n    assert 'System:' in output\n    assert 'BLAS:' in output\n    assert 'Python deps:' in output\n\n    # Check if some key packages are printed in the Python deps section\n    assert 'pip' in output\n    assert 'numpy' in output\n    assert 'scipy' in output\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert \"System:\" in captured.out\n    assert \"python\" in captured.out\n    assert \"executable\" in captured.out\n    assert \"machine\" in captured.out\n\n    assert \"BLAS:\" in captured.out\n    # assuming _get_blas_info works correctly, we can only check for presence of keys\n    assert \"macros\" in captured.out\n    assert \"lib_dirs\" in captured.out\n    assert \"cblas_libs\" in captured.out\n\n    assert \"Python deps:\" in captured.out\n    assert \"pip\" in captured.out\n    assert \"setuptools\" in captured.out\n    assert \"sklearn\" in captured.out\n    assert \"numpy\" in captured.out\n    assert \"scipy\" in captured.out\n    assert \"Cython\" in captured.out\n    assert \"pandas\" in captured.out\n    assert \"matplotlib\" in captured.out\n    assert \"joblib\" in captured.out\n"], "sample_469": ["def test_alias_with_aggregate_annotation(self):\n    qs = Author.objects.alias(\n        total_pages=Sum(\"book__pages\"),\n    ).annotate(total_pages_annotated=F(\"total_pages\"))\n    for author in qs:\n        with self.subTest(author=author):\n            self.assertEqual(author.total_pages_annotated, author.total_pages)\n", "def test_alias_exists_aggregate_values_chaining(self):\n    qs = (\n        Publisher.objects.alias(publisher_alias=Value(1))\n        .values(\"name\")\n        .annotate(\n            has_books=Exists(\n                Book.objects.filter(publisher=OuterRef(\"pk\"))\n            ),\n            total_awards=Sum(\"num_awards\"),\n        )\n        .values_list(\"total_awards\", flat=True)\n        .order_by(\"total_awards\")\n    )\n    self.assertCountEqual(\n        qs,\n        [0, 3, 3, 3, 12],\n    )\n", "def test_alias_exists_aggregate_values_chaining(self):\n    qs = (\n        Publisher.objects.alias(total_books=Count(\"book\"))\n        .annotate(has_authors=Exists(Book.authors.through.objects.filter(book__publisher=OuterRef(\"pk\"))))\n        .values(\"total_books\")\n        .order_by()\n    )\n    self.assertCountEqual(\n        qs,\n        [\n            {\"total_books\": 4},\n            {\"total_books\": 4},\n            {\"total_books\": 4},\n            {\"total_books\": 0},\n            {\"total_books\": 0},\n        ],\n    )\n", "def test_annotation_with_subquery(self):\n    subquery = Store.objects.annotate(num_books=Count(\"books\")).filter(num_books__gt=2).values(\"id\")\n    books = Book.objects.annotate(num_stores=Subquery(subquery.annotate(count=Value(1)).values(\"count\"))).filter(num_stores__gt=0)\n    self.assertEqual(books.count(), 3)  # b1, b3, and b4 are in more than 2 stores\n", "def test_custom_function_with_null_input(self):\n    Company.objects.create(\n        name=\"Null Company\",\n        motto=None,\n        ticker_name=None,\n        description=None,\n    )\n\n    qs = Company.objects.annotate(\n        tagline=Func(\n            F(\"motto\"),\n            F(\"ticker_name\"),\n            F(\"description\"),\n            Value(\"No Tag\"),\n            function=\"COALESCE\",\n        )\n    ).order_by(\"name\")\n\n    self.assertQuerySetEqual(\n        qs,\n        [\n            (\"Apple\", \"APPL\"),\n            (\"Django Software Foundation\", \"No Tag\"),\n            (\"Google\", \"Do No Evil\"),\n            (\"Null Company\", \"No Tag\"),\n            (\"Yahoo\", \"Internet Company\"),\n        ],\n        lambda c: (c.name, c.tagline),\n    )\n"], "sample_267": ["def test_sqlite_datetime_parse(self):\n    from django.db.backends.sqlite3.base import _sqlite_datetime_parse\n    from datetime import datetime\n    from pytz import timezone\n\n    # Test with datetime object\n    dt = datetime(2022, 1, 1, 12, 0, 0)\n    self.assertEqual(_sqlite_datetime_parse(dt), dt)\n\n    # Test with datetime object and tzname\n    dt = datetime(2022, 1, 1, 12, 0, 0, tzinfo=timezone('UTC'))\n    self.assertEqual(_sqlite_datetime_parse(dt, 'Europe/Paris'), dt.astimezone(timezone('Europe/Paris')))\n\n    # Test with datetime object and conn_tzname\n    dt = datetime(2022, 1, 1, 12, 0, 0)\n    self.assertEqual(_sqlite_datetime_parse(dt, conn_tzname='UTC'), dt.replace(tzinfo=timezone('UTC')))\n\n    # Test with invalid datetime object\n    self.assertIsNone(_sqlite_datetime_parse('invalid_datetime'))\n\n    # Test with None\n    self.assertIsNone(_sqlite_datetime_parse(None))\n", "def test_disable_constraint_checking(self):\n    \"\"\"\n    Test the disable_constraint_checking method.\n    \"\"\"\n    self.assertTrue(connection.disable_constraint_checking())\n    with connection.cursor() as cursor:\n        enabled = cursor.execute('PRAGMA foreign_keys').fetchone()[0]\n    self.assertFalse(bool(enabled))\n\n    connection.enable_constraint_checking()\n    with connection.cursor() as cursor:\n        enabled = cursor.execute('PRAGMA foreign_keys').fetchone()[0]\n    self.assertTrue(bool(enabled))\n", "    def setUp(self):\n        self.author = Author.objects.create(name=\"Author1\")\n        self.item = Item.objects.create(name=\"Item1\", author=self.author)\n", "    def test_disable_constraint_checking_and_enable(self):\n        \"\"\"\n        Disabling and enabling constraint checking works correctly.\n        \"\"\"\n        with connection.cursor() as cursor:\n            self.assertTrue(connection.disable_constraint_checking())\n            cursor.execute('PRAGMA foreign_keys')\n            self.assertEqual(cursor.fetchone()[0], 0)\n            self.assertTrue(connection.enable_constraint_checking())\n            cursor.execute('PRAGMA foreign_keys')\n            self.assertEqual(cursor.fetchone()[0], 1)\n", "    def test_disable_constraint_checking_and_enable(self):\n        with connection.cursor() as cursor:\n            # Create a table with a foreign key constraint\n            cursor.execute('CREATE TABLE referenced (id INTEGER PRIMARY KEY)')\n            cursor.execute('CREATE TABLE main (ref_id INTEGER, FOREIGN KEY (ref_id) REFERENCES referenced(id))')\n\n        # Disable constraint checking\n        connection.disable_constraint_checking()\n\n        # Insert a row with an invalid foreign key reference\n        with connection.cursor() as cursor:\n            cursor.execute('INSERT INTO main (ref_id) VALUES (999)')\n\n        # Enable constraint checking\n        connection.enable_constraint_checking()\n\n        # Checking constraints should raise an IntegrityError\n        with self.assertRaises(IntegrityError):\n            connection.check_constraints()\n"], "sample_364": ["    def test_path_lookup_with_inclusion_namespace(self):\n        match = resolve('/included_urls/namespaced/d29ybGQ=/')\n        self.assertEqual(match.url_name, 'subpattern-base64')\n        self.assertEqual(match.app_name, 'namespaced-base64')\n        self.assertEqual(match.namespace, 'namespaced-base64')\n        self.assertEqual(match.kwargs, {'base': b'hello', 'value': b'world'})\n", "    def test_invalid_namespace_without_app_name(self):\n        msg = \"Specifying a namespace in include() without providing an app_name is not supported. Set the app_name attribute in the included module, or pass a 2-tuple containing the list of patterns and app_name instead.\"\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            path('included_urls/', include('urlpatterns.included_urls_no_app_name'), namespace='test')\n", "    def test_path_reverse_with_empty_string_argument(self):\n        url = reverse('inner-more', kwargs={'extra': ''})\n        self.assertEqual(url, '/more/')\n", "    def test_include_with_dynamic_module(self):\n        match = resolve('/dynamic_module/test/')\n        self.assertEqual(match.url_name, 'dynamic-test')\n        self.assertEqual(match.route, 'dynamic_module/<str:value>/')\n", "    def test_path_inclusion_is_matchable_with_namespace(self):\n        match = resolve('/included_urls/namespace/extra/something/')\n        self.assertEqual(match.url_name, 'namespaced-inner-extra')\n        self.assertEqual(match.namespace, 'namespaced')\n        self.assertEqual(match.kwargs, {'extra': 'something'})\n"], "sample_1091": ["def test_issue_17248():\n    x, y = symbols('x y')\n    assert Eq(x, y).subs({x: 0, y: 0}) is S.true\n    assert Ne(x, y).subs({x: 0, y: 0}) is S.false\n    assert Gt(x, y).subs({x: 0, y: 0}) is S.false\n    assert Lt(x, y).subs({x: 0, y: 0}) is S.false\n    assert Ge(x, y).subs({x: 0, y: 0}) is S.true\n    assert Le(x, y).subs({x: 0, y: 0}) is S.true\n", "def test_issue_16736():\n    s, c = sin(2*x), cos(2*x)\n    eq = Eq(s, c)\n    assert trigsimp(eq) == eq  # no rearrangement of sides\n    # simplification of sides might result in\n    # an unevaluated Eq\n    changed = trigsimp(Eq(s + c, sqrt(2)))\n    assert isinstance(changed, Eq)\n    assert changed.subs(x, pi/8) is S.true\n    # or an evaluated one\n    assert trigsimp(Eq(cos(x)**2 + sin(x)**2, 1)) is S.true\n", "def test_issue_17892():\n    assert simplify(Eq(x/x, 1)) is S.true\n    assert simplify(Ne(x/x, 1)) is S.false\n", "def test_issue_19072():\n    a = Symbol('a', positive=True)\n    b = Symbol('b', real=True)\n    c = Symbol('c', real=True)\n    expr = a * b + a * c\n    assert expr.simplify() == a * (b + c)\n", "def test_issue_19944():\n    x = symbols('x', real=True)\n    assert simplify(x + sqrt(2)*I) != x\n    assert simplify(x + sqrt(2)*I) != x + sqrt(2)*I\n    assert simplify(x + sqrt(2)*I) == x + I*sqrt(2)\n    assert simplify(x - sqrt(2)*I) == x - I*sqrt(2)\n"], "sample_102": ["def test_union_with_different_type(self):\n    qs1 = Number.objects.filter(num__lt=5).values('num')\n    qs2 = ReservedName.objects.filter(order__gt=5).values('order')\n    with self.assertRaises(TypeError):\n        qs1.union(qs2)\n", "def test_union_with_multiple_models_and_ordering(self):\n    ReservedName.objects.create(name='99 little bugs', order=99)\n    qs1 = Number.objects.filter(num=1).values('num')\n    qs2 = ReservedName.objects.values('order')\n    union_qs = qs1.union(qs2)\n    self.assertCountEqual(union_qs.order_by('num').values_list('num', flat=True), [1, 99])\n    self.assertCountEqual(union_qs.order_by('order').values_list('order', flat=True), [1, 99])\n", "def test_union_with_values_list_and_filter(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=1),\n        ReservedName(name='rn2', order=2),\n    ])\n    qs1 = ReservedName.objects.filter(order__gt=0).values_list('order', flat=True)\n    qs2 = ReservedName.objects.filter(order__lt=2).values_list('order', flat=True)\n    union_qs = qs1.union(qs2)\n    self.assertCountEqual(union_qs.filter(order__gt=1), [2])\n", "def test_union_with_different_fields(self):\n    qs1 = Number.objects.filter(num=1).values('num')\n    qs2 = Number.objects.filter(num=2).values('other_num')\n    msg = \"Merging 'QuerySet' classes must involve the same values in each case.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        qs1.union(qs2)\n", "def test_union_with_values_and_annotations(self):\n    Number.objects.create(num=10, other_num=5)\n    qs1 = Number.objects.filter(num=10).annotate(double_num=F('num') * 2)\n    qs2 = Number.objects.filter(num=5).annotate(double_num=F('num') * 2)\n    result = qs1.union(qs2).values('num', 'double_num')\n    self.assertCountEqual(result, [{'num': 10, 'double_num': 20}, {'num': 5, 'double_num': 10}])\n"], "sample_487": ["def test_actions_unique_with_different_names(self):\n    @admin.action(name=\"unique_action\")\n        pass\n\n    @admin.action(name=\"unique_action\")\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsInvalid(\n        BandAdmin,\n        Band,\n        \"__name__ attributes of actions defined in BandAdmin must be \"\n        \"unique. Name 'unique_action' is not unique.\",\n        id=\"admin.E130\",\n    )\n", "    def test_actions_unique_different_names(self):\n        @admin.action\n            pass\n\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsValid(BandAdmin, Band)\n", "def test_actions_unique_with_different_names(self):\n    @admin.action(name=\"custom_action\")\n        pass\n\n    @admin.action(name=\"custom_action\")\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsInvalid(\n        BandAdmin,\n        Band,\n        \"__name__ attributes of actions defined in BandAdmin must be \"\n        \"unique. Name 'custom_action' is not unique.\",\n        id=\"admin.E130\",\n    )\n", "    def test_custom_permissions_with_matching_has_method(self):\n        @admin.action(permissions=[\"custom\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n", "def test_custom_permissions_dont_require_matching_has_method(self):\n    @admin.action\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action,)\n\n    self.assertIsValid(BandAdmin, Band)\n"], "sample_1183": ["def test_Domain_is_zero():\n    I = S.ImaginaryUnit\n    a, b, c = [CC.convert(x) for x in (0, 0 + 0*I, 5)]\n    assert CC.is_zero(a) == True\n    assert CC.is_zero(b) == True\n    assert CC.is_zero(c) == False\n", "def test_Domain_is_zero():\n    assert ZZ.is_zero(ZZ(0)) == True\n    assert ZZ.is_zero(ZZ(1)) == False\n    assert QQ.is_zero(QQ(0)) == True\n    assert QQ.is_zero(QQ(1, 2)) == False\n    assert RR.is_zero(RR(0)) == True\n    assert RR.is_zero(RR(1e-100)) == False\n    assert CC.is_zero(CC(0, 0)) == True\n    assert CC.is_zero(CC(1, 0)) == False\n    assert CC.is_zero(CC(0, 1)) == False\n    assert ZZ_I.is_zero(ZZ_I(0, 0)) == True\n    assert ZZ_I.is_zero(ZZ_I(1, 0)) == False\n    assert ZZ_I.is_zero(ZZ_I(0, 1)) == False\n    assert QQ_I.is_zero(QQ_I(0, 0)) == True\n    assert QQ_I.is_zero(QQ_I(1, 0)) == False\n    assert QQ_I.is_zero(QQ_I(0, 1)) == False\n    assert EX.is_zero(EX(0)) == True\n    assert EX.is_zero(EX(1)) == False\n    assert ZZ[x].is_zero(ZZ[x](0)) == True\n    assert ZZ[x].is_zero(ZZ[x](1)) == False\n    assert QQ[x].is_zero(QQ[x](0)) == True\n    assert QQ[x].is_zero(QQ[x](1, 2)) == False\n    assert ZZ.frac_field(x).is_zero(ZZ.frac_field(x)(0)) == True\n    assert ZZ.frac_field(x).is_zero(ZZ.frac_field(x)(1)) == False\n    assert ZZ.frac_field(x).is_zero(ZZ.frac_field(x)(1, 2", "def test_Domain_is_prime():\n    assert ZZ.is_prime(2) == True\n    assert ZZ.is_prime(3) == True\n    assert ZZ.is_prime(4) == False\n    assert ZZ.is_prime(15) == False\n    assert ZZ.is_prime(29) == True\n    assert QQ.is_prime(2) == False  # QQ is not a domain of integers\n    assert QQ.is_prime(2.5) == False  # Non-integer values are not prime\n", "def test_Domain_gcd():\n    assert ZZ.gcd(24, 36) == 12\n    assert ZZ.gcd(24, 0) == 24\n    assert ZZ.gcd(0, 36) == 36\n    assert ZZ.gcd(0, 0) == 0\n    assert QQ.gcd(Rational(3, 4), Rational(5, 6)) == Rational(1, 12)\n    assert ZZ_I.gcd(ZZ_I(6, 8), ZZ_I(9, 12)) == ZZ_I(3, 4)\n", "def test_Domain_is_zero():\n    assert ZZ.is_zero(ZZ(0)) == True\n    assert ZZ.is_zero(ZZ(1)) == False\n    assert QQ.is_zero(QQ(0)) == True\n    assert QQ.is_zero(QQ(1, 2)) == False\n    assert RR.is_zero(RR(0.0)) == True\n    assert RR.is_zero(RR(3.14)) == False\n    assert CC.is_zero(CC(0, 0)) == True\n    assert CC.is_zero(CC(1, 1)) == False\n    assert CC.is_zero(CC(1, 0)) == False\n    assert CC.is_zero(CC(0, 1)) == False\n"], "sample_316": ["    def test_image_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            self.assertEqual(image.width, 100)\n            self.assertEqual(image.height, 100)\n", "    def test_image_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            self.assertEqual(image.width, 100)  # Replace 100 with actual width\n            self.assertEqual(image.height, 100)  # Replace 100 with actual height\n", "    def test_image_dimensions(self):\n        \"\"\"\n        Test the _get_image_dimensions method of ImageFile class.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            size = image._get_image_dimensions()\n            self.assertEqual(size, (800, 600))  # Assuming test.png is 800x600\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            image_pil = Image.open(fh)\n            self.assertEqual(image.width, image_pil.width)\n            self.assertEqual(image.height, image_pil.height)\n", "    def test_image_dimensions_caching(self):\n        \"\"\"\n        get_image_dimensions() should cache its result in _dimensions_cache.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            # Ensure _dimensions_cache doesn't exist before getting dimensions\n            self.assertFalse(hasattr(image, '_dimensions_cache'))\n            size = image._get_image_dimensions()\n            # Check that the dimensions were cached\n            self.assertEqual(image._dimensions_cache, size)\n"], "sample_524": ["def test_colorbar_set_label_color():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, label='cbar')\n    cbar.set_label_text('cbar', color='red')\n    assert cbar.ax.yaxis.label.get_color() == 'red'\n", "def test_colorbar_no_mappable():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=\"No mappable was found\"):\n        fig.colorbar(ax=ax)\n", "def test_colorbar_update_clim():\n    fig, ax = plt.subplots()\n    data = np.random.randn(10, 10)\n    pc = ax.pcolormesh(data)\n    cb = fig.colorbar(pc)\n    vmin, vmax = -1.5, 1.5\n    pc.set_clim(vmin, vmax)\n    fig.canvas.draw()\n    np.testing.assert_almost_equal(cb.vmin, vmin)\n    np.testing.assert_almost_equal(cb.vmax, vmax)\n", "def test_colorbar_ticklabels():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ticks=np.arange(0, 1, 0.2))\n    fig.canvas.draw()\n    labels = [l.get_text() for l in cb.ax.yaxis.get_ticklabels()]\n    expected = ['0.0', '0.2', '0.4', '0.6', '0.8']\n    assert labels == expected\n", "def test_colorbar_ticks_with_offset():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10)*1e6)\n    cb = fig.colorbar(pc, location='right', extend='max', ticks=[0, 1e6, 2e6])\n    fig.draw_without_rendering()\n    # Check that the ticks are at the correct positions\n    assert np.allclose(cb.ax.yaxis.get_ticklocs(), [0, 1e6, 2e6])\n    # Check that the offset text is set correctly\n    assert cb.ax.yaxis.offsetText.get_text() == '\u00d710\u2076'\n"], "sample_1074": ["def test_is_polycyclic_group():\n    a = Permutation([0, 1, 2])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    assert G.is_polycyclic_group() == True\n\n    a = Permutation([1, 2, 3, 4, 0])\n    b = Permutation([1, 0, 2, 3, 4])\n    G = PermutationGroup([a, b])\n    with raises(ValueError):\n        G.is_polycyclic_group()\n", "def test_abelian():\n    G = AbelianGroup(2, 3, 4)\n    assert G.is_abelian == True\n    G = PermutationGroup([Permutation(1, 2, 3, 4), Permutation(1, 2), Permutation(5, 6)])\n    assert G.is_abelian == True\n    G = AlternatingGroup(7)\n    assert G.is_abelian == False\n    G = DihedralGroup(4)\n    assert G.is_abelian == False\n    G = SymmetricGroup(3)\n    assert G.is_abelian == False\n", "def test_subgroup_search_with_tests():\n    S = SymmetricGroup(5)\n    prop_even = lambda x: x.is_even\n    test1 = lambda x: x(0) == 0\n    base, strong_gens = S.schreier_sims_incremental()\n    G = S.subgroup_search(prop_even, base=base, strong_gens=strong_gens, tests=[test1])\n    assert G.is_subgroup(AlternatingGroup(5))\n    assert _verify_bsgs(G, base, G.generators)\n", "def test_is_isomorphic():\n    G = PermutationGroup(Permutation(0,1,2), Permutation(0,2,3))\n    H = PermutationGroup(Permutation(0,1,3), Permutation(0,2,3))\n    assert not is_isomorphic(G, H)\n    H = PermutationGroup(Permutation(0,1,2), Permutation(0,1,3))\n    assert is_isomorphic(G, H)\n    G = SymmetricGroup(3)\n    H = PermutationGroup(Permutation(0,1,2), Permutation(0,1))\n    assert not is_isomorphic(G, H)\n    G = PermutationGroup(Permutation(0,1,2), Permutation(0,2,1))\n    H = PermutationGroup(Permutation(0,1,2), Permutation(0,1))\n    assert not is_isomorphic(G, H)\n", "def test_chains():\n    # Testing lower central series chains\n    G = SymmetricGroup(3)\n    chains = G.lower_central_series_chains()\n    assert len(chains) == 1\n    assert chains[0] == [G, PermutationGroup([])]\n\n    # Testing upper central series chains\n    chains = G.upper_central_series_chains()\n    assert len(chains) == 1\n    assert chains[0] == [PermutationGroup([]), G]\n\n    # Testing derived series chains\n    chains = G.derived_series_chains()\n    assert len(chains) == 1\n    assert chains[0] == [G, PermutationGroup([])]\n\n    # Testing composition series chains\n    chains = G.composition_series_chains()\n    assert len(chains) == 1\n    assert chains[0] == [G, PermutationGroup([])]\n"], "sample_854": ["def test_svc_ovr_decision_function_shape():\n    # Test that decision_function_shape='ovr' gives\n    # correct shape and is consistent with predict\n    X, y = make_blobs(n_samples=80, centers=5, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovr').fit(X_train, y_train)\n    dec = clf.decision_function(X_test)\n    assert dec.shape == (len(X_test), 5)\n    assert_array_equal(clf.predict(X_test), np.argmax(dec, axis=1))\n", "def test_svc_gamma_scale_sparse():\n    # Test that gamma='scale' with sparse input raises no warnings\n    X, y = sparse.csr_matrix([[0.], [1.]]), [0, 1]\n\n    clf = svm.SVC()\n    assert_no_warnings(clf.fit, X, y)\n", "def test_svc_sparse_gamma_scale():\n    # Test that gamma='scale' works correctly with sparse input\n    X = sparse.csr_matrix([[0.0], [1.0]])\n    y = [0, 1]\n\n    clf = svm.SVC(gamma='scale')\n    assert_no_warnings(clf.fit, X, y)\n    assert_almost_equal(clf._gamma, 1.0)\n\n    # X_var ~= 1 shouldn't raise warning, for when\n    # gamma is not explicitly set.\n    X = sparse.csr_matrix([[1.0], [np.sqrt(6) / 3 + 1]])\n    assert_no_warnings(clf.fit, X, y)\n", "def test_svc_gamma_scale():\n    # Test that gamma='scale' works correctly with a simple dataset\n    X, y = [[0.], [1.]], [0, 1]\n    clf = svm.SVC(gamma='scale')\n    clf.fit(X, y)\n    assert_almost_equal(clf._gamma, 1.0)\n", "def test_svc_decision_function_shape_ovo(SVCClass):\n    \"\"\"Test decision_function_shape='ovo' returns correct shape.\n    Related issue: https://github.com/scikit-learn/scikit-learn/issues/12011\n    \"\"\"\n    X, y = make_blobs(n_samples=10, centers=3, random_state=42)\n\n    clf = SVCClass(kernel='linear', decision_function_shape='ovo').fit(X, y)\n\n    # For n_classes=3, there are n_classes*(n_classes-1)/2 = 3 combinations\n    # of two classes.\n    assert clf.decision_function(X).shape == (10, 3)\n"], "sample_1101": ["def test_schur_number():\n    raises(ValueError, lambda: SchurNumber(S.Infinity))\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(6).lower_bound() == 364\n", "def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert isinstance(SchurNumber(5), SchurNumber)\n    assert SchurNumber(5).lower_bound() == 364\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    assert SchurNumber(S.Infinity) == S.Infinity\n", "def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5) == SchurNumber(5)\n    assert SchurNumber(6).lower_bound() == 364\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    x = symbols(\"x\")\n    raises(ValueError, lambda: SchurNumber(x))\n", "def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5) == SchurNumber(5)\n    assert SchurNumber(6).lower_bound() == 364\n    assert SchurNumber(S.Infinity) == S.Infinity\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(S.NaN))\n    raises(ValueError, lambda: SchurNumber(S.ComplexInfinity))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n", "def test_lower_bound():\n    assert SchurNumber(1).lower_bound() == 0\n    assert SchurNumber(2).lower_bound() == 1\n    assert SchurNumber(3).lower_bound() == 3\n    assert SchurNumber(4).lower_bound() == 13\n    assert SchurNumber(5).lower_bound() == 36\n    assert SchurNumber(10).lower_bound() == 2585\n"], "sample_53": ["def test_render_options_localized(self):\n    \"\"\"Options are localized if the current language is not English.\"\"\"\n    with translation.override('fr'):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        form = AlbumForm(initial={'band': beatles.pk})\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>Les Beatles</option>' % beatles.pk\n        self.assertIn(selected_option, output)\n", "    def test_date_input_format(self):\n        widget = DateInput(format=\"%d-%m-%Y\")\n        formatted_value = widget.format_value(datetime.date(2022, 1, 15))\n        self.assertEqual(formatted_value, \"15-01-2022\")\n", "def test_render_options_with_empty_value(self):\n    \"\"\"Empty option is present even if the field has an empty value.\"\"\"\n    form = NotRequiredBandForm(data={'band': ''})\n    output = form.as_table()\n    self.assertIn(self.empty_option, output)\n", "def test_render_options_localized_field(self):\n    \"\"\"The widget's output is localized.\"\"\"\n    translation.activate('fr')\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    form = NotRequiredBandForm()\n    output = form.as_table()\n    option = '<option value=\"%s\">The Beatles</option>' % beatles.pk\n    localized_option = '<option value=\"%s\">Les Beatles</option>' % beatles.pk\n    self.assertIn(option, output)\n    self.assertNotIn(localized_option, output)\n    translation.activate('en')\n    form = NotRequiredBandForm()\n    output = form.as_table()\n    self.assertIn(option, output)\n    self.assertNotIn(localized_option, output)\n", "    def test_build_attrs_with_placeholder(self):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.build_attrs({'placeholder': 'Select a band'})\n        self.assertEqual(attrs['data-placeholder'], 'Select a band')\n"], "sample_650": ["def test_log_format_log(pytester: Pytester) -> None:\n    \"\"\"Check that log_format affects output.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(levelname)s - %(message)s\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"WARNING - text\"])\n", "def test_log_file_custom_formatter(pytester: Pytester) -> None:\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format = [%(asctime)s] [%(levelname)s] %(message)s\n        log_file_date_format = %Y-%m-%d %H:%M:%S\n        \"\"\".format(\n            log_file\n        )\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger('catchlog').info(\"Custom formatter test message\")\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert re.match(r\"\\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\] \\[INFO\\] Custom formatter test message\", contents)\n", "def test_log_file_cli_format(pytester: Pytester) -> None:\n    # Custom log file format\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert isinstance(plugin.log_file_handler.formatter, DatetimeFormatter)\n            logging.getLogger('catchlog').info(\"INFO message\")\n            logging.getLogger('catchlog').warning(\"WARNING message\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n\n    custom_format = \"%(levelname)s:%(message)s\"\n    custom_date_format = \"%H:%M:%S\"\n\n    result = pytester.runpytest(\n        \"-s\", f\"--log-file={log_file}\", f\"--log-file-format={custom_format}\", f\"--log-file-date-format={custom_date_format}\"\n    )\n\n    # make sure that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"INFO:INFO message\" in contents\n        assert \"WARNING:WARNING message\" in contents\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Check that log_auto_indent affects output.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text\\\\nwith newline')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent = true\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"WARNING  test_log_auto_indent:test_log_auto_indent.py:6 text\",\n            \"          with newline\",\n        ]\n    )\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import textwrap\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning(textwrap.dedent('''\n                This is a multi-line message that\n                should be automatically indented.\n            ''').strip())\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-auto-indent\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"WARNING  test_log_auto_indent:test_log_auto_indent.py:7   This is a multi-line message that\",\n            \"WARNING  test_log_auto_indent:test_log_auto_indent.py:8   should be automatically indented.\",\n        ]\n    )\n"], "sample_553": ["def test_funcanimation_invalid_init_func(anim):\n        return None\n\n    with pytest.raises(RuntimeError):\n        animation.FuncAnimation(**anim, init_func=init_func, blit=True)\n", "def test_anim_delete_no_warning(anim):\n    anim = animation.FuncAnimation(**anim)\n    anim._draw_was_started = True\n    del anim\n    np.testing.break_cycles()\n", "def test_funcanimation_savefig_kwargs(tmpdir):\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    anim = animation.FuncAnimation(fig=fig, func=animate, init_func=init, frames=5)\n\n    with tmpdir.as_cwd():\n        savefig_kwargs = {'bbox_inches': 'tight', 'dpi': 100, 'format': 'png'}\n        with pytest.raises(TypeError, match=\"grab_frame got an unexpected keyword argument 'dpi'\"):\n            anim.save('unused.null', writer=NullMovieWriter(), savefig_kwargs=savefig_kwargs)\n        with pytest.raises(TypeError, match=\"grab_frame got an unexpected keyword argument 'bbox_inches'\"):\n            anim.save('unused.null', writer=NullMovieWriter(), savefig_kwargs=savefig_kwargs)\n        with pytest.raises(TypeError, match=\"grab_frame got an unexpected keyword argument 'format'\"):\n            anim.save('unused.null', writer=NullMovieWriter(), savefig_kwargs=savefig_kwargs)\n", "def test_funcanimation_kwargs(anim):\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(a*x + b*i)\n        line.set_data(x, y)\n        return line,\n\n    a = 2\n    b = 3\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=5, fargs=(a, b))\n    writer = NullMovieWriter()\n    anim.save('unused.null', writer=writer)\n    assert writer.args == (a, b)\n", "def test_animation_repeat_delay(anim):\n    # Test the repeat delay functionality of the animation\n    # This test checks if the animation pauses for the specified repeat delay\n    # after each iteration when repeat is True\n    repeat_delay = 1000  # 1 second delay\n    anim = animation.FuncAnimation(**anim, repeat_delay=repeat_delay)\n    anim.save('unused.null', writer=NullMovieWriter())\n    # Here we need to check if the delay is actually implemented, but since we are not in an interactive framework,\n    # we can't actually measure the delay. However, we can check if the delay is correctly set in the event source.\n    assert anim.event_source.interval == repeat_delay\n"], "sample_670": ["def test_complex_expressions(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is eval(expr.replace(\"true\", \"True\").replace(\"false\", \"False\"))\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_combined_operators(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_parentheses(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    expected = eval(expr.replace(\"not\", \"not \").replace(\"true\", \"True\").replace(\"false\", \"False\"))\n    assert evaluate(expr, matcher) is expected\n"], "sample_1096": ["def test_IndexedBase_strides():\n    i, j, m, n = symbols('i j m n', integer=True)\n    a = IndexedBase('a', strides=(m, n))\n    assert a.strides == Tuple(m, n)\n    assert Indexed(a, i, j).strides == Tuple(m, n)\n\n    a = IndexedBase('a', strides='C')\n    assert a.strides == 'C'\n    assert Indexed(a, i, j).strides == 'C'\n\n    a = IndexedBase('a', strides='F')\n    assert a.strides == 'F'\n    assert Indexed(a, i, j).strides == 'F'\n", "def test_Indexed_strides():\n    i, j = symbols('i j', integer=True)\n    l, m, n, o = symbols('l m n o', integer=True)\n    A = IndexedBase('A', strides=(l, m, n), offset=o)\n    assert A.strides == (l, m, n)\n    assert A[i, j, k].strides == (l, m, n)\n    assert A.offset == o\n    assert A[i, j, k].offset == o\n", "def test_Indexed_derivative():\n    i, j, k = symbols('i j k', cls=Idx)\n    A = IndexedBase('A')\n    assert A[i].diff(A[i]) == 1\n    assert A[i].diff(A[j]) == KroneckerDelta(i, j)\n    assert A[i, j].diff(A[i, k]) == KroneckerDelta(j, k)\n    assert A[i, j].diff(A[k, j]) == KroneckerDelta(i, k)\n    assert A[i, j].diff(A[k, l]) == KroneckerDelta(i, k)*KroneckerDelta(j, l)\n", "def test_Indexed_derivative_shape():\n    m, n = symbols('m n', integer=True)\n    A = IndexedBase('A', shape=(m, n))\n    i, j = Idx('i', m), Idx('j', n)\n    assert A[i, j].diff(i).shape == (m, n)\n    assert A[i, j].diff(j).shape == (m, n)\n", "def test_Indexed_derivative():\n    i, j, k = symbols('i j k', cls=Idx)\n    A = IndexedBase('A')\n    expr = A[i, j]\n    assert expr.diff(A[i, j]) == 1\n    assert expr.diff(A[k, l]) == KroneckerDelta(i, k) * KroneckerDelta(j, l)\n    assert expr.diff(A[i, k]) == KroneckerDelta(j, k)\n    assert expr.diff(A[k, j]) == KroneckerDelta(i, k)\n    assert expr.diff(A[i, i]) == 0\n"], "sample_871": ["def test_silhouette_samples_non_csr_sparse():\n    \"\"\"Check that silhouette_samples raises TypeError for non-CSR sparse matrices.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    pdist_sparse = csc_matrix(pairwise_distances(X))\n    with pytest.raises(\n        TypeError,\n        match=\"Expected CSR matrix. Please pass sparse matrix in CSR format.\",\n    ):\n        silhouette_samples(pdist_sparse, y, metric=\"precomputed\")\n", "def test_silhouette_score_random_state():\n    # Assert that silhouette_score returns the same result with a fixed random_state\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    score_with_random_state = silhouette_score(X, y, sample_size=4, random_state=42)\n    score_without_random_state = silhouette_score(X, y, sample_size=4, random_state=42)\n    assert score_with_random_state == score_without_random_state\n", "def test_silhouette_samples_different_sparse_formats():\n    \"\"\"Check that silhouette_samples returns the same results for different sparse formats.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    sparse_formats = [csr_matrix, csc_matrix, dok_matrix, lil_matrix]\n\n    results = []\n    for sparse_format in sparse_formats:\n        X_sparse = sparse_format(X)\n        results.append(silhouette_samples(X_sparse, y, metric=\"euclidean\"))\n\n    for i in range(1, len(results)):\n        assert_allclose(results[i], results[0])\n", "def test_silhouette_samples_non_csr_sparse():\n    \"\"\"Check that silhouette_samples raises error for non-CSR sparse input.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    pdist_dense = pairwise_distances(X)\n    pdist_sparse = csc_matrix(pdist_dense)  # Use a non-CSR sparse matrix\n    with pytest.raises(ValueError, match=\"Expected CSR matrix. Please pass sparse matrix in CSR format.\"):\n        silhouette_samples(pdist_sparse, y, metric=\"precomputed\")\n", "def test_silhouette_samples_invalid_sparse_format():\n    \"\"\"Check for non-CSR sparse format for silhouette_samples.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    pdist_sparse = csc_matrix(pairwise_distances(X))\n    with pytest.raises(\n        TypeError,\n        match=\"Expected CSR matrix. Please pass sparse matrix in CSR format.\",\n    ):\n        silhouette_samples(pdist_sparse, y, metric=\"precomputed\")\n"], "sample_493": ["def test_aggregation_default_using_integer_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum(\"pages\", default=0),\n    )\n    self.assertEqual(result[\"value\"], 0)\n", "def test_referenced_subquery_requires_wrapping_with_filter(self):\n    total_books_qs = (\n        Author.book_set.through.objects.values(\"author\")\n        .filter(author=OuterRef(\"pk\"))\n        .annotate(total=Count(\"book\"))\n        .filter(total__gt=0)\n    )\n    with self.assertNumQueries(1) as ctx:\n        aggregate = (\n            Author.objects.annotate(\n                total_books=Subquery(total_books_qs.values(\"total\"))\n            )\n            .values(\"pk\", \"total_books\")\n            .filter(total_books__isnull=False)\n            .aggregate(\n                sum_total_books=Sum(\"total_books\"),\n            )\n        )\n    sql = ctx.captured_queries[0][\"sql\"].lower()\n    self.assertEqual(sql.count(\"select\"), 3, \"Subquery wrapping required\")\n    self.assertEqual(aggregate, {\"sum_total_books\": 2})\n", "def test_referenced_subquery_annotation_kept(self):\n    total_books_qs = (\n        Author.book_set.through.objects.values(\"author\")\n        .filter(author=OuterRef(\"pk\"))\n        .annotate(total=Count(\"book\"))\n    )\n    with self.assertNumQueries(1) as ctx:\n        author_list = (\n            Author.objects.annotate(\n                total_books=Subquery(total_books_qs.values(\"total\"))\n            )\n            .values_list(\"pk\", \"total_books\")\n        )\n    sql = ctx.captured_queries[0][\"sql\"].lower()\n    self.assertEqual(sql.count(\"select\"), 2, \"Subquery required\")\n    self.assertCountEqual(author_list, [(self.a1.pk, 2), (self.a2.pk, 1)])\n", "def test_window_function_aggregation(self):\n    from django.db.models import Window\n    from django.db.models.functions import Rank\n\n    books = Book.objects.annotate(\n        rank=Window(expression=Rank(), order_by=F('rating').desc())\n    ).order_by('rating')\n\n    self.assertEqual(books[0].rank, 3)\n    self.assertEqual(books[1].rank, 2)\n    self.assertEqual(books[2].rank, 2)\n    self.assertEqual(books[3].rank, 1)\n    self.assertEqual(books[4].rank, 5)\n    self.assertEqual(books[5].rank, 4)\n", "def test_aggregation_default_multiple_annotations(self):\n    result = Book.objects.annotate(\n        price_double=F(\"price\") * 2,\n        page_double=F(\"pages\") * 2,\n    ).aggregate(\n        sum_price_double=Sum(\"price_double\", default=0),\n        sum_page_double=Sum(\"page_double\", default=0),\n    )\n    self.assertEqual(result[\"sum_price_double\"], 190)\n    self.assertEqual(result[\"sum_page_double\"], 2666)\n"], "sample_893": ["def test_plot_tree_regression(pyplot):\n    # Smoke test for regression tree\n    reg = DecisionTreeRegressor(max_depth=2, random_state=0)\n    reg.fit(X, y)\n\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(reg, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\nmse = 1.0\\nsamples = 6\\nvalue = -0.67\"\n    )\n    assert nodes[1].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = 1.0\"\n", "def test_plot_tree_regression(pyplot):\n    # Check correctness of export_graphviz for regression trees\n    regr = DecisionTreeRegressor(max_depth=3, random_state=2)\n    regr.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(regr, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == \"first feat <= 0.0\\nsquared_error = 1.0\\nsamples = 6\\nvalue = 0.0\"\n    assert nodes[1].get_text() == \"squared_error = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"squared_error = 0.0\\nsamples = 3\\nvalue = 1.0\"\n", "def test_plot_tree_regression(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for regression problem\n    reg = DecisionTreeRegressor(max_depth=3, min_samples_split=2, random_state=2)\n    reg.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(reg, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text().startswith(\"first feat <= \")\n    assert nodes[1].get_text().startswith(\"samples = \")\n    assert nodes[2].get_text().startswith(\"samples = \")\n", "def test_plot_tree_class_names(pyplot):\n    # Check correctness of export_graphviz with class_names\n    clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=2)\n    clf.fit(X, y)\n\n    # Test export code with class_names as True\n    nodes = plot_tree(clf, class_names=True)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == \"x[0] <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\\nclass = y[0]\"\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\\nclass = y[0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\\nclass = y[1]\"\n\n    # Test export code with custom class_names\n    custom_classes = [\"classA\", \"classB\"]\n    nodes = plot_tree(clf, class_names=custom_classes)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == \"x[0] <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\\nclass = classA\"\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\\nclass = classA\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\\nclass = classB\"\n", "def test_plot_tree_friedman_mse(pyplot):\n    # Check correctness of export_graphviz for criterion = friedman_mse\n    reg = DecisionTreeRegressor(\n        max_depth=3, min_samples_split=2, criterion=\"friedman_mse\", random_state=2\n    )\n    reg.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(reg, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\nfriedman_mse = 1.0\\nsamples = 6\\nvalue = -0.33\"\n    )\n    assert nodes[1].get_text() == \"friedman_mse = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"friedman_mse = 0.0\\nsamples = 3\\nvalue = 1.0\"\n"], "sample_444": ["def test_url_converter(self):\n    name = \"cached/styles.css\"\n    hashed_files = {\"cached/other.css\": \"other.d41d8cd98f00.css\"}\n    template = 'url(\"%(url)s\")'\n    converter = storage.staticfiles_storage.url_converter(name, hashed_files, template)\n    matchobj = mock.Mock()\n    matchobj.groupdict.return_value = {\"matched\": 'url(\"other.css\")', \"url\": \"other.css\"}\n    result = converter(matchobj)\n    self.assertEqual(result, 'url(\"other.d41d8cd98f00.css\")')\n", "def test_css_source_map_with_comments(self):\n    relpath = self.hashed_file_path(\"cached/source_map_comments.css\")\n    self.assertEqual(relpath, \"cached/source_map_comments.b2fceaf426aa.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"/*# sourceMappingURL=source_map.css.map */ /* some comment */\", content)\n        self.assertIn(\n            b\"/*# sourceMappingURL=source_map.css.99914b932bd3.map */ /* some comment */\",\n            content,\n        )\n    self.assertPostCondition()\n", "    def test_custom_pattern(self):\n        \"\"\"\n        Test custom patterns for file substitution.\n        \"\"\"\n        finders.get_finder.cache_clear()\n        self.run_collectstatic()\n        relpath = self.hashed_file_path(\"custom_pattern.txt\")\n        self.assertEqual(relpath, \"custom_pattern.5e0040571e1a.txt\")\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertNotIn(b\"cached/other.css\", content)\n            self.assertIn(b\"other.d41d8cd98f00.css\", content)\n        self.assertPostCondition()\n", "    def setUp(self):\n        super().setUp()\n        self.manifest_path = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.manifest_path)\n\n        self.manifest_name = \"staticfiles.json\"\n        self.manifest_content = '{\"version\": \"1.1\", \"paths\": {}, \"hash\": \"test_hash\"}'\n\n        self.manifest_file_path = os.path.join(self.manifest_path, self.manifest_name)\n        with open(self.manifest_file_path, \"w\") as f:\n            f.write(self.manifest_content)\n\n        storage.staticfiles_storage = CustomManifestStorage(\n            manifest_location=self.manifest_path,\n        )\n", "def test_alternate_import_syntax(self):\n    finders.get_finder.cache_clear()\n    self.run_collectstatic(clear=True)\n    relpath = self.hashed_file_path(\"alternate_syntax.js\")\n    self.assertEqual(relpath, \"alternate_syntax.6686c5ff6d4b.js\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertIn(b'import(\"alternate_module.9b62a6374499.js\")', content)\n    self.assertPostCondition()\n"], "sample_668": ["def test_fixture_positional_arguments_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('function')\n            pass\n\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.*\",\n        ]\n    )\n", "def test_fixture_positional_arguments(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('function')\n            pass\n\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n            \"*pass them as a keyword argument instead.*\",\n        ]\n    )\n", "def test_fixargnames_is_deprecated(pytestconfig):\n    \"\"\"Check that using `funcargnames` is deprecated (#XXXX)\n    This attribute has been deprecated in favor of `fixturenames`.\n    \"\"\"\n    pytestconfig.funcargnames = [\"arg1\", \"arg2\"]\n    with pytest.warns(pytest.PytestDeprecationWarning):\n        assert pytestconfig.fixturenames == pytestconfig.funcargnames\n", "def test_fixture_positional_arguments_is_deprecated(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture()\n            pass\n    \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--collect-only\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.*\"\n        ]\n    )\n", "def test_fix_positional_args_in_fixture_warning(testdir, recwarn):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('function')\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert len(recwarn) == 1\n    w = recwarn.pop(DeprecationWarning)\n    assert issubclass(w.category, pytest.PytestDeprecationWarning)\n    assert \"Passing arguments to pytest.fixture() as positional arguments is deprecated\" in str(w.message)\n"], "sample_718": ["def test_check_estimator_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n    from sklearn.datasets import load_boston\n    from sklearn.utils.estimator_checks import check_classifiers_regression_target\n\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    e = BaseBadClassifier()\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, check_classifiers_regression_target, 'BaseBadClassifier', e, X, y)\n", "def test_check_estimator_multioutput():\n    # check that check_estimator() works on multi-output estimators\n\n    # test MultiTaskElasticNet\n    est = MultiTaskElasticNet()\n    check_estimator(est)\n\n    # add more tests for other multi-output estimators if needed\n", "def test_check_estimator_custom_classifiers_regression_target():\n    # Check if custom classifiers throw an exception when fed regression targets\n    class CustomClassifier(BaseEstimator, ClassifierMixin):\n            return self\n\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    e = CustomClassifier()\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n", "def test_check_estimator_multi_output():\n    # check that check_estimator() works on multi-output estimators\n    est = MultiTaskElasticNet()\n    check_estimator(est)\n", "def test_check_estimators_pickle():\n    # check that the estimators can be pickled and unpickled\n    from sklearn.base import clone\n    from sklearn.datasets import make_blobs\n    from sklearn.utils.estimator_checks import check_estimators_pickle\n\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n    estimator = clone(LinearRegression())\n    y = y.reshape(-1, 1)  # LinearRegression is mono-output task\n    check_estimators_pickle(\"LinearRegression\", estimator, X, y)\n"], "sample_280": ["def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum('price', default=0.0),\n    )\n    self.assertEqual(result['value'], 0.0)\n", "def test_stddev_aggregate(self):\n    # Test with sample=False (population standard deviation)\n    vals = Book.objects.aggregate(stddev_price=StdDev('price'))\n    self.assertIsInstance(vals['stddev_price'], Decimal)\n\n    # Test with sample=True (sample standard deviation)\n    vals = Book.objects.aggregate(stddev_price=StdDev('price', sample=True))\n    self.assertIsInstance(vals['stddev_price'], Decimal)\n\n    # Test with a filter\n    vals = Book.objects.filter(rating__gt=3.5).aggregate(stddev_price=StdDev('price'))\n    self.assertIsInstance(vals['stddev_price'], Decimal)\n", "def test_aggregation_filter_distinct(self):\n    \"\"\"\n    Tests that aggregate functions with a filter and distinct=True\n    work correctly.\n    \"\"\"\n    authors = Author.objects.annotate(\n        distinct_book_count=Count('book', distinct=True, filter=Q(book__rating__gt=3.5))\n    ).order_by('name')\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty', 1),\n            ('Brad Dayley', 0),\n            ('Jacob Kaplan-Moss', 1),\n            ('James Bennett', 1),\n            ('Jeffrey Forcier', 0),\n            ('Paul Bissex', 0),\n            ('Peter Norvig', 1),\n            ('Stuart Russell', 0),\n            ('Wesley J. Chun', 0),\n        ],\n        lambda a: (a.name, a.distinct_book_count)\n    )\n", "def test_aggregation_default_using_decimal_from_expression(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum('price', default=Value(5) * Value(2)),\n    )\n    self.assertEqual(result['value'], Decimal('10.00'))\n", "def test_aggregate_distinct_star(self):\n    \"\"\"\n    Count(*) with distinct=True is invalid, and should raise an exception.\n    \"\"\"\n    msg = \"COUNT(DISTINCT *) is not valid.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        Book.objects.aggregate(n=Count(\"*\", distinct=True))\n"], "sample_949": ["def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'http://www.sphinx-doc.org' in content\n", "def test_footnotes_in_manpage(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'footnotes.1').read_text()\n    assert 'Footnotes' not in content\n", "def test_no_man_pages_config(app, status, warning):\n    app.config.man_pages = []\n    app.builder.build_all()\n    assert not (app.outdir / 'python.1').exists()\n", "def test_images_in_manpage(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # The manpage builder does not support images, so they should not be present in the output\n    assert '.PS' not in content\n    assert '.PE' not in content\n", "def test_inline_markup(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert r'\\fBbold\\fP' in content\n    assert r'\\fIitalic\\fP' in content\n    assert r'\\fBbold \\fIitalic\\fP' in content\n    assert r'\\fB\\fIbold italic\\fP' in content\n    assert r'\\\\-hyphenation' in content\n    assert r'\\\\fRmonospaced\\\\fP' in content\n    assert r'\\\\fB\\\\fIbold italic monospaced\\\\fP' in content\n    assert r'\\\\f(CRcopyright\\\\fP' in content\n    assert r'\\\\f(CREGISTERED\\\\fP' in content\n"], "sample_367": ["    def test_cache_page_decorator_caching(self, mocked_set, mocked_get):\n        mocked_get.return_value = None\n\n        @cache_page(60)\n            return HttpResponse('Test')\n\n        request = HttpRequest()\n        response = a_view(request)\n        self.assertEqual(response.content, b'Test')\n        mocked_set.assert_called_once_with(mock.ANY, b'Test', 60)\n\n        mocked_get.return_value = b'Cached'\n        response = a_view(request)\n        self.assertEqual(response.content, b'Cached')\n        mocked_set.assert_called_once()  # No additional calls to set\n", "    def test_cache_page_decorator_headers(self):\n        @cache_page(60)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertIn('Cache-Control', response.headers)\n        self.assertIn('Vary', response.headers)\n        self.assertEqual(response.headers['Vary'], 'Cookie')\n        self.assertIn('max-age=60', response.headers['Cache-Control'])\n", "    def test_cache_page_decorator_http_request(self, mock_cache_page):\n        mock_cache_page.return_value = lambda func: func\n\n        class MyClass:\n            @cache_page(120)\n                return HttpResponse()\n\n        request = HttpRequest()\n        MyClass().a_view(request)\n        mock_cache_page.assert_called_once_with(120)\n\n        request = HttpRequestProxy(request)\n        MyClass().a_view(request)\n        mock_cache_page.assert_called_with(120)\n", "    def test_cache_page_decorator(self, mock_set, mock_get):\n        mock_get.return_value = None\n\n        @cache_page(60)\n            return HttpResponse(\"Cached response\")\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.content, b\"Cached response\")\n        mock_get.assert_called_once()\n        mock_set.assert_called_once()\n\n        # Test cache hit\n        mock_get.return_value = HttpResponse(\"Cached response\").render().content\n        response = a_view(HttpRequest())\n        self.assertEqual(response.content, b\"Cached response\")\n        self.assertEqual(mock_get.call_count, 2)\n        self.assertEqual(mock_set.call_count, 1)\n", "    def test_cache_page_decorator_http_request(self):\n        class MyClass:\n            @cache_page(60)\n                return HttpResponse()\n\n        request = HttpRequest()\n        response = MyClass().a_view(request)\n        self.assertIsInstance(response, HttpResponse)\n        response_key_prefix = MyClass().a_view.cache_page.key_prefix\n        self.assertIsNone(response_key_prefix)\n\n        @cache_page(120, key_prefix='test_prefix')\n            return HttpResponse()\n\n        response = another_view(request)\n        self.assertIsInstance(response, HttpResponse)\n        response_key_prefix = another_view.cache_page.key_prefix\n        self.assertEqual(response_key_prefix, 'test_prefix')\n"], "sample_713": ["def test_ridge_classifier_multi_output():\n    X, y = make_regression(n_samples=100, n_features=2, n_targets=2, random_state=42)\n    ridge = RidgeClassifier()\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (2, 2))\n    y_pred = ridge.predict(X)\n    assert_equal(y_pred.shape, (100, 2))\n", "def test_ridge_classifier_cv_multi_label():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n", "def test_ridge_classifier_cv_with_sample_weights():\n    # Test class weights and sample weights together for cross validated ridge classifier.\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n    sample_weight = np.array([1.0, 1.0, 1.0, 0.5, 0.5])\n\n    reg = RidgeClassifierCV(class_weight={1: 0.001}, sample_weight=sample_weight, alphas=[.01, .1, 1, 10])\n    reg.fit(X, y)\n\n    assert_array_equal(reg.predict([[-.2, 2]]), np.array([-1]))\n", "def test_ridge_classifier_cv_sparse_svd():\n    X = sp.csr_matrix(X_iris)\n    ridge = RidgeClassifierCV(gcv_mode=\"svd\")\n    assert_raises(TypeError, ridge.fit, X, y_iris)\n", "def test_ridge_classifier_cv_no_support_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n"], "sample_281": ["def test_model_admin_not_registered(self):\n    opts = {\n        'app_label': 'admin_views',\n        'model_name': 'NotRegisteredModel',\n        'field_name': 'question'\n    }\n    request = self.factory.get(self.url, {'term': 'is', **opts})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_to_field_resolution_with_fk_to_pk(self):\n    a = Author.objects.create(name='John Doe')\n    b = Book.objects.create(title='Test Book')\n    Authorship.objects.create(author=a, book=b)\n    opts = {\n        'app_label': Authorship._meta.app_label,\n        'model_name': Authorship._meta.model_name,\n        'field_name': 'book',\n    }\n    request = self.factory.get(self.url, {'term': 'test', **opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data, {\n        'results': [{'id': str(b.pk), 'text': b.title}],\n        'pagination': {'more': False},\n    })\n", "    def test_filtered_queryset(self):\n        \"\"\"\n        Verify that the queryset is filtered correctly by the search term.\n        \"\"\"\n        # Create some test data\n        questions = [\n            Question.objects.create(question='Question 1'),\n            Question.objects.create(question='Question 2'),\n            Question.objects.create(question='Another question'),\n        ]\n\n        # Test search term 'Question'\n        request = self.factory.get(self.url, {'term': 'Question', **self.opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(len(data['results']), 2)\n        self.assertEqual(data['results'][0]['id'], str(questions[0].pk))\n        self.assertEqual(data['results'][1]['id'], str(questions[1].pk))\n\n        # Test search term 'Another'\n        request = self.factory.get(self.url, {'term': 'Another', **self.opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(len(data['results']), 1)\n        self.assertEqual(data['results'][0]['id'], str(questions[2].pk))\n", "def test_permission_denied_on_invalid_request(self):\n    \"\"\"\n    The view should raise PermissionDenied for invalid requests.\n    \"\"\"\n    request = self.factory.get(self.url, {'term': 'is', **self.opts, 'field_name': 'invalid_field'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_permission_denied(self):\n    \"\"\"\n    The view should raise PermissionDenied if the user doesn't have\n    appropriate permissions for the related model.\n    \"\"\"\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.user\n    self.user.user_permissions.clear()\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_905": ["def test_is_builtin_class_method(app):\n    from target.methods import Base\n\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(Base, 'meth') is False\n", "def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_is_builtin_class_method():\n    class MyList(list):\n            super().__init__(*args, **kwargs)\n\n    assert inspect.is_builtin_class_method(MyList, '__init__') is False\n    assert inspect.is_builtin_class_method(list, '__init__') is True\n", "def test_is_builtin_class_method_edge_cases():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is False\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n", "def test_getdoc_inherited_decorated_method_with_functools_wraps():\n    class Foo:\n            \"\"\"\n            docstring\n                indented text\n            \"\"\"\n\n    class Bar(Foo):\n        @functools.wraps(Foo.meth)\n            # inherited and decorated method with functools.wraps\n            pass\n\n    assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") == Foo.meth.__doc__\n    assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") == Foo.meth.__doc__\n"], "sample_683": ["def test_encodedfile_repr(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert repr(ef) == \"<_pytest.capture.EncodedFile name='{0}' mode='w' encoding='utf-8'>\".format(tmpfile.name)\n", "def test_sys_capture_binary_writeorg(capsysbinary):\n    sys.stdout.buffer.write(b\"hello\")\n    sys.stderr.buffer.write(b\"world\")\n    capsysbinary.writeorg(b\"additional output\")\n    captured = capsysbinary.readouterr()\n    assert captured.out == b\"helloadditional output\"\n    assert captured.err == b\"worldadditional output\"\n", "def test_capture_fixture_close_after_teardown(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            capsys.close()\n            print(\"after close\")\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n    assert \"after close\" in result.stdout.str()\n", "def test_encodedfile_seek(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.write(\"hello world\")\n    ef.seek(0, 2)\n    assert ef.tell() == 11\n    ef.seek(6)\n    ef.write(\"there\")\n    ef.seek(0)\n    assert ef.read() == \"hello there\"\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"hello there\"\n    tmpfile.close()\n    with pytest.raises(ValueError):\n        ef.seek(0)\n", "def test_capturing_and_logging_resume(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys, os\n        import py, logging\n        from _pytest import capture\n        cap = capture.MultiCapture(\n            in_=None,\n            out=None,\n            err=capture.FDCapture(2),\n        )\n        cap.start_capturing()\n\n        logging.warning(\"hello1\")\n        outerr = cap.readouterr()\n        print(\"suspend, captured %s\" %(outerr,))\n        logging.warning(\"hello2\")\n\n        cap.pop_outerr_to_orig()\n        logging.warning(\"hello3\")\n\n        cap.suspend_capturing()\n        logging.warning(\"hello4\")\n        cap.resume_capturing()\n\n        outerr = cap.readouterr()\n        print(\"resume, captured %s\" % (outerr,))\n    \"\"\"\n    )\n    result = testdir.runpython(p)\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        suspend, captured*hello1*\n        resume, captured*WARNING:root:hello4*\n    \"\"\"\n    )\n    result.stderr.fnmatch_lines(\n        \"\"\"\n        WARNING:root:hello2\n        WARNING:root:hello3\n    \"\"\"\n    )\n    assert \"atexit\" not in result.stderr.str()\n"], "sample_1054": ["def test_ComplexRegion_from_real():\n    unit = Interval(0, 1)\n    complex_unit = ComplexRegion.from_real(unit)\n    assert complex_unit == ComplexRegion(unit * FiniteSet(0))\n    assert complex_unit.polar == False\n    assert complex_unit.a_interval == unit\n    assert complex_unit.b_interval == FiniteSet(0)\n", "def test_complex_region_from_real():\n    unit = Interval(0, 1)\n    expected = ComplexRegion(unit * FiniteSet(0))\n    assert ComplexRegion.from_real(unit) == expected\n\n    # Test with non-real set\n    raises(ValueError, lambda: ComplexRegion.from_real(S.Complexes))\n", "def test_ComplexRegion_product():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c1 = ComplexRegion(a * b)\n    c2 = ComplexRegion(b * a)\n    assert c1 == c2\n\n    r1 = Interval(0, 1)\n    theta1 = Interval(0, 2*S.Pi)\n    c3 = ComplexRegion(r1 * theta1, polar=True)\n    c4 = ComplexRegion(theta1 * r1, polar=True)\n    assert c3 == c4\n", "def test_complex_region_from_real():\n    a = Interval(2, 5)\n    b = FiniteSet(3, 7)\n    c = Interval(1, 8)\n\n    assert ComplexRegion.from_real(a) == ComplexRegion(a * FiniteSet(0))\n    assert ComplexRegion.from_real(b) == ComplexRegion(b * FiniteSet(0))\n\n    with raises(ValueError):\n        ComplexRegion.from_real(c * b)  # b is not a subset of the real line\n", "def test_issue_12345():\n    # Test the functionality of ComplexRegion with non-trivial expressions\n    x, y = symbols('x y', real=True)\n    c1 = ComplexRegion(Interval(x, y)*Interval(x**2, y**2), polar=True)\n    c2 = ComplexRegion(Interval(x, y)*Interval(-x**2, -y**2), polar=True)\n    assert Intersection(c1, c2) == ComplexRegion(Interval(x, y)*FiniteSet(0), polar=True)\n"], "sample_1182": ["def test_log2():\n    from sympy.functions.elementary.exponential import log\n\n    expr = log(x, 2)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log2(x)'\n", "def test_expm1_log1p():\n    expr1 = expm1(x)\n    expr2 = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.expm1(x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.expm1(x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.expm1(x)'\n    assert prntr.doprint(expr2) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.expm1(x)'\n    assert prntr.doprint(expr2) == 'mpmath.log1p(x)'\n", "def test_log1p_log2():\n    expr1 = log1p(x)\n    expr2 = log2(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log1p(x)'\n    assert prntr.doprint(expr2) == 'numpy.log2(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log1p(x)'\n    assert prntr.doprint(expr2) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log1p(x)'\n    assert prntr.doprint(expr2) == 'math.log(x) / math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log1p(x)'\n    assert prntr.doprint(expr2) == 'mpmath.log(x) / mpmath.log(2)'\n", "def test_log1p_log2():\n    from sympy.core.function import Lambda\n    from sympy.functions import log1p, log2\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(log1p(x)) == 'math.log1p(x)'\n    assert prntr.doprint(log2(x)) == 'math.log(x) / math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(log1p(x)) == 'mpmath.log1p(x)'\n    assert prntr.doprint(log2(x)) == 'mpmath.log(x) / mpmath.log(2)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(log1p(x)) == 'numpy.log1p(x)'\n    assert prntr.doprint(log2(x)) == 'numpy.log2(x)'\n", "def test_log1p_and_expm1():\n    expr1 = log1p(x)\n    expr2 = expm1(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.log1p(x)'\n    assert prntr.doprint(expr2) == 'scipy.special.expm1(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log1p(x)'\n    assert prntr.doprint(expr2) == 'numpy.expm1(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log1p(x)'\n    assert prntr.doprint(expr2) == 'expm1(x)'  # expm1 is a custom function\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log1p(x)'\n    assert \"Not supported\" in prntr.doprint(expr2)  # expm1 is not supported in mpmath\n"], "sample_1160": ["def test_issue_18134_boundary():\n    assert S.Rationals.boundary == S.Reals\n    assert S.Integers.boundary == S.Reals\n    assert S.Naturals.boundary == S.Reals\n    assert S.Naturals0.boundary == S.Reals\n    assert S.Complexes.boundary == S.EmptySet\n    assert S.Reals.boundary == {oo, -oo}\n", "def test_imageset_contain_issue_17858():\n    n = Dummy()\n    s = ImageSet(Lambda(n, -I*(I*(2*pi*n - pi/4) + log(Abs(sqrt(-I))))), S.Integers)\n    assert s.contains(S.Pi) == Contains(S.Pi, s, evaluate=False)\n", "def test_imageset_contains():\n    from sympy.abc import x\n    assert (2, S.Half) in imageset(x, (x, 1/x), S.Integers)\n    assert imageset(x, x + I*3, S.Integers).intersection(S.Reals) is S.EmptySet\n    i = Dummy(integer=True)\n    q = imageset(x, x + I*y, S.Integers).intersection(S.Reals)\n    assert q.subs(y, I*i).intersection(S.Integers) is S.Integers\n    q = imageset(x, x + I*y/x, S.Integers).intersection(S.Reals)\n    assert q.subs(y, 0) is S.Integers\n    assert q.subs(y, I*i*x).intersection(S.Integers) is S.Integers\n    z = cos(1)**2 + sin(1)**2 - 1\n    q = imageset(x, x + I*z, S.Integers).intersection(S.Reals)\n    assert q is not S.EmptySet\n", "def test_imageset_intersection_rational():\n    n = Dummy()\n    s = ImageSet(Lambda(n, (n + 1) / (n - 1)), S.Integers)\n    assert s.intersect(S.Rationals) == ImageSet(Lambda(n, (n + 1) / (n - 1)), S.Integers)\n", "def test_imageset_complex_symbols():\n    x, y = symbols('x y', real=True)\n    assert ImageSet(Lambda(x, x + I*y), S.Integers) == ComplexRegion(S.Integers*S.Reals)\n    assert ImageSet(Lambda(x, x + I*y), S.Reals) == S.Complexes\n    assert ImageSet(Lambda(x, x + I*y), Interval(0, 1)) == ComplexRegion(Interval(0, 1)*S.Reals)\n    z = symbols('z', complex=True)\n    assert ImageSet(Lambda(x, z), S.Integers).is_subset(FiniteSet(z))\n    assert ImageSet(Lambda(x, z), S.Reals).is_subset(ComplexRegion(S.Reals*FiniteSet(z.imag)))\n    assert ImageSet(Lambda(x, z), Interval(0, 1)).is_subset(ComplexRegion(Interval(0, 1)*FiniteSet(z.imag)))\n"], "sample_1006": ["def test_binomial_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).series(n, 0, 3) == binomial(0, k) + k*binomial(1, k)*n + O(n**2)\n    assert binomial(n, k).series(k, 0, 3) == binomial(n, 0) - n*binomial(n - 1, 0)*k + O(k**2)\n", "def test_binomial_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).series(n, 0, 3) == \\\n        binomial(0, k) + n*binomial(1, k) + O(n**2)\n", "def test_subfactorial():\n    n = Symbol('n', integer=True, nonnegative=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n + 1).is_even is None\n    assert subfactorial(n + 2).is_even is True\n    assert subfactorial(n + 3).is_even is True\n\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(n + 1).is_odd is True\n    assert subfactorial(n + 2).is_odd is False\n    assert subfactorial(n + 3).is_odd is False\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(n + 1).is_integer is None\n    assert subfactorial(n + 2).is_integer is True\n    assert subfactorial(n + 3).is_integer is True\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(n + 1).is_nonnegative is None\n    assert subfactorial(n + 2).is_nonnegative is True\n    assert subfactorial(n + 3).is_nonnegative is True\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n", "def test_subfactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    r = Symbol('r', integer=False)\n    s = Symbol('s', integer=False, negative=True)\n    t = Symbol('t', nonnegative=True)\n    u = Symbol('u', noninteger=True)\n\n    assert subfactorial(-1) == 0\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(n).func == subfactorial\n\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n    assert subfactorial(r).is_integer is None\n\n    assert subfactorial(x).is_nonnegative is None\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_nonnegative is True\n    assert subfactorial(r).is_nonnegative is None\n\n    assert subfactorial(x).is_odd is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(k).is_odd is None\n    assert subfactorial(k + 1).is_odd is True\n    assert subfactorial(r).is_odd is None\n    assert subfactorial(s).is_odd is None\n    assert subfactorial(t).is_odd is None\n    assert subfactorial(u).is_odd is None\n\n    assert subfactorial(x).is_even is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(k - 1).is_even is True\n    assert subfactorial(r).is_even is None\n    assert", "def test_binomial_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).series(n, 0, 3) == bin(n).series(n, 0, 3).func(*[s.subs(n, k + n) for s in bin(n).series(n, 0, 3).args])\n"], "sample_208": ["def test_add_model_with_removed_field_from_base_model(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_alter_textfield_and_charfield(self):\n    \"\"\"\n    #23405 - Altering a `CharField` or `TextField`\n    without default should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_with_biography_blank], [self.author_with_biography_non_blank])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\", \"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_add_field_with_foreign_key_to_self(self):\n    \"\"\"#23938 - Adding a ForeignKey field to self should work.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_foreign_key_to_self])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"parent\")\n", "def test_add_field_with_default_db_column(self):\n    \"\"\"#26171 - Adding a field with a default should not require a db_column.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_name_default_db_column])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, db_column=None)\n", "    def test_alter_model_options_with_custom_base_manager(self):\n        \"\"\"Changing custom base manager of a model should also make a change.\"\"\"\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [(\"id\", models.AutoField(primary_key=True))],\n                bases=(models.Model,),\n                options={\"base_manager_name\": \"custom_manager\"},\n                managers=[(\"custom_manager\", models.Manager())],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [(\"id\", models.AutoField(primary_key=True))],\n                bases=(models.Model,),\n                options={\"base_manager_name\": \"new_custom_manager\"},\n                managers=[(\"new_custom_manager\", models.Manager())],\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelManagers\", \"AlterModelOptions\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", managers=[(\"custom_manager\", None)])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"author\", options={\"base_manager_name\": \"new_custom_manager\"})\n"], "sample_233": ["def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with a different hashing algorithm by\n    using the PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'sha512'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertEqual(p1.algorithm, settings.DEFAULT_HASHING_ALGORITHM)\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with a different algorithm by using the\n    PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'sha512'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertEqual(p1.algorithm, settings.DEFAULT_HASHING_ALGORITHM)\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with a different algorithm by\n    using the PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'sha3_256'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with a different algorithm than the\n    DEFAULT_HASHING_ALGORITHM by using the PasswordResetTokenGenerator.algorithm\n    attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'md5'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertEqual(p1.algorithm, settings.DEFAULT_HASHING_ALGORITHM)\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with an algorithm other than DEFAULT_HASHING_ALGORITHM by\n    using the PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'sha3_256'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertEqual(p1.algorithm, settings.DEFAULT_HASHING_ALGORITHM)\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n"], "sample_496": ["    def test_multiple_suggestions(self):\n        args = ['makemig', '--settings=test_project.settings']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'makemig'. Did you mean makemigrations or migrate?\")\n", "    def setUp(self):\n        self.write_settings('settings.py')\n", "def test_unknown_command_with_suggestion(self):\n    args = ['rnserver']\n    out, err = self.run_django_admin(args)\n    self.assertNoOutput(out)\n    self.assertOutput(err, \"Unknown command: 'rnserver'. Did you mean runserver?\")\n", "def test_manage_command(self):\n    \"\"\"\n    Test that the manage.py command calls the execute_from_command_line function.\n    \"\"\"\n    with mock.patch('django.core.management.execute_from_command_line') as mock_execute_from_command_line:\n        self.run_manage(['help'])\n        mock_execute_from_command_line.assert_called_once_with(['manage.py', 'help'])\n", "def test_pythonpath(self):\n    \"\"\"\n    The --pythonpath option allows to extend sys.path from the command line\n    \"\"\"\n    custom_path = os.path.join(self.test_dir, 'custom_path')\n    os.mkdir(custom_path)\n    os.mkdir(os.path.join(custom_path, 'custom_module'))\n    with open(os.path.join(custom_path, 'custom_module', '__init__.py'), 'w'):\n        pass\n    with open(os.path.join(custom_path, 'custom_module', 'custom_command.py'), 'w') as f:\n        f.write(\"from django.core.management.base import BaseCommand\\n\\n\"\n                \"class Command(BaseCommand):\\n\"\n                \"    help = 'Custom command for testing --pythonpath'\\n\"\n                \"    requires_system_checks = False\\n\\n\"\n                \"    def handle(self, *args, **options):\\n\"\n                \"        pass\")\n    args = ['custom_command', '--pythonpath', custom_path]\n    out, err = self.run_manage(args)\n    self.assertNoOutput(err)\n    self.assertNoOutput(out)\n"], "sample_190": ["def test_contains_query_rhs(self):\n    qs = Article.objects.filter(headline__contains=OuterRef('headline'))\n    articles = Article.objects.annotate(\n        contains_headline=Exists(qs),\n    ).filter(\n        contains_headline=True,\n    )\n    self.assertCountEqual(articles, Article.objects.all())\n", "def test_isnull_deprecated_value(self):\n    msg = (\n        'Using a non-boolean value for an isnull lookup is deprecated, use '\n        'True or False instead.'\n    )\n    with self.assertWarnsMessage(RemovedInDjango40Warning, msg):\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__isnull='foo'),\n            [],\n        )\n", "def test_exact_query_rhs_with_selected_columns_limit_one(self):\n    newest_author = Author.objects.create(name='Author 3')\n    authors_max_ids = Author.objects.filter(\n        name='Author 3',\n    ).values(\n        'name',\n    ).annotate(\n        max_id=Max('id'),\n    ).values('max_id')\n    authors = Author.objects.filter(id__exact=authors_max_ids[:1])\n    self.assertEqual(authors.get(), newest_author)\n", "def test_exact_query_rhs_with_multiple_columns_in_selected_columns(self):\n    new_author = Author.objects.create(name='Author 2', alias='a2')\n    authors_max_ids = Author.objects.filter(\n        name='Author 2',\n        alias='a2',\n    ).values(\n        'name',\n        'alias',\n    ).annotate(\n        max_id=Max('id'),\n    ).values('name', 'alias', 'max_id')\n    authors = Author.objects.filter(\n        name=authors_max_ids[:1].values('name'),\n        alias=authors_max_ids[:1].values('alias'),\n        id=authors_max_ids[:1].values('max_id'),\n    )\n    self.assertEqual(authors.get(), new_author)\n", "def test_exact_with_uuid(self):\n    # Test __exact lookup with a UUIDField\n    import uuid\n    obj = Freebie.objects.create(uuid_field=uuid.UUID('123e4567-e89b-12d3-a456-426614174000'))\n    self.assertEqual(Freebie.objects.get(uuid_field=uuid.UUID('123e4567-e89b-12d3-a456-426614174000')), obj)\n    self.assertEqual(Freebie.objects.get(uuid_field='123e4567-e89b-12d3-a456-426614174000'), obj)\n"], "sample_841": ["def test_ridge_classifier_cv_no_support_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n", "def test_ridge_regression_with_sample_weight():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    coef = rng.randn(n_features)\n    y = np.dot(X, coef) + 0.01 * rng.randn(n_samples)\n    alpha = 1.0\n    sample_weight = rng.rand(n_samples)\n    coef_weighted = ridge_regression(X, y, alpha=alpha, sample_weight=sample_weight)\n    coef_unweighted = ridge_regression(X, y, alpha=alpha)\n    assert_array_almost_equal(coef_weighted, coef_unweighted, decimal=3)\n", "def test_ridge_regression_sparse_input(arr_type):\n    rng = np.random.RandomState(42)\n    X = rng.rand(1000, 3)\n    true_coefs = [1, 2, 0.1]\n    y = np.dot(X, true_coefs)\n    X_testing = arr_type(X)\n\n    alpha, atol, tol = 1e-3, 1e-4, 1e-6\n    out = ridge_regression(X_testing, y, alpha=alpha, solver='auto', tol=tol)\n    assert_allclose(out, true_coefs, rtol=0, atol=atol)\n", "def test_ridge_regression_fit_intercept(solver):\n    X, y, coef = _make_sparse_offset_regression(\n        n_samples=10, n_features=5, random_state=0, coef=True)\n    ridge = Ridge(alpha=1.0, solver=solver, fit_intercept=True)\n    ridge.fit(X, y)\n    assert_allclose(ridge.coef_, coef, atol=1e-5)\n", "def test_ridge_fit_intercept_sparse_sag_max_iter_warning():\n    X, y = _make_sparse_offset_regression(\n        n_features=5, n_samples=20, random_state=0, X_offset=5.)\n    X_csr = sp.csr_matrix(X)\n\n    params = dict(alpha=1., solver='sag', fit_intercept=True,\n                  tol=1e-10, max_iter=5)  # reduce max_iter to trigger warning\n    sparse_ridge = Ridge(**params)\n    with pytest.warns(UserWarning, match='\"sag\" solver requires.*'):\n        sparse_ridge.fit(X_csr, y)\n"], "sample_876": ["def test_mlp_partial_fit_with_early_stopping(MLPEstimator):\n    \"\"\"Check that early stopping works with partial fit.\"\"\"\n    mlp = MLPEstimator(\n        max_iter=10, random_state=0, warm_start=True, early_stopping=True\n    )\n    mlp.partial_fit(X_iris[:50], y_iris[:50], classes=np.unique(y_iris))\n    n_validation_scores = len(mlp.validation_scores_)\n    mlp.partial_fit(X_iris[50:], y_iris[50:])\n    assert len(mlp.validation_scores_) > n_validation_scores\n", "def test_mlp_learning_rate_init():\n    # Test that the learning_rate_init parameter affects the training process\n    X, y = make_regression(n_samples=50, n_features=5, n_targets=1, random_state=0)\n\n    # The coefficients will be identical if both use the same learning_rate_init\n    for learning_rate_init in [0.01, 0.1]:\n        mlp1 = MLPRegressor(\n            hidden_layer_sizes=1,\n            max_iter=1,\n            batch_size=1,\n            random_state=0,\n            learning_rate_init=learning_rate_init,\n        )\n        mlp2 = MLPRegressor(\n            hidden_layer_sizes=1,\n            max_iter=1,\n            batch_size=1,\n            random_state=0,\n            learning_rate_init=learning_rate_init,\n        )\n        mlp1.fit(X, y)\n        mlp2.fit(X, y)\n\n        assert np.array_equal(mlp1.coefs_[0], mlp2.coefs_[0])\n\n    # The coefficients will be slightly different if learning_rate_init is different\n    mlp1 = MLPRegressor(\n        hidden_layer_sizes=1, max_iter=1, batch_size=1, random_state=0, learning_rate_init=0.01\n    )\n    mlp2 = MLPRegressor(\n        hidden_layer_sizes=1, max_iter=1, batch_size=1, random_state=0, learning_rate_init=0.1\n    )\n    mlp1.fit(X, y)\n    mlp2.fit(X, y)\n\n    assert not np.array_equal(mlp1.coefs_[0], mlp2.coefs_[0])\n", "def test_batch_size_auto():\n    # Test that batch_size=\"auto\" sets batch_size to 200 or n_samples\n    # whichever is smaller\n    X, y = make_classification(n_samples=300, n_features=20, random_state=1)\n    clf = MLPClassifier(solver=\"sgd\", batch_size=\"auto\", random_state=1)\n    clf.fit(X, y)\n    assert clf._optimizer.batch_size == 200\n\n    X, y = make_classification(n_samples=150, n_features=20, random_state=1)\n    clf = MLPClassifier(solver=\"sgd\", batch_size=\"auto\", random_state=1)\n    clf.fit(X, y)\n    assert clf._optimizer.batch_size == 150\n", "def test_mlp_classifier_early_stopping_with_validation_fraction():\n    # Test early stopping with validation_fraction parameter\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n    tol = 0.2\n    validation_fraction = 0.2\n    mlp_estimator = MLPClassifier(\n        tol=tol, max_iter=3000, solver=\"sgd\", early_stopping=True, validation_fraction=validation_fraction\n    )\n    mlp_estimator.fit(X, y)\n    assert mlp_estimator.max_iter > mlp_estimator.n_iter_\n    assert len(mlp_estimator.validation_scores_) == mlp_estimator.n_iter_\n    assert int(len(X) * validation_fraction) == int(len(mlp_estimator.validation_scores_) * (1 / (1 - validation_fraction)))\n", "def test_mlp_regressor_early_stopping_validation_fraction():\n    X = X_reg\n    y = y_reg\n\n    # Test early stopping with validation_fraction parameter\n    mlp = MLPRegressor(\n        solver=\"sgd\",\n        max_iter=100,\n        activation=\"relu\",\n        random_state=1,\n        learning_rate_init=0.01,\n        batch_size=X.shape[0],\n        early_stopping=True,\n        validation_fraction=0.2,\n    )\n    mlp.fit(X, y)\n    assert mlp.n_iter_ < 100\n    assert mlp.best_loss_ is None\n    assert isinstance(mlp.validation_scores_, list)\n\n    valid_scores = mlp.validation_scores_\n    best_valid_score = mlp.best_validation_score_\n    assert max(valid_scores) == best_valid_score\n"], "sample_145": ["def test_actions_unique_with_different_names(self):\n        action1.__name__ = 'action1'\n        pass\n\n        action2.__name__ = 'action2'\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_actions_unique_with_different_names(self):\n        action1.__name__ = 'action1'\n        pass\n\n        action2.__name__ = 'action2'\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_actions_unique_with_different_names(self):\n        action1.__name__ = 'unique_name1'\n        pass\n\n        action2.__name__ = 'unique_name2'\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_actions_unique_different_names(self):\n        pass\n\n    action1.__name__ = 'unique_name'\n\n        pass\n\n    action2.__name__ = 'another_unique_name'\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "def test_custom_permissions_are_valid(self):\n        pass\n\n    custom_permission_action.allowed_permissions = ('custom',)\n\n    class BandAdmin(ModelAdmin):\n        actions = (custom_permission_action,)\n            return True\n\n    self.assertIsValid(BandAdmin, Band)\n"], "sample_479": ["def test_create_model_add_index(self):\n    \"\"\"\n    AddIndex should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n            ),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"], name=\"name_idx\")),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"indexes\": [models.Index(fields=[\"name\"], name=\"name_idx\")]},\n            ),\n        ],\n    )\n", "def test_create_model_rename_model_optimize(self):\n    \"\"\"\n    RenameModel should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.RenameModel(\"Foo\", \"Bar\"),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Bar\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_create_model_and_index_operations(self):\n    \"\"\"\n    CreateModel and AddIndex/RemoveIndex should not collapse into nothing.\n    \"\"\"\n    self.assertDoesNotOptimize(\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))]\n            ),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"])),\n            migrations.RemoveIndex(\"Foo\", \"name\"),\n            migrations.DeleteModel(\"Foo\"),\n        ],\n    )\n", "def test_create_model_alter_table_comment(self):\n    \"\"\"\n    AlterModelTableComment should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterModelTableComment(\"Foo\", \"New table comment\"),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\", \"db_table_comment\": \"New table comment\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_rename_model_alter_index(self):\n    \"\"\"\n    RenameModel should be absorbed by AlterIndexTogether.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255)), (\"age\", models.IntegerField())]\n            ),\n            migrations.AlterIndexTogether(\"Foo\", [[\"name\", \"age\"]]),\n            migrations.RenameModel(\"Foo\", \"Bar\"),\n        ],\n        [\n            migrations.CreateModel(\n                \"Bar\", [(\"name\", models.CharField(max_length=255)), (\"age\", models.IntegerField())]\n            ),\n            migrations.AlterIndexTogether(\"Bar\", [[\"name\", \"age\"]]),\n        ],\n    )\n"], "sample_313": ["def test_mixed_template_engines(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            ROOT / 'templates/django',\n            Path.cwd() / 'template_tests/templates/jinja2',\n        }\n    )\n", "def test_non_django_backend(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n        }\n    )\n", "def test_ignore_non_django_templates(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n        }\n    )\n", "    def test_custom_template_tags_changed(self, mock_reset):\n        tag_path = Path(__file__).parent / 'template_tests' / 'custom_tags.py'\n        self.assertTrue(autoreload.template_changed(None, tag_path))\n        mock_reset.assert_called_once()\n", "    def test_get_template_directories_excludes_django_path(self, mock_get_dirs):\n        mock_get_dirs.return_value = ['/path/to/django', EXTRA_TEMPLATES_DIR]\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                EXTRA_TEMPLATES_DIR,\n                ROOT / 'templates',\n            }\n        )\n"], "sample_258": ["def test_receiver_with_sender(self):\n    @receiver(a_signal, sender=self)\n        self.state = val\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n    a_signal.send(sender=object(), val=True)\n    self.assertFalse(self.state)\n", "def test_receiver_decorator_with_sender(self):\n    @receiver(a_signal, sender=self)\n        self.state = val\n\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n    a_signal.disconnect(f, sender=self)\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertFalse(self.state)\n", "def test_receiver_sender_filter(self):\n    @receiver(a_signal, sender=self)\n        self.state = val\n\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n\n    self.state = False\n    a_signal.send(sender=object(), val=True)\n    self.assertFalse(self.state)\n", "def test_receiver_signal_sender(self):\n    @receiver(a_signal, sender=self)\n        self.state = val\n\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n\n    self.state = False\n    a_signal.send(sender=object(), val=True)\n    self.assertFalse(self.state)\n", "def test_receiver_sender_specified(self):\n    @receiver(a_signal, sender=self)\n        self.state = val\n\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n    self.state = False\n    a_signal.send(sender=object(), val=True)\n    self.assertFalse(self.state)\n"], "sample_645": ["def test_caplog_handles_errors(caplog):\n    caplog.set_level(logging.ERROR)\n    try:\n        raise Exception(\"test exception\")\n    except Exception:\n        logger.exception(\"Caught an exception\")\n    assert \"Caught an exception\" in caplog.text\n    assert \"test exception\" in caplog.text\n", "def test_log_access_with_format(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\", \"arg\")\n    assert caplog.records[0].levelname == \"INFO\"\n    assert caplog.records[0].msg == \"boo %s\"\n    assert \"boo arg\" in caplog.text\n    assert caplog.records[0].args == (\"arg\",)\n", "def test_caplog_records_persist_across_stages(caplog, logging_during_setup_and_teardown):\n    logger.info(\"a_call_log\")\n    assert [x.message for x in caplog.records] == [\"a_setup_log\", \"a_call_log\"]\n\n    logger.info(\"another_call_log\")\n    assert [x.message for x in caplog.records] == [\"a_setup_log\", \"a_call_log\", \"another_call_log\"]\n", "def test_caplog_captures_after_clear(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"first log\")\n    assert \"first log\" in caplog.text\n\n    caplog.clear()\n    logger.info(\"second log\")\n    assert \"second log\" in caplog.text\n    assert \"first log\" not in caplog.text\n", "def test_caplog_captures_with_multiple_handlers(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logger = logging.getLogger('catchlog')\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.WARNING\n\n            logger.warning(\"WARNING message will be shown\")\n\n            handler = logging.StreamHandler()\n            handler.setLevel(logging.INFO)\n            formatter = logging.Formatter('%(levelname)s - %(message)s')\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n\n            logger.info(\"INFO message will be shown by new handler\")\n\n            assert \"WARNING message will be shown\" in caplog.text\n            assert \"INFO message will be shown by new handler\" in caplog.text\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_level=WARNING\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n"], "sample_501": ["def test_legend_fontsize():\n    \"\"\"Test legend fontsize\"\"\"\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    ax.plot(x, 50 - x, 'o', label='y=1')\n    ax.legend(loc='best', fontsize='small')\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    legend = ax.legend()\n\n    # Test setting bbox_to_anchor with a tuple\n    legend.set_bbox_to_anchor((0.5, 0.5))\n    assert legend.get_bbox_to_anchor().x0 == 0.5\n    assert legend.get_bbox_to_anchor().y0 == 0.5\n\n    # Test setting bbox_to_anchor with a Bbox\n    from matplotlib.transforms import Bbox\n    bbox = Bbox.from_bounds(0.2, 0.2, 0.3, 0.3)\n    legend.set_bbox_to_anchor(bbox)\n    assert legend.get_bbox_to_anchor() == bbox\n\n    # Test setting bbox_to_anchor with None\n    legend.set_bbox_to_anchor(None)\n    assert legend.get_bbox_to_anchor() == ax.bbox\n", "def test_legend_markerfirst():\n    # Test markerfirst parameter\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line', marker='o')\n    leg = ax.legend(markerfirst=False)\n    handlebox, textbox = leg._legend_handle_box._children[0]._children\n    assert textbox._text.get_position()[0] < handlebox._children[0]._children[0]._children[0]._children[0].get_position()[0]\n", "def test_legend_no_handle_label():\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10), label='hello world')\n    with mock.patch('matplotlib.legend.Legend') as Legend:\n        ax.legend(handles=lines, labels=[''])\n    Legend.assert_called_with(ax, lines, [''], loc='best')\n", "def test_handlerline2d_with_points():\n    \"\"\"Test legend handlerline2d with points and linestyles.\"\"\"\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 1000)\n    ax.plot(x, np.sin(x), '-', label='sin', marker='o', markevery=50)\n    ax.plot(x, np.cos(x), '--', label='cos', marker='s', markevery=100)\n    ax.legend()\n"], "sample_1144": ["def test_requires_partial_complex():\n    x, y, z = symbols('x y z', complex=True)\n    f = x * y\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, y)) is True\n    assert requires_partial(Derivative(f, z)) is True\n    assert requires_partial(Derivative(f, x, y)) is True\n    assert requires_partial(Derivative(f, x, z)) is True\n    assert requires_partial(Derivative(f, y, z)) is True\n    assert requires_partial(Derivative(f, x, y, z)) is True\n", "def test_requires_partial_with_integer_variable():\n    x, n = symbols('x n', integer=True)\n\n    f = x ** n\n    assert requires_partial(Derivative(f, x)) is False\n    assert requires_partial(Derivative(f, n)) is False\n", "def test_requires_partial_mixed_variables():\n    x, y, z = symbols('x y z')\n    f = Function('f')\n    # Check that a function with both integer and non-integer variables requires a partial derivative symbol\n    assert requires_partial(Derivative(f(x, y, z), x, y)) is True\n    # Check that a function with only integer variables does not require a partial derivative symbol\n    n = symbols('n', integer=True)\n    assert requires_partial(Derivative(f(n, z), n, z)) is False\n", "def test_requires_partial_multi_var_function():\n    x, y, z = symbols('x y z')\n    f = Function('f')(x, y, z)\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, y)) is True\n    assert requires_partial(Derivative(f, z)) is True\n    assert requires_partial(Derivative(f, x, y)) is True\n    assert requires_partial(Derivative(f, y, z)) is True\n    assert requires_partial(Derivative(f, x, z)) is True\n    assert requires_partial(Derivative(f, x, y, z)) is True\n", "def test_requires_partial_multiple_variables():\n    x, y, z, t = symbols('x y z t')\n    f = x * y * z\n    assert requires_partial(Derivative(f, x, y, z)) is True\n    assert requires_partial(Derivative(f, x, y)) is True\n    assert requires_partial(Derivative(f, x, z)) is True\n    assert requires_partial(Derivative(f, y, z)) is True\n\n    # parametric equation with multiple variables\n    f = (exp(t), cos(t), t)\n    g = sum(f)\n    assert requires_partial(Derivative(g, t)) is False\n    assert requires_partial(Derivative(g, x, y, z)) is True\n"], "sample_991": ["def test_issue_13546_simplification():\n    n = Symbol('n')\n    k = Symbol('k')\n    p = Product(n + 1 / 2**k, (k, 0, n-1)).doit()\n    assert simplify(p) == (n*2**n - 2**n + 1) / (2**n - 1)\n", "def test_product_with_negative_range():\n    i = Symbol(\"i\", integer=True)\n\n    # Product with a negative range\n    m = 5\n    n = 3\n\n    a = m\n    b = n - 1\n    P = Product(i, (i, a, b)).doit()\n\n    assert P == 1 / Product(i, (i, n, m)).doit()\n", "def test_product_with_negative_limits():\n    assert product(k, (k, 5, 1)) == factorial(5) / factorial(1)\n    assert product(k, (k, 1, 5)).doit() == factorial(5)\n    assert product(k, (k, 5, 1)).doit() == factorial(5) / factorial(1)\n    assert product(k, (k, n, n-3)).doit() == factorial(n) / factorial(n-3)\n    assert product(k, (k, n, n-3)) == factorial(n) / factorial(n-3)\n    assert product(k, (k, n, n)).doit() == 1\n    assert product(k, (k, n, n)) == 1\n", "def test_product_with_fractional_limits():\n    n = Symbol('n', real=True)\n    p = Product(1/sqrt(n), (n, 1, oo))\n    assert p.is_convergent() is S.false\n    assert product(1/sqrt(n), (n, 1, oo)) == p.doit()\n", "def test_issue_18522():\n    n = Symbol('n')\n    k = Symbol('k')\n    p = Product(n**k, (k, 0, n-1)).doit()\n    assert p.subs(n, 3).doit() == 6\n"], "sample_144": ["def test_filter_with_child_fk(self):\n    r = Restaurant.objects.create()\n    s = Supplier.objects.create(restaurant=r)\n    # The mismatch between Supplier and Place is intentional (#28175).\n    self.assertSequenceEqual(Restaurant.objects.filter(supplier__in=Place.objects.all()), [r])\n", "def test_get_FIELD_display(self):\n    \"\"\"\n    Test the _get_FIELD_display() method.\n    \"\"\"\n    politician = Politician.objects.create(name='John Doe', party='D')\n    self.assertEqual(politician._get_FIELD_display('party'), 'Democrat')\n\n    congressman = Congressman.objects.create(name='Jane Doe', district=1)\n    self.assertEqual(congressman._get_FIELD_display('district'), '1st District')\n", "def test_inheritance_select_related_reverse(self):\n    # Test for #28560 - select_related() with reverse ForeignKey.\n    r = Restaurant.objects.create(\n        name=\"Nobu\", serves_hot_dogs=True, serves_pizza=False\n    )\n    s = Supplier.objects.create(restaurant=r)\n\n    self.assertQuerysetEqual(\n        Restaurant.objects.order_by(\"name\").select_related(\"supplier\"), [\n            r,\n        ],\n        attrgetter(\"name\")\n    )\n\n    nobu = Restaurant.objects.order_by(\"name\").select_related(\"supplier\")[0]\n    self.assertEqual(nobu.supplier.name, \"s\")\n", "def test_queryset_update_with_explicit_pk(self):\n    \"\"\"\n    Test updating a model with an explicit primary key.\n    \"\"\"\n    derivedm1 = DerivedM.objects.create(\n        customPK=44,\n        base_name=\"b1\",\n        derived_name='d1',\n    )\n    self.assertEqual(derivedm1.customPK, 44)\n    self.assertEqual(derivedm1.base_name, 'b1')\n    self.assertEqual(derivedm1.derived_name, 'd1')\n\n    # Update the instance using the queryset update method\n    updated = DerivedM.objects.filter(customPK=44).update(base_name=\"b2\", derived_name=\"d2\")\n    self.assertEqual(updated, 1)\n\n    # Fetch the updated instance\n    derivedm1.refresh_from_db()\n\n    self.assertEqual(derivedm1.customPK, 44)\n    self.assertEqual(derivedm1.base_name, 'b2')\n    self.assertEqual(derivedm1.derived_name, 'd2')\n", "def test_get_order_and_set_order_methods(self):\n    parent1 = Politician.objects.create(name='Parent1', state='State1')\n    child1 = Senator.objects.create(name='Child1', title='Senator', state='State1', politician_ptr=parent1)\n    child2 = Senator.objects.create(name='Child2', title='Senator', state='State1', politician_ptr=parent1)\n\n    # Test get_order method\n    order = parent1.get_senator_order()\n    self.assertListEqual(list(order), [child1.id, child2.id])\n\n    # Test set_order method\n    new_order = [child2.id, child1.id]\n    parent1.set_senator_order(new_order)\n    updated_order = parent1.get_senator_order()\n    self.assertListEqual(list(updated_order), new_order)\n"], "sample_749": ["def test_column_transformer_invalid_remainder_transformer():\n\n    class NoTrans(BaseEstimator):\n            return self\n\n            return X\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])], remainder=NoTrans())\n    assert_raise_message(TypeError, \"All estimators should implement fit and transform\", ct.fit, X_array)\n", "def test_column_transformer_invalid_remainder_transformer():\n    class NoTrans(BaseEstimator):\n            return self\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])], remainder=NoTrans())\n    assert_raise_message(TypeError, \"All estimators should implement transform\", ct.fit, X_array)\n", "def test_column_transformer_invalid_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # test with invalid remainder values\n    for remainder in [123, np.array([1, 2])]:\n        ct = ColumnTransformer([('trans', Trans(), [0])], remainder=remainder)\n        assert_raise_message(\n            ValueError,\n            \"remainder keyword needs to be one of 'drop', 'passthrough', \"\n            \"or estimator.\", ct.fit, X_array)\n        assert_raise_message(\n            ValueError,\n            \"remainder keyword needs to be one of 'drop', 'passthrough', \"\n            \"or estimator.\", ct.fit_transform, X_array)\n", "def test_column_transformer_remainder_with_transformer_weights():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans(),\n                           transformer_weights={'trans1': 2.0, 'remainder': 0.5})\n\n    X_res_both = X_array.copy()\n    # first column is transformed by 2 and doubled by 0.5\n    X_res_both[:, 0] *= 2.0 * 0.5\n    # second and third columns are doubled by 0.5\n    X_res_both[:, 1:3] *= 0.5\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_remainder_transformer_numpy_input():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    X_res_both = X_array.copy()\n\n    # second and third columns are doubled when remainder = DoubleTrans\n    X_res_both[:, 1:3] *= 2\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans())\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n"], "sample_1016": ["def test_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == 'existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_LambertW_printing():\n    assert mcode(LambertW(x)) == 'lambertw(x)'\n    assert mcode(LambertW(x, n)) == 'lambertw(n, x)'\n", "def test_OctaveCodePrinter_methods():\n    printer = OctaveCodePrinter()\n    assert printer._rate_index_position(2) == 10\n    assert printer._get_statement(\"code\") == \"code;\"\n    assert printer._get_comment(\"comment\") == \"% comment\"\n    assert printer._declare_number_const(\"name\", \"value\") == \"name = value;\"\n    assert printer._format_code([\"line1\", \"line2\"]) == [\"  line1\", \"  line2\"]\n    assert list(printer._traverse_matrix_indices(Matrix([[1, 2], [3, 4]]))) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n", "def test_LambertW_with_MatrixElement():\n    A = MatrixSymbol(\"A\", 1, 1)\n    assert mcode(LambertW(A[0, 0])) == 'lambertw(A(1, 1))'\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 5, 5)\n    assert mcode(A[:2, :2]) == \"A(1:2, 1:2)\"\n    assert mcode(A[1:4, 1:4]) == \"A(2:4, 2:4)\"\n    assert mcode(A[::2, ::2]) == \"A(1:2:end, 1:2:end)\"\n    assert mcode(A[1:, 1:]) == \"A(2:end, 2:end)\"\n    assert mcode(A[:, 1:]) == \"A(:, 2:end)\"\n    assert mcode(A[1, :]) == \"A(2, :)\"\n"], "sample_131": ["    def test_serialize_db_to_string(self):\n        # Test that the serialize_db_to_string method correctly serializes data\n        creation = connection.creation_class(connection)\n        with mock.patch('django.db.models.query.QuerySet.iterator') as mock_iterator:\n            mock_iterator.return_value = [MockModelInstance()]\n            serialized_data = creation.serialize_db_to_string()\n            self.assertIsInstance(serialized_data, str)\n            self.assertIn('\"model\": \"testapp.mockmodel\"', serialized_data)\n", "    def test_keepdb_true(self):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['KEEPDB'] = True\n            with mock.patch.object(creation, '_destroy_test_db') as mocked_destroy_test_db:\n                creation.destroy_test_db(verbosity=0, keepdb=True)\n            mocked_destroy_test_db.assert_not_called()\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called()\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n"], "sample_256": ["    def test_validates_password(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'testclient',\n            'password2': 'testclient',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form[\"password2\"].errors), 2)\n        self.assertIn('The password is too similar to the username.', form[\"password2\"].errors)\n        self.assertIn(\n            'This password is too short. It must contain at least 12 characters.',\n            form[\"password2\"].errors\n        )\n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'password',  # this password is too short\n            'password2': 'password',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            'This password is too short. It must contain at least 8 characters.',\n            form.errors['password2']\n        )\n", "    def test_integer_username(self):\n        user = IntegerUsernameUser.objects.create_user(username=123456789, password='password')\n        data = {\n            'username': 123456789,\n            'password': 'password',\n        }\n        form = AuthenticationForm(None, data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['username'], data['username'])\n        self.assertEqual(form.cleaned_data['password'], data['password'])\n        self.assertEqual(form.user_cache, user)\n", "    def test_unicode_email(self):\n        email = 'test@example.com'\n        user = User.objects.create_user('test_user', 'test_password', email)\n        user.email = 't\u00ebst@example.com'\n        user.save()\n        form = PasswordResetForm({'email': email})\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(form.cleaned_data['email'], email)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, [email])\n", "def test_password_reset_form_validates_password(self):\n    (user, username, email) = self.create_dummy_user()\n    form = PasswordResetForm({'email': email})\n    self.assertTrue(form.is_valid())\n    form.save(domain_override='example.com')\n    token = context.get_url_kwargs()['token']\n    uid = context.get_url_kwargs()['uid']\n    data = {\n        'new_password1': username,\n        'new_password2': username,\n    }\n    form = SetPasswordForm(user, data)\n    self.assertFalse(form.is_valid())\n    self.assertEqual(len(form[\"new_password2\"].errors), 2)\n    self.assertIn('The password is too similar to the username.', form[\"new_password2\"].errors)\n    self.assertIn('This password is too short. It must contain at least 12 characters.', form[\"new_password2\"].errors)\n"], "sample_331": ["    def test_parse_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P2DT3H4M5S', timedelta(days=2, hours=3, minutes=4, seconds=5)),\n            ('P23DT23H', timedelta(days=23, hours=23)),\n            ('P4Y', None),  # ISO 8601 duration doesn't support years\n            ('P4M', None),  # ISO 8601 duration doesn't support months\n            ('P1W', None),  # ISO 8601 duration doesn't support weeks\n            ('P-1D', timedelta(days=-1)),\n            ('P-2DT-3H-4M-5S', timedelta(days=-2, hours=-3, minutes=-4, seconds=-5)),\n            ('P-23DT-23H', timedelta(days=-23, hours=-23)),\n            ('P0.5D', timedelta(hours=12)),\n            ('PT0.5M', timedelta(seconds=30)),\n            ('PT0.5S', timedelta(milliseconds=500)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_iso8601_format(self):\n        test_values = (\n            ('P4Y', timedelta(days=4*365)),  # years\n            ('P4M', timedelta(days=4*30)),  # months\n            ('P4W', timedelta(weeks=4)),  # weeks\n            ('P4D', timedelta(days=4)),  # days\n            ('PT10H', timedelta(hours=10)),  # hours\n            ('PT10M', timedelta(minutes=10)),  # minutes\n            ('PT10S', timedelta(seconds=10)),  # seconds\n            ('PT10.5S', timedelta(seconds=10, microseconds=500000)),  # seconds with fraction\n            ('P4DT10H30M', timedelta(days=4, hours=10, minutes=30)),  # days, hours, minutes\n            ('P4DT10.5S', timedelta(days=4, seconds=10, microseconds=500000)),  # days, seconds with fraction\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_datetime_invalid_offsets(self):\n        invalid_inputs = (\n            '2012-04-23T09:15:00+25:00',\n            '2012-04-23T09:15:00-25:00',\n            '2012-04-23T09:15:00+99:59',\n            '2012-04-23T09:15:00-99:59',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n", "def test_parse_duration_iso8601(self):\n    test_values = (\n        ('P4Y', timedelta(days=4*365)),  # Years\n        ('P4M', None),  # Months are not supported by timedelta\n        ('P4W', timedelta(weeks=4)),  # Weeks\n        ('P4D', timedelta(days=4)),  # Days\n        ('PT4H', timedelta(hours=4)),  # Hours\n        ('PT4M', timedelta(minutes=4)),  # Minutes\n        ('PT4.5S', timedelta(seconds=4, microseconds=500000)),  # Seconds with fraction\n        ('P4DT4H30M20.5S', timedelta(days=4, hours=4, minutes=30, seconds=20, microseconds=500000)),  # Combined duration\n        ('P-4DT-4H-30M-20.5S', timedelta(days=-4, hours=-4, minutes=-30, seconds=-20, microseconds=-500000)),  # Negative duration\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT1S', timedelta(days=1, seconds=1)),\n            ('P1Y2M3DT4H5M6S', timedelta(days=753, hours=4, minutes=5, seconds=6)),\n            ('P1Y2M3W4DT5H6M7S', timedelta(days=798, hours=5, minutes=6, seconds=7)),\n            ('P0.5Y', timedelta(days=182)),\n            ('P0.5M', timedelta(days=15)),\n            ('P0.5W', timedelta(days=3)),\n            ('P0.5D', timedelta(hours=12)),\n            ('P0.5H', timedelta(minutes=30)),\n            ('P0.5M', timedelta(seconds=30)),\n            ('P0.5S', timedelta(microseconds=500000)),\n            ('P-1Y2M3W4DT5H6M7S', timedelta(days=-798, hours=-5, minutes=-6, seconds=-7)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_217": ["def test_value_omitted_from_data(self):\n    widget = MyWidget1()\n    data = {'name': 'value'}\n    self.assertFalse(widget.value_omitted_from_data(data, files=None, name='name'))\n    self.assertTrue(widget.value_omitted_from_data(data, files=None, name='other_name'))\n", "def test_media_inheritance_no_overwrite(self):\n    # A widget can disable media inheritance for specific media types\n    # by specifying 'extend' as a tuple\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n    class MyWidget13(MyWidget1):\n        class Media:\n            extend = ('js',)\n            css = {\n                'all': ('/path/to/css3', 'path/to/css1')\n            }\n            js = ('/path/to/js1', '/path/to/js4')\n\n    w13 = MyWidget13()\n    self.assertEqual(\n        str(w13.media),\n        \"\"\"<link href=\"/path/to/css3\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_inheritance_overrides_extend(self):\n    # A widget can override media inheritance by specifying 'extend=False' in a child class\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n    class MyWidget13(MyWidget1):\n        class Media:\n            extend = True\n            css = {\n                'all': ('/path/to/css3', 'path/to/css1')\n            }\n            js = ('/path/to/js1', '/path/to/js4')\n\n    class MyWidget14(MyWidget13):\n        class Media:\n            extend = False\n            css = {\n                'all': ('/path/to/css4',)\n            }\n            js = ('/path/to/js5',)\n\n    w14 = MyWidget14()\n    self.assertEqual(\n        str(w14.media),\n        \"\"\"<link href=\"/path/to/css4\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_overriding(self):\n    ###############################################################\n    # Media overriding for forms\n    ###############################################################\n\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n    class MyWidget2(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css4', '/path/to/css5')\n            }\n            js = ('/path/to/js5', '/path/to/js6')\n\n    class FormWithMedia(Form):\n        field1 = CharField(max_length=20, widget=MyWidget1())\n\n        class Media:\n            js = ('/some/form/javascript',)\n            css = {\n                'all': ('/some/form/css',)\n            }\n\n    f3 = FormWithMedia()\n    f3.fields['field1'].widget = MyWidget2()\n    self.assertEqual(\n        str(f3.media),\n        \"\"\"<link href=\"/path/to/css4\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_widget_media_inheritance(self):\n    class ParentWidget(TextInput):\n        class Media:\n            css = {\n                'all': ('parent.css',)\n            }\n            js = ('parent.js',)\n\n    class ChildWidget(ParentWidget):\n        class Media:\n            css = {\n                'all': ('child.css',)\n            }\n            js = ('child.js',)\n\n    parent_widget = ParentWidget()\n    child_widget = ChildWidget()\n    self.assertEqual(str(child_widget.media), \"\"\"<link href=\"parent.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_981": ["def test_inverse():\n    p = Permutation([0, 2, 1, 3])\n    assert ~p == Permutation([0, 2, 1, 3])\n    assert p * ~p == Permutation(3)\n    assert ~(~p) == p\n", "def test_mul_inv():\n    a = Permutation([0, 2, 1, 3])\n    b = Permutation([1, 2, 3, 0])\n    assert a.mul_inv(b) == Permutation([3, 2, 0, 1])\n", "def test_Cycle_size():\n    assert Cycle(1, 2).size == 3\n    assert Cycle(1, 2, size=4).size == 4\n    assert Cycle(1, 2).list(5) == [0, 2, 1, 3, 4]\n    assert Cycle(1, 2, 3, 4).size == 5\n", "def test_Permutation_coercion():\n    p = Permutation([0, 1, 2])\n    assert Permutation(p) == p\n    assert Permutation([p, [3]]) == Permutation([0, 1, 2, 3])\n    assert Permutation([p, [1, 2]]) == Permutation([0, 2, 1])\n    assert Permutation([p, Permutation([3, 4])]) == Permutation([0, 1, 2, 3, 4])\n    assert Permutation([p, Permutation([1, 2, 3])]) == Permutation([0, 2, 3, 1])\n", "def test_cycle_structure():\n    assert Permutation(3).cycle_structure == {1: 4}\n    assert Permutation(0, 4, 3)(1, 2)(5, 6).cycle_structure == {2: 2, 3: 1}\n    assert Permutation([1, 0, 2, 3]).cycle_structure == {2: 1, 1: 2}\n"], "sample_1003": ["def test_Method_postprocess():\n    opt = {'method': 'sympy'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'sympy'}\n", "def test_Gen_preprocess():\n    assert Gen.preprocess(x) == x\n    assert Gen.preprocess(10) == 10\n\n    raises(OptionError, lambda: Gen.preprocess(\"invalid\"))\n", "def test_Method_postprocess():\n    opt = {'method': 'HEAP'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'heap'}\n", "def test_Options_init_default_flags():\n    opt = Options((x, y, z), {'domain': 'ZZ'})\n\n    assert opt.auto is True\n    assert opt.frac is False\n    assert opt.formal is False\n    assert opt.polys is None\n    assert opt.include is False\n    assert opt.all is False\n    assert opt.gen == 0\n    assert opt.series is False\n    assert opt.method is None\n", "def test_Options_strict():\n    # Testing strict mode\n    opt = Options((x, y, z), {'domain': 'ZZ', 'strict': True})\n    assert opt.strict is True\n\n    # Testing strict mode with an unallowed flag\n    raises(OptionError, lambda: Options((x, y, z), {'domain': 'ZZ', 'strict': True, 'not_an_option': True}))\n\n    # Testing strict mode with an unallowed option\n    raises(OptionError, lambda: Options((x, y, z), {'domain': 'ZZ', 'strict': True, 'not_an_option': 'value'}))\n"], "sample_997": ["def test_implicit_multiplication():\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr(\"xy\", transformations=transformations) == x*y\n", "def test_convert_equals_signs_multiple():\n    transformations = standard_transformations + (convert_equals_signs, )\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"1*2=x=3*4\", transformations=transformations) == Eq(Eq(2, x), Eq(12, y))\n", "def test_convert_xor_to_exponentiation():\n    from sympy.parsing.sympy_parser import convert_xor\n\n    transformations = standard_transformations + (convert_xor,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"x^y\", transformations=transformations) == x**y\n", "def test_convert_xor_to_power():\n    transformations = standard_transformations + \\\n                      (convert_xor,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"x^y\", transformations=transformations) == Pow(x, y)\n", "def test_implicit_multiplication():\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr(\"xy\") == x*y\n    assert parse_expr(\"xy\", transformations=transformations) == x*y\n    assert parse_expr(\"2x\") == 2*x\n    assert parse_expr(\"2x\", transformations=transformations) == 2*x\n    assert parse_expr(\"x y\") == x*y\n    assert parse_expr(\"x y\", transformations=transformations) == x*y\n    assert parse_expr(\"sin xy\") == sin(xy)\n    assert parse_expr(\"sin xy\", transformations=transformations) == sin(x*y)\n    assert parse_expr(\"sin x y\") == sin(x)*y\n    assert parse_expr(\"sin x y\", transformations=transformations) == sin(x)*y\n    assert parse_expr(\"sin x (cos y)\") == sin(x)*cos(y)\n    assert parse_expr(\"sin x (cos y)\", transformations=transformations) == sin(x)*cos(y)\n    assert parse_expr(\"(cos x) sin y\") == cos(x)*sin(y)\n    assert parse_expr(\"(cos x) sin y\", transformations=transformations) == cos(x)*sin(y)\n    assert parse_expr(\"(cos x) (sin y)\") == cos(x)*sin(y)\n    assert parse_expr(\"(cos x) (sin y)\", transformations=transformations) == cos(x)*sin(y)\n    assert parse_expr(\"2 (x + 1)\") == 2*(x + 1)\n    assert parse_expr(\"2 (x + 1)\", transformations=transformations) == 2*(x + 1)\n    assert parse_expr(\"(x + 1) 2\") == (x + 1)*2\n    assert parse_expr(\"(x + 1) 2\", transformations=transformations) == (x + 1)*2\n    assert parse_expr(\"(x + 1) (x + 2"], "sample_558": ["def test_imagegrid_cbar_mode_single():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(9, 9))\n\n    positions = (221, 222, 223, 224)\n    cbar_locations = ['left', 'right', 'top', 'bottom']\n\n    for position, location in zip(positions, cbar_locations):\n        grid = ImageGrid(fig, position,\n                         nrows_ncols=(1, 1),\n                         cbar_location=location,\n                         cbar_size='20%',\n                         cbar_mode='single')\n        ax = grid[0]\n\n        ax.imshow(arr, cmap='nipy_spectral')\n\n        cb = ax.cax.colorbar(ax.images[0])\n", "def test_imagegrid_axes_position(direction):\n    \"\"\"Test positioning of the axes in ImageGrid.\"\"\"\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, (2, 2), direction=direction)\n    loc = [ax.get_axes_locator() for ax in np.ravel(grid.axes_row)]\n    # Test nx.\n    assert loc[1].args[0] > loc[0].args[0]\n    assert loc[0].args[0] == loc[2].args[0]\n    assert loc[3].args[0] == loc[1].args[0]\n    # Test ny.\n    assert loc[2].args[1] < loc[0].args[1]\n    assert loc[0].args[1] == loc[1].args[1]\n    assert loc[3].args[1] == loc[2].args[1]\n", "def test_grid_aspect_ratio():\n    arr1 = np.arange(20).reshape((4, 5))\n    arr2 = np.arange(20).reshape((5, 4))\n\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    ax1.imshow(arr1)\n    ax2.imshow(arr2)\n\n    grid = Grid(fig, 111, (1, 2), aspect=False)\n    grid[0].set_axes_locator(grid.get_divider().new_locator(0))\n    grid[1].set_axes_locator(grid.get_divider().new_locator(1))\n\n    fig.canvas.draw()\n    p1 = ax1.get_position()\n    p2 = ax2.get_position()\n    assert p1.width / p1.height == pytest.approx(5 / 4)\n    assert p2.width / p2.height == pytest.approx(4 / 5)\n", "def test_imagegrid_cbar_mode_single():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111,\n                     nrows_ncols=(2, 2),\n                     cbar_location=\"right\",\n                     cbar_mode=\"single\",\n                     cbar_size=\"7%\",\n                     cbar_pad=\"2%\")\n    ax1, ax2, ax3, ax4 = grid\n\n    ax1.imshow(arr, cmap='nipy_spectral')\n    ax2.imshow(arr.T, cmap='hot')\n    ax3.imshow(np.hypot(arr, arr.T), cmap='jet')\n    ax4.imshow(np.arctan2(arr, arr.T), cmap='hsv')\n\n    for ax in grid:\n        ax.cax.colorbar(ax.images[0])\n\n    fig.canvas.draw()\n    # Add assertions to verify the layout of the colorbars\n", "def test_imagegrid_cbar_mode_single():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(12, 6))\n\n    positions = (121, 122)\n    directions = ['row', 'column']\n    cbar_locations = ['left', 'right']\n\n    for position, direction, location in zip(\n            positions, directions, cbar_locations):\n        grid = ImageGrid(fig, position,\n                         nrows_ncols=(2, 2),\n                         direction=direction,\n                         cbar_location=location,\n                         cbar_size='10%',\n                         cbar_mode='single')\n        ax1, ax2 = grid\n\n        ax1.imshow(arr, cmap='nipy_spectral')\n        ax2.imshow(arr.T, cmap='hot')\n\n        # The single colorbar for each row/column\n        cb = grid.cbar_axes[0].colorbar(ax1.images[0])\n"], "sample_1098": ["def test_appellf1_properties():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    assert appellf1(a, b1, b2, c, x, y).is_commutative is True\n    assert appellf1(a, b1, b2, c, x, y).is_number is False\n    assert appellf1(a, b1, b2, c, x, y).is_integer is False\n", "def test_appellf1_properties():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    f = appellf1(a, b1, b2, c, x, y)\n    assert f.is_commutative\n    assert f.diff(a) == Derivative(f, a)\n    assert f.diff(b1) == Derivative(f, b1)\n    assert f.diff(b2) == Derivative(f, b2)\n    assert f.diff(c) == Derivative(f, c)\n", "def test_appellf1_differentiation():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    f = appellf1(a, b1, b2, c, x, y)\n    assert td(f, x)\n    assert td(f, y)\n    assert td(f, a)\n", "def test_hyper_nseries():\n    from sympy import series\n    assert hyper((1, 2), (3,), z).nseries(z, n=2) == 1 + z + O(z**2)\n    assert hyper((1, 2), (3, 4), z).nseries(z, n=3) == 1 + z + z**2/2 + O(z**3)\n    assert hyper((1, 2, 3), (4,), z).nseries(z, n=1) == O(z)\n    assert hyper((1, 2), (3, 4), z).nseries(z, n=0) == O(1)\n", "def test_meijerg_special_cases():\n    # Test special cases of Meijer G function that can be expressed in terms of other functions\n    assert meijerg([[], []], [[0], []], -x) == exp(x)\n    assert meijerg([[], []], [[Rational(1, 2)], []], (x/2)**2) == sqrt(pi)*cos(x)\n    assert meijerg([[], []], [[1], []], -x) == exp(x)\n    assert meijerg([[], []], [[Rational(1, 2)], [0]], (x/2)**2) == sqrt(pi)*cos(x)/sqrt(2)\n    assert meijerg([[], []], [[Rational(3, 2)], []], (x/2)**2) == sqrt(pi)*sin(x)/sqrt(2)\n    assert meijerg([[Rational(1, 2)], []], [[Rational(3, 2)], []], -x**2) == sqrt(pi)*sin(x)/sqrt(2*x)\n    assert meijerg([[Rational(1, 2)], []], [[Rational(1, 2)], []], -x**2) == sqrt(pi)*cos(x)/sqrt(2*x)\n    assert meijerg([[], []], [[0], [Rational(1, 2)]], -x**2) == exp(-x**2)\n    assert meijerg([[], []], [[0], [1]], -x**2) == exp(-x**2)\n"], "sample_746": ["def test_log_loss_sample_weight():\n    y_true = np.array([1, 1, 2, 2])\n    y_pred = np.array([[0.1, 0.9], [0.1, 0.9], [0.8, 0.2], [0.3, 0.7]])\n    sample_weight = np.array([0.2, 0.3, 0.4, 0.5])\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight)\n    true_loss = (-0.2 * np.log(0.9) - 0.3 * np.log(0.9) - 0.4 * np.log(0.2) - 0.5 * np.log(0.7)) / 2.4\n    assert_almost_equal(loss, true_loss)\n", "def test_log_loss_with_sample_weights():\n    # test log_loss with sample weights\n    y_true = [0, 1, 0, 1, 0]\n    y_pred = [[0.9, 0.1], [0.2, 0.8], [0.7, 0.3], [0.1, 0.9], [0.6, 0.4]]\n    sample_weight = [1, 2, 1, 3, 2]\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight, normalize=False)\n    assert_almost_equal(loss, 6.09861229, decimal=6)\n", "def test_log_loss_with_sample_weights():\n    # Test log_loss function with sample weights\n    y_true = [0, 1, 0, 1]\n    y_pred = [[0.9, 0.1], [0.2, 0.8], [0.7, 0.3], [0.4, 0.6]]\n    sample_weight = [1, 2, 3, 4]\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight)\n    true_loss = (1 * 0.10536051565782628 + 2 * 0.7287930235320348 +\n                 3 * 1.0986122886681098 + 4 * 0.916290731874155) / 10\n    assert_almost_equal(loss, true_loss)\n", "def test_log_loss_with_weights():\n    # case when sample_weight is given\n    y_true = [0, 1, 2]\n    y_pred = [[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]]\n    sample_weight = [1, 2, 3]\n    loss = log_loss(y_true, y_pred, normalize=True, sample_weight=sample_weight)\n    assert_almost_equal(loss, 0.5520865)\n\n    # check if normalize=False works correctly\n    loss = log_loss(y_true, y_pred, normalize=False, sample_weight=sample_weight)\n    assert_almost_equal(loss, 0.5520865 * sum(sample_weight))\n", "def test_brier_score_loss_binary():\n    # Test brier_score_loss for binary classification task\n    y_true, y_prob, _ = make_prediction(binary=True, return_proba=True)\n\n    # Test that brier_score_loss returns the same result with '1' and '-1' as positive label\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label=1),\n                        brier_score_loss(y_true, y_prob, pos_label=-1))\n\n    # Test that brier_score_loss returns the same result with 'ham' and 'spam' as labels\n    y_true_str = np.array([\"ham\", \"spam\", \"ham\", \"spam\"])\n    assert_almost_equal(brier_score_loss(y_true_str, y_prob, pos_label=\"spam\"),\n                        brier_score_loss(y_true, y_prob, pos_label=1))\n\n    # Test that brier_score_loss raises an error for multiclass classification\n    y_true_multiclass = np.array([0, 1, 2, 0])\n    assert_raises(ValueError, brier_score_loss, y_true_multiclass, y_prob)\n\n    # Test that brier_score_loss raises an error for invalid probabilities\n    y_prob_invalid = np.array([0.1, 1.1, 0.5, 0.8])\n    assert_raises(ValueError, brier_score_loss, y_true, y_prob_invalid)\n"], "sample_244": ["def test_formset_with_file_field(self):\n    \"\"\"FormSet works with FileField.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    FileFormSet = formset_factory(FileForm)\n    formset = FileFormSet(data={}, files={'form-0-file': SimpleUploadedFile('test.txt', b'file_content')})\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.cleaned_data, [{'file': SimpleUploadedFile('test.txt', b'file_content')}])\n", "def test_formset_with_extra_fields(self):\n    \"\"\"\n    FormSets can have additional fields that are not part of the form.\n    \"\"\"\n    class BaseExtraFieldsFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields['extra_field'] = CharField()\n\n    ExtraFieldsFormSet = formset_factory(ArticleForm, formset=BaseExtraFieldsFormSet)\n    formset = ExtraFieldsFormSet()\n    self.assertIn('extra_field', formset.forms[0].fields)\n", "def test_formset_with_file_upload(self):\n    \"\"\"\n    FormSets with FileField work as expected.\n    \"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    FileFormSet = formset_factory(FileForm)\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    files = {\n        'form-0-file': SimpleUploadedFile(\"test.txt\", b\"file_content\"),\n    }\n    formset = FileFormSet(data, files, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(len(formset.forms[0].files), 1)\n", "    def test_initial_data_with_empty_forms(self):\n        \"\"\"\n        A FormSet can be prefilled with existing data by providing a list of\n        dicts to the `initial` argument. Extra empty forms are included.\n        \"\"\"\n        initial = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n        ChoiceFormSet = formset_factory(Choice, extra=1)\n        formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_formset_with_file_field(self):\n    \"\"\"Formsets with a FileField.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    FileFormSet = formset_factory(FileForm)\n    formset = FileFormSet()\n    # An empty formset with a FileField is not multipart\n    self.assertFalse(formset.is_multipart())\n    # A file is present\n    formset = FileFormSet({'form-0-file': SimpleUploadedFile('file.txt', b'file_content')})\n    self.assertTrue(formset.is_multipart())\n"], "sample_884": ["def test_pickle_deprecated():\n    # Test if deprecated classes and functions can be pickled and unpickled\n    pickled_class1 = pickle.dumps(MockClass1())\n    pickled_class2 = pickle.dumps(MockClass2())\n    pickled_class3 = pickle.dumps(MockClass3())\n    pickled_class5 = pickle.dumps(MockClass5(42))\n    pickled_class6 = pickle.dumps(MockClass6(42))\n    pickled_function = pickle.dumps(mock_function)\n\n    unpickled_class1 = pickle.loads(pickled_class1)\n    unpickled_class2 = pickle.loads(pickled_class2)\n    unpickled_class3 = pickle.loads(pickled_class3)\n    unpickled_class5 = pickle.loads(pickled_class5)\n    unpickled_class6 = pickle.loads(pickled_class6)\n    unpickled_function = pickle.loads(pickled_function)\n\n    assert isinstance(unpickled_class1, MockClass1)\n    assert isinstance(unpickled_class2, MockClass2)\n    assert isinstance(unpickled_class3, MockClass3)\n    assert isinstance(unpickled_class5, MockClass5)\n    assert isinstance(unpickled_class6, MockClass6)\n    assert unpickled_function() == 10\n", "def test_deprecated_pickle():\n    # Test if deprecated classes and functions can be pickled\n    pickled_class = pickle.dumps(MockClass1)\n    unpickled_class = pickle.loads(pickled_class)\n    with pytest.warns(FutureWarning, match=\"qwerty\"):\n        unpickled_class()\n\n    pickled_function = pickle.dumps(mock_function)\n    unpickled_function = pickle.loads(pickled_function)\n    with pytest.warns(FutureWarning, match=\"deprecated\"):\n        val = unpickled_function()\n    assert val == 10\n", "def test_pickle_deprecated():\n    # Test if deprecated classes and functions can be pickled and unpickled\n    obj = MockClass1()\n    pickled_obj = pickle.dumps(obj)\n    unpickled_obj = pickle.loads(pickled_obj)\n    assert isinstance(unpickled_obj, MockClass1)\n\n    func = mock_function\n    pickled_func = pickle.dumps(func)\n    unpickled_func = pickle.loads(pickled_func)\n    assert unpickled_func() == 10\n", "def test_pickle_deprecated():\n    # Test if deprecated classes and functions can be pickled and unpickled\n    mock_class1 = MockClass1()\n    pickled_mock_class1 = pickle.dumps(mock_class1)\n    unpickled_mock_class1 = pickle.loads(pickled_mock_class1)\n    assert isinstance(unpickled_mock_class1, MockClass1)\n\n    pickled_mock_function = pickle.dumps(mock_function)\n    unpickled_mock_function = pickle.loads(pickled_mock_function)\n    assert unpickled_mock_function() == 10\n", "def test_pickle_deprecated():\n    # Test pickling and unpickling of deprecated objects\n    # This test ensures that deprecated objects can be pickled and unpickled correctly\n    obj = MockClass1()\n    pickled_obj = pickle.dumps(obj)\n    unpickled_obj = pickle.loads(pickled_obj)\n    assert type(unpickled_obj) == MockClass1\n\n    with pytest.warns(FutureWarning, match=\"qwerty\"):\n        unpickled_obj.__new__(MockClass1)\n\n    func = mock_function\n    pickled_func = pickle.dumps(func)\n    unpickled_func = pickle.loads(pickled_func)\n    assert _is_deprecated(unpickled_func)\n    with pytest.warns(FutureWarning, match=\"deprecated\"):\n        val = unpickled_func()\n    assert val == 10\n"], "sample_264": ["    def test_cookie_settings_alternative(self):\n        \"\"\"\n        CookieStorage honors SESSION_COOKIE_DOMAIN, SESSION_COOKIE_SECURE, and\n        SESSION_COOKIE_HTTPONLY with alternative settings (#15618, #20972).\n        \"\"\"\n        # Test before the messages have been consumed\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.add(constants.INFO, 'test')\n        storage.update(response)\n        messages = storage._decode(response.cookies['messages'].value)\n        self.assertEqual(len(messages), 1)\n        self.assertEqual(messages[0].message, 'test')\n        self.assertEqual(response.cookies['messages']['domain'], '.test.com')\n        self.assertEqual(response.cookies['messages']['expires'], '')\n        self.assertIs(response.cookies['messages']['secure'], False)\n        self.assertIs(response.cookies['messages']['httponly'], False)\n        self.assertEqual(response.cookies['messages']['samesite'], '')\n\n        # Test deletion of the cookie (storing with an empty value) after the messages have been consumed\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.add(constants.INFO, 'test')\n        for m in storage:\n            pass  # Iterate through the storage to simulate consumption of messages.\n        storage.update(response)\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['domain'], '.test.com')\n        self.assertEqual(response.cookies['messages']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(response.cookies['messages']['samesite'], '')\n", "def test_legacy_hash_deprecation(self):\n    \"\"\"\n    The legacy hash used for encoding and decoding messages is deprecated and\n    will be removed in Django 4.0.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    # Encode/decode a message using the pre-Django 3.1 hash.\n    encoder = MessageEncoder()\n    value = encoder.encode(messages)\n    encoded_messages = '%s$%s' % (storage._legacy_hash(value), value)\n    with self.assertWarns(RemovedInDjango40Warning):\n        decoded_messages = storage._decode(encoded_messages)\n    self.assertEqual(messages, decoded_messages)\n", "def test_legacy_hash_decode_invalid(self):\n    # Test that decoding fails when the hash is invalid.\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    encoder = MessageEncoder()\n    value = encoder.encode(messages)\n    invalid_hash = storage._legacy_hash('invalid value')\n    encoded_messages = '%s$%s' % (invalid_hash, value)\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_legacy_decode_invalid(self):\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    encoder = MessageEncoder()\n    value = encoder.encode(messages)\n    encoded_messages = '%s$%s' % (storage._legacy_hash(value), 'invalid data')\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_legacy_hash_compatibility(self):\n    \"\"\"\n    The legacy hash function can still decode messages encoded with Django 3.1.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    messages = ['legacy', 'message']\n    # Encode a message using the legacy hash function.\n    value = json.dumps(messages)\n    encoded_messages = '%s$%s' % (storage._legacy_hash(value), value)\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertEqual(messages, decoded_messages)\n"], "sample_127": ["def test_bulk_insert_ignore_conflicts(self):\n    Country.objects.bulk_create(self.data)\n    with self.assertRaises(IntegrityError):\n        Country.objects.bulk_create(self.data, ignore_conflicts=True)\n", "def test_ignore_conflicts_no_duplicates(self):\n    TwoFields.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.count(), len(self.data))\n    # Try to insert the same data again\n    TwoFields.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.count(), len(self.data))\n", "def test_bulk_insert_with_ignore_conflicts(self):\n    TwoFields.objects.bulk_create([\n        TwoFields(f1=i, f2=i+1) for i in range(5)\n    ])\n    with self.assertRaises(IntegrityError):\n        TwoFields.objects.bulk_create([\n            TwoFields(f1=1, f2=2)  # This will cause a conflict\n        ], ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.count(), 5)  # The count should remain unchanged\n", "def test_ignore_conflicts(self):\n    initial_data = [\n        TwoFields(id=1, f1=1, f2=2),\n        TwoFields(id=2, f1=3, f2=4),\n    ]\n    TwoFields.objects.bulk_create(initial_data)\n    new_data = [\n        TwoFields(id=1, f1=5, f2=6),  # This will be ignored because id=1 already exists\n        TwoFields(id=3, f1=7, f2=8),  # This will be inserted\n    ]\n    TwoFields.objects.bulk_create(new_data, ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.count(), 3)\n    self.assertEqual(TwoFields.objects.get(id=1).f1, 1)  # The existing record should not be updated\n    self.assertEqual(TwoFields.objects.get(id=3).f1, 7)  # The new record should be inserted\n", "def test_ignore_conflicts(self):\n    TwoFields.objects.bulk_create([TwoFields(f1=1, f2=2), TwoFields(f1=1, f2=3)], ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.count(), 1)\n    with self.assertRaises(IntegrityError):\n        TwoFields.objects.bulk_create([TwoFields(f1=1, f2=4)], ignore_conflicts=True)\n"], "sample_951": ["def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is True\n", "def test_is_builtin_class_method_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n    assert inspect.is_builtin_class_method(int, 'my_method') is False\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n", "def test_is_builtin_class_method_builtins():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(int, '__add__') is True\n    assert inspect.is_builtin_class_method(str, '__init__') is False\n    assert inspect.is_builtin_class_method(list, '__init__') is False\n    assert inspect.is_builtin_class_method(dict, '__init__') is False\n    assert inspect.is_builtin_class_method(object, '__init__') is False\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig, globals())\n\n    assert evaluated_sig.parameters['x'].annotation == list\n    assert evaluated_sig.parameters['y'].annotation == types.GenericAlias(types.FunctionType, (int, str))\n    assert evaluated_sig.parameters['z'].annotation == types.UnionType(None, types.GenericAlias(dict, (str, int)))\n    assert evaluated_sig.return_annotation == tuple\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @func.register(int)\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.register) is False\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(meth) is False\n"], "sample_838": ["def test_column_transformer_named_columns():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n    norm = Normalizer()\n    ct = ColumnTransformer([('norm', norm, ['first'])])\n    ct.fit(X_df)\n    assert 'norm' in ct.named_transformers_\n    assert ct.named_transformers_.norm is norm\n", "def test_column_transformer_column_reordering():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n    ct = ColumnTransformer([('trans1', Trans(), ['first']),\n                            ('trans2', Trans(), ['second'])],\n                           remainder='passthrough')\n    ct.fit(X_df)\n\n    # Reorder columns\n    X_df_reordered = X_df[['second', 'first']]\n    with pytest.raises(ValueError, match='Column ordering must be equal for fit and for transform'):\n        ct.transform(X_df_reordered)\n", "def test_column_transformer_multiple_transformers_same_column():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # multiple transformers applied to the same column\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', DoubleTrans(), [0])])\n    exp = np.array([[0], [2], [4]])\n    assert_array_equal(ct.fit_transform(X_array), exp)\n    assert_array_equal(ct.fit(X_array).transform(X_array), exp)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_column_reordering():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n    ct = ColumnTransformer([('trans', Trans(), 'first')], remainder='passthrough')\n    ct.fit(X_df)\n    # Reorder the columns of the input dataframe\n    X_df_reordered = X_df.copy()\n    X_df_reordered = X_df_reordered[['second', 'first']]\n    with pytest.raises(ValueError, match='Column ordering must be equal for fit and for transform'):\n        ct.transform(X_df_reordered)\n", "def test_column_transformer_reorder_columns_pandas():\n    pd = pytest.importorskip('pandas')\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n    # test with dataframe and named columns\n    ct = ColumnTransformer([('trans1', Trans(), ['second']),\n                            ('trans2', Trans(), ['first'])],\n                           remainder='passthrough')\n    # reordered columns, so should raise error\n    X_df_reordered = X_df[['second', 'first']]\n    with pytest.raises(ValueError,\n                       match=\"Column ordering must be equal for fit and for \"\n                             \"transform when using the remainder keyword\"):\n        ct.fit(X_df).transform(X_df_reordered)\n\n    # test with dataframe and integer columns\n    X_df.columns = [1, 0]\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           remainder='passthrough')\n    # reordered columns, so should raise error\n    X_df_reordered = X_df[[0, 1]]\n    with pytest.raises(ValueError,\n                       match=\"Column ordering must be equal for fit and for \"\n                             \"transform when using the remainder keyword\"):\n        ct.fit(X_df).transform(X_df_reordered)\n"], "sample_475": ["def test_actions_unique_with_different_names(self):\n    @admin.action\n        pass\n\n    @admin.action(name=\"custom_name\")\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_unique_with_different_name(self):\n        @admin.action(name=\"custom_action\")\n            pass\n\n        @admin.action(name=\"custom_action\")\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"__name__ attributes of actions defined in BandAdmin must be \"\n            \"unique. Name 'custom_action' is not unique.\",\n            id=\"admin.E130\",\n        )\n", "    def test_actions_callable_name(self):\n            pass\n\n        custom_action.__name__ = 'custom_action_with_long_name'\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"__name__ attributes of actions defined in BandAdmin must be \"\n            \"unique. Name 'custom_action_with_long_name' is not unique.\",\n            id=\"admin.E130\",\n        )\n", "    def test_actions_unique_with_different_names(self):\n        @admin.action\n            pass\n\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsValid(BandAdmin, Band)\n", "def test_custom_permissions_require_matching_has_method_multiple_permissions(self):\n    @admin.action(permissions=[\"custom1\", \"custom2\"])\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (custom_permission_action,)\n\n    errors = BandAdmin(Band, AdminSite()).check()\n    self.assertEqual(len(errors), 2)\n    self.assertEqual(\n        errors[0].msg,\n        \"BandAdmin must define a has_custom1_permission() method for the \"\n        \"custom_permission_action action.\",\n    )\n    self.assertEqual(\n        errors[1].msg,\n        \"BandAdmin must define a has_custom2_permission() method for the \"\n        \"custom_permission_action action.\",\n    )\n"], "sample_404": ["def test_compile_tag_error_in_included_template(self):\n    \"\"\"Errors in an included template are displayed correctly.\"\"\"\n    e = self._engine(app_dirs=True)\n    template = e.get_template(\"test_include_tag_error.html\")\n    context = Context()\n    with self.assertRaises(TemplateDoesNotExist) as cm:\n        template.render(context)\n    if self.debug_engine:\n        self.assertEqual(\n            cm.exception.template_debug[\"during\"],\n            escape('{% include \"missing.html\" %}'),\n        )\n", "def test_compile_filter_expression_error_in_extended_block(self):\n    \"\"\"Errors in compile filter expression of extended block are displayed correctly.\"\"\"\n    e = self._engine(app_dirs=True)\n    template = e.get_template(\"test_extends_filter_error.html\")\n    context = Context()\n    with self.assertRaises(TemplateSyntaxError) as cm:\n        template.render(context)\n    if self.debug_engine:\n        self.assertEqual(\n            cm.exception.template_debug[\"during\"],\n            escape('{{ missing|filter@missing }}'),\n        )\n", "def test_text_node_render(self):\n    \"\"\"\n    TextNode should return the text as is without any modification.\n    \"\"\"\n    node = TextNode(\"This is a test text node.\")\n    context = Context()\n    self.assertEqual(node.render(context), \"This is a test text node.\")\n", "def test_compile_filter_expression_error_with_variable(self):\n    \"\"\"\n    Test that the correct token is highlighted for FilterExpression errors\n    when a variable is involved.\n    \"\"\"\n    engine = self._engine()\n    msg = \"Could not find variable at start of 'filter_does_not_exist|variable'.\"\n\n    with self.assertRaisesMessage(TemplateSyntaxError, msg) as e:\n        engine.from_string(\"{{ filter_does_not_exist|variable }}\")\n\n    if self.debug_engine:\n        debug = e.exception.template_debug\n        self.assertEqual((debug[\"start\"], debug[\"end\"]), (0, 35))\n        self.assertEqual((debug[\"during\"]), \"{{ filter_does_not_exist|variable }}\")\n", "def test_compile_filter_expression_error_with_args(self):\n    \"\"\"\n    Test FilterExpression errors with arguments.\n    \"\"\"\n    engine = self._engine()\n    msg = \"Could not parse the remainder: '|default:bar' from 'foo|default:bar'\"\n\n    with self.assertRaisesMessage(TemplateSyntaxError, msg) as e:\n        engine.from_string(\"{% if 1 %}{{ foo|default:bar }}{% endif %}\")\n\n    if self.debug_engine:\n        debug = e.exception.template_debug\n        self.assertEqual((debug[\"start\"], debug[\"end\"]), (10, 32))\n        self.assertEqual((debug[\"during\"]), \"{{ foo|default:bar }}\")\n"], "sample_149": ["def test_method_fields_in_required_fields(self):\n    \"\"\"METHODS should not appear in REQUIRED_FIELDS.\"\"\"\n    class CustomUserMethodFields(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        date_of_birth = models.DateField()\n\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = ['get_full_name']\n\n            return self.username\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The field named as the 'USERNAME_FIELD' for a custom user model \"\n            \"must not be included in 'REQUIRED_FIELDS'.\",\n            obj=CustomUserMethodFields,\n            id='auth.E002',\n        ),\n    ])\n", "    def test_username_field_existence(self):\n        \"\"\"USERNAME_FIELD must be defined in a custom user model.\"\"\"\n        class CustomUser(AbstractBaseUser):\n            USERNAME_FIELD = None\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"'CustomUser' must have a USERNAME_FIELD attribute.\",\n                obj=CustomUser,\n                id='auth.E004',\n            ),\n        ])\n", "def test_is_authenticated_method(self):\n    \"\"\"is_authenticated should not be a method.\"\"\"\n    class CustomUserNonMethodIsAuthenticated(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        @property\n            return False\n\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Critical(\n            '%s.is_authenticated must be an attribute or property rather '\n            'than a method. Ignoring this is a security issue as anonymous '\n            'users will be treated as authenticated!' % CustomUserNonMethodIsAuthenticated,\n            obj=CustomUserNonMethodIsAuthenticated,\n            id='auth.C010',\n        ),\n    ])\n", "def test_is_staff_method(self):\n    \"\"\"\n    <User Model>.is_staff must be an attribute or property rather than a method.\n    \"\"\"\n    class CustomUserWithMethodIsStaff(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Critical(\n            '%s.is_staff must be an attribute or property rather than '\n            'a method. Ignoring this is a security issue as anonymous '\n            'users will be treated as staff!' % CustomUserWithMethodIsStaff,\n            obj=CustomUserWithMethodIsStaff,\n            id='auth.C011',\n        ),\n    ])\n", "    def test_username_field_valid(self):\n        \"\"\"The USERNAME_FIELD must be a valid field name for the model.\"\"\"\n        class CustomUserBadUsernameField(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            date_of_birth = models.DateField()\n\n            USERNAME_FIELD = 'invalid_field'\n            REQUIRED_FIELDS = ['date_of_birth']\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The 'USERNAME_FIELD' of a custom user model must be the name of a \"\n                \"field on the model.\",\n                obj=CustomUserBadUsernameField,\n                id='auth.E004',\n            ),\n        ])\n"], "sample_830": ["def test_show_versions():\n    # Redirect stdout to a buffer to capture print output\n    import io\n    import sys\n    captured_output = io.StringIO()\n    sys.stdout = captured_output\n\n    # Call the function\n    show_versions()\n\n    # Reset redirect\n    sys.stdout = sys.__stdout__\n\n    # Check if the expected output is present in the captured output\n    assert \"System:\" in captured_output.getvalue()\n    assert \"BLAS:\" in captured_output.getvalue()\n    assert \"Python deps:\" in captured_output.getvalue()\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n    assert \"System:\" in captured.out\n    assert \"BLAS:\" in captured.out\n    assert \"Python deps:\" in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    out, err = capsys.readouterr()\n\n    assert 'System:' in out\n    assert 'python' in out\n    assert 'executable' in out\n    assert 'machine' in out\n\n    assert 'BLAS:' in out\n    assert 'macros' in out\n    assert 'lib_dirs' in out\n    assert 'cblas_libs' in out\n\n    assert 'Python deps:' in out\n    assert 'pip' in out\n    assert 'setuptools' in out\n    assert 'sklearn' in out\n    assert 'numpy' in out\n    assert 'scipy' in out\n    assert 'Cython' in out\n    assert 'pandas' in out\n    assert 'matplotlib' in out\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n\n    assert 'python' in captured.out\n    assert 'executable' in captured.out\n    assert 'machine' in captured.out\n\n    assert 'pip' in captured.out\n    assert 'setuptools' in captured.out\n    assert 'sklearn' in captured.out\n    assert 'numpy' in captured.out\n    assert 'scipy' in captured.out\n    assert 'Cython' in captured.out\n    assert 'pandas' in captured.out\n    assert 'matplotlib' in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert \"System:\" in captured.out\n    assert \"BLAS:\" in captured.out\n    assert \"Python deps:\" in captured.out\n\n    sys_info = _get_sys_info()\n    deps_info = _get_deps_info()\n    blas_info = _get_blas_info()\n\n    for k in sys_info.keys():\n        assert k in captured.out\n\n    for k in deps_info.keys():\n        assert k in captured.out\n\n    for k in blas_info.keys():\n        assert k in captured.out\n"], "sample_414": ["    def test_admin_login(self):\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.assertIn(\"Logged In\", self.selenium.page_source)\n", "def test_ManyToManyRawIdWidget_render_disabled(self):\n    band = Band.objects.create(name=\"Linkin Park\")\n\n    m1 = Member.objects.create(name=\"Chester\")\n    m2 = Member.objects.create(name=\"Mike\")\n    band.members.add(m1, m2)\n    rel = Band._meta.get_field(\"members\").remote_field\n\n    w = widgets.ManyToManyRawIdWidget(rel, widget_admin_site)\n    w.attrs['disabled'] = 'disabled'\n    self.assertHTMLEqual(\n        w.render(\"test\", [m1.pk, m2.pk], attrs={}),\n        '<input type=\"text\" name=\"test\" value=\"%(m1pk)s,%(m2pk)s\" disabled>'\n        % {\"m1pk\": m1.pk, \"m2pk\": m2.pk},\n    )\n\n    self.assertHTMLEqual(\n        w.render(\"test\", [m1.pk]),\n        '<input type=\"text\" name=\"test\" value=\"%(m1pk)s\" disabled>'\n        % {\"m1pk\": m1.pk},\n    )\n", "def test_formfield_overrides_date_field(self):\n    \"\"\"\n    Overriding the widget for DateField doesn't overrides the default\n    form_class for that field (#26449).\n    \"\"\"\n\n    class EventAdmin(admin.ModelAdmin):\n        formfield_overrides = {\n            DateField: {\"widget\": widgets.AdminDateWidget}\n        }\n\n    ma = EventAdmin(Event, admin.site)\n    f1 = ma.formfield_for_dbfield(Event._meta.get_field(\"start_date\"), request=None)\n    self.assertIsInstance(f1.widget, widgets.AdminDateWidget)\n    self.assertIsInstance(f1, forms.DateField)\n", "    def setUp(self):\n        super().setUp()\n        Band.objects.create(id=42, name=\"Bogey Blues\", genre=\"Blues\")\n        Band.objects.create(id=98, name=\"Green Potatoes\", genre=\"Rock\")\n", "def test_fk_with_non_pk_target_field(self):\n    band = Band.objects.create(name=\"Linkin Park\")\n    album = band.album_set.create(name=\"Hybrid Theory\")\n    self.client.force_login(self.superuser)\n    response = self.client.get(reverse(\"admin:admin_widgets_album_change\", args=(album.id,)))\n    self.assertContains(response, '<input type=\"text\" name=\"band\" value=\"Linkin Park\" id=\"id_band\" readonly=\"readonly\">')\n"], "sample_321": ["def test_https_good_referer_matches_session_cookie_domain(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted from a\n    subdomain that's allowed by SESSION_COOKIE_DOMAIN.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'https://foo.example.com/'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_process_view_token_too_long_with_sessions(self):\n    \"\"\"\n    If the token is longer than expected, it is ignored and a new token is\n    created when using sessions.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    req.session[CSRF_SESSION_KEY] = 'x' * 100000\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_session = req.session.get(CSRF_SESSION_KEY, False)\n    self.assertEqual(len(csrf_session), CSRF_TOKEN_LENGTH)\n", "def test_https_reject_invalid_referer(self):\n    \"\"\"\n    A POST HTTPS request from an invalid referer should be rejected.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'https://evil.com/'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertContains(\n        response,\n        'Referer checking failed - https://evil.com/ does not match any trusted origins.',\n        status_code=403,\n    )\n", "def test_process_view_token_too_long_with_sessions(self):\n    \"\"\"\n    If the token is longer than expected, it is ignored and a new token is\n    created when using sessions.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    req.session[CSRF_SESSION_KEY] = 'x' * 100000\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_session_token = req.session.get(CSRF_SESSION_KEY, False)\n    self.assertEqual(len(csrf_session_token), CSRF_TOKEN_LENGTH)\n", "def test_https_reject_malformed_referer(self):\n    \"\"\"\n    A POST HTTPS request with a malformed referer should be rejected.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'http://http://example.com/'\n    mw = CsrfViewMiddleware(post_form_view)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertContains(\n        response,\n        'Referer checking failed - Referer is malformed.',\n        status_code=403,\n    )\n"], "sample_714": ["def test_log_loss_invalid_probabilities():\n    y_true = [\"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\"]\n    y_pred = np.array([[0.5, 0.5], [0.1, 0.9], [0.01, 1.01],\n                       [0.9, 0.1], [0.75, 0.25], [0.001, 0.999]])\n    assert_raises(ValueError, log_loss, y_true, y_pred)\n", "def test_log_loss_single_class():\n    # Test that log_loss function returns an error when trying to compute log_loss\n    # for a single class task.\n    rng = check_random_state(404)\n    y_pred = rng.rand(10)\n\n    # y_true contains only one class value\n    y_true = [0] * 10\n    assert_raise_message(ValueError, \"y_true contains only one label (0). Please \"\n                         \"provide the true labels explicitly through the labels \"\n                         \"argument.\", log_loss, y_true, y_pred)\n", "def test_hinge_loss_binary_pandas_input():\n    # Test hinge loss with binary targets and pandas series input\n    y_true = pd.Series([-1, 1, 1, -1])\n    pred_decision = pd.Series([-8.5, 0.5, 1.5, -0.3])\n    assert_equal(hinge_loss(y_true, pred_decision), 1.2 / 4)\n", "def test_log_loss_sample_weight():\n    # test sample_weight option\n    y_true = [0, 1, 1, 0]\n    y_pred = [[0.1, 0.9], [0.9, 0.1], [0.8, 0.2], [0.35, 0.65]]\n    sample_weight = [1, 2, 3, 4]\n    loss = log_loss(y_true, y_pred, sample_weight=sample_weight, normalize=False)\n    assert_almost_equal(loss, 7.3732789, decimal=6)\n", "def test_log_loss_single_class():\n    # case when y_true is a single class\n    y_true = [0, 0]\n    y_pred = [[0.9, 0.1], [0.95, 0.05]]\n    error_str = ('y_true contains only one label (0). Please provide the true labels explicitly through the labels argument.')\n    assert_raise_message(ValueError, error_str, log_loss, y_true, y_pred)\n"], "sample_622": ["def test_decode_cf_variable_with_invalid_units() -> None:\n    variable = Variable([\"time\"], [1, 2, 3], {\"units\": \"invalid_units\"})\n    with pytest.raises(ValueError, match=\"unable to decode time\"):\n        conventions.decode_cf_variable(\"time\", variable)\n", "def test_decode_cf_variable_with_time_units_in_encoding() -> None:\n    variable = Variable([\"time\"], [1, 2, 3], encoding={\"units\": \"days since 2000-01-01\"})\n    decoded = conventions.decode_cf_variable(\"time\", variable)\n    expected = Variable([\"time\"], pd.date_range(\"2000-01-01\", periods=3))\n    assert_identical(decoded, expected)\n", "def test_decode_cf_variable_with_invalid_bounds_attribute():\n    variable = Variable([\"time\"], pd.date_range(\"2000\", periods=2), {\"bounds\": \"invalid\"})\n    with pytest.warns(UserWarning, match=\"Attribute bounds malformed\"):\n        conventions.decode_cf_variable(\"time\", variable)\n", "def test_decode_cf_variable_datetime_like_objects():\n    # Test that datetime-like objects are passed through unmodified\n    datetime_like_objects = [\n        np.array(\"2000-01-01\", dtype=np.datetime64),\n        pd.Timestamp(\"2000-01-01\"),\n    ]\n\n    for obj in datetime_like_objects:\n        variable = Variable([\"time\"], obj)\n        decoded = conventions.decode_cf_variable(\"time\", variable)\n        assert_identical(decoded, variable)\n", "def test_decode_cf_variable_with_invalid_attribute(self) -> None:\n    v = Variable([\"t\"], [1, 2, 3], {\"units\": \"foobar\", \"invalid_attribute\": \"value\"})\n    with pytest.raises(AttributeError):\n        conventions.decode_cf_variable(\"test\", v)\n"], "sample_1051": ["def test_dotprint_styles():\n    custom_styles = [(Symbol, {'color': 'red'}),\n                     (Integer, {'color': 'green'}),\n                     (Expr, {'shape': 'box'})]\n    text = dotprint(x + 2, styles=custom_styles)\n    assert '\"Symbol(\\'x\\')_()\" [\"color\"=\"red\", \"label\"=\"x\", \"shape\"=\"box\"];' in text\n    assert '\"Integer(2)_(1,)\" [\"color\"=\"green\", \"label\"=\"2\", \"shape\"=\"box\"];' in text\n", "def test_atom_function():\n    a, b, c = symbols('a b c')\n    text = dotprint(a + b*c, atom=lambda x: x.is_symbol)\n    assert \"Add(Symbol('a'), Mul(Symbol('b'), Symbol('c')))\" in text\n    assert \"Symbol('a')\" in text\n    assert \"Symbol('b')\" in text\n    assert \"Symbol('c')\" in text\n    assert \"Mul(Symbol('b'), Symbol('c'))\" not in text\n", "def test_dotprint_with_styles():\n    styles = [(Symbol, {'color': 'red', 'shape': 'box'}),\n              (Integer, {'color': 'green', 'shape': 'diamond'})]\n    text = dotprint(x + 2, styles=styles)\n    assert 'color=\"red\"' in text\n    assert 'color=\"green\"' in text\n    assert 'shape=\"box\"' in text\n    assert 'shape=\"diamond\"' in text\n", "def test_dotprint_styles():\n    styles = [(Symbol, {'color': 'red'}),\n              (Integer, {'color': 'green'})]\n    text = dotprint(x + 2, styles=styles)\n    assert '\"Symbol(\\'x\\')_()\" [\"color\"=\"red\", \"label\"=\"x\", \"shape\"=\"ellipse\"];' in text\n    assert '\"Integer(2)_(1,)\" [\"color\"=\"green\", \"label\"=\"2\", \"shape\"=\"ellipse\"];' in text\n", "def test_dotprint_labelfunc():\n        return str(expr) + '_custom'\n    text = dotprint(x + 2, labelfunc=custom_labelfunc)\n    assert \"x_custom\" in text\n    assert \"2_custom\" in text\n"], "sample_495": ["def test_paginate_unordered_list(self):\n    \"\"\"\n    Paginator should raise UnorderedObjectListWarning when object_list is unordered.\n    \"\"\"\n    unordered_list = ['b', 'a', 'c']\n    with self.assertWarns(UnorderedObjectListWarning):\n        Paginator(unordered_list, 2)\n", "def test_paginator_iterator_length(self):\n    \"\"\"\n    Paginator iterator should have the same length as the number of pages.\n    \"\"\"\n    paginator = Paginator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n    page_iterator = iter(paginator)\n    self.assertEqual(len(list(page_iterator)), paginator.num_pages)\n", "    def test_paginator_with_orphans(self):\n        \"\"\"\n        Tests the paginator attributes with orphans using varying inputs.\n        \"\"\"\n        ten = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        eleven = ten + [11]\n        tests = (\n            # Each item is two tuples:\n            #     First tuple is Paginator parameters - object_list, per_page,\n            #         orphans, and allow_empty_first_page.\n            #     Second tuple is resulting Paginator attributes - count,\n            #         num_pages, and page_range.\n            # Eleven items, varying orphans, no empty first page.\n            ((eleven, 5, 2, False), (11, 3, [1, 2, 3])),\n            ((eleven, 5, 3, False), (11, 2, [1, 2])),\n            ((eleven, 5, 5, False), (11, 2, [1, 2])),\n            ((eleven, 5, 6, False), (11, 1, [1])),\n        )\n        for params, output in tests:\n            self.check_paginator(params, output)\n", "def test_paginator_with_custom_object_list(self):\n    \"\"\"\n    Paginator should work with custom object lists that don't implement count()\n    or have a length.\n    \"\"\"\n    class CustomObjectList:\n            self.data = data\n\n            return self.data[key]\n\n    custom_list = CustomObjectList([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    paginator = Paginator(custom_list, 5)\n\n    # Check that the count is correct\n    self.assertEqual(10, paginator.count)\n\n    # Check the number of pages\n    self.assertEqual(2, paginator.num_pages)\n\n    # Check the items on each page\n    self.assertEqual([1, 2, 3, 4, 5], list(paginator.page(1)))\n    self.assertEqual([6, 7, 8, 9, 10], list(paginator.page(2)))\n", "def test_paginator_iteration_with_empty_list(self):\n    paginator = Paginator([], 2)\n    page_iterator = iter(paginator)\n    with self.assertRaises(StopIteration):\n        next(page_iterator)\n"], "sample_589": ["def test_interpolate_na_max_gap_integer(da_time):\n    da_time[\"t\"] = np.arange(11)\n    expected = da_time.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da_time.interpolate_na(\"t\", max_gap=1, use_coordinate=False)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_fill_value_sequence():\n    da = xr.DataArray(\n        [np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10],\n        dims=[\"t\"],\n    )\n    expected = da.copy(data=[0, 1, 2, 3, 4, 5, 9, 9, 9, 9, 10])\n    actual = da.interpolate_na(\"t\", fill_value=(0, 9))\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_numeric(da_time, max_gap):\n    da_time = da_time.reset_coords(drop=True)\n    expected = da_time.copy(data=[np.nan, 1, 2, 3, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da_time.interpolate_na(\"t\", max_gap=max_gap, use_coordinate=False)\n    assert_equal(actual, expected)\n", "def test_interpolate_fill_value():\n    da = xr.DataArray(np.array([0, np.nan, 2, 3, np.nan, 5], dtype=np.float64), dims=\"x\")\n    expected = xr.DataArray(np.array([0, -999, 2, 3, 4, 5], dtype=np.float64), dims=\"x\")\n    actual = da.interpolate_na(dim=\"x\", fill_value=-999)\n    assert_equal(actual, expected)\n", "def test_interpolate_fill_value():\n    da = xr.DataArray(np.array([4, np.nan, 6], dtype=np.float64), dims=\"x\")\n    expected = xr.DataArray(np.array([4, 5, 6], dtype=np.float64), dims=\"x\")\n    actual = da.interpolate_na(dim=\"x\", method=\"linear\", fill_value=(4, 6))\n    assert_equal(actual, expected)\n"], "sample_353": ["def test_fields_with_fk_non_existent_fk(self):\n    new_io = StringIO()\n    non_existent_pk = 999\n    msg = 'email instance with id %s does not exist.' % non_existent_pk\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=non_existent_pk,\n            email='mymail@gmail.com',\n            group=1,  # Assuming group 1 exists\n            stdout=new_io,\n        )\n", "def test_fields_with_fk_non_interactive(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username=email.pk,\n        email=email.email,\n        group=group.pk,\n        stdout=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertEqual(command_output, 'Superuser created successfully.')\n    u = CustomUserWithFK._default_manager.get(email=email)\n    self.assertEqual(u.username, email)\n    self.assertEqual(u.group, group)\n", "def test_fields_with_fk_invalid_input(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    invalid_group_id = group.pk + 1\n\n    @mock_inputs({\n        'password': 'nopasswd',\n        'Username (Email.id): ': email.pk,\n        'Email (Email.email): ': email.email,\n        'Group (Group.id): ': invalid_group_id,\n    })\n        with self.assertRaisesMessage(CommandError, f'group instance with id {invalid_group_id} does not exist.'):\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdout=new_io,\n                stdin=MockTTY(),\n            )\n\n    test(self)\n", "    def test_user_proxy_model(self):\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username='joe',\n            email='joe@somewhere.org',\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = UserProxy.objects.get(username=\"joe\")\n        self.assertEqual(u.email, 'joe@somewhere.org')\n\n        # created password should be unusable\n        self.assertFalse(u.has_usable_password())\n", "    def test_user_proxy_model(self):\n        new_io = StringIO()\n        call_command(\n            \"createsuperuser\",\n            interactive=False,\n            username=\"joe\",\n            email=\"joe@somewhere.org\",\n            stdout=new_io\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = UserProxy.objects.get(username=\"joe\")\n        self.assertEqual(u.email, 'joe@somewhere.org')\n        self.assertFalse(u.has_usable_password())\n"], "sample_95": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=60, private=True, no_store=False)\n        self.assertEqual(response['Cache-Control'], 'max-age=60, private')\n", "    def test_vary_on_headers(self):\n            return \"response\"\n        my_vary_view = vary_on_headers('Accept-Encoding')(my_view)\n        request = HttpRequest()\n        request.META['HTTP_ACCEPT_ENCODING'] = 'gzip'\n        response = my_vary_view(request)\n        self.assertIn('Accept-Encoding', response['Vary'])\n", "def test_cache_control_decorator(self):\n    @cache_control(max_age=60, no_cache=True, no_store=False, must_revalidate=True, private=False, public=True)\n        return HttpResponse()\n    r = a_view(HttpRequest())\n    self.assertEqual(\n        r['Cache-Control'],\n        'max-age=60, no-cache, public, must-revalidate',\n    )\n", "def test_get_cache_key(self):\n    \"\"\"\n    Test the get_cache_key function.\n    \"\"\"\n    request = HttpRequest()\n    request.META = {'HTTP_ACCEPT_LANGUAGE': 'en-US,en;q=0.8'}\n    request.build_absolute_uri = lambda: '/test/'\n    key_prefix = 'test'\n    method = 'GET'\n    cache = caches['default']\n    cache.set(_generate_cache_header_key(key_prefix, request), ['HTTP_ACCEPT_LANGUAGE'])\n    cache_key = get_cache_key(request, key_prefix, method, cache)\n    expected_cache_key = _generate_cache_key(request, method, ['HTTP_ACCEPT_LANGUAGE'], key_prefix)\n    self.assertEqual(cache_key, expected_cache_key)\n", "def test_vary_headers(self):\n    response = HttpResponse()\n    vary_on_headers('Accept-language')(response)\n    self.assertEqual(response['Vary'], 'Accept-language')\n    vary_on_cookie(response)\n    self.assertEqual(response['Vary'], 'Accept-language, Cookie')\n"], "sample_113": ["    def test_replace_named_groups(self):\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), '^<a>/b/(\\\\w+)$')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), '^<a>/b/<c>/$')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)'), '^<a>/b/(\\\\w+)')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), '^<a>/b/<c>')\n", "def test_replace_named_groups(self):\n    self.assertEqual(\n        utils.replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'),\n        r'^<a>/b/(\\w+)$'\n    )\n    self.assertEqual(\n        utils.replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'),\n        r'^<a>/b/<c>/$'\n    )\n    self.assertEqual(\n        utils.replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)'),\n        r'^<a>/b/(\\w+)'\n    )\n    self.assertEqual(\n        utils.replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'),\n        r'^<a>/b/<c>'\n    )\n", "    def test_replace_named_groups(self):\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), r'^<a>/b/<c>/$')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)'), r'^<a>/b/(\\w+)')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), r'^<a>/b/<c>')\n", "    def test_replace_named_groups(self):\n        self.assertEqual(utils.replace_named_groups('^(?P<a>\\\\w+)/b/(\\\\w+)$'), '^<a>/b/(\\\\w+)$')\n        self.assertEqual(utils.replace_named_groups('^(?P<a>\\\\w+)/b/(?P<c>\\\\w+)/$'), '^<a>/b/<c>/$')\n        self.assertEqual(utils.replace_named_groups('^(?P<a>\\\\w+)/b/(\\\\w+)'), '^<a>/b/(\\\\w+)')\n        self.assertEqual(utils.replace_named_groups('^(?P<a>\\\\w+)/b/(?P<c>\\\\w+)'), '^<a>/b/<c>')\n", "    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_status_count'), 'Integer')\n        self.assertEqual(get_return_data_type('get_groups_list'), 'List')\n        self.assertEqual(get_return_data_type('non_existing_method'), 'Unknown')\n"], "sample_944": ["def test_restify_type_hints_union_with_typevars():\n    KT = TypeVar('KT')\n    VT = TypeVar('VT')\n\n    assert restify(Dict[KT, VT]) == \":class:`Dict`\\\\ [:obj:`tests.test_util_typing.KT`, :obj:`tests.test_util_typing.VT`]\"\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_restify_type_hints_NewType():\n    assert restify(MyInt) == \":class:`MyInt`\"\n", "def test_stringify_type_union_forward_ref():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"int\") | None) == \"Optional[int]\"  # type: ignore\n    assert stringify(ForwardRef(\"int\") | str) == \"int | str\"  # type: ignore\n    assert stringify(ForwardRef(\"int\") | str | None) == \"Optional[int | str]\"  # type: ignore\n", "def test_stringify_type_hints_custom_generic_class():\n    T = TypeVar('T')\n    MyGeneric = List[T]\n    assert stringify(MyGeneric[int]) == \"List[int]\"\n    assert stringify(MyGeneric[str]) == \"List[str]\"\n"], "sample_37": ["def test_to_fits_2():\n    \"\"\"\n    Test to_fits() with SIP distortion.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    wfits = w.to_fits()\n    assert isinstance(wfits, fits.HDUList)\n    assert isinstance(wfits[0], fits.PrimaryHDU)\n    assert isinstance(wfits[1], fits.ImageHDU)\n", "def test_all_world2pix_with_sip_distortion():\n    \"\"\"Test all_world2pix with SIP distortion\"\"\"\n\n    # Open test FITS file with SIP distortion:\n    fname = get_pkg_data_filename('data/sip.fits')\n    h = fits.open(fname)\n    w = wcs.WCS(h[0].header, h)\n    h.close()\n    del h\n\n    crpix = w.wcs.crpix\n    ncoord = crpix.shape[0]\n\n    # Assume that CRPIX is at the center of the image and that the image has\n    # a power-of-2 number of pixels along each axis. Only use the central\n    # 1/64 for this testing purpose:\n    naxesi_l = list((7. / 16 * crpix).astype(int))\n    naxesi_u = list((9. / 16 * crpix).astype(int))\n\n    # Generate integer indices of pixels (image grid):\n    img_pix = np.dstack([i.flatten() for i in\n                         np.meshgrid(*map(range, naxesi_l, naxesi_u))])[0]\n\n    # Generage random data (in image coordinates):\n    with NumpyRNGContext(123456789):\n        rnd_pix = np.random.rand(random_npts, ncoord)\n\n    # Scale random data to cover the central part of the image\n    mwidth = 2 * (crpix * 1. / 8)\n    rnd_pix = crpix - 0.5 * mwidth + (mwidth - 1) * rnd_pix\n\n    # Reference pixel coordinates in image coordinate system (CS):\n    test_pix = np.append(img_pix, rnd_pix, axis=0)\n    # Reference pixel coordinates in sky CS using forward transformation:\n    all_world = w.all_pix2world(test_pix, origin)\n\n    try:\n        runtime_begin = datetime.now()\n        # Apply the inverse iterative process to", "def test_dropaxis():\n    w = wcs.WCS(naxis=3)\n    w.wcs.crpix = [1, 2, 3]\n    w.wcs.crval = [10, 20, 30]\n    w.wcs.cdelt = [0.1, 0.2, 0.3]\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'WAVE']\n\n    w_dropped = w.dropaxis(1)\n    assert w_dropped.naxis == 2\n    assert w_dropped.wcs.crpix == [1, 3]\n    assert w_dropped.wcs.crval == [10, 30]\n    assert w_dropped.wcs.cdelt == [0.1, 0.3]\n    assert w_dropped.wcs.ctype == ['RA---TAN', 'WAVE']\n", "def test_read_fits():\n    \"\"\"\n    Test reading from a fits file.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    assert isinstance(w, wcs.WCS)\n    assert w.wcs.crpix[0] == 1024\n    assert w.wcs.crpix[1] == 1024\n    assert w.sip is not None\n    assert w.sip.crpix[0] == 1024\n    assert w.sip.crpix[1] == 1024\n", "def test_sip_foc2pix_error():\n    \"\"\"\n    Test that Sip.foc2pix raises ValueError when invalid coordinate transformation parameters are encountered.\n    \"\"\"\n    a = np.array([[1, 2], [3, 4]])\n    b = np.array([[5, 6], [7, 8]])\n    crpix = [10, 20]\n\n    # Test invalid 'a' array shape\n    with pytest.raises(ValueError):\n        wcs.Sip(a=np.array([1, 2, 3]), b=b, crpix=crpix)\n\n    # Test invalid 'b' array shape\n    with pytest.raises(ValueError):\n        wcs.Sip(a=a, b=np.array([1, 2]), crpix=crpix)\n\n    # Test invalid 'crpix' length\n    with pytest.raises(ValueError):\n        wcs.Sip(a=a, b=b, crpix=[10])\n\n    # Test invalid 'ap' array shape\n    with pytest.raises(ValueError):\n        wcs.Sip(a=a, b=b, ap=np.array([1, 2, 3]), crpix=crpix)\n\n    # Test invalid 'bp' array shape\n    with pytest.raises(ValueError):\n        wcs.Sip(a=a, b=b, ap=np.array([1, 2]), bp=np.array([1, 2, 3]), crpix=crpix)\n"], "sample_878": ["def test_verbose_feature_names_out_with_slices(transformers, remainder, expected_names):\n    \"\"\"Check feature_names_out for verbose_feature_names_out=True with slices\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        transformers,\n        remainder=remainder,\n    )\n    ct.fit(df)\n\n    names = ct.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n    assert_array_equal(names, expected_names)\n", "def test_verbose_feature_names_out_false_remainder_drop(\n    transformers, remainder, expected_names", "def test_column_transform_set_output_remainder(remainder):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=False, dtype=\"int16\"),\n                [\"pet\"],\n            ),\n        ],\n        remainder=remainder,\n        verbose_feature_names_out=False,\n    ).set_output(transform=\"pandas\")\n\n    X_trans_df = ct.fit_transform(df)\n    expected_dtypes = {\n        \"pet_cat\": \"int16\",\n        \"pet_dog\": \"int16\",\n        \"pet_snake\": \"int16\",\n    }\n    if remainder == \"passthrough\":\n        expected_dtypes.update({\n            \"height\": \"int64\",\n            \"age\": \"float64\",\n        })\n    for col, dtype in X_trans_df.dtypes.items():\n        assert dtype == expected_dtypes[col]\n", "def test_column_transformer_get_feature_names_out_with_drop():\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\n        \"col1\": [\"a\", \"b\", \"c\"],\n        \"col2\": [\"z\", \"z\", \"z\"],\n        \"col3\": [1, 2, 3],\n        \"col4\": [3, 4, 5],\n    })\n    ct = ColumnTransformer([(\"ohe\", OneHotEncoder(), [\"col1\", \"col2\"]), (\"drop\", \"drop\", [\"col3\"])])\n    ct.fit(df)\n    assert_array_equal(ct.get_feature_names_out(), [\"ohe__col1_a\", \"ohe__col1_b\", \"ohe__col1_c\", \"ohe__col2_z\"])\n", "def test_column_transformer_sparse_threshold_with_single_transformer():\n    X_array = np.array([[\"a\", \"b\"], [\"A\", \"B\"]], dtype=object).T\n\n    # apply threshold even if all sparse\n    col_trans = ColumnTransformer(\n        [(\"trans1\", OneHotEncoder(), [0])],\n        sparse_threshold=0.2,\n    )\n    res = col_trans.fit_transform(X_array)\n    assert not sparse.issparse(res)\n    assert not col_trans.sparse_output_\n\n    # mixed -> sparsity of (4 + 2) / 8 = 0.75\n    for thres in [0.75001, 1]:\n        col_trans = ColumnTransformer(\n            [(\"trans1\", OneHotEncoder(sparse_output=True), [0])],\n            sparse_threshold=thres,\n        )\n        res = col_trans.fit_transform(X_array)\n        assert sparse.issparse(res)\n        assert col_trans.sparse_output_\n\n    for thres in [0.75, 0]:\n        col_trans = ColumnTransformer(\n            [(\"trans1\", OneHotEncoder(sparse_output=True), [0])],\n            sparse_threshold=thres,\n        )\n        res = col_trans.fit_transform(X_array)\n        assert not sparse.issparse(res)\n        assert not col_trans.sparse_output_\n\n    # if nothing is sparse -> no sparse\n    for thres in [0.33, 0, 1]:\n        col_trans = ColumnTransformer(\n            [(\"trans1\", OneHotEncoder(sparse_output=False), [0])],\n            sparse_threshold=thres,\n        )\n        res = col_trans.fit_transform(X_array)\n        assert not sparse.issparse(res)\n        assert not col_trans.sparse_output_\n"], "sample_143": ["def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseExample'), 'camel case example')\n    self.assertEqual(text.camel_case_to_spaces('AnotherExample'), 'another example')\n    self.assertEqual(text.camel_case_to_spaces('NoCamelCase'), 'no camel case')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('LazyCamelCase')), 'lazy camel case')\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('CamelCaseExample', 'camel case example'),\n        ('AnotherCamelCase', 'another camel case'),\n        ('NoCamelCaseHere', 'no camel case here'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('camelCaseString'), 'camel case string')\n    self.assertEqual(text.camel_case_to_spaces('AnotherCamelCaseString'), 'another camel case string')\n    self.assertEqual(text.camel_case_to_spaces('NoCamelCaseString'), 'no camel case string')\n    self.assertEqual(text.camel_case_to_spaces('lazystr(CamelCaseString)'), 'lazystr(camel case string)')\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('CamelCaseString', 'camel case string'),\n        ('IPAddress', 'ip address'),\n        ('HTTPResponse', 'http response'),\n        ('SomeRandomString', 'some random string'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('ThisIsATest', 'this is a test'),\n        ('CamelCaseExample', 'camel case example'),\n        ('AnotherTest', 'another test'),\n        ('OneMoreTest', 'one more test'),\n        ('NoChangeHere', 'no change here'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n"], "sample_502": ["def test_subplot_mosaic():\n    fig, ax_dict = plt.subplot_mosaic({'a': [0, 0], 'b': [0, 1], 'c': [1, 0], 'd': [1, 1]})\n    assert isinstance(fig, mpl.figure.Figure)\n    assert isinstance(ax_dict, dict)\n    assert set(ax_dict.keys()) == {'a', 'b', 'c', 'd'}\n    assert all(isinstance(ax, mpl.axes.Axes) for ax in ax_dict.values())\n", "def test_polar_without_args():\n    # Calling plt.polar() without arguments should raise a TypeError\n    with pytest.raises(TypeError):\n        plt.polar()\n", "def test_subplot_mosaic_reuse():\n    ax = plt.subplot_mosaic({'A': [0, 0]})['A']\n    ax_next = plt.subplot_mosaic({'A': [0, 0]})['A']\n    assert ax_next is ax\n    ax_next = plt.subplot_mosaic({'A': [0, 1]})['A']\n    assert ax_next is not ax\n", "def test_subplot_kwargs_match():\n    # plt.subplot() searches for axes with the same subplot spec, and if one\n    # exists, and the kwargs match returns it, create a new one if they do not\n    fig = plt.figure()\n    ax = plt.subplot(1, 2, 1, label='ax1')\n    ax1 = plt.subplot(1, 2, 1, label='ax1')\n    ax2 = plt.subplot(1, 2, 2, label='ax2')\n    ax3 = plt.subplot(1, 2, 1, label='ax3')\n    assert ax is not None\n    assert ax1 is ax\n    assert ax2 is not ax\n    assert ax3 is not ax\n    assert ax3 is not ax1\n    assert ax3 in fig.axes\n    assert ax3.get_label() == 'ax3'\n", "def test_subplot_invalid_argument():\n    with pytest.raises(TypeError):\n        plt.subplot(1, 2, False)\n"], "sample_158": ["def test_foreign_object_to_non_unique_superset_field(self):\n    class Person(models.Model):\n        country_id = models.IntegerField()\n        city_id = models.IntegerField()\n        state_id = models.IntegerField()\n\n        class Meta:\n            unique_together = (('country_id', 'city_id'),)\n\n    class MMembership(models.Model):\n        person_country_id = models.IntegerField()\n        person_city_id = models.IntegerField()\n        person = models.ForeignObject(\n            Person,\n            on_delete=models.CASCADE,\n            from_fields=['person_country_id', 'person_city_id'],\n            to_fields=['country_id', 'city_id'],\n        )\n\n    field = MMembership._meta.get_field('person')\n    self.assertEqual(field.check(), [\n        Error(\n            \"No subset of the fields 'country_id', 'city_id' on model \"\n            \"'Person' is unique.\",\n            hint=(\n                'Mark a single field as unique=True or add a set of '\n                'fields to a unique constraint (via unique_together or a '\n                'UniqueConstraint (without condition) in the model '\n                'Meta.constraints).'\n            ),\n            obj=field,\n            id='fields.E310',\n        ),\n    ])\n", "def test_foreign_object_to_unique_fields_with_meta_constraint(self):\n    class Parent(models.Model):\n        a = models.IntegerField(unique=True)\n        b = models.IntegerField(unique=True)\n\n        class Meta:\n            unique_together = (('a', 'b'),)\n\n    class Child(models.Model):\n        a = models.IntegerField()\n        b = models.IntegerField()\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('a', 'b'),\n            to_fields=('a', 'b'),\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(), [])\n", "def test_foreign_key_to_different_db(self):\n    class DifferentDbModel(models.Model):\n        class Meta:\n            app_label = 'different_db'\n            db_table = 'different_db_table'\n\n    class Model(models.Model):\n        foreign_key = models.ForeignKey(DifferentDbModel, models.CASCADE)\n\n    field = Model._meta.get_field('foreign_key')\n    self.assertEqual(field.check(), [\n        Error(\n            \"Field defines a relation with model 'different_db.DifferentDbModel', \"\n            \"which is either not installed, or is abstract.\",\n            id='fields.E300',\n        ),\n    ])\n", "def test_foreign_object_without_unique_fields(self):\n    class Person(models.Model):\n        pass\n\n    class MMembership(models.Model):\n        person_id = models.IntegerField()\n        person = models.ForeignObject(\n            Person,\n            on_delete=models.SET_NULL,\n            from_fields=['person_id'],\n            to_fields=[],\n        )\n\n    field = MMembership._meta.get_field('person')\n    self.assertEqual(field.check(), [])\n", "def test_foreign_key_to_parent_link_without_explicit_to_field(self):\n    class Parent(models.Model):\n        pass\n\n    class Child(Parent):\n        parent_fk = models.ForeignKey(Parent, models.CASCADE)\n\n    field = Child._meta.get_field('parent_fk')\n    self.assertEqual(field.check(from_model=Child), [\n        Error(\n            \"ForeignKey to parent model 'Parent' with parent_link=True must have an explicit to_field.\",\n            obj=field,\n            id='fields.E314',\n        ),\n    ])\n"], "sample_1111": ["def test_non_numeric_input():\n    x = Symbol('x')\n    lines = [\n        '     54 |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '   27.5 |-----------------------------------------------------',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '      1 |_____________________________________________________',\n        '         0                          1                        2'\n    ]\n    assert lines == list(textplot_str(sqrt(x), 0, 2))\n", "def test_sqrt_function():\n    x = Symbol('x')\n    lines = [\n        '     2.0 |                                                     ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '    1.5 |.......................................................',\n        '        |                      .                                ',\n        '        |                     .                                 ',\n        '        |                    .                                  ',\n        '        |                   .                                   ',\n        '        |                  .                                    ',\n        '        |                 .                                     ',\n        '        |                .                                      ',\n        '        |               .                                       ',\n        '        |              .                                        ',\n        '      0 |_____________._........................................',\n        '         0                          0.5                        1'\n    ]\n    assert lines == list(textplot_str(sqrt(x), 0, 1))\n", "def test_constant_function():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |_______________________________________________________',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(3, -1, 1))\n", "def test_complex_values():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '      0 |-----------------------------------------------------',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '     -1 |_____________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(sqrt(1-x**2), -1, 1))\n", "def test_discontinuity():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                                     ..',\n        '        |                                                  ...  ',\n        '        |                                                ..     ',\n        '        |                                             ...       ',\n        '        |                                          ...          ',\n        '        |                                        ..             ',\n        '        |                                     ...               ',\n        '        |                                  ...                  ',\n        '        |                                ..                     ',\n        '        |                             ...                       ',\n        '      0 |--------------------------.......----------------------',\n        '        |                       ...                             ',\n        '        |                     ..                                ',\n        '        |                  ...                                  ',\n        '        |               ...                                     ',\n        '        |             ..                                        ',\n        '        |          ...                                          ',\n        '        |       ...                                             ',\n        '        |     ..                                                ',\n        '        |  ...                                                  ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(sqrt(abs(x)), -1, 1))\n"], "sample_40": ["def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    with u.set_enabled_equivalencies(u.with_H0(H0)):\n        assert_quantity_allclose(1*u.littleh, H0.to(u.km/u.s/u.Mpc)/100)\n        assert_quantity_allclose(1/u.littleh, H0.to(u.km/u.s/u.Mpc)*100)\n", "def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    lh_value = 0.5 * u.littleh\n    mh_value = 0.5 * u.Msun\n\n    lh_value_converted = lh_value.to(u.Msun, u.with_H0(H0))\n    assert_allclose(lh_value_converted.value, mh_value.value)\n\n    mh_value_converted = mh_value.to(u.littleh, u.with_H0(H0))\n    assert_allclose(mh_value_converted.value, lh_value.value)\n", "def test_with_H0():\n    q = 100*u.Mpc\n    h0_val = 70*(u.km/u.s)/u.Mpc\n    q_physical = q.to(u.km, u.with_H0(h0_val))\n    assert_quantity_allclose(q_physical, 7000*u.km)\n\n    q_physical = q.to(u.km, u.with_H0())\n    assert_quantity_allclose(q_physical, 7000*u.km)\n", "def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    q = 1 * u.Mpc\n    expected_value = q.to(u.Mpc, u.with_H0(H0)).value\n    assert_allclose(expected_value, q.value)\n\n    q_littleh = q / H0.value * u.littleh\n    assert_allclose(q_littleh.to(u.Mpc, u.with_H0(H0)).value, q.value)\n\n    # Test default H0\n    expected_value_default = q.to(u.Mpc, u.with_H0()).value\n    assert_allclose(expected_value_default, q.value)\n\n    cosmo = cosmology.FlatLambdaCDM(H0=H0)\n    with cosmology.set_current(cosmo):\n        expected_value_cosmo = q.to(u.Mpc, u.with_H0()).value\n        assert_allclose(expected_value_cosmo, q.value)\n", "def test_with_H0():\n    q = 100 * u.Mpc / u.s\n    H0 = 70 * u.km / u.s / u.Mpc\n\n    q_h100 = q.to(u.Mpc / u.s, u.with_H0(H0))\n    assert_allclose(q_h100.value, q.value / (H0.to(u.Mpc / u.s) / 100))\n\n    q_h100_default = q.to(u.Mpc / u.s, u.with_H0())\n    assert_allclose(q_h100_default.value, q.value / (cosmology.default_cosmology.get().H0.value / 100))\n"], "sample_580": ["def test_categorical_order():\n\n    s = pd.Series([1, 2, 3, 1, 2, 3, np.nan])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\", np.nan])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([1, 2, 3, 1, 2, 3, np.nan], dtype=\"category\")\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\", np.nan], dtype=\"category\")\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([1, 2, 3, 1, 2, 3, np.nan])\n    assert categorical_order(s, order=[\"3\", \"2\", \"1\"]) == [\"3\", \"2\", \"1\"]\n", "def test_categorical_order():\n\n    s = pd.Series([\"a\", \"b\", \"c\", np.nan])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([\"a\", \"b\", \"c\"])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n    assert categorical_order(s, order=[\"c\", \"b\", \"a\"]) == [\"c\", \"b\", \"a\"]\n\n    s = pd.Series([3, 1, 2, np.nan])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([3, 1, 2, np.nan], dtype=\"category\")\n    assert categorical_order(s) == [3, 1, 2]\n", "def test_variable_type_extension_dtypes():\n\n    s = pd.Series([True, False, pd.NA], dtype=\"boolean\")\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n    assert variable_type(s, boolean_type=\"numeric\", strict_boolean=True) == \"numeric\"\n\n    s = pd.Series([pd.Timestamp(1), pd.NaT])\n    assert variable_type(s) == \"datetime\"\n", "def test_categorical_order():\n\n    s = pd.Series([1, 2, 3, 2, 1])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series(['a', 'b', 'a', 'c', 'b'])\n    assert categorical_order(s) == ['a', 'b', 'c']\n\n    s = pd.Series(['a', 'b', 'a', 'c', 'b'], dtype='category')\n    assert categorical_order(s) == ['a', 'b', 'c']\n\n    s = pd.Series(['a', 'b', 'a', 'c', 'b'])\n    assert categorical_order(s, order=['c', 'b', 'a']) == ['c', 'b', 'a']\n", "def test_categorical_order():\n\n    s = pd.Series([\"b\", \"c\", \"a\", \"b\", \"c\", \"a\"])\n    assert categorical_order(s) == [\"b\", \"c\", \"a\"]\n\n    s_cat = s.astype(\"category\")\n    assert categorical_order(s_cat) == [\"b\", \"c\", \"a\"]\n\n    s_num = pd.Series([3, 1, 2, 1, 2, 3])\n    assert categorical_order(s_num) == [1, 2, 3]\n\n    s_order = pd.Series([\"c\", \"b\", \"a\"])\n    assert categorical_order(s, order=[\"c\", \"b\", \"a\"]) == [\"c\", \"b\", \"a\"]\n\n    s_nan = pd.Series([np.nan, \"a\", \"b\", \"a\"])\n    assert categorical_order(s_nan) == [\"a\", \"b\"]\n"], "sample_639": ["def test_base_checker_doc_different() -> None:\n    different = DifferentBasicChecker()\n    expected_beginning = \"\"\"\\", "def test_message_definition() -> None:\n    basic = OtherBasicChecker()\n    msg_def = basic.get_message_definition(\"W0001\")\n    assert msg_def.msgid == \"W0001\"\n    assert msg_def.msg == \"Basic checker has an example.\"\n    assert msg_def.symbol == \"basic-checker-example\"\n    assert msg_def.descr == \"Used nowhere and serves no purpose.\"\n\n    different = DifferentBasicChecker()\n    msg_def = different.get_message_definition(\"W0002\")\n    assert msg_def.msgid == \"W0002\"\n    assert msg_def.msg == \"Blah blah example.\"\n    assert msg_def.symbol == \"blah-blah-example\"\n    assert msg_def.descr == \"I only exist to be different to OtherBasicChecker :(\"\n", "def test_base_checker_consistency() -> None:\n    consistent_checker = OtherBasicChecker()\n    consistent_checker.msgs[\"W0003\"] = (\n        \"Another example.\",\n        \"another-example\",\n        \"Another message description.\",\n    )\n    consistent_checker.check_consistency()  # Should not raise an exception\n\n    inconsistent_checker = DifferentBasicChecker()\n    inconsistent_checker.msgs[\"W0101\"] = (\n        \"Inconsistent example.\",\n        \"inconsistent-example\",\n        \"This msgid is inconsistent with the other ones.\",\n    )\n    with pytest.raises(InvalidMessageError):\n        inconsistent_checker.check_consistency()\n", "def test_base_checker_sorting() -> None:\n    basic = OtherBasicChecker()\n    different = DifferentBasicChecker()\n    less_basic = LessBasicChecker()\n\n    checkers = [basic, different, less_basic]\n    checkers.sort()\n\n    assert checkers[0] == basic\n    assert checkers[1] == less_basic\n    assert checkers[2] == different\n", "def test_check_consistency() -> None:\n    basic = OtherBasicChecker()\n    basic.check_consistency()  # No error since there's only one message\n\n    different = DifferentBasicChecker()\n    try:\n        different.check_consistency()\n        assert False, \"Expected InvalidMessageError\"\n    except InvalidMessageError as error:\n        assert str(error) == \"Inconsistent checker part in message id 'W0002' (expected 'x00xx' because we already had ['W0002']).\"\n"], "sample_704": ["def test_get_fslocation_from_item(pytester: Pytester) -> None:\n    \"\"\"Test that get_fslocation_from_item returns the correct location.\"\"\"\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    path, lineno = nodes.get_fslocation_from_item(item)\n    assert path == item.path\n    assert lineno == 2  # line number of the test function definition\n", "def test_collector_repr_failure(pytester: Pytester) -> None:\n    \"\"\"Test the repr_failure method of Collector class.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture\n            raise ValueError(\"Expected error\")\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(errors=1)\n    assert \"Expected error\" in result.stdout.str()\n", "def test_node_reportinfo_returns_correct_type():\n    node = nodes.Node(\"test_node\", session=pytest.Session())\n    reportinfo = node.reportinfo()\n    assert isinstance(reportinfo[0], (str, Path))\n    assert isinstance(reportinfo[1], (int, type(None)))\n    assert isinstance(reportinfo[2], str)\n", "def test_collector_repr_failure(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class CustomCollectError(pytest.CollectError):\n            pass\n\n        class TestCollector(pytest.Collector):\n                raise CustomCollectError(\"Custom collect error message\")\n\n            assert True\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines([\"*Custom collect error message\"])\n", "def test_node_addfinalizer(pytester: Pytester) -> None:\n    \"\"\"Test the addfinalizer method of Node class.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n                pass\n            request.node.addfinalizer(fin)\n            assert request.node.session._setupstate.finalizers[-1][0] == fin\n        \"\"\"\n    )\n    pytester.runpytest().assert_outcomes(passed=1)\n"], "sample_752": ["def test_iforest_threshold():\n    \"\"\"Test threshold property and deprecation warning\"\"\"\n    X_train = np.array([[0, 1], [1, 2], [2, 1]])\n    clf = IsolationForest(contamination='auto').fit(X_train)\n    assert_warns_message(DeprecationWarning,\n                         \"threshold_ attribute is deprecated\",\n                         getattr, clf, 'threshold_')\n    clf = IsolationForest(contamination=0.2).fit(X_train)\n    assert_warns_message(DeprecationWarning,\n                         \"threshold_ attribute is deprecated\",\n                         getattr, clf, 'threshold_')\n", "def test_iforest_contamination_parameter():\n    # Test the contamination parameter of IsolationForest\n    X = iris.data\n    clf = IsolationForest(contamination=0.2).fit(X)\n    assert_equal(clf._contamination, 0.2)\n    clf = IsolationForest(contamination='auto').fit(X)\n    assert_equal(clf._contamination, 'auto')\n    clf = IsolationForest().fit(X)\n    assert_equal(clf._contamination, 0.1)\n    assert_warns_message(FutureWarning,\n                         \"default contamination parameter 0.1 will change\",\n                         IsolationForest().fit, X)\n", "def test_iforest_max_features_consistency():\n    # Make sure validated max_features in iforest and BaseBagging are identical\n    X = iris.data\n    clf = IsolationForest().fit(X)\n    assert_equal(clf.max_features_, clf._max_features)\n", "def test_iforest_oob_score():\n    # Test that OOB score is not supported by iforest\n    X = iris.data\n    clf = IsolationForest()\n    with pytest.raises(NotImplementedError):\n        clf.fit(X, oob_score=True)\n", "def test_iforest_contamination():\n    \"\"\"Test contamination parameter in IsolationForest.\"\"\"\n    X_train = np.array([[0, 1], [1, 2], [2, 1], [3, 1], [4, 1], [5, 2], [6, 1]])\n    X_test = np.array([[2, 1], [1, 1], [10, 10], [11, 11]])\n\n    # Test contamination 'auto'\n    clf = IsolationForest(contamination='auto', random_state=rng).fit(X_train)\n    assert_equal(clf._contamination, 0.1)  # Default value\n    assert_almost_equal(clf.offset_, -0.5)\n    pred = clf.predict(X_test)\n    assert_array_equal(pred, [1, 1, -1, -1])\n\n    # Test contamination as float\n    clf = IsolationForest(contamination=0.25, random_state=rng).fit(X_train)\n    assert_equal(clf._contamination, 0.25)\n    pred = clf.predict(X_test)\n    assert_array_equal(pred, [1, 1, 1, -1])\n\n    # Test contamination as integer\n    assert_raises(ValueError, IsolationForest(contamination=2).fit, X_train)\n\n    # Test contamination with threshold_ attribute deprecation warning\n    with pytest.warns(DeprecationWarning):\n        thresh = clf.threshold_\n"], "sample_1024": ["def test_Float_abs():\n    assert abs(Float('-1.2')) == Float('1.2')\n    assert abs(Float('1.2')) == Float('1.2')\n    assert abs(Float('-inf')) == Float('inf')\n    assert abs(Float('inf')) == Float('inf')\n    assert abs(Float('nan')) == Float('nan')\n", "def test_Float_from_numpy_float():\n    import numpy as np\n    a = np.float32(3.14159)\n    b = Float(a)\n    assert b == 3.14159\n    assert b._prec == 24\n", "def test_NumberSymbol_to_mpmath():\n    from mpmath import mp, mpf\n\n    # Test conversion of NumberSymbol to mpmath\n    mp.dps = 20\n    assert mp.mpf(pi) == mpf(pi.evalf(20))\n    assert mp.mpf(E) == mpf(E.evalf(20))\n    assert mp.mpf(Catalan) == mpf(Catalan.evalf(20))\n    assert mp.mpf(EulerGamma) == mpf(EulerGamma.evalf(20))\n    assert mp.mpf(GoldenRatio) == mpf(GoldenRatio.evalf(20))\n    assert mp.mpf(TribonacciConstant) == mpf(TribonacciConstant.evalf(20))\n", "def test_Float_from_mpmath():\n    from sympy import Float, pi, sqrt\n    import mpmath\n\n    # Test converting a mpmath float to a sympy Float\n    mpmath_pi = mpmath.mpf(pi)\n    sympy_pi = Float(mpmath_pi)\n    assert sympy_pi == pi\n\n    # Test converting a sympy Float to an mpmath float\n    sympy_sqrt_2 = sqrt(2)\n    mpmath_sqrt_2 = mpmath.mpf(sympy_sqrt_2)\n    assert mpmath_sqrt_2 == mpmath.sqrt(2)\n", "def test_issue_10670():\n    assert Float(1e-300, precision=15)._prec == 15\n    assert Float(1e-300, dps=15)._prec == 53\n    assert Float(1e-300, precision=15) == Float(1e-300, dps=15)\n"], "sample_239": ["def test_empty_form_media(self):\n    \"\"\"Media is available on empty form.\"\"\"\n    class MediaForm(Form):\n        class Media:\n            js = ('some-file.js',)\n    self.assertIn('some-file.js', str(formset_factory(MediaForm)().empty_form.media))\n", "def test_empty_formset_as_p(self):\n    \"\"\"as_p() works with an empty formset.\"\"\"\n    class EmptyForm(Form):\n        pass\n    formset = formset_factory(EmptyForm, extra=0)()\n    self.assertEqual(formset.as_p(), '')\n", "def test_formset_validate_min_empty_forms_with_can_delete(self):\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=2, min_num=1, validate_min=True, can_delete=True)\n    formset = ChoiceFormSet(data, prefix='choices')\n    self.assertFalse(formset.has_changed())\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at least 1 form.'])\n", "def test_formset_min_num(self):\n    \"\"\"FormSets validate the minimum number of forms.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, min_num=2)\n    data = {\n        'choices-TOTAL_FORMS': '1',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at least 2 forms.'])\n", "def test_custom_prefix(self):\n    \"\"\"Formset uses the specified prefix.\"\"\"\n    formset = FavoriteDrinksFormSet(prefix='drinks')\n    self.assertEqual(formset.prefix, 'drinks')\n    data = {\n        'drinks-TOTAL_FORMS': '2',\n        'drinks-INITIAL_FORMS': '0',\n        'drinks-MIN_NUM_FORMS': '0',\n        'drinks-MAX_NUM_FORMS': '0',\n        'drinks-0-name': 'Gin and Tonic',\n        'drinks-1-name': 'Bloody Mary',\n    }\n    formset = FavoriteDrinksFormSet(data=data, prefix='drinks')\n    self.assertTrue(formset.is_valid())\n"], "sample_92": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            email='test@example.com', is_active=False,\n            **cls.user_credentials\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username=cls.remote_user, email='test@example.com', is_active=False\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username=cls.remote_user, email='test@example.com', is_active=False,\n            password='test'\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username=cls.remote_user,\n            email='test@example.com',\n            is_active=False\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username='test', email='test@example.com', is_active=False\n        )\n"], "sample_224": ["def test_group_by_subquery_annotation_collision(self):\n    \"\"\"\n    Subquery annotations and external aliases are included in the GROUP BY\n    if they are grouped against.\n    \"\"\"\n    books_rating_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        price=Decimal('29.69'),\n    ).values('rating')\n    publisher_qs = Publisher.objects.filter(\n        book__contact__age__gt=20,\n        name=self.p1.name,\n    ).annotate(\n        rating=Subquery(books_rating_qs),\n        contacts_count=Count('book__contact'),\n    ).values('rating').annotate(total_count=Count('rating'))\n    self.assertEqual(list(publisher_qs), [\n        {'rating': 4.0, 'total_count': 2},\n    ])\n", "def test_aggregation_subquery_annotation_with_join(self):\n    \"\"\"\n    Subquery annotations are included in the GROUP BY if they are\n    grouped against and involve a join.\n    \"\"\"\n    long_books_count_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n        authors__in=Author.objects.filter(age__gt=30),\n    ).values(\n        'publisher'\n    ).annotate(count=Count('pk')).values('count')\n    long_books_count_breakdown = Publisher.objects.values_list(\n        Subquery(long_books_count_qs, IntegerField()),\n    ).annotate(total=Count('*'))\n    self.assertEqual(dict(long_books_count_breakdown), {None: 2, 1: 3})\n", "def test_aggregate_expression_annotation(self):\n    authors = Author.objects.annotate(\n        combined_ages=Sum(F('age') + F('friends__age')),\n        avg_friend_age=Avg('friends__age')\n    ).order_by('name')\n    authors2 = Author.objects.annotate(\n        combined_ages=Sum('age') + Sum('friends__age'),\n        avg_friend_age=Avg('friends__age')\n    ).order_by('name')\n    for qs in (authors, authors2):\n        self.assertQuerysetEqual(\n            qs, [\n                ('Adrian Holovaty', 132, 32.0),\n                ('Brad Dayley', None, None),\n                ('Jacob Kaplan-Moss', 129, 29.5),\n                ('James Bennett', 63, 34.0),\n                ('Jeffrey Forcier', 128, 27.0),\n                ('Paul Bissex', 120, 31.0),\n                ('Peter Norvig', 103, 46.0),\n                ('Stuart Russell', 103, 57.0),\n                ('Wesley J. Chun', 176, 33.666666666666664)\n            ],\n            lambda a: (a.name, a.combined_ages, a.avg_friend_age)\n        )\n", "def test_annotate_with_subquery(self):\n    \"\"\"\n    Test annotate() with a Subquery() instance.\n    \"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).values('name', 'latest_book_pubdate')\n    self.assertEqual(\n        list(publisher_qs), [\n            {'name': 'Apress', 'latest_book_pubdate': datetime.date(2008, 6, 23)},\n            {'name': 'Morgan Kaufmann', 'latest_book_pubdate': datetime.date(1991, 10, 15)},\n            {\"name\": \"Jonno's House of Books\", 'latest_book_pubdate': None},\n            {'name': 'Prentice Hall', 'latest_book_pubdate': datetime.date(1995, 1, 15)},\n            {'name': 'Sams', 'latest_book_pubdate': datetime.date(2008, 3, 3)},\n        ]\n    )\n", "def test_aggregation_subquery_annotation_filter(self):\n    \"\"\"\n    Subquery annotations can be filtered and they are not included in the\n    GROUP BY.\n    \"\"\"\n    recent_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pubdate__year=OuterRef('founded_year'),\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        recent_book_pubdate=Subquery(recent_book_pubdate_qs),\n    ).annotate(count=Count('book'))\n    self.assertEqual(list(publisher_qs.values('name', 'recent_book_pubdate')), [\n        {'name': 'Apress', 'recent_book_pubdate': datetime.date(2007, 12, 6)},\n        {'name': 'Morgan Kaufmann', 'recent_book_pubdate': datetime.date(1991, 10, 15)},\n        {\"name\": \"Jonno's House of Books\", 'recent_book_pubdate': None},\n        {'name': 'Prentice Hall', 'recent_book_pubdate': None},\n        {'name': 'Sams', 'recent_book_pubdate': datetime.date(2008, 3, 3)},\n    ])\n"], "sample_1072": ["def test_frac_properties():\n    assert frac(x).is_finite\n    assert not frac(x).is_integer\n    assert frac(x).is_real\n    assert frac(I*x).is_imaginary\n    assert frac(x).is_zero == (x.is_zero | x.is_integer)\n    assert not frac(x).is_negative\n", "def test_complex_frac():\n    # Test frac function with complex arguments\n    assert frac(I) == 0\n    assert frac(-I) == 0\n    assert frac(2 + 3*I) == 0.2 + 0.3*I\n    assert frac(-2 - 3*I) == -0.2 - 0.3*I\n    assert frac(Rational(23, 10) + E*I) == 0.3 - 0.7*I\n", "def test_floor_ceiling_imaginary():\n    assert floor(I) == I\n    assert ceiling(I) == I\n    assert floor(-I) == -I\n    assert ceiling(-I) == -I\n    assert floor(I/2) == 0\n    assert ceiling(I/2) == I\n    assert floor(-I/2) == -I\n    assert ceiling(-I/2) == 0\n    assert floor(2*I) == 2*I\n    assert ceiling(2*I) == 2*I\n    assert floor(-2*I) == -2*I\n    assert ceiling(-2*I) == -2*I\n    assert floor(I/2 + 3*I/2) == 3*I\n    assert ceiling(I/2 + 3*I/2) == 4*I\n    assert floor(-I/2 + 3*I/2) == 3*I\n    assert ceiling(-I/2 + 3*I/2) == 3*I\n    assert floor(I/2 - 3*I/2) == -2*I\n    assert ceiling(I/2 - 3*I/2) == I\n    assert floor(-I/2 - 3*I/2) == -3*I\n    assert ceiling(-I/2 - 3*I/2) == -2*I\n", "def test_floor_and_ceiling_equality():\n    # Test that floor and ceiling functions handle equality correctly\n    assert floor(2.5) == 2\n    assert ceiling(2.5) == 3\n    assert floor(2.5) != ceiling(2.5)\n    assert floor(2) == ceiling(2) == 2\n", "def test_frac_properties():\n    # Test frac properties\n    assert frac(0).is_zero\n    assert frac(0).is_finite\n    assert frac(0).is_integer\n    assert frac(0).is_real\n\n    assert frac(1).is_finite\n    assert frac(1).is_real\n    assert not frac(1).is_zero\n    assert not frac(1).is_integer\n\n    assert frac(x).is_zero is None\n    assert frac(x).is_integer is None\n\n    assert frac(x + 1).is_zero is None\n    assert frac(x + 1).is_integer is None\n\n    assert frac(n).is_zero\n    assert frac(n).is_finite\n    assert frac(n).is_integer\n    assert frac(n).is_real\n\n    assert frac(p_r).is_finite\n    assert frac(p_r).is_real\n    assert not frac(p_r).is_zero\n    assert frac(p_r).is_integer is None\n"], "sample_609": ["def test_apply_where_two_inputs() -> None:\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    zero_array = np.zeros_like(array)\n    zero_variable = xr.Variable(\"x\", zero_array)\n    zero_data_array = xr.DataArray(zero_variable, [(\"x\", -array)])\n    zero_dataset = xr.Dataset({\"y\": zero_variable}, {\"x\": -array})\n\n    assert_identical(array, xr.where(cond, array, zero_array))\n    assert_identical(array, xr.where(cond, zero_array, array))\n\n    assert_identical(variable, xr.where(cond, variable, zero_array))\n    assert_identical(variable, xr.where(cond, variable, zero_variable))\n    assert_identical(variable, xr.where(cond, zero_array, variable))\n    assert_identical(variable, xr.where(cond, zero_variable, variable))\n\n    assert_identical(data_array, xr.where(cond, data_array, zero_array))\n    assert_identical(data_array, xr.where(cond, data_array, zero_variable))\n    assert_identical(data_array, xr.where(cond, data_array, zero_data_array))\n    assert_identical(data_array, xr.where(cond, zero_array, data_array))\n    assert_identical(data_array, xr.where(cond, zero_variable, data_array))\n    assert_identical(data_array, xr.where(cond, zero_data_array, data_array))\n\n    assert_identical(dataset, xr.where(cond, dataset, zero_array))\n    assert_identical(dataset, xr.where(cond, dataset, zero_variable))\n", "def test_where_with_dataset() -> None:\n    cond = xr.DataArray([True, False], dims=\"x\")\n    x = xr.DataArray([1, 2], dims=\"y\")\n    y = xr.DataArray([3, 4], dims=\"y\")\n    data = xr.Dataset({\"x\": x, \"y\": y})\n    actual = xr.where(cond, data, 0)\n    expected = xr.Dataset({\"x\": xr.DataArray([1, 0], dims=\"x\"), \"y\": xr.DataArray([3, 0], dims=\"x\")})\n    assert_identical(expected, actual)\n", "def test_where_alignment() -> None:\n    cond = xr.DataArray([True, False], dims=\"x\")\n    x = xr.DataArray([1, 2], dims=\"y\")\n    y = xr.DataArray([0, 1], dims=\"z\")\n\n    # Test alignment of three arguments\n    with pytest.raises(ValueError):\n        xr.where(cond, x, y)\n", "def test_dot_align_coords_explicit(use_dask) -> None:\n    # GH 3694\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n\n    a = np.arange(30 * 4).reshape(30, 4)\n    b = np.arange(30 * 4 * 5).reshape(30, 4, 5)\n\n    # use partially overlapping coords\n    coords_a = {\"a\": np.arange(30), \"b\": np.arange(4)}\n    coords_b = {\"a\": np.arange(5, 35), \"b\": np.arange(1, 5)}\n\n    da_a = xr.DataArray(a, dims=[\"a\", \"b\"], coords=coords_a)\n    da_b = xr.DataArray(b, dims=[\"a\", \"b\", \"c\"], coords=coords_b)\n\n    if use_dask:\n        da_a = da_a.chunk({\"a\": 3})\n        da_b = da_b.chunk({\"a\": 3})\n\n    # explicit join\n    actual = xr.dot(da_a, da_b, join=\"inner\")\n    expected = (da_a * da_b).sum([\"a\", \"b\"])\n    xr.testing.assert_allclose(expected, actual)\n\n    with pytest.raises(ValueError, match=r\"indexes along dimension\"):\n        xr.dot(da_a, da_b, join=\"exact\")\n\n    actual = xr.dot(da_a, da_b, join=\"outer\")\n    expected = (da_a * da_b).sum([\"a\", \"b\"])\n    xr.testing.assert_allclose(expected, actual)\n\n    actual = xr.dot(da_a, da_b, join=\"left\")\n", "def test_where_dataset_variable_alignment() -> None:\n    ds = xr.Dataset({\"a\": ([\"x\", \"y\"], np.arange(4).reshape(2, 2))})\n    var = xr.Variable((\"x\", \"y\"), np.arange(4).reshape(2, 2))\n\n    # Test alignment of dataset and variable\n    actual = xr.where(ds.a > 1, ds, var)\n    expected = xr.Dataset({\"a\": ([\"x\", \"y\"], np.array([[0, 1], [2, 3]]))})\n    assert_identical(expected, actual)\n"], "sample_1202": ["def test_Float_negation():\n    assert -Float('0.0') == Float('-0.0')\n    assert -Float('1.0') == Float('-1.0')\n    assert -Float('-1.0') == Float('1.0')\n", "def test_Rational_ceiling_floor():\n    a = Rational(3, 2)\n\n    assert a.floor() == 1\n    assert a.ceiling() == 2\n", "def test_abs_value():\n    assert abs(S.Zero) is S.Zero\n    assert abs(Float(0)) is not S.Zero and abs(Float(0)) == 0\n", "def test_issue_16725():\n    assert comp(Float(2), sqrt(4))\n    assert comp(Float(2), sqrt(5), eps=1)\n    assert not comp(Float(2), sqrt(5), eps=0.1)\n", "def test_Integer_truediv():\n    assert Integer(5) / Integer(2) == Rational(5, 2)\n    assert Integer(5) / 2 == Rational(5, 2)\n    assert 5 / Integer(2) == Rational(5, 2)\n    assert Integer(5) / 2.0 == 2.5\n    assert 5.0 / Integer(2) == 2.5\n\n    assert Integer(5) / Float(2) == Float(2.5)\n    assert Float(5) / Integer(2) == Float(2.5)\n    assert Float(5) / Float(2) == Float(2.5)\n\n    assert Integer(5) / Rational(2, 3) == Rational(15, 6)\n    assert Rational(5, 6) / Integer(2) == Rational(5, 12)\n\n    assert Integer(5) / zoo is S.NaN\n    assert Integer(5) / oo == 0\n    assert Integer(5) / -oo == -0\n    assert Integer(5) / nan is S.NaN\n\n    assert Integer(0) / Integer(2) == 0\n    assert Integer(0) / 2 == 0\n    assert 0 / Integer(2) == 0\n    assert Integer(0) / 2.0 == 0.0\n    assert 0.0 / Integer(2) == 0.0\n    assert Integer(0) / Float(2) == Float(0.0)\n    assert Float(0) / Integer(2) == Float(0.0)\n    assert Float(0) / Float(2) == Float(0.0)\n    assert Integer(0) / Rational(2, 3) == 0\n    assert Rational(0, 3) / Integer(2) == 0\n\n    assert Integer(5) / S.Half == 10\n    assert Integer(5) / pi == Float(5) / pi\n    assert Integer(5) / E == Float(5) / E\n    assert Integer(5) / GoldenRatio == Float(5) / GoldenRatio\n    assert Integer"], "sample_653": ["def test_log_in_runtest_logstart_logfinish(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info(f\"logstart {nodeid} {location}\")\n\n            logger.info(f\"logfinish {nodeid} {location}\")\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n                assert True\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*logstart*\", \"*logfinish*\"])\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"logstart\" in contents\n        assert \"logfinish\" in contents\n", "def test_log_file_formatting(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    custom_format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    custom_date_format = \"%Y-%m-%d %H:%M:%S\"\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format = {}\n        log_file_date_format = {}\n        \"\"\".format(\n            log_file, custom_format, custom_date_format\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger().info(\"Custom formatted message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert re.match(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} - root - INFO - Custom formatted message\", contents.strip())\n", "def test_log_file_format(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format = %(asctime)s - %(levelname)s - %(message)s\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import time\n\n            logging.getLogger('catchlog').info(\"This is a test log message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"INFO\" in contents\n        assert \"This is a test log message\" in contents\n        # Check if the timestamp is present and in the correct format\n        assert re.match(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}\", contents.split(\" - \")[0])\n", "def test_log_cli_format(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.warning('custom format')\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format=%(message)s\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"test_log_cli_format.py::test_log_cli \",\n            \"*custom format*\",\n            \"PASSED*\",\n        ]\n    )\n    assert \"WARNING\" not in result.stdout.str()\n", "def test_log_cli_format(testdir):\n    # Test log_cli_format option\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.formatter._fmt == 'custom format %(levelname)s %(message)s'\n            logging.getLogger('catchlog').warning(\"test log message\")\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format = custom format %(levelname)s %(message)s\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"*test_log_cli_format.py* custom format WARNING test log message\"])\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n"], "sample_739": ["def test_label_binarize_empty_input():\n    y = []\n    classes = [0, 1]\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([])\n\n    assert_raises(ValueError, label_binarize, y, classes, neg_label=neg_label, pos_label=pos_label)\n", "def test_label_binarize_multilabel_errors():\n    y = [0, 1, 2]\n    classes = [0, 1]\n    pos_label = 2\n    neg_label = 0\n    assert_raises(ValueError, label_binarize, y, classes)\n\n    y = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1]\n    assert_raises(ValueError, label_binarize, y, classes)\n", "def test_label_binarize_multiclass_with_missing_classes():\n    y = [0, 1, 2, 4]\n    classes = [0, 1, 2]\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([[1, 0, 0],\n                         [0, 1, 0],\n                         [0, 0, 1],\n                         [0, 0, 0]])\n\n    assert_raises(ValueError, label_binarize, y, classes, neg_label, pos_label)\n", "def test_label_binarize_multiclass_unseen_labels():\n    lb = LabelBinarizer()\n    lb.fit([1, 2, 3, -1, 1])\n    assert_raises(ValueError, lb.transform, [0, 6])\n", "def test_label_binarize_sparse_output():\n    y = [0, 1, 2]\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n    expected = 2 * np.eye(3)\n    sparse_expected = csr_matrix(expected)\n\n    # Sparse output case\n    binarized = label_binarize(y, classes, neg_label=neg_label, pos_label=pos_label, sparse_output=True)\n    assert_array_equal(binarized.toarray(), expected)\n    assert_true(issparse(binarized))\n    assert_array_equal(binarized.toarray(), sparse_expected.toarray())\n\n    # Check label binarizer with sparse output\n    lb = LabelBinarizer(neg_label=neg_label, pos_label=pos_label, sparse_output=True)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(binarized.toarray(), expected)\n    assert_true(issparse(binarized))\n    assert_array_equal(binarized.toarray(), sparse_expected.toarray())\n"], "sample_579": ["def test_no_cbar(self):\n    kws = self.default_kws.copy()\n    kws['cbar'] = False\n    cg = mat.ClusterGrid(self.df_norm, **kws)\n    cg.plot(**self.default_plot_kws)\n    assert cg.ax_cbar is None\n", "def test_tree_kws_linecolor(self):\n\n    rgb = (1, .5, .2)\n    g = mat.clustermap(self.df_norm, tree_kws=dict(linewidths=2, color=rgb))\n    for ax in [g.ax_col_dendrogram, g.ax_row_dendrogram]:\n        tree, = ax.collections\n        assert tree.get_linewidths()[0] == 2\n        assert tuple(tree.get_color().squeeze())[:3] == rgb\n", "def test_tree_kws_colors(self):\n\n    colors = ['blue', 'red']\n    g = mat.clustermap(self.df_norm, tree_kws=dict(colors=colors))\n    for ax in [g.ax_col_dendrogram, g.ax_row_dendrogram]:\n        tree, = ax.collections\n        assert [c[0] for c in tree.get_colors()] == colors\n", "def test_custom_linkage_colors(self):\n    kws = self.default_kws.copy()\n\n    linkage = fastcluster.linkage_vector(self.x_norm, method='single', metric='euclidean')\n    dendrogram = hierarchy.dendrogram(linkage, no_plot=True, color_threshold=-np.inf)\n    kws['linkage'] = linkage\n    kws['tree_kws'] = {'colors': 'red'}\n    p = mat._DendrogramPlotter(self.df_norm, **kws)\n\n    npt.assert_array_equal(p.linkage, linkage)\n    assert p.dendrogram == dendrogram\n\n    # Check if the colors of the dendrogram are red\n    ax = plt.gca()\n    tree, = ax.collections\n    assert tuple(tree.get_color().squeeze())[:3] == (1, 0, 0)\n", "def test_no_cbar(self):\n    kws = self.default_kws.copy()\n    kws[\"cbar\"] = False\n\n    g = mat.clustermap(self.df_norm, **kws)\n    assert g.ax_cbar is None\n    assert g.cax is None\n"], "sample_47": ["def test_ajax_response_content_type(self):\n    response = self.client.get('/raises500/', HTTP_X_REQUESTED_WITH='XMLHttpRequest')\n    self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n", "def test_cleanse_setting_dict_value(self):\n    \"\"\"\n    The debug page should filter out some sensitive information found in\n    dict settings.\n    \"\"\"\n    sensitive_settings = [\n        'SECRET_KEY',\n        'PASSWORD',\n        'API_KEY',\n        'AUTH_TOKEN',\n    ]\n    for setting in sensitive_settings:\n        FOOBAR = {\n            'normal': \"should be displayed\",\n            setting: \"should not be displayed\",\n        }\n        cleansed = cleanse_setting('FOOBAR', FOOBAR)\n        self.assertEqual(cleansed['normal'], \"should be displayed\")\n        self.assertEqual(cleansed[setting], CLEANSED_SUBSTITUTE)\n", "    def test_unicode_error(self):\n        \"\"\"\n        A UnicodeError displays a portion of the problematic string.\n        \"\"\"\n        try:\n            'abcdefghijkl\u1f40pqrstuwxyz'.encode('ascii')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        text = reporter.get_traceback_text()\n        self.assertIn('Unicode error hint', text)\n        self.assertIn('The string that could not be encoded/decoded was: ', text)\n        self.assertIn('\u1f40', text)\n", "def test_cleanse_setting_dict(self):\n    \"\"\"\n    The cleanse_setting function should also cleanse dictionaries.\n    \"\"\"\n    sensitive_dict = {\n        'SECRET_KEY': \"should not be displayed\",\n        'PASSWORD': \"should not be displayed\",\n        'INNER_DICT': {\n            'API_KEY': \"should not be displayed\",\n            'AUTH_TOKEN': \"should not be displayed\",\n        },\n    }\n    cleansed_dict = cleanse_setting('FOOBAR', sensitive_dict)\n    self.assertEqual(cleansed_dict['SECRET_KEY'], CLEANSED_SUBSTITUTE)\n    self.assertEqual(cleansed_dict['PASSWORD'], CLEANSED_SUBSTITUTE)\n    self.assertEqual(cleansed_dict['INNER_DICT']['API_KEY'], CLEANSED_SUBSTITUTE)\n    self.assertEqual(cleansed_dict['INNER_DICT']['AUTH_TOKEN'], CLEANSED_SUBSTITUTE)\n", "def test_sensitive_function_arguments_in_debug_false(self):\n    self.verify_safe_response(sensitive_args_function_caller, check_for_POST_params=False)\n    self.verify_safe_email(sensitive_args_function_caller, check_for_POST_params=False)\n"], "sample_507": ["def test_plot_empty_categories():\n    ax = plt.figure().subplots()\n    with pytest.raises(ValueError):\n        ax.plot([], [])\n", "def test_convert_with_empty_unit():\n    with pytest.raises(ValueError):\n        cat.StrCategoryConverter.convert(['a', 'b'], None, None)\n", "def test_convert_with_none_unit():\n    cc = cat.StrCategoryConverter()\n    ax = FakeAxis(None)\n    with pytest.raises(ValueError):\n        cc.convert(\"value\", None, ax)\n", "    def test_convert_with_none_unit(self):\n        with pytest.raises(ValueError):\n            self.cc.convert(['a', 'b'], None, self.ax)", "def test_update_plot_with_different_types(self, plotter):\n    ax = plt.figure().subplots()\n    plotter(ax, ['a', 'b'], [1, 2])\n    plotter(ax, [3, 4], ['c', 'd'])\n    axis_test(ax.xaxis, ['a', 'b', 3, 4])\n    axis_test(ax.yaxis, [1, 2, 'c', 'd'])\n"], "sample_62": ["def test_unregister_not_registered_model(self):\n    \"\"\"Unregistering a model that is not registered should raise NotRegistered.\"\"\"\n    msg = \"The model Person is not registered\"\n    with self.assertRaisesMessage(admin.sites.NotRegistered, msg):\n        self.site.unregister(Person)\n", "def test_unregister_registered_model(self):\n    \"Test that unregistering a registered model works correctly.\"\n    self.site.register(Person)\n    self.assertTrue(self.site.is_registered(Person))\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n", "def test_unregister_registered_model(self):\n    self.site.register(Person)\n    self.assertTrue(self.site.is_registered(Person))\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n", "def test_unregister_model(self):\n    self.site.register(Person)\n    self.assertTrue(self.site.is_registered(Person))\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n", "def test_unregister_registered_model(self):\n    self.site.register(Person)\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n"], "sample_79": ["def test_tuples(self):\n    self.check_values(('0', 'items'), ('1', 'item'), ('2', 'items'))\n", "def test_singular_and_plural_suffix_with_separator(self):\n    self.check_values(('0', 'items'), ('1', 'item'), ('2', 'items,s'))\n", "def test_singular_and_plural_suffix_with_edge_cases(self):\n    self.check_values(('0', 'items'), ('1', 'item'), ('2', 'items'), (0.5, 'items'), (1.5, 'items'), (Decimal(1), 'item'), (Decimal(0), 'items'), (Decimal(2), 'items'), ([1], 'item'), ([], 'items'), ([1, 2, 3], 'items'))\n", "def test_multiple_suffixes(self):\n    self.check_values(('0', 'items'), ('1', 'item'), ('2', 'items'))\n", "def test_floatformat(self):\n    self.check_values(('34.23234', '34.2'), ('34.00000', '34'), ('34.26000', '34.3'), ('34.232', '34.232'), ('34.000', '34.000'), ('34.260', '34.260'), ('34.23234', '34.232'), ('34.00000', '34.000'), ('34.26000', '34.260'))\n"], "sample_301": ["def test_watched_files_include_extra_files(self, mocked_modules, notify_mock):\n    self.reloader.extra_files = {self.nonexistent_file}\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    watched_files = list(self.reloader.watched_files())\n    self.assertIn(self.nonexistent_file, watched_files)\n", "    def test_notify_file_changed(self, mock_trigger_reload):\n        reloader = autoreload.BaseReloader()\n        path = Path('/path/to/file')\n        file_changed.connect(reloader.notify_file_changed)\n        file_changed.send(sender=reloader, file_path=path)\n        self.assertEqual(mock_trigger_reload.call_count, 1)\n        self.assertEqual(mock_trigger_reload.call_args[0][0], path)\n", "def test_watched_files_include_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "def test_snapshot_files_multiple_files(self, mock_notify_file_changed):\n    existing_file2 = self.ensure_file(self.tempdir / 'test2.py')\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, existing_file2]):\n        snapshot1 = dict(self.reloader.snapshot_files())\n        self.assertIn(self.existing_file, snapshot1)\n        self.assertIn(existing_file2, snapshot1)\n        self.increment_mtime(self.existing_file)\n        snapshot2 = dict(self.reloader.snapshot_files())\n        self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n        self.assertEqual(snapshot1[existing_file2], snapshot2[existing_file2])\n", "def test_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n"], "sample_193": ["def test_generic_fk_self(self):\n    A = self.create_model(\"A\", foreign_keys=[\n        GenericForeignKey(),\n    ])\n    self.assertRelated(A, [A])\n", "def test_abstract_base_exclude_fields(self):\n    class Abstract(models.Model):\n        name = models.CharField(max_length=50)\n        age = models.IntegerField()\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n\n    class Child(Abstract):\n        class Meta:\n            exclude = ('age',)\n\n    child_state = ModelState.from_model(Child)\n    self.assertEqual(list(child_state.fields), ['id', 'name'])\n", "def test_m2m_through_field_name(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T', through_fields=('a', 'b'))])\n    B = self.create_model(\"B\")\n    T = self.create_model(\"T\", foreign_keys=[\n        models.ForeignKey('A', models.CASCADE),\n        models.ForeignKey('B', models.CASCADE),\n    ])\n    self.assertEqual(A.b.field.m2m_field_name(), 'a')\n    self.assertEqual(A.b.field.m2m_reverse_field_name(), 'b')\n    self.assertEqual(B.a.field.m2m_field_name(), 'b')\n    self.assertEqual(B.a.field.m2m_reverse_field_name(), 'a')\n", "def test_generic_fk_with_through_model(self):\n    A = self.create_model(\"A\", foreign_keys=[\n        GenericForeignKey('content_type', 'object_id'),\n    ])\n    B = self.create_model(\"B\", foreign_keys=[\n        models.ForeignKey('C', models.CASCADE),\n    ])\n    T = self.create_model(\"T\", foreign_keys=[\n        models.ForeignKey('A', models.CASCADE),\n        models.ForeignKey('B', models.CASCADE),\n        models.ForeignKey('contenttypes.ContentType', models.CASCADE),\n        models.PositiveIntegerField('object_id'),\n    ])\n    self.assertRelated(A, [B, T])\n    self.assertRelated(B, [A, T])\n    self.assertRelated(T, [A, B])\n", "def test_custom_index_together(self):\n    \"\"\"\n    Tests making a ProjectState from an Apps with a custom index_together\n    \"\"\"\n    new_apps = Apps(['migrations'])\n\n    class Author(models.Model):\n        name = models.CharField(max_length=255)\n        bio = models.TextField()\n        age = models.IntegerField(blank=True, null=True)\n\n        class Meta:\n            app_label = 'migrations'\n            apps = new_apps\n            index_together = [['name', 'bio']]\n\n    author_state = ModelState.from_model(Author)\n    self.assertEqual(author_state.options['index_together'], {('name', 'bio')})\n"], "sample_238": ["def test_aggregation_subquery_annotation_related_field_values(self):\n    \"\"\"\n    Subquery annotations are excluded from the GROUP BY if they are not\n    selected.\n    \"\"\"\n    books_qs = Book.objects.annotate(\n        contact_publisher=Subquery(\n            Publisher.objects.filter(\n                pk=OuterRef('publisher'),\n                name=OuterRef('contact__name'),\n            ).values('name')[:1],\n        )\n    ).filter(\n        contact_publisher__isnull=False,\n    ).annotate(\n        author_count=Count('authors'),\n    ).values('name', 'author_count').order_by('name')\n    self.assertEqual(list(books_qs), [\n        {'name': 'Artificial Intelligence: A Modern Approach', 'author_count': 2},\n        {'name': 'Python Web Development with Django', 'author_count': 3},\n    ])\n", "def test_aggregation_subquery_annotation_filtered(self):\n    \"\"\"\n    Subquery annotations are included in the GROUP BY if they are filtered.\n    \"\"\"\n    books_qs = Book.objects.annotate(\n        min_age=Min('authors__age'),\n    ).filter(\n        min_age__isnull=False,\n    ).annotate(count=Count('authors'))\n    self.assertEqual(books_qs.count(), Book.objects.exclude(authors=None).count())\n", "def test_aggregation_subquery_annotation_multi_join(self):\n    \"\"\"\n    Subquery annotations are included in the GROUP BY if they involve a multi-join.\n    \"\"\"\n    store_books_qs = Store.objects.filter(\n        books=OuterRef('pk'),\n    ).values(\n        'books'\n    ).annotate(store_count=Count('pk')).values('store_count')\n    book_store_count_breakdown = Book.objects.values_list(\n        Subquery(store_books_qs, IntegerField()),\n    ).annotate(total=Count('*'))\n    self.assertEqual(dict(book_store_count_breakdown), {1: 1, 2: 4, 3: 1})\n", "def test_aggregation_expression_wrappers(self):\n    # Test using ExpressionWrapper to convert the output of an aggregation\n    from django.db.models import ExpressionWrapper, IntegerField\n\n    # Annotate with the sum of pages and price, then divide by the count of books\n    books = Book.objects.annotate(\n        avg_pages_price=ExpressionWrapper(Sum('pages') + Sum('price'), output_field=IntegerField()) / Count('id')\n    )\n    book = books.first()\n    self.assertEqual(book.avg_pages_price, (sum(b.pages + b.price for b in books) / books.count()))\n", "def test_annotation_with_expression_in_filter(self):\n    authors = Author.objects.annotate(friend_count=Count('friends')).filter(friend_count__gt=F('age'))\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty',),\n            ('Jacob Kaplan-Moss',),\n            ('Jeffrey Forcier',),\n            ('Wesley J. Chun',)\n        ],\n        lambda a: (a.name,)\n    )\n"], "sample_182": ["def test_union_with_values_list_and_empty_result(self):\n    qs = Number.objects.filter(pk__in=[])\n    self.assertEqual(list(qs.union(qs).values_list('num', flat=True)), [])\n", "def test_intersection_with_extra_and_values_list(self):\n    qs1 = Number.objects.filter(num=1).extra(\n        select={'count': 0},\n    ).values_list('num', 'count')\n    qs2 = Number.objects.filter(num=2).extra(select={'count': 1})\n    self.assertCountEqual(qs1.intersection(qs2), [])\n", "def test_union_with_extra_and_annotation(self):\n    qs1 = Number.objects.filter(num=1).annotate(\n        count=Value(0, IntegerField()),\n    ).values_list('num', 'count')\n    qs2 = Number.objects.filter(num=2).extra(select={'count': 1})\n    self.assertCountEqual(qs1.union(qs2), [(1, 0), (2, 1)])\n", "def test_union_with_distinct(self):\n    Number.objects.create(num=1, other_num=2)\n    Number.objects.create(num=1, other_num=2)\n    qs1 = Number.objects.filter(num=1)\n    qs2 = Number.objects.filter(other_num=2)\n    self.assertEqual(qs1.union(qs2, all=True).count(), 2)\n    self.assertEqual(qs1.union(qs2, all=False).count(), 1)\n", "def test_union_with_different_model(self):\n    # Test union operation with two different models\n    class AnotherNumber(Number):\n        class Meta:\n            app_label = 'tests'\n\n    Number.objects.bulk_create(Number(num=i) for i in range(5))\n    AnotherNumber.objects.bulk_create(AnotherNumber(num=i) for i in range(5, 10))\n\n    qs1 = Number.objects.all()\n    qs2 = AnotherNumber.objects.all()\n    union_qs = qs1.union(qs2)\n    self.assertEqual(union_qs.count(), 10)\n    self.assertNumbersEqual(union_qs.order_by('num'), list(range(10)))\n"], "sample_743": ["def test_callable_metric_with_sparse_input():\n    X = csr_matrix(np.array([[1, 2], [3, 4]]))\n    y = np.array([0, 1])\n\n    nn = neighbors.NearestNeighbors(n_neighbors=1, metric=sparse_metric)\n    nn.fit(X)\n    dist, ind = nn.kneighbors(X)\n\n    assert_array_equal(dist, [[0], [0]])\n    assert_array_equal(ind, [[0], [1]])\n\n    knn = neighbors.KNeighborsClassifier(n_neighbors=1, metric=sparse_metric)\n    knn.fit(X, y)\n    y_pred = knn.predict(X)\n\n    assert_array_equal(y_pred, y)\n", "def test_sparse_metric_callable_kneighbors():\n        assert_true(issparse(x) and issparse(y))\n        return x.dot(y.T).A.item()\n\n    X = csr_matrix([[1, 0], [0, 1], [1, 1]])\n    nn = neighbors.NearestNeighbors(n_neighbors=1, metric=sparse_metric)\n    nn.fit(X)\n    dist, ind = nn.kneighbors(csr_matrix([[0, 1]]))\n    assert_array_equal(dist, [[0]])\n    assert_array_equal(ind, [[1]])\n", "def test_sparse_metric_compatibility():\n    # Test compatibility of sparse metrics with different algorithms\n    X = csr_matrix(np.random.rand(10, 10))\n    y = np.random.randint(0, 2, 10)\n\n    for algorithm in ALGORITHMS:\n        clf = neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm, metric=sparse_metric)\n        reg = neighbors.KNeighborsRegressor(n_neighbors=3, algorithm=algorithm, metric=sparse_metric)\n\n        if algorithm == 'kd_tree':\n            # sparse_metric is not compatible with kd_tree\n            assert_raises(ValueError, clf.fit, X, y)\n            assert_raises(ValueError, reg.fit, X, y)\n        else:\n            clf.fit(X, y)\n            reg.fit(X, y)\n", "def test_predict_proba_with_neighbors_fit():\n    # Test predict_proba when Neighbors is fit with a different object\n    X = np.array([[0, 2, 0], [0, 2, 1], [2, 0, 0], [2, 2, 0], [0, 0, 2], [0, 0, 1]])\n    y = np.array([4, 4, 5, 5, 1, 1])\n    cls = neighbors.KNeighborsClassifier(n_neighbors=3, p=1)  # cityblock dist\n    cls.fit(X, y)\n    X_new = np.array([[0, 2, 0], [2, 2, 2]])\n    y_new = np.array([4, 5])\n    cls_new = neighbors.KNeighborsClassifier(n_neighbors=3, p=1)\n    cls_new.fit(X_new, y_new)\n    y_prob = cls.predict_proba(X_new)\n    y_prob_new = cls_new.predict_proba(X_new)\n    assert_array_almost_equal(y_prob, y_prob_new)\n", "def test_sparse_metric_warning():\n    # Test warning for sparse matrices and callable metric\n    X = csr_matrix([[1, 0], [0, 1]])\n    y = [0, 1]\n        assert_true(issparse(x) and issparse(y))\n        return x.dot(y.T).A.item()\n\n    msg = (\"Using callable metric with sparse matrices is \"\n           \"slow. Precomputing the metric is recommended.\")\n    with assert_warns_message(RuntimeWarning, msg):\n        knn = neighbors.KNeighborsClassifier(n_neighbors=1, metric=sparse_metric)\n        knn.fit(X, y)\n        knn.predict(X)\n"], "sample_623": ["    def test_requested_chunks(self, shape, pref_chunks, req_chunks, expected_chunks):\n        \"\"\"Use the requested chunks when opening a dataset.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial,\n            engine=PassThroughBackendEntrypoint,\n            chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n        )\n        self.check_dataset(initial, final, expected_chunks)\n", "    def test_auto_chunks(self, shape, pref_chunks):\n        \"\"\"Use the backend's preferred chunks when opening a dataset with chunks='auto'.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(initial, engine=PassThroughBackendEntrypoint, chunks='auto')\n        self.check_dataset(initial, final, explicit_chunks(pref_chunks, shape))\n", "    def test_auto_chunks(self, shape, pref_chunks):\n        \"\"\"Test the 'auto' chunking option.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial, engine=PassThroughBackendEntrypoint, chunks=\"auto\"\n        )\n        # 'auto' chunking should honor the backend's preferred chunks.\n        self.check_dataset(initial, final, explicit_chunks(pref_chunks, shape))\n", "    def create_dataset(self, shape, pref_chunks):\n        \"\"\"Return a dataset with a variable with the given shape and preferred chunks.\"\"\"\n        dims = tuple(f\"dim_{idx}\" for idx in range(len(shape)))\n        return xr.Dataset(\n            {\n                self.var_name: xr.Variable(\n                    dims,\n                    np.empty(shape, dtype=np.dtype(\"V1\")),\n                    encoding={\"preferred_chunks\": dict(zip(dims, pref_chunks))},\n                )\n            }\n        )\n", "def test_no_preferred_chunks(self, shape, pref_chunks, req_chunks):\n    \"\"\"Fall back to requested chunks when the backend does not specify preferred chunks.\"\"\"\n    initial = xr.Dataset(\n        {self.var_name: xr.Variable(tuple(f\"dim_{idx}\" for idx in range(len(shape))), np.empty(shape, dtype=np.dtype(\"V1\")))}\n    )\n    final = xr.open_dataset(\n        initial,\n        engine=PassThroughBackendEntrypoint,\n        chunks=dict(zip(initial[self.var_name].dims, req_chunks)) if req_chunks is not None else req_chunks,\n    )\n    self.check_dataset(initial, final, explicit_chunks(req_chunks, shape) if req_chunks is not None else shape)\n"], "sample_956": ["def test_missing_reference_fallback(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'fallback': ('https://docs.python.org/py3k/', inv_file),\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    rn = reference_check(app, 'py', 'func', 'fallback:module1.func', 'func')\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n    assert rn['reftitle'] == '(in foo v2.0)'\n    assert rn[0].astext() == 'func'\n", "def test_normalize_intersphinx_mapping(app):\n    \"\"\"normalize_intersphinx_mapping interface\"\"\"\n    app.config.intersphinx_mapping = {\n        'old1': 'http://old1.com/',\n        ('new1', 'http://new1.com/', 'inv1'),\n        'old2': ('http://old2.com/', 'inv2'),\n        ('new2', ('http://new2.com/', 'inv2')),\n    }\n\n    normalize_intersphinx_mapping(app, app.config)\n\n    assert app.config.intersphinx_mapping == {\n        'old1': (None, ('http://old1.com/', ('',))),\n        'new1': ('new1', ('http://new1.com/', ('inv1',))),\n        'old2': (None, ('http://old2.com/', ('inv2',))),\n        'new2': ('new2', ('http://new2.com/', ('inv2',))),\n    }\n", "def test_load_mappings_concurrent(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n        'py3k': ('https://docs.python.org/py3k/', inv_file),\n        'repoze.workflow': ('http://docs.repoze.org/workflow/', inv_file),\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n    inv = app.env.intersphinx_inventory\n\n    assert 'py:module' in inv\n    assert 'std:doc' in inv\n    assert 'std:term' in inv\n    assert 'py:function' in inv\n\n    assert 'module1' in inv['py:module']\n    assert 'docname' in inv['std:doc']\n    assert 'a term' in inv['std:term']\n    assert 'module1.func' in inv['py:function']\n", "def test_inspect_main_error(capsys):\n    \"\"\"inspect_main interface, with a non-existent file argument\"\"\"\n    inspect_main([\"non_existent_file\"])\n\n    stdout, stderr = capsys.readouterr()\n    assert stdout == \"\"\n    assert stderr.startswith(\"Failed to read intersphinx inventory 'non_existent_file'\")\n", "def test_inspect_main_exception(capsys, tempdir):\n    \"\"\"inspect_main interface, with invalid url argument\"\"\"\n    inspect_main([\"http://invalidurl/\"])\n\n    stdout, stderr = capsys.readouterr()\n    assert stderr.startswith(\"Failed to read inventory file:\")\n"], "sample_9": ["def test_write_table_html_table_id():\n    \"\"\"\n    Test that passing a table_id should result in the HTML table having that id.\n    \"\"\"\n    buffer_output = StringIO()\n    t = Table([[1], [2]], names=('a', 'b'))\n    ascii.write(t, buffer_output, format='html', htmldict={'table_id': 'test_id'})\n\n    assert '<table id=\"test_id\">' in buffer_output.getvalue()\n", "def test_html_table_id():\n    \"\"\"\n    Test to make sure that the HTML writer writes the table id correctly.\n    \"\"\"\n    table_id = 'test_id'\n    col1 = [1, 2, 3]\n    col2 = [(1.0, 1.0), (2.0, 2.0), (3.0, 3.0)]\n    table = Table([col1, col2], names=('C1', 'C2'))\n    expected = f'<table id=\"{table_id}\">'\n    out = html.HTML({'table_id': table_id}).write(table)[0].strip()\n    assert expected in out\n", "def test_html_with_empty_td():\n    \"\"\"\n    Test reading a table with empty <td> elements\n    \"\"\"\n    table_in = ['<table>',\n                '<tr><th>A</th><th>B</th></tr>',\n                '<tr><td></td><td>2</td></tr>',\n                '</table>']\n\n    dat = Table.read(table_in, format='ascii.html')\n    assert dat.masked is False\n    assert np.all(dat['A'].mask == [True, False])\n    assert dat['A'].dtype.kind == 'i'\n", "def test_html_write_table_with_id():\n    \"\"\"\n    Test to make sure that the HTML writer includes the table id in the output.\n    \"\"\"\n\n    col1 = [1, 2, 3]\n    col2 = ['a', 'b', 'c']\n    table = Table([col1, col2], names=('C1', 'C2'))\n    expected = \"\"\"\\", "def test_write_table_html_fill_values_explicit_column():\n    \"\"\"\n    Test that passing fill_values for a specific column should replace any matching row in that column\n    \"\"\"\n    buffer_output = StringIO()\n    t = Table([[1], [2]], names=('a', 'b'))\n    ascii.write(t, buffer_output, fill_values=({'a': (1, 'Hello world')}),\n                format='html')\n\n    t_expected = Table([['Hello world'], [2]], names=('a', 'b'))\n    buffer_expected = StringIO()\n    ascii.write(t_expected, buffer_expected, format='html')\n\n    assert buffer_output.getvalue() == buffer_expected.getvalue()\n"], "sample_591": ["def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n    ds2 = xr.Dataset({\"x\": 2, \"z\": 3})\n    expected = xr.Dataset({\"x\": 2, \"y\": 1, \"z\": 3})\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=\"x\"))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"x\"]))\n", "def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [0, 1]})\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=\"a\"))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"a\"]))\n", "def test_merge_no_conflicts_with_overwrite_vars(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 3]), \"x\": [0, 1, 2]})\n\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\", overwrite_vars=\"a\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\", overwrite_vars=\"a\"))\n", "def test_merge_no_conflicts_multi_var_overlap(self):\n    data = create_test_data()\n    ds1 = data[[\"var1\", \"var2\"]]\n    ds2 = data[[\"var2\", \"var3\"]]\n    expected = data[[\"var1\", \"var2\", \"var3\"]]\n    actual = ds1.merge(ds2, compat=\"no_conflicts\")\n    assert expected.identical(actual)\n", "def test_merge_non_overlapping_same_name_different_dtype(self):\n    ds1 = xr.Dataset({\"x\": (\"y\", [0, 1])})\n    ds2 = xr.Dataset({\"x\": (\"y\", [0.0, 1.0])})\n    with pytest.raises(xr.MergeError):\n        ds1.merge(ds2)\n"], "sample_582": ["def test_load_dotenv_no_path(monkeypatch):\n    for item in (\"FOO\", \"BAR\"):\n        monkeypatch._setitem.append((os.environ, item, notset))\n\n    monkeypatch.chdir(test_path)\n    load_dotenv(None)\n    assert \"FOO\" in os.environ\n    assert \"BAR\" in os.environ\n", "def test_run_cert_ssl_context(monkeypatch):\n    ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    ssl_context.load_cert_chain(certfile='mycert.pem', keyfile='mykey.pem')\n\n    monkeypatch.setitem(sys.modules, 'ssl_context', ssl_context)\n    ctx = run_command.make_context('run', ['--cert', 'ssl_context'])\n    assert ctx.params['cert'] is ssl_context\n\n    # no --key with SSLContext\n    with pytest.raises(click.BadParameter):\n        run_command.make_context('run', ['--cert', 'ssl_context', '--key', __file__])\n", "def test_run_cert_import_error(monkeypatch):\n    monkeypatch.setitem(sys.modules, \"bad_import\", object())\n\n    # non-SSLContext object imported\n    with pytest.raises(click.BadParameter):\n        run_command.make_context(\"run\", [\"--cert\", \"bad_import\"])\n", "def test_flaskgroup_app_loading_error(runner, capsys):\n    # Test that an error message is shown when the app fails to load\n        raise Exception(\"App loading error\")\n\n    cli = FlaskGroup(create_app=create_app)\n    result = runner.invoke(cli, [\"--help\"])\n    assert result.exit_code == 0\n    captured = capsys.readouterr()\n    assert \"Error: App loading error\\n\" in captured.err\n", "def test_run_command_extra_files(runner, monkeypatch):\n    monkeypatch.setenv(\"FLASK_APP\", \"cliapp.app:testapp\")\n    result = runner.invoke(run_command, [\"--extra-files\", __file__])\n    assert result.exit_code == 0\n"], "sample_794": ["def test_ridge_regression_return_n_iter(solver):\n    rng = np.random.RandomState(42)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n    alpha = 1.0\n    coef, n_iter = ridge_regression(X, y, alpha=alpha, solver=solver, return_n_iter=True)\n    assert isinstance(n_iter, int)\n", "def test_ridge_classifier_cv_sample_weight():\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n    sample_weight = np.array([1.0, 1.0, 1.0, 2.0, 2.0])\n\n    reg = RidgeClassifierCV(class_weight=None, alphas=[.01, .1, 1], fit_intercept=False)\n    reg.fit(X, y, sample_weight=sample_weight)\n\n    # Check that sample weights are taken into account by comparing to the result\n    # of RidgeClassifierCV without sample weights.\n    reg_no_weight = RidgeClassifierCV(class_weight=None, alphas=[.01, .1, 1], fit_intercept=False)\n    reg_no_weight.fit(X, y)\n\n    assert_array_almost_equal(reg.coef_, reg_no_weight.coef_)\n", "def test_ridge_cv_with_list_alphas():\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = [1, 1, 1, -1, -1]\n\n    alphas = [0.1, 1.0, 10.0]\n    ridge = RidgeCV(alphas=alphas, cv=None, store_cv_values=True)\n    ridge.fit(X, y)\n\n    assert ridge.alpha_ in alphas\n", "def test_ridge_classifier_predict_proba():\n    # Test predict_proba method of RidgeClassifier\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    reg = RidgeClassifier()\n    reg.fit(X, y)\n\n    proba = reg.predict_proba([[0.2, -1.0]])\n    assert_equal(proba.shape, (1, 2))\n    assert_greater(proba[0, 0], 0.5)\n    assert_greater(proba[0, 1], 0.5)\n", "def test_ridge_classifier_multi_label():\n    # Test RidgeClassifier with multi-label target\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    # Adding a constant feature to check if intercept is fitted correctly\n    X = np.hstack((X, np.ones((X.shape[0], 1))))\n    ridge = RidgeClassifier()\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (y.shape[1], X.shape[1]))\n    y_pred = ridge.predict(X)\n    # Checking if the predicted output is multi-label\n    assert_equal(len(y_pred.shape), 2)\n"], "sample_829": ["def test_incremental_pca_sparse_transform():\n    # Test that transform works with sparse input.\n    rng = np.random.RandomState(1999)\n    n, p = 100, 3\n    X = rng.randn(n, p)\n    X_sparse = sparse.csr_matrix(X)\n    ipca = IncrementalPCA(n_components=2, batch_size=10)\n    ipca.fit(X)\n    X_transformed = ipca.transform(X_sparse)\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\n", "def test_incremental_pca_sparse_transform():\n    # Test that the transformation of sparse data is correct.\n    rng = np.random.RandomState(1999)\n    n, p = 100, 3\n    X = rng.randn(n, p)\n    X[:10] += np.array([3, 4, 5])\n    Xt = 0.1 * rng.randn(1, p) + np.array([3, 4, 5])\n    X_sparse = sparse.csr_matrix(X)\n    Xt_sparse = sparse.csr_matrix(Xt)\n\n    # Get the transformation of the generated data Xt\n    ipca = IncrementalPCA(n_components=2).fit(X_sparse)\n    Yt = ipca.transform(Xt_sparse)\n\n    # Normalize\n    Yt /= np.sqrt((Yt ** 2).sum())\n\n    # Make sure that the first element of Yt is ~1, this means\n    # the transformation worked as expected\n    assert_almost_equal(np.abs(Yt[0][0]), 1., 1)\n", "def test_incremental_pca_sparse_transform():\n    # Test that transform works with sparse input.\n    X = iris.data\n    X_sparse = sparse.csr_matrix(X)\n    batch_size = X.shape[0] // 3\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n    ipca.fit(X)\n\n    X_transformed = ipca.transform(X_sparse)\n\n    assert X_transformed.shape == (X.shape[0], 2)\n", "def test_incremental_pca_sparse_transform():\n    # Test that transform works correctly with sparse input.\n    X = iris.data\n    X_sparse = sparse.csr_matrix(X)\n    batch_size = X.shape[0] // 3\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n    pca = PCA(n_components=2)\n    pca.fit_transform(X)\n\n    X_transformed_sparse = ipca.fit_transform(X_sparse)\n    X_transformed_dense = ipca.transform(X)\n\n    assert_array_almost_equal(X_transformed_sparse, X_transformed_dense)\n", "def test_incremental_pca_sparse_transform():\n    # Test that the transform of sparse data is correct.\n    rng = np.random.RandomState(1999)\n    n, p = 100, 3\n    X = rng.randn(n, p) * .1\n    X[:10] += np.array([3, 4, 5])\n    X_sparse = sparse.csr_matrix(X)\n\n    # Get the transformation of the generated data X\n    ipca = IncrementalPCA(n_components=2, batch_size=10).fit(X_sparse)\n    Yt = ipca.transform(X_sparse)\n\n    # Normalize\n    Yt /= np.sqrt((Yt ** 2).sum())\n\n    # Make sure that the first element of Yt is ~1, this means\n    # the transformation worked as expected\n    assert_almost_equal(np.abs(Yt[0][0]), 1., 1)\n"], "sample_514": ["def test_colorbar_set_alpha():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    assert cb.alpha is None\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n", "def test_offset_text_loc_bottom():\n    fig, ax = plt.subplots()\n    np.random.seed(seed=19680808)\n    pc = ax.pcolormesh(np.random.randn(10, 10)*1e6)\n    cb = fig.colorbar(pc, location='bottom', extend='max')\n    fig.draw_without_rendering()\n    # check that the offsetText is in the proper place below the\n    # colorbar axes.  In this case the colorbar axes is the same\n    # height as the parent, so use the parents bbox.\n    assert cb.ax.xaxis.offsetText.get_position()[1] < ax.bbox.y0\n", "def test_colorbar_with_format():\n    fig, ax = plt.subplots()\n    data = np.random.randn(10, 10)\n    pc = ax.pcolormesh(data)\n    cb = fig.colorbar(pc, ax=ax, format='%.2e')\n", "def test_nonorm_with_alpha():\n    plt.rcParams['svg.fonttype'] = 'none'\n    data = [1, 2, 3, 4, 5]\n    alpha = [0.2, 0.4, 0.6, 0.8, 1.0]\n\n    fig, ax = plt.subplots(figsize=(6, 1))\n    fig.subplots_adjust(bottom=0.5)\n\n    norm = NoNorm(vmin=min(data), vmax=max(data))\n    cmap = cm.get_cmap(\"viridis\", len(data))\n    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n    mappable.set_array(alpha)\n    cbar = fig.colorbar(mappable, cax=ax, orientation=\"horizontal\")\n", "def test_colorbar_ticks_autoticks():\n    fig, ax = plt.subplots()\n    x = np.linspace(-10, 10, 100)\n    y = np.linspace(-10, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) + np.cos(Y)\n    cax = ax.pcolormesh(X, Y, Z, cmap='RdBu', vmin=-2, vmax=2)\n    cbar = fig.colorbar(cax, ax=ax, ticks=[-2, -1, 0, 1, 2])\n    cbar.minorticks_on()\n"], "sample_383": ["    def test_ticket_24605_extra(self):\n        \"\"\"\n        Subquery table names should be quoted when using extra().\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(alive=False).extra(\n                where=['NOT EXISTS (SELECT 1 FROM \"queries_relatedindividual\" WHERE \"queries_individual\".\"id\" = \"queries_relatedindividual\".\"related_id\")']\n            ),\n            [i4],\n        )\n        self.assertSequenceEqual(\n            Individual.objects.exclude(alive=False).extra(\n                where=['EXISTS (SELECT 1 FROM \"queries_relatedindividual\" WHERE \"queries_individual\".\"id\" = \"queries_relatedindividual\".\"related_id\")']\n            ).order_by(\"pk\"),\n            [i1, i2, i3],\n        )\n", "    def test_subqueries_in_values(self):\n        a1 = Author.objects.create(name=\"a1\", num=1)\n        a2 = Author.objects.create(name=\"a2\", num=2)\n        Note.objects.create(note=\"n1\", misc=\"foo\", id=1, author=a1)\n        Note.objects.create(note=\"n2\", misc=\"bar\", id=2, author=a2)\n\n        subquery = Note.objects.filter(note=\"n1\").values(\"author\")\n        authors = Author.objects.filter(id__in=subquery)\n        self.assertSequenceEqual(authors, [a1])\n", "    def test_subquery_quoting(self):\n        a = Author.objects.create(name='a')\n        e = ExtraInfo.objects.create(info='e', author=a)\n        n = Note.objects.create(note='n', misc='m', extra=e)\n        an = Annotation.objects.create(name='an', note=n)\n        self.assertSequenceEqual(\n            Annotation.objects.filter(\n                note__in=Note.objects.filter(extra__author__name='a')\n            ),\n            [an],\n        )\n", "def test_custom_lookup(self):\n    # Test a custom lookup that checks if a field's value is in a given list\n    i1 = Individual.objects.create(name='John')\n    i2 = Individual.objects.create(name='Jane')\n    i3 = Individual.objects.create(name='Jim')\n    self.assertSequenceEqual(\n        Individual.objects.filter(name__inlist=['John', 'Jane']),\n        [i1, i2],\n    )\n    self.assertSequenceEqual(\n        Individual.objects.exclude(name__inlist=['Jane']),\n        [i1, i3],\n    )\n", "    def test_subquery_names_quoted(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=True), Q(related_individual__isnull=False)\n            ).order_by(\"pk\"),\n            [i1, i2],\n        )\n        self.assertSequenceEqual(\n            Individual.objects.exclude(\n                Q(alive=True), Q(related_individual__isnull=False)\n            ).order_by(\"pk\"),\n            [i3, i4],\n        )\n"], "sample_961": ["def test_pyfunction_with_positional_only_args(app):\n    text = \".. py:function:: hello(a, /, b, *, c)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc_sig_operator, \"/\"],\n                                      [desc_parameter, desc_sig_name, \"b\"],\n                                      [desc_parameter, desc_sig_operator, \"*\"],\n                                      [desc_parameter, desc_sig_name, \"c\"])])\n", "def test_pydata_with_default(app):\n    text = (\".. py:data:: version\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 1\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"version\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])],\n                                                    [desc_annotation, \" = 1\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n    assert 'version' in domain.objects\n    assert domain.objects['version'] == ('index', 'version', 'data', False)\n", "def test_domain_py_exceptions(app, status, warning):\n    app.builder.build_all()\n\n    doctree = app.env.get_doctree('exceptions')\n    refnodes = list(doctree.traverse(pending_xref))\n    assert_refnode(refnodes[0], None, None, 'BaseException', 'exc')\n    assert_refnode(refnodes[1], None, None, 'Exception', 'exc')\n    assert_refnode(refnodes[2], None, None, 'ArithmeticError', 'exc')\n    assert_refnode(refnodes[3], None, None, 'FloatingPointError', 'exc')\n    assert_refnode(refnodes[4], None, None, 'OverflowError', 'exc')\n    assert_refnode(refnodes[5], None, None, 'ZeroDivisionError', 'exc')\n    assert_refnode(refnodes[6], None, None, 'AssertionError', 'exc')\n    assert_refnode(refnodes[7], None, None, 'AttributeError', 'exc')\n    assert_refnode(refnodes[8], None, None, 'BufferError', 'exc')\n    assert_refnode(refnodes[9], None, None, 'EOFError', 'exc')\n    assert_refnode(refnodes[10], None, None, 'ImportError', 'exc')\n    assert_refnode(refnodes[11], None, None, 'ModuleNotFoundError', 'exc')\n    assert_refnode(refnodes[12], None, None, 'LookupError', 'exc')\n    assert_refnode(refnodes[13], None, None, 'IndexError', 'exc')\n    assert_refnode(refnodes[14], None, None, 'KeyError', 'exc')\n    assert_refnode(refnodes[15], None, None, 'MemoryError', 'exc')\n    assert_refnode(refnodes[16], None, None, 'NameError', 'exc')\n    assert_refnode(refnodes[17], None, None, '", "def test_python_python_use_unqualified_type_names_with_type_alias(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content\n    assert '<p><strong>age</strong> (<em>Age</em>) \u2013 blah blah</p>' in content\n", "def test_python_class_inheritance(app, status, warning):\n    app.build()\n    content = (app.outdir / 'inheritance.html').read_text()\n    assert ('<p>Bases: <a class=\"reference internal\" href=\"#parent\" title=\"parent\">'\n            '<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">'\n            'parent</span></code></a></p>' in content)\n    assert ('<p>Bases: <a class=\"reference internal\" href=\"#parent\" title=\"parent\">'\n            '<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">'\n            'parent</span></code></a>, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">'\n            'other</span></code></p>' in content)\n    assert warning.getvalue() == ''\n"], "sample_332": ["def test_formset_with_ordering_and_deletion_extra_forms(self):\n    \"\"\"FormSets with ordering + deletion + extra forms.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True, extra=2)\n    initial = [\n        {'choice': 'Calexico', 'votes': 100},\n        {'choice': 'Fergie', 'votes': 900},\n        {'choice': 'The Decemberists', 'votes': 500},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    # Let's delete Fergie, and put The Decemberists ahead of Calexico.\n    data = {\n        'choices-TOTAL_FORMS': '5',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '3',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-0-ORDER': '1',\n        'choices-0-DELETE': '',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-1-ORDER': '2',\n        'choices-1-DELETE': 'on',\n        'choices-2-choice': 'The Decemberists',\n        'choices-2-votes': '500',\n        'choices-2-ORDER': '0',\n        'choices-2-DELETE': '',\n        'choices-3-choice': '',\n        'choices-3-votes': '',\n        'choices-3-ORDER': '',\n        'choices-3-DELETE': '',\n        'choices-4-choice': '',\n        'choices-4-votes': '',\n", "def test_empty_formset_total_error_count(self):\n    \"\"\"A valid empty formset should have 0 total errors.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=0)\n    formset = ChoiceFormSet({})\n    self.assertEqual(formset.total_error_count(), 0)\n", "def test_formset_media(self):\n    \"\"\"Formset media includes all media from individual forms.\"\"\"\n    class MediaForm(Form):\n        class Media:\n            js = ('some-file.js',)\n            css = {'screen': ('style.css',)}\n    formset = formset_factory(MediaForm)()\n    self.assertIn('some-file.js', str(formset.media))\n    self.assertIn('style.css', str(formset.media))\n", "def test_formset_with_custom_prefix(self):\n    \"\"\"The prefix attribute can be customized.\"\"\"\n    formset = self.make_choiceformset(prefix='custom_prefix')\n    self.assertEqual(formset.prefix, 'custom_prefix')\n    self.assertEqual(formset.management_form.prefix, 'custom_prefix')\n    self.assertHTMLEqual(\n        str(formset.management_form),\n        \"\"\"<input type=\"hidden\" name=\"custom_prefix-TOTAL_FORMS\" value=\"1\">", "def test_formset_with_empty_permitted(self):\n    \"\"\"\n    A form in a formset may have empty_permitted set to True.\n    This form should be allowed to be empty.\n    \"\"\"\n    class CustomForm(Form):\n        name = CharField()\n\n            empty_permitted = kwargs.pop('empty_permitted', False)\n            super().__init__(*args, **kwargs)\n            if empty_permitted:\n                self.empty_permitted = True\n\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-0-name': '',\n    }\n    CustomFormSet = formset_factory(CustomForm, extra=1)\n    formset = CustomFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([{'name': ''}], formset.cleaned_data)\n"], "sample_265": ["def test_file_charset_option(self):\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {\n            'file_charset': 'iso-8859-1',\n        },\n    })\n    self.assertEqual(engine.engine.file_charset, 'iso-8859-1')\n", "def test_template_origin(self):\n    \"\"\"Test the origin property of the Template class.\"\"\"\n    engine = DjangoTemplates({\n        'DIRS': [Path(__file__).parent],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n    template = engine.get_template('test_template.html')\n    self.assertEqual(template.origin.name, 'test_template.html')\n", "def test_no_apps_installed(self):\n    \"\"\"\n    Test that the engine correctly handles no applications installed.\n    \"\"\"\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n\n    self.assertEqual(engine.engine.libraries, {})\n", "def test_origin(self):\n    engine = DjangoTemplates({\n        'DIRS': [str(Path(__file__).parent / 'test_templates')],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n    template = engine.get_template('template.html')\n    self.assertEqual(template.origin.name, 'template.html')\n", "def test_get_template(self):\n    engine = DjangoTemplates({\n        'DIRS': [str(Path(__file__).resolve().parent / 'test_templates')],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n    template = engine.get_template('test_template.html')\n    self.assertIsNotNone(template)\n    self.assertEqual(template.render({'name': 'Test'}), 'Hello, Test')\n\n    with self.assertRaises(TemplateDoesNotExist):\n        engine.get_template('nonexistent_template.html')\n"], "sample_6": ["def test_angle_array_operations():\n    \"\"\"\n    Test arithmetic operations with Angle arrays.\n    \"\"\"\n    a1 = Angle([0, 45, 90, 180, 270, 360], unit=u.degree)\n    a2 = Angle([90, 180, 270, 360, 45, 0], unit=u.degree)\n\n    result_add = a1 + a2\n    expected_add = Angle([90, 225, 360, 540, 315, 360], unit=u.degree)\n    npt.assert_array_almost_equal(result_add.degree, expected_add.degree)\n\n    result_sub = a1 - a2\n    expected_sub = Angle([-90, -135, -180, -180, 225, 0], unit=u.degree)\n    npt.assert_array_almost_equal(result_sub.degree, expected_sub.degree)\n\n    result_mul = a1 * 2\n    expected_mul = Angle([0, 90, 180, 360, 540, 720], unit=u.degree)\n    npt.assert_array_almost_equal(result_mul.degree, expected_mul.degree)\n\n    result_div = a1 / 2\n    expected_div = Angle([0, 22.5, 45, 90, 135, 180], unit=u.degree)\n    npt.assert_array_almost_equal(result_div.degree, expected_div.degree)\n", "def test_angle_wrap_at():\n    \"\"\"\n    Test wrapping of Angle objects.\n    \"\"\"\n    a1 = Angle([350, 10, -10], unit=u.degree)\n    a2 = a1.wrap_at(180 * u.degree)\n    npt.assert_almost_equal(a2.value, [-10, 10, -10])\n\n    a3 = Angle([350, 10, -10], unit=u.degree)\n    a3.wrap_at(180 * u.degree, inplace=True)\n    npt.assert_almost_equal(a3.value, [-10, 10, -10])\n\n    a4 = Angle([-200, 200, -200], unit=u.degree)\n    a5 = a4.wrap_at('180d')\n    npt.assert_almost_equal(a5.value, [160, -160, 160])\n\n    a6 = Angle([-200, 200, -200], unit=u.degree)\n    a6.wrap_at('180d', inplace=True)\n    npt.assert_almost_equal(a6.value, [160, -160, 160])\n\n    # Test wrapping with Latitude and Longitude objects\n    with pytest.raises(TypeError):\n        Latitude([0, 45, 90, 180, 270, 360, 720.], unit=u.degree).wrap_at(180 * u.degree)\n    with pytest.raises(TypeError):\n        Longitude([0, 45, 90, 180, 270, 360, 720.], unit=u.degree).wrap_at(180 * u.degree)\n\n    # Test is_within_bounds method\n    a7 = Angle([-20, ", "def test_angle_wrap_at():\n    a1 = Angle([360, 365, -1], unit=u.degree)\n    a2 = a1.wrap_at(360 * u.degree)\n    npt.assert_almost_equal(a2.value, [0, 5, 359])\n\n    a3 = Angle([180, 185, -5], unit=u.degree)\n    a4 = a3.wrap_at(180 * u.degree)\n    npt.assert_almost_equal(a4.value, [-180, -175, 5])\n\n    # Test inplace wrap_at\n    a1.wrap_at(360 * u.degree, inplace=True)\n    npt.assert_almost_equal(a1.value, [0, 5, 359])\n\n    a3.wrap_at(180 * u.degree, inplace=True)\n    npt.assert_almost_equal(a3.value, [-180, -175, 5])\n\n    # Test with a scalar wrap_angle\n    a5 = Angle([360, 365, -1], unit=u.degree)\n    a6 = a5.wrap_at(Angle(360, unit=u.degree))\n    npt.assert_almost_equal(a6.value, [0, 5, 359])\n\n    # Test with a Quantity wrap_angle\n    a7 = Angle([180, 185, -5], unit=u.degree)\n    a8 = a7.wrap_at(180 * u.degree * u.Unit(\"\"))\n    npt.assert_almost_equal(a8.value, [-180, -175, 5])\n\n    # Test with a string wrap_angle\n    a9 = Angle([180, 185, -5], unit=u.degree)\n    a10 = a9.wrap_at('180d", "def test_angle_wrap_at():\n    \"\"\"\n    Test the wrap_at method of the Angle class.\n    \"\"\"\n    a1 = Angle([360, 720, -360, -720], unit=u.degree)\n    a2 = a1.wrap_at(180 * u.degree)\n    npt.assert_almost_equal(a2.value, [0, 0, 0, 0])\n    a3 = a1.wrap_at(90 * u.degree, inplace=True)\n    npt.assert_almost_equal(a1.value, [0, 0, -90, -90])\n    assert a3 is None\n    a4 = Angle([175, 180, 185], unit=u.degree)\n    a5 = a4.wrap_at('180d')\n    npt.assert_almost_equal(a5.value, [-185, -180, -175])\n", "def test_array_coordinates_from_quantity():\n    \"\"\"\n    Test creating coordinates from arrays of Quantity objects.\n    \"\"\"\n    c = ICRS(np.array([1, 2])*u.degree, np.array([3, 4])*u.degree)\n    assert not c.ra.isscalar\n\n    with pytest.raises(ValueError):\n        c = ICRS(np.array([1, 2])*u.degree, np.array([3, 4, 5])*u.degree)\n\n    # ensure arrays of Quantity objects can be used\n    c = ICRS(ra=[1, 2]*u.degree, dec=[3, 4]*u.degree)\n    assert not c.ra.isscalar\n\n    with pytest.raises(ValueError):\n        c = ICRS(ra=[1, 2]*u.degree, dec=[3, 4, 5]*u.degree)\n"], "sample_269": ["    def test_i18n_with_different_domain(self):\n        \"\"\"\n        Test JavaScriptCatalog view with a different domain.\n        \"\"\"\n        view = JavaScriptCatalog.as_view(domain='mydomain')\n        request = RequestFactory().get('/jsi18n/mydomain/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'mydomain')\n", "def test_i18n_with_missing_translation(self):\n    \"\"\"\n    Test that the JavaScriptCatalog view returns an empty catalog for a language\n    that has a missing translation.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode())\n        self.assertEqual(data['catalog'], {})\n", "def test_i18n_different_plural_forms(self):\n    \"\"\"\n    Test that the JavaScript catalog correctly handles different plural forms.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('pl'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, '1 form')\n        self.assertContains(response, '2 forms')\n        self.assertContains(response, '5 forms')\n", "def test_i18n_language_english_default_invalid_package(self):\n    \"\"\"\n    Check if a ValueError is raised when an invalid package is provided to JavaScriptCatalog.\n    \"\"\"\n    view = JavaScriptCatalog.as_view()\n    request = RequestFactory().get('/')\n    msg = 'Invalid package(s) provided to JavaScriptCatalog: unknown_package'\n    with self.assertRaisesMessage(ValueError, msg):\n        view(request, packages='unknown_package')\n", "    def test_i18n_translation_reload(self):\n        \"\"\"\n        Check if the JavaScript i18n view reloads translations when the\n        LANGUAGE_COOKIE_NAME cookie changes.\n        \"\"\"\n        # Set initial language to 'en-us'\n        response = self.client.get('/jsi18n/')\n        self.assertContains(response, '\"this is to be translated\": \"this is to be translated\"')\n\n        # Change language to 'fr'\n        lang_code = 'fr'\n        post_data = {'language': lang_code, 'next': '/'}\n        response = self.client.post('/i18n/setlang/', post_data)\n\n        # Check if the JavaScript i18n view returns 'fr' translations\n        response = self.client.get('/jsi18n/')\n        self.assertContains(response, '\"this is to be translated\": \"il faut le traduire\"')\n"], "sample_38": ["def test_dropaxis():\n    w = wcs.WCS(naxis=3)\n    w.wcs.crval = [50, 50, 2.12345678e11]\n    w.wcs.cdelt = [1e-3, 1e-3, 1e8]\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'FREQ']\n    w.wcs.set()\n\n    w_dropped = w.dropaxis(1)\n    assert w_dropped.naxis == 2\n    assert w_dropped.wcs.ctype == ['RA---TAN', 'FREQ']\n", "def test_all_world2pix_with_adaptive():\n    \"\"\"Test all_world2pix with adaptive parameter set to True\"\"\"\n\n    # Open test FITS file:\n    fname = get_pkg_data_filename('data/j94f05bgq_flt.fits')\n    ext = ('SCI', 1)\n    h = fits.open(fname)\n    w = wcs.WCS(h[ext].header, h)\n    h.close()\n    del h\n\n    crpix = w.wcs.crpix\n    ncoord = crpix.shape[0]\n\n    # Assume that CRPIX is at the center of the image and that the image has\n    # a power-of-2 number of pixels along each axis. Only use the central\n    # 1/64 for this testing purpose:\n    naxesi_l = list((7. / 16 * crpix).astype(int))\n    naxesi_u = list((9. / 16 * crpix).astype(int))\n\n    # Generate integer indices of pixels (image grid):\n    img_pix = np.dstack([i.flatten() for i in\n                         np.meshgrid(*map(range, naxesi_l, naxesi_u))])[0]\n\n    # Generage random data (in image coordinates):\n    with NumpyRNGContext(123456789):\n        rnd_pix = np.random.rand(random_npts, ncoord)\n\n    # Scale random data to cover the central part of the image\n    mwidth = 2 * (crpix * 1. / 8)\n    rnd_pix = crpix - 0.5 * mwidth + (mwidth - 1) * rnd_pix\n\n    # Reference pixel coordinates in image coordinate system (CS):\n    test_pix = np.append(img_pix, rnd_pix, axis=0)\n    # Reference pixel coordinates in sky CS using forward transformation:\n    all_world = w.all_pix2world(test_pix, origin)\n\n    try:\n        runtime_begin", "def test_naxis_mismatch():\n    w = wcs.WCS(naxis=2)\n\n    x = np.random.random((2, 3, 4))\n    y = np.random.random((3, 2, 4))\n\n    with pytest.raises(ValueError) as exc:\n        xw, yw = w.all_pix2world(x, y, 1)\n    assert exc.value.args[0] == \"Coordinate arrays are not broadcastable to each other\"\n\n    with pytest.raises(ValueError) as exc:\n        xp, yp = w.all_world2pix(x, y, 1)\n    assert exc.value.args[0] == \"Coordinate arrays are not broadcastable to each other\"\n", "def test_header_string():\n    w = wcs.WCS()\n    header_string = w.to_header_string()\n    h = fits.Header.fromstring(header_string.strip())\n    if 'COMMENT' in h:\n        del h['COMMENT']\n    if '' in h:\n        del h['']\n    h2 = fits.Header.fromstring(w.to_header_string().strip())\n    assert dict(h) == dict(h2)\n", "def test_sip_with_altkey_no_sip():\n    \"\"\"\n    Test that when creating a WCS object using a key, CTYPE with\n    that key is looked at and not the primary CTYPE. But if the key\n    CTYPE doesn't have SIP, no SIP object should be created.\n    \"\"\"\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n        w = wcs.WCS(f[0].header)\n    # create a header with two WCSs.\n    h1 = w.to_header(relax=True, key='A')\n    h2 = w.to_header(relax=False)\n    h1['CTYPE1A'] = \"RA---SIN\"\n    h1['CTYPE2A'] = \"DEC--SIN\"\n    h1.update(h2)\n    w = wcs.WCS(h1, key='A')\n    assert w.sip is None\n"], "sample_1172": ["def test_solve_generic():\n    x, y = symbols('x y')\n    a = Poly(x - y + 5, x, y, domain='ZZ')\n    b = Poly(x + y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], Options((x, y), {'domain': 'ZZ'})) == [(-1, 4)]\n\n    a = Poly(x - 2*y + 5, x, y, domain='ZZ')\n    b = Poly(2*x - y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], Options((x, y), {'domain': 'ZZ'})) == [(11/3, 13/3)]\n\n    a = Poly(x**2 + y, x, y, domain='ZZ')\n    b = Poly(x + y*4, x, y, domain='ZZ')\n    assert solve_generic([a, b], Options((x, y), {'domain': 'ZZ'})) == [(0, 0), (1/4, -1/16)]\n", "def test_solve_generic():\n    x, y = symbols('x y')\n    a = Poly(x - y + 5, x, y, domain='ZZ')\n    b = Poly(x + y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], Options((x, y), {'domain': 'ZZ'})) == [(-1, 4)]\n\n    a = Poly(x - 2*y + 5, x, y, domain='ZZ')\n    b = Poly(2*x - y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], Options((x, y), {'domain': 'ZZ'})) == [(11/3, 13/3)]\n\n    a = Poly(x**2 + y, x, y, domain='ZZ')\n    b = Poly(x + y*4, x, y, domain='ZZ')\n    assert solve_generic([a, b], Options((x, y), {'domain': 'ZZ'})) == [(0, 0), (1/4, -1/16)]\n\n    a = Poly(x**3 - y**3, x, y)\n    b = Poly(x - y, x, y)\n    raises(NotImplementedError, lambda: solve_generic([a, b], Options((x, y))))\n\n    a = Poly(1/x, x)\n    raises(NotImplementedError, lambda: solve_generic([a], Options((x,))))\n", "def test_solve_generic():\n    # Test case with one solution\n    f1 = x - y + 5\n    f2 = x + y - 3\n    assert solve_poly_system([f1, f2], x, y) == [(-1, 4)]\n\n    # Test case with no solution\n    f1 = x - y + 5\n    f2 = x + y - 6\n    assert solve_poly_system([f1, f2], x, y) == []\n\n    # Test case with multiple solutions\n    f1 = x**2 + y\n    f2 = x + y*4\n    assert solve_poly_system([f1, f2], x, y) == [(0, 0), (1/4, -1/16)]\n", "def test_solve_generic():\n    x, y = symbols('x y')\n    f = x**2 + y - 2\n    g = x + y**2 - 3\n    solutions = solve_poly_system([f, g], x, y)\n    assert solutions == [(-sqrt(5)/2 + 1/2, -sqrt(5)/2 + 1), (sqrt(5)/2 + 1/2, sqrt(5)/2 - 1)]\n", "def test_solve_generic():\n    x0, y0, x1, y1, r = symbols('x0 y0 x1 y1 r')\n\n    f_1 = (x - x0)**2 + (y - y0)**2 - r**2\n    f_2 = (x - x1)**2 + (y - y1)**2 - r**2\n\n    result = solve_poly_system([f_1, f_2], x, y)\n\n    assert len(result) == 2 and all(len(r) == 2 for r in result)\n    assert all(len(r.find(query)) == 2 for r in flatten(result))\n\n    seq = (x**2 + y**2 - 2, y**2 - 1)\n    gens = (x, y)\n    polys, opt = parallel_poly_from_expr(seq, *gens)\n    assert solve_generic(polys, opt) == [\n        (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\n    seq = (x**2 + y**2 - 1, y**2 - 1)\n    polys, opt = parallel_poly_from_expr(seq, *gens)\n    ans = [(0, -1), (0, 1)]\n    assert solve_generic(polys, opt) == ans\n\n    seq = (x**2 + y**2 - 1, x**2 - x + y**2 - 1)\n    polys, opt = parallel_poly_from_expr(seq, *gens)\n    assert solve_generic(polys, opt) == ans\n"], "sample_118": ["def test_isnull_with_none_rhs(self):\n    \"\"\"Lookup.can_use_none_as_rhs=True allows None as a lookup value.\"\"\"\n    season = Season.objects.create(year=2012, nulled_text_field='not null')\n    self.assertFalse(Season.objects.filter(pk=season.pk, nulled_text_field__isnull_none_rhs=False))\n    self.assertTrue(Season.objects.filter(pk=season.pk, nulled_text_field__isnull_none_rhs=None))\n", "def test_year_lookup(self):\n    # Test YearLookup and its subclasses\n    # Test YearExact\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__exact=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    # Test YearGt\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__gt=2005),\n        []\n    )\n    # Test YearGte\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__gte=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    # Test YearLt\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__lt=2005),\n        []\n    )\n    # Test YearLte\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__lte=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n", "def test_exact_with_queryset_having_select_fields(self):\n    qs = Article.objects.filter(pk=OuterRef('pk'), headline__startswith='Article')\n    seasons = Season.objects.annotate(\n        pk_exists=Exists(qs),\n    ).filter(\n        pk_exists=Exists(qs),\n    )\n    self.assertCountEqual(seasons, [])\n", "def test_exact_outerref_lhs(self):\n    author = Author.objects.create(name='Test Author', alias='TestAlias')\n    article = Article.objects.create(headline='Test Article', pub_date=datetime(2022, 1, 1), author=author)\n    qs = Article.objects.filter(author__alias=OuterRef('author__alias'))\n    self.assertEqual(qs.get(author__alias=author.alias), article)\n", "def test_gt_lookup_with_float_value(self):\n    # IntegerField should allow floats for gt lookup and round them up\n    author = Author.objects.create(name='Author 10', alias='a10')\n    self.assertQuerysetEqual(\n        Author.objects.filter(id__gt=9.5),\n        ['<Author: Author 10>']\n    )\n"], "sample_549": ["def test_safe_first_element_with_datetime():\n    datetime_lst = [date.today() + timedelta(days=i) for i in range(10)]\n    actual = cbook.safe_first_element(datetime_lst)\n    assert actual is not None and actual == datetime_lst[0]\n", "def test_safe_first_element_with_non_finite():\n    arr = np.array([np.inf, 2, np.nan, 4])\n    ret = cbook._safe_first_finite(arr)\n    assert ret == 2\n", "def test_safe_first_element_with_nan():\n    float_lst = [np.nan, 1.0, 2.0]\n    actual = cbook._safe_first_finite(float_lst)\n    assert actual is not None and np.isfinite(actual) and actual == float_lst[1]\n", "def test_g_sig_digits():\n    g = cbook._g_sig_digits\n    assert g(0, 0) == 0\n    assert g(0, 1) == 1\n    assert g(1, 0) == 1\n    assert g(1, 1) == 1\n    assert g(12, 0) == 2\n    assert g(12, 1) == 2\n    assert g(12, 2) == 2\n    assert g(12, 3) == 2\n    assert g(12, 4) == 1\n    assert g(12, 5) == 1\n    assert g(123, 0) == 3\n    assert g(123, 1) == 3\n    assert g(123, 10) == 2\n    assert g(123, 11) == 2\n    assert g(123, 12) == 2\n    assert g(123, 13) == 2\n    assert g(123, 14) == 1\n    assert g(123, 15) == 1\n    assert g(1234, 0) == 4\n    assert g(1234, 10) == 3\n    assert g(1234, 100) == 2\n    assert g(1234, 1000) == 1\n", "def test_violin_stats():\n    # Create some sample data\n    X = [np.random.normal(0, 1, 100), np.random.normal(1, 1, 100), np.random.normal(2, 1, 100)]\n    quantiles = [0.25, 0.5, 0.75]\n\n    # Define a mock method for gaussian kernel density estimate\n        return np.exp(-0.5 * ((x[:, None] - coords) / 0.5) ** 2).mean(axis=0)\n\n    # Call the violin_stats function\n    vpstats = cbook.violin_stats(X, method, points=100, quantiles=quantiles)\n\n    # Check that the output is a list of dictionaries\n    assert isinstance(vpstats, list)\n    assert all(isinstance(d, dict) for d in vpstats)\n\n    # Check that each dictionary contains the expected keys\n    expected_keys = ['coords', 'vals', 'mean', 'median', 'min', 'max', 'quantiles']\n    assert all(all(key in d for key in expected_keys) for d in vpstats)\n\n    # Check that the length of each array in the dictionary is as expected\n    assert all(len(d['coords']) == 100 for d in vpstats)\n    assert all(len(d['vals']) == 100 for d in vpstats)\n    assert all(len(d['quantiles']) == len(quantiles) for d in vpstats)\n\n    # Check that the quantiles are as expected\n    for i, x in enumerate(X):\n        q = np.percentile(x, np.array(quantiles) * 100)\n        np.testing.assert_array_almost_equal(vpstats[i]['quantiles'], q)\n\n    # Check that the mean, median, min, and max are as expected\n    for i, x in enumerate(X):\n        np.testing.assert_almost_equal(vpstats["], "sample_107": ["def test_cleanse_setting_dict(self):\n    test_dict = {\n        'TEST': 'TEST',\n        'PASSWORD': 'super_secret',\n        'NESTED': {\n            'PASSWORD': 'nested_secret',\n            'TEST': 'nested_test'\n        }\n    }\n    cleansed = cleanse_setting('TEST', test_dict)\n    self.assertEqual(cleansed, {\n        'TEST': 'TEST',\n        'PASSWORD': CLEANSED_SUBSTITUTE,\n        'NESTED': {\n            'PASSWORD': CLEANSED_SUBSTITUTE,\n            'TEST': 'nested_test'\n        }\n    })\n", "def test_unicode_error(self):\n    try:\n        raise UnicodeError('unicode error', 'ascii', 0, 1, 'invalid char')\n    except Exception:\n        exc_type, exc_value, tb = sys.exc_info()\n    reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n    html = reporter.get_traceback_html()\n    self.assertIn('<h2>Unicode error hint</h2>', html)\n    self.assertIn('The string that could not be encoded/decoded was: ', html)\n    self.assertIn('<strong>invalid char</strong>', html)\n", "    def test_repr(self):\n        class WrappedCallable:\n                return \"repr with unicode \u00e3\u0081\u0093\u00e3\u0082\u0093\u00e3\u0081\u00ab\u00e3\u0081\u00a1\u00e3\u0081\u00af\"\n\n                pass\n\n        actual = repr(CallableSettingWrapper(WrappedCallable()))\n        self.assertEqual(actual, \"repr with unicode \u00e3\u0081\u0093\u00e3\u0082\u0093\u00e3\u0081\u00ab\u00e3\u0081\u00a1\u00e3\u0081\u00af\")\n", "    def test_callable_behavior(self):\n        \"\"\"\n        CallableSettingWrapper should wrap a callable object without changing its behavior.\n        \"\"\"\n            return x + y\n\n        wrapper = CallableSettingWrapper(wrapped_callable)\n        self.assertEqual(wrapper(3, 5), wrapped_callable(3, 5))\n", "def test_cleanse_setting_dict(self):\n    \"\"\"\n    cleanse_setting should cleanse sensitive information found in\n    dict settings.\n    \"\"\"\n    sensitive_settings = [\n        'SECRET_KEY',\n        'PASSWORD',\n        'API_KEY',\n        'AUTH_TOKEN',\n    ]\n    for setting in sensitive_settings:\n        TEST_DICT = {\n            setting: \"should not be displayed\",\n            'recursive': {setting: \"should not be displayed\"},\n        }\n        cleansed = cleanse_setting('TEST_DICT', TEST_DICT)\n        self.assertEqual(cleansed, {\n            'recursive': {setting: CLEANSED_SUBSTITUTE},\n            setting: CLEANSED_SUBSTITUTE,\n        })\n"], "sample_776": ["def test_lars_path_Xy_precomputed():\n    # Test that lars_path with precomputed Xy gives the right answer\n    Xy = np.dot(X.T, y)\n    for method in 'lar', 'lasso':\n        output = linear_model.lars_path(X, y, method=method)\n        output_pre = linear_model.lars_path(X, y, Xy=Xy, method=method)\n        for expected, got in zip(output, output_pre):\n            assert_array_almost_equal(expected, got)\n", "def test_lasso_lars_vs_R_implementation_with_positive():\n    # Test that sklearn LassoLars implementation agrees with the LassoLars\n    # implementation available in R (lars library) under the following\n    # scenarios:\n    # 1) fit_intercept=False, normalize=False, and positive=True\n    # 2) fit_intercept=True, normalize=True, and positive=True\n\n    # Let's generate the data used in the bug report 7778\n    y = np.array([-6.45006793, -3.51251449, -8.52445396, 6.12277822,\n                  -19.42109366])\n    x = np.array([[0.47299829, 0, 0, 0, 0],\n                  [0.08239882, 0.85784863, 0, 0, 0],\n                  [0.30114139, -0.07501577, 0.80895216, 0, 0],\n                  [-0.01460346, -0.1015233, 0.0407278, 0.80338378, 0],\n                  [-0.69363927, 0.06754067, 0.18064514, -0.0803561,\n                   0.40427291]])\n\n    X = x.T\n\n    ###########################################################################\n    # Scenario 1: Let's compare R vs sklearn when fit_intercept=False,\n    # normalize=False, and positive=True\n    ###########################################################################\n    #\n    # The R result was obtained using the following code:\n    #\n    # library", "def test_lasso_lars_with_custom_Gram_and_Xy():\n    # Test that LassoLars works correctly when custom Gram and Xy are provided\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n    Gram = np.array([[14, 20], [20, 34]])\n    Xy = np.array([30, 50])\n    lasso_lars = linear_model.LassoLars(alpha=0.1)\n    lasso_lars.fit(X, y, Xy=Xy, Gram=Gram)\n    assert np.allclose(lasso_lars.coef_, np.array([0.2, 0.4]))\n", "def test_lasso_lars_input_validation():\n    # Test that lars_path raises a ValueError when an invalid method is passed\n    X = diabetes.data\n    y = diabetes.target\n    assert_raises(ValueError, linear_model.lars_path, X, y, method='invalid')\n\n    # Test that LassoLarsIC raises a ValueError when an invalid criterion is passed\n    assert_raises(ValueError, LassoLarsIC, criterion='invalid')\n", "def test_lasso_lars_n_nonzero_coefs():\n    # Test that LassoLars and LassoLarsIC correctly handle the n_nonzero_coefs parameter\n    lars = linear_model.LassoLars(n_nonzero_coefs=5)\n    lars.fit(X, y)\n    assert_equal(len(np.nonzero(lars.coef_)[0]), 5)\n\n    lars_ic = linear_model.LassoLarsIC()\n    lars_ic.fit(X, y)\n    assert_less_equal(len(np.nonzero(lars_ic.coef_)[0]), X.shape[1])\n"], "sample_485": ["def test_urlize_autoescape(self):\n    value = \"<script>alert('Hello, world!');</script>\"\n    expected = '&lt;script&gt;alert(&#x27;Hello, world!&#x27;);&lt;/script&gt;'\n    self.assertEqual(urlize(value, autoescape=True), expected)\n", "def test_urlize_nofollow(self):\n    value = \"Check out www.example.com.\"\n    output = 'Check out <a href=\"http://www.example.com\" rel=\"nofollow\">www.example.com</a>.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_trim_url_limit(self):\n    value = \"Search for google.com/?q=django-is-great and see.\"\n    output = 'Search for <a href=\"http://google.com/?q=django-is-great\">google.co\u2026</a> and see.'\n    self.assertEqual(urlize(value, trim_url_limit=14), output)\n", "def test_urlize_nofollow(self):\n    value = \"Check out https://example.com and https://anotherexample.com.\"\n    output = (\n        'Check out <a href=\"https://example.com\" rel=\"nofollow\">'\n        'https://example.com</a> and <a href=\"https://anotherexample.com\" '\n        'rel=\"nofollow\">https://anotherexample.com</a>.'\n    )\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_autoescape(self):\n    value = \"Search for google.com/?q=<script> and see.\"\n    output = 'Search for <a href=\"http://google.com/?q=\">google.com/?q=&lt;script&gt;</a> and see.'\n    self.assertEqual(urlize(value, autoescape=True), output)\n"], "sample_1022": ["def test_custom_symbol_splitting():\n        if symbol not in ('list', 'of', 'unsplittable', 'names'):\n            return _token_splittable(symbol)\n        return False\n\n    transformation = split_symbols_custom(can_split)\n    assert parse_expr('unsplittable', transformations=standard_transformations +\n                      (transformation, implicit_multiplication)) == sympy.Symbol('unsplittable')\n", "def test_implicit_multiplication_application():\n    cases = {\n        '3xyz': '3*x*y*z',\n        '10sin**2 x**2 + 3xyz + tan theta': '3*x*y*z + 10*sin(x**2)**2 + tan(theta)'\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (implicit_multiplication_application,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal), (implicit, normal)\n\n    other_implicit = ['sin x', 'cos 2*x', 'sin cos x']\n    for case in other_implicit:\n        raises(SyntaxError,\n               lambda: parse_expr(case, transformations=transformations2))\n", "def test_split_symbols_custom():\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (split_symbols_custom(lambda s: True),)\n    cases = {\n        'unsplittable': 'unsplittable',\n        'names': 'names',\n        'splittable': 's*p*l*i*t*t*a*b*l*e',\n    }\n    for case in cases:\n        custom_split = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(custom_split == normal)\n", "def test_implicit_multiplication_application():\n    cases = {\n        '3sin**2 x**2 + 3xyz + tan theta': '3*sin(x**2)**2 + 3*x*y*z + tan(theta)',\n        '5x * 2y': '5*x*2*y',\n        'sin(x) * cos(x)': 'sin(x)*cos(x)',\n        'pi * (x + 2)': 'pi*(x + 2)',\n        '(x + 2) * pi': '(x + 2)*pi',\n        'pi * sin(x)': 'pi*sin(x)',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (implicit_multiplication_application,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n", "def test_split_symbols_custom():\n    custom_transformations = standard_transformations + (\n        split_symbols_custom(lambda symbol: symbol != 'custom'),\n        implicit_multiplication\n    )\n    assert parse_expr('custom', transformations=custom_transformations) == sympy.Symbol('custom')\n    assert parse_expr('uncustom', transformations=custom_transformations) == sympy.Symbol('u') * sympy.Symbol('n') * sympy.Symbol('c') * sympy.Symbol('u') * sympy.Symbol('s') * sympy.Symbol('t') * sympy.Symbol('o') * sympy.Symbol('m')\n"], "sample_20": ["def test_masked_columns_serialize_data(tmp_path):\n    filename = tmp_path / \"test_masked_columns_serialize_data.fits\"\n\n    t = simple_table(masked=True)  # int, float, and str cols with one masked element\n\n    # MaskedColumn but no masked elements.\n    t[\"d\"] = [1, 2, 3]\n\n    t.write(filename, serialize_method=\"data\")\n\n    t2 = Table.read(filename)\n    assert t2.masked is False\n    assert t2.colnames == t.colnames\n    for name in t2.colnames:\n        assert not hasattr(t2[name], \"mask\")\n        assert np.all(t2[name] == t[name])\n\n        # Data under the mask round-trips also (unmask data to show this).\n        t[name].mask = False\n        assert np.all(t2[name] == t[name])\n", "def test_fits_mixins_write_read_per_column(name_col, tmp_path):\n    \"\"\"Test write and read one column at a time individually\"\"\"\n    filename = tmp_path / \"test_simple.fits\"\n    name, col = name_col\n\n    t = Table([col], names=[name])\n    t[name].info.description = \"my \\n\\n\\n description\"\n    t[name].info.meta = {\"list\": list(range(50)), \"dict\": {\"a\": \"b\" * 200}}\n\n    t.write(filename, format=\"fits\")\n    t2 = Table.read(filename, format=\"fits\", astropy_native=True)\n\n    assert t.colnames == t2.colnames\n\n    for colname in t.colnames:\n        compare = compare_attrs[colname]\n        assert_objects_equal(t[colname], t2[colname], compare)\n", "def test_write_append_different_shape(tmp_path):\n    t1 = Table(self.data1)\n    t2 = Table(self.data2)\n    filename = tmp_path / \"test_write_append_different_shape.fits\"\n    t1.write(filename, append=True)\n    with pytest.raises(ValueError):\n        t2.write(filename, append=True)\n", "def test_fits_mixins_time_subclass_no_mixins(tmp_path):\n    \"\"\"Test write/read a subclass of Time with no mixins and validate intermediate column names\"\"\"\n    filename = tmp_path / \"test_simple.fits\"\n    class MyTime(Time):\n        pass\n    t = MyTime([1.0, 2.0], format=\"mjd\")\n    t.write(filename, format=\"fits\")\n    t2 = Table.read(filename, format=\"fits\", astropy_native=True)\n    assert isinstance(t2[\"col0\"], MyTime)\n    assert np.all(t2[\"col0\"] == t)\n    assert t2[\"col0\"].format == t.format\n    # Read directly via fits and confirm column names\n    with fits.open(filename) as hdus:\n        assert hdus[1].columns.names == [\"col0\"]\n", "def test_astropy_native_parameter(tmp_path):\n    filename = tmp_path / \"test_astropy_native.fits\"\n    t = Table([Time([1, 2], format='jd')], names=['time'])\n    t.write(filename, format='fits', astropy_native=True)\n\n    t2 = Table.read(filename, format='fits', astropy_native=True)\n    assert isinstance(t2['time'], Time)\n\n    t3 = Table.read(filename, format='fits', astropy_native=False)\n    assert not isinstance(t3['time'], Time)\n"], "sample_245": ["    def test_custom_extensions(self):\n        management.call_command('makemessages', locale=[LOCALE], extensions=['txt', 'md'], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('This is a markdown string.', po_contents)\n            self.assertMsgId('This is a text string.', po_contents)\n", "    def test_no_default_ignore_patterns(self):\n        out, po_contents = self._run_makemessages(use_default_ignore_patterns=False)\n        self.assertNotIn(\"ignoring file .ignore_file\", out)\n        self.assertNotIn(\"ignoring directory CVS\", out)\n        self.assertNotIn(\"ignoring file .hidden_file\", out)\n        self.assertNotIn(\"ignoring file file_with_extension.pyc\", out)\n        self.assertMsgId('This file should not be ignored.', po_contents)\n", "def test_makemessages_with_javascript_extension(self):\n    \"\"\"\n    makemessages correctly handles the --extension option with 'js' for the djangojs domain.\n    \"\"\"\n    management.call_command('makemessages', locale=[LOCALE], domain='djangojs', extensions=['js'], verbosity=0)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE, encoding='utf-8') as fp:\n        po_contents = fp.read()\n        self.assertIn('This literal should be included.', po_contents)\n        self.assertNotIn('This literal should not be included.', po_contents)\n", "def test_template_trans_tags(self):\n    \"\"\"\n    Test that {% trans %} and {% blocktrans %} tags are correctly extracted\n    \"\"\"\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertMsgId('Translate this text', po_contents)\n        self.assertMsgId('Translate this text with context', po_contents)\n        self.assertMsgId('Translate this pluralized text singular', po_contents)\n        self.assertMsgId('Translate this pluralized text plural', po_contents)\n        self.assertMsgId('Translate this block of text', po_contents)\n        self.assertMsgId('Translate this block of text with context', po_contents)\n        self.assertMsgId('Translate this block of pluralized text singular', po_contents)\n        self.assertMsgId('Translate this block of pluralized text plural', po_contents)\n", "    def test_multiple_domains(self):\n        management.call_command('makemessages', locale=[LOCALE], domain='djangojs', verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        self.assertTrue(os.path.exists(self.PO_FILE_JS))\n"], "sample_50": ["def test_no_user_host_port(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n        }), (\n            ['psql', 'dbname'],\n            None,\n        )\n    )\n", "def test_no_host_no_port(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n        }), (\n            ['psql', '-U', 'someuser', 'dbname'],\n            'somepassword',\n        )\n    )\n", "def test_no_host(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'port': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-p', '444', 'dbname'],\n            'somepassword',\n        )\n    )\n", "def test_no_user(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        }), (\n            ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n            'somepassword',\n        )\n    )\n", "def test_no_user(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        }), (\n            ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n            'somepassword',\n        )\n    )\n"], "sample_162": ["    def test_no_obsolete_disabled(self):\n        \"\"\"Behavior is correct if --no-obsolete switch isn't specified.\"\"\"\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=False)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertIn('#, fuzzy', po_contents)\n", "    def test_no_default_ignore_patterns(self):\n        out, _ = self._run_makemessages(no_default_ignore=True)\n        self.assertNotIn(\"ignoring directory CVS\", out)\n        self.assertNotIn(\"ignoring file .hidden\", out)\n        self.assertNotIn(\"ignoring file *~\", out)\n        self.assertNotIn(\"ignoring file *.pyc\", out)\n", "def test_default_ignore_patterns(self):\n    \"\"\"\n    Test that default ignore patterns are used if no custom ignore patterns are provided.\n    \"\"\"\n    out, po_contents = self._run_makemessages()\n    self.assertIn(\"ignoring file code.pyc\", out)\n    self.assertIn(\"ignoring directory CVS\", out)\n    self.assertIn(\"ignoring file xxx_~\", out)\n    self.assertIn(\"ignoring file .hidden\", out)\n", "def test_no_obsolete_enabled(self):\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        # Check that obsolete strings are not present in the file\n        self.assertNotIn('#~', po_contents)\n", "    def test_settings_available(self):\n        cmd = MakeMessagesCommand()\n        # Assume settings are not available by default\n        self.assertFalse(cmd.settings_available)\n\n        # Override settings to make them available\n        with override_settings(LOCALE_PATHS=['/path/to/locale']):\n            cmd = MakeMessagesCommand()\n            self.assertTrue(cmd.settings_available)\n\n        # Ensure the command still works without settings\n        cmd = MakeMessagesCommand()\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n"], "sample_1147": ["def test_latex_as_name():\n    from sympy.printing.latex import latex as lx\n    assert lx(x) == r\"x\"\n", "def test_latex_Derivative():\n    f = Function('f')\n    assert latex(Derivative(f(x), x)) == r\"\\frac{d}{d x} f{\\left(x \\right)}\"\n    assert latex(Derivative(f(x), (x, 2))) == r\"\\frac{d^{2}}{d x^{2}} f{\\left(x \\right)}\"\n    assert latex(Derivative(f(x, y), x, y)) == r\"\\frac{\\partial^{2}}{\\partial x\\partial y} f{\\left(x, y \\right)}\"\n    assert latex(Derivative(f(x, y), x, y, x)) == r\"\\frac{\\partial^{3}}{\\partial x^{2}\\partial y} f{\\left(x, y \\right)}\"\n", "def test_latex_sqrt():\n    assert latex(sqrt(2)) == r'\\sqrt{2}'\n    assert latex(sqrt(3*x)) == r'\\sqrt{3 x}'\n    assert latex(sqrt(x**2)) == r'x'\n    assert latex(sqrt(x)**2) == r'\\left(\\sqrt{x}\\right)^{2}'\n", "def test_complex_latex():\n    # Testing complex numbers\n    z = 1 + 2j\n    assert latex(z) == r\"1 + 2 i\"\n    assert latex(z.conjugate()) == r\"1 - 2 i\"\n", "def test_latex_ODE_system():\n    x, y = symbols('x y')\n    eq1 = Eq(x.diff(t), -x + y)\n    eq2 = Eq(y.diff(t), x - y)\n    system = [eq1, eq2]\n    assert latex(system) == r'\\left[ \\frac{d}{d t} x = y - x, \\  \\frac{d}{d t} y = x - y\\right]'\n"], "sample_734": ["def test_fowlkes_mallows_score_symmetry():\n    # Test symmetry of the Fowlkes-Mallows score\n    labels_a = np.array([1, 1, 1, 2, 2, 2])\n    labels_b = np.array([1, 1, 2, 2, 3, 3])\n    score_ab = fowlkes_mallows_score(labels_a, labels_b)\n    score_ba = fowlkes_mallows_score(labels_b, labels_a)\n    assert_almost_equal(score_ab, score_ba)\n", "def test_symmetry_of_mutual_information():\n    # Check symmetry of mutual_info_score function\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    mi_ab = mutual_info_score(labels_a, labels_b)\n    mi_ba = mutual_info_score(labels_b, labels_a)\n    assert_almost_equal(mi_ab, mi_ba, 5)\n", "def test_mutual_info_score_with_contingency():\n    # Test mutual_info_score with precomputed contingency matrix\n    labels_true = np.array([0, 0, 1, 1])\n    labels_pred = np.array([0, 1, 0, 1])\n    contingency = np.array([[2, 0], [1, 1]])\n    mi_score = mutual_info_score(labels_true, labels_pred, contingency=contingency)\n    assert_almost_equal(mi_score, 0.8525161, decimal=5)\n", "def test_fowlkes_mallows_score_symmetry():\n    # Test symmetry of Fowlkes-Mallows score\n    score_1 = fowlkes_mallows_score([0, 0, 0, 1, 1, 1],\n                                    [0, 0, 1, 1, 2, 2])\n    score_2 = fowlkes_mallows_score([0, 0, 1, 1, 2, 2],\n                                    [0, 0, 0, 1, 1, 1])\n    assert_almost_equal(score_1, score_2)\n", "def test_mutual_info_score_with_provided_contingency():\n    # Test mutual_info_score with a provided contingency matrix\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    C = contingency_matrix(labels_a, labels_b)\n    mi = mutual_info_score(labels_a, labels_b, contingency=C)\n    assert_almost_equal(mi, 0.41022, 5)\n"], "sample_172": ["def test_multiple_selection(self):\n    \"\"\"\n    Multiple options can be selected in the popup window.\n    \"\"\"\n    self.admin_login(username='super', password='secret', login_url='/')\n    self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n    main_window = self.selenium.current_window_handle\n\n    # Open the popup window and select multiple bands\n    self.selenium.find_element_by_id('lookup_id_supporting_bands').click()\n    self.wait_for_and_switch_to_popup()\n    self.selenium.find_element_by_xpath(\"//input[@value='42']\").click()\n    self.selenium.find_element_by_xpath(\"//input[@value='98']\").click()\n    self.selenium.find_element_by_id('done_selecting').click()\n\n    # The field now contains the selected bands' ids\n    self.selenium.switch_to.window(main_window)\n    self.wait_for_value('#id_supporting_bands', '42,98')\n", "def test_ManyToManyRawIdWidget_render_with_queryset(self):\n    band = Band.objects.create(name='Linkin Park')\n\n    m1 = Member.objects.create(name='Chester')\n    m2 = Member.objects.create(name='Mike')\n    band.members.add(m1, m2)\n    rel = Band._meta.get_field('members').remote_field\n    w = widgets.ManyToManyRawIdWidget(rel, widget_admin_site)\n\n    # Test rendering with a queryset\n    queryset = Member.objects.filter(name='Chester')\n    output = w.render('test', [m1.pk], attrs={}, queryset=queryset)\n    self.assertHTMLEqual(\n        output,\n        '<input type=\"text\" name=\"test\" value=\"%(m1pk)s\" class=\"vManyToManyRawIdAdminField\">'\n        '<a href=\"/admin_widgets/member/?name=Chester\" class=\"related-lookup\" id=\"lookup_id_test\" title=\"Lookup\"></a>'\n        % {'m1pk': m1.pk}\n    )\n", "def test_ForeignKey_invalid_id(self):\n    self.admin_login(username='super', password='secret', login_url='/')\n    self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n    # Test with an invalid ID\n    invalid_id = 'invalid'\n    self.selenium.find_element_by_id('id_main_band').send_keys(invalid_id)\n\n    # Submit the form and check for error message\n    self.selenium.find_element_by_name('_save').click()\n    error_message = self.selenium.find_element_by_css_selector('.errornote li').text\n    self.assertEqual(error_message, 'Please enter a valid number.')\n\n    # Test with a non-existent ID\n    non_existent_id = '999'\n    self.selenium.find_element_by_id('id_main_band').clear()\n    self.selenium.find_element_by_id('id_main_band').send_keys(non_existent_id)\n\n    # Submit the form and check for error message\n    self.selenium.find_element_by_name('_save').click()\n    error_message = self.selenium.find_element_by_css_selector('.errornote li').text\n    self.assertEqual(error_message, 'Select a valid choice. That choice is not one of the available choices.')\n", "def test_many_to_many_help_text(self):\n    # Help text for the field is displayed when no help text is defined\n    self.assertEqual(\n        self.selenium.find_element_by_css_selector('.field-bands div.help').text,\n        'Hold down \u201cControl\u201d, or \u201cCommand\u201d on a Mac, to select more than one.'\n    )\n", "    def test_to_field(self):\n        band = Band.objects.create(name='Linkin Park')\n        band.album_set.create(\n            name='Hybrid Theory', cover_art=r'albums\\hybrid_theory.jpg'\n        )\n        rel = Album._meta.get_field('band').remote_field\n\n        w = widgets.ForeignKeyRawIdWidget(rel, widget_admin_site)\n        widget_html = w.render('test', band.pk, attrs={'data-admin-rel-field': 'name'})\n        self.assertIn('data-admin-rel-field=\"name\"', widget_html)\n        self.assertIn('?_to_field=name\">Linkin Park</a>', widget_html)\n"], "sample_767": ["def test_column_transformer_empty_input():\n    # test case that ensures the ColumnTransformer works correctly when no transformers are provided\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_both = X_array\n\n    ct = ColumnTransformer([])\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 0\n", "def test_column_transformer_empty_input():\n    X_array = np.array([[]])\n    ct = ColumnTransformer([('trans1', Trans(), [])])\n    assert_array_equal(ct.fit_transform(X_array).shape, (0, 0))\n    assert_array_equal(ct.fit(X_array).transform(X_array).shape, (0, 0))\n    assert len(ct.transformers_) == 1\n", "def test_column_transformer_remainder_transformer_errors():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n\n    # Invalid remainder transformer\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder='invalid')\n    assert_raise_message(\n        ValueError,\n        \"remainder keyword needs to be one of 'drop', 'passthrough', \"\n        \"or estimator.\", ct.fit, X_array)\n    assert_raise_message(\n        ValueError,\n        \"remainder keyword needs to be one of 'drop', 'passthrough', \"\n        \"or estimator.\", ct.fit_transform, X_array)\n\n    # Remainder transformer does not implement fit\n    class NoFitTrans(BaseEstimator):\n            return X\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder=NoFitTrans())\n    assert_raise_message(\n        TypeError,\n        \"All estimators should implement fit and transform, or can be 'drop' \"\n        \"or 'passthrough' specifiers.\", ct.fit, X_array)\n    assert_raise_message(\n        TypeError,\n        \"All estimators should implement fit and transform, or can be 'drop' \"\n        \"or 'passthrough' specifiers.\", ct.fit_transform, X_array)\n\n    # Remainder transformer does not implement transform\n    class NoTransformTrans(BaseEstimator):\n            return self\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder=NoTransformTrans())\n    assert_raise_message(\n        TypeError,\n        \"All estimators should implement fit and transform, or can be 'drop' \"\n        \"or 'passthrough' specifiers.\", ct.fit, X_array)\n    assert_raise_message(\n        TypeError,\n        \"All estimators should implement fit", "def test_column_transformer_empty_transformer_list():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([])\n    assert_raises(ValueError, ct.fit, X_array)\n    assert_raises(ValueError, ct.fit_transform, X_array)\n", "def test_column_transformer_with_numeric_columns():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]], dtype=float).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])], remainder='drop')\n    expected_result = np.array([[-1], [1]])\n    assert_array_equal(ct.fit_transform(X_array), expected_result)\n"], "sample_1044": ["def test_issue_10303():\n    x = Symbol('x', integer=True, positive=True)\n    y = Symbol('y', integer=True, positive=True)\n    assert (x**y).is_prime is None\n    assert (x**(y+1)).is_prime is False\n    assert (x**(y+1)).is_composite is None\n    assert ((x+1)**(y+1)).is_composite is True\n", "def test_Pow_is_zero():\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    z = Symbol('z', zero=True)\n\n    assert (z**p).is_zero is True\n    assert (z**n).is_zero is False\n    assert (z**z).is_zero is True\n\n    assert (p**z).is_zero is False\n    assert (n**z).is_zero is False\n", "def test_issue_10506():\n    x = Dummy('x', real=True, nonnegative=True)\n    y = Dummy('y', real=True, nonnegative=True)\n    assert (x + y).is_nonnegative is True\n", "def test_issue_10572():\n    n = Symbol('n', integer=True, positive=True)\n    assert (n**0).is_positive is True\n    assert (n**0).is_nonpositive is False\n", "def test_issue_10620():\n    x = Dummy('x')\n    y = Dummy('y')\n    assert (x*y**-1).is_positive is None  # x and y could be negative\n"], "sample_620": ["def test_concat_indexes() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [3, 4])})\n\n    expected = Dataset(coords={\"x\": (\"x\", [1, 2, 3, 4])})\n    actual = concat([ds1, ds2], dim=\"x\")\n\n    assert_identical(actual, expected)\n    assert isinstance(actual.indexes[\"x\"], PandasIndex)\n", "def test_concat_dim_as_pandas_index() -> None:\n    objs = [Dataset({\"x\": 0}), Dataset({\"x\": 1})]\n    index = pd.Index([3, 4], name=\"y\")\n    expected = Dataset({\"x\": (\"y\", [0, 1])}, {\"y\": index})\n    actual = concat(objs, index)\n    assert_identical(actual, expected)\n    assert isinstance(actual.y, pd.Index)\n", "def test_concat_different_coords():\n    ds1 = Dataset({\"a\": (\"x\", [1, 2])}, {\"x\": [0, 1], \"y\": 0})\n    ds2 = Dataset({\"a\": (\"x\", [3, 4])}, {\"x\": [2, 3], \"y\": 1})\n    expected = Dataset({\"a\": (\"x\", [1, 2, 3, 4])}, {\"x\": [0, 1, 2, 3], \"y\": (\"x\", [0, 0, 1, 1])})\n    actual = concat([ds1, ds2], dim=\"x\")\n    assert_identical(actual, expected)\n", "def test_concat_index_on_dim():\n    # Test concatenation when the index is on the dimension\n    da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3]})\n    da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6]})\n    expected = DataArray([1, 2, 3, 4, 5, 6], dims=\"x\", coords={\"x\": [1, 2, 3, 4, 5, 6]})\n    actual = concat([da1, da2], dim=\"x\")\n    assert_identical(actual, expected)\n", "def test_concat_single_dataset() -> None:\n    data = create_test_data()\n    actual = concat([data], \"dim1\")\n    assert_identical(data, actual)\n"], "sample_785": ["def test_build_repr_with_class_arguments():\n    splitter = MockSplitter(1, 2, c=3)\n    expected_repr = \"MockSplitter(a=1, b=2, c=3)\"\n    assert _build_repr(splitter) == expected_repr\n", "def test_stratified_shuffle_split_with_none_y():\n    # Check that StratifiedShuffleSplit doesn't require y\n    X = np.arange(10)\n    sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n    train, test = next(sss.split(X))\n    assert_equal(len(train), 8)\n    assert_equal(len(test), 2)\n", "def test_validate_shuffle_split_init():\n    # Test valid inputs\n    _validate_shuffle_split_init(test_size=0.5, train_size=0.5)\n    _validate_shuffle_split_init(test_size=0.5, train_size=None)\n    _validate_shuffle_split_init(test_size=None, train_size=0.5)\n    _validate_shuffle_split_init(test_size=5, train_size=None)\n    _validate_shuffle_split_init(test_size=None, train_size=5)\n    _validate_shuffle_split_init(test_size=5, train_size=5)\n\n    # Test invalid inputs\n    with pytest.raises(ValueError):\n        _validate_shuffle_split_init(test_size=-0.5, train_size=0.5)\n    with pytest.raises(ValueError):\n        _validate_shuffle_split_init(test_size=0.5, train_size=-0.5)\n    with pytest.raises(ValueError):\n        _validate_shuffle_split_init(test_size=\"invalid\", train_size=0.5)\n    with pytest.raises(ValueError):\n        _validate_shuffle_split_init(test_size=0.5, train_size=\"invalid\")\n    with pytest.raises(ValueError):\n        _validate_shuffle_split_init(test_size=1.5, train_size=0.5)\n    with pytest.raises(ValueError):\n        _validate_shuffle_split_init(test_size=0.5, train_size=1.5)\n    with pytest.raises(ValueError):\n        _validate_shuffle_split_init(test_size=0.5, train_size=0.6)\n", "def test_build_repr_with_keyword_args():\n    splitter = MockSplitter(a=1, b=2, c='three')\n    assert _build_repr(splitter) == \"MockSplitter(a=1, b=2, c='three')\"\n", "def test_kfold_no_shuffle_with_deprecation_warning():\n    # Test that a DeprecationWarning is raised when using the deprecated shuffle=False argument\n    with pytest.warns(DeprecationWarning, match=\"The default value of shuffle will change from False to True\"):\n        kf = KFold(n_splits=3, shuffle=False)\n        kf.split(X2[:-1])\n\n    with pytest.warns(DeprecationWarning, match=\"The default value of shuffle will change from False to True\"):\n        kf = KFold(n_splits=3)\n        kf.split(X2[:-1])\n"], "sample_213": ["    def test_save_form_data_none(self):\n        obj = Storage()\n        field = Storage._meta.get_field('normal')\n        field.save_form_data(obj, None)\n        self.assertEqual(obj.normal.name, '')\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.storage = FileSystemStorage(location=self.temp_dir)\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.storage = FileSystemStorage(location=self.temp_dir, base_url='/test_media_url/')\n", "def test_file_read_write_mode(self):\n    \"\"\"\n    Test that files opened in read mode can't be written to.\n    \"\"\"\n    obj = Storage()\n    obj.normal.save(\"read_only.txt\", ContentFile(\"read-only content\"))\n    obj.normal.close()\n\n    with self.assertRaises(io.UnsupportedOperation):\n        with obj.normal.open('wb') as normal:\n            normal.write(b'updated')\n", "    def test_file_url_encoding(self):\n        \"\"\"\n        File storage returns a url to access a given file from the Web with\n        correct encoding.\n        \"\"\"\n        filename = 'r\u00e9sum\u00e9.txt'\n        self.storage.save(filename, ContentFile('custom content'))\n        self.assertEqual(self.storage.url(filename), '/test_media_url/r%C3%A9sum%C3%A9.txt')\n        self.storage.delete(filename)\n\n        filename = 'r\u00e9sum\u00e9.txt'\n        self.storage.save(filename, ContentFile('custom content'))\n        with self.storage.open(filename) as f:\n            self.assertEqual(f.read(), b'custom content')\n        self.storage.delete(filename)\n"], "sample_345": ["def test_snapshot_files_deduplicates(self):\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.existing_file]):\n        files = list(self.reloader.snapshot_files())\n        self.assertEqual(len(files), 1)\n", "    def test_watched_files_contains_extra_files(self):\n        reloader = autoreload.StatReloader()\n        extra_file = self.temporary_file('extra_file.py')\n        extra_file.touch()\n        reloader.extra_files.add(extra_file)\n        watched_files = list(reloader.watched_files())\n        self.assertIn(extra_file, watched_files)\n", "    def test_tick_triggers_notify_file_changed(self):\n        mock_notify_file_changed = mock.MagicMock()\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            with mock.patch.object(self.reloader, 'notify_file_changed', mock_notify_file_changed):\n                ticker = self.reloader.tick()\n                next(ticker)\n                self.increment_mtime(self.existing_file)\n                next(ticker)\n                mock_notify_file_changed.assert_called_once_with(self.existing_file)\n", "    def test_watched_files_includes_extra_files(self):\n        reloader = autoreload.StatReloader()\n        extra_file = self.temporary_file('extra.py')\n        extra_file.touch()\n        reloader.extra_files.add(extra_file)\n        self.assertIn(extra_file, reloader.watched_files())\n", "def test_glob_directory_creation(self, mocked_modules, notify_mock):\n    new_dir = self.tempdir / 'new_directory'\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        new_dir.mkdir()\n        new_file = self.ensure_file(new_dir / 'new_file.py')\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [new_file])\n"], "sample_219": ["def test_expression_wrapper_with_outer_ref(self):\n    inner = Employee.objects.filter(\n        lastname__startswith=Left(OuterRef(OuterRef('lastname')), 1),\n    )\n    qs = Employee.objects.annotate(\n        ceo_company=Subquery(\n            Company.objects.filter(\n                point_of_contact__in=inner,\n                ceo__pk=OuterRef('pk'),\n            ).values('name'),\n        ),\n    ).filter(ceo_company__isnull=False)\n    self.assertEqual(qs.get().ceo_company, 'Test GmbH')\n", "def test_expression_wrapper_with_none(self):\n    wrapper = ExpressionWrapper(Value(None), output_field=IntegerField())\n    self.assertIsNone(wrapper.resolve_expression(query=None, allow_joins=True, reuse=None, summarize=False, for_save=False).value)\n", "def test_expression_wrapper_with_expression(self):\n    expr = ExpressionWrapper(F('integer') * 2, output_field=IntegerField())\n    self.assertEqual(expr.get_group_by_cols(alias=None), [expr])\n", "def test_expression_wrapper_in_filter(self):\n    wrapper = ExpressionWrapper(Value(1), output_field=IntegerField())\n    qs = Employee.objects.filter(salary__gt=wrapper)\n    self.assertNotEqual(qs.query.where.children[0].rhs, wrapper)\n    self.assertEqual(qs.query.where.children[0].rhs, Value(1))\n", "def test_expression_wrapper_with_outerref(self):\n    inner = Employee.objects.annotate(\n        outer_salary=OuterRef('salary'),\n    ).filter(salary=F('outer_salary'))\n    qs = Employee.objects.annotate(\n        same_salary_employees=Subquery(\n            inner.values('id'),\n        ),\n    ).filter(same_salary_employees__isnull=False)\n    self.assertEqual(qs.count(), 0)\n"], "sample_1185": ["def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([x], x) == x\n    assert compogen([1], x) == 1\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n", "def test_decompogen_exp():\n    assert decompogen(exp(sin(x)), x) == [exp(sin(x)), sin(x)]\n    assert decompogen(exp(2*x), x) == [exp(2*x)]\n    assert decompogen(exp(x + y), x) == [exp(x + y), x]\n    assert decompogen(exp(x**2), x) == [exp(x**2), x]\n    assert decompogen(exp(sqrt(x)), x) == [exp(sqrt(x)), sqrt(x)]\n"], "sample_1189": ["def test_lambdify_with_default_arguments():\n        return x + y\n\n    f_lambdified = lambdify((x, y), f(x, y))\n\n    assert f_lambdified(3) == 5  # using default value for y\n    assert f_lambdified(3, 4) == 7  # overriding default value for y\n", "def test_lambdify_numpy_printer():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    funcs = [numpy.sin, numpy.cos, numpy.tan, numpy.arcsin, numpy.arccos, numpy.arctan]\n    for func in funcs:\n        expr = func(x)\n        f = lambdify(x, expr, printer=NumPyPrinter)\n        result = f(0.5)\n        reference = expr.subs(x, 0.5).evalf()\n        assert numpy.allclose(result, float(reference))\n", "def test_issue_23224():\n    # test with empty list\n    f = lambdify([], (1,))\n    assert f() == (1,)\n", "def test_lambdify_with_custom_module():\n    # Test lambdify with a custom module\n    my_module = {\"sqrt\": lambda x: x**0.5, \"cos\": lambda x: math.cos(x)}\n    expr = sqrt(1 - cos(x)**2)\n    f = lambdify(x, expr, modules=my_module)\n    assert abs(f(0.5) - 0.8660254037844386) < 1e-15\n", "def test_lambdify_with_numerical_implemented_functions():\n    f = implemented_function('f', lambda x: x + 1)\n    g = implemented_function('g', lambda x: x * 2)\n    expr = f(g(x))\n    lambdified_expr = lambdify(x, expr, modules='numpy')\n    assert lambdified_expr(3) == 8\n"], "sample_379": ["def test_safe_string_concatenation(self):\n    s1 = SafeString('a&')\n    s2 = SafeString('b')\n    s = s1 + s2\n    self.assertIsInstance(s, SafeString)\n    self.assertEqual(s, 'a&b')\n    self.assertRenderEqual('{{ s }}', 'a&b', s=s)\n", "def test_safe_string_inheritance(self):\n    \"\"\"\n    SafeString is a subclass of str and SafeData.\n    \"\"\"\n    s = mark_safe('a&b')\n    self.assertIsInstance(s, str)\n    self.assertIsInstance(s, SafeData)\n", "def test_safe_string_interpolation(self):\n    \"\"\"\n    Interpolating a safe string with another safe string is safe.\n    \"\"\"\n    s1 = mark_safe('a')\n    s2 = mark_safe('b')\n    s = f'{s1}&{s2}'\n    self.assertRenderEqual('{{ s }}', 'a&b', s=s)\n", "def test_mark_safe_decorator_with_args(self):\n    \"\"\"\n    mark_safe used as a decorator with arguments leaves the result of a function\n    unchanged.\n    \"\"\"\n        return f'<html><body>{arg}</body></html>'\n\n    self.assertEqual(mark_safe(clean_string_provider)('safe'), clean_string_provider('safe'))\n", "def test_safe_string_subclass_behaviour(self):\n    class MySafeString(SafeString):\n        pass\n\n    s = MySafeString('a&b')\n\n    self.assertRenderEqual('{{ s }}', 'a&b', s=s)\n    self.assertRenderEqual('{{ s|force_escape }}', 'a&amp;b', s=s)\n"], "sample_167": ["def test_naturaltime_i18n(self):\n    time_format = '%d %b %Y %H:%M:%S'\n    documented_now = datetime.datetime.strptime('17 Feb 2007 16:30:00', time_format)\n\n    test_data = (\n        ('17 Feb 2007 16:30:00', '\u00e0 pr\u00e9sent'),\n        ('17 Feb 2007 16:29:31', 'il y a 29\u00a0secondes'),\n        ('17 Feb 2007 16:29:00', 'il y a 1\u00a0minute'),\n        ('17 Feb 2007 16:25:35', 'il y a 4\u00a0minutes'),\n        ('17 Feb 2007 15:30:29', 'il y a 59\u00a0minutes'),\n        ('17 Feb 2007 15:30:01', 'il y a 59\u00a0minutes'),\n        ('17 Feb 2007 15:30:00', 'il y a 1\u00a0heure'),\n        ('17 Feb 2007 13:31:29', 'il y a 2\u00a0heures'),\n        ('16 Feb 2007 13:31:29', 'il y a 1\u00a0jour, 2\u00a0heures'),\n        ('16 Feb 2007 13:30:01', 'il y a 1\u00a0jour, 2\u00a0heures'),\n        ('16 Feb 2007 13:30:00', 'il y a 1\u00a0jour, 3\u00a0heures'),\n        ('17 Feb 2007 16:30:30', 'dans 30\u00a0secondes'),\n        ('17 Feb 2007 16:30:29', 'dans 29\u00a0secondes'),\n        ('17 Feb 2007 16:31:0", "    def test_intword_large_numbers(self):\n        test_list = (\n            '1000000000000000000000000',\n            '1234567890123456789012345',\n            '9999999999999999999999999',\n            '10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "def test_naturaltime_with_i18n(self):\n    \"\"\"\n    Verify that humanize.naturaltime uses i18n with USE_L10N.\n    \"\"\"\n    time_format = '%d %b %Y %H:%M:%S'\n    documented_now = datetime.datetime.strptime('17 Feb 2007 16:30:00', time_format)\n\n    test_data = (\n        ('17 Feb 2007 16:30:00', 'now'),\n        ('17 Feb 2007 16:29:31', '31\\xa0seconds ago'),\n        ('17 Feb 2007 16:29:00', 'a minute ago'),\n        ('17 Feb 2007 16:25:35', '4\\xa0minutes ago'),\n        ('17 Feb 2007 15:30:29', '59\\xa0minutes ago'),\n        ('17 Feb 2007 15:30:01', '59\\xa0minutes ago'),\n        ('17 Feb 2007 15:30:00', 'an hour ago'),\n        ('17 Feb 2007 13:31:29', '2\\xa0hours ago'),\n        ('16 Feb 2007 13:31:29', '1\\xa0day, 2\\xa0hours ago'),\n        ('16 Feb 2007 13:30:01', '1\\xa0day, 2\\xa0hours ago'),\n        ('16 Feb 2007 13:30:00', '1\\xa0day, 3\\xa0hours ago'),\n        ('17 Feb 2007 16:30:30', '30\\xa0seconds from now'),\n        ('17 Feb 2007 16:30:29', '29\\xa0seconds from now'),\n       ", "def test_naturaltime_i18n(self):\n    # Verify that naturaltime uses the correct strings for i18n.\n    test_data = (\n        ('17 Feb 2007 16:30:00', 'now'),\n        ('17 Feb 2007 16:29:31', '31 segundos atr\u00e1s'),\n        ('17 Feb 2007 16:29:00', 'un minuto atr\u00e1s'),\n        ('17 Feb 2007 16:25:35', '4 minutos atr\u00e1s'),\n        ('17 Feb 2007 15:30:29', '59 minutos atr\u00e1s'),\n        ('17 Feb 2007 15:30:01', '59 minutos atr\u00e1s'),\n        ('17 Feb 2007 15:30:00', 'una hora atr\u00e1s'),\n        ('17 Feb 2007 13:31:29', '2 horas atr\u00e1s'),\n        ('16 Feb 2007 13:31:29', '1 d\u00eda, 2 horas atr\u00e1s'),\n        ('16 Feb 2007 13:30:01', '1 d\u00eda, 2 horas atr\u00e1s'),\n        ('16 Feb 2007 13:30:00', '1 d\u00eda, 3 horas atr\u00e1s'),\n        ('17 Feb 2007 16:30:30', 'dentro de 30 segundos'),\n        ('17 Feb 2007 16:30:29', 'dentro de 29 segundos'),\n        ('17 Feb 2007 16:31:00', 'dentro de un minuto'),\n        ('17 Feb 2007 16:34:35', 'dentro de 4 minutos'),\n        ('17 Feb 2007 17:30:29', 'dentro", "def test_naturaltime_with_future_days(self):\n    \"\"\"\n    Test naturaltime with dates in the future beyond 1 day.\n    \"\"\"\n    test_list = [\n        now + datetime.timedelta(days=10),\n        now + datetime.timedelta(days=30),\n        now + datetime.timedelta(days=60),\n    ]\n    result_list = [\n        '10\\xa0days from now',\n        '1\\xa0month from now',\n        '2\\xa0months from now',\n    ]\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n"], "sample_421": ["def test_when_constructor_with_lookups(self):\n    condition = When(integer=1, string=\"2\")\n    self.assertEqual(str(condition.condition), \"AND: ('integer', 1), ('string', '2')\")\n", "def test_lookup_different_conditions(self):\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            test=Case(\n                When(integer=2, integer2=3, then=Value(\"when\")),\n                When(integer=2, integer2=2, then=Value(\"different condition\")),\n                default=Value(\"default\"),\n            ),\n        ).order_by(\"pk\"),\n        [\n            (1, 1, \"default\"),\n            (2, 3, \"when\"),\n            (3, 4, \"default\"),\n            (2, 2, \"different condition\"),\n            (3, 4, \"default\"),\n            (3, 3, \"default\"),\n            (4, 5, \"default\"),\n        ],\n        transform=attrgetter(\"integer\", \"integer2\", \"test\"),\n    )\n", "def test_combined_expression_with_null(self):\n    CaseTestModel.objects.create(integer=None, integer2=1, string=\"1\")\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            test=Case(\n                When(integer=1, then=2),\n                default=Value(0),\n                output_field=IntegerField(),\n            )\n            + 1,\n        ).order_by(\"pk\"),\n        [(1, 2), (2, 3), (3, 4), (2, 3), (3, 4), (3, 4), (4, 4), (None, 1)],\n        transform=attrgetter(\"integer\", \"test\"),\n    )\n", "def test_case_with_various_values(self):\n    obj = CaseTestModel.objects.create(integer=5, decimal=Decimal(\"3.14\"), float=3.14, string=\"test\")\n    qs = CaseTestModel.objects.annotate(\n        value=Case(\n            When(integer=5, then=Value(10)),\n            When(decimal=Decimal(\"3.14\"), then=Value(20)),\n            When(float=3.14, then=Value(30)),\n            When(string=\"test\", then=Value(40)),\n            default=Value(0),\n        ),\n    )\n    self.assertEqual(qs.get(pk=obj.pk).value, 10)\n\n    obj.integer = 10\n    obj.save()\n    self.assertEqual(qs.get(pk=obj.pk).value, 20)\n\n    obj.decimal = Decimal(\"2.71\")\n    obj.save()\n    self.assertEqual(qs.get(pk=obj.pk).value, 30)\n\n    obj.float = 2.71\n    obj.save()\n    self.assertEqual(qs.get(pk=obj.pk).value, 40)\n\n    obj.string = \"example\"\n    obj.save()\n    self.assertEqual(qs.get(pk=obj.pk).value, 0)\n", "def test_when_with_q_object_and_lookups(self):\n    qs = CaseTestModel.objects.annotate(\n        test=Case(\n            When(Q(integer=1), string=\"1\", then=Value(True)),\n            When(Q(integer=1), string=\"2\", then=Value(False)),\n            default=Value(None),\n            output_field=BooleanField(),\n        ),\n    )\n    self.assertIs(qs.get(integer=1, string=\"1\").test, True)\n    self.assertIs(qs.get(integer=1, string=\"2\").test, False)\n"], "sample_849": ["def test_leave_p_out_empty_trainset():\n    cv = LeavePOut(p=2)\n    X, y = [[1]], [0]  # 1 sample\n    with pytest.raises(\n            ValueError,\n            match='Cannot perform LeavePOut with p=2 and n_samples=1'):\n        next(cv.split(X, y))\n", "def test_shuffle_split_single_group():\n    # Test ShuffleSplit with a single group\n    cv = ShuffleSplit(test_size=0.5)\n    X, y = [[1], [1]], [0, 0]  # 2 samples, same group\n    train, test = next(cv.split(X, y, groups=[1, 1]))\n    assert len(train) == 1\n    assert len(test) == 1\n", "def test_shuffle_split_empty_testset():\n    cv = ShuffleSplit(test_size=0)\n    X, y = [[1], [2]]  # 2 samples\n    with pytest.raises(\n            ValueError,\n            match='With n_samples=2, test_size=0 and train_size=None, '\n            'the resulting test set will be empty'):\n        next(cv.split(X, y, groups=[1, 2]))\n", "def test_shuffle_split_empty_testset():\n    cv = ShuffleSplit(train_size=.99)\n    X, y = [[1], [2], [3]]  # 3 samples\n    train, test = next(cv.split(X, y))\n    assert len(train) == 3\n    assert len(test) == 0\n\n    # Test the repr\n    assert repr(cv) == (\"ShuffleSplit(n_splits=10, random_state=None, \"\n                        \"test_size=None, train_size=0.99)\")\n", "def test_group_kfold_shuffle(shuffle):\n    # Test if GroupKFold can shuffle the groups correctly\n    rng = np.random.RandomState(0)\n\n    # Parameters of the test\n    n_groups = 15\n    n_samples = 1000\n    n_splits = 5\n\n    X = y = np.ones(n_samples)\n\n    # Construct the test data\n    groups = rng.randint(0, n_groups, n_samples)\n\n    lkf = GroupKFold(n_splits=n_splits, shuffle=shuffle, random_state=0)\n    folds = np.zeros(n_samples)\n    for i, (_, test) in enumerate(lkf.split(X, y, groups)):\n        folds[test] = i\n\n    # Check that the groups are shuffled when shuffle is True\n    if shuffle:\n        with pytest.raises(AssertionError):\n            np.testing.assert_array_equal(np.unique(groups), np.unique(folds))\n    else:\n        np.testing.assert_array_equal(np.unique(groups), np.unique(folds))\n"], "sample_12": ["compilation error", "def test_latitude_out_of_range(value):\n    \"\"\"\n    Test that creating a Latitude object with a value outside the valid range\n    raises a ValueError.\n    \"\"\"\n    with pytest.raises(ValueError):\n        Latitude(value, u.rad)\n", "def test_angle_from_string_units():\n    \"\"\"\n    Test that Angle can parse strings with units\n    \"\"\"\n    a = Angle('1.5 rad')\n    assert a.value == 1.5\n    assert a.unit == u.rad\n\n    a = Angle('2.5 deg')\n    assert a.value == 2.5\n    assert a.unit == u.deg\n\n    a = Angle('3.5 hr')\n    assert a.value == 3.5\n    assert a.unit == u.hourangle\n", "def test_angle_string_with_directions(input, expected_output):\n    \"\"\"\n    Test the string representation of angles with N, S, E, W directions.\n    \"\"\"\n    a = Angle(input)\n    assert str(a) == expected_output\n", "def test_angle_conversion(input, expected_output, unit):\n    \"\"\"\n    Test conversion of different angle representations to degrees\n    \"\"\"\n    angle = Angle(input)\n    assert np.isclose(angle.to_value(unit), expected_output)"], "sample_523": ["def test_legend_no_labels():\n    # Test that legend is not created when no labels are provided\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4])\n    leg = ax.legend()\n    assert leg is None\n", "def test_legend_multiple_lines_single_label():\n    # test ax.plot() with multidimensional input\n    # and single label\n    x = [1, 2, 3]\n    y1 = [1, 2, 3]\n    y2 = [2, 5, 6]\n    fig, ax = plt.subplots()\n    ax.plot(x, y1)\n    ax.plot(x, y2, label='single_label')\n    leg = ax.legend()\n    legend_texts = [entry.get_text() for entry in leg.get_texts()]\n    assert legend_texts == ['single_label']\n", "def test_legend_with_empty_labels():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label='')\n    leg = ax.legend()\n    assert len(leg.get_texts()) == 0\n    assert len(leg.legendHandles) == 0\n", "def test_legend_lines_from_collection():\n    # Test that lines can be copied for legend collections (#17960)\n    fig, ax = plt.subplots()\n    collections = [mcollections.LineCollection([[(0, 0), (1, 1)]]),\n                   mcollections.LineCollection([[(0, 1), (1, 0)]])]\n    labels = [\"foo\", \"bar\"]\n    legend = ax.legend(collections, labels)\n\n    new_lines = legend.get_lines()\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert len(new_lines) == 2\n    assert all(isinstance(line, mlines.Line2D) for line in new_lines)\n    assert labels == new_labels\n", "def test_legend_markers_from_line2d_with_invalid_markers():\n    # Test that invalid markers raise a ValueError for legend lines\n    _markers = ['invalid', '*', 'v']\n    fig, ax = plt.subplots()\n    lines = [mlines.Line2D([0], [0], ls='None', marker=mark)\n             for mark in _markers]\n    labels = [\"foo\", \"bar\", \"xyzzy\"]\n    with pytest.raises(ValueError, match=\"Unrecognized marker style\"):\n        legend = ax.legend(lines, labels)\n"], "sample_68": ["def test_get_safe_settings(self):\n    \"\"\"\n    The debug page should not show some settings with callable values.\n    \"\"\"\n        return \"This should not be displayed\"\n    with self.settings(DEBUG=True, FOOBAR=callable_setting):\n        safe_settings = get_safe_settings()\n        self.assertNotIsInstance(safe_settings['FOOBAR'], CallableSettingWrapper)\n", "    def test_cleanse_setting_with_dict(self):\n        dict_setting = {\n            'PASSWORD': 'secret',\n            'USERNAME': 'user',\n            'API_KEY': 'api_key',\n        }\n        cleansed_dict = cleanse_setting('DICT_SETTING', dict_setting)\n        self.assertEqual(cleansed_dict['PASSWORD'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_dict['API_KEY'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_dict['USERNAME'], 'user')\n", "    def test_cleanse_setting_dict(self):\n        setting = {\n            'PASSWORD': 'super_secret',\n            'DATABASE_NAME': 'my_db',\n            'API_KEY': 'api_key',\n        }\n        cleansed = cleanse_setting('SETTING', setting)\n        self.assertEqual(cleansed, {\n            'PASSWORD': CLEANSED_SUBSTITUTE,\n            'DATABASE_NAME': 'my_db',\n            'API_KEY': CLEANSED_SUBSTITUTE,\n        })\n", "    def test_multivaluedict_in_sensitive_request(self):\n        \"\"\"\n        MultivalueDict objects in sensitive requests should have their\n        sensitive keys cleansed.\n        \"\"\"\n        data = {\n            'sensitive-key': ['sensitive-value1', 'sensitive-value2'],\n            'nonsensitive-key': ['nonsensitive-value1', 'nonsensitive-value2'],\n        }\n        request = self.rf.post('/some_url/', data)\n        request.sensitive_post_parameters = ['sensitive-key']\n        reporter = ExceptionReporter(request, None, None, None)\n        html = reporter.get_traceback_html()\n        # Sensitive MultivalueDict key is cleansed.\n        self.assertInHTML('<td>sensitive-key</td><td class=\"code\"><pre>[\\'%s\\', \\'%s\\']</pre></td>' % (\n            CLEANSED_SUBSTITUTE, CLEANSED_SUBSTITUTE\n        ), html)\n        # Nonsensitive MultivalueDict key is not cleansed.\n        self.assertInHTML('<td>nonsensitive-key</td><td class=\"code\"><pre>[\\'nonsensitive-value1\\', \\'nonsensitive-value2\\']</pre></td>', html)\n", "    def test_template_loader(self):\n        \"\"\"\n        The template loader source is used to display the template source in the\n        exception report.\n        \"\"\"\n        class SourceLoader:\n                return 'Source from template loader'\n\n        try:\n            raise ValueError('Oops')\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        request = RequestFactory().get('/test_view/')\n        request.resolver_match = mock.Mock(template_name='test_template.html')\n        request.resolver_match.templatename = 'test_template.html'\n        request.resolver_match.template_name = 'test_template.html'\n\n        with mock.patch.dict(sys.modules, {'test_module': mock.Mock(spec=['get_template'])}):\n            sys.modules['test_module'].get_template.return_value = mock.Mock(spec=['template'],\n                template=mock.Mock(spec=['loader'], loader=SourceLoader()))\n\n            with self.settings(TEMPLATES=[{'BACKEND': 'django.template.backends.django.DjangoTemplates'}]):\n                html = ExceptionReporter(request, exc_type, exc_value, tb).get_traceback_html()\n\n        self.assertIn('Source from template loader', html)\n"], "sample_90": ["    def test_descriptor_raises_validation_error_field_specific(self):\n        \"\"\"\n        A model ValidationError using the dict form should put the error\n        message into the correct key of form.errors.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentDescriptorFieldSpecific, fields=['title'])\n        form = form_class(data={'title': 'testing descriptor'}, files=None)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            'title': ['Cannot set attribute', 'This field cannot be blank.']\n        })\n", "def test_setattr_raises_validation_error_custom_field(self):\n    \"\"\"\n    A model ValidationError using a custom field name should put the error\n    message into the correct key of form.errors.\n    \"\"\"\n    form_class = modelform_factory(model=StrictAssignmentCustomField, fields=['title'])\n    form = form_class(data={'title': 'testing setattr'}, files=None)\n    # This line turns on the ValidationError; it avoids the model erroring\n    # when its own __init__() is called when creating form.instance.\n    form.instance._should_error = True\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {\n        'custom_title': ['Cannot set attribute', 'This field cannot be blank.']\n    })\n", "    def test_descriptor_raises_validation_error_field_specific(self):\n        form_class = modelform_factory(model=StrictAssignmentDescriptorFieldSpecific, fields=['title'])\n        form = form_class(data={'title': 'testing descriptor'}, files=None)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            'title': ['Cannot set attribute', 'This field cannot be blank.']\n        })\n", "    def test_descriptor_raises_validation_error(self):\n        form_class = modelform_factory(model=StrictAssignmentDescriptor, fields=['title'])\n        form = form_class(data={'title': 'testing descriptor'}, files=None)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            'title': ['Cannot set attribute']\n        })\n", "    def test_setattr_raises_validation_error(self):\n        \"\"\"\n        A model ValidationError using the dict form should put the error\n        message into the correct key of form.errors.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignment, fields=['title'])\n        form = form_class(data={'title': 'testing setattr'}, files=None)\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            '__all__': ['Cannot set attribute'],\n            'title': ['This field cannot be blank.']\n        })\n"], "sample_381": ["    def test_add_operation_with_no_suggested_name(self):\n        class NoSuggestNameOperation(migrations.Operation):\n                return None\n\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                NoSuggestNameOperation(),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertIs(migration.suggest_name().startswith('auto_'), True)\n", "def test_add_model_with_unique_constraint_and_index(self):\n    \"\"\"\n    Adding a model with a unique constraint and index does not create the same\n    operation multiple times.\n    \"\"\"\n    after = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200, unique=True)),\n    ], options={'indexes': [models.Index(fields=['name'], name='author_name_idx')]})\n    changes = self.get_changes([], [after])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(\n        changes, 'testapp', 0, 0, name='Author',\n        options={'indexes': [models.Index(fields=['name'], name='author_name_idx')]},\n    )\n", "def test_custom_deconstructible_with_args(self):\n    \"\"\"Custom deconstructible objects with args are correctly detected.\"\"\"\n    author_name_deconstructible_5 = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=CustomDeconstructibleObjectWithArgs('test'))),\n        ],\n    )\n    changes = self.get_changes([self.author_name_deconstructible_1], [author_name_deconstructible_5])\n    self.assertEqual(len(changes), 1)\n", "def test_add_model_with_many_to_many_field(self):\n    book = ModelState('testapp', 'Book', [\n        ('id', models.AutoField(primary_key=True)),\n    ])\n    author = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('books', models.ManyToManyField('testapp.Book')),\n    ])\n    changes = self.get_changes([book], [book, author])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author')\n", "def test_alter_order_with_respect_to_model_deleted(self):\n    \"\"\"\n    Removing a model with order_with_respect_to should remove it\n    and its _order field in the same migration.\n    \"\"\"\n    changes = self.get_changes([self.book, self.author_with_book_order_wrt], [self.book])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\", \"DeleteModel\", \"RemoveField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=None)\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"Author\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 2, name=\"_order\", model_name=\"Author\")\n"], "sample_373": ["    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(\"^(?P<sport_slug>\\\\w+)/athletes/(?P<athlete_slug>\\\\w+)/$\"),\n                         \"/<sport_slug>/athletes/<athlete_slug>/\")\n        self.assertEqual(simplify_regex(\"^$\"), \"/\")\n        self.assertEqual(simplify_regex(\"^(?P<pk>[0-9]+)/$\"), \"/<pk>/\")\n        self.assertEqual(simplify_regex(\"^(?P<pk>[0-9]+)/(?P<action>[a-z]+)/$\"), \"/<pk>/<action>/\")\n", "    def test_simplify_regex_with_named_groups(self):\n        pattern = \"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        expected_output = \"/<sport_slug>/athletes/<athlete_slug>/\"\n        self.assertEqual(simplify_regex(pattern), expected_output)\n", "    def test_get_list_data_type(self):\n        self.assertEqual(get_return_data_type('get_some_list'), 'List')\n", "def test_simplify_regex(self):\n    self.assertEqual(simplify_regex('^(?P<sport_slug>\\\\w+)/athletes/(?P<athlete_slug>\\\\w+)/$'),\n                     '/<sport_slug>/athletes/<athlete_slug>/')\n    self.assertEqual(simplify_regex('^(?P<pk>[0-9]+)/change/$'), '/<pk>/change/')\n    self.assertEqual(simplify_regex('^$'), '/')\n    self.assertEqual(simplify_regex('^(?P<app_label>[a-z_]+)/(?P<model_name>[a-z_]+)/$'),\n                     '/<app_label>/<model_name>/')\n", "def test_simplify_regex(self):\n    # Test simple regex patterns\n    self.assertEqual(simplify_regex(r'^simple/$'), '/simple/')\n    self.assertEqual(simplify_regex(r'^no_start_slash'), '/no_start_slash')\n\n    # Test regex patterns with capturing groups\n    self.assertEqual(simplify_regex(r'^(?P<group>\\w+)/$'), '/<group>/')\n    self.assertEqual(simplify_regex(r'^(\\d+)/$'), '/<0>/')\n\n    # Test regex patterns with non-capturing groups\n    self.assertEqual(simplify_regex(r'^(?:\\d+)/$'), '/<0>/')\n\n    # Test regex patterns with metacharacters\n    self.assertEqual(simplify_regex(r'^.*?/(\\w+)/$'), '/<1>/')\n    self.assertEqual(simplify_regex(r'^.+?/(\\w+)/$'), '/<1>/')\n    self.assertEqual(simplify_regex(r'^\\d*/(\\w+)/$'), '/<1>/')\n"], "sample_261": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT1M', timedelta(days=1, minutes=1)),\n            ('P1DT1S', timedelta(days=1, seconds=1)),\n            ('P1DT1.1S', timedelta(days=1, seconds=1, milliseconds=100)),\n            ('P-1D', timedelta(days=-1)),\n            ('P-1DT1H', timedelta(days=-1, hours=1)),\n            ('P-1DT1M', timedelta(days=-1, minutes=1)),\n            ('P-1DT1S', timedelta(days=-1, seconds=1)),\n            ('P-1DT1.1S', timedelta(days=-1, seconds=1, milliseconds=100)),\n            ('P1Y2M3DT1H2M3S', None),  # Years and months not supported\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_datetime_microseconds(self):\n        valid_inputs = (\n            ('2012-04-23T09:15:00.123456', datetime(2012, 4, 23, 9, 15, 0, 123456)),\n            ('2012-04-23T09:15:00.123456Z', datetime(2012, 4, 23, 9, 15, 0, 123456, get_fixed_timezone(0))),\n        )\n        for source, expected in valid_inputs:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n\n        # Invalid inputs\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T09:15:00.1234567')\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P4Y', timedelta(days=4 * 365)),\n            ('P4M', None),  # ISO 8601 doesn't support months\n            ('P4W', timedelta(weeks=4)),\n            ('P4D', timedelta(days=4)),\n            ('PT4H', timedelta(hours=4)),\n            ('PT4M', timedelta(minutes=4)),\n            ('PT4S', timedelta(seconds=4)),\n            ('P4DT4H3M2S', timedelta(days=4, hours=4, minutes=3, seconds=2)),\n            ('P4.5D', timedelta(days=4, hours=12)),\n            ('PT4.5S', timedelta(seconds=4, microseconds=500000)),\n            ('P-4D', timedelta(days=-4)),\n            ('-P4D', timedelta(days=-4)),\n            ('PT-4S', timedelta(seconds=-4)),\n            ('P-4DT-4H-3M-2S', timedelta(days=-4, hours=-4, minutes=-3, seconds=-2)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_iso8601_duration(self):\n        test_values = (\n            ('P4Y', timedelta(days=4*365)),  # years are approximated to days\n            ('P1M', timedelta(days=30)),  # months are approximated to days\n            ('P4DT10H30M', timedelta(days=4, hours=10, minutes=30)),\n            ('PT15M30S', timedelta(minutes=15, seconds=30)),\n            ('PT30S', timedelta(seconds=30)),\n            ('P-4DT-10H-30M', timedelta(days=-4, hours=-10, minutes=-30)),\n            ('PT-15M-30S', timedelta(minutes=-15, seconds=-30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P3DT6H8M', timedelta(days=3, hours=6, minutes=8)),\n            ('P5D', timedelta(days=5)),\n            ('PT12H30M', timedelta(hours=12, minutes=30)),\n            ('PT30S', timedelta(seconds=30)),\n            ('P2DT3H4M', timedelta(days=2, hours=3, minutes=4)),\n            ('PT12H30M5S', timedelta(hours=12, minutes=30, seconds=5)),\n            ('P1YT1M', None),  # Years are not supported by timedelta\n            ('P1MT1H', None),  # Months are not supported by timedelta\n            ('P1M', None),  # Months are not supported by timedelta\n            ('P1Y2MT1H', None),  # Mixed years and months are not supported by timedelta\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_306": ["    def test_parse_iso8601_format(self):\n        test_values = (\n            ('P4Y', timedelta(days=4*365)),\n            ('P4M', timedelta(days=4*30)),\n            ('P4W', timedelta(weeks=4)),\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('P1DT2H3M4.5S', timedelta(days=1, hours=2, minutes=3, seconds=4, milliseconds=500)),\n            ('P1Y2M3DT4H5M6S', timedelta(days=365*1+30*2+3, hours=4, minutes=5, seconds=6)),\n            ('P-1Y-2M-3DT-4H-5M-6S', timedelta(days=-365*1-30*2-3, hours=-4, minutes=-5, seconds=-6)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P4DT15M30S', timedelta(days=4, minutes=15, seconds=30)),\n            ('P4Y6M15DT15M30S', None),  # unsupported year-month duration\n            ('P4W', None),  # unsupported week duration\n            ('P10.5D', timedelta(days=10, hours=12)),  # fractional days\n            ('P10DT15M30S', timedelta(days=10, minutes=15, seconds=30)),\n            ('P10.5DT15M30S', timedelta(days=10, hours=12, minutes=15, seconds=30)),\n            ('P-10DT15M30S', timedelta(days=-10, minutes=-15, seconds=-30)),\n            ('PT15M30S', timedelta(minutes=15, seconds=30)),\n            ('PT15.5M30S', timedelta(minutes=15, seconds=30, milliseconds=500)),\n            ('PT15M30.5S', timedelta(minutes=15, seconds=30, milliseconds=500)),\n            ('PT15M30.500000S', timedelta(minutes=15, seconds=30, microseconds=500)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_duration_iso8601_format(self):\n        test_values = (\n            ('P4DT15H30M', timedelta(days=4, hours=15, minutes=30)),\n            ('P4DT15M30S', timedelta(days=4, minutes=15, seconds=30)),\n            ('P4D', timedelta(days=4)),\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('P-4DT15H30M', timedelta(days=-4, hours=-15, minutes=-30)),\n            ('P-4D', timedelta(days=-4)),\n            ('PT15M30S', timedelta(minutes=15, seconds=30)),\n            ('PT30S', timedelta(seconds=30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_datetime_with_spaces(self):\n        # Test datetime parsing with spaces instead of 'T' separator\n        valid_inputs = (\n            ('2012-04-23 09:15:00', datetime(2012, 4, 23, 9, 15)),\n            ('2012-4-9  4:8:16', datetime(2012, 4, 9, 4, 8, 16)),\n        )\n        for source, expected in valid_inputs:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n", "    def test_parse_datetime_invalid_offset(self):\n        # Invalid timezone offsets\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T09:15:00Z00')\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T09:15:00-25')\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T09:15:00+99')\n"], "sample_1103": ["def test_div():\n    eq = a/b\n    assert eq == a*b**(-1)\n    eq = a/b + c/2\n    assert eq == a*b**(-1) + Rational(1)/2*c\n    eq = (1 - b)/(b - 1)\n    assert eq == (1 + -b)*((-1) + b)**(-1)\n\n    # Add a new test case for division by zero\n    with pytest.raises(ZeroDivisionError):\n        eq = a/0\n", "def test_divmod_with_zero():\n    x = symbols('x')\n    y = Symbol('y', zero=True)\n    z = Symbol('z', zero=False)\n    assert divmod(x, y) == (nan, nan)\n    assert divmod(y, x) == (nan, 0)\n    assert divmod(z, y) == (zoo, 0)\n    assert divmod(y, z) == (0, 0)\n", "def test_floordiv():\n    assert x // y == floor(x / y)\n    assert x // 3 == floor(x / 3)\n    assert 3 // x == floor(3 / x)\n", "def test_div_does_not_cancel_infinities():\n    a, b = symbols('a b')\n    assert ((zoo + 3*a)/(3*a - zoo)) is nan\n    assert ((b + oo)/(b - oo)) is nan\n    # issue 13904\n    expr = (1/(a+b) + 1/(a-b))/(1/(a+b) + 1/(a-b))\n    assert expr.subs(b, a) is nan\n", "def test__floordiv__():\n    assert (x*y) // z == x*(y // z)\n    assert (x*y) // 3 == x*(y // 3)\n    assert 3 // (x*y) == 3 // x*y\n    assert x // y == x*y**-1\n    assert x // 3 == x*Rational(1, 3)\n    assert 3 // x == Rational(3, x)\n"], "sample_411": ["def test_call_command_with_unknown_option(self):\n    msg = \"Unknown option(s) for dance command: unknown_option. Valid options are: example, force_color, help, integer, no_color, opt_3, option3, pythonpath, settings, skip_checks, stderr, stdout, style, traceback, verbosity, version.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        management.call_command(\"dance\", unknown_option=1)\n", "def test_call_command_with_missing_required_parameters(self):\n    msg = \"Error: the following arguments are required: need_me, needme2\"\n    with self.assertRaisesMessage(CommandError, msg):\n        management.call_command(\"required_option\", stdout=StringIO())\n", "def test_call_command_with_invalid_argument(self):\n    msg = \"Error: unrecognized arguments: invalid_arg\"\n    with self.assertRaisesMessage(SystemExit, msg):\n        management.call_command(\"dance\", \"invalid_arg\")\n", "def test_system_exit_with_traceback(self):\n    \"\"\"Exception raised in a command should raise CommandError with traceback\"\"\"\n    with self.assertRaises(CommandError) as cm:\n        management.call_command(\"dance\", example=\"raise\", traceback=True)\n    self.assertEqual(cm.exception.returncode, 3)\n    dance.Command.requires_system_checks = []\n    try:\n        with captured_stderr() as stderr, self.assertRaises(SystemExit) as cm:\n            management.ManagementUtility(\n                [\"manage.py\", \"dance\", \"--example=raise\", \"--traceback\"]\n            ).execute()\n        self.assertEqual(cm.exception.code, 3)\n    finally:\n        dance.Command.requires_system_checks = \"__all__\"\n    self.assertIn(\"Traceback\", stderr.getvalue())\n", "def test_no_color_force_color_exception(self):\n    \"\"\"\n    no_color and force_color options raise an exception.\n    \"\"\"\n    msg = \"'no_color' and 'force_color' can't be used together.\"\n    with self.assertRaisesMessage(CommandError, msg):\n        management.call_command(\"dance\", no_color=True, force_color=True)\n"], "sample_1168": ["def test_rotations_empty():\n    assert list(rotations([])) == []\n    assert list(rotations([], dir=-1)) == []\n", "def test_permutations():\n    assert list(permutations([])) == [()]\n    assert list(permutations([1])) == [(1,)]\n    assert list(permutations([1, 2])) == [(1, 2), (2, 1)]\n    assert list(permutations([1, 2, 3])) == [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n    assert list(permutations([1, 1, 2])) == [(1, 1, 2), (1, 2, 1), (1, 1, 2), (1, 2, 1), (2, 1, 1), (2, 1, 1)]\n", "def test_rotations_edge_cases():\n    assert list(rotations('')) == []\n    assert list(rotations([])) == []\n    assert list(rotations('a')) == [['a']]\n    assert list(rotations([1])) == [[1]]\n    assert list(rotations('ab', dir=-1)) == [['a', 'b'], ['b', 'a']]\n    assert list(rotations(range(1), dir=-1)) == [[0]]\n", "def test_is_palindromic_with_list():\n    assert is_palindromic([1, 2, 1])\n    assert not is_palindromic([1, 2, 3])\n    assert is_palindromic([1, 2, 2, 1])\n    assert is_palindromic([1, 2, 3, 2, 1], 1)\n    assert not is_palindromic([1, 2, 3, 2, 1], 2)\n    assert is_palindromic([1, 2, 3, 2, 1], 2, -1)\n    assert is_palindromic([1, 2, 3, 2, 1], 2, 4)\n", "def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n"], "sample_809": ["def test_mutual_info_classif_sparse():\n    # Here the input is a sparse matrix and the target is discrete.\n    # This tests the handling of sparse matrices.\n    X = csr_matrix([[0, 0, 0],\n                    [1, 1, 0],\n                    [2, 0, 1],\n                    [2, 0, 1],\n                    [2, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n\n    # Here X[:, 0] is the most informative feature, and X[:, 1] is weakly\n    # informative.\n    mi = mutual_info_classif(X, y, discrete_features=True)\n    assert_array_equal(np.argsort(-mi), np.array([0, 2, 1]))\n", "def test_mutual_info_regression_sparse():\n    # Test that sparse input with continuous features raises an error.\n    X = csr_matrix(np.random.rand(100, 5))\n    y = np.random.rand(100)\n    assert_raises(ValueError, mutual_info_regression, X, y)\n", "def test_mutual_info_regression_discrete_features():\n    # Here we test the case when some features are discrete and the target is continuous.\n    X = np.array([[0, 2],\n                  [1, 1],\n                  [2, 0],\n                  [2, 0],\n                  [1, 1]])\n    y = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n\n    # Here X[:, 0] is the most informative feature, and X[:, 1] is weakly\n    # informative.\n    mi = mutual_info_regression(X, y, discrete_features=[True, False])\n    assert_array_equal(np.argsort(-mi), np.array([0, 1]))\n", "def test_mutual_info_classif_sparse():\n    # Test that sparse matrix with continuous features raises an error.\n    X = csr_matrix([[0, 1], [2, 0]])\n    y = np.array([0, 1])\n\n    with assert_raises(ValueError):\n        mutual_info_classif(X, y, discrete_features=False)\n", "def test_mutual_info_regression_discrete_features():\n    # Test the mutual_info_regression function with discrete features\n    X = np.array([[0, 0],\n                  [1, 1],\n                  [2, 0],\n                  [2, 1],\n                  [2, 1]])\n    y = np.array([0.1, 1.2, 2.0, 2.5, 1.8])\n\n    # Here X[:, 0] is a discrete feature, and X[:, 1] is also a discrete feature,\n    # but treated as continuous in this test.\n    mi = mutual_info_regression(X, y, discrete_features=[0], random_state=0)\n\n    # Since X[:, 0] is discrete and has no mutual information with the continuous target,\n    # its mutual information score should be close to zero.\n    assert_almost_equal(mi[0], 0, decimal=1)\n\n    # X[:, 1] is treated as continuous, so it should have a non-zero mutual information score.\n    assert_greater(mi[1], 0)\n"], "sample_42": ["def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    distance = 10 * u.Mpc\n\n    with_hubble = distance.to(u.Mpc / u.littleh, u.with_H0(H0))\n    assert_allclose(with_hubble.value, 10 / (H0 / (100 * u.km / u.s / u.Mpc)))\n\n    without_hubble = with_hubble.to(u.Mpc, u.with_H0(H0))\n    assert_allclose(without_hubble.value, distance.value)\n", "def test_with_H0():\n    from astropy.cosmology import Planck15\n    H0 = Planck15.H0\n    eq = u.with_H0(H0)\n    distance = 10 * u.Mpc / u.littleh\n    physical_distance = distance.to(u.Mpc)\n    assert_quantity_allclose(physical_distance, 10 * u.Mpc)\n", "def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    with u.set_enabled_equivalencies(u.with_H0(H0)):\n        dist = 10 * u.Mpc\n        assert_quantity_allclose(dist.to(u.littleh), 10 / H0.value * u.littleh)\n\n        hdist = 0.5 * u.littleh\n        assert_quantity_allclose(hdist.to(u.Mpc), 0.5 * H0.value * u.Mpc)\n", "def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    redshift = 0.5\n    distance = redshift / H0\n\n    with_H0_eq = u.with_H0(H0)\n\n    assert_quantity_allclose(distance.to(u.Mpc, equivalencies=with_H0_eq), distance / u.littleh)\n    assert_quantity_allclose((distance / u.littleh).to(u.Mpc, equivalencies=with_H0_eq), distance)\n", "def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    little_h = u.Quantity(1, u.littleh)\n    physical_distance = little_h.to(u.Mpc, u.with_H0(H0))\n    assert_quantity_allclose(physical_distance, (1/70) * u.Mpc)\n\n    # Test with the default cosmology\n    from astropy.cosmology import default_cosmology\n    H0_default = default_cosmology.get().H0\n    physical_distance_default = little_h.to(u.Mpc, u.with_H0())\n    expected_distance_default = (1/H0_default.to_value((u.km/u.s)/u.Mpc)) * u.Mpc\n    assert_quantity_allclose(physical_distance_default, expected_distance_default)\n"], "sample_210": ["def test_template_params_in_context(self):\n    \"\"\"A generic template view has kwargs in context.kwargs.\"\"\"\n    response = self.client.get('/template/simple/bar/')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context['kwargs']['foo'], 'bar')\n", "    def test_setup_with_args_kwargs(self):\n        \"\"\"\n        Test a view correctly sets up args, kwargs and request.\n        \"\"\"\n        request = self.rf.get('/')\n        args = ('arg 1', 'arg 2')\n        kwargs = {'kwarg_1': 1, 'kwarg_2': 'year'}\n\n        view = View()\n        view.setup(request, *args, **kwargs)\n\n        self.assertEqual(view.request, request)\n        self.assertEqual(view.args, args)\n        self.assertEqual(view.kwargs, kwargs)\n", "    def test_template_view_get_context_data(self):\n        class CustomTemplateView(TemplateView):\n                context = super().get_context_data(**kwargs)\n                context['custom_key'] = 'custom_value'\n                return context\n\n        request = self.rf.get('/')\n        view = CustomTemplateView.as_view(template_name='generic_views/about.html')\n        response = view(request)\n        self.assertEqual(response.context_data['custom_key'], 'custom_value')\n", "def test_template_params_inheritance(self):\n    \"\"\"A subclassed template view inherits extra context.\"\"\"\n    response = self.client.get('/template/subclass/bar/')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context['foo'], 'bar')\n    self.assertEqual(response.context['extra_key'], 'extra_value')\n    self.assertIsInstance(response.context['view'], View)\n", "    def test_cached_redirect(self):\n        \"\"\"\n        A redirect view can be cached\n        \"\"\"\n        response = self.client.get('/redirect/cached/bar/')\n        self.assertEqual(response.status_code, 302)\n\n        time.sleep(1.0)\n\n        response2 = self.client.get('/redirect/cached/bar/')\n        self.assertEqual(response2.status_code, 302)\n\n        self.assertEqual(response.url, response2.url)\n\n        time.sleep(2.0)\n\n        # Let the cache expire and test again\n        response2 = self.client.get('/redirect/cached/bar/')\n        self.assertEqual(response2.status_code, 302)\n\n        self.assertNotEqual(response.url, response2.url)\n"], "sample_800": ["def test_check_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n    from sklearn.datasets import load_boston\n    from sklearn.linear_model import LogisticRegression\n\n    X, y = load_boston(return_X_y=True)\n    e = LogisticRegression()\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n", "def test_check_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n    from sklearn.datasets import load_boston\n    X, y = load_boston(return_X_y=True)\n    e = DecisionTreeClassifier()\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n", "def test_check_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n\n    X, y = load_boston(return_X_y=True)\n    e = SGDClassifier()\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n", "def test_check_estimator_class_weight_balanced_linear_classifier():\n    # check that check_class_weight_balanced_linear_classifier() works correctly\n    check_class_weight_balanced_linear_classifier(\"LinearRegression\", LinearRegression)\n    check_class_weight_balanced_linear_classifier(\"SGDClassifier\", SGDClassifier)\n\n    # check that an error is raised for non-linear classifiers\n    msg = \"The parameter 'class_weight' is not supported for non-linear classifiers.\"\n    assert_raises_regex(ValueError, msg, check_class_weight_balanced_linear_classifier, \"SVC\", SVC)\n", "def test_check_estimator_correct_not_fitted_error():\n    # check that check_estimator() raises an AssertionError\n    # if the estimator raises CorrectNotFittedError in predict but not in fit\n    assert_raises_regex(AssertionError, \"The estimator raised an exception \"\n                        \"of type CorrectNotFittedError during the prediction \"\n                        \"phase, but not during the fitting phase.\",\n                        check_estimator,\n                        CorrectNotFittedErrorClassifier())\n"], "sample_652": ["def test_call_fixture_function_outside_of_test(request):\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(pytest.UsageError):\n        fix(request)\n", "def test_fixture_called_directly():\n    with pytest.raises(RuntimeError) as excinfo:\n        fix()\n    assert \"fixture 'fix' called directly\" in str(excinfo.value)\n", "def test_call_fixture_function_error_with_request(request):\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(pytest.fail.Exception):\n        assert request.getfixturevalue(\"fix\") == 1\n", "def test_getfixturevalue_teardown_multi_param(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        values = []\n\n        @pytest.fixture(params=[1,2])\n            values.append('setup')\n            yield request.param\n            values.append('teardown')\n\n            pass\n\n            assert values == ['setup', 1, 'teardown', 'setup', 2, 'teardown']\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=3)\n", "def test_call_fixture_function_no_request():\n    \"\"\"Check if an error is raised if a fixture function without request is called directly (#4545)\"\"\"\n\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(RuntimeError, match=\"fixture 'fix' called directly\"):\n        assert fix() == 1\n"], "sample_862": ["def test_callable_analyzer_reraise_error_custom_exception(tmpdir, Estimator):\n    # check if a custom exception from the analyzer is reraised\n        raise ValueError(\"custom exception\")\n\n    data = ['this is text, not file or filename']\n    with pytest.raises(ValueError, match=\"custom exception\"):\n        Estimator(analyzer=analyzer).fit_transform(data)\n", "def test_callable_analyzer_exception(tmpdir, Estimator):\n    data = ['this is text, not file or filename']\n        raise Exception(\"testing\")\n    with pytest.raises(Exception, match=\"testing\"):\n        Estimator(analyzer=analyzer).fit_transform(data)\n", "def test_countvectorizer_max_df_float(Estimator):\n    data = ['this is a sample document', 'this is another document']\n    vec = Estimator(max_df=0.5)\n    vec.fit_transform(data)\n    assert len(vec.vocabulary_) == 4\n", "def test_callable_analyzer_no_input(Estimator):\n    # check if the analyzer is called with the correct input when input is not specified\n    data = ['this is text, not file or filename']\n    analyzer = mock.MagicMock(return_value=data[0].split())\n    Estimator(analyzer=analyzer).fit_transform(data)\n    analyzer.assert_called_once_with(data[0])\n", "def test_callable_analyzer_input_type(tmpdir, Estimator, input_type):\n    data = [str(tmpdir.join('test1.txt')), str(tmpdir.join('test2.txt'))]\n    with open(data[0], 'w') as f:\n        f.write('this is test1')\n    with open(data[1], 'w') as f:\n        f.write('this is test2')\n    analyzer = mock.Mock(side_effect=lambda doc: doc.read().split())\n    X = Estimator(analyzer=analyzer, input=input_type).fit_transform(data)\n    assert analyzer.call_count == 2\n    if input_type == 'file':\n        assert analyzer.call_args_list[0][0][0].name == data[0]\n        assert analyzer.call_args_list[1][0][0].name == data[1]\n    else:\n        assert analyzer.call_args_list[0][0][0] == data[0]\n        assert analyzer.call_args_list[1][0][0] == data[1]\n    assert X.shape == (2, 4)\n"], "sample_729": ["def test_multi_task_lasso_and_enet_float_precision():\n    X, y, X_test, y_test = build_dataset()\n    Y = np.c_[y, y]\n    for dtype in [np.float64, np.float32]:\n        X = dtype(X)\n        Y = dtype(Y)\n        clf = MultiTaskLasso(alpha=1, tol=1e-8).fit(X, Y)\n        clf_float = MultiTaskLasso(alpha=1, tol=1e-8).fit(X, dtype(Y))\n        assert_array_almost_equal(clf.coef_, clf_float.coef_)\n        assert_equal(clf.coef_.dtype, dtype)\n\n        clf = MultiTaskElasticNet(alpha=1, tol=1e-8).fit(X, Y)\n        clf_float = MultiTaskElasticNet(alpha=1, tol=1e-8).fit(X, dtype(Y))\n        assert_array_almost_equal(clf.coef_, clf_float.coef_)\n        assert_equal(clf.coef_.dtype, dtype)\n", "def test_enet_path_l1_ratio_list():\n    # Test l1_ratio as a list\n    X, y, _, _ = build_dataset()\n    l1_ratio_list = [0.1, 0.5, 0.7]\n    clf = ElasticNetCV(l1_ratio=l1_ratio_list)\n    clf.fit(X, y)\n    assert_true(clf.l1_ratio_ in l1_ratio_list)\n", "def test_lasso_path_multitask():\n    # Test that lasso_path works for multi-task\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20, n_targets=3)\n    alphas, _, _ = lasso_path(X, y)\n    assert_equal(alphas.shape, (3,))\n", "def test_enet_multioutput_with_none_input():\n    X, y, _, _ = build_dataset(n_features=10, n_targets=3)\n    y_none = y.copy()\n    y_none[0] = None\n    clf = ElasticNet(alpha=0.01, fit_intercept=True)\n    assert_raises(ValueError, clf.fit, X, y_none)\n", "def test_sparse_input_dtype_enetcv():\n    X, y, _, _ = build_dataset(n_features=10)\n    clf = ElasticNetCV(n_alphas=5)\n    clf.fit(sparse.csr_matrix(X), y)\n    clf1 = ElasticNetCV(n_alphas=5)\n    clf1.fit(sparse.csr_matrix(X, dtype=np.float32), y)\n    assert_almost_equal(clf.alpha_, clf1.alpha_, decimal=6)\n    assert_almost_equal(clf.coef_, clf1.coef_, decimal=6)\n"], "sample_516": ["def test_alpha_blending():\n    fig, ax = plt.subplots()\n    rect1 = plt.Rectangle((0.1, 0.1), 0.5, 0.5, color='red', alpha=0.5)\n    rect2 = plt.Rectangle((0.3, 0.3), 0.5, 0.5, color='blue', alpha=0.5)\n    ax.add_patch(rect1)\n    ax.add_patch(rect2)\n", "def test_multipage_metadata():\n    # Test that metadata is preserved in a multipage pdf\n    fig, ax = plt.subplots()\n    ax.plot(range(5))\n\n    md = {\n        'Author': 'me',\n        'Title': 'Multipage PDF',\n    }\n    buf = io.BytesIO()\n    with PdfPages(buf, metadata=md) as pdf:\n        pdf.savefig(fig)\n        pdf.savefig(fig)\n\n    # Open the pdf with pikepdf and check the metadata\n    pikepdf = pytest.importorskip('pikepdf')\n    with pikepdf.Pdf.open(buf) as pdf:\n        info = {k: str(v) for k, v in pdf.docinfo.items()}\n\n    assert info['/Author'] == 'me'\n    assert info['/Title'] == 'Multipage PDF'\n", "def test_mathtext_unicode():\n    fig = plt.figure()\n    fig.text(0.5, 0.5, u\"\u00b2\", ha=\"center\", va=\"center\", size=72)\n    fig.text(0.5, 0.3, u\"$\u00b2$\", ha=\"center\", va=\"center\", size=72)\n    fig.text(0.5, 0.1, u\"$\\\\sqrt{2}$\", ha=\"center\", va=\"center\", size=72)\n", "def test_vertical_text_urls():\n    pikepdf = pytest.importorskip('pikepdf')\n\n    test_url = 'https://test_text_urls.matplotlib.org/'\n\n    fig = plt.figure(figsize=(1, 2))\n    fig.text(0.1, 0.1, 'N', rotation=90, url=f'{test_url}')\n\n    with io.BytesIO() as fd:\n        fig.savefig(fd, format='pdf')\n\n        with pikepdf.Pdf.open(fd) as pdf:\n            annots = pdf.pages[0].Annots\n\n            # Iteration over Annots must occur within the context manager,\n            # otherwise it may fail depending on the pdf structure.\n            annot = next(\n                (a for a in annots if a.A.URI == f'{test_url}'),\n                None)\n            assert annot is not None\n            assert getattr(annot, 'QuadPoints', None) is not None\n            # Positions in points (72 per inch)\n            assert annot.Rect[1] == \\\n               annot.QuadPoints[1] - decimal.Decimal('0.00001')\n", "def test_pdf_font_encoding():\n    rcParams['pdf.use14corefonts'] = False\n    rcParams['font.family'] = 'sans-serif'\n    rcParams['font.size'] = 8\n    rcParams['font.sans-serif'] = ['DejaVu Sans']\n    rcParams['pdf.compression'] = 0\n\n    text = 'A three-line text with special characters: \u00e9\u00e8\u00ea\u00eb\u00e0\u00e2\u00f4'\n\n    fig, ax = plt.subplots()\n    ax.set_title('Test PDF backend with DejaVu Sans font')\n    ax.text(0.5, 0.5, text, horizontalalignment='center',\n            verticalalignment='bottom',\n            fontsize=14)\n    ax.axhline(0.5, linewidth=0.5)\n"], "sample_287": ["def test_check_fields_duplicates(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        fields = ['title', 'title']\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'fields' contains duplicate field(s).\",\n            obj=MyModelAdmin,\n            id='admin.E006'\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_invalid_save_as(self):\n    class SongAdmin(admin.ModelAdmin):\n        save_as = 'invalid'\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'save_as' must be a boolean.\",\n            obj=SongAdmin,\n            id='admin.E101',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_autocomplete_fields(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ['album']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class SongAdminWithInvalidField(admin.ModelAdmin):\n        autocomplete_fields = ['invalid_field']\n\n    errors = SongAdminWithInvalidField(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' refers to 'invalid_field', \"\n            \"which is not a field of 'admin_checks.Song'.\",\n            obj=SongAdminWithInvalidField,\n            id='admin.E037',\n        )\n    ]\n    self.assertEqual(errors, expected)\n\n    class SongAdminWithNonRelatedField(admin.ModelAdmin):\n        autocomplete_fields = ['title']\n\n    errors = SongAdminWithNonRelatedField(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n            obj=SongAdminWithNonRelatedField,\n            id='admin.E038',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_display_missing_field_method(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = (\"pk\", \"nonexistent_method\")\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_display[1]' refers to 'nonexistent_method', \"\n            \"which is not a callable, an attribute of 'SongAdmin', or an attribute \"\n            \"or method on 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E108',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_list_display_with_invalid_callable(self):\n        return obj.nonexistent\n\n    class SongAdmin(admin.ModelAdmin):\n        list_display = [invalid_callable]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_display[0]' refers to '<function invalid_callable at ...>', which is not a \"\n            \"callable, an attribute of 'SongAdmin', or an attribute or method on 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E108',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_880": ["def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 1, 2, 2, 2], [0, 0, 1, 2, 2, 2]])\n    confidences = np.array([[0.2, 0.3, 0.4, 0.5, 0.6, 0.7], [0.8, 0.9, 1.0, 1.1, 1.2, 1.3]])\n    n_classes = 3\n\n    expected_result = np.array([[0.36666667, 0.53333333, 0.1], [0.16666667, 0.33333333, 0.5]])\n    result = _ovr_decision_function(predictions, confidences, n_classes)\n\n    assert_allclose(result, expected_result, atol=1e-6)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 0, 1, 1, 1], [0, 1, 0, 1, 1], [0, 0, 0, 0, 0]])\n    confidences = np.array([[0.2, 0.3, 0.4, 0.5, 0.6], [0.7, 0.8, 0.9, 1.0, 1.1], [0.0, 0.0, 0.0, 0.0, 0.0]])\n    n_classes = 3\n\n    result = _ovr_decision_function(predictions, confidences, n_classes)\n    expected_result = np.array([[-0.5, 0.5, 1.5], [-1.5, 0.5, 1.5], [-1.0, -1.0, -1.0]])\n\n    assert_allclose(result, expected_result)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 1], [1, 0, 0]])\n    confidences = np.array([[0.3, -0.2, 0.1], [-0.1, 0.4, 0.2]])\n    n_classes = 3\n\n    expected_result = np.array([[0.5, -0.13333333, 0.06666667],\n                                [0.33333333, 0.5, 0.16666667]])\n    result = _ovr_decision_function(predictions, confidences, n_classes)\n\n    assert_allclose(result, expected_result, atol=1e-8)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 0], [1, 0, 1], [1, 1, 0]])\n    confidences = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\n    n_classes = 3\n    expected_output = np.array([[0.3, 0.4, 0.3], [-0.3, 0.2, 0.1], [0.1, 0.2, 0.3]])\n\n    output = _ovr_decision_function(predictions, confidences, n_classes)\n    assert_allclose(output, expected_output)\n", "def test_ovr_decision_function():\n    # Test case 1: Simple binary classification\n    predictions = np.array([[0, 1, 0], [1, 0, 1]])\n    confidences = np.array([[0.2, 0.3, 0.1], [0.6, 0.5, 0.7]])\n    n_classes = 3\n    expected_output = np.array([[0.5, -0.1, -0.4], [0.4, 0.1, 0.3]])\n    assert_allclose(_ovr_decision_function(predictions, confidences, n_classes), expected_output)\n\n    # Test case 2: Tie-breaking with confidences\n    predictions = np.array([[0, 0, 1], [1, 1, 0]])\n    confidences = np.array([[0.2, 0.3, 0.4], [0.6, 0.5, 0.7]])\n    n_classes = 3\n    expected_output = np.array([[1.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n    assert_allclose(_ovr_decision_function(predictions, confidences, n_classes), expected_output)\n"], "sample_71": ["    def test_decimal_subclass_grouping(self):\n        class EuroDecimal(Decimal):\n            \"\"\"\n            Wrapper for Decimal which prefixes each amount with the \u20ac symbol.\n            \"\"\"\n                amount = super().__format__(specifier, **kwargs)\n                return '\u20ac {}'.format(amount)\n\n        price = EuroDecimal('1234567.89')\n        self.assertEqual(nformat(price, '.', grouping=3, thousand_sep=',', force_grouping=True), '\u20ac 1,234,567.89')\n", "    def test_decimal_grouping_sequence(self):\n        self.assertEqual(nformat(Decimal('1234567'), '.', grouping=[3, 2], thousand_sep=','), '1,23,45,67')\n        self.assertEqual(nformat(Decimal('123456789'), '.', grouping=[3, 2, 0], thousand_sep=','), '123,45,6789')\n", "def test_decimal_subclass_formatting(self):\n    class CustomDecimal(Decimal):\n        \"\"\"\n        Wrapper for Decimal which prefixes and suffixes each amount with custom symbols.\n        \"\"\"\n            amount = super().__format__(specifier, **kwargs)\n            return '$ {} %'.format(amount)\n\n    amount = CustomDecimal('1234.56')\n    self.assertEqual(nformat(amount, '.', grouping=3, thousand_sep=',', decimal_pos=2), '$ 1,234.56 %')\n", "def test_decimal_subclass_grouping(self):\n    class EuroDecimal(Decimal):\n        \"\"\"\n        Wrapper for Decimal which prefixes each amount with the \u20ac symbol.\n        \"\"\"\n            amount = super().__format__(specifier, **kwargs)\n            return '\u20ac {}'.format(amount)\n\n    price = EuroDecimal('12345.67')\n    self.assertEqual(nformat(price, '.', grouping=3, thousand_sep=',', force_grouping=True), '\u20ac 12,345.67')\n", "    def test_grouping_sequence(self):\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2), thousand_sep=','), '12,34,56,789')\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,567,89')\n"], "sample_562": ["def test_valid_markerstyles():\n    line = mlines.Line2D([], [])\n    with pytest.raises(ValueError):\n        line.set_marker('invalid_marker')\n", "def test_line_with_large_markersize():\n    \"\"\"Test the behavior of a Line2D object with a large markersize.\"\"\"\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3], marker='o', markersize=100)\n    fig.canvas.draw()\n    assert line.get_markersize() == 100\n", "def test_drawstyle_step_post(fig_test, fig_ref):\n    x = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 2, 3, 2, 1])\n\n    fig_test.add_subplot().plot(x, y, drawstyle='steps-post')\n    fig_ref.add_subplot().plot([1, 2, 3, 4, 5, 5], [1, 1, 2, 3, 2, 1], drawstyle='default')\n", "def test_marker_as_markerstyle_prop_cycle(fig_test, fig_ref):\n    \"\"\"Test that we can set marker as MarkerStyle prop_cycle.\"\"\"\n    markers = [MarkerStyle('o'), 's', 'D', '*']\n\n    x = np.linspace(0, 2 * np.pi)\n    y = np.sin(x)\n\n    axs = fig_ref.add_subplot()\n    for i, marker in enumerate(markers):\n        axs.plot(x, y + i, marker=marker)\n\n    matplotlib.rcParams['axes.prop_cycle'] = cycler(marker=markers)\n\n    ax = fig_test.add_subplot()\n    for _ in range(len(markers)):\n        ax.plot(x, y)\n", "def test_subslice_optimization_zoom(fig_test, fig_ref):\n    \"\"\"Test that subslice optimization works with zoom.\"\"\"\n    x = np.linspace(0, 1, 10000)\n    y = np.sin(x * 10 * np.pi)\n\n    fig_test.add_subplot().plot(x, y)\n    fig_ref.add_subplot().plot(x, y)\n    fig_ref.subplots_adjust(bottom=0.2)\n\n    # Zoom in\n    ax = fig_ref.axes[0]\n    ax.set_xlim(0.1, 0.9)\n    ax.set_ylim(-0.9, 0.9)\n\n    ax = fig_test.axes[0]\n    ax.set_xlim(0.1, 0.9)\n    ax.set_ylim(-0.9, 0.9)\n"], "sample_180": ["def test_unique_constraint_pointing_to_fk(self):\n    class Foo(models.Model):\n        pass\n\n    class Bar(models.Model):\n        foo_1 = models.ForeignKey(Foo, on_delete=models.CASCADE, related_name='bar_1')\n        foo_2 = models.ForeignKey(Foo, on_delete=models.CASCADE, related_name='bar_2')\n\n        class Meta:\n            constraints = [models.UniqueConstraint(fields=['foo_1_id', 'foo_2'], name='unique_constraint')]\n\n    self.assertEqual(Bar.check(databases=self.databases), [])\n", "def test_unique_constraint_pointing_to_fk(self):\n    class Foo(models.Model):\n        pass\n\n    class Bar(models.Model):\n        foo_1 = models.ForeignKey(Foo, on_delete=models.CASCADE, related_name='bar_1')\n        foo_2 = models.ForeignKey(Foo, on_delete=models.CASCADE, related_name='bar_2')\n\n        class Meta:\n            constraints = [models.UniqueConstraint(fields=['foo_1_id', 'foo_2'], name='unique_constraint')]\n\n    self.assertEqual(Bar.check(databases=self.databases), [])\n", "def test_unique_constraint_with_foreign_key_field(self):\n    class RelatedModel(models.Model):\n        pass\n\n    class Model(models.Model):\n        related = models.ForeignKey(RelatedModel, on_delete=models.CASCADE)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(fields=['related'], name='unique_related'),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_name_constraints(self):\n        class Model(models.Model):\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['id'], name='_unique_name'),\n                    models.UniqueConstraint(fields=['id'], name='5unique_name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"The unique constraint name '%sunique_name' cannot start with \"\n                \"an underscore or a number.\" % prefix,\n                obj=Model,\n                id='models.E035',\n            ) for prefix in ('_', '5')\n        ])\n", "def test_proxy_model_contains_model_fields(self):\n    class ProxyModel(models.Model):\n        field = models.CharField(max_length=100)\n\n        class Meta:\n            proxy = True\n\n    self.assertEqual(ProxyModel.check(), [\n        Error(\n            \"Proxy model 'ProxyModel' contains model fields.\",\n            id='models.E017',\n        )\n    ])\n"], "sample_1209": ["def test_prefix_latex():\n    from sympy.printing.latex import latex\n    m = PREFIXES['m']\n    assert latex(m) == r'\\text{m}'\n    mu = PREFIXES['mu']\n    assert latex(mu) == r'\\mu'\n", "def test_prefix_multiplication():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    Mi = BIN_PREFIXES['Mi']\n\n    assert m * k == kilo\n    assert k * m == kilo\n    assert m * Mi != kilo\n    assert Mi * m != kilo\n    assert Mi * kilo != mega\n    assert kilo * Mi != mega\n", "def test_prefix_multiplication():\n    assert kilo * kilo == 1e6\n    assert kibi * kibi == 1024**2\n    assert kilo * kibi == 125 * kilo\n    assert kibi * kilo == 8000 * kibi\n", "def test_prefix_division_with_quantity():\n    m = Quantity(\"fake_meter\", abbrev=\"m\")\n    m.set_global_relative_scale_factor(1, meter)\n\n    km = Quantity(\"kilo_fake_meter\", abbrev=\"km\")\n    km.set_global_relative_scale_factor(1000, meter)\n\n    assert km / m == kilo\n    assert m / km == S.One / kilo\n", "def test_prefix_multiplication():\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n\n    assert k * M == PREFIXES['G']\n\n    Ki = BIN_PREFIXES['Ki']\n    Mi = BIN_PREFIXES['Mi']\n\n    assert Ki * Mi == BIN_PREFIXES['Gi']\n"], "sample_1130": ["def test_auto_vel_connected_frames_with_velocity_in_intermediate_frame():\n    t = dynamicsymbols._t\n    q, q1, q2, u1, u2 = dynamicsymbols('q q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n    B.set_vel(N, u2 * N.y)\n    P = Point('P')\n    P.set_pos(O, q1 * N.x + q2 * B.y)\n    N.orient(B, 'Axis', (q, B.x))\n    assert P.vel(N) == (u1 + q1.diff(t)) * N.x + (u2 + q2.diff(t)) * B.y - q2 * q.diff(t) * B.z\n", "def test_point_acc():\n    q1, q2 = dynamicsymbols('q1 q2')\n    q1d, q2d = dynamicsymbols('q1 q2', 1)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(N, q1 * N.x + q2 * N.y)\n    assert P.acc(N) == q1d * N.x + q2d * N.y\n    P.set_acc(N, q1d * N.z + q2d * B.x)\n    assert P.acc(N) == q1d * N.z + q2d * B.x\n", "def test_auto_vel_inconsistent_values_warning_msg():\n    P = Point('P')\n    P1 = Point('P1')\n    P2 = Point('P2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    P.set_vel(N, N.x)\n    P1.set_pos(P, N.x)\n    P2.set_pos(P1, B.x)\n    N.orient(B, 'Axis', (q, N.z))\n    with warnings.catch_warnings(record=True) as w:  # The velocities from P1 and P2 are inconsistent, thus a warning is raised\n        warnings.simplefilter(\"always\")\n        P2.vel(N)\n        assert issubclass(w[-1].category, UserWarning)\n        assert 'Velocity automatically calculated based on point P1 but it is also possible from point(s):' in str(w[-1].message)\n", "def test_auto_vel_different_frames_for_points():\n    q, u1, u2 = dynamicsymbols('q u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    O.set_vel(N, u1 * N.x)\n    P.set_pos(O, q * B.y)\n    P.set_vel(B, u2 * B.z)\n    assert P.vel(N) == u1 * N.x + q.diff(t) * B.y + u2 * B.z\n", "def test_auto_point_vel_different_frames_warning_msg():\n    q, u = dynamicsymbols('q u')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    Q = Point('Q')\n    P.set_vel(N, u * N.x)\n    Q.set_vel(B, u * B.y)\n    O.set_pos(P, q * N.z)\n    O.set_pos(Q, q * B.y)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        O.vel(N)\n        assert issubclass(w[-1].category, UserWarning)\n        assert 'Velocity automatically calculated based on point P but it is also possible from points(s): [' in str(w[-1].message)\n        assert ']' in str(w[-1].message)\n"], "sample_494": ["def test_serialize_complex_number(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex('1+2j')\", {}),\n    )\n", "def test_serialize_deconstructable(self):\n    deconstructable = DeconstructibleInstances()\n    self.assertSerializedResultEqual(\n        deconstructable,\n        (\n            \"migrations.test_writer.DeconstructibleInstances()\",\n            {\"import migrations.test_writer\"},\n        ),\n    )\n\n    field = models.CharField(default=deconstructable)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.CharField(default=migrations.test_writer.DeconstructibleInstances())\",\n    )\n", "def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(complex(1, 2), (\"complex((1+2j))\", set()))\n", "def test_serialize_deconstructible(self):\n    instance = DeconstructibleInstances()\n    string, imports = MigrationWriter.serialize(instance)\n    self.assertEqual(string, \"migrations.test_writer.DeconstructibleInstances()\")\n    self.assertIn(\"import migrations.test_writer\", imports)\n", "def test_serialize_custom_class(self):\n    class CustomClass:\n            self.a = a\n            self.b = b\n\n            return (\"CustomClass\", [self.a, self.b], {})\n\n    custom_obj = CustomClass(1, 2)\n    self.assertSerializedEqual(custom_obj)\n    self.assertSerializedResultEqual(\n        custom_obj,\n        (\"CustomClass(1, 2)\", {\"import migrations.test_writer\"}),\n    )\n"], "sample_116": ["def test_cache_versioning_delete_many(self):\n    cache.set('answer1', 37, version=1)\n    cache.set('answer1', 42, version=2)\n    cache.set('answer2', 37, version=1)\n    cache.set('answer2', 42, version=2)\n    cache.delete_many(['answer1', 'answer2'])\n    self.assertIsNone(cache.get('answer1', version=1))\n    self.assertIsNone(cache.get('answer1', version=2))\n    self.assertIsNone(cache.get('answer2', version=1))\n    self.assertIsNone(cache.get('answer2', version=2))\n\n    cache.set('answer3', 37, version=1)\n    cache.set('answer3', 42, version=2)\n    cache.set('answer4', 37, version=1)\n    cache.set('answer4', 42, version=2)\n    cache.delete_many(['answer3', 'answer4'], version=2)\n    self.assertEqual(cache.get('answer3', version=1), 37)\n    self.assertIsNone(cache.get('answer3', version=2))\n    self.assertEqual(cache.get('answer4', version=1), 37)\n    self.assertIsNone(cache.get('answer4', version=2))\n\n    cache.set('answer5', 37, version=1)\n    cache.set('answer5', 42, version=2)\n    cache.set('answer6', 37, version=1)\n    cache.set('answer6', 42, version=2)\n    caches['v2'].delete_many(['answer5', 'answer6'])\n    self.assertEqual(cache.get('answer5', version=1), 37)\n    self.assertIsNone(cache.get('answer5', version=2))\n    self.assertEqual(cache.get('answer6', version=1), 37)\n    self.assertIsNone(cache.get('answer6', version=2))", "def test_cache_middleware_with_vary_headers(self):\n    middleware = CacheMiddleware()\n    request = self.factory.get('/view/', HTTP_ACCEPT_ENCODING='gzip')\n    response = HttpResponse('Hello World')\n    response['Vary'] = 'Accept-Encoding'\n    middleware.process_response(request, response)\n    cached_response = middleware.process_request(request)\n    self.assertIsNotNone(cached_response)\n    self.assertEqual(cached_response.content, b'Hello World')\n    request = self.factory.get('/view/', HTTP_ACCEPT_ENCODING='identity')\n    cached_response = middleware.process_request(request)\n    self.assertIsNone(cached_response)\n", "    def test_thread_safety(self):\n        \"\"\"\n        Accessing the cache from separate threads should not raise exceptions.\n        \"\"\"\n            cache = caches['default']\n            cache.set(f'key{i}', f'value{i}')\n            self.assertEqual(cache.get(f'key{i}'), f'value{i}')\n\n        threads = []\n        for i in range(5):\n            t = threading.Thread(target=runner, args=(i,))\n            t.start()\n            threads.append(t)\n\n        for t in threads:\n            t.join()\n", "    def test_concurrency(self):\n        \"\"\"\n        Requesting the same alias from separate threads should not cause any\n        race conditions.\n        \"\"\"\n        cache_keys = ['key1', 'key2', 'key3']\n        cache_values = ['value1', 'value2', 'value3']\n\n            for key, value in zip(cache_keys, cache_values):\n                caches['default'].set(key, value)\n\n        threads = []\n        for _ in range(2):\n            t = threading.Thread(target=set_cache_values, args=(cache_keys, cache_values))\n            t.start()\n            threads.append(t)\n\n        for t in threads:\n            t.join()\n\n        for key, value in zip(cache_keys, cache_values):\n            self.assertEqual(caches['default'].get(key), value)\n", "def test_custom_key_function(self):\n    custom_key_func = lambda key, key_prefix, version: 'CUSTOM-' + key\n    cache = caches['default']\n    cache.key_func = custom_key_func\n\n    cache.set('foo', 'bar')\n    self.assertEqual(cache.get('foo'), 'bar')\n    self.assertEqual(cache.make_key('foo'), 'CUSTOM-foo')\n"], "sample_295": ["def test_expression_wrapper_with_f_expression(self):\n    employee = Employee.objects.create(firstname='John', lastname='Doe', salary=1000)\n    queryset = Employee.objects.annotate(\n        double_salary=ExpressionWrapper(F('salary') * 2, output_field=IntegerField())\n    )\n    self.assertEqual(queryset.get().double_salary, 2000)\n", "    def test_expression_wrapper_ordering(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        order_by_expr = OrderBy(expr)\n        self.assertEqual(order_by_expr.get_group_by_cols(alias=None), [expr])\n", "def test_resolve_expression_with_reuse(self):\n    expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n    new_expr = expr.resolve_expression(query=None, allow_joins=True, reuse=expr, summarize=False, for_save=False)\n    self.assertIs(expr, new_expr)\n", "def test_expression_wrapper_complex(self):\n    expr = ExpressionWrapper(F('start') + F('estimated_time'), output_field=DateTimeField())\n    self.assertIsInstance(expr.output_field, DateTimeField)\n", "def test_nested_expression_wrapper_with_function(self):\n    self.gmbh.point_of_contact = Employee.objects.get(lastname='Meyer')\n    self.gmbh.save()\n    inner = Employee.objects.annotate(\n        outer_lastname=OuterRef(OuterRef('lastname')),\n    ).filter(lastname__startswith=Left('outer_lastname', 1))\n    qs = Employee.objects.annotate(\n        ceo_company=Subquery(\n            Company.objects.filter(\n                point_of_contact__in=inner,\n                ceo__pk=OuterRef('pk'),\n            ).values('name'),\n        ),\n    ).filter(ceo_company__isnull=False)\n    expr = ExpressionWrapper(qs.first().ceo_company, output_field=CharField())\n    self.assertEqual(expr.resolve_expression(), qs.first().ceo_company)\n"], "sample_76": ["    def test_language_settings_consistent(self):\n        for tag in self.valid_tags:\n            with self.subTest(tag), self.settings(LANGUAGES=[(tag, tag)], LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n\n        for tag in self.invalid_tags:\n            with self.subTest(tag), self.settings(LANGUAGES=[(tag, tag)], LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(\n                        'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.',\n                        id='translation.E004'\n                    ),\n                ])\n", "def test_inconsistent_language_settings(self):\n    settings_dict = {\n        'LANGUAGES': [('en', 'English'), ('fr', 'French')],\n        'LANGUAGE_CODE': 'de',\n    }\n    msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.'\n    with self.settings(**settings_dict):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(msg, id='translation.E004'),\n        ])\n", "def test_inconsistent_language_settings(self):\n    msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.'\n    valid_tag = 'en'\n    invalid_tag = 'fr'\n    with self.settings(LANGUAGE_CODE=invalid_tag, LANGUAGES=[(valid_tag, valid_tag)]):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(msg, id='translation.E004'),\n        ])\n", "def test_consistent_language_settings(self):\n    settings_data = [\n        {'LANGUAGE_CODE': 'en', 'LANGUAGES': [('en', 'English')]},  # consistent\n        {'LANGUAGE_CODE': 'fr', 'LANGUAGES': [('en', 'English'), ('fr', 'French')]},  # consistent\n        {'LANGUAGE_CODE': 'de', 'LANGUAGES': [('en', 'English'), ('fr', 'French')]},  # inconsistent\n    ]\n\n    for data in settings_data:\n        with self.subTest(data), self.settings(**data):\n            if data['LANGUAGE_CODE'] in [i for i, _ in data['LANGUAGES']]:\n                self.assertEqual(check_language_settings_consistent(None), [])\n            else:\n                self.assertEqual(check_language_settings_consistent(None), [E004])\n", "def test_consistent_language_settings(self):\n    settings_data = [\n        ({'LANGUAGES': [('en', 'English')], 'LANGUAGE_CODE': 'en'}, []),\n        ({'LANGUAGES': [('en', 'English')], 'LANGUAGE_CODE': 'fr'}, [Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004')]),\n    ]\n    for setting, expected_output in settings_data:\n        with self.settings(**setting):\n            self.assertEqual(check_language_settings_consistent(None), expected_output)\n"], "sample_48": ["def test_expression_on_aggregation_with_filter(self):\n    # Test a filter on an aggregation\n    avg_price_filtered = Book.objects.filter(pages__gt=300).aggregate(avg_price=Avg('price'))\n    self.assertEqual(avg_price_filtered['avg_price'], Decimal('75.00'))\n\n    # Test a filter on an annotation\n    books_filtered = Book.objects.annotate(num_authors=Count('authors')).filter(num_authors__gt=2)\n    self.assertQuerysetEqual(books_filtered, ['Python Web Development with Django'], lambda b: b.name)\n", "def test_distinct_aggregation(self):\n    # Test distinct aggregation\n    distinct_ratings = Book.objects.aggregate(distinct_ratings=Count('rating', distinct=True))\n    self.assertEqual(distinct_ratings['distinct_ratings'], 4)\n\n    # Test distinct aggregation with annotation\n    authors = Author.objects.annotate(distinct_books=Count('book__rating', distinct=True)).order_by('name')\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty', 1),\n            ('Brad Dayley', 1),\n            ('Jacob Kaplan-Moss', 1),\n            ('James Bennett', 1),\n            ('Jeffrey Forcier', 1),\n            ('Paul Bissex', 1),\n            ('Peter Norvig', 2),\n            ('Stuart Russell', 1),\n            ('Wesley J. Chun', 1)\n        ],\n        lambda a: (a.name, a.distinct_books)\n    )\n", "def test_annotate_filter_aggregate(self):\n    authors = Author.objects.annotate(num_books=Count(\"book\")).filter(num_books__gt=1)\n    vals = authors.aggregate(total_authors=Count(\"id\"))\n    self.assertEqual(vals, {\"total_authors\": 4})\n", "def test_filter_aggregate_with_distinct(self):\n    vals = Book.objects.filter(rating__gt=4).aggregate(distinct_pages=Count('pages', distinct=True))\n    self.assertEqual(vals['distinct_pages'], 2)\n", "def test_stddev_aggregate(self):\n    stddev = Book.objects.aggregate(stddev_price=StdDev('price'))['stddev_price']\n    self.assertIsInstance(stddev, Decimal)\n    self.assertEqual(stddev, Approximate(Decimal('22.37'), places=2))\n\n    stddev_pop = Book.objects.aggregate(stddev_price=StdDev('price', sample=False))['stddev_price']\n    self.assertIsInstance(stddev_pop, Decimal)\n    self.assertEqual(stddev_pop, Approximate(Decimal('21.56'), places=2))\n"], "sample_333": ["    def test_attribute_class_instance(self):\n        custom = CustomRenderer()\n\n        class CustomForm(Form):\n            default_renderer = custom\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, custom)\n", "    def test_kwarg_override_attribute(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        custom = CustomRenderer()\n        form = CustomForm(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n", "def test_field_deep_copy_widget(self):\n    class CustomTextInput(TextInput):\n            super().__init__(**kwargs)\n\n    field = CharField(widget=CustomTextInput())\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsNot(field_copy.widget, field.widget)\n", "def test_renderer_class_error(self):\n    class InvalidRenderer:\n        pass\n\n    with self.assertRaises(ImproperlyConfigured):\n        Form(renderer=InvalidRenderer)\n", "def test_field_custom_renderer(self):\n    class CustomField(CharField):\n        default_renderer = CustomRenderer()\n\n    class CustomForm(Form):\n        field = CustomField()\n\n    form = CustomForm()\n    self.assertIsInstance(form.fields['field'].widget.renderer, CustomRenderer)\n"], "sample_577": ["def test_legend_title_from_variable(self, long_df):\n\n    s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n    p = Plot(long_df, x=\"x\", y=\"y\").add(MockMark(), color=s).plot()\n    legend, = p._figure.legends\n    assert legend.get_title().get_text() == s.name\n", "def test_legend_with_different_scales(self, xy):\n\n    s = pd.Series([\"a\", \"b\", \"a\", \"c\"])\n    p = Plot(**xy, color=s).add(MockMark()).scale(color=\"nominal\").plot()\n    e, = p._legend_contents\n\n    labels = categorical_order(s)\n\n    assert e[0] == (s.name, s.name)\n    assert e[-1] == labels\n\n    artists = e[1]\n    assert len(artists) == len(labels)\n    for a, label in zip(artists, labels):\n        assert isinstance(a, mpl.artist.Artist)\n        assert a.value == label\n        assert a.variables == [\"color\"]\n", "def test_single_layer_single_variable_custom_legend(self, xy):\n    s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n    p = Plot(**xy).add(MockMark(), color=s, legend=\"custom\").plot()\n    e, = p._legend_contents\n\n    assert e[0] == (\"custom\", s.name)\n    assert e[-1] == [\"a\", \"b\", \"c\"]\n\n    artists = e[1]\n    assert len(artists) == 3\n    for a, label in zip(artists, [\"a\", \"b\", \"c\"]):\n        assert isinstance(a, mpl.artist.Artist)\n        assert a.value == label\n        assert a.variables == [\"color\"]\n", "def test_legend_with_facets(self, long_df):\n    cols, rows = [\"a\", \"b\"], [\"x\", \"y\"]\n    p = Plot(long_df, color=\"a\").facet(cols, rows).add(MockMark()).plot()\n\n    legend, = p._figure.legends\n    names = categorical_order(long_df[\"a\"])\n    labels = [t.get_text() for t in legend.get_texts()]\n    assert labels == names\n\n    if not _version_predates(mpl, \"3.4\"):\n        contents = legend.get_children()[0]\n        assert len(contents.findobj(mpl.lines.Line2D)) == len(names)\n", "def test_multiple_layers_with_facets(self, long_df):\n\n    p = (\n        Plot(long_df, y=\"y\")\n        .pair(x=[\"a\", \"b\"])\n        .facet(row=\"c\")\n        .add(MockMark(), color=\"a\")\n        .add(MockMark(), color=\"b\")\n        .plot()\n    )\n\n    subplots = p._subplots\n    legend_contents = p._legend_contents\n\n    # Check that the number of legend contents is equal to the number of unique values in \"a\" and \"b\"\n    assert len(legend_contents) == len(long_df[\"a\"].unique()) + len(long_df[\"b\"].unique())\n\n    # Check that the legend contents are correct for each subplot\n    for subplot in subplots:\n        ax = subplot[\"ax\"]\n        legend = ax.get_legend()\n        handles, labels = legend.legendHandles, legend.get_texts()\n        if subplot[\"row\"] == \"a\":\n            assert labels == categorical_order(long_df[\"a\"])\n        elif subplot[\"row\"] == \"b\":\n            assert labels == categorical_order(long_df[\"b\"])\n"], "sample_565": ["def test_imagegrid_cbar_mode_single():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(6, 6))\n\n    positions = (221, 222, 223, 224)\n    cbar_locations = ['left', 'right', 'top', 'bottom']\n\n    for position, location in zip(positions, cbar_locations):\n        grid = ImageGrid(fig, position,\n                         nrows_ncols=(1, 1),\n                         cbar_location=location,\n                         cbar_size='20%',\n                         cbar_mode='single')\n        ax = grid[0]\n\n        ax.imshow(arr, cmap='nipy_spectral')\n        cb = ax.cax.colorbar(ax.images[0])\n", "def test_inset_axes_axes_class():\n    fig, ax = plt.subplots()\n    ax_class = type(ax)\n    inset_ax = inset_axes(ax, width=2., height=2., axes_class=ax_class)\n    assert isinstance(inset_ax, ax_class)\n", "def test_inset_axes_with_transform():\n    fig, ax = plt.subplots(figsize=[5, 4])\n\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n    Z2 = np.zeros((150, 150))\n    ny, nx = Z.shape\n    Z2[30:30+ny, 30:30+nx] = Z\n\n    ax.imshow(Z2, extent=extent, interpolation=\"nearest\",\n              origin=\"lower\")\n\n    ax_transform = ax.transAxes + ax.transData.inverted()\n    ins = inset_axes(ax, width=\"40%\", height=\"30%\",\n                     bbox_to_anchor=(0.4, 0.5),\n                     bbox_transform=ax_transform)\n    ins.imshow(Z2, extent=extent, interpolation=\"nearest\",\n               origin=\"lower\")\n\n    plt.xticks(visible=False)\n    plt.yticks(visible=False)\n\n    mark_inset(ax, ins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n\n    asb = AnchoredSizeBar(ax.transData,\n                          0.5,\n                          '0.5',\n                          loc='lower center',\n                          pad=0.1, borderpad=0.5, sep=5,\n                          frameon=False)\n    ax.add_artist(asb)\n", "def test_bbox_connector():\n    bbox1 = Bbox.from_extents(0, 0, 1, 1)\n    bbox2 = Bbox.from_extents(2, 2, 3, 3)\n\n    path = BboxConnector.connect_bbox(bbox1, bbox2, 1, 2)\n    expected_path = [(1, 1), (2, 2)]\n    assert np.allclose(path.vertices, expected_path)\n\n    path = BboxConnector.connect_bbox(bbox1, bbox2, 1, 3)\n    expected_path = [(1, 1), (3, 2)]\n    assert np.allclose(path.vertices, expected_path)\n\n    path = BboxConnector.connect_bbox(bbox1, bbox2, 2, 1)\n    expected_path = [(0, 0), (2, 2)]\n    assert np.allclose(path.vertices, expected_path)\n\n    path = BboxConnector.connect_bbox(bbox1, bbox2, 4, 3)\n    expected_path = [(1, 0), (3, 2)]\n    assert np.allclose(path.vertices, expected_path)\n", "def test_inset_axes_exceptions():\n    fig, ax = plt.subplots()\n    # Test for ValueError when bbox_to_anchor is a tuple of length 3\n    with pytest.raises(ValueError):\n        inset_axes(ax, width=\"40%\", height=\"30%\",\n                   bbox_to_anchor=(0.4, 0.5, 0.6))\n\n    # Test for ValueError when width or height are strings but bbox_to_anchor is a 2-tuple\n    with pytest.raises(ValueError):\n        inset_axes(ax, width=\"40%\", height=\"30%\",\n                   bbox_to_anchor=(0.4, 0.5))\n\n    # Test for UserWarning when bbox_transform is axes transform but bbox_to_anchor is not specified\n    with pytest.warns(UserWarning):\n        inset_axes(ax, width=\"40%\", height=\"30%\",\n                   bbox_transform=ax.transAxes)\n"], "sample_1083": ["def test_asinh_real_assumptions():\n    z = Symbol('z', real=False)\n    assert asinh(z).is_real is None\n", "def test_tanh_positive():\n    x = symbols('x')\n    k = symbols('k', real=True)\n    n = symbols('n', integer=True)\n\n    assert tanh(k).is_positive is True\n    assert tanh(k + (2*n + 1)*pi*I).is_positive is True\n    assert tanh(k + (2*n - 1)*pi*I).is_positive is False\n    assert tanh(I*pi/4).is_positive is True\n    assert tanh(3*I*pi/4).is_positive is False\n    assert tanh(S.Zero).is_positive is False\n", "def test_tanh_real():\n    x = symbols('x')\n    k = symbols('k', real=True)\n    n = symbols('n', integer=True)\n\n    assert tanh(k).is_real is True\n    assert tanh(k + 2*n*pi*I).is_real is True\n    assert tanh(I*pi/2).is_real is None\n    assert tanh(3*I*pi/2).is_real is None\n    assert tanh(S.Zero).is_real is True\n", "def test_asinh_is_real():\n    x = Symbol('x', real=True)\n    assert asinh(x).is_real is True\n    assert asinh(I*x).is_real is False\n    assert asinh(0).is_real is True\n    assert asinh(1).is_real is True\n    assert asinh(-1).is_real is True\n    assert asinh(oo).is_real is None\n    assert asinh(-oo).is_real is None\n    assert asinh(zoo).is_real is False\n", "def test_asech_negative():\n    x = Symbol('x')\n    assert asech(-x).is_real is False\n    assert asech(sqrt(2)).is_real is True\n"], "sample_662": ["def test_count_towards_summary(self, testdir):\n    \"\"\"Test the count_towards_summary property of the report.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 6\n    for rep in reports:\n        assert rep.count_towards_summary is True\n        rep.count_towards_summary = False\n        data = rep._to_json()\n        new_rep = TestReport._from_json(data)\n        assert new_rep.count_towards_summary is False\n", "def test_report_count_towards_summary(self):\n    \"\"\"Test that the count_towards_summary property is True for a TestReport.\"\"\"\n    report = TestReport(\"test_nodeid\", None, {}, \"passed\", None, \"call\")\n    assert report.count_towards_summary is True\n", "def test_report_properties(self, testdir):\n    \"\"\"Test the properties of BaseReport\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    test_a_call = reports[1]\n\n    # Test longreprtext property\n    assert \"AssertionError: assert False\" in test_a_call.longreprtext\n\n    # Test caplog property\n    test_a_call.sections.append((\"Captured log\", \"Test log message\"))\n    assert test_a_call.caplog == \"Test log message\"\n\n    # Test capstdout property\n    test_a_call.sections.append((\"Captured stdout\", \"Test stdout message\"))\n    assert test_a_call.capstdout == \"Test stdout message\"\n\n    # Test capstderr property\n    test_a_call.sections.append((\"Captured stderr\", \"Test stderr message\"))\n    assert test_a_call.capstderr == \"Test stderr message\"\n\n    # Test passed, failed, and skipped properties\n    assert test_a_call.failed\n    assert not test_a_call.passed\n    assert not test_a_call.skipped\n\n    # Test fspath property\n    assert test_a_call.fspath == test_a_call.nodeid.split(\"::\")[0]\n\n    # Test count_towards_summary property\n    assert test_a_call.count_towards_summary is True\n\n    # Test head_line property\n    assert test_a_call.head_line == test_a_call.location[2]\n", "def test_report_caplog_capstderr_capstdout(self, testdir):\n    \"\"\"Test the caplog, capstdout, and capstderr properties of the BaseReport class.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n            logging.info('Test log')\n            print('Test stdout')\n            print('Test stderr', file=sys.stderr)\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    test_a_call = reports[1]\n    assert test_a_call.caplog == 'Test log'\n    assert test_a_call.capstdout == 'Test stdout\\n'\n    assert test_a_call.capstderr == 'Test stderr\\n'\n", "def test_from_item_and_call_method(self, testdir):\n    \"\"\"Test the from_item_and_call method of TestReport class\"\"\"\n    from _pytest.main import Session\n    from _pytest.python import Function\n\n    # Create a test file\n    testdir.makepyfile(\n        \"\"\"\n            assert 1 == 0\n        \"\"\"\n    )\n\n    # Collect the test items\n    session = Session()\n    config = session.config\n    config._preparse([\"--tb=short\"], [\"test_func\"])\n    collector = session._getinitialpaths()\n    item = collector.collect()[0]\n\n    # Simulate a call to the test item\n    call = item.ihook.pytest_runtest_makereport(item=item, when=\"call\")\n\n    # Create a TestReport instance using the from_item_and_call method\n    report = TestReport.from_item_and_call(item, call)\n\n    # Check the attributes of the report object\n    assert report.nodeid == \"test_func\"\n    assert report.location == item.location\n    assert report.keywords == {\"test_func\": 1}\n    assert report.outcome == \"failed\"\n    assert report.when == \"call\"\n    assert report.user_properties == item.user_properties\n    assert report.sections == []  # No sections added in this case\n    assert report.duration == call.stop - call.start\n    assert \"AssertionError: assert 1 == 0\" in str(report.longrepr)\n"], "sample_410": ["    def setUp(self):\n        self.user = User.objects.create_user(username=\"testuser\", password=\"password\")\n        self.permission = Permission.objects.create(name=\"test_permission\", codename=\"test_codename\", content_type=ContentType.objects.get_for_model(User))\n", "    def setUp(self):\n        self.group = Group.objects.create(name=\"Test Group\")\n        self.permission = Permission.objects.create(\n            name=\"Test Permission\",\n            content_type=ContentType.objects.get_for_model(User),\n            codename=\"test_permission\",\n        )\n", "    def setUp(self):\n        self.user = User.objects.create_user(\"testuser\", password=\"testpassword\")\n        self.permission = Permission.objects.create(name=\"can_test\", codename=\"can_test\", content_type=ContentType.objects.get_for_model(User))\n", "    def setUp(self):\n        self.user = User.objects.create_user('test_user', password='12345')\n        self.content_type = ContentType.objects.get_for_model(User)\n        self.permission = Permission.objects.create(\n            codename='can_change_user',\n            name='Can change user',\n            content_type=self.content_type,\n        )\n", "    def test_check_password_upgrades(self):\n        # Create a user with an old password hash\n        old_password = \"old_password\"\n        user = get_user_model().objects.create_user(username=\"user\", password=old_password)\n\n        # Modify the password hash to simulate an upgrade\n        old_hash = user.password\n        new_hash = make_password(old_password, hasher=\"new_hasher\")\n        user.password = new_hash\n\n        # Check if the password is still valid and if password_changed() is not called\n        with mock.patch(\n            \"django.contrib.auth.password_validation.password_changed\"\n        ) as pw_changed:\n            self.assertTrue(user.check_password(old_password))\n            self.assertEqual(pw_changed.call_count, 0)\n\n        # Check if the password hash was upgraded\n        self.assertNotEqual(old_hash, user.password)\n        self.assertEqual(new_hash, user.password)\n"], "sample_290": ["def test_add_model_with_field_removed_from_base_model_initial(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name when initial is True.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after, initial=True)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_add_model_with_field_removed_from_base_model_order_with_respect_to(self):\n    \"\"\"\n    Removing a base field takes place before setting order_with_respect_to.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={\n            'order_with_respect_to': 'title',\n        }),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'AlterOrderWithRespectTo'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='readable', order_with_respect_to='title')\n", "def test_add_model_with_field_removed_from_base_model_and_unique_together(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name and unique_together constraint.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n            ('author', models.CharField(max_length=200)),\n        ], options={\n            'unique_together': {('title', 'author')},\n        }),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.CharField(max_length=200)),\n        ], options={\n            'unique_together': {('author',)},\n        }),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n            ('author', models.CharField(max_length=200)),\n        ], bases=('app.readable',), options={\n            'unique_together': {('title', 'author')},\n        }),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, [\n        'AlterUniqueTogether', 'RemoveField', 'CreateModel', 'AlterUniqueTogether',\n    ])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='readable', unique_together={('author',)})\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 2, name='book')\n    self.assertOperationAttributes(changes, 'app', 0, 3, name='book', unique_together={('", "def test_remove_field_with_foreign_key(self):\n    changes = self.get_changes(\n        [self.author_with_publisher, self.publisher],\n        [self.author_empty, self.publisher]\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='publisher')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='Author')\n", "def test_add_field_with_non_ascii_default(self):\n    \"\"\"#24073 - Adding a field with a non-ASCII default should work.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_name_non_ascii_default])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n"], "sample_525": ["def test_set_constrained_layout_pads():\n    fig = plt.figure(constrained_layout=True)\n    fig.set_constrained_layout_pads(w_pad=0.2, h_pad=0.5, wspace=0.1, hspace=0.3)\n    info = fig.get_constrained_layout_pads()\n    assert info[0] == 0.2  # w_pad\n    assert info[1] == 0.5  # h_pad\n    assert info[2] == 0.1  # wspace\n    assert info[3] == 0.3  # hspace\n", "def test_set_constrained_layout_pads():\n    fig = plt.figure()\n    fig.set_constrained_layout_pads(w_pad=0.05, h_pad=0.1)\n    ax = fig.add_subplot(111)\n    fig.draw_without_rendering()\n    pad = fig.get_constrained_layout_pads()\n    assert pad[0] == 0.05\n    assert pad[1] == 0.1\n", "def test_pick_event():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [2, 3])\n    fig.pick(None)  # Should not raise any exception\n    event = SimpleNamespace(x=0.5, y=2.5, inaxes=ax, guiEvent=None)\n    fig.pick(event)\n    assert line in fig.stale_callback.artists\n", "def test_add_subplot_invalid_triple_tuple():\n    fig = plt.figure()\n    with pytest.raises(ValueError, match='num must be 1 <= num <= 4'):\n        fig.add_subplot((2, 2, 3, 4))\n", "def test_rcparams_override(fig_test, fig_ref):\n    fig_ref.supxlabel(\"xlabel\", weight='bold', size=15)\n    fig_ref.supylabel(\"ylabel\", weight='bold', size=15)\n    fig_ref.suptitle(\"Title\", weight='light', size=20)\n    with mpl.rc_context({'figure.labelweight': 'bold',\n                         'figure.labelsize': 15,\n                         'figure.titleweight': 'light',\n                         'figure.titlesize': 20}):\n        fig_test.supxlabel(\"xlabel\", weight='normal', size=12)\n        fig_test.supylabel(\"ylabel\", weight='normal', size=12)\n        fig_test.suptitle(\"Title\", weight='bold', size=25)\n"], "sample_157": ["    def setUp(self):\n        Object.objects.create(name='object1')\n        ObjectReference.objects.create(referenced_object=Object.objects.get(name='object1'))\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n            # Assert that the serialized data is not empty\n            self.assertIsNotNone(test_connection._test_serialized_contents)\n            self.assertNotEqual(test_connection._test_serialized_contents, '')\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_clone_test_db(self):\n        # Test the cloning of a test database\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        suffix = 'clone'\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix, verbosity=0, keepdb=True)\n            cloned_db_settings = creation.get_test_db_clone_settings(suffix)\n            self.assertEqual(cloned_db_settings['NAME'], old_database_name + '_' + suffix)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0, suffix=suffix)\n", "    def setUp(self):\n        self.test_connection = get_connection_copy()\n        self.creation = self.test_connection.creation_class(self.test_connection)\n        self.old_database_name = self.test_connection.settings_dict['NAME']\n        with mock.patch.object(self.creation, '_create_test_db'):\n            self.creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n", "    def setUp(self):\n        self.test_connection = get_connection_copy()\n        self.creation = self.test_connection.creation_class(self.test_connection)\n        self.old_database_name = self.test_connection.settings_dict['NAME']\n\n        # Create a test database\n        with mock.patch.object(self.creation, '_create_test_db'):\n            self.creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n"], "sample_338": ["def test_add_model_with_field_removed_from_base_model_and_custom_name(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'custom_name', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='custom_name')\n", "    def test_rename_field_preserved_db_column(self):\n        before = [\n            ModelState('app', 'Foo', [\n                ('id', models.AutoField(primary_key=True)),\n                ('field', models.IntegerField()),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Foo', [\n                ('id', models.AutoField(primary_key=True)),\n                ('renamed_field', models.IntegerField(db_column='field')),\n            ]),\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterField', 'RenameField'])\n        self.assertOperationAttributes(\n            changes, 'app', 0, 0, model_name='foo', name='field',\n        )\n        self.assertEqual(changes['app'][0].operations[0].field.deconstruct(), (\n            'field', 'django.db.models.IntegerField', [], {'db_column': 'field'},\n        ))\n        self.assertOperationAttributes(\n            changes, 'app', 0, 1, model_name='foo', old_name='field',\n            new_name='renamed_field',\n        )\n", "def test_change_fk_before_model_creation(self):\n    \"\"\"\n    ForeignKeys are changed _before_ the model they point to is created.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_name, self.book_migrations_fk],\n        [self.aardvark_testapp, self.book_migrations_aardvark_fk]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterField\", \"CreateModel\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"author\")\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name=\"Aardvark\")\n", "def test_swappable_first_inheritance_without_base_model(self):\n    \"\"\"Swappable models get their CreateModel first, even without base model.\"\"\"\n    changes = self.get_changes([], [self.custom_user_no_inherit])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'thirdapp', 1)\n    self.assertOperationTypes(changes, 'thirdapp', 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, 'thirdapp', 0, 0, name=\"CustomUser\")\n", "    def test_alter_model_options_without_changes(self):\n        \"\"\"No change should not create a migration.\"\"\"\n        author = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], {\"verbose_name\": \"Author\"})\n        changes = self.get_changes([author], [author])\n        self.assertEqual(len(changes), 0)\n"], "sample_497": ["def test_minorticks_off():\n    fig, ax = plt.subplots()\n    ax.minorticks_off()\n    assert len(ax.xaxis.get_minor_ticks()) == 0\n    assert len(ax.yaxis.get_minor_ticks()) == 0\n", "def test_minorticks_off_axes_method():\n    fig, ax = plt.subplots()\n    ax.minorticks_off()\n    assert len(ax.xaxis.get_minor_ticks()) == 0\n    assert len(ax.yaxis.get_minor_ticks()) == 0\n", "def test_minorticks_manual(xminor, yminor):\n    fig, ax = plt.subplots()\n    ax.minorticks_on()\n    ax.xaxis.set_minor_visible(xminor)\n    ax.yaxis.set_minor_visible(yminor)\n    assert (len(ax.xaxis.get_minor_ticks()) > 0) == xminor\n    assert (len(ax.yaxis.get_minor_ticks()) > 0) == yminor\n", "def test_minortick_direction():\n    fig, ax = plt.subplots()\n    ax.minorticks_on()\n    for tick in ax.xaxis.get_minor_ticks():\n        assert tick.tick1line.get_markeredgewidth() == 0.8\n        assert tick.tick2line.get_markeredgewidth() == 0.8\n        tick.tick1line.set_markeredgewidth(1.2)\n        assert tick.tick1line.get_markeredgewidth() == 1.2\n    for tick in ax.yaxis.get_minor_ticks():\n        assert tick.tick1line.get_markeredgewidth() == 0.8\n        assert tick.tick2line.get_markeredgewidth() == 0.8\n        tick.tick2line.set_markeredgewidth(1.2)\n        assert tick.tick2line.get_markeredgewidth() == 1.2\n", "def test_decimal_places(self, xmax, decimals, symbol, x, display_range, expected):\n    formatter = mticker.PercentFormatter(xmax, decimals, symbol)\n    assert formatter.format_pct(x, display_range) == expected\n"], "sample_46": ["def test_filter_with_subquery(self):\n    subquery = NullableUUIDModel.objects.filter(field__isnull=False).values('field')\n    self.assertSequenceEqual(\n        NullableUUIDModel.objects.filter(field__in=subquery),\n        [self.objs[0], self.objs[1]]\n    )\n", "    def test_dumping_nullable(self):\n        instance = NullableUUIDModel(field=None)\n        data = serializers.serialize('json', [instance])\n        self.assertEqual(json.loads(data), json.loads(self.nullable_test_data))\n", "def test_in_lookup(self):\n    u1 = UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440000')\n    u2 = UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440001')\n    u3 = UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440002')\n    result = UUIDModel.objects.filter(field__in=[u1.field, u2.field])\n    self.assertSequenceEqual(result, [u1, u2])\n", "def test_regex(self):\n    self.assertSequenceEqualWithoutHyphens(\n        NullableUUIDModel.objects.filter(field__regex=r'^550e8400e29b'),\n        [self.objs[1]],\n    )\n    self.assertSequenceEqual(\n        NullableUUIDModel.objects.filter(field__regex=r'^550e8400-e29b'),\n        [self.objs[1]],\n    )\n", "def test_uuid_as_default(self):\n    instance = PrimaryKeyUUIDModel.objects.create()\n    self.assertIsInstance(instance.pk, uuid.UUID)\n    self.assertEqual(PrimaryKeyUUIDModel.objects.get(pk=instance.pk), instance)\n"], "sample_977": ["def test_Sum():\n    assert mcode(Sum(k, (k, 1, n))) == \"Hold[Sum[k, {k, 1, n}]]\"\n    assert mcode(Sum(k**2, (k, 1, n))) == \"Hold[Sum[k^2, {k, 1, n}]]\"\n    assert mcode(Sum(k**2, (k, 1, n), (n, 1, m))) == \"Hold[Sum[k^2, {k, 1, n}, {n, 1, m}]]\"\n", "def test_Sum():\n    assert mcode(Sum(k, (k, 1, n))) == \"Hold[Sum[k, {k, 1, n}]]\"\n    assert mcode(Sum(k**2, (k, 1, n))) == \"Hold[Sum[k^2, {k, 1, n}]]\"\n    assert mcode(Sum(sin(k), (k, 0, n))) == \"Hold[Sum[Sin[k], {k, 0, n}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**k, (k, 1, n))) == \"Hold[Sum[x^k, {k, 1, n}]]\"\n    assert mcode(Sum(k*x**(n - k), (k, 0, n))) == \"Hold[Sum[k*x^(-k + n), {k, 0, n}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**n, (n, 1, oo))) == \"Hold[Sum[x^n, {n, 1, Infinity}]]\"\n    assert mcode(Sum(1/n**2, (n, 1, 10))) == \"Hold[Sum[1/n^2, {n, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**k, (k, 1, n))) == \"Hold[Sum[x^k, {k, 1, n}]]\"\n    assert mcode(Sum(k, (k, 1, n))) == \"Hold[Sum[k, {k, 1, n}]]\"\n    assert mcode(Sum(1/k, (k, 1, n))) == \"Hold[Sum[1/k, {k, 1, n}]]\"\n"], "sample_463": ["def test_alter_index_together_to_unique_together(self):\n    changes = self.get_changes(\n        [self.author_empty, self.book_index_together],\n        [\n            self.author_empty,\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n                {\n                    \"unique_together\": {(\"author\", \"title\")},\n                },\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"AlterIndexTogether\", \"AlterUniqueTogether\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        index_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        name=\"book\",\n        unique_together={(\"author\", \"title\")},\n    )\n", "def test_alter_field_to_index(self):\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n    )\n    author_with_index = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, db_index=True)),\n            (\"age\", models.IntegerField()),\n        ],\n    )\n    changes = self.get_changes([initial_author], [author_with_index])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, model_name=\"author\", name=\"name\")\n    self.assertTrue(changes[\"testapp\"][0].operations[0].field.db_index)\n", "def test_alter_field_and_index_together_reverse(self):\n    author_reversed_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, unique=True)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"index_together\": {(\"age\",)},\n        },\n    )\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField(db_index=True)),\n        ],\n        {\n            \"index_together\": {(\"name\",)},\n        },\n    )\n    changes = self.get_changes([author_reversed_constraints], [initial_author])\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\n            \"AlterIndexTogether\",\n            \"AlterField\",\n            \"AlterField\",\n            \"AlterIndexTogether\",\n        ],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        index_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        model_name=\"author\",\n        name=\"name\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        2,\n        model_name=\"author\",\n        name=\"age\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        3,\n        name=\"author\",\n        index_together={(\"name\",)},\n    )\n", "def test_alter_unique_together_in_model_with_index_together(self):\n    before = [\n        AutodetectorTests.author_empty,\n        ModelState(\n            \"otherapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"author\", \"title\")},\n                \"unique_together\": {(\"author\", \"title\")},\n            },\n        ),\n    ]\n    after = [\n        AutodetectorTests.author_empty,\n        ModelState(\n            \"otherapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": set(),\n                \"unique_together\": set(),\n            },\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"AlterIndexTogether\", \"AlterUniqueTogether\"],\n    )\n", "def test_add_field_with_default_and_db_index(self):\n    \"\"\"#23405 - Adding a NOT NULL and blank `CharField` or `TextField` with default and db_index should not prompt for a default.\"\"\"\n    changes = self.get_changes(\n        [self.author_empty], [self.author_with_biography_blank_default_index]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"biography\")\n"], "sample_440": ["def test_update_conflicts_unique_fields_different_order(self):\n    self._test_update_conflicts(unique_fields=[\"rank\", \"number\"])\n", "def test_update_conflicts_unique_fields_subset(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\"),\n            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n            UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n\n    conflicting_objects = [\n        UpsertConflict(number=1, rank=4, name=\"Steve\"),\n        UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n        UpsertConflict(number=3, rank=1, name=\"Hannah\"),\n    ]\n    UpsertConflict.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"name\"],\n        unique_fields=[\"number\", \"rank\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": 1, \"name\": \"Steve\"},\n            {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n            {\"number\": 3, \"rank\": 3, \"name\": \"Hannah\"},\n        ],\n    )\n\n    UpsertConflict.objects.bulk_create(\n        conflicting_objects + [UpsertConflict(number=4, rank=4, name=\"Mark\")],\n        update_conflicts=True,\n        update_fields=[\"name\"],\n        unique_fields=[\"number\", \"rank\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 4)\n    self.assertCountEqual(\n        UpsertConflict", "def test_update_conflicts_unique_field_unsupported_no_unique_fields(self):\n    msg = (\n        \"This database backend does not support updating conflicts with \"\n        \"specifying unique fields that can trigger the upsert.\"\n    )\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        TwoFields.objects.bulk_create(\n            [TwoFields(f1=1, f2=1), TwoFields(f1=2, f2=2)],\n            update_conflicts=True,\n            update_fields=[\"f2\"],\n        )\n", "def test_update_conflicts_non_concrete_fields(self):\n    msg = \"bulk_create() can only be used with concrete fields in update_fields.\"\n    # One-to-one relationship.\n    with self.assertRaisesMessage(ValueError, msg):\n        Country.objects.bulk_create(\n            self.data,\n            update_conflicts=True,\n            update_fields=[\"relatedmodel\"],\n            unique_fields=[\"pk\"],\n        )\n", "def test_update_conflicts_multiple_unique_fields(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\"),\n            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n            UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n\n    conflicting_objects = [\n        UpsertConflict(number=1, rank=1, name=\"Steve\"),\n        UpsertConflict(number=2, rank=3, name=\"Mary\"),\n        UpsertConflict(number=3, rank=2, name=\"Hannah\"),\n    ]\n    UpsertConflict.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\", \"rank\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": 1, \"name\": \"Steve\"},\n            {\"number\": 2, \"rank\": 2, \"name\": \"Mary\"},\n            {\"number\": 3, \"rank\": 3, \"name\": \"Hannah\"},\n        ],\n    )\n\n    UpsertConflict.objects.bulk_create(\n        conflicting_objects + [UpsertConflict(number=4, rank=4, name=\"Mark\")],\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\", \"rank\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 4)\n    self.assertCountEqual(\n       "], "sample_177": ["    def test_unique_together(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=50)\n            age = models.IntegerField()\n\n            class Meta:\n                app_label = 'migrations'\n                unique_together = [['name', 'age']]\n\n        model_state = ModelState.from_model(TestModel)\n        unique_together = model_state.options['unique_together']\n        self.assertEqual(unique_together, {('name', 'age')})\n\n        # Modifying the state doesn't modify the unique_together on the model.\n        model_state.options['unique_together'] = {('name',)}\n        self.assertEqual(TestModel._meta.unique_together, {('name', 'age')})\n", "def test_proxy_base_related_models(self):\n    class BaseModel(models.Model):\n        name = models.CharField(max_length=50)\n\n        class Meta:\n            app_label = 'migrations'\n\n    class ProxyModel(BaseModel):\n        class Meta:\n            app_label = 'migrations'\n            proxy = True\n\n    class RelatedModel(models.Model):\n        base_model = models.ForeignKey(BaseModel, models.CASCADE)\n        proxy_model = models.ForeignKey(ProxyModel, models.CASCADE)\n\n        class Meta:\n            app_label = 'migrations'\n\n    base_model_state = ModelState.from_model(BaseModel)\n    proxy_model_state = ModelState.from_model(ProxyModel)\n    related_model_state = ModelState.from_model(RelatedModel)\n\n    # Check that the related models of BaseModel include ProxyModel and RelatedModel\n    self.assertRelated(BaseModel, [ProxyModel, RelatedModel])\n\n    # Check that the related models of ProxyModel include BaseModel and RelatedModel\n    self.assertRelated(ProxyModel, [BaseModel, RelatedModel])\n\n    # Check that the related models of RelatedModel include BaseModel and ProxyModel\n    self.assertRelated(RelatedModel, [BaseModel, ProxyModel])\n", "def test_nested_abstract_base_with_fields(self):\n    class AbstractBase(models.Model):\n        base_field = models.CharField(max_length=50)\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n\n    class AbstractChild(AbstractBase):\n        child_field = models.IntegerField()\n\n        class Meta:\n            abstract = True\n\n    class ConcreteModel(AbstractChild):\n        concrete_field = models.BooleanField()\n\n    state = ModelState.from_model(ConcreteModel)\n    self.assertEqual(len(state.fields), 3)\n    self.assertEqual(state.fields['base_field'].max_length, 50)\n    self.assertIsInstance(state.fields['child_field'], models.IntegerField)\n    self.assertIsInstance(state.fields['concrete_field'], models.BooleanField)\n", "def test_proxy_base_with_foreign_key(self):\n    A = self.create_model(\"A\")\n    B = self.create_model(\"B\", bases=(A,), proxy=True, foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n    C = self.create_model(\"C\")\n    self.assertRelated(A, [B, C])\n    self.assertRelated(B, [C])\n    self.assertRelated(C, [A, B])\n", "def test_intermediate_m2m_extern_fk_base(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n    B = self.create_model(\"B\")\n    Z = self.create_model(\"Z\")\n    S = self.create_model(\"S\")\n    T = self.create_model(\"T\", foreign_keys=[\n        models.ForeignKey('A', models.CASCADE),\n        models.ForeignKey('B', models.CASCADE),\n        models.ForeignKey('Z', models.CASCADE),\n    ], bases=(S,))\n    self.assertRelated(A, [B, S, T, Z])\n    self.assertRelated(B, [A, S, T, Z])\n    self.assertRelated(S, [A, B, T, Z])\n    self.assertRelated(T, [A, B, S, Z])\n    self.assertRelated(Z, [A, B, S, T])\n"], "sample_853": ["def test_transform_target_regressor_default_regressor():\n    X, y = friedman\n    # check that the default regressor is LinearRegression\n    regr = TransformedTargetRegressor(transformer=StandardScaler())\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n", "def test_transform_target_regressor_no_regressor():\n    # Test case when no regressor is provided, should default to LinearRegression\n    X, y = friedman\n    regr = TransformedTargetRegressor(transformer=StandardScaler())\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n", "def test_transform_target_regressor_score():\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    regr.fit(X, y)\n    score = regr.score(X, y)\n    assert 0 <= score <= 1  # The score should be between 0 and 1\n", "def test_transform_target_regressor_default_regressor():\n    X, y = friedman\n    regr = TransformedTargetRegressor(transformer=StandardScaler())\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n", "def test_transform_target_regressor_default_regressor():\n    X, y = friedman\n    regr = TransformedTargetRegressor(transformer=StandardScaler())\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n    y_pred = regr.predict(X)\n    assert y.shape == y_pred.shape\n"], "sample_933": ["def test_gettext_location_uuid(app, monkeypatch):\n    # Mocking the gettext_location and gettext_uuid config values\n    monkeypatch.setattr(app.config, 'gettext_location', False)\n    monkeypatch.setattr(app.config, 'gettext_uuid', True)\n\n    app.builder.build_all()\n\n    # Check if the location and uuid are not present in the generated pot files\n    for pot_file in app.outdir.glob('*.pot'):\n        content = pot_file.read_text()\n        assert \"#: \" not in content\n        assert \"#, uuid-\" not in content\n", "def test_gettext_template_duplicated_msgids(app):\n    app.builder.build_all()\n    assert (app.outdir / 'sphinx.pot').isfile()\n\n    result = (app.outdir / 'sphinx.pot').read_text()\n    assert result.count('msgid \"Welcome\"') == 1\n    assert result.count('msgid \"Sphinx %(version)s\"') == 1\n", "def test_should_write(app):\n    app.builder.build_all()\n\n    # Check that should_write returns True if the file does not exist\n    assert should_write('non_existent_file', '') is True\n\n    # Check that should_write returns True if the header of the file is different\n    content = 'This is a test content\\n\"POT-Creation-Date: 2020-01-01 00:00+0000\\n\"PO-Revision-Date: 2020-01-01 00:00+0000\\n'\n    (app.outdir / 'test_file.pot').write_text(content)\n    new_content = 'This is a different test content\\n\"POT-Creation-Date: 2020-01-02 00:00+0000\\n\"PO-Revision-Date: 2020-01-01 00:00+0000\\n'\n    assert should_write(app.outdir / 'test_file.pot', new_content) is True\n\n    # Check that should_write returns True if the body of the file is different\n    new_content = 'This is a test content\\n\"POT-Creation-Date: 2020-01-01 00:00+0000\\n\"PO-Revision-Date: 2020-01-02 00:00+0000\\n'\n    assert should_write(app.outdir / 'test_file.pot', new_content) is True\n\n    # Check that should_write returns False if the content is the same\n    assert should_write(app.outdir / 'test_file.pot', content) is False\n", "def test_gettext_template_with_escaped_characters(app):\n    app.builder.build_all()\n    assert (app.outdir / 'sphinx.pot').isfile()\n\n    result = (app.outdir / 'sphinx.pot').read_text()\n    assert 'msgid \"Template with \\\\\"escaped\\\\\" characters\"' in result\n    assert 'msgstr \"\"' in result\n", "def test_gettext_location_and_uuid(app, monkeypatch):\n    monkeypatch.setenv('SOURCE_DATE_EPOCH', '1580515200')\n    app.builder.build_all()\n\n    pot_content = (app.outdir / 'index.pot').read_text()\n    assert '\"POT-Creation-Date: 2020-01-31 12:00+0000\\\\n\"' in pot_content\n\n    uuid_pattern = re.compile(r'#: .*#, ([a-f0-9]{32})')\n    locations = re.findall(r'\"#: (.*?)\"', pot_content)\n    uuids = re.findall(uuid_pattern, pot_content)\n\n    assert len(locations) == len(uuids)\n    assert len(set(uuids)) == len(uuids)  # Ensure all UUIDs are unique\n\n    for location in locations:\n        assert location in pot_content\n"], "sample_424": ["    def test_references_field_by_through_fields(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\"Other\", through_fields=[\"from\", \"to\"]),\n        )\n        self.assertIs(operation.references_field(\"Model\", \"from\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"from\", \"migrations\"), False)\n        self.assertIs(operation.references_field(\"Other\", \"to\", \"migrations\"), False)\n", "def test_references_field_by_related_query_name(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, related_query_name=\"other\"),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"other\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_related_name(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, related_name=\"other_field\"),\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"other_field\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"other_field\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"other_field\", \"migrations\"), False\n    )\n", "def test_references_field_by_through_with_db_table(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ManyToManyField(\"Other\", through=\"Through\", db_table=\"custom_table\")\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"custom_table\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_db_column(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, db_column=\"other_id\")\n    )\n    self.assertIs(operation.references_field(\"Other\", \"id\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"other_id\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), False)\n"], "sample_326": ["def test_urlize_nofollow(self):\n    value = 'Visit <a href=\"http://example.com\">example.com</a> for more info.'\n    output = 'Visit <a href=\"http://example.com\" rel=\"nofollow\">example.com</a> for more info.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_nofollow(self):\n    value = 'Check out this site: example.com.'\n    output = 'Check out this site: <a href=\"http://example.com/\" rel=\"nofollow\">example.com</a>.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_autoescape(self):\n    tests = (\n        (\n            'Search for <script>google.com</script>/?q=! and see.',\n            'Search for &lt;script&gt;google.com&lt;/script&gt;/?q=! and see.'\n        ),\n        (\n            lazystr('Search for <script>google.com</script>/?q=!'),\n            'Search for &lt;script&gt;google.com&lt;/script&gt;/?q=!'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n", "def test_urlize_trim_url_limit(self):\n    value = 'Search for google.com/?q=abcdefghijklmnopqrstuvwxyz and see.'\n    output = 'Search for <a href=\"http://google.com/?q=abcdefghijklmnopqrstuvwx\">google.com/?q=abcdefghijklmnopqrstuvwx...</a> and see.'\n    self.assertEqual(urlize(value, trim_url_limit=30), output)\n", "def test_urlize_autoescape(self):\n    tests = (\n        (\n            '<b>Search</b> for google.com/?q=! and see.',\n            '<b>Search</b> for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'\n        ),\n        (\n            'Search for <b>google.com/?q=!</b> and see.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'\n        ),\n        (\n            lazystr('<b>Search</b> for google.com/?q=!'),\n            '<b>Search</b> for <a href=\"http://google.com/?q=\">google.com/?q=</a>!'\n        ),\n        ('foo@<b>example.com</b>', '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n"], "sample_351": ["def test_modelchoiceiteratorvalue_str_method(self):\n    value = ModelChoiceIteratorValue(self.c1.pk, self.c1)\n    self.assertEqual(str(value), str(self.c1.pk))\n", "def test_modelchoicefield_limit_choices_to(self):\n    # Test limit_choices_to callable\n    limit_choices_to = lambda: {'name__startswith': 'T'}\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=limit_choices_to)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n\n    # Test limit_choices_to dictionary\n    limit_choices_to = {'name__startswith': 'E'}\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=limit_choices_to)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n    ])\n", "def test_queryset_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name': 'A test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=lambda: {'name': 'A test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n", "def test_modelchoicefield_custom_iterator_label_from_instance(self):\n    class CustomModelChoiceIterator(ModelChoiceIterator):\n            return obj.url\n\n    class CustomModelChoiceField(forms.ModelChoiceField):\n        iterator = CustomModelChoiceIterator\n\n    field = CustomModelChoiceField(Category.objects.all())\n    self.assertEqual(list(field.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'entertainment'),\n        (self.c2.pk, 'test'),\n        (self.c3.pk, 'third'),\n    ])\n", "def test_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'slug__contains': 'test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=lambda: {'slug__contains': 'test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n"], "sample_448": ["def test_contains_expressions(self):\n    constraint_with_expressions = models.UniqueConstraint(\n        Lower(\"title\"),\n        F(\"author\"),\n        name=\"book_func_uq\",\n    )\n    constraint_without_expressions = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique_fields\",\n    )\n    self.assertTrue(constraint_with_expressions.contains_expressions)\n    self.assertFalse(constraint_without_expressions.contains_expressions)\n", "    def test_contains_expressions(self):\n        constraint_with_expressions = models.UniqueConstraint(\n            Lower(\"title\"),\n            F(\"author\"),\n            name=\"book_func_uq\",\n        )\n        constraint_without_expressions = models.UniqueConstraint(\n            fields=[\"foo\", \"bar\"],\n            name=\"unique_fields\",\n        )\n        self.assertIs(constraint_with_expressions.contains_expressions, True)\n        self.assertIs(constraint_without_expressions.contains_expressions, False)\n", "def test_custom_violation_error_message_unique_constraint(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"],\n        name=\"custom_name_uniq\",\n        violation_error_message=\"custom %(name)s error\",\n    )\n    self.assertEqual(\n        constraint.get_violation_error_message(),\n        \"custom custom_name_uniq error\",\n    )\n", "def test_validate_expression_with_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    product = UniqueConstraintProduct(name=self.p1.name.upper())\n    # Field from the expression is excluded.\n    constraint.validate(UniqueConstraintProduct, product, exclude={\"name\"})\n", "def test_include_in_unique_constraint(self):\n    constraint = UniqueConstraintInclude._meta.constraints[0]\n    non_unique_product = UniqueConstraintInclude(name=self.p1.name, color=self.p1.color)\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintInclude, non_unique_product)\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintInclude, self.p1)\n    # Unique fields are excluded.\n    constraint.validate(UniqueConstraintInclude, non_unique_product, exclude={\"name\"})\n    constraint.validate(UniqueConstraintInclude, non_unique_product, exclude={\"color\"})\n    constraint.validate(UniqueConstraintInclude, non_unique_product, exclude={\"name\", \"color\"})\n    # Included fields are not considered for uniqueness.\n    constraint.validate(UniqueConstraintInclude, UniqueConstraintInclude(name=self.p1.name, color=\"different\"))\n"], "sample_17": ["def test_inv(self):\n    inv = np.linalg.inv(self.q)\n    expected = np.linalg.inv(self.q.value) / self.q.unit\n    assert_array_equal(inv, expected)\n\n    # Test with a singular matrix\n    q = np.zeros((3, 3)) * u.m\n    with pytest.raises(np.linalg.LinAlgError):\n        np.linalg.inv(q)\n", "def test_rec_functions_stack_arrays(self):\n    arr1 = np.array([(1, 2, 3)], dtype=self.pv_dtype) * self.pv_unit\n    arr2 = np.array([(4, 5, 6)], dtype=self.pv_dtype) * self.pv_unit\n    stacked = rfn.stack_arrays((arr1, arr2), asrecarray=True, usemask=False, autoconvert=True)\n    expected = np.array([(1, 2, 3), (4, 5, 6)], dtype=self.pv_dtype) * self.pv_unit\n    assert_array_equal(stacked, expected)\n", "def test_structured_to_unstructured_quantity(self):\n    q = u.Quantity([(1, 2), (3, 4)], dtype=[('a', int), ('b', float)])\n    result = rfn.structured_to_unstructured(q)\n    expected = np.array([1, 2, 3, 4])\n    assert_array_equal(result.value, expected)\n", "def test_rec_functions_structured_to_unstructured(self):\n    unstruct = rfn.structured_to_unstructured(self.q_pv)\n    expected = np.array([self.q_pv[\"p\"].value, self.q_pv[\"v\"].value]).T << self.q_pv.unit\n    assert_array_equal(unstruct, expected)\n\n    # Also test with a nested structured array\n    unstruct = rfn.structured_to_unstructured(self.q_pv_t)\n    expected = np.array([self.q_pv_t[\"pv\"][\"pp\"].value, self.q_pv_t[\"pv\"][\"vv\"].value, self.q_pv_t[\"t\"].value]).T << self.q_pv_t.unit\n    assert_array_equal(unstruct, expected)\n", "def test_structured_to_unstructured(self):\n    # Test with a structured quantity\n    struct = u.Quantity([(1.0, 2.0), (3.0, 4.0)], dtype=[('a', float), ('b', float)], unit='m')\n    unstruct = rfn.structured_to_unstructured(struct)\n    assert_array_equal(unstruct, [1.0, 2.0, 3.0, 4.0] * u.m)\n\n    # Test with a structured quantity and flatten=False\n    struct = u.Quantity([(1.0, 2.0), (3.0, 4.0)], dtype=[('a', float), ('b', float)], unit='m')\n    unstruct = rfn.structured_to_unstructured(struct, flatten=False)\n    assert_array_equal(unstruct, [(1.0, 2.0), (3.0, 4.0)] * u.m)\n\n    # Test with a structured quantity and dtype=None\n    struct = u.Quantity([(1.0, 2.0), (3.0, 4.0)], dtype=[('a', float), ('b', float)], unit='m')\n    with pytest.raises(ValueError, match=\"dtype must be specified\"):\n        rfn.structured_to_unstructured(struct, dtype=None)\n\n    # Test with a structured quantity and dtype not matching the quantity\n    struct = u.Quantity([(1.0, 2.0), (3.0, 4.0)], dtype=[('a', float), ('b', float)], unit='m')\n    with pytest.raises(ValueError, match=\"dtype does not match the quantity\"):\n        rfn.structured_to_unstructured(struct, dtype=[('x', float), ('y', float)])\n"], "sample_760": ["def test_scorer_custom_function():\n    # Test that custom scoring functions can be used\n        return np.mean(y_true == y_pred)\n\n    X, y = make_classification(random_state=0)\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    scorer = make_scorer(custom_scorer)\n    score = scorer(clf, X, y)\n    assert_almost_equal(score, custom_scorer(y, clf.predict(X)))\n", "def test_scorer_with_custom_metrics(metric, expected_score):\n    estimator = EstimatorWithFitAndPredict()\n    estimator.fit([[1], [2], [3]], [1, 1, 0])\n\n    scorer = make_scorer(metric)\n    assert_almost_equal(scorer(estimator, [[1], [2], [3]], [1, 0, 0]), expected_score)\n", "def test_unsupervised_cluster_scorers():\n    # Test clustering scorers against each other.\n    X, _ = make_blobs(random_state=0, centers=2)\n    km = KMeans(n_clusters=2)\n    km.fit(X)\n    labels_true = km.labels_\n    labels_pred = km.predict(X)\n    for name1 in CLUSTER_SCORERS:\n        for name2 in CLUSTER_SCORERS:\n            score1 = get_scorer(name1)(km, X, labels_true)\n            score2 = get_scorer(name2)(km, X, labels_pred)\n            assert_almost_equal(score1, score2)\n", "def test_scorer_with_custom_metric():\n    # Test check_scoring with a custom metric\n        return np.mean(y_true == y_pred)\n\n    estimator = EstimatorWithFitAndPredict()\n    estimator.fit([[1], [2], [3]], [1, 0, 1])\n\n    scorer = check_scoring(estimator, custom_metric)\n    assert isinstance(scorer, _PredictScorer)\n    assert_almost_equal(scorer(estimator, [[1], [2], [3]], [1, 0, 0]), 0.6666667)\n\n    # Test that check_scoring raises ValueError for custom scorer\n        return custom_metric(y, est.predict(X))\n\n    with pytest.raises(ValueError, match=\"looks like it is a metric function\"):\n        check_scoring(estimator, custom_scorer)\n", "def test_scorer_multiclass_label_encoder():\n    # Test that scorers handle label encoding correctly\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3,\n                               n_informative=2, n_redundant=10, n_clusters_per_class=1,\n                               class_sep=0.5, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    # Encode the labels as integers 2, 3, 4\n    y_train += 2\n    y_test += 2\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X_train, y_train)\n\n    for name in CLF_SCORERS:\n        score1 = get_scorer(name)(clf, X_test, y_test)\n        score2 = get_scorer(name)(clf, X_test, y_test - 2)\n        assert_almost_equal(score1, score2)\n"], "sample_657": ["def test_pytest_param_unknown_kwargs(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [1, 2], unknown_kwarg=True)\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-Werror\")\n    out = capsys.readouterr().err\n    assert \"PytestUnknownMarkWarning: Unknown parameters: {'unknown_kwarg'} passed to parametrize.\" in out\n    assert result.ret == 1\n", "def test_pytest_param_id_allows_none_or_string_with_id(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"a\", [pytest.param(1, id=\"one\"), pytest.param(2, id=\"two\")])\n            pass\n    \"\"\"\n    )\n    reprec = testdir.inline_run(p)\n    reprec.assertoutcome(passed=2)\n    test_ids = [item.nodeid.split(\"::\")[-1] for item in reprec.getcalls(\"pytest_runtest_call\")[0].item.session.items]\n    assert test_ids == [\"test_func[one]\", \"test_func[two]\"]\n", "def test_pytest_param_unknown_kwargs(recwarn):\n    pytest.param(foo=1)\n    warning = recwarn.pop()\n    assert issubclass(warning.category, pytest.PytestWarning)\n    assert \"PytestParamUnknownKwargs\" in str(warning.message)\n", "def test_mark_decorator_order_of_application(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.a\n        @pytest.mark.b(1, x=2)\n        @pytest.mark.c(\"3\", y=4)\n        @pytest.mark.b(5, z=6)\n            pass\n        \"\"\"\n    )\n    items, rec = testdir.inline_genitems(p)\n    test_func = items[0]\n    marks = list(test_func.iter_markers())\n    assert marks[0].name == \"a\"\n    assert marks[1].name == \"b\"\n    assert marks[1].args == (1, 5)\n    assert marks[1].kwargs == {\"x\": 2, \"z\": 6}\n    assert marks[2].name == \"c\"\n    assert marks[2].args == (\"3\",)\n    assert marks[2].kwargs == {\"y\": 4}\n", "def test_mark_generator_warns_for_unregistered_markers(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.unregisteredmark\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"Unknown pytest.mark.unregisteredmark - is this a typo?\"]\n    )\n"], "sample_346": ["def test_cache_control_decorator(self):\n    @cache_control(max_age=60, no_cache=True, no_store=True, must_revalidate=True, private=True)\n        return HttpResponse()\n    r = a_view(HttpRequest())\n    self.assertEqual(\n        set(r.headers['Cache-Control'].split(', ')),\n        {'max-age=60', 'no-cache', 'no-store', 'must-revalidate', 'private'},\n    )\n", "    def test_classonlymethod(self):\n        class MyClass:\n            @classonlymethod\n                return 'class method'\n\n        with self.assertRaises(AttributeError):\n            MyClass().my_method()\n\n        self.assertEqual(MyClass.my_method(), 'class method')\n", "    def test_cache_page_decorator_http_request(self):\n        class MyClass:\n            @cache_page(60 * 15)\n                return HttpResponse()\n        msg = (\n            \"cache_page didn't receive an HttpRequest. If you are decorating \"\n            \"a classmethod, be sure to use @method_decorator.\"\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            MyClass().a_view(HttpRequest())\n", "    def test_cache_page_decorator_http_request(self):\n        class MyClass:\n            @cache_page(60 * 15)\n                return HttpResponse()\n        msg = (\n            \"cache_page didn't receive an HttpRequest. If you are decorating \"\n            \"a classmethod, be sure to use @method_decorator.\"\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            MyClass().a_view(HttpRequest())\n", "    def test_sync_and_async_middleware_decorator(self):\n        @sync_and_async_middleware\n            class Middleware:\n                    self.get_response = get_response\n\n                    return self.get_response(request)\n\n            return Middleware(*args, **kwargs)\n\n        self.assertTrue(a_middleware_factory.sync_capable)\n        self.assertTrue(a_middleware_factory.async_capable)\n"], "sample_922": ["def test_pyfunction_signature_default_arglist(app):\n    text = \".. py:function:: hello(name='World') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"'World'\"])])\n", "def test_pyclass_signature(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"   :noindex:\\n\"\n            \"   \\n\"\n            \"   .. py:method:: my_method(self, arg1, arg2=None)\\n\"\n            \"      :noindex:\\n\"\n            \"      \\n\"\n            \"      This is the docstring for my_method.\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"MyClass\"])],\n                                  [desc_content, desc])))\n    assert_node(doctree[1], addnodes.desc, desctype=\"class\",\n                domain=\"py\", objtype=\"class\", noindex=True)\n\n    assert 'MyClass' in domain.objects\n    assert domain.objects['MyClass'] == ('index', 'MyClass', 'class')\n    assert 'MyClass.my_method' in domain.objects\n    assert domain.objects['MyClass.my_method'] == ('index', 'MyClass.my_method', 'method')\n", "def test_pyexception_inherited(app):\n    text = \".. py:exception:: MyException\\n   :base-exception: IOError\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"MyException\"],\n                                                    [desc_parameterlist, ([desc_parameter, pending_xref, \"IOError\"])]\n                                                    )],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n    assert_node(doctree[1][0][1][2], desc_parameterlist, ([desc_parameter, pending_xref, \"IOError\"]))\n    assert_node(doctree[1][0][1][2][0], desc_parameter, ([pending_xref, \"IOError\"]))\n    assert_node(doctree[1][0][1][2][0][0], pending_xref, refdomain=\"py\", reftype=\"exc\", reftarget=\"IOError\")\n", "def test_pyattribute_signature_old(app):\n    text = (\".. py:attribute:: attr\\n\"\n            \"   :annotation: = ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, \" = ''\"])],\n                                  [desc_content, ()])]))\n    assert 'attr' in domain.objects\n    assert domain.objects['attr'] == ('index', 'attr', 'attribute')\n", "def test_pydecoratormethod_signature_without_parens(app):\n    text = \".. py:decoratormethod:: deco()\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"deco\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"method\",\n                domain=\"py\", objtype=\"method\", noindex=False)\n\n    assert 'deco' in domain.objects\n    assert domain.objects['deco'] == ('index', 'deco', 'method')\n"], "sample_314": ["    def test_validates_password(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'testclient',\n            'password2': 'testclient',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form[\"password2\"].errors), 2)\n        self.assertIn('The password is too similar to the username.', form[\"password2\"].errors)\n        self.assertIn(\n            'This password is too short. It must contain at least 12 characters.',\n            form[\"password2\"].errors\n        )\n", "    def test_custom_user_model(self):\n        data = {\n            'email': 'testclient@example.com',\n            'password1': 'testclient',\n            'password2': 'testclient',\n            'date_of_birth': '1988-02-24',\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.email, 'testclient@example.com')\n        self.assertEqual(user.date_of_birth, datetime.date(1988, 2, 24))\n", "    def test_validates_password(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'password',  # This password is too similar to the username.\n            'password2': 'password',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form[\"password2\"].errors), 1)\n        self.assertIn('The password is too similar to the username.', form[\"password2\"].errors)\n", "def test_integer_username_in_authentication_form(self):\n    class CustomAuthenticationForm(AuthenticationForm):\n        username = IntegerField()\n\n    user = IntegerUsernameUser.objects.create_user(username=123456, password='pwd')\n    data = {\n        'username': 123456,\n        'password': 'pwd',\n    }\n    form = CustomAuthenticationForm(None, data)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['username'], data['username'])\n    self.assertEqual(form.cleaned_data['password'], data['password'])\n    self.assertEqual(form.errors, {})\n    self.assertEqual(form.user_cache, user)\n", "def test_username_field_max_length(self):\n    username = 'a' * 151\n    data = {\n        'username': username,\n        'password1': 'test123',\n        'password2': 'test123',\n    }\n    form = UserCreationForm(data)\n    self.assertFalse(form.is_valid())\n    max_length_error = User._meta.get_field('username').max_length\n    self.assertEqual(form[\"username\"].errors,\n                     [_(\"Ensure this value has at most %(max)s characters (it has %(show_value)s).\") % {'max': max_length_error, 'show_value': len(username)}])\n"], "sample_656": ["def test_suspend_resume_capturing(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            print(\"begin\")\n            capsys.suspend()\n            print(\"suspended\")\n            capsys.resume()\n            print(\"resumed\")\n            captured = capsys.readouterr()\n            assert captured.out == \"begin\\\\nresumed\\\\n\"\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n", "def test_capture_with_live_logging_and_disabled(testdir, capfd):\n    # Issue 3819\n    # capture should work with live cli logging and disabled\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n\n            with capfd.disabled():\n                logging.info(\"this should not be captured\")\n                print(\"disabled\")\n\n            captured = capfd.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n\n            logging.info(\"something\")\n            print(\"next\")\n            logging.info(\"something\")\n\n            captured = capfd.readouterr()\n            assert captured.out == \"next\\\\n\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n", "def test_capture_with_live_logging_and_disabled(testdir, capture_fixture):\n    # Test capture fixture with live logging and disabled capture\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n\n            with {0}.disabled():\n                logging.info(\"this should not be captured\")\n                print(\"not captured\")\n\n            captured = {0}.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n\n            logging.info(\"something\")\n            print(\"next\")\n            logging.info(\"something\")\n\n            captured = {0}.readouterr()\n            assert captured.out == \"next\\\\n\"\n        \"\"\".format(\n            capture_fixture\n        )\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n", "def test_capture_with_live_logging_teardown(testdir, capsys, capfd):\n    # Issue 3819\n    # capture should work with live cli logging in teardown\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            yield\n            logging.info(\"teardown something\")\n            print(\"next\")\n            logging.info(\"teardown something\")\n\n            captured = capsys.readouterr()\n            assert captured.out == \"next\\\\n\"\n            captured = capfd.readouterr()\n            assert captured.err == \"teardown something\\\\n\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n", "def test_capture_with_live_logging_disabled(testdir, caplog, capture_fixture):\n    # Test that live logging can be disabled during test execution\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            with {0}.disabled():\n                logging.info(\"disabled logging\")\n            captured = {0}.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n\n            print(\"next\")\n            logging.info(\"something\")\n\n            captured = {0}.readouterr()\n            assert captured.out == \"next\\\\n\"\n        \"\"\".format(\n            capture_fixture\n        )\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n    assert \"disabled logging\" not in caplog.text\n"], "sample_453": ["def test_cell_count(self):\n    \"\"\"\n    Test the cell_count template tag.\n    \"\"\"\n    class DummyInlineAdminForm:\n            self.formset = formset\n            self.fields = fields\n\n    class DummyField:\n            self.is_hidden = is_hidden\n\n    # Test with a formset that can delete and visible fields\n    formset = type('', (), {'can_delete': True})()\n    fields = [DummyField(False), DummyField(False)]\n    inline_admin_form = DummyInlineAdminForm(formset, fields)\n    self.assertEqual(cell_count(inline_admin_form), 4)\n\n    # Test with a formset that can delete and some hidden fields\n    fields = [DummyField(True), DummyField(False)]\n    inline_admin_form = DummyInlineAdminForm(formset, fields)\n    self.assertEqual(cell_count(inline_admin_form), 3)\n\n    # Test with a formset that cannot delete and visible fields\n    formset = type('', (), {'can_delete': False})()\n    inline_admin_form = DummyInlineAdminForm(formset, fields)\n    self.assertEqual(cell_count(inline_admin_form), 2)\n", "def test_cell_count(self):\n    \"\"\"\n    Test that cell_count template tag returns correct number of cells.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:admin_views_article_add\")\n    )\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    admin.inlines = [InlineModelAdmin(Question, admin_site=site)]\n    response = admin.add_view(request)\n    template_context = response.context_data\n    inline_admin_formset = template_context[\"inline_admin_formsets\"][0]\n    cell_count_result = cell_count(inline_admin_formset.form)\n    # Calculate the expected cell count manually\n    expected_count = 1  # Hidden cell with hidden 'id' field\n    for fieldset in inline_admin_formset.form:\n        for line in fieldset:\n            for field in line:\n                try:\n                    is_hidden = field.field.is_hidden\n                except AttributeError:\n                    is_hidden = field.field[\"is_hidden\"]\n                if not is_hidden:\n                    expected_count += 1\n    if inline_admin_formset.can_delete:\n        # Delete checkbox\n        expected_count += 1\n    self.assertEqual(cell_count_result, expected_count)\n", "def test_cell_count(self):\n    \"\"\"\n    Test cell_count function.\n    \"\"\"\n    from django.contrib.admin.templatetags.admin_modify import cell_count\n    from django.contrib.admin.helpers import AdminForm\n\n    # Create a mock form with some fields\n    class MockField:\n            self.is_hidden = is_hidden\n\n    class MockForm:\n            self.fields = {field: MockField(is_hidden) for field, is_hidden in fields.items()}\n\n    class MockFormset:\n            self.can_delete = can_delete\n\n    # Test with some visible and hidden fields\n    fields = {\"visible1\": False, \"visible2\": False, \"hidden\": True}\n    form = MockForm(fields)\n    formset = MockFormset(can_delete=True)\n    inline_admin_form = AdminForm(form, fieldsets=[(None, {\"fields\": list(fields.keys())})], formset=formset)\n    self.assertEqual(cell_count(inline_admin_form), 4)  # 2 visible + 1 hidden + 1 delete checkbox\n\n    # Test without delete checkbox\n    formset = MockFormset(can_delete=False)\n    inline_admin_form = AdminForm(form, fieldsets=[(None, {\"fields\": list(fields.keys())})], formset=formset)\n    self.assertEqual(cell_count(inline_admin_form), 3)  # 2 visible + 1 hidden\n", "def test_cell_count_with_hidden_fields(self):\n    \"\"\"\n    cell_count template filter should count only visible fields.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:admin_views_article_add\")\n    )\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.add_view(request)\n    inline_admin_form = response.context_data[\"inline_admin_formsets\"][0]\n    hidden_fields = 2\n    visible_fields = 5\n    total_fields = hidden_fields + visible_fields\n    self.assertEqual(cell_count(inline_admin_form), total_fields)\n\n    # Hide a field\n    inline_admin_form[0][0][0].field.is_hidden = True\n    total_fields -= 1\n    self.assertEqual(cell_count(inline_admin_form), total_fields)\n", "    def test_cell_count(self):\n        \"\"\"\n        Test the cell_count template filter.\n        \"\"\"\n        class DummyInlineAdminForm:\n                self.formset = type('', (), {'can_delete': can_delete})()\n                self.fields = fields\n\n                yield self.fields\n\n        # Test with visible and hidden fields\n        fields = [\n            {'field': {'is_hidden': False}},\n            {'field': {'is_hidden': True}},\n        ]\n        inline_admin_form = DummyInlineAdminForm(fields, can_delete=True)\n        self.assertEqual(cell_count(inline_admin_form), 3)  # 1 visible field + 1 delete checkbox + 1 hidden id field\n\n        # Test with only visible fields\n        fields = [\n            {'field': {'is_hidden': False}},\n            {'field': {'is_hidden': False}},\n        ]\n        inline_admin_form = DummyInlineAdminForm(fields, can_delete=False)\n        self.assertEqual(cell_count(inline_admin_form), 3)  # 2 visible fields + 1 hidden id field\n\n        # Test with only hidden fields\n        fields = [\n            {'field': {'is_hidden': True}},\n            {'field': {'is_hidden': True}},\n        ]\n        inline_admin_form = DummyInlineAdminForm(fields, can_delete=False)\n        self.assertEqual(cell_count(inline_admin_form), 1)  # Only 1 hidden id field\n\n        # Test with no fields and can_delete=True\n        inline_admin_form = DummyInlineAdminForm([], can_delete=True)\n        self.assertEqual(cell_count(inline_admin_form), 2)  # 1 delete checkbox + 1 hidden id field\n"], "sample_171": ["def test_squashmigrations_app_name_specified_as_label(self):\n    err = io.StringIO()\n    with self.assertRaises(SystemExit):\n        call_command('squashmigrations', 'django.contrib.auth', '0002', stderr=err)\n    self.assertIn(self.did_you_mean_auth_error, err.getvalue())\n", "def test_migrate_conflict_exit_without_merge(self):\n    \"\"\"\n    migrate exits if it detects a conflict and merge is False.\n    \"\"\"\n    with self.temporary_migration_module(module=\"migrations.test_migrations_conflict\"):\n        with self.assertRaises(CommandError) as context:\n            call_command(\"migrate\", \"migrations\", merge=False)\n    exception_message = str(context.exception)\n    self.assertIn(\"Conflicting migrations detected\", exception_message)\n    self.assertIn(\"0002_second\", exception_message)\n    self.assertIn(\"0002_conflicting_second\", exception_message)\n    self.assertIn(\"in migrations\", exception_message)\n    self.assertIn(\"To fix them run 'python manage.py migrate --merge'\", exception_message)\n", "def test_squashmigrations_app_name_specified_as_label(self):\n    \"\"\"--squashed-name also works if a start migration is omitted.\"\"\"\n    with self.temporary_migration_module(module=\"migrations.test_migrations\") as migration_dir:\n        msg = (\n            \"No installed app with label 'django.contrib.auth'. Did you mean \"\n            \"'auth'?\"\n        )\n        with self.assertRaisesMessage(CommandError, msg):\n            call_command('squashmigrations', 'django.contrib.auth', '0001')\n", "def test_makemigrations_with_custom_name_and_squash(self):\n    \"\"\"\n    makemigrations --name and --squash work together.\n    \"\"\"\n    with self.temporary_migration_module() as migration_dir:\n        call_command(\"makemigrations\", \"migrations\", \"--verbosity\", \"0\", \"--name\", \"my_custom_migration\")\n        call_command(\"squashmigrations\", \"migrations\", \"0001\", squashed_name=\"my_squashed_migration\")\n\n        migration_file = os.path.join(migration_dir, \"0001_my_squashed_migration.py\")\n        self.assertTrue(os.path.exists(migration_file))\n        with open(migration_file, encoding='utf-8') as fp:\n            content = fp.read()\n            self.assertIn(\"0001_my_custom_migration\", content)\n", "def test_makemigrations_with_nonexistent_custom_name(self):\n    \"\"\"\n    makemigrations --name raises a CommandError if the migration name\n    doesn't exist.\n    \"\"\"\n    msg = \"The migration 'migrations.0003_nonexistent' cannot be found.\"\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command('makemigrations', 'migrations', '--name', '0003_nonexistent')\n"], "sample_1208": ["def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy is not installed. Abort tests for _sample_numpy.')\n    else:\n        # Currently, numpy_rv_map is empty in SampleMatrixNumpy class.\n        # Once matrix distributions are added to numpy_rv_map, uncomment the following lines to test.\n        # distribs_numpy = [\n        #     MatrixNormal('M', [[5, 6]], [4], [[2, 1], [1, 2]]),\n        #     Wishart('W', 5, [[1, 0], [0, 1]])\n        # ]\n        # size = 5\n        # for X in distribs_numpy:\n        #     samps = sample(X, size=size, library='numpy')\n        #     for sam in samps:\n        #         assert Matrix(sam) in X.pspace.distribution.set\n", "def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy is not installed. Abort tests for _sample_numpy.')\n    else:\n        # Note: Currently, numpy_rv_map is empty in SampleMatrixNumpy class,\n        # so it will raise NotImplementedError for any distribution.\n        M = MatrixNormal('M', [[5, 6]], [4], [[2, 1], [1, 2]])\n        raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n", "def test_MatrixGamma_symbolic_sample():\n    a, b = symbols('a b', positive=True)\n    d = symbols('d', positive=True, integer=True)\n    SM = MatrixSymbol('SM', d, d)\n    M2 = MatrixGamma('M2', a, b, SM)\n    size = 5\n    scipy = import_module('scipy')\n    pymc = import_module('pymc')\n    if not scipy and not pymc:\n        skip('Scipy and PyMC are not installed. Abort test for symbolic sampling.')\n    else:\n        for library in ['scipy', 'pymc']:\n            if import_module(library):\n                samps = sample(M2, size=size, library=library)\n                for sam in samps:\n                    assert Matrix(sam) in M2.pspace.distribution.set\n", "def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy is not installed. Abort tests for _sample_numpy.')\n    else:\n        # Currently, numpy_rv_map is empty in SampleMatrixNumpy, so the test is skipped.\n        # When there are distributions added to numpy_rv_map, uncomment the lines below and add appropriate tests.\n        # distribs_numpy = [\n        #     # Add distributions here that are supported by NumPy\n        # ]\n        # size = 3\n        # for X in distribs_numpy:\n        #     samps = sample(X, size=size, library='numpy')\n        #     for sam in samps:\n        #         assert Matrix(sam) in X.pspace.distribution.set\n", "def test_MatrixGamma_sample():\n    M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n    samps = sample(M, size=5, library='scipy')\n    for sam in samps:\n        assert Matrix(sam) in M.pspace.distribution.set\n    raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n    raises(NotImplementedError, lambda: sample(M, size=3, library='pymc'))\n    raises(ValueError, lambda: sample(M, size=3, library='invalid_library'))\n"], "sample_1164": ["def test_cg_simp():\n    cg1 = CG(1, 1, 0, 0, 1, 1)\n    cg2 = CG(1, 0, 0, 0, 1, 0)\n    cg3 = CG(1, -1, 0, 0, 1, -1)\n    assert str(cg_simp(cg1 + cg2 + cg3)) == \"3\"\n    assert pretty(cg_simp(cg1 + cg2 + cg3)) == \"3\"\n    assert upretty(cg_simp(cg1 + cg2 + cg3)) == \"3\"\n    assert latex(cg_simp(cg1 + cg2 + cg3)) == \"3\"\n    sT(cg_simp(cg1 + cg2 + cg3), \"Integer(3)\")\n\n    cg4 = CG(1, 1, 1, 0, 2, 1)\n    cg5 = CG(1, 0, 1, 0, 2, 0)\n    cg6 = CG(1, -1, 1, 0, 2, -1)\n    assert str(cg_simp(cg4 + cg5 + cg6)) == \"3/2\"\n    assert pretty(cg_simp(cg4 + cg5 + cg6)) == \"3/2\"\n    assert upretty(cg_simp(cg4 + cg5 + cg6)) == \"3/2\"\n    assert latex(cg_simp(cg4 + cg5 + cg6)) == \"\\\\frac{3}{2}\"\n    sT(cg_simp(cg4 + cg5 + cg6), \"Rational(3, 2)\")\n", "def test_cg_simp():\n    # Test the simplification of CG coefficients\n    a, b, c = symbols('a b c')\n    e = CG(a, a, b, -b, c, 0) + CG(a, -a, b, b, c, 0)\n    simp_e = cg_simp(e)\n    assert simp_e == sqrt(2*a + 1)*KroneckerDelta(c, 0)\n", "def test_cg_simp():\n    a = symbols('a')\n    alpha = symbols('alpha')\n    b = symbols('b')\n    c = symbols('c')\n\n    # Test simplification of CG coefficients\n    cg1 = CG(a, alpha, b, 0, a, alpha)\n    simplified_cg1 = cg_simp(cg1)\n    assert simplified_cg1 == 2*a + 1\n\n    cg2 = CG(a, alpha, a, -alpha, c, 0)\n    simplified_cg2 = cg_simp(cg2)\n    assert simplified_cg2 == sqrt(2*a + 1)*KroneckerDelta(c, 0)\n\n    # Add more assertions for other simplifications if they exist\n", "def test_cg_simp():\n    # Adding tests for the cg_simp function\n    a = symbols('a')\n    b = symbols('b')\n    c = symbols('c')\n\n    # Test for Varsh_871_1\n    expr1 = Sum(CG(a, alpha, b, 0, a, alpha), (alpha, -a, a))\n    result1 = cg_simp(expr1)\n    assert result1 == 2*a + 1\n\n    # Test for Varsh_871_2\n    expr2 = Sum((-1)**(a-alpha)*CG(a, alpha, a, -alpha, c, 0), (alpha, -a, a))\n    result2 = cg_simp(expr2)\n    assert result2 == sqrt(2*a + 1)*KroneckerDelta(c, 0)\n\n    # Test for Varsh_872_9\n    expr3 = Sum(CG(a, alpha, b, beta, c, gamma)*CG(a, alpha, b, beta, c, gamma), (alpha, -a, a), (beta, -b, b))\n    result3 = cg_simp(expr3)\n    assert result3 == 1\n", "def test_cg_simp():\n    cg1 = CG(1, 1, 0, 0, 1, 1)\n    cg2 = CG(1, 0, 0, 0, 1, 0)\n    cg3 = CG(1, -1, 0, 0, 1, -1)\n    simplified_sum = cg_simp(cg1 + cg2 + cg3)\n    assert simplified_sum == 3\n    sT(simplified_sum, \"3\")\n"], "sample_1122": ["def test_issue_14239():\n    # does not cause recursion error\n    x = Symbol('x')\n    assert Abs(x + Piecewise((0, x > 0), (1 - x, True)))\n", "def test_polar_lift():\n    from sympy import polar_lift, I, pi\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n\n    x = Symbol('x')\n    p = Symbol('p', polar=True)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n\n    a, b = symbols('a b', real=True)\n    assert polar_lift(a + b*I).is_polar\n    assert polar_lift(a - b*I).is_polar\n    assert polar_lift(-a + b*I).is_polar\n    assert polar_lift(-a - b*I).is_polar\n", "def test_abs_derivative():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    assert Derivative(Abs(x), x) == sign(x)\n    assert Derivative(Abs(y), y) == I*sign(y)\n    assert Derivative(Abs(x + y), x) == re(x + y) / Abs(x + y)\n    assert Derivative(Abs(x + y), y) == I*im(x + y) / Abs(x + y)\n", "def test_Abs_properties_complex():\n    z = Symbol('z', complex=True)\n    assert Abs(z).is_real is True\n    assert Abs(z).is_extended_real is True\n    assert Abs(z).is_rational is None\n    assert Abs(z).is_positive is None\n    assert Abs(z).is_nonnegative is None\n    assert Abs(z).is_extended_positive is None\n    assert Abs(z).is_extended_nonnegative is None\n    assert Abs(z).is_zero is None\n\n    z_pos = Symbol('z_pos', complex=True, positive=True)\n    assert Abs(z_pos).is_real is True\n    assert Abs(z_pos).is_extended_real is True\n    assert Abs(z_pos).is_rational is None\n    assert Abs(z_pos).is_positive is True\n    assert Abs(z_pos).is_nonnegative is None\n    assert Abs(z_pos).is_extended_positive is None\n    assert Abs(z_pos).is_extended_nonnegative is None\n    assert Abs(z_pos).is_zero is False\n\n    z_zero = Symbol('z_zero', complex=True, zero=True)\n    assert Abs(z_zero).is_real is True\n    assert Abs(z_zero).is_extended_real is True\n    assert Abs(z_zero).is_rational is None\n    assert Abs(z_zero).is_positive is None\n    assert Abs(z_zero).is_nonnegative is True\n    assert Abs(z_zero).is_extended_positive is False\n    assert Abs(z_zero).is_extended_nonnegative is True\n    assert Abs(z_zero).is_zero is True\n", "def test_Abs_nseries():\n    x = Symbol('x')\n    assert Abs(x).nseries(x, n=2) == Abs(x)\n    assert Abs(x + 1).nseries(x, n=2) == Abs(x) + 1\n    assert Abs(x - 1).nseries(x, n=2) == Piecewise((2, Eq(x, 1)), (2*x - 2, True))\n    assert Abs(x**2).nseries(x, n=2) == Abs(x)**2\n    assert Abs(x**3).nseries(x, n=2) == Abs(x)**3\n    assert Abs(sqrt(x)).nseries(x, n=2) == Abs(x)**Rational(1, 2)\n    assert Abs(sqrt(x + 1)).nseries(x, n=2) == sqrt(2) + sqrt(x)/2 - sqrt(2)*x/8 + O(x**2)\n    assert Abs(sin(x)).nseries(x, n=2) == x + O(x**3)\n"], "sample_78": ["    def test_call_command_with_unknown_app_label(self):\n        msg = \"No installed app with label 'unknown_app'. Are you sure your INSTALLED_APPS setting is correct?\"\n        with self.assertRaisesMessage(CommandError, msg):\n            management.call_command('hal', 'unknown_app')\n", "    def test_call_command_with_invalid_database_option(self):\n        msg = \"Error: database option not recognized: 'invalid_database'\"\n        with self.assertRaisesMessage(CommandError, msg):\n            management.call_command('dance', database='invalid_database')\n", "    def test_call_command_no_translations(self):\n        \"\"\"\n        Test that the command is called with translations deactivated when\n        decorated with @no_translations.\n        \"\"\"\n        out = StringIO()\n        management.call_command('no_translations', stdout=out)\n        self.assertIn(\"Translations are deactivated inside the command.\", out.getvalue())\n", "def test_call_command_with_required_parameters_in_positional_arguments(self):\n    out = StringIO()\n    management.call_command('required_option', 'foo', 'bar', stdout=out)\n    self.assertIn('need_me', out.getvalue())\n    self.assertIn('needme2', out.getvalue())\n", "def test_call_command_with_invalid_integer_option(self):\n    msg = \"argument --integer: invalid int value: 'invalid'\"\n    with self.assertRaisesMessage(CommandError, msg):\n        management.call_command('dance', '--integer', 'invalid')\n"], "sample_882": ["def test_mlp_estimator_n_features_in_(MLPEstimator):\n    # Test that n_features_in_ is set correctly\n    mlp = MLPEstimator(max_iter=1)\n    mlp.fit(X_iris, y_iris)\n    assert mlp.n_features_in_ == X_iris.shape[1]\n", "def test_mlp_classifier_with_custom_activation():\n    # Test that MLPClassifier works with custom activation functions\n        return np.exp(-np.logaddexp(0, -x))\n\n        return x * (1 - x)\n\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n\n    mlp = MLPClassifier(hidden_layer_sizes=50, activation=custom_activation, max_iter=1000, random_state=1)\n    mlp.fit(X, y)\n    assert mlp.score(X, y) > 0.95\n", "def test_mlp_learning_rate_init(MLPEstimator):\n    \"\"\"Check that learning_rate_init is used in the solver.\"\"\"\n    mlp = MLPEstimator(learning_rate_init=0.1, random_state=0, max_iter=1)\n    mlp.fit(X_iris, y_iris)\n    assert mlp._optimizer.learning_rate == 0.1\n", "def test_mlp_max_fun_lbfgs(MLPEstimator):\n    \"\"\"Check that max_fun parameter is respected for lbfgs solver.\"\"\"\n    mlp = MLPEstimator(solver=\"lbfgs\", max_fun=10)\n    mlp.fit(X_iris, y_iris)\n    assert mlp.n_iter_ <= 10\n", "def test_mlp_invalid_hidden_layer_sizes():\n    # Test that the algorithm raises an error when invalid hidden_layer_sizes is provided\n    X = np.array([[0.6, 0.8, 0.7]])\n    y = np.array([0])\n\n    with pytest.raises(ValueError) as excinfo:\n        MLPClassifier(hidden_layer_sizes=0).fit(X, y)\n    assert \"hidden_layer_sizes must be > 0, got 0.\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        MLPClassifier(hidden_layer_sizes=-1).fit(X, y)\n    assert \"hidden_layer_sizes must be > 0, got -1.\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        MLPClassifier(hidden_layer_sizes=(-1, 2)).fit(X, y)\n    assert \"hidden_layer_sizes must be > 0, got (-1, 2).\" in str(excinfo.value)\n"], "sample_347": ["def test_localtime(self):\n    naive = datetime.datetime(2015, 1, 1, 0, 0, 1)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n\n    aware = datetime.datetime(2015, 1, 1, 0, 0, 1, tzinfo=ICT)\n    self.assertEqual(timezone.localtime(aware, timezone=EAT), datetime.datetime(2014, 12, 31, 18, 0, 1, tzinfo=EAT))\n    with timezone.override(EAT):\n        self.assertEqual(timezone.localtime(aware), datetime.datetime(2014, 12, 31, 18, 0, 1, tzinfo=EAT))\n\n    with mock.patch('django.utils.timezone.now', return_value=aware):\n        self.assertEqual(timezone.localtime(timezone=EAT), datetime.datetime(2014, 12, 31, 18, 0, 1, tzinfo=EAT))\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(), datetime.datetime(2014, 12, 31, 18, 0, 1, tzinfo=EAT))\n", "def test_localtime_naive(self):\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n", "def test_localtime(self):\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n\n    aware = datetime.datetime(2011, 9, 1, 13, 20, 30, tzinfo=ICT)\n    self.assertEqual(\n        timezone.localtime(aware, timezone=EAT),\n        datetime.datetime(2011, 9, 1, 5, 20, 30, tzinfo=EAT))\n    with timezone.override(EAT):\n        self.assertEqual(\n            timezone.localtime(aware),\n            datetime.datetime(2011, 9, 1, 5, 20, 30, tzinfo=EAT))\n\n    with mock.patch('django.utils.timezone.now', return_value=aware):\n        self.assertEqual(\n            timezone.localtime(timezone=EAT),\n            datetime.datetime(2011, 9, 1, 5, 20, 30, tzinfo=EAT))\n        with timezone.override(EAT):\n            self.assertEqual(\n                timezone.localtime(),\n                datetime.datetime(2011, 9, 1, 5, 20, 30, tzinfo=EAT))\n", "def test_localtime_naive_datetime(self):\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    with self.assertRaises(ValueError):\n        timezone.localtime(naive)\n", "def test_get_current_timezone(self):\n    with override_settings(TIME_ZONE='Europe/Paris'):\n        self.assertIsInstance(timezone.get_current_timezone(), pytz.tzinfo.BaseTzInfo)\n        self.assertEqual(timezone.get_current_timezone_name(), 'Europe/Paris')\n\n    with timezone.override('Asia/Bangkok'):\n        self.assertIsInstance(timezone.get_current_timezone(), datetime.tzinfo)\n        self.assertEqual(timezone.get_current_timezone_name(), 'Asia/Bangkok')\n\n    timezone.deactivate()\n    self.assertIsInstance(timezone.get_current_timezone(), pytz.tzinfo.BaseTzInfo)\n    self.assertEqual(timezone.get_current_timezone_name(), 'America/Chicago')\n"], "sample_397": ["def test_templatetag_discovery_unicode_error(self):\n    \"\"\"\n    Unicode errors in tag modules should be reraised with a helpful message.\n    \"\"\"\n    with self.assertRaisesMessage(\n        InvalidTemplateLibrary,\n        \"UnicodeDecodeError raised when trying to load \"\n        \"'template_backends.apps.unicodeerror.templatetags.broken_tags'\",\n    ) as cm:\n        DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        )\n    self.assertIsInstance(cm.exception.__cause__, UnicodeDecodeError)\n", "def test_templatetag_discovery_invalid_library(self):\n    \"\"\"\n    Invalid libraries should be detected and a helpful message should be raised.\n    \"\"\"\n    with self.assertRaisesMessage(\n        InvalidTemplateLibrary,\n        \"Invalid template library specified. ImportError raised when trying to \"\n        \"load 'template_backends.apps.invalid.templatetags.invalid_tags': \"\n        \"cannot import name 'nonexistent_function' from \"\n        \"'template_backends.apps.invalid.templatetags.invalid_tags'\",\n    ) as cm:\n        DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        )\n    self.assertIsInstance(cm.exception.__cause__, ImportError)\n", "def test_app_dirs_false_with_loaders_defined(self):\n    \"\"\"\n    Raise an error if app_dirs is True when loaders is defined.\n    \"\"\"\n    with self.assertRaises(ImproperlyConfigured):\n        DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": True,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        \"django.template.loaders.filesystem.Loader\",\n                    ],\n                },\n            }\n        )\n", "def test_templatetag_discovery_invalid_library(self):\n    \"\"\"\n    Invalid libraries should raise an ImproperlyConfigured error.\n    \"\"\"\n    with self.assertRaisesMessage(\n        ImproperlyConfigured,\n        \"Invalid template library specified. ImportError raised when trying to load \"\n        \"'template_backends.apps.invalid.templatetags.invalid_tags': \"\n        \"No module named 'invalid_module'\",\n    ):\n        DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        )\n", "def test_file_charset_configuration(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"file_charset\": \"utf-16\",\n            },\n        }\n    )\n\n    self.assertEqual(engine.engine.file_charset, \"utf-16\")\n"], "sample_390": ["    def test_directory_listing_disallowed(self):\n        response = self.client.get(\"/%s/subdir\" % self.prefix)\n        self.assertEqual(404, response.status_code)\n        self.assertEqual(\"Directory indexes are not allowed here.\", response.content.decode())\n", "def test_directory_index_template_translation(self):\n    \"\"\"\n    Test that the directory index template is correctly translated.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='es'):\n        response = self.client.get(\"/%s/\" % self.prefix)\n        self.assertContains(response, \"\u00cdndice de ./\")\n", "def test_directory_index_template(self):\n    # Test if the directory_index function returns the correct response\n    # using the default template\n    path = \"subdir\"\n    fullpath = Path(media_dir) / path\n    response = directory_index(path, fullpath)\n    self.assertContains(response, \"Index of subdir/\")\n    self.assertContains(response, '<a href=\"visible\">visible</a>')\n    self.assertNotContains(response, '<a href=\".hidden\">.hidden</a>')\n", "def test_directory_index_template_fallback(self):\n    # Test that the default directory index template is used when a custom one is not provided\n    file_name = \"subdir/\"\n    response = self.client.get(\"/%s/%s\" % (self.prefix, file_name))\n    self.assertIn(\"Index of subdir/\", str(response.content))\n", "    def test_directory_index_template_does_not_exist(self):\n        \"\"\"\n        Check if the default directory index template is used when the custom template does not exist.\n        \"\"\"\n        with self.settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {\n                        \"loaders\": [\n                            (\n                                \"django.template.loaders.locmem.Loader\",\n                                {\n                                    \"static/directory_index.html\": \"\",\n                                },\n                            ),\n                        ],\n                    },\n                }\n            ]\n        ):\n            response = self.client.get(\"/%s/\" % self.prefix)\n            self.assertContains(response, \"<title>Index of /</title>\")\n"], "sample_386": ["def test_safe_string_is_immutable(self):\n    \"\"\"\n    SafeString is an immutable type.\n    \"\"\"\n    s = mark_safe(\"a&b\")\n    with self.assertRaises(AttributeError):\n        s.replace(\"a\", \"c\")\n", "    def test_mark_safe_decorator_with_arguments(self):\n        \"\"\"\n        mark_safe used as a decorator works correctly with functions that take arguments.\n        \"\"\"\n\n            return f\"<html><body>{arg1} {arg2}</body></html>\"\n\n        self.assertEqual(\n            mark_safe(clean_string_provider)(\"dummy1\", \"dummy2\"),\n            clean_string_provider(\"dummy1\", \"dummy2\")\n        )\n", "def test_mark_safe_decorator_with_args(self):\n    \"\"\"\n    mark_safe used as a decorator with arguments leaves the result of a function\n    unchanged.\n    \"\"\"\n\n        return f\"<html><body>{arg}</body></html>\"\n\n    decorated_func = mark_safe(clean_string_provider)\n    self.assertEqual(decorated_func(\"test\"), clean_string_provider(\"test\"))\n", "def test_mark_safe_as_decorator_with_args(self):\n    \"\"\"\n    mark_safe used as a decorator with arguments leaves the result of a function\n    unchanged.\n    \"\"\"\n\n        return f\"<html><body>{s}</body></html>\"\n\n    safe_clean_string_provider = mark_safe(clean_string_provider)\n    self.assertEqual(safe_clean_string_provider(\"dummy\"), \"<html><body>dummy</body></html>\")\n", "    def test_safe_string_concatenation(self):\n        \"\"\"\n        Test concatenation of SafeString objects with other strings and SafeData objects.\n        \"\"\"\n        safe_s1 = mark_safe(\"a&b\")\n        safe_s2 = mark_safe(\"c&d\")\n        unsafe_s = \"e&f\"\n\n        # SafeString + SafeString is SafeString\n        result = safe_s1 + safe_s2\n        self.assertIsInstance(result, SafeString)\n        self.assertRenderEqual(\"{{ s }}\", \"a&bcd&d\", s=result)\n\n        # SafeString + str is str\n        result = safe_s1 + unsafe_s\n        self.assertIsInstance(result, str)\n        self.assertRenderEqual(\"{{ s }}\", \"a&be&f\", s=result)\n\n        # str + SafeString is str\n        result = unsafe_s + safe_s2\n        self.assertIsInstance(result, str)\n        self.assertRenderEqual(\"{{ s }}\", \"e&fc&d\", s=result)\n\n        # SafeString + SafeData is SafeString\n        safe_data = customescape(\"<data>\")\n        result = safe_s1 + safe_data\n        self.assertIsInstance(result, SafeString)\n        self.assertRenderEqual(\"{{ s }}\", \"a&b<<data>>\", s=result)\n\n        # SafeData + SafeString is str\n        result = safe_data + safe_s2\n        self.assertIsInstance(result, str)\n        self.assertRenderEqual(\"{{ s }}\", \"<<data>>c&d\", s=result)\n"], "sample_119": ["def test_foreign_key_transform(self):\n    query = Query(Item)\n    with register_lookup(CharField, Lower):\n        with self.assertRaises(FieldError):\n            query.build_where(Q(creator__name__lower='foo'))\n", "def test_transform_with_f(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower=F('first_name')))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, SimpleCol)\n    self.assertIsInstance(lookup.rhs, SimpleCol)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n    self.assertEqual(lookup.rhs.target, Author._meta.get_field('first_name'))\n", "def test_transform_with_annotation(self):\n    query = Query(Author)\n    query.add_annotation(Lower('name'), 'name_lower')\n    where = query.build_where(~Q(name_lower='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertEqual(lookup.lhs.lhs.alias, 'name_lower')\n", "def test_foreign_key_lookup(self):\n    query = Query(Item)\n    where = query.build_where(Q(creator__name='John Doe'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n    self.assertEqual(lookup.rhs, 'John Doe')\n", "def test_foreign_key_exact(self):\n    query = Query(Item)\n    author = Author.objects.get(id=1)\n    where = query.build_where(Q(creator=author))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertIsInstance(lookup.rhs, SimpleCol)\n    self.assertEqual(lookup.lhs.target, Item._meta.get_field('creator'))\n    self.assertEqual(lookup.rhs.target, Author._meta.get_field('id'))\n"], "sample_881": ["def test_label_ranking_avg_precision_score_should_raise_error_for_mismatched_shapes():\n    # Test that label_ranking_avg_precision_score raises a ValueError if y_true and y_score have mismatched shapes.\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.9], [0, 0]])\n    with pytest.raises(ValueError, match=\"y_true and y_score have different shapes.\"):\n        label_ranking_average_precision_score(y_true, y_score)\n", "def test_top_k_accuracy_score_2d_y_true_multiclass():\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y_score = np.array([[0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9]])\n    true_score = 1\n    score = top_k_accuracy_score(y_true, y_score, k=1)\n    assert score == pytest.approx(true_score)\n", "def test_label_ranking_avg_precision_score_should_allow_dok_matrix_for_y_true_input():\n    # Test that label_ranking_avg_precision_score accept sparse y_true.\n    # Non-regression test for #22575\n    y_true = dok_matrix([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(2 / 3)\n", "def test_top_k_accuracy_score_binary_with_labels():\n    \"\"\"Test when labels and y_score are binary.\"\"\"\n    y_true = [0, 0, 1, 1]\n    y_score = np.array([0.1, 0.2, 0.7, 0.8])\n    labels = [0, 1]\n    score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n    assert score == pytest.approx(1)\n", "def test_top_k_accuracy_score_multilabel():\n    # Test when labels and y_score are multilabel.\n    y_true = np.array([[0, 1, 1], [1, 0, 1], [0, 0, 1]])\n    y_score = np.array([[0.4, 0.3, 0.2], [0.1, 0.4, 0.3], [0.3, 0.1, 0.4]])\n\n    score = top_k_accuracy_score(y_true, y_score, k=2)\n    true_score = 0.6666666666666666  # (2/3 + 1/3) / 2\n    assert score == pytest.approx(true_score)\n"], "sample_832": ["def test_ard_intercept_offset():\n    # Test that ARDRegression predicts correctly when there is an intercept offset\n    X = np.array([[1], [2], [3]])\n    Y = np.array([2, 3, 4])\n    clf = ARDRegression(fit_intercept=True)\n    clf.fit(X, Y)\n\n    # Check that the model could approximately learn the identity function with offset\n    test = [[1], [3], [4]]\n    assert_array_almost_equal(clf.predict(test), [2, 4, 5], 2)\n", "def test_fit_intercept_bayesian_ridge_ard():\n    # Test BayesianRidge and ARDRegression predictions for fit_intercept=True\n    n_samples = 4\n    n_features = 5\n    random_state = check_random_state(42)\n    X = random_state.random_sample((n_samples, n_features))\n    y = 2 * X[:, 0] + 3 * X[:, 1] + random_state.rand()\n\n    for clf in [BayesianRidge(fit_intercept=True), ARDRegression(fit_intercept=True)]:\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert_almost_equal(clf.intercept_, 0, decimal=1)\n        assert_array_almost_equal(clf.coef_, np.array([2, 3]), decimal=1)\n", "def test_bayesian_ridge_intercept_fit_intercept_false():\n    # Test BayesianRidge when fit_intercept is False\n    X = np.array([[1], [2], [3]])\n    Y = np.array([1, 2, 3])\n    clf = BayesianRidge(compute_score=True, fit_intercept=False)\n    clf.fit(X, Y)\n\n    # Check that the intercept is zero\n    assert_almost_equal(clf.intercept_, 0.0)\n", "def test_ard_accuracy_on_correlated_features():\n    # Check that ARD handles correlated features correctly\n    # Create data with two correlated features and one uncorrelated feature\n    rng = np.random.RandomState(0)\n    X = np.concatenate([rng.normal(size=(100, 1)),\n                        rng.normal(size=(100, 1)) + rng.normal(size=(100, 1)),\n                        rng.normal(size=(100, 1))],\n                       axis=1)\n    y = 2 * X[:, 0] + 3 * X[:, 1] + rng.normal(size=100)\n\n    regressor = ARDRegression()\n    regressor.fit(X, y)\n\n    # Check that the model correctly estimates the coefficients for the correlated features\n    assert_almost_equal(regressor.coef_[0], 2, decimal=1)\n    assert_almost_equal(regressor.coef_[1], 3, decimal=1)\n\n    # Check that the model correctly estimates the coefficient for the uncorrelated feature\n    assert_almost_equal(regressor.coef_[2], 0, decimal=1)\n", "def test_bayesian_ridge_noise_variance():\n    # Test BayesianRidge noise variance estimation (GitHub issue #12826)\n    X = np.array([[1.], [2.], [3.]])\n    y = np.array([1., 2., 2.])  # y = X + noise\n    br = BayesianRidge()\n    br.fit(X, y)\n    # The noise variance should be close to 1.0 in this case\n    assert_almost_equal(1.0 / br.alpha_, 1.0, decimal=1)\n"], "sample_231": ["    def test_template_encoding(self):\n        try:\n            raise ValueError('Oops')\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertEqual(html.encoding, 'utf-8')\n        text = reporter.get_traceback_text()\n        self.assertEqual(text.encoding, 'utf-8')\n", "def test_non_html_response_filtering(self):\n    \"\"\"\n    Sensitive information is filtered out of error reports when\n    the request doesn't accept HTML content.\n    \"\"\"\n    request = self.rf.post('/some_url/', self.breakfast_data)\n    response = sensitive_view(request)\n    self.assertNotContains(response, 'sausage-value', status_code=500)\n    self.assertNotContains(response, 'bacon-value', status_code=500)\n    self.assertNotContains(response, 'worcestershire', status_code=500)\n", "    def test_multiple_causes(self):\n        try:\n            raise ValueError('First exception') from RuntimeError('Second exception')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('First exception', html)\n        self.assertIn('Second exception', html)\n        self.assertIn('The above exception (ValueError) was the direct cause of the following exception:', html)\n        self.assertIn('During handling of the above exception (RuntimeError), another exception occurred:', html)\n\n        text = reporter.get_traceback_text()\n        self.assertIn('First exception', text)\n        self.assertIn('Second exception', text)\n        self.assertIn('The above exception (ValueError) was the direct cause of the following exception:', text)\n        self.assertIn('During handling of the above exception (RuntimeError), another exception occurred:', text)\n", "    def test_unicode_exception_message(self):\n        \"\"\"\n        Unicode exception messages should not break the debug page (#20449).\n        \"\"\"\n        try:\n            raise ValueError('\u00a1Esto es una prueba de valor unicode!')\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('\u00a1Esto es una prueba de valor unicode!', html)\n", "    def test_cleansed_multivaluedict(self):\n        \"\"\"\n        Sensitive POST parameters are replaced with stars when request.POST is\n        converted to a MultiValueDict.\n        \"\"\"\n        request = self.rf.post('/some_url/', {'secret': 'sausage', 'safe': 'toast'})\n        request.sensitive_post_parameters = ['secret']\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_dict = reporter_filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleansed_dict['secret'], '********************')\n        self.assertEqual(cleansed_dict['safe'], 'toast')\n"], "sample_1019": ["def test_mask_nc():\n    A, B, C = symbols('A,B,C', commutative=False)\n\n    eq, rep, nc_syms = _mask_nc(A**2 - x**2, 'd')\n    assert eq == _d0**2 - x**2\n    assert rep == {_d0: A}\n    assert nc_syms == []\n\n    eq, rep, nc_syms = _mask_nc(A**2 - B**2, 'd')\n    assert eq == A**2 - B**2\n    assert rep is None\n    assert nc_syms == [A, B]\n\n    eq, rep, nc_syms = _mask_nc(x*Commutator(A, B) + x*Commutator(A, C)*Commutator(A, B), 'd')\n    assert eq == x*_d0 + x*_d1*_d0\n    assert rep == {_d0: Commutator(A, B), _d1: Commutator(A, C)}\n    assert nc_syms == [_d0, _d1]\n", "def test_factor_nc_commutator():\n    from sympy.physics.secondquant import Commutator\n    A, B, C = symbols('A,B,C', commutative=False)\n\n    eq = Commutator(A, Commutator(B, C)) - Commutator(B, Commutator(A, C))\n    result = factor_nc(eq)\n    expected = Commutator(A, Commutator(B, C)) - Commutator(Commutator(A, B), C)\n    assert result == expected\n", "def test_issue_9876():\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    eq = A**2 - B**2\n    assert factor_nc(eq) == (A - B)*(A + B)\n", "def test_monotonic_sign():\n    x = symbols('x', real=True, nonnegative=True)\n    y = symbols('y', integer=True, positive=True)\n    z = symbols('z', integer=True, nonpositive=True)\n    p = symbols('p', positive=True)\n    nn = symbols('nn', integer=True, nonnegative=True)\n    p2 = symbols('p2', integer=True, positive=True)\n\n    assert _monotonic_sign(nn + 1) == 1\n    assert _monotonic_sign(p - 1) == _eps\n    assert _monotonic_sign(nn*p + 1) == 1\n    assert _monotonic_sign(p2*p + 1) == 2\n    assert _monotonic_sign(nn - 1) is None\n    assert _monotonic_sign(p*log(x) + 1) is None\n    assert _monotonic_sign(p*exp(x) + 1) is None\n    assert _monotonic_sign(p*cos(x) + 1) is None\n    assert _monotonic_sign(p*sin(x) + 1) is None\n    assert _monotonic_sign(p*y + 1) == 2\n    assert _monotonic_sign(z*y + 1) == -1\n    assert _monotonic_sign(p*sqrt(x)) is None\n    assert _monotonic_sign(p*root(x, 3)) is None\n\n    # univariate polynomial\n    assert _monotonic_sign(x**2) is None\n    assert _monotonic_sign(x**3) is None\n    assert _monotonic_sign(x**4) == _eps\n    assert _monotonic_sign(x**5) == 1\n    assert _monotonic_sign(x**6) == 2\n\n    # multivariate\n    assert _monotonic_sign(x*y) is None\n    assert _monotonic_sign(x*y + z) is None\n    assert _monot", "def test_issue_8992():\n    x, y = symbols('x, y')\n    eq = x**(x + y) + x**(x - y)\n    assert factor_terms(eq) == x**x * (x**y + x**(-y))\n"], "sample_21": ["def test_read_write_simple_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"serr\": [2]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n", "def test_read_write_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.add_column(Column(name=\"b\", data=[4.0, 5.0, 6.0]))\n    t1.add_column(Column(name=\"b_perr\", data=[0.4, 0.5, 0.6]))\n    t1.add_column(Column(name=\"b_nerr\", data=[0.7, 0.8, 0.9]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"serr\": [2], \"terr\": [4, 5]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n    assert np.allclose(t2[\"b_perr\"], t1[\"b_perr\"])\n    assert np.allclose(t2[\"b_nerr\"], t1[\"b_nerr\"])\n", "def test_read_write_simple_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"b\", data=[0.1, 0.2, 0.3]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"serr\": [2]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\"])\n    assert np.all(t2[\"a\"] == t1[\"a\"])\n    assert np.allclose(t2[\"b_err\"], t1[\"b\"])\n", "def test_read_write_with_errors(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_perr\", data=[0.1, 0.2, 0.3]))\n    t1.add_column(Column(name=\"a_nerr\", data=[0.05, 0.15, 0.25]))\n    t1.add_column(Column(name=\"b\", data=[4, 5, 6]))\n    t1.add_column(Column(name=\"b_err\", data=[0.4, 0.5, 0.6]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [4]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_perr\"], t1[\"a_perr\"])\n    assert np.allclose(t2[\"a_nerr\"], t1[\"a_nerr\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n    assert np.allclose(t2[\"b_err\"], t1[\"b_err\"])\n", "def test_read_write_simple_with_errors(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"serr\": [2]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n"], "sample_765": ["def test_balanced_accuracy_score_single_class():\n    y_true = [0, 0, 0]\n    y_pred = [0, 0, 0]\n    assert_equal(balanced_accuracy_score(y_true, y_pred), 1.0)\n    y_pred = [1, 1, 1]\n    assert_equal(balanced_accuracy_score(y_true, y_pred), 0.0)\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = np.array([0, 1, 2, 2, 0])\n    y_pred = np.array([0, 1, 2, 0, 1])\n    true_balanced_accuracy = (0.5 + 0.5 + 0.5) / 3\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), true_balanced_accuracy)\n", "def test_balanced_accuracy_score_multiclass():\n    # Check balanced_accuracy_score function for multiclass case\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 2, 1, 0, 1, 2])\n    true_score = np.mean([np.mean(y_true == i) for i in range(3)])\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_true), 1.0)\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), true_score)\n", "def test_balanced_accuracy_score_weighted():\n    y_true = [0, 1, 2, 2]\n    y_pred = [0, 1, 2, 0]\n    weighted_accuracy = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(weighted_accuracy, 0.8, decimal=1)\n", "def test_balanced_accuracy_score_binary():\n    # Test balanced accuracy score for binary classification\n    y_true, _, _ = make_prediction(binary=True)\n    y_pred = np.where(np.array(y_true) == 0, 1, 0)  # Flip predictions\n\n    # Calculate balanced accuracy score\n    bas = balanced_accuracy_score(y_true, y_pred)\n\n    # Compare with the expected value\n    assert_almost_equal(bas, 0.625, decimal=3)\n"], "sample_253": ["def test_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "    def test_notify_file_changed_no_handler(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.utils.autoreload.trigger_reload') as mocked_trigger:\n            reloader.notify_file_changed('test.py')\n            self.assertEqual(mocked_trigger.call_count, 1)\n", "def test_notify_file_changed_signal_handled(self, mocked_modules, notify_mock):\n    results = [(mock.Mock(), True)]\n    notify_mock.side_effect = lambda *args, **kwargs: results\n    self.reloader.notify_file_changed(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "def test_watch_dir_without_glob(self, mocked_modules, notify_mock):\n    # Test case where watch_dir is called without any glob pattern\n    self.reloader.watch_dir(self.tempdir)\n    with self.tick_twice():\n        self.increment_mtime(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "    def test_notify_file_changed_trigger_reload(self, mock_trigger_reload):\n        reloader = autoreload.BaseReloader()\n        reloader.notify_file_changed('test_file.py')\n        self.assertEqual(mock_trigger_reload.call_count, 1)\n        self.assertEqual(mock_trigger_reload.call_args[0][0], 'test_file.py')\n"], "sample_246": ["    def test_obsolete_messages_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=False)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertIn(\"#, fuzzy\", po_contents)\n", "    def test_no_obsolete_enabled(self):\n        \"\"\"Behavior is correct if --no-obsolete switch is specified.\"\"\"\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            # Assuming an obsolete string exists in the PO file\n            self.assertNotIn('#~ msgid \"Obsolete string\"', po_contents)\n", "    def test_blocktranslate_trimming(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            # should be trimmed\n            self.assertMsgId(\"Blocktranslate should be trimmed.\", po_contents)\n            # should not be trimmed\n            self.assertMsgId(\"This blocktranslate should not be trimmed.\", po_contents)\n        # #21406 -- Should adjust for eaten line numbers\n        self.assertMsgId(\"Get my line number\", po_contents)\n        self.assertLocationCommentPresent(self.PO_FILE, 'Get my line number', 'templates', 'test.html')\n", "def test_no_obsolete_enabled(self):\n    \"\"\"\n    Test that obsolete messages are removed if the --no-obsolete switch is specified.\n    \"\"\"\n    # Create a .po file with an obsolete message\n    with open(self.PO_FILE, 'w', encoding='utf-8') as fp:\n        fp.write('#, fuzzy\\nmsgid \"Obsolete message\"\\nmsgstr \"Old translation\"\\n')\n\n    # Call makemessages with the --no-obsolete switch\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n\n    # Check that the obsolete message is not present in the updated .po file\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE, encoding='utf-8') as fp:\n        po_contents = fp.read()\n        self.assertNotIn(\"#, fuzzy\\nmsgid \\\"Obsolete message\\\"\", po_contents)\n", "def test_makemessages_symlink_in_locale_path(self):\n    \"\"\"\n    makemessages should follow symlinks to directories in LOCALE_PATHS.\n    \"\"\"\n    if symlinks_supported():\n        locale_path_symlink = os.path.join(self.test_dir, 'project_locale_symlink')\n        os.symlink(os.path.join(self.test_dir, 'project_locale'), locale_path_symlink)\n    else:\n        self.skipTest(\"os.symlink() not available on this OS + Python version combination.\")\n\n    with override_settings(LOCALE_PATHS=[locale_path_symlink]):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        de_locale = os.path.join(locale_path_symlink, 'de', 'LC_MESSAGES', 'django.po')\n        self.assertTrue(os.path.exists(de_locale))\n\n        with open(de_locale) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('This is a project-level string', po_contents)\n"], "sample_796": ["def test_huber_epsilon_value():\n    # Test that epsilon should be greater than or equal to 1.0\n    with pytest.raises(ValueError):\n        huber = HuberRegressor(epsilon=0.5)\n", "def test_huber_invalid_epsilon():\n    # Test that an error is raised when epsilon is less than 1.0\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError, match=\"epsilon should be greater than or equal to 1.0, got -1.0\"):\n        huber = HuberRegressor(epsilon=-1.0)\n        huber.fit(X, y)\n", "def test_huber_epsilon_validation():\n    # Test that HuberRegressor raises ValueError if epsilon is less than 1.0\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError):\n        huber = HuberRegressor(epsilon=0.5)\n        huber.fit(X, y)\n", "def test_huber_epsilon_validation():\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError, match=\"epsilon should be greater than or equal to 1.0\"):\n        huber = HuberRegressor(epsilon=0.99)\n        huber.fit(X, y)\n", "def test_huber_zero_alpha_outliers():\n    # Test that when alpha is zero, outliers are still detected\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100)\n    huber.fit(X, y)\n    linear_loss = np.dot(X, huber.coef_) + huber.intercept_ - y\n    mask = np.abs(linear_loss) < huber.epsilon * huber.scale_\n    assert not np.all(mask)  # Assert that not all samples are non-outliers\n"], "sample_35": ["def test_isinstancemethod():\n    class MetaClass(type):\n\n    class MyClass(metaclass=MetaClass):\n\n        @classmethod\n\n        @staticmethod\n\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.another_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n", "def test_isinstancemethod():\n    class MyClass:\n\n        @classmethod\n\n        @staticmethod\n\n    assert isinstancemethod(MyClass, MyClass.a_classmethod) is False\n    assert isinstancemethod(MyClass, MyClass.a_staticmethod) is False\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod) is True\n", "def test_minversion():\n    assert minversion(\"astropy\", \"0.4.4\") == True\n    assert minversion(\"astropy\", \"5.0\") == False\n    assert minversion(\"collections\", \"0.0\") == True\n    assert minversion(\"collections\", \"5.0\") == False\n    assert minversion(\"doesntexist\", \"0.0\") == False\n", "def test_minversion():\n    # Test minversion function with a module\n    import astropy\n    assert minversion(astropy, '0.4.4')\n    assert not minversion(astropy, '99.99')\n\n    # Test minversion function with a module name\n    assert minversion('astropy', '0.4.4')\n    assert not minversion('astropy', '99.99')\n\n    # Test minversion function with a non-existing module name\n    assert not minversion('non_existing_module', '1.0')\n\n    # Test minversion function with a non-valid module\n    with pytest.raises(ValueError):\n        minversion(123, '1.0')\n", "def test_minversion():\n    \"\"\"\n    Tests that the `minversion` function works correctly.\n    \"\"\"\n    # Test with a module object\n    import astropy\n    assert minversion(astropy, '0.4.4')\n    assert not minversion(astropy, '99.99')\n\n    # Test with a module name\n    assert minversion('astropy', '0.4.4')\n    assert not minversion('astropy', '99.99')\n\n    # Test with an invalid module\n    assert not minversion('nonexistent_module', '1.0')\n\n    # Test with inclusive and exclusive versions\n    assert minversion(astropy, '0.4.3', inclusive=False)\n    assert not minversion(astropy, '0.4.4', inclusive=False)\n\n    # Test with a version path\n    assert minversion(astropy, '0.4', version_path='__version__', inclusive=False)\n\n    # Test with a version that includes dev, rc1, etc.\n    assert minversion('pytest', '7.0.0')\n    assert not minversion('pytest', '7.1.0')\n"], "sample_913": ["def test_pyfunction_signature_with_annotations(app):\n    text = \".. py:function:: hello(name: 'str') -> 'str'\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, \"'str'\"])])\n", "def test_pyfunction_signature_with_annotation(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1][0], desc_parameter,\n                [desc_sig_name, \"name\"],\n                [desc_sig_punctuation, \":\"],\n                \" \",\n                [desc_sig_name, pending_xref, \"str\"])\n    assert_node(doctree[1][0][1][2], desc_returns,\n                pending_xref, \"\",\n                [desc_sig_name, \"str\"])\n", "def test_pyclasslike_signature(app):\n    text = \".. py:class:: Class\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, ()])]))\n    assert 'Class' in domain.objects\n    assert domain.objects['Class'] == ('index', 'Class', 'class')\n\n    text = \".. py:exception:: Exc\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"Exc\"])],\n                                  [desc_content, ()])]))\n    assert 'Exc' in domain.objects\n    assert domain.objects['Exc'] == ('index', 'Exc', 'exception')\n", "def test_pyfunction_signature_with_annotation(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1][0],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [desc_sig_name, pending_xref, \"str\"])])\n    assert_node(doctree[1][0][1][2],\n                [desc_returns, pending_xref, \"str\"])\n", "def test_pyfunction_signature_with_type_hint(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [desc_sig_name, pending_xref, \"str\"])])\n    assert_node(doctree[1][0][2],\n                [desc_returns, pending_xref, \"str\"])\n"], "sample_508": ["def test_set_is_not_overwritten():\n    \"\"\"set() defined in Artist subclasses should not be overwritten.\"\"\"\n    class MyArtist4(martist.Artist):\n            super().set(**kwargs)\n            # Add additional behavior here\n\n    art = MyArtist4()\n    art.set(myparam1=1, myparam2=2)\n    assert art.myparam1 == 1\n    assert art.myparam2 == 2\n", "def test_set_is_not_overwritten_by_parent():\n    \"\"\"set() defined in Artist subclasses should not be overwritten by parent.\"\"\"\n    class MyArtist4(martist.Artist):\n            \"\"\"Not overwritten by parent.\"\"\"\n            pass\n\n    class MyArtist5(MyArtist4):\n            pass\n\n    assert MyArtist5.set.__doc__ == \"Not overwritten by parent.\"\n", "def test_set_is_not_overwritten():\n    \"\"\"Test that set() is not overwritten by subclass definition.\"\"\"\n    class MyArtist4(martist.Artist):\n            pass\n\n    # Check that set() is still the auto-generated method.\n    assert hasattr(MyArtist4.set, '_autogenerated_signature')\n    assert 'myparam' in MyArtist4.set.__doc__\n", "def test_set_is_not_overwritten():\n    \"\"\"set() defined in Artist subclasses should not be overwritten.\"\"\"\n    class MyArtist4(martist.Artist):\n            pass\n\n    assert MyArtist4.set.__doc__ == \"Not overwritten.\"\n", "def test_set_is_not_overwritten():\n    \"\"\"set() defined in Artist subclasses should not be overwritten.\"\"\"\n    class MyArtist4(martist.Artist):\n            \"\"\"Custom set() implementation.\"\"\"\n            pass\n\n    assert MyArtist4.set.__doc__ == \"Custom set() implementation.\"\n    assert hasattr(MyArtist4.set, '_autogenerated_signature')\n"], "sample_865": ["def test_prune_tree_classifier_raises_negative_ccp_alpha(tree_cls):\n    clf = tree_cls()\n    msg = \"ccp_alpha must be greater than or equal to 0\"\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(ccp_alpha=-1.0)\n        clf.fit(X, y)\n\n    clf.set_params(ccp_alpha=0.0)\n    clf.fit(X, y)\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(ccp_alpha=-1.0)\n        clf._prune_tree()\n", "def test_prune_tree_raises_invalid_ccp_alpha():\n    clf = DecisionTreeClassifier()\n    msg = \"Invalid value for ccp_alpha\"\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(ccp_alpha=\"invalid\")\n        clf.fit(X, y)\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(ccp_alpha=None)\n        clf.fit(X, y)\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(ccp_alpha=[])\n        clf.fit(X, y)\n", "def test_prune_tree_classifier_monotonicity(criterion, dataset, tree_cls):\n    dataset = DATASETS[dataset]\n    X, y = dataset[\"X\"], dataset[\"y\"]\n    est = tree_cls(max_leaf_nodes=20, random_state=0)\n    info = est.cost_complexity_pruning_path(X, y)\n\n    pruning_path = info.ccp_alphas\n    assert np.all(np.diff(pruning_path) >= 0)\n\n    # A pruned tree must have a lower or equal impurity than the previous tree (which had a smaller ccp_alpha)\n    for prev_imp, next_imp in zip(info.impurities, info.impurities[1:]):\n        assert prev_imp >= next_imp\n", "def test_prune_tree_classifier_cost_complexity(criterion, dataset, tree_cls):\n    dataset = DATASETS[dataset]\n    X, y = dataset[\"X\"], dataset[\"y\"]\n    est = tree_cls(max_leaf_nodes=20, random_state=0)\n    info = est.cost_complexity_pruning_path(X, y)\n\n    pruning_path = info.ccp_alphas\n    impurities = info.impurities\n\n    for ccp_alpha in pruning_path:\n        pruned_est = tree_cls(random_state=0, ccp_alpha=ccp_alpha).fit(X, y)\n        assert pruned_est.tree_.node_count <= est.tree_.node_count\n\n        # Check that the pruned tree is a subtree of the original tree\n        assert_is_subtree(est.tree_, pruned_est.tree_)\n\n        # Check that the impurity of the pruned tree is less than or equal to the original tree\n        assert pruned_est.tree_.impurity <= est.tree_.impurity\n", "def test_prune_tree_with_sample_weights():\n    # Test pruning with sample weights\n    X = np.random.RandomState(0).random_sample((10, 2))\n    y = np.array([0, 0, 1, 1, 1, 0, 0, 0, 1, 1])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n\n    est = DecisionTreeClassifier(max_leaf_nodes=5, random_state=0)\n    est.fit(X, y, sample_weight=sample_weight)\n\n    info = est.cost_complexity_pruning_path(X, y, sample_weight=sample_weight)\n    pruning_path = info.ccp_alphas\n\n    for ccp_alpha in pruning_path:\n        est_pruned = DecisionTreeClassifier(\n            max_leaf_nodes=5, ccp_alpha=ccp_alpha, random_state=0)\n        est_pruned.fit(X, y, sample_weight=sample_weight)\n\n        assert_is_subtree(est.tree_, est_pruned.tree_)\n"], "sample_941": ["def test_restify_type_hints_NewType():\n    assert restify(MyInt) == \":class:`MyInt`\"\n", "def test_restify_type_hints_Annotated():\n    from typing import Annotated  # type: ignore\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":class:`str`\"  # NOQA\n", "def test_restify_type_hints_Union_with_ellipsis():\n    assert restify(Union[int, ...]) == \":obj:`Union`\\\\ [:class:`int`, ...]\"\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_restify_type_hints_literal():\n    from typing import Literal\n    assert restify(Literal[\"foo\"]) == \":obj:`Literal`\\\\ [:obj:`'foo'`]\"\n    assert restify(Literal[\"foo\", \"bar\"]) == \":obj:`Literal`\\\\ [:obj:`'foo'`, :obj:`'bar'`]\"\n"], "sample_109": ["    def test_render_options_multiple_selection(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        form = MultipleSelectionBandForm(initial={'band': [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option_beatles = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        selected_option_who = '<option value=\"%s\" selected>The Who</option>' % who.pk\n        self.assertIn(selected_option_beatles, output)\n        self.assertIn(selected_option_who, output)\n", "    def test_translations_fr(self):\n        with translation.override('fr'):\n            form = AlbumForm()\n            attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n            self.assertEqual(attrs['data-ajax--url'], '/fr/admin_widgets/band/autocomplete/')\n", "def test_render_options_i18n(self):\n    \"\"\"Selected options are localized if the current language is supported.\"\"\"\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    translation.activate('de')\n    form = AlbumForm(initial={'band': beatles.pk})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>Die Beatles</option>' % beatles.pk\n    self.assertIn(selected_option, output)\n", "    def test_render_options_multiple_select(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        form = AutocompleteSelectMultipleForm(initial={'featuring': [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option1 = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        selected_option2 = '<option value=\"%s\" selected>The Who</option>' % who.pk\n        self.assertIn(selected_option1, output)\n        self.assertIn(selected_option2, output)\n", "def test_localized_media_files(self):\n    # Test that the correct localized media files are included in the widget's media\n    # English language\n    translation.activate('en')\n    form = AlbumForm()\n    media = form['band'].field.widget.media\n    self.assertIn('admin/js/vendor/select2/i18n/en.js', media._js)\n\n    # French language\n    translation.activate('fr')\n    form = AlbumForm()\n    media = form['band'].field.widget.media\n    self.assertIn('admin/js/vendor/select2/i18n/fr.js', media._js)\n\n    # Unsupported language\n    translation.activate('xx')\n    form = AlbumForm()\n    media = form['band'].field.widget.media\n    self.assertNotIn('admin/js/vendor/select2/i18n/xx.js', media._js)\n"], "sample_380": ["def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum('price', default=0.0),\n    )\n    self.assertEqual(result['value'], 0.0)\n", "def test_aggregation_filter_star(self):\n    vals = Book.objects.aggregate(Count(\"*\", filter=Q(rating__gt=4.0)))\n    self.assertEqual(vals, {\"__count\": 2})\n", "def test_aggregation_filter_in_annotation(self):\n    authors = Author.objects.annotate(\n        count=Count('book', filter=Q(book__rating__gt=3.5)),\n    ).filter(count__gt=0).order_by('name')\n    self.assertQuerysetEqual(authors, [\n        ('Adrian Holovaty', 1),\n        ('Brad Dayley', 0),\n        ('Jacob Kaplan-Moss', 1),\n        ('James Bennett', 0),\n        ('Jeffrey Forcier', 1),\n        ('Paul Bissex', 1),\n        ('Peter Norvig', 2),\n        ('Stuart Russell', 0),\n        ('Wesley J. Chun', 1),\n    ])\n", "def test_count_distinct_expression_filter(self):\n    aggs = Book.objects.aggregate(\n        distinct_ratings=Count(Case(When(pages__gt=300, then='rating'), filter=Q(price__gt=Decimal('30.00')), distinct=True),),\n    )\n    self.assertEqual(aggs['distinct_ratings'], 2)\n", "def test_stddev_variance(self):\n    # Test StdDev and Variance aggregates with sample=False (population)\n    qs = Book.objects.aggregate(stddev_rating=StdDev('rating'), variance_rating=Variance('rating'))\n    self.assertAlmostEqual(qs['stddev_rating'], 1.1547005383792515, places=6)\n    self.assertAlmostEqual(qs['variance_rating'], 1.3333333333333333, places=6)\n\n    # Test StdDev and Variance aggregates with sample=True (sample)\n    qs = Book.objects.aggregate(stddev_rating=StdDev('rating', sample=True), variance_rating=Variance('rating', sample=True))\n    self.assertAlmostEqual(qs['stddev_rating'], 1.217558560933513, places=6)\n    self.assertAlmostEqual(qs['variance_rating'], 1.487012987012987, places=6)\n"], "sample_615": ["def test_polyval_single_dimension() -> None:\n    xcoord = xr.DataArray(np.arange(10), dims=(\"x\",), name=\"x\")\n    da = xr.DataArray(1.0 + xcoord + 2.0 * xcoord**2, dims=(\"x\",))\n    coeffs = xr.DataArray([2, 1, 1], dims=(\"degree\",), coords={\"degree\": [2, 1, 0]})\n\n    da_pv = xr.polyval(da.x, coeffs)\n\n    xr.testing.assert_allclose(da, da_pv)\n", "def test_polyval_errors() -> None:\n    xcoord = xr.DataArray(np.arange(10), dims=(\"x\",), name=\"x\")\n    coeffs = xr.DataArray([[2, 1, 1]], dims=(\"degree\",), coords={\"degree\": [2, 1, 0]})\n\n    with pytest.raises(ValueError, match=\"the size of the 'degree' dimension must be greater than or equal to 1\"):\n        xr.polyval(xcoord, coeffs)\n", "def test_apply_ufunc_coords(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    data = xr.DataArray(np.arange(10), dims=\"x\", coords={\"x\": np.arange(10)})\n    if use_dask:\n        data = data.chunk({\"x\": 2})\n\n        return x * 2\n\n    result = xr.apply_ufunc(double, data, input_core_dims=[[\"x\"]], output_core_dims=[[\"x\"]])\n    expected = xr.DataArray(np.arange(0, 20, 2), dims=\"x\", coords={\"x\": np.arange(10)})\n\n    assert_identical(result, expected)\n", "def test_polyval_errors() -> None:\n    x = np.arange(10)\n    xcoord = xr.DataArray(x, dims=(\"x\",), name=\"x\")\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n    )\n\n    # Test error when x coordinate is not provided\n    with pytest.raises(KeyError, match=\"'x'\"):\n        xr.polyval(xcoord.drop_vars(\"x\"), coeffs)\n\n    # Test error when degree dimension is not provided\n    with pytest.raises(KeyError, match=\"'degree'\"):\n        xr.polyval(xcoord, coeffs.drop_vars(\"degree\"))\n\n    # Test error when degree dimension is not present in coeffs\n    with pytest.raises(KeyError, match=\"'degree'\"):\n        xr.polyval(xcoord, coeffs.drop_dims(\"degree\"))\n\n    # Test error when degree dimension size is less than expected\n    with pytest.raises(ValueError, match=\"size of dimension 'degree'\"):\n        xr.polyval(xcoord, coeffs.isel(degree=slice(0, -1)))\n", "def test_polyval_with_nan() -> None:\n    x = xr.DataArray([0, np.nan, 2], dims=(\"x\",), name=\"x\")\n    coeffs = xr.DataArray([[1, 0], [2, 0]], dims=(\"d\", \"degree\"), coords={\"d\": [0, 1], \"degree\": [1, 0]})\n\n    da_pv = xr.polyval(x, coeffs)\n\n    expected = xr.DataArray([0, np.nan, 2], dims=(\"d\", \"x\"), coords={\"d\": [0, 1], \"x\": x})\n\n    assert_identical(da_pv, expected)\n"], "sample_605": ["def test_groupby_apply_func_kwargs():\n        return arg1 + arg2 + arg3\n\n    array = xr.DataArray([1, 1, 1], [(\"x\", [1, 2, 3])])\n    expected = xr.DataArray([3, 3, 3], [(\"x\", [1, 2, 3])])\n    actual = array.groupby(\"x\").apply(func, arg2=1, arg3=1)\n    assert_identical(expected, actual)\n", "def test_groupby_map_keep_attrs(array):\n    array.attrs[\"metadata\"] = \"test\"\n    grouped = array.groupby(\"x\")\n    result = grouped.map(lambda x: x * 2)\n    assert result.attrs == array.attrs\n", "def test_groupby_duplicate_index_labels():\n    # test for duplicate index labels\n    array = xr.DataArray([1, 2, 3], [(\"x\", [1, 1, 2])])\n    array[\"y\"] = (\"x\", [1, 2, 1])\n    expected = xr.DataArray([5], [(\"x\", [1]), (\"y\", [1])])\n    actual = array.groupby(\"x\").sum()\n    assert_equal(expected, actual)\n\n    expected = xr.DataArray([2, 3], [(\"y\", [1, 2])])\n    actual = array.groupby(\"y\").sum()\n    assert_equal(expected, actual)\n", "def test_groupby_on_dataset_with_multiple_dimensional_coordinates():\n    # Test groupby on a Dataset with multiple dimensional coordinates\n    ds = xr.Dataset(\n        {\"foo\": ((\"x\", \"y\", \"z\"), np.random.randn(3, 4, 2))},\n        {\"x\": [\"a\", \"b\", \"c\"], \"y\": [\"d\", \"e\", \"f\", \"g\"], \"z\": [1, 2]},\n    )\n    grouped = ds.groupby(\"x\")\n    for group_value, group_ds in grouped:\n        assert group_value in ds[\"x\"].values\n        assert group_ds[\"x\"].size == 1\n        assert group_ds[\"x\"] == group_value\n        assert group_ds[\"y\"].size == ds[\"y\"].size\n        assert group_ds[\"z\"].size == ds[\"z\"].size\n", "def test_groupby_bins_custom_labels():\n    data = np.array([0.2, 0.4, 0.6, 0.8])\n    da = xr.DataArray(data, dims=\"x\")\n    bins = [0, 0.5, 1]\n    labels = [\"low\", \"high\"]\n    actual = da.groupby_bins(\"x\", bins, labels=labels).mean()\n    expected = xr.DataArray([0.3, 0.7], dims=[\"x_bins\"], coords={\"x_bins\": labels}).to_dataset(name=\"x\")\n    assert_identical(actual, expected)\n"], "sample_628": ["def test_skip_correct_spelling(self):\n    stmt = astroid.extract_node(\n        'class CorrectSpelling(object):\\n   \"\"\"This is a class with correct spelling\"\"\"\\n   pass'\n    )\n    self.checker.visit_classdef(stmt)\n    assert self.linter.release_messages() == []\n", "def test_docstring_lines_that_look_like_comments_with_sphinx_directive(self):\n    stmt = astroid.extract_node(\n        '''def f():\n    \"\"\"\n    # :cat: cat\n    \"\"\"'''\n    )\n    with self.assertAddsMessages():\n        self.checker.visit_functiondef(stmt)\n", "def test_docstring_with_unicode_literals(self):\n    stmt = astroid.extract_node(\n        '''def f():\n    \"\"\"\n    un\u00edcode\n    \"\"\"'''\n    )\n    with self.assertAddsMessages():\n        self.checker.visit_functiondef(stmt)\n", "def test_skip_unicode_and_raw_strings(self):\n    stmt = astroid.extract_node(\n        'class TestClass(object):\\n   \"\"\"u\\'uni\\'c\u00f4de r\\'raw\\'\"\"\"\\n   pass'\n    )\n    self.checker.visit_classdef(stmt)\n    assert self.linter.release_messages() == []\n", "def test_skip_words_with_digits_and_letters_in_docstrings(self):\n    stmt = astroid.extract_node(\n        'def fff(param_name123):\\n   \"\"\"test param_name123\"\"\"\\n   pass'\n    )\n    self.checker.visit_functiondef(stmt)\n    assert self.linter.release_messages() == []\n"], "sample_583": ["def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]), np.array([-1, 1, 2])))\n    expected = indexing.OuterIndexer((np.array([0, 1, 2]), np.array([0, 1, 2])))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert actual == expected\n\n    indexer = indexing.VectorizedIndexer((np.array([0, -1, 2]), np.array([-1, 1, 2])))\n    expected = indexing.VectorizedIndexer((np.array([0, 1, 2]), np.array([0, 1, 2])))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert actual == expected\n", "def test_posify_mask_subindexer():\n    index = np.array([0, -1, 2, -1, 4, -1])\n    expected = np.array([0, 0, 2, 2, 4, 4])\n    actual = indexing._posify_mask_subindexer(index)\n    np.testing.assert_array_equal(expected, actual)\n\n    index = np.array([-1, -1, -1])\n    expected = np.zeros_like(index)\n    actual = indexing._posify_mask_subindexer(index)\n    np.testing.assert_array_equal(expected, actual)\n\n    index = np.array([0, 1, 2, 3, 4])\n    expected = index.copy()\n    actual = indexing._posify_mask_subindexer(index)\n    np.testing.assert_array_equal(expected, actual)\n", "def test_posify_mask_subindexer():\n    index = np.array([1, -1, 2, -1, 4, -1])\n    expected = np.array([1, 2, 2, 4, 4, 4])\n    actual = indexing._posify_mask_subindexer(index)\n    np.testing.assert_array_equal(expected, actual)\n\n    index = np.array([-1, -1, -1])\n    expected = np.zeros_like(index)\n    actual = indexing._posify_mask_subindexer(index)\n    np.testing.assert_array_equal(expected, actual)\n\n    index = np.array([1, 2, 3, 4, 5])\n    np.testing.assert_array_equal(index, indexing._posify_mask_subindexer(index))\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = np.array([0, 1, 2])\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected, actual.tuple[0])\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = np.array([[0, 1, 1], [0, 1, 2]])\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected, np.array(actual.tuple))\n", "def test_posify_mask_subindexer():\n    index = np.array([0, 1, -1, 3, -1, 5])\n    expected = np.array([0, 1, 2, 3, 4, 5])\n    actual = indexing._posify_mask_subindexer(index)\n    np.testing.assert_array_equal(expected, actual)\n\n    index = np.array([-1, -1, -1])\n    expected = np.array([0, 0, 0])\n    actual = indexing._posify_mask_subindexer(index)\n    np.testing.assert_array_equal(expected, actual)\n\n    index = np.array([0, 1, 2, 3, 4, 5])\n    expected = np.array([0, 1, 2, 3, 4, 5])\n    actual = indexing._posify_mask_subindexer(index)\n    np.testing.assert_array_equal(expected, actual)\n"], "sample_170": ["def test_sensitive_request_email(self):\n    \"\"\"\n    Sensitive POST parameters are not shown in the email report for\n    sensitive requests.\n    \"\"\"\n    self.verify_safe_email(sensitive_view)\n", "def test_non_ascii_exception(self):\n    \"\"\"\n    Non-ASCII exceptions/values should not make the output generation choke.\n    \"\"\"\n    try:\n        class NonAsciiOutput(Exception):\n                return 'EXC\u00e8EXC'\n        somevar = 'VAL\u00e8VAL'  # NOQA\n        raise NonAsciiOutput()\n    except Exception:\n        exc_type, exc_value, tb = sys.exc_info()\n    reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n    html = reporter.get_traceback_html()\n    self.assertIn('VAL\u00e8VAL', html)\n    self.assertIn('EXC\u00e8EXC', html)\n", "    def test_sensitive_variables_decorator(self):\n        @sensitive_variables()\n            return password\n\n        request = self.rf.get('/test_view/')\n        response = test_func(request, 'secret')\n        self.assertNotContains(response, 'secret', status_code=500)\n", "def test_hide_sensitive_data_in_email_report(self):\n    with self.settings(ADMINS=[('Admin', 'admin@fattie-breakie.com')]):\n        mail.outbox = []  # Empty outbox\n        request = self.rf.post('/some_url/', self.breakfast_data)\n        sensitive_view(request)\n        self.assertEqual(len(mail.outbox), 1)\n        email = mail.outbox[0]\n\n        # Sensitive data is not shown in plain text email reports.\n        body_plain = str(email.body)\n        self.assertNotIn('sausage-value', body_plain)\n        self.assertNotIn('bacon-value', body_plain)\n\n        # Sensitive data is not shown in HTML email reports.\n        body_html = str(email.alternatives[0][0])\n        self.assertNotIn('sausage-value', body_html)\n        self.assertNotIn('bacon-value', body_html)\n", "    def test_sensitive_variables_decorator_with_all_variables(self):\n        @sensitive_variables()\n            return sensitive, not_sensitive\n\n        request = self.rf.get('/some_url/')\n        request.sensitive_variables = '__ALL__'\n        sensitive, not_sensitive = test_func('secret', 'public')\n        self.assertEqual(sensitive, SafeExceptionReporterFilter.cleansed_substitute)\n        self.assertEqual(not_sensitive, 'public')\n"], "sample_241": ["def test_annotation_with_outerref_in_subquery(self):\n    self.gmbh.point_of_contact = Employee.objects.get(lastname='Meyer')\n    self.gmbh.save()\n    inner = Employee.objects.annotate(\n        outer_lastname=OuterRef('lastname'),\n    ).filter(lastname__startswith=Left('outer_lastname', 1))\n    qs = Employee.objects.annotate(\n        ceo_company=Subquery(\n            Company.objects.filter(\n                point_of_contact__in=inner,\n                ceo__pk=OuterRef('pk'),\n            ).values('name'),\n        ),\n    ).filter(ceo_company__isnull=False)\n    self.assertEqual(qs.get().ceo_company, 'Test GmbH')\n", "def test_expression_wrapper_with_none_value(self):\n    expr = ExpressionWrapper(Value(None, output_field=IntegerField()), output_field=IntegerField())\n    self.assertIsNone(expr.resolve_expression(query=None, allow_joins=True, reuse=None, summarize=False))\n", "def test_expression_wrapper_with_aggregate(self):\n    Number.objects.create(integer=1000, float=1.2)\n    qs = Number.objects.annotate(\n        int_sum=ExpressionWrapper(Sum('integer'), output_field=IntegerField())\n    )\n    self.assertEqual(qs.get().float, 1.2)\n", "def test_subquery_group_by_outerref_with_filter(self):\n    inner = Company.objects.annotate(\n        employee=OuterRef('pk'),\n    ).filter(\n        ceo__isnull=False,\n    ).values('employee').annotate(\n        min_num_chairs=Min('num_chairs'),\n    ).values('ceo')\n    self.assertIs(Employee.objects.filter(pk__in=Subquery(inner)).exists(), True)\n", "def test_expression_wrapper_with_distinct(self):\n    queryset = Experiment.objects.annotate(\n        distinct_value=ExpressionWrapper(\n            F('name'),\n            output_field=CharField(),\n        ),\n    ).distinct('distinct_value')\n    distinct_names = set(queryset.values_list('distinct_value', flat=True))\n    self.assertEqual(distinct_names, {'e0', 'e1', 'e2', 'e3', 'e4', 'e5'})\n"], "sample_772": ["def test_backend_counter():\n    # Test if the custom backend is used and counts the number of calls\n    X, y = hastie_X, hastie_y\n    ForestEstimator = RandomForestClassifier\n    backend = MyBackend()\n    with register_parallel_backend('test', backend):\n        with parallel_backend('test'):\n            est = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False, random_state=1, n_jobs=2)\n            est.fit(X, y)\n    assert backend.count > 0\n", "def test_parallel_backend(forest):\n    # Test that the parallel backend is used\n    with parallel_backend('multiprocessing', n_jobs=2):\n        est = forest(n_jobs=2, random_state=42)\n        est.fit(X, y)\n        assert_greater(est.n_jobs, 1)\n        assert_equal(est.n_jobs, 2)\n        assert_equal(est._backend.count, est.n_estimators)\n", "def test_parallel_backend_forest():\n    # Test that the parallel backend is used by the forest estimators\n    backend = MyBackend()\n    with parallel_backend(backend):\n        check_parallel(\"RandomForestClassifier\", iris.data, iris.target)\n        check_parallel(\"RandomForestRegressor\", boston.data, boston.target)\n    assert backend.count > 0\n", "def test_my_backend_with_forest():\n    # Test if the custom backend is used and the count is incremented\n    X, y = iris.data, iris.target\n    backend = MyBackend()\n    with parallel_backend(backend):\n        est = RandomForestClassifier(n_jobs=2, random_state=0)\n        est.fit(X, y)\n        assert backend.count > 0\n", "def test_custom_backend():\n    # Test if the custom backend is used and the count is incremented.\n    X, y = hastie_X, hastie_y\n    with parallel_backend('custom', MyBackend()):\n        ForestEstimator = FOREST_ESTIMATORS[\"RandomForestClassifier\"]\n        est = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False,\n                              random_state=1, n_jobs=2)\n        est.fit(X, y)\n    assert est.n_jobs > 1\n    assert est.n_jobs <= 5\n    assert est._backend.count > 0\n"], "sample_1097": ["def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, n, complex=True)\n    B = MatrixSymbol('B', n, m, complex=True)\n    C = MatrixSymbol('C', m, n, complex=True)\n    D = MatrixSymbol('D', m, m, complex=True)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    real_X, imag_X = X.as_real_imag()\n    assert real_X == BlockMatrix([[re(A), re(B)], [re(C), re(D)]])\n    assert imag_X == BlockMatrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_as_real_imag():\n    A, B, C, D = [MatrixSymbol(s, 3, 3, complex=True) for s in 'ABCD']\n    X = BlockMatrix([[A, B], [C, D]])\n    real_matrices, im_matrices = X.as_real_imag()\n    assert real_matrices == BlockMatrix([[re(A), re(B)], [re(C), re(D)]])\n    assert im_matrices == BlockMatrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_as_real_imag():\n    A, B, C, D = [MatrixSymbol(s, 3, 3, complex=True) for s in 'ABCD']\n    X = BlockMatrix([[A, B], [C, D]])\n\n    real_part, imag_part = X.as_real_imag()\n\n    assert real_part == BlockMatrix([[re(A), re(B)], [re(C), re(D)]])\n    assert imag_part == BlockMatrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_as_real_imag():\n    A, B, C, D = [MatrixSymbol(s, 3, 3) for s in 'ABCD']\n    X = BlockMatrix([[A, B], [C, D]])\n    real, imag = X.as_real_imag()\n    assert real == BlockMatrix([[A.real, B.real], [C.real, D.real]])\n    assert imag == BlockMatrix([[A.imag, B.imag], [C.imag, D.imag]])\n", "def test_BlockMatrix_as_real_imag():\n    A, B, C, D = [MatrixSymbol(s, 3, 3) for s in 'ABCD']\n    X = BlockMatrix([[A, B], [C, D]])\n    real, imag = X.as_real_imag()\n    assert real == BlockMatrix([[A.as_real_imag()[0], B.as_real_imag()[0]], [C.as_real_imag()[0], D.as_real_imag()[0]]])\n    assert imag == BlockMatrix([[A.as_real_imag()[1], B.as_real_imag()[1]], [C.as_real_imag()[1], D.as_real_imag()[1]]])\n"], "sample_1187": ["def test_hyperplane_parameters():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    assert hyperplane_parameters(triangle) == [((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)]\n\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    assert hyperplane_parameters(cube[1:], cube[0]) == [([0, -1, 0], -5), ([0, 0, -1], -5),\n                                                       ([-1, 0, 0], -5), ([0, 1, 0], 0),\n                                                       ([1, 0, 0], 0), ([0, 0, 1], 0)]\n", "def test_left_integral3D():\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    facets = cube[1:]\n    vertices = cube[0]\n    hp_param = hyperplane_parameters(cube[1:], vertices)[0]\n    assert left_integral3D(facets, 3, 1, vertices, hp_param, 0) == -50\n    assert left_integral3D(facets, 3, 0, vertices, hp_param, 0) == 0\n", "def test_integration_reduction_3d():\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    facets = cube[1:]\n    vertices = cube[0]\n    hp_params = hyperplane_parameters(facets, vertices)\n    assert integration_reduction(facets, 0, hp_params[0][0], hp_params[0][1], x, 1, (x, y, z), 1) == -25\n    assert integration_reduction(facets, 0, hp_params[0][0], hp_params[0][1], 0, 1, (x, y, z), 1) == 0\n", "def test_integration_reduction_dynamic_3d():\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    facets = cube[1:]\n    hp_params = hyperplane_parameters(facets, vertices)\n    monomial_values = [[[[0, 0, 0, 0, 0, 0, 0, 0]]],\n                       [[[x, 1, 0, 1, 1, 1, 0, None],\n                         [y, 0, 1, 1, 1, 0, 1, None],\n                         [z, 0, 0, 1, 1, 0, 0, 1]]],\n                       [[[x**2, 2, 0, 2, 2, 2, 0, None],\n                         [x*y, 1, 1, 2, 2, 1, 1, None],\n                         [x*z, 1, 0, 2, 2, 1, 0, 1]]],\n                       [[[y**2, 0, 2, 2, 2, 0, 2, None],\n                         [y*z, 0, 1, 2, 2, 0, 1, 1]],\n                        [[x*y, 1, 1, 2, 2, 1, 1, None]]],\n                       [[[y**3, 0,", "def test_integration_reduction_max_degree():\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    facets = cube[1:]\n    vertices = cube[0]\n    hp_params = hyperplane_parameters(facets, vertices)\n    assert main_integrate3d(1, facets, vertices, hp_params, max_degree=2) == \\\n        {1: -125, y: Rational(-625, 2), z: Rational(-625, 2), x: Rational(-625, 2), y*z: Rational(-3125, 4), x*z: Rational(-3125, 4), x*y: Rational(-3125, 4)}\n"], "sample_322": ["def test_unapply_migration_with_deferred_sql(self):\n    \"\"\"Unapplying a migration with deferred SQL works as expected.\"\"\"\n    class DeferredSQL:\n            return \"DEFERRED SQL\"\n\n    class Migration(migrations.Migration):\n        atomic = False\n\n            schema_editor.deferred_sql.append(DeferredSQL())\n\n    executor = MigrationExecutor(connection)\n    with mock.patch('django.db.migrations.executor.MigrationExecutor.record_migration') as record_migration:\n        executor.unapply_migration(\n            ProjectState(),\n            Migration('0001_initial', 'deferred_sql'),\n        )\n        record_migration.assert_called_once_with(Migration('0001_initial', 'deferred_sql'))\n", "    def test_minimize_rollbacks_with_no_applied_migrations(self):\n        \"\"\"\n        Minimize rollbacks when there are no applied migrations.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a2, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a1_impl, False)])\n", "def test_migration_with_operations(self):\n    \"\"\"\n    Tests running a migration with operations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Check the plan\n    plan = executor.migration_plan([(\"migrations\", \"0001_with_operations\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_with_operations\"], False),\n        ],\n    )\n    # Were the tables there before?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n    # Alright, let's try running it\n    executor.migrate([(\"migrations\", \"0001_with_operations\")])\n    # Are the tables there now?\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableExists(\"migrations_book\")\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Alright, let's undo what we did\n    plan = executor.migration_plan([(\"migrations\", None)])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_with_operations\"], True),\n        ],\n    )\n    executor.migrate([(\"migrations\", None)])\n    # Are the tables gone?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_book\")\n", "def test_migrations_recorded_when_fake_initial(self):\n    \"\"\"\n    Migrations are recorded when they are applied with --fake-initial.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Create the tables for the initial migration but don't record it as applied.\n    executor.migrate([('migrations', '0001_initial')])\n    executor.loader.build_graph()\n    # Migrate with --fake-initial and check the migration is recorded as applied.\n    executor.migrate([('migrations', '0001_initial')], fake_initial=True)\n    self.assertIn(('migrations', '0001_initial'), executor.loader.applied_migrations)\n    # Clean up the database.\n    executor.migrate([('migrations', None)])\n    self.assertTableNotExists('migrations_author')\n    self.assertTableNotExists('migrations_tribble')\n", "def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Test the migration plan with replaced migrations.\n\n    When replaced migrations are present, the migration plan should account for them\n    and return the correct plan for applying or unapplying migrations.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a2)\n\n    replacements = {\n        a2: FakeMigration('a2_replaced')\n    }\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n    executor.loader.replacements = replacements\n\n    plan = executor.migration_plan({a3})\n\n    # Since a2 is replaced, the plan should only include applying a3\n    self.assertEqual(plan, [(a3_impl, False)])\n\n    # Now let's assume a3 is applied, and we want to unapply it\n    executor.loader.applied_migrations = {\n        a1: a1_impl,\n        a2: a2_impl,\n        a3: a3_impl,\n    }\n    plan = executor.migration_plan({a1})\n\n    # Since a2 is replaced, the plan should only include unapplying a3 and a2_replaced\n    self.assertEqual(plan, [(a3_impl, True), (replacements[a2], True)])\n"], "sample_912": ["def test_pyfunction_signature_with_annotation(app):\n    text = \".. py:function:: hello(name: 'str') -> 'str'\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, nodes.literal, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.literal, \"str\"])])\n", "def test_pydata_noindex(app):\n    text = \".. py:data:: var\\n   :noindex:\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, ([desc_signature, desc_name, \"var\"], [desc_content, ()])))\n    assert 'var' not in domain.objects\n", "def test_pyfunction_signature_with_kwonlyargs(app):\n    text = \".. py:function:: hello(a, *, b: int = 1, c: str)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc_sig_operator, \"*\"],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"int\"],\n                                                        \" \",\n                                                        [desc_sig_operator, \"=\"],\n                                                        \" \",\n                                                        [nodes.inline, \"1\"])],\n                                      [desc_parameter, ([desc_sig_name, \"c\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"str\"])])])\n", "def test_pyclass_signature(app):\n    text = \".. py:class:: sphinx.builders.html.StandaloneHTMLBuilder\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"sphinx.builders.html.StandaloneHTMLBuilder\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"class\",\n                domain=\"py\", objtype=\"class\", noindex=False)\n\n    assert 'sphinx.builders.html.StandaloneHTMLBuilder' in domain.objects\n    assert domain.objects['sphinx.builders.html.StandaloneHTMLBuilder'] == ('index', 'sphinx.builders.html.StandaloneHTMLBuilder', 'class')\n", "def test_pyfunction_signature_with_annotation(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"str\"])])\n"], "sample_155": ["def test_file_from_buffer_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='test.bin')\n    self.assertEqual(response['Content-Length'], '10')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"test.bin\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_disk_with_custom_filename(self):\n    custom_filename = 'custom_test_file.py'\n    response = FileResponse(open(__file__, 'rb'), filename=custom_filename)\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{custom_filename}\"')\n    response.close()\n", "def test_file_from_contentfile_response(self):\n    content = b'content of the file'\n    response = FileResponse(ContentFile(content))\n    self.assertEqual(response['Content-Length'], '21')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertFalse(response.has_header('Content-Disposition'))\n    self.assertEqual(list(response), [content])\n", "def test_file_from_disk_with_custom_filename(self):\n    custom_filename = 'custom_test_file.py'\n    response = FileResponse(open(__file__, 'rb'), filename=custom_filename)\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"{}\"'.format(custom_filename))\n    response.close()\n", "def test_file_from_disk_with_custom_filename(self):\n    custom_filename = 'custom_filename.py'\n    response = FileResponse(open(__file__, 'rb'), filename=custom_filename)\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{custom_filename}\"')\n    response.close()\n"], "sample_625": ["def test_apply_dask_dataset_join() -> None:\n    ds0 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds1 = xr.Dataset({\"a\": (\"x\", [99, 3]), \"x\": [1, 2]})\n\n        return apply_ufunc(\n            operator.add,\n            a,\n            b,\n            dataset_join=dataset_join,\n            dataset_fill_value=np.nan,\n            dask=\"allowed\",\n        )\n\n    actual = add(ds0.chunk(), ds1.chunk(), \"inner\")\n    expected = xr.Dataset({\"a\": (\"x\", [np.nan, 101, np.nan]), \"x\": [0, 1, 2]})\n    assert_identical(actual, expected)\n\n    actual = add(ds0.chunk(), ds1.chunk(), \"outer\")\n    assert_identical(actual, expected)\n", "def test_polyfit_polyval_integration_degree_argument(\n    use_dask: bool, x: xr.DataArray, y: xr.DataArray, deg: int", "def test_polyval_integration(\n    use_dask: bool,\n    x: xr.DataArray,\n    expected: xr.DataArray,", "def test_polyfit(use_dask: bool, dim: str) -> None:\n    y = xr.DataArray([[1, 6, 17], [34, 57, 86]], dims=(\"x\", \"y\"))\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n        y = y.chunk({dim: 2})\n\n    fit = y.polyfit(dim=dim, deg=1)\n    expected_slope = xr.DataArray([3.5, 4.5], dims=\"x\") if dim == \"y\" else xr.DataArray([23, 32], dims=\"y\")\n    expected_intercept = xr.DataArray(-3.5, dims=()) if dim == \"y\" else xr.DataArray(-7, dims=())\n\n    xr.testing.assert_allclose(fit.polyfit_coefficients.sel(degree=1), expected_slope)\n    xr.testing.assert_allclose(fit.polyfit_coefficients.sel(degree=0), expected_intercept)\n", "def test_polyfit(use_dask: bool, degree: int) -> None:\n    x = xr.DataArray([0, 1, 2], dims=\"x\")\n    y = xr.DataArray([1, 6, 17], dims=\"x\")\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n        y = y.chunk({\"x\": 2})\n\n    with raise_if_dask_computes():\n        fit = y.polyfit(dim=\"x\", deg=degree)\n\n    assert \"degree\" in fit.coords\n    assert \"degree\" in fit.polyfit_coefficients.coords\n    assert fit.polyfit_coefficients.degree.size == degree + 1\n    assert fit.coords[\"degree\"].size == degree + 1\n\n    # Check that the coefficients are correct\n    xr.testing.assert_allclose(fit.polyfit_coefficients, y.polyfit(dim=\"x\", deg=degree, raw=True))\n"], "sample_137": ["def test_parse_rst_with_roles(self):\n    markup = '<p><a class=\"reference external\" href=\"/admindocs/%s\">%s</a></p>\\n'\n    self.assertEqual(parse_rst(':model:`MyApp.MyModel`', ''), markup % ('models/myapp.mymodel/', 'MyApp.MyModel'))\n    self.assertEqual(parse_rst(':view:`myapp.views.my_view`', ''), markup % ('views/myapp.views.my_view/', 'myapp.views.my_view'))\n    self.assertEqual(parse_rst(':template:`myapp/my_template.html`', ''), markup % ('templates/myapp/my_template.html/', 'myapp/my_template.html'))\n    self.assertEqual(parse_rst(':filter:`my_filter`', ''), markup % ('filters/#my_filter', 'my_filter'))\n    self.assertEqual(parse_rst(':tag:`my_tag`', ''), markup % ('tags/#my_tag', 'my_tag'))\n", "def test_replace_named_groups(self):\n    pattern = '^(?P<a>\\w+)/b/(\\w+)$'\n    result = replace_named_groups(pattern)\n    self.assertEqual(result, '^<a>/b/(\\\\w+)$')\n\n    pattern = '^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    result = replace_named_groups(pattern)\n    self.assertEqual(result, '^<a>/b/<c>/$')\n", "    def test_parse_rst_roles(self):\n        \"\"\"\n        Test the custom roles created for parsing reST.\n        \"\"\"\n        markup = '<p><a class=\"reference external\" href=\"/admindocs/%s\">title</a></p>\\n'\n        self.assertEqual(parse_rst(':model:`title`', 'model'), markup % 'models/title/')\n        self.assertEqual(parse_rst(':view:`title`', 'view'), markup % 'views/title/')\n        self.assertEqual(parse_rst(':template:`title`', 'template'), markup % 'templates/title/')\n        self.assertEqual(parse_rst(':filter:`title`', 'filter'), markup % 'filters/#title')\n        self.assertEqual(parse_rst(':tag:`title`', 'tag'), markup % 'tags/#title')\n", "def test_parse_rst_with_roles(self):\n    markup = '<p><a class=\"reference external\" href=\"/admindocs/%s\">%s</a></p>\\n'\n    self.assertEqual(parse_rst(':model:`title`', 'model'), markup % ('models/title/', 'title'))\n    self.assertEqual(parse_rst(':view:`title`', 'view'), markup % ('views/title/', 'title'))\n    self.assertEqual(parse_rst(':template:`title`', 'template'), markup % ('templates/title/', 'title'))\n    self.assertEqual(parse_rst(':filter:`title`', 'filter'), markup % ('filters/#title', 'title'))\n    self.assertEqual(parse_rst(':tag:`title`', 'tag'), markup % ('tags/#title', 'title'))\n", "def test_parse_rst_with_default_role(self):\n    \"\"\"\n    Test that parse_rst() uses the `default_reference_context` to set the\n    default role URL.\n    \"\"\"\n    markup = '<p><a class=\"reference external\" href=\"/admindocs/%s\">title</a></p>\\n'\n    self.assertEqual(parse_rst('title', 'model'), markup % 'models/title/')\n    self.assertEqual(parse_rst('title', 'view'), markup % 'views/title/')\n    self.assertEqual(parse_rst('title', 'template'), markup % 'templates/title/')\n    self.assertEqual(parse_rst('title', 'filter'), markup % 'filters/#title')\n    self.assertEqual(parse_rst('title', 'tag'), markup % 'tags/#title')\n"], "sample_125": ["def test_cookie_secure_flag(self):\n    \"\"\"Cookie should have secure flag if key starts with __Secure- or __Host-.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('__Secure-example', 'value')\n    self.assertEqual(response.cookies['__Secure-example']['secure'], True)\n    response.set_cookie('__Host-example', 'value')\n    self.assertEqual(response.cookies['__Host-example']['secure'], True)\n", "def test_cookie_secure_flag(self):\n    \"\"\"Cookie with secure flag is set when key starts with __Secure- or __Host-.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('__Secure-example', 'secure-value')\n    self.assertEqual(response.cookies['__Secure-example']['secure'], True)\n    response.set_cookie('__Host-example', 'host-value')\n    self.assertEqual(response.cookies['__Host-example']['secure'], True)\n    response.set_cookie('example', 'value')\n    self.assertEqual(response.cookies['example']['secure'], '')\n", "def test_cookie_secure_flag(self):\n    \"\"\"The secure flag should be set for cookies starting with '__Secure-' and '__Host-'.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('__Secure-example', 'value')\n    self.assertEqual(response.cookies['__Secure-example']['secure'], True)\n    response = HttpResponse()\n    response.set_cookie('__Host-example', 'value')\n    self.assertEqual(response.cookies['__Host-example']['secure'], True)\n", "def test_secure_cookie(self):\n    \"\"\"Cookie with secure flag is set when the key starts with '__Secure-' or '__Host-'.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('__Secure-example', 'value')\n    secure_cookie = response.cookies['__Secure-example']\n    self.assertIn('; %s' % cookies.Morsel._reserved['secure'], str(secure_cookie))\n    self.assertIs(secure_cookie['secure'], True)\n\n    response = HttpResponse()\n    response.set_cookie('__Host-example', 'value')\n    host_cookie = response.cookies['__Host-example']\n    self.assertIn('; %s' % cookies.Morsel._reserved['secure'], str(host_cookie))\n    self.assertIs(host_cookie['secure'], True)\n\n    response = HttpResponse()\n    response.set_cookie('example', 'value')\n    insecure_cookie = response.cookies['example']\n    self.assertNotIn('; %s' % cookies.Morsel._reserved['secure'], str(insecure_cookie))\n    self.assertIs(insecure_cookie['secure'], '')\n", "    def test_ascii_cookie(self):\n        \"\"\"Cookie key and value should be ASCII strings.\"\"\"\n        response = HttpResponse()\n        response.set_cookie('test', 'value')\n        self.assertEqual(response.cookies['test'].key, 'test')\n        self.assertEqual(response.cookies['test'].value, 'value')\n"], "sample_457": ["def test_expressions_with_deferrable(self):\n    message = \"UniqueConstraint with expressions and deferrable cannot be used together.\"\n    with self.assertRaisesMessage(ValueError, message):\n        models.UniqueConstraint(\n            Lower(\"name\"),\n            name=\"deferrable_expression_unique\",\n            deferrable=models.Deferrable.DEFERRED,\n        )\n", "def test_deferrable_database_constraint(self):\n    obj_1 = UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"front\")\n    obj_2 = UniqueConstraintDeferrable.objects.create(name=\"p2\", shelf=\"back\")\n\n        obj_1.name, obj_2.name = obj_2.name, obj_1.name\n        obj_1.save()\n        obj_2.save()\n\n    swap_and_save()\n    # Behavior can be changed with SET CONSTRAINTS.\n    with self.assertRaises(IntegrityError):\n        with atomic(), connection.cursor() as cursor:\n            constraint_name = connection.ops.quote_name(\"name_shelf_unique\")\n            cursor.execute(\"SET CONSTRAINTS %s IMMEDIATE\" % constraint_name)\n            swap_and_save()\n", "def test_validate_expression_include(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_inc_color_uniq\",\n        include=[\"color\"],\n    )\n    non_unique_product = UniqueConstraintProduct(name=self.p2.name.upper(), color=\"blue\")\n    msg = \"Constraint \u201cname_lower_inc_color_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    # Values not matching the lowercased name are ignored.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name, color=self.p1.color),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p2)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n    # Included field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"color\"},\n    )\n", "def test_initially_deferred_validation(self):\n    with atomic():\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"name_initially_deferred_unique\",\n            deferrable=models.Deferrable.DEFERRED,\n        )\n        model = models.Model.from_db(\n            db=self.connection,\n            field_names=[\"id\", \"name\"],\n            model_name=\"TestModel\",\n            attrs={\n                \"name\": models.CharField(max_length=255, unique=True),\n            },\n        )\n        obj_1 = model.objects.create(name=\"p1\")\n        obj_2 = model.objects.create(name=\"p2\")\n        # Initially deferred constraints are ignored during validation.\n        constraint.validate(model, model(name=obj_2.name))\n        # Behavior can be changed with SET CONSTRAINTS.\n        with self.connection.cursor() as cursor:\n            constraint_name = self.connection.ops.quote_name(\"name_initially_deferred_unique\")\n            cursor.execute(\"SET CONSTRAINTS %s IMMEDIATE\" % constraint_name)\n            with self.assertRaises(ValidationError):\n                constraint.validate(model, model(name=obj_1.name))\n", "def test_validate_expressions(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), F(\"color\"), name=\"name_color_expr_uniq\")\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper(), color=self.p1.color)\n    msg = \"Constraint \u201cname_color_expr_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name, color=self.p2.color),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique fields are excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"color\"},\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\", \"color\"},\n    )\n"], "sample_67": ["def test_setattr_raises_validation_error_multiple_fields(self):\n    \"\"\"\n    A model ValidationError using the dict form should put the error\n    messages into the correct keys of form.errors for multiple fields.\n    \"\"\"\n    form_class = modelform_factory(model=StrictAssignmentMultipleFields, fields=['title', 'description'])\n    form = form_class(data={'title': 'testing setattr', 'description': 'testing setattr'}, files=None)\n    # This line turns on the ValidationError; it avoids the model erroring\n    # when its own __init__() is called when creating form.instance.\n    form.instance._should_error = True\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {\n        'title': ['Cannot set attribute', 'This field cannot be blank.'],\n        'description': ['Cannot set attribute', 'This field cannot be blank.']\n    })\n", "    def test_descriptor_raises_validation_error(self):\n        \"\"\"\n        A model ValidationError using the dict form should put the error\n        message into the correct key of form.errors when raised by a descriptor.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentDescriptor, fields=['title'])\n        form = form_class(data={'title': 'testing descriptor'}, files=None)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            'title': ['Cannot set descriptor', 'This field cannot be blank.']\n        })\n", "def test_setattr_raises_validation_error_field_specific_non_required(self):\n    \"\"\"\n    A model ValidationError using the dict form should put the error\n    message into the correct key of form.errors, even if the field is not required.\n    \"\"\"\n    form_class = modelform_factory(model=StrictAssignmentFieldSpecific, fields=['title'], exclude=['title'])\n    form = form_class(data={}, files=None)\n    form.instance._should_error = True\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.errors, {})\n", "def test_modelform_factory_custom_metaclass(self):\n    \"\"\"\n    modelform_factory should use a custom metaclass if provided.\n    \"\"\"\n    class CustomMetaclassForm(forms.ModelForm, metaclass=CustomMetaclass):\n        pass\n\n    new_cls = modelform_factory(Person, fields=\"__all__\", form=CustomMetaclassForm)\n    self.assertEqual(new_cls.base_fields, {})\n", "def test_setattr_raises_validation_error_list(self):\n    \"\"\"\n    A model ValidationError using the list form should put all error\n    messages into __all__ (i.e. non-field errors) on the form.\n    \"\"\"\n    form_class = modelform_factory(model=StrictAssignmentAll, fields=['title'])\n    form = form_class(data={'title': 'testing setattr'}, files=None)\n    form.instance._should_error = True\n    form.instance._error_list = True\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {\n        '__all__': ['Cannot set attribute', 'Another error'],\n        'title': ['This field cannot be blank.']\n    })\n"], "sample_627": ["def test_concat_no_variables() -> None:\n    ds1 = Dataset(coords={\"x\": [1, 2]})\n    ds2 = Dataset(coords={\"x\": [3, 4]})\n\n    with pytest.raises(ValueError, match=r\"At least one variable must be present to concatenate.\"):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_index_replacement() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [3, 4])})\n\n    expected = Dataset(coords={\"x\": (\"x\", [1, 2, 3, 4])})\n    actual = concat([ds1, ds2], dim=\"x\")\n\n    assert_identical(actual, expected)\n    assert isinstance(actual.indexes[\"x\"], PandasIndex)\n    assert_array_equal(actual.x.values, expected.x.values)\n", "def test_concat_overlapping_indexes() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [2, 3])})\n\n    with pytest.raises(ValueError, match=r\"Indexes overlap but no `join` method was provided.\"):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_not_all_indexes_mixed_types() -> None:\n    da1 = DataArray(1, coords={\"x\": 1}, dims=\"x\")\n    da2 = DataArray([3, 4], dims=\"y\")\n\n    with pytest.raises(\n        ValueError, match=r\"'y' must have either an index or no index in all datasets.*\"\n    ):\n        concat([da1, da2], dim=\"x\")\n", "def test_concat_different_index_types() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", pd.Index([1, 2], dtype=int))})\n    ds2 = Dataset(coords={\"x\": (\"x\", pd.Index([3, 4], dtype=float))})\n\n    with pytest.raises(\n        ValueError, match=r\"'x' must have either an index or no index in all datasets.*\"\n    ):\n        concat([ds1, ds2], dim=\"x\")\n"], "sample_606": ["def test_polyval_with_missing_values(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    xcoord = xr.DataArray(\n        pd.date_range(\"2000-01-01\", freq=\"D\", periods=10), dims=(\"x\",), name=\"x\"\n    )\n    xcoord[2] = np.nan\n\n    da = xr.DataArray(\n        np.stack((1.0 + xcoord + 2.0 * xcoord ** 2, 1.0 + 2.0 * xcoord + 3.0 * xcoord ** 2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": xcoord, \"d\": [0, 1]},\n    )\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n    )\n    if use_dask:\n        coeffs = coeffs.chunk({\"d\": 2})\n\n    da_pv = xr.polyval(da.x, coeffs)\n\n    # Exclude missing values from the comparison\n    xr.testing.assert_allclose(da.isel(x=~np.isnan(xcoord)), da_pv.T.isel(x=~np.isnan(xcoord)))\n", "def test_polyval_deg_coord_not_in_descending_order() -> None:\n    xcoord = np.arange(10)\n    da = xr.DataArray(\n        np.stack((1.0 + xcoord + 2.0 * xcoord ** 2, 1.0 + 2.0 * xcoord + 3.0 * xcoord ** 2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": xcoord, \"d\": [0, 1]},\n    )\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [0, 1, 2]},  # degree coord not in descending order\n    )\n\n    with pytest.raises(ValueError, match=\"degree coordinate is not in descending order\"):\n        xr.polyval(da.x, coeffs)\n", "def test_apply_ufunc_output_dtypes(use_dask) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n\n    array = np.arange(10, dtype=float)\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    expected_array = array.astype(int)\n    expected_variable = xr.Variable(\"x\", expected_array)\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -expected_array)])\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -expected_array})\n\n    if use_dask:\n        array = array.chunk(5)\n        variable = variable.chunk(5)\n        data_array = data_array.chunk(5)\n        dataset = dataset.chunk(5)\n\n    actual_array = apply_ufunc(np.floor, array, output_dtypes=[int])\n    actual_variable = apply_ufunc(np.floor, variable, output_dtypes=[int])\n    actual_data_array = apply_ufunc(np.floor, data_array, output_dtypes=[int])\n    actual_dataset = apply_ufunc(np.floor, dataset, output_dtypes=[int])\n\n    assert_identical(expected_array, actual_array)\n    assert_identical(expected_variable, actual_variable)\n    assert_identical(expected_data_array, actual_data_array)\n    assert_identical(expected_dataset, actual_dataset)\n", "def test_polyval_with_missing_values() -> None:\n    xcoord = xr.DataArray(\n        pd.date_range(\"2000-01-01\", freq=\"D\", periods=10).drop(labels=[2, 4]), dims=(\"x\",), name=\"x\"\n    )\n    x = xr.core.missing.get_clean_interp_index(xcoord, \"x\")\n\n    da = xr.DataArray(\n        np.stack((1.0 + x + 2.0 * x ** 2, 1.0 + 2.0 * x + 3.0 * x ** 2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": xcoord, \"d\": [0, 1]},\n    )\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n    )\n\n    da_pv = xr.polyval(da.x, coeffs)\n\n    # Check that the result has the same dimensions as the input\n    assert da_pv.dims == da.dims\n\n    # Check that the result is equal to the input where the input is not missing\n    xr.testing.assert_allclose(da.where(~da.isnull()), da_pv.where(~da.isnull()))\n\n    # Check that the result is missing where the input is missing\n    assert da_pv.isnull().equals(da.isnull())\n", "def test_apply_output_dtypes() -> None:\n    array = np.arange(10, dtype=np.float64)\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n        return x.astype(np.int64)\n\n    assert_identical(array.astype(np.int64), apply_ufunc(to_int, array))\n    assert_identical(variable.astype(np.int64), apply_ufunc(to_int, variable))\n    assert_identical(data_array.astype(np.int64), apply_ufunc(to_int, data_array))\n    assert_identical(dataset.astype(np.int64), apply_ufunc(to_int, dataset))\n\n    # with output_dtypes\n    assert_identical(array.astype(np.int64), apply_ufunc(to_int, array, output_dtypes=[np.int64]))\n    assert_identical(variable.astype(np.int64), apply_ufunc(to_int, variable, output_dtypes=[np.int64]))\n    assert_identical(data_array.astype(np.int64), apply_ufunc(to_int, data_array, output_dtypes=[np.int64]))\n    assert_identical(dataset.astype(np.int64), apply_ufunc(to_int, dataset, output_dtypes=[np.int64]))\n\n    # with output_dtypes and multiple outputs\n        return x.astype(np.float64), x.astype(np.int64)\n\n    assert_identical((array.astype(np.float64), array.astype(np.int64)), apply_ufunc(to_float_and_int, array, output_dtypes=[np.float"], "sample_867": ["def test_search_cv_with_groups():\n    # Test that GridSearchCV and RandomizedSearchCV handle groups correctly\n    X, y = make_classification(n_samples=15, n_classes=2, random_state=0)\n    groups = np.random.RandomState(0).randint(0, 3, 15)\n\n    clf = LinearSVC(random_state=0)\n    grid = {'C': [1]}\n\n    group_cvs = [LeaveOneGroupOut(), LeavePGroupsOut(2),\n                 GroupKFold(n_splits=3), GroupShuffleSplit()]\n    for cv in group_cvs:\n        gs = GridSearchCV(clf, grid, cv=cv)\n        rs = RandomizedSearchCV(clf, grid, n_iter=3, cv=cv)\n        gs.fit(X, y, groups=groups)\n        rs.fit(X, y, groups=groups)\n        assert_array_equal(gs.cv_results_['params'], rs.cv_results_['params'])\n        assert_array_almost_equal(gs.cv_results_['mean_test_score'],\n                                  rs.cv_results_['mean_test_score'])\n", "def test_search_cv_empty_parameter_grid():\n    # Test that GridSearchCV and RandomizedSearchCV raise a ValueError\n    # when an empty parameter grid is provided.\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    clf = SVC(random_state=0)\n    empty_param_grid = {}\n    with pytest.raises(ValueError, match=\"Parameter grid for parameter search is empty.\"):\n        GridSearchCV(clf, param_grid=empty_param_grid).fit(X, y)\n    with pytest.raises(ValueError, match=\"Parameter grid for parameter search is empty.\"):\n        RandomizedSearchCV(clf, param_distributions=empty_param_grid, n_iter=1).fit(X, y)\n", "def test_search_cv_with_estimator_without_fit():\n    class NoFitEstimator:\n            return np.zeros(X.shape[0])\n\n    X = np.arange(20).reshape(5, -1)\n    y = [0, 0, 1, 1, 1]\n    with pytest.raises(ValueError, match=\"Estimator NoFitEstimator must implement a 'fit' method.\"):\n        grid_search = GridSearchCV(NoFitEstimator(), {'foo_param': [1, 2, 3]}, cv=2)\n        grid_search.fit(X, y)\n", "def test_search_cv_with_group_k_fold():\n    # Test search with GroupKFold\n    X, y = make_blobs(n_samples=15, n_features=2, centers=3, random_state=0)\n    groups = np.array([0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5])\n\n    estimator = LinearSVC()\n    param_grid = {'C': [1, 10]}\n\n    cv = GroupKFold(n_splits=3)\n\n    search = GridSearchCV(estimator, param_grid, cv=cv)\n    search.fit(X, y, groups=groups)\n\n    assert hasattr(search, \"cv_results_\")\n    assert \"params\" in search.cv_results_\n    assert \"mean_test_score\" in search.cv_results_\n    assert \"rank_test_score\" in search.cv_results_\n", "def test_grid_search_cv_results_rank_tie_breaking_multimetric():\n    X, y = make_classification(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n    scoring = {'accuracy': make_scorer(accuracy_score),\n               'recall': make_scorer(recall_score)}\n\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid,\n                               scoring=scoring, refit=False,\n                               return_train_score=True)\n    grid_search.fit(X, y)\n\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_accuracy'][0],\n                        cv_results['mean_test_accuracy'][1])\n    assert_almost_equal(cv_results['mean_train_accuracy'][0],\n                        cv_results['mean_train_accuracy'][1])\n    assert not np.allclose(cv_results['mean_test_accuracy'][1],\n                           cv_results['mean_test_accuracy'][2])\n    assert not np.allclose(cv_results['mean_train_accuracy'][1],\n                           cv_results['mean_train_accuracy'][2])\n    # 'min' rank should be assigned to the tied candidates for accuracy\n    assert_almost_equal(grid_search.cv_results_['rank_test_accuracy'], [1, 1, 3])\n    # For recall, the rank should be different as the scores are different\n    assert_array_equal(grid_search.cv_results_['rank_test_recall'], [1, 3, 2])\n"], "sample_178": ["def test_formset_with_extra_and_deletion(self):\n    \"\"\"FormSets with extra and deletion.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=2, can_delete=True)\n    initial = [{'choice': 'Calexico', 'votes': 100}]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    self.assertHTMLEqual(\n        '\\n'.join(form.as_ul() for form in formset.forms),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_formset_with_custom_error_class(self):\n    \"\"\"Formsets' errors can use a custom error class.\"\"\"\n    class CustomErrorList(ErrorList):\n        pass\n\n    data = {\n        'drinks-TOTAL_FORMS': '2',\n        'drinks-INITIAL_FORMS': '0',\n        'drinks-MIN_NUM_FORMS': '0',\n        'drinks-MAX_NUM_FORMS': '0',\n        'drinks-0-name': 'Gin and Tonic',\n        'drinks-1-name': 'Gin and Tonic',\n    }\n    formset = FavoriteDrinksFormSet(data, prefix='drinks', error_class=CustomErrorList)\n    self.assertFalse(formset.is_valid())\n    self.assertIsInstance(formset.non_form_errors(), CustomErrorList)\n", "def test_formset_initial_form_count(self):\n    \"\"\"Test the initial_form_count method of the BaseFormSet class.\"\"\"\n    # Test with an unbound formset\n    formset = self.make_choiceformset()\n    self.assertEqual(formset.initial_form_count(), 0)\n\n    # Test with a bound formset\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '1',\n    }\n    formset = self.make_choiceformset(data)\n    self.assertEqual(formset.initial_form_count(), 1)\n\n    # Test with an unbound formset and initial data\n    initial = [{'choice': 'Calexico', 'votes': 100}]\n    formset = self.make_choiceformset(initial=initial)\n    self.assertEqual(formset.initial_form_count(), 1)\n", "def test_formset_with_extra_initial_forms(self):\n    \"\"\"\n    Extra forms are not included in the initial_form_count if they don't have\n    initial data.\n    \"\"\"\n    LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, extra=3, min_num=2)\n    formset = LimitedFavoriteDrinkFormSet(initial=[{'name': 'Gin Tonic'}])\n    self.assertEqual(formset.initial_form_count(), 1)\n", "def test_extra_forms_unfilled_after_initial(self):\n    \"\"\"\n    Extra forms after initial forms are not considered when checking if\n    formset.has_changed().\n    \"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '1',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=1)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.has_changed())\n"], "sample_1124": ["def test_FracElement_compose():\n    F, x, y = field(\"x, y\", QQ)\n    G, t = field(\"t\", F)\n\n    f = (x + 1) / (x * (x + y)**Rational(3, 2))\n    g = t.compose(x, (t - 1) / t)\n\n    assert g == (t + 1) / (t * (2*t - 1)**Rational(3, 2))\n", "def test_FracElement___eq__():\n    F, x, y, z = field(\"x,y,z\", QQ)\n\n    assert F(1) == 1\n    assert F(1) != 2\n\n    assert x == x\n    assert x != y\n\n    assert x/y == x/y\n    assert x/y != y/x\n\n    assert x/y != x\n    assert x/y != y\n", "def test_FracElement_compose():\n    F, x, y, z = field(\"x,y,z\", ZZ)\n    Fxy = field(\"x,y\", ZZ)[0]\n    f = (x*y + z)/(x + y)\n\n    g = Fxy(x, y)\n    h = Fxy(y, x)\n\n    assert f.compose(x, g) == (x*y + z)/(x + y)\n    assert f.compose(y, h) == (x*y + z)/(y + x)\n", "def test_FracElement_compose():\n    F, x, y = field(\"x,y\", ZZ)\n    G, u, v = field(\"u,v\", F)\n    f = (u + v)/(u - v)\n    g = (x + y)/(x - y)\n    h = f.compose(u, g)\n    assert h == (2*x + y)/(x - y)\n", "def test_FracElement_compose():\n    F, x, y = field(\"x, y\", ZZ)\n    G, u, v = field(\"u, v\", F)\n    f = (x*u + y*v) / (u + v)\n    g = (u + 1) / (v + 1)\n    h = f.compose(u, g)\n    expected_h = (x + y) / (2 + y)\n    assert h == expected_h\n"], "sample_100": ["    def test_extra_files_included(self):\n        reloader = autoreload.StatReloader()\n        extra_file = self.temporary_file('extra_file.py')\n        extra_file.touch()\n        reloader.extra_files.add(extra_file)\n        watched_files = list(reloader.watched_files())\n        self.assertIn(extra_file, watched_files)\n", "def test_snapshot_files_does_not_trigger_twice(self, mock_notify_file_changed):\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n        self.increment_mtime(self.existing_file)\n        snapshot1 = dict(self.reloader.snapshot_files())\n        snapshot2 = dict(self.reloader.snapshot_files())\n        self.assertEqual(snapshot1, snapshot2)\n", "def test_watch_dir_with_nonexistent_directory(self, mocked_modules, notify_mock):\n    nonexistent_directory = self.tempdir / 'nonexistent_dir'\n    self.reloader.watch_dir(nonexistent_directory, '*.py')\n    with self.tick_twice():\n        # Simulate a change in a nonexistent directory\n        self.increment_mtime(nonexistent_directory / 'nonexistent_file.py')\n    self.assertEqual(notify_mock.call_count, 0)\n", "    def test_notify_file_changed(self):\n        reloader = autoreload.BaseReloader()\n        path = Path('/path/to/file')\n        with mock.patch.object(autoreload, 'trigger_reload') as mocked_trigger_reload:\n            reloader.notify_file_changed(path)\n            self.assertEqual(autoreload.file_changed.send.call_count, 1)\n            self.assertSequenceEqual(\n                autoreload.file_changed.send.call_args[1],\n                {'sender': reloader, 'file_path': path}\n            )\n            self.assertEqual(mocked_trigger_reload.call_count, 1)\n            self.assertSequenceEqual(mocked_trigger_reload.call_args[0], [path])\n", "def test_glob_non_existing_file(self, mocked_modules, notify_mock):\n    non_py_file = self.ensure_file(self.tempdir / 'non_exist.txt')\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        self.increment_mtime(non_py_file)\n    self.assertEqual(notify_mock.call_count, 0)\n"], "sample_755": ["def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 10)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5)\n    labels = [0] * 10 + [1] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  (2.5 / 3 + 2.5 / 3) / 2)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  0.75, abs=1e-2)  # Expected value can be changed as per requirement\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    # Precomputed scores for this specific case\n    centroids = np.array([[0.5, 2.0], [3.5, 1.5], [0.5, 1.5], [3.5, 2.0]])\n    centroid_distances = pairwise_distances(centroids)\n    intra_dists = np.array([2.0, 2.0, 1.5, 1.5])\n    combined_intra_dists = intra_dists[:, None] + intra_dists\n    scores = np.max(combined_intra_dists / centroid_distances, axis=1)\n    expected_score = np.mean(scores)\n    pytest.approx(davies_bouldin_score(X, labels), expected_score)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  0.9833333333333334)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  (80 / 16 + 80 / 12 + 80 / 8 + 80 / 12) / 4, abs=1e-2)\n"], "sample_879": ["def test_ordinal_encoder_missing_unknown_encoding_min():\n    \"\"\"Check missing value or unknown encoding can equal the minimum cardinality.\"\"\"\n    X = np.array([[\"dog\"], [\"cat\"], [np.nan]], dtype=object)\n    X_trans = OrdinalEncoder(encoded_missing_value=-1).fit_transform(X)\n    assert_allclose(X_trans, [[0], [1], [-1]])\n\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(X)\n    X_test = np.array([[\"snake\"]])\n    X_trans = enc.transform(X_test)\n    assert_allclose(X_trans, [[-1]])\n", "def test_ordinal_encoder_missing_unknown_encoding_min():\n    \"\"\"Check missing value or unknown encoding can equal the minimum category.\"\"\"\n    X = np.array([[\"dog\"], [\"cat\"], [np.nan]], dtype=object)\n    X_trans = OrdinalEncoder(encoded_missing_value=-1).fit_transform(X)\n    assert_allclose(X_trans, [[0], [1], [-1]])\n\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(X)\n    X_test = np.array([[\"snake\"]])\n    X_trans = enc.transform(X_test)\n    assert_allclose(X_trans, [[-1]])\n", "def test_ordinal_encoder_missing_unknown_encoding_min():\n    \"\"\"Check missing value or unknown encoding can equal the minimum category.\"\"\"\n    X = np.array([[\"dog\"], [\"cat\"], [np.nan]], dtype=object)\n    X_trans = OrdinalEncoder(encoded_missing_value=-1).fit_transform(X)\n    assert_allclose(X_trans, [[1], [0], [-1]])\n\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(X)\n    X_test = np.array([[\"snake\"]])\n    X_trans = enc.transform(X_test)\n    assert_allclose(X_trans, [[-1]])\n", "def test_one_hot_encoder_sparse_output(sparse_output):\n    X = np.array([[\"Male\", 1], [\"Female\", 3], [\"Female\", 2]])\n    ohe = OneHotEncoder(sparse_output=sparse_output)\n    X_trans = ohe.fit_transform(X)\n\n    if sparse_output:\n        assert sparse.isspmatrix_csr(X_trans)\n    else:\n        assert isinstance(X_trans, np.ndarray)\n", "def test_ordinal_encoder_unknown_value_error():\n    \"\"\"Check OrdinalEncoder errors when unknown_value is used by an known category.\"\"\"\n    X = np.array([[\"a\"], [\"b\"]], dtype=object)\n\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=\"a\")\n\n    with pytest.raises(ValueError, match=\"The used value for unknown_value \"\n                                         \"\\\\('a'\\\\) is one of the values already used \"\n                                         \"for encoding the seen categories.\"):\n        enc.fit(X)\n"], "sample_335": ["def test_decimalfield_max_value_validation(self):\n    f = DecimalField(max_digits=4, decimal_places=2, max_value=1000)\n    with self.assertRaisesMessage(ValidationError, \"'Ensure this value is less than or equal to 1000.'\"):\n        f.clean('1000.01')\n    self.assertEqual(f.clean('999.99'), decimal.Decimal(\"999.99\"))\n", "def test_decimalfield_out_of_range(self):\n    f = DecimalField(max_digits=4, decimal_places=2)\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('1e309')\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('-1e309')\n", "def test_decimalfield_min_max_values(self):\n    f = DecimalField(max_value=10, min_value=0)\n    with self.assertRaisesMessage(ValidationError, \"'Ensure this value is less than or equal to 10.'\"):\n        f.clean('10.00000001')\n    with self.assertRaisesMessage(ValidationError, \"'Ensure this value is greater than or equal to 0.'\"):\n        f.clean('-0.00000001')\n", "def test_decimalfield_infinity_and_nan(self):\n    f = DecimalField(max_digits=4, decimal_places=2)\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean(float('inf'))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean(float('-inf'))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean(float('nan'))\n", "def test_decimalfield_negative_values(self):\n    f = DecimalField(max_digits=5, decimal_places=2)\n    self.assertEqual(f.clean('-1.23'), decimal.Decimal(\"-1.23\"))\n    self.assertEqual(f.clean('-.23'), decimal.Decimal(\"-0.23\"))\n    self.assertEqual(f.clean('-0.23'), decimal.Decimal(\"-0.23\"))\n    self.assertEqual(f.clean('-000.23'), decimal.Decimal(\"-0.23\"))\n    with self.assertRaisesMessage(ValidationError, \"'Ensure that there are no more than 2 decimal places.'\"):\n        f.clean('-000.234')\n    with self.assertRaisesMessage(ValidationError, \"'Ensure that there are no more than 3 digits before the decimal point.'\"):\n        f.clean('-0000.23')\n"], "sample_222": ["def test_unlock_without_lock(self):\n    file_path = Path(__file__).parent / 'test.png'\n    with open(file_path) as f:\n        self.assertIs(locks.unlock(f), True)\n", "    def test_unlock_without_lock_raises_error(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path) as f:\n            with self.assertRaises(IOError):\n                locks.unlock(f)\n", "def test_lock_context_manager(self):\n    file_path = Path(__file__).parent / 'test.png'\n    with open(file_path) as f:\n        with locks.lock(f, locks.LOCK_EX):\n            self.assertRaises(BlockingIOError, locks.lock, f, locks.LOCK_EX | locks.LOCK_NB)\n        # File should be unlocked after exiting the context manager\n        self.assertIs(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB), True)\n", "    def test_locked_file_is_not_readable(self):\n        file_path = Path(__file__).parent / 'test.txt'\n        with open(file_path, 'w') as f:\n            f.write('content')\n        with open(file_path) as f1:\n            locks.lock(f1, locks.LOCK_EX)\n            with open(file_path, 'r') as f2:\n                with self.assertRaises(IOError):\n                    f2.read()\n            locks.unlock(f1)\n", "    def test_file_move_safe_no_overwrite_error(self):\n        handle_a, self.file_a = tempfile.mkstemp()\n        handle_b, self.file_b = tempfile.mkstemp()\n        os.remove(self.file_b)\n\n        # file_move_safe() raises FileNotFoundError if the destination file does not exist and\n        # allow_overwrite is False.\n        with self.assertRaises(FileNotFoundError):\n            file_move_safe(self.file_a, self.file_b, allow_overwrite=False)\n\n        os.close(handle_a)\n        os.close(handle_b)\n"], "sample_392": ["def test_key_transform_numeric_lookups(self):\n    obj = NullableJSONModel.objects.create(value={\"num\": 10})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__num__lt=20),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__num__lte=10),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__num__gt=5),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__num__gte=10),\n        [obj],\n    )\n", "def test_key_lt(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__lt=4.3),\n        [self.objs[4]],\n    )\n    self.assertIs(NullableJSONModel.objects.filter(value__p__lt=4.2).exists(), False)\n", "def test_key_transform_annotation_expression_with_none(self):\n    obj = NullableJSONModel.objects.create(value={\"d\": [\"e\", None, \"g\"]})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__0__isnull=False)\n        .annotate(\n            key=F(\"value__d\"),\n            chain=F(\"key__1\"),\n            expr=Cast(\"key\", models.JSONField()),\n        )\n        .filter(chain=None),\n        [obj],\n    )\n", "def test_key_transform_with_sql_injection(self):\n    with CaptureQueriesContext(connection) as queries:\n        self.assertIs(\n            NullableJSONModel.objects.filter(\n                value__contains=KeyTransform('test\\' = \\'\"a\"\\') OR 1 = 1 OR (\\'d', 'value')\n            ).exists(),\n            False,\n        )\n    self.assertIn(\n        \"\"\".\"value\" @> '{\"test\\' = \\'\"a\"\\'\"') OR 1 = 1 OR (\\'\"d\"}': '\"x\"' \"\"\",\n        queries[0][\"sql\"],\n    )\n", "def test_key_transform_numeric_lookups(self):\n    tests = [\n        (\"value__c__lt\", 15),\n        (\"value__c__lte\", 14),\n        (\"value__c__gt\", 13),\n        (\"value__c__gte\", 14),\n        (\"value__p__lt\", 4.3),\n        (\"value__p__lte\", 4.2),\n        (\"value__p__gt\", 4.1),\n        (\"value__p__gte\", 4.2),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(**{lookup: value}),\n                [self.objs[3], self.objs[4]],\n            )\n"], "sample_1109": ["def test_complex_floor_ceiling():\n    assert floor(2 + 3*I) == 2 + 3*I\n    assert ceiling(2 + 3*I) == 2 + 3*I\n    assert floor(2 - 3*I) == 2 - 3*I\n    assert ceiling(2 - 3*I) == 2 - 3*I\n    assert floor(-2 + 3*I) == -2 + 3*I\n    assert ceiling(-2 + 3*I) == -2 + 3*I\n    assert floor(-2 - 3*I) == -2 - 3*I\n    assert ceiling(-2 - 3*I) == -2 - 3*I\n", "def test_frac_rewrite():\n    assert frac(y).rewrite(floor).subs(y, pi) == pi - floor(pi)\n    assert frac(y).rewrite(floor).subs(y, -E) == -E - floor(-E)\n    assert frac(y).rewrite(ceiling).subs(y, -pi) == -pi + ceiling(pi)\n    assert frac(y).rewrite(ceiling).subs(y, E) == E - ceiling(-E)\n", "def test_issue_18689_variation():\n    assert floor(floor(ceiling(x)) + 2) == ceiling(x) + 2\n    assert ceiling(ceiling(floor(x)) + 2) == floor(x) + 2\n    assert floor(ceiling(ceiling(x)) + 2) == ceiling(x) + 2\n", "def test_frac_comparison_with_symbols():\n    x, y = symbols('x y', real=True)\n    assert frac(x) >= 0\n    assert frac(x) < 1\n    assert frac(x) <= y\n    assert frac(x) >= -y\n    assert frac(x) < y + 1\n    assert frac(x) >= y - 1\n    assert frac(x) < -y + 1\n    assert frac(x) >= -y - 1\n", "def test_complex_floor_ceiling():\n    assert floor(3 + 2*I) == 3 + 2*I\n    assert ceiling(3 + 2*I) == 3 + 2*I\n    assert floor(3 - 2*I) == 3 - 2*I\n    assert ceiling(3 - 2*I) == 3 - 2*I\n    assert floor(3 + 2*I + 1) == 4 + 2*I\n    assert ceiling(3 - 2*I + 1) == 4 - 2*I\n    assert floor(3*I + 2) == 3*I + 2\n    assert ceiling(3*I - 2) == 3*I - 2\n    assert floor(3*I + 2*I) == 5*I\n    assert ceiling(3*I - 2*I) == I\n    assert floor(3*I - 2*I + 1) == 1 + I\n"], "sample_310": ["    def test_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_list'), 'List')\n        self.assertEqual(get_return_data_type('get_count'), 'Integer')\n        self.assertEqual(get_return_data_type('other_function'), '')\n", "def test_extract_views_from_urlpatterns(self):\n    urlpatterns = [\n        path('admin/doc/', include('django.contrib.admindocs.urls'), name='django-admindocs-docroot'),\n        path('admin/', admin.site.urls),\n    ]\n    extracted_views = views.extract_views_from_urlpatterns(urlpatterns)\n    self.assertEqual(len(extracted_views), 2)\n    self.assertIn(views.BaseAdminDocsView.as_view(), [view[0] for view in extracted_views])\n    self.assertIn(admin.site.index, [view[0] for view in extracted_views])\n", "def test_simplify_regex(self):\n    self.assertEqual(simplify_regex('^(?P<sport_slug>\\\\w+)/athletes/(?P<athlete_slug>\\\\w+)/$'), '/<sport_slug>/athletes/<athlete_slug>/')\n    self.assertEqual(simplify_regex('^(?P<pk>[0-9]+)/$'), '/<pk>/')\n    self.assertEqual(simplify_regex('^(?P<year>[0-9]{4})/(?P<month>[0-9]{2})/$'), '/<year>/<month>/')\n    self.assertEqual(simplify_regex('^(?P<slug>[\\\\w-]+)/$'), '/<slug>/')\n    self.assertEqual(simplify_regex('^$'), '/')\n    self.assertEqual(simplify_regex('^(?P<pk>[0-9]+)/edit/$'), '/<pk>/edit/')\n", "    def test_simplify_regex_basic(self):\n        self.assertEqual(simplify_regex(r'^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'), '/<sport_slug>/athletes/<athlete_slug>/')\n", "def test_simplify_regex(self):\n    # Test cases for simplify_regex function\n    test_cases = {\n        '^(?P<sport_slug>\\\\w+)/athletes/(?P<athlete_slug>\\\\w+)/$': '/<sport_slug>/athletes/<athlete_slug>/',\n        '^$': '/',\n        '^admin/$': '/admin/',\n        '^admin/doc/$': '/admin/doc/',\n    }\n    for pattern, expected in test_cases.items():\n        with self.subTest(pattern=pattern):\n            self.assertEqual(simplify_regex(pattern), expected)\n"], "sample_1053": ["def test_GoldenRatio_approximation_interval():\n    assert GoldenRatio.approximation_interval(Integer) == (S.One, Rational(2))\n    assert GoldenRatio.approximation_interval(Rational) is None\n", "def test_Float_floor_ceiling():\n    a = Float(3.7)\n    b = Float(3.2)\n\n    assert a.floor() == 3\n    assert b.floor() == 3\n    assert a.ceiling() == 4\n    assert b.ceiling() == 4\n", "def test_Float_power():\n    x = Float('1.5')\n    y = Float('2.0')\n    z = Float('-3.0')\n\n    assert (x ** y) == Float('2.25')\n    assert (x ** z) == Float('0.111111111111111')\n    assert (y ** z) == Float('0.125')\n    assert (z ** y) == Float('-9.0')\n", "def test_NumberSymbol_expand_complex():\n    assert (pi.expand(complex=True) == pi)\n    assert ((E + I).expand(complex=True) == E * exp(I))\n    assert ((1 + I).expand(complex=True) == exp(I / 2))\n    assert ((1 - I).expand(complex=True) == exp(-I / 2))\n", "def test_Number_as_coefficient():\n    assert S(2).as_coefficient(Integer(2)) == (S.One, S(2))\n    assert S(2).as_coefficient(Integer(3)) == (S(2), S.One)\n    assert S(2).as_coefficient(Rational(1, 2)) == (S(4), S.One)\n    assert S(2).as_coefficient(Rational(2, 1)) == (S.One, S(2))\n    assert S(2).as_coefficient(Float(2.0)) == (S(2), S.One)\n    assert S(2).as_coefficient(S(3)) == (S(2), S.One)\n"], "sample_1129": ["def test_log1p():\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log1p(x)'\n", "def test_expm1_log1p():\n    expr1 = expm1(x)\n    expr2 = log1p(x)\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.expm1(x)'\n    assert prntr.doprint(expr2) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.expm1(x)'\n    assert prntr.doprint(expr2) == 'mpmath.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.expm1(x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.expm1(x)'\n    assert prntr.doprint(expr2) == 'scipy.special.log1p(x)'\n", "def test_log1p():\n    expr = log1p(x)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log1p(x)'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'sympy.functions.special.loggamma.loggamma(x + 1)'\n", "def test_log1p():\n    expr = log1p(x)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x + 1)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log1p(x)'\n", "def test_SymPyPrinter_print_func():\n    from sympy import Function\n\n    f = Function('f')\n    expr = f(x)\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'f(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # f\\nf(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # f\\nf(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with SciPy:\\n  # f\\nf(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with mpmath:\\n  # f\\nf(x)'\n"], "sample_528": ["def test_context_after_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n        assert mpl.rcParams[PARAM] == original_value\n        with style.context('test', after_reset=False):\n            assert mpl.rcParams[PARAM] == VALUE\n        assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] == original_value\n", "def test_context_after_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n        # Check that this value is not reset after the exiting the context.\n        assert mpl.rcParams[PARAM] == VALUE\n    # Check that this value is reset after exiting the outer context.\n    assert mpl.rcParams[PARAM] == original_value\n", "def test_context_with_reset():\n    # Test context with after_reset=True\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n    # Check that this value is reset after the exiting the context.\n    assert mpl.rcParams[PARAM] == original_value\n", "def test_blacklist():\n    blacklisted_param = 'interactive'\n    settings = {blacklisted_param: 'True'}\n    with temp_style('test_blacklist', settings):\n        with pytest.warns(UserWarning, match=f\"Style includes a parameter, '{blacklisted_param}', that is not related to style.\"):\n            style.use('test_blacklist')\n    assert mpl.rcParams[blacklisted_param] != 'True'\n", "def test_use_list():\n    mpl.rcParams[PARAM] = 'gray'\n    with temp_style('test1', DUMMY_SETTINGS):\n        with temp_style('test2', {PARAM: 'yellow'}):\n            with style.context(['test1', 'test2']):\n                assert mpl.rcParams[PARAM] == 'yellow'\n    assert mpl.rcParams[PARAM] == 'gray'\n"], "sample_700": ["def test_mark_skip_item(pytester: Pytester) -> None:\n    # Ensure pytest.mark.skip works with non-Python Item\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        class MyItem(pytest.Item):\n            nodeid = 'foo'\n                marker = pytest.mark.skip(\"Skipping this test\")\n                self.add_marker(marker)\n\n            return MyItem.from_parent(name=\"foo\", parent=parent)\n    \"\"\"\n    )\n    result = pytester.inline_run()\n    passed, skipped, failed = result.listoutcomes()\n    assert not failed\n    assert skipped\n", "def test_mark_skip_item(pytester: Pytester) -> None:\n    # Ensure pytest.mark.skip works with non-Python Item\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        class MyItem(pytest.Item):\n            nodeid = 'foo'\n                marker = pytest.mark.skip(reason=\"Skipped for testing\")\n                self.add_marker(marker)\n                assert False\n\n            return MyItem.from_parent(name=\"foo\", parent=parent)\n    \"\"\"\n    )\n    result = pytester.inline_run()\n    passed, skipped, failed = result.listoutcomes()\n    assert not failed\n    assert skipped\n", "def test_mark_skip_item(pytester: Pytester) -> None:\n    # Ensure pytest.mark.skip works with non-Python Item\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        class MyItem(pytest.Item):\n            nodeid = 'foo'\n                marker = pytest.mark.skip(\"Skipped test\")\n                self.add_marker(marker)\n                assert False\n\n            return MyItem.from_parent(name=\"foo\", parent=parent)\n    \"\"\"\n    )\n    result = pytester.inline_run()\n    passed, skipped, failed = result.listoutcomes()\n    assert not failed\n    skipped_test = [r for r in skipped if hasattr(r, \"skipped\")]\n    assert skipped_test\n", "def test_skipif_mark_eval_namespace(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            return {\"color\": \"red\"}\n        \"\"\"\n    )\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"color == 'green'\")\n            assert False\n\n        @pytest.mark.skipif(\"color == 'red'\")\n            assert False\n    \"\"\"\n    )\n    res = pytester.runpytest(p)\n    assert res.ret == 0\n    res.stdout.fnmatch_lines([\"*1 skipped*\"])\n    res.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_skipif_condition_keyword_no_run(self, pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(condition=False, run=False)\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(p, \"-rxX\")\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n    assert result.ret == 0\n"], "sample_248": ["def test_shell_with_bpython_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n        call_command('shell', interface='bpython')\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "def test_shell_with_bpython_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n        call_command('shell', interface='bpython')\n", "def test_shell_with_bpython_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n        call_command('shell', interface='bpython')\n", "def test_shell_with_bpython_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n        call_command('shell', interface='bpython')\n"], "sample_519": ["def test_deepcopy_with_colorbar():\n    fig1, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(2, 2))\n    fig1.colorbar(pc)\n\n    fig2 = copy.deepcopy(fig1)\n\n    # Make sure it is a new object\n    assert fig2.axes[0] is not ax\n    # And that the colorbar got propagated\n    assert len(fig2.axes) == 2\n    assert fig2.axes[1].colorbar is not None\n", "def test_add_subplot_multiple_calls():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(2, 2, 1)\n    ax2 = fig.add_subplot(2, 2, 2)\n    ax3 = fig.add_subplot(2, 2, 3)\n    ax4 = fig.add_subplot(2, 2, 4)\n    ax1_new = fig.add_subplot(ax1)\n    ax2_new = fig.add_subplot(ax2)\n    ax3_new = fig.add_subplot(ax3)\n    ax4_new = fig.add_subplot(ax4)\n    assert ax1_new is ax1\n    assert ax2_new is ax2\n    assert ax3_new is ax3\n    assert ax4_new is ax4\n", "def test_subfigures_clear():\n    fig = plt.figure()\n    subfig1 = fig.subfigures(1, 2)[0]\n    ax1 = subfig1.add_subplot(111)\n    subfig2 = fig.subfigures(1, 2)[1]\n    ax2 = subfig2.add_subplot(111)\n\n    fig.clear()\n\n    assert len(fig.axes) == 0\n    assert len(fig.subfigs) == 0\n", "def test_subfigure_add_axes():\n    # test that we can add axes to a subfigure...\n    fig = plt.figure(constrained_layout=True)\n    sub_fig = fig.subfigures()\n    ax = sub_fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    ax.plot([1, 2, 3], [1, 2, 3])\n    fig.draw_without_rendering()\n    assert ax in sub_fig.axes\n    assert ax in fig.axes\n", "def test_tight_layout_parameters(tight_layout, w_pad, h_pad, wspace, hspace):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot([0, 1], [2, 3])\n\n    if tight_layout:\n        fig.tight_layout(pad=1.08, w_pad=w_pad, h_pad=h_pad, rect=(0, 0, 1, 1))\n        assert fig.get_tight_layout()\n    else:\n        fig.set_tight_layout(tight_layout)\n        assert not fig.get_tight_layout()\n\n    info = fig.get_layout_engine().get_info()\n    assert info['w_pad'] == w_pad\n    assert info['h_pad'] == h_pad\n    assert info['wspace'] == wspace\n    assert info['hspace'] == hspace\n"], "sample_1163": ["def test_issue_14239():\n    # doesn't cause recursion error\n    r = Symbol('r', real=True)\n    assert sign(r + Piecewise((0, r > 0), (1 - r, True)))\n", "def test_Abs_is_finite():\n    x = Symbol('x')\n    assert Abs(x).is_finite is None\n\n    f = Symbol('f', finite=True)\n    assert Abs(f).is_finite is True\n\n    z = Symbol('z', complex=True, zero=False)\n    assert Abs(z).is_finite is True\n\n    p = Symbol('p', positive=True)\n    assert Abs(p).is_finite is True\n", "def test_sign_zero():\n    z = Symbol('z', zero=True, complex=True)\n    assert sign(z).is_zero is True\n    assert sign(z).is_integer is True\n    assert sign(z).is_real is True\n    assert sign(z).is_imaginary is False\n    assert sign(z).doit() == 0\n    assert sign(Abs(z)) == 0\n    assert Abs(sign(z)) == 0\n", "def test_Abs_properties_zero():\n    z = Symbol('z', zero=True)\n    assert Abs(z).is_real is True\n    assert Abs(z).is_extended_real is True\n    assert Abs(z).is_rational is True\n    assert Abs(z).is_positive is False\n    assert Abs(z).is_nonnegative is True\n    assert Abs(z).is_extended_positive is False\n    assert Abs(z).is_zero is True\n\n    assert Abs(z).is_integer is None\n    assert Abs(z).is_even is None\n    assert Abs(z).is_odd is None\n", "def test_issue_14238_integral():\n    # doesn't cause recursion error with Integral\n    r = Symbol('r', real=True)\n    f = Function('f')\n    assert Abs(Integral(r + Piecewise((0, r > 0), (1 - r, True)), (r, 0, 1))).doit() == \\\n           Integral(Piecewise((r, r <= 1), (0, True)), (r, 0, 1)) + Integral(Piecewise((1, r > 1), (-r, True)), (r, 0, 1))\n"], "sample_747": ["def test_power_transformer_y_johnson():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_y_johnson_method():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_yjohnson_2d():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_yeo_johnson():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    pt = PowerTransformer(method='yeo-johnson')\n    X_trans = pt.fit_transform(X)\n\n    for j in range(X_trans.shape[1]):\n        X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n        assert_almost_equal(X_trans[:, j], X_expected)\n        assert_almost_equal(lmbda, pt.lambdas_[j])\n\n    # Test inverse transformation\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n\n    assert len(pt.lambdas_) == X.shape[1]\n    assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_partial_fit_exception():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_2d)\n\n    # An exception should be raised if partial_fit is called with a different number of features\n    wrong_shape_message = 'Input data has a different number of features'\n\n    pt.partial_fit(X)\n    assert_raise_message(ValueError, wrong_shape_message,\n                         pt.partial_fit, X[:, 0:1])\n"], "sample_1021": ["def test_quaternion_multiplication_with_scalar():\n    q = Quaternion(1, 2, 3, 4)\n    scalar = 2\n\n    # Test multiplication with a scalar\n    result = q * scalar\n    expected_result = Quaternion(2, 4, 6, 8)\n    assert result == expected_result\n\n    # Test multiplication with a scalar using __rmul__\n    result = scalar * q\n    expected_result = Quaternion(2, 4, 6, 8)\n    assert result == expected_result\n", "def test_quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    assert q1.mul(q2) == Quaternion(-60, 12, 30, 24)\n    assert q1.mul(2) == Quaternion(2, 4, 6, 8)\n    assert q1 * q2 == Quaternion(-60, 12, 30, 24)\n    assert q1 * 2 == Quaternion(2, 4, 6, 8)\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    assert q3.mul(2 + 3*I) == Quaternion((2 + 3*I)*(3 + 4*I), (2 + 3*I)*(2 + 5*I), 0, (2 + 3*I)*(7 + 8*I))\n    assert q3 * (2 + 3*I) == Quaternion((2 + 3*I)*(3 + 4*I), (2 + 3*I)*(2 + 5*I), 0, (2 + 3*I)*(7 + 8*I))\n", "def test_quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 * q2 == Quaternion(-60, 12, 30, 24)\n    assert q1 * 2 == Quaternion(2, 4, 6, 8)\n    assert 2 * q1 == Quaternion(2, 4, 6, 8)\n    assert q1 * (2 + 3*I) == Quaternion(2, 7, 6, 8)\n    assert (2 + 3*I) * q1 == Quaternion(2, 7, 6, 8)\n\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field = False)\n    q4 = Quaternion(1, 4, 7, 8)\n\n    assert q3 * q4 == Quaternion(-58, 12, 26, 20)\n    assert q3 * (2 + 3*I) == Quaternion(2 + 3*I, 11 + 20*I, 0, 23 + 24*I)\n", "def test_quaternion_noncommutative_arguments():\n    # Create a non-commutative symbol\n    nc_sym = symbols('nc', commutative=False)\n\n    # Try to create a Quaternion with non-commutative arguments\n    # This should raise a ValueError\n    with raises(ValueError):\n        q = Quaternion(x, nc_sym, z, w)\n", "def test_quaternion_complex_field_operations():\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field = False)\n    q4 = Quaternion(1 - 2*I, 3 + I, 5, 7 - 3*I, real_field = False)\n\n    assert q3 + q4 == Quaternion(4 + 2*I, 5 + 6*I, 5, 14 - 2*I)\n    assert q3 * q4 == Quaternion(-39 - 16*I, -21 + 14*I, 35, 26 + 12*I)\n    assert q3 - q4 == Quaternion(2 + 6*I, -1 + 4*I, -5, 10 + I)\n    assert q3 * (1 + 2*I) == Quaternion(1 + 6*I, 2 + 11*I, 0, 14 + 16*I)\n    assert q3 / (1 + 2*I) == Quaternion(1/5 - 4/25*I, -2/25 + 11/25*I, 0, 7/5 - 8/25*I)\n"], "sample_641": ["def test_save_and_load_result_nonexistent_path(tmp_path: Path) -> None:\n    nonexistent_path = tmp_path / \"nonexistent_path\"\n    loaded = load_results(nonexistent_path)\n    assert loaded is None\n", "def test_load_results_nonexistent(base: str) -> None:\n    assert load_results(base) is None\n", "def test_save_and_load_result_with_custom_pylint_home(path: str, linter_stats: LinterStats) -> None:\n    custom_pylint_home = \"/custom/pylint/home\"\n    save_results(linter_stats, path, custom_pylint_home)\n    loaded = load_results(path, custom_pylint_home)\n    assert loaded is not None\n    assert loaded.bad_names == linter_stats.bad_names\n", "def test_load_results_invalid_data(tmp_path: Path, linter_stats: LinterStats) -> None:\n    data_file = _get_pdata_path(tmp_path, 1)\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(\"invalid data\", stream)\n\n    with pytest.warns(UserWarning):\n        loaded = load_results(tmp_path)\n    assert loaded is None\n", "def test_load_results(\n    base: str, pylint_home: Path, exists: bool, expected_warning: str | None, tmp_path: Path"], "sample_104": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "def test_template_tag_custom_extension(self):\n    relpath = self.hashed_file_path(\"cached/custom.ext\")\n    self.assertEqual(relpath, \"cached/custom.abcd1234.ext\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"cached/other.css\", content)\n        self.assertIn(b\"other.d41d8cd98f00.css\", content)\n    self.assertPostCondition()\n", "def test_cache_update_on_file_change(self):\n    # Write some content to a file\n    original_content = \"body { background-color: red; }\"\n    file_path = self._get_filename_path(\"styles.css\")\n    with open(file_path, \"w\") as f:\n        f.write(original_content)\n\n    # Add the new file to the STATICFILES_DIRS setting\n    self.patched_settings.update(\n        STATICFILES_DIRS=settings.STATICFILES_DIRS + [self._temp_dir]\n    )\n\n    # Run collectstatic and check the hashed file is created\n    self.run_collectstatic()\n    hashed_filename = storage.staticfiles_storage.hashed_files[\"styles.css\"]\n    hashed_file_path = storage.staticfiles_storage.path(hashed_filename)\n    self.assertTrue(os.path.exists(hashed_file_path))\n\n    # Change the content of the file\n    new_content = \"body { background-color: blue; }\"\n    with open(file_path, \"w\") as f:\n        f.write(new_content)\n\n    # Run collectstatic again and check the hashed file is updated\n    self.run_collectstatic()\n    new_hashed_filename = storage.staticfiles_storage.hashed_files[\"styles.css\"]\n    self.assertNotEqual(hashed_filename, new_hashed_filename)\n    new_hashed_file_path = storage.staticfiles_storage.path(new_hashed_filename)\n    self.assertTrue(os.path.exists(new_hashed_file_path))\n\n    # Check the content of the new hashed file\n    with open(new_hashed_file_path, \"r\") as f:\n        self.assertEqual(f.read(), new_content)\n", "def test_post_processing_order(self):\n    with open(self._get_filename_path('stylesheet.css'), 'w') as f:\n        f.write('@import \"imported.css\"; url(\"image.png\");')\n    with open(self._get_filename_path('imported.css'), 'w') as f:\n        f.write('body { background: url(\"background.png\"); }')\n    with open(self._get_filename_path('image.png'), 'wb') as f:\n        f.write(b'image content')\n    with open(self._get_filename_path('background.png'), 'wb') as f:\n        f.write(b'background content')\n\n    with self.settings(\n        STATICFILES_DIRS=settings.STATICFILES_DIRS + [self._temp_dir],\n    ):\n        self.run_collectstatic()\n\n    # Clear the hashed files cache to ensure it's loaded from the manifest.\n    storage.staticfiles_storage.hashed_files.clear()\n\n    stylesheet_path = self.hashed_file_path('test/stylesheet.css')\n    with storage.staticfiles_storage.open(stylesheet_path) as f:\n        content = f.read().decode('utf-8')\n        self.assertIn('imported.hash.css', content)\n        self.assertIn('image.hash.png', content)\n        self.assertIn('background.hash.png', content)\n", "def test_hash_key_consistency(self):\n    \"\"\"\n    Ensure hash keys are consistent with the configured file character set.\n    \"\"\"\n    configured_storage = storage.staticfiles_storage\n    test_filename = 'cached/styles.css'\n    original_charset = settings.FILE_CHARSET\n    try:\n        # Test with a file character set different from UTF-8\n        settings.FILE_CHARSET = 'latin1'\n        key_original_charset = configured_storage.hash_key(test_filename)\n\n        # Test with the original UTF-8 file character set\n        settings.FILE_CHARSET = 'utf-8'\n        key_utf8_charset = configured_storage.hash_key(test_filename)\n\n        self.assertNotEqual(key_original_charset, key_utf8_charset)\n    finally:\n        settings.FILE_CHARSET = original_charset\n"], "sample_894": ["def test_random_trees_embedding_feature_names_out_sparse():\n    \"\"\"Check feature names out for Random Trees Embedding with sparse output.\"\"\"\n    random_state = np.random.RandomState(0)\n    X = np.abs(random_state.randn(100, 4))\n    hasher = RandomTreesEmbedding(\n        n_estimators=2, max_depth=2, sparse_output=True, random_state=0\n    ).fit(X)\n    names = hasher.get_feature_names_out()\n    expected_names = [\n        f\"randomtreesembedding_{tree}_{leaf}\"\n        # Note: nodes with indices 0, 1 and 4 are internal split nodes and\n        # therefore do not appear in the expected output feature names.\n        for tree, leaf in [\n            (0, 2),\n            (0, 3),\n            (0, 5),\n            (0, 6),\n            (1, 2),\n            (1, 3),\n            (1, 5),\n            (1, 6),\n        ]\n    ]\n    assert_array_equal(expected_names, names)\n", "def test_random_trees_embedding_feature_names_out_custom_input_features():\n    \"\"\"Check feature names out for Random Trees Embedding with custom input feature names.\"\"\"\n    random_state = np.random.RandomState(0)\n    X = np.abs(random_state.randn(100, 4))\n    feature_names = [\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\"]\n    hasher = RandomTreesEmbedding(\n        n_estimators=2, max_depth=2, sparse_output=False, random_state=0\n    ).fit(X, feature_names=feature_names)\n    names = hasher.get_feature_names_out(input_features=feature_names)\n    expected_names = [\n        f\"randomtreesembedding_{tree}_{leaf}\"\n        # Note: nodes with indices 0, 1 and 4 are internal split nodes and\n        # therefore do not appear in the expected output feature names.\n        for tree, leaf in [\n            (0, 2),\n            (0, 3),\n            (0, 5),\n            (0, 6),\n            (1, 2),\n            (1, 3),\n            (1, 5),\n            (1, 6),\n        ]\n    ]\n    assert_array_equal(expected_names, names)\n", "def test_random_trees_embedding_max_samples():\n    \"\"\"Check that max_samples is correctly passed to the base estimator.\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    hasher = RandomTreesEmbedding(n_estimators=1, max_samples=3, random_state=0)\n    hasher.fit(X)\n    estimator = hasher.estimators_[0]\n    assert estimator.max_samples == 3\n", "def test_max_samples_with_small_n_samples(ForestClass):\n    rng = np.random.RandomState(1)\n\n    X = rng.randn(2, 2)\n    y = rng.randn(2) > 0\n\n    est = ForestClass(\n        n_estimators=1,\n        random_state=rng,\n        max_samples=3,\n    )\n\n    msg = \"`max_samples` must be <= n_samples=2 but got value 3\"\n    with pytest.raises(ValueError, match=msg):\n        est.fit(X, y)\n", "def test_random_forest_regressor_zero_division():\n    # Test that RandomForestRegressor handles division by zero in the variance reduction calculation\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 0])  # y is constant, so variance is zero\n    reg = RandomForestRegressor(n_estimators=1, random_state=0)\n    reg.fit(X, y)\n    # Check that fit does not raise an error\n    assert True\n"], "sample_51": ["    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P4Y', None),\n            ('P4M', None),\n            ('P4W', None),\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('P1D', timedelta(days=1)),\n            ('PT2H3M4S', timedelta(hours=2, minutes=3, seconds=4)),\n            ('PT3M4S', timedelta(minutes=3, seconds=4)),\n            ('PT4S', timedelta(seconds=4)),\n            ('-P1DT2H3M4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n            ('-P1D', timedelta(days=-1)),\n            ('-PT2H3M4S', timedelta(hours=-2, minutes=-3, seconds=-4)),\n            ('-PT3M4S', timedelta(minutes=-3, seconds=-4)),\n            ('-PT4S', timedelta(seconds=-4)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT1S', timedelta(days=1, seconds=1)),\n            ('P4DT15M30S', timedelta(days=4, minutes=15, seconds=30)),\n            ('P4DT15M30.1S', timedelta(days=4, minutes=15, seconds=30, milliseconds=100)),\n            ('P4DT15M30.000001S', timedelta(days=4, minutes=15, seconds=30, microseconds=1)),\n            ('P1Y2M3DT10H30M5S', None),  # Unsupported: years and months\n            ('-P4DT15M30S', timedelta(days=-4, minutes=-15, seconds=-30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT1S', timedelta(days=1, seconds=1)),\n            ('P4DT15M30S', timedelta(days=4, minutes=15, seconds=30)),\n            ('P4DT15M30.5S', timedelta(days=4, minutes=15, seconds=30, milliseconds=500)),\n            ('P-4DT15M30S', timedelta(days=-4, minutes=15, seconds=30)),\n            ('PT15M30S', timedelta(minutes=15, seconds=30)),\n            ('PT1H2M3S', timedelta(hours=1, minutes=2, seconds=3)),\n            ('P1Y2M3DT4H5M6S', None),  # Unsupported - years and months\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT1S', timedelta(days=1, seconds=1)),\n            ('P4DT15M30S', timedelta(days=4, minutes=15, seconds=30)),\n            ('P1Y2M3DT4H5M6S', None),  # Not supported by timedelta\n            ('P-1D', timedelta(days=-1)),\n            ('P-1DT-1S', timedelta(days=-1, seconds=-1)),\n            ('P-4DT-15M-30S', timedelta(days=-4, minutes=-15, seconds=-30)),\n            ('P1DT1.5S', timedelta(days=1, seconds=1, microseconds=500000)),\n            ('P0.5D', timedelta(hours=12)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P1Y', None),\n            ('P1M', None),\n            ('P1W', None),\n            ('P4D', timedelta(days=4)),\n            ('PT15M30S', timedelta(minutes=15, seconds=30)),\n            ('PT10H15M30S', timedelta(hours=10, minutes=15, seconds=30)),\n            ('P4DT10H15M30S', timedelta(days=4, hours=10, minutes=15, seconds=30)),\n            ('P4.5DT10.5H15M30S', timedelta(days=4, hours=10, minutes=15, seconds=30, microseconds=500000)),\n            ('P0.5D', timedelta(hours=12)),\n            ('P0.5DT0.5H', timedelta(hours=12, minutes=30)),\n            ('P0.5DT0.5H0.5M', timedelta(hours=12, minutes=30, seconds=30)),\n            ('P0.5DT0.5H0.5M0.5S', timedelta(hours=12, minutes=30, seconds=30, milliseconds=500)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_355": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            email='test@example.com', is_active=False,\n            **cls.user_credentials\n        )\n", "    def setUp(self):\n        self.user = User.objects.create_user('test', 'test@example.com', 'test')\n        self.content_type = ContentType.objects.get_for_model(Group)\n        self.perm = Permission.objects.create(name='test', content_type=self.content_type, codename='test')\n", "    def test_authenticate_inactive(self):\n        \"\"\"\n        An inactive custom user should be able to authenticate.\n        \"\"\"\n        test_user = CustomUser._default_manager.create_user(\n            email='test@example.com',\n            password='test',\n            date_of_birth=date(2006, 4, 25),\n            is_active=False\n        )\n        authenticated_user = authenticate(email='test@example.com', password='test')\n        self.assertEqual(test_user, authenticated_user)\n", "    def setUp(self):\n        self.user = CustomUser._default_manager.create_user(\n            email='test@example.com',\n            password='test',\n            date_of_birth=date(2006, 4, 25)\n        )\n        self.superuser = CustomUser._default_manager.create_superuser(\n            email='test2@example.com',\n            password='test',\n            date_of_birth=date(1976, 11, 8)\n        )\n"], "sample_461": ["def test_urlfield_clean_with_assume_scheme(self):\n    f = URLField(assume_scheme=\"ftp\")\n    self.assertEqual(f.clean(\"example.com\"), \"ftp://example.com\")\n    self.assertEqual(f.clean(\"http://example.com\"), \"http://example.com\")\n", "def test_urlfield_clean_empty_value(self):\n    f = URLField(required=False, empty_value=\"http://default.com\")\n    self.assertEqual(f.clean(\"\"), \"http://default.com\")\n    self.assertEqual(f.clean(None), \"http://default.com\")\n", "def test_urlfield_default_scheme(self):\n    with self.assertWarnsMessage(RemovedInDjango60Warning, expected_warning):\n        f = URLField()\n    self.assertEqual(f.assume_scheme, \"http\")\n", "def test_urlfield_clean_with_schemeless_url(self):\n    f = URLField(assume_scheme=\"https\")\n    self.assertEqual(f.clean(\"//example.com\"), \"https://example.com\")\n    self.assertEqual(f.clean(\"//example.com/path\"), \"https://example.com/path\")\n    self.assertEqual(f.clean(\"//example.com?query=value\"), \"https://example.com?query=value\")\n", "def test_urlfield_clean_valid_schemes(self):\n    schemes = [\"http\", \"https\", \"ftp\", \"ftps\"]\n    for scheme in schemes:\n        f = URLField(assume_scheme=scheme)\n        url = \"example.com\"\n        expected = f\"{scheme}://example.com\"\n        self.assertEqual(f.clean(url), expected)\n"], "sample_22": ["def test_matrix_product():\n    \"\"\"Test the matrix product function ``matrix_product``.\"\"\"\n    # Two 2x2 matrices\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    assert_array_equal(matrix_product(m1, m2), np.dot(m1, m2))\n    # Three 2x2 matrices\n    m3 = np.array([[9, 10], [11, 12]])\n    assert_array_equal(matrix_product(m1, m2, m3), np.dot(np.dot(m1, m2), m3))\n    # (M, 2, 2) matrices\n    n1 = np.tile(m1, (3, 1, 1))\n    n2 = np.tile(m2, (3, 1, 1))\n    assert_array_equal(matrix_product(n1, n2), np.matmul(n1, n2))\n", "def test_matrix_product():\n    \"\"\"Test the matrix product function ``matrix_product``.\"\"\"\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    m2 = rotation_matrix(-89 * u.deg, \"y\")\n    m3 = rotation_matrix(45 * u.deg, \"z\")\n\n    result = matrix_product(m1, m2, m3)\n    expected = np.dot(m1, np.dot(m2, m3))\n\n    assert_allclose(result, expected, atol=1e-12)\n\n    # Test with a single matrix\n    assert_allclose(matrix_product(m1), m1, atol=1e-12)\n\n    # Test with deprecated decorator\n    with pytest.warns(AstropyDeprecationWarning):\n        matrix_product(m1, m2)\n", "def test_matrix_product():\n    \"\"\"Test the matrix multiplication function ``matrix_product``.\"\"\"\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    expected_product = np.array([[19, 22], [43, 50]])\n    assert_array_equal(matrix_product(m1, m2), expected_product)\n\n    # Test with multiple matrices\n    m3 = np.array([[9, 10], [11, 12]])\n    expected_product_multiple = np.array([[95, 110], [227, 260]])\n    assert_array_equal(matrix_product(m1, m2, m3), expected_product_multiple)\n\n    # Test with deprecated function and check for warning\n    with pytest.warns(AstropyDeprecationWarning):\n        assert_array_equal(matrix_product(m1, m2), expected_product)\n", "def test_matrix_product():\n    \"\"\"Test the matrix product function ``matrix_product``.\"\"\"\n    # Two simple rotation matrices\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    m2 = rotation_matrix(45 * u.deg, \"y\")\n    # Compute their product using matrix_product\n    prod = matrix_product(m1, m2)\n    # Compute the product manually\n    manual_prod = np.matmul(m1, m2)\n    # Check if they are equal\n    assert np.allclose(prod, manual_prod)\n    # and (M, 3, 3)\n    n1 = np.tile(m1, (2, 1, 1))\n    n2 = np.tile(m2, (2, 1, 1))\n    prod_stack = matrix_product(n1, n2)\n    manual_prod_stack = np.matmul(n1, n2)\n    assert np.allclose(prod_stack, manual_prod_stack)\n", "def test_matrix_product():\n    \"\"\"Test the matrix product function ``matrix_product``.\"\"\"\n    m1 = rotation_matrix(30 * u.deg, \"x\")\n    m2 = rotation_matrix(45 * u.deg, \"y\")\n    m3 = rotation_matrix(60 * u.deg, \"z\")\n\n    # Test matrix product with two matrices\n    assert_allclose(matrix_product(m1, m2), np.dot(m1, m2), atol=1e-12)\n\n    # Test matrix product with three matrices\n    assert_allclose(matrix_product(m1, m2, m3), np.dot(m1, np.dot(m2, m3)), atol=1e-12)\n\n    # Test matrix product with deprecation warning\n    with pytest.warns(AstropyDeprecationWarning, match=\"matrix_product is deprecated\"):\n        matrix_product(m1, m2, m3)\n"], "sample_375": ["def test_abstract_model_children_inherit_constraints(self):\n    class Abstract(models.Model):\n        size = models.IntegerField()\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n            constraints = [models.CheckConstraint(check=models.Q(size__gt=1), name='size_gt_1')]\n\n    class Child1(Abstract):\n        pass\n\n    class Child2(Abstract):\n        pass\n\n    child1_state = ModelState.from_model(Child1)\n    child2_state = ModelState.from_model(Child2)\n    constraint_names = [constraint.name for constraint in child1_state.options['constraints']]\n    self.assertEqual(constraint_names, ['size_gt_1'])\n    constraint_names = [constraint.name for constraint in child2_state.options['constraints']]\n    self.assertEqual(constraint_names, ['size_gt_1'])\n\n    # Modifying the state doesn't modify the constraint on the model.\n    child1_state.options['constraints'][0].name = 'bar'\n    self.assertEqual(Child1._meta.constraints[0].name, 'size_gt_1')\n", "def test_get_related_models_tuples(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    B = self.create_model(\"B\", foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n    C = self.create_model(\"C\")\n    related_models = get_related_models_tuples(A)\n    self.assertEqual(\n        related_models,\n        {('related_models_app', 'b'), ('related_models_app', 'c')},\n    )\n", "    def test_generic_fk_with_abstract_base(self):\n        A = self.create_model(\"A\", abstract=True, foreign_keys=[\n            models.ForeignKey('B', models.CASCADE),\n            GenericForeignKey(),\n        ])\n        B = self.create_model(\"B\", foreign_keys=[\n            models.ForeignKey('C', models.CASCADE),\n        ])\n        C = self.create_model(\"C\", bases=(A,))\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "def test_proxy_model_fk(self):\n    A = self.create_model(\"A\")\n    B = self.create_model(\"B\", bases=(A,), proxy=True)\n    C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    self.assertRelated(A, [B, C])\n    self.assertRelated(B, [C])\n    self.assertRelated(C, [A, B])\n", "    def test_constraints_from_abstract(self):\n        class AbstractBase(models.Model):\n            name = models.CharField(max_length=20)\n\n            class Meta:\n                abstract = True\n                constraints = [models.UniqueConstraint(fields=['name'], name='unique_name')]\n\n        class Concrete(AbstractBase):\n            pass\n\n        constraint = AbstractBase._meta.constraints[0]\n        state = ModelState.from_model(Concrete)\n        self.assertEqual(state.options['constraints'], [constraint])\n"], "sample_539": ["def test_polygon_selector_box_props(ax, draw_bounding_box):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with custom box props\n    box_props = dict(edgecolor='r', facecolor='g', alpha=0.5)\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_props=box_props)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the box props were set correctly\n    box = tool._box._selection_artist\n    assert box.get_edgecolor() == 'r'\n    assert box.get_facecolor() == 'g'\n    assert box.get_alpha() == 0.5\n", "def test_polygon_selector_box_props(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_handle_props={'color': 'red'},\n                                   box_props={'facecolor': 'yellow'})\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that box handle props and box props are set correctly\n    for handle in tool._box._handles_artists:\n        assert handle.get_color() == 'red'\n    assert tool._box._selection_artist.get_facecolor() == 'yellow'\n", "def test_polygon_selector_move_all_vertices(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        # Polygon completed, now move all verts.\n        ('on_key_press', dict(key='shift')),\n        ('onmove', dict(xdata=100, ydata=100)),\n        ('press', dict(xdata=100, ydata=100)),\n        ('onmove', dict(xdata=110, ydata=110)),\n        ('release', dict(xdata=110, ydata=110)),\n        ('on_key_release', dict(key='shift')),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    # After moving all verts, they should be updated\n    assert tool.verts == [(60, 60), (160, 60), (60, 160)]\n", "def test_polygon_selector_box_handle_props(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with custom box handle properties\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_handle_props={'color': 'r', 'size': 5})\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the box handle properties were set correctly\n    for handle in tool._box._corner_handles.artists:\n        assert handle.get_color() == 'r'\n        assert handle.get_size() == 5\n\n    # Check that the box handle properties can be updated using the set_box_handle_props method\n    tool.set_box_handle_props(color='b', size=10)\n    for handle in tool._box._corner_handles.artists:\n        assert handle.get_color() == 'b'\n        assert handle.get_size() == 10\n", "def test_polygon_selector_rotate_move(ax):\n    # Create a square shape\n    verts = [(20, 20), (40, 20), (40, 40), (20, 40)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Rotate and move the shape\n    canvas = ax.figure.canvas\n    t = ax.transData\n\n    # Rotate 45 degrees counter-clockwise around the center\n    MouseEvent(\n        \"button_press_event\", canvas, *t.transform((30, 30)), 1, key='r')._process()\n    MouseEvent(\n        \"motion_notify_event\", canvas, *t.transform((35, 25)))._process()\n    MouseEvent(\n        \"button_release_event\", canvas, *t.transform((35, 25)), 1, key='r')._process()\n    np.testing.assert_allclose(\n        tool.verts, [(25, 20), (35, 10), (45, 30), (35, 40)], atol=1e-1)\n\n    # Move the shape to a new location\n    MouseEvent(\n        \"button_press_event\", canvas, *t.transform((30, 30)), 1)._process()\n    MouseEvent(\n        \"motion_notify_event\", canvas, *t.transform((50, 50)))._process()\n    MouseEvent(\n        \"button_release_event\","], "sample_220": ["def test_delete_cookie_samesite_none(self):\n    \"\"\"\n    delete_cookie() sets the secure flag if samesite='none' (without that,\n    browsers ignore cookies with that samesite attribute).\n    \"\"\"\n    response = HttpResponse()\n    response.delete_cookie('c', samesite='none')\n    self.assertIs(response.cookies['c']['secure'], True)\n", "def test_secure_cookie(self):\n    \"\"\"HttpResponse.set_cookie() sets the secure flag if requested.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('example', secure=True)\n    example_cookie = response.cookies['example']\n    self.assertIn('; %s' % cookies.Morsel._reserved['secure'], str(example_cookie))\n    self.assertIs(example_cookie['secure'], True)\n", "def test_delete_cookie_secure_samesite_lax(self):\n    # delete_cookie() doesn't set the secure flag if samesite='lax'.\n    response = HttpResponse()\n    response.delete_cookie('c', samesite='lax')\n    self.assertIs(response.cookies['c']['secure'], False)\n", "def test_delete_cookie_samesite_none(self):\n    \"\"\"\n    delete_cookie() sets the secure flag if samesite='none'.\n    \"\"\"\n    response = HttpResponse()\n    response.delete_cookie('c', samesite='none')\n    self.assertIs(response.cookies['c']['secure'], True)\n", "    def test_set_cookie_with_path_domain(self):\n        \"\"\"Cookie can be set with a specific path and domain.\"\"\"\n        response = HttpResponse()\n        response.set_cookie('example', path='/test', domain='example.com')\n        example_cookie = response.cookies['example']\n        self.assertEqual(example_cookie['path'], '/test')\n        self.assertEqual(example_cookie['domain'], 'example.com')\n"], "sample_1128": ["def test_point_vel_error_message():\n    N = ReferenceFrame('N')\n    P = Point('P')\n    with raises(ValueError) as e:\n        P.vel(N)\n    assert str(e.value) == 'Velocity of point P has not been defined in ReferenceFrame N'\n", "def test_point_vel_frame_mismatch():\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(N, N.x)\n    raises(ValueError, lambda: P.vel(B))\n", "def test_point_pos_from_exception():\n    N = ReferenceFrame('N')\n    P = Point('P')\n    Q = Point('Q')\n    raises(ValueError, lambda: P.pos_from(Q))  # No connecting path found between Q and P\n", "def test_point_a2pt_theory_with_velocity():\n    q = dynamicsymbols('q')\n    qd = dynamicsymbols('q', 1)\n    qdd = dynamicsymbols('q', 2)\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * B.x)\n    O.set_vel(N, 5 * N.x + qd * N.y)\n    P.set_vel(B, 2 * qd * B.y)\n    assert P.a2pt_theory(O, N, B) == (qdd - 25) * B.x + 2 * (qdd - qd**2) * B.y + 10 * qd * B.z\n", "def test_point_acc():\n    q = dynamicsymbols('q')\n    qd = dynamicsymbols('q', 1)\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * B.x)\n    O.set_vel(N, 5 * N.x + qd * B.y)\n    assert O.acc(N) == qd.diff() * B.y\n    assert P.a2pt_theory(O, N, B) == (-5 * qd - 10 * qd**2) * B.x + (qd.diff()) * B.y\n"], "sample_763": ["def test_retrieve_samples_from_numeric_shape():\n    class TestNumericShape:\n            self.shape = (5,)\n\n    samples = _num_samples(TestNumericShape())\n    assert samples == 5\n", "def test_retrieve_samples_from_nested_lists():\n    X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    assert _num_samples(X) == 3\n    X = [[1, 2], [3, 4]]\n    assert _num_samples(X) == 2\n    X = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n    assert _num_samples(X) == 2\n", "def test_check_array_force_all_finite_multi_dim():\n    X = np.array([[[1, 2, 3], [4, np.nan, 6]], [[7, 8, np.inf], [10, 11, 12]]])\n    X_checked = check_array(X, force_all_finite='allow-nan', ensure_2d=False)\n    assert_allclose_dense_sparse(X_checked, X)\n", "def test_num_samples():\n    # Test function for _num_samples\n    X = np.ones((3, 10))\n    assert _num_samples(X) == 3\n    X = sp.csr_matrix(X)\n    assert _num_samples(X) == 3\n    X = [[1, 2, 3], [4, 5, 6]]\n    assert _num_samples(X) == 2\n    X = \"string\"\n    assert_raises(TypeError, _num_samples, X)\n    X = object()\n    assert_raises(TypeError, _num_samples, X)\n    X = np.array(1)\n    assert_raises(TypeError, _num_samples, X)\n    X = np.array(1, dtype=object)\n    assert_raises(TypeError, _num_samples, X)\n    X = np.array([1, 2, 3])\n    assert _num_samples(X) == 3\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    assert _num_samples(X) == 2\n    X = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    assert _num_samples(X) == 2\n    X = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    assert _num_samples(X) == 3\n    X = RandomForestRegressor()\n    assert_raises(TypeError, _num_samples, X)\n", "def test_retrieve_samples_from_standard_shape():\n    X = np.ones((5, 3))\n    assert _num_samples(X) == 5\n\n    y = np.ones((5,))\n    assert _num_samples(y) == 5\n\n    # Test with a single sample\n    X = np.ones((1, 3))\n    assert _num_samples(X) == 1\n\n    y = np.ones((1,))\n    assert _num_samples(y) == 1\n\n    # Test with an empty array\n    X = np.empty((0, 3))\n    assert_raises_regex(ValueError, \"0 samples\", _num_samples, X)\n\n    y = np.empty((0,))\n    assert_raises_regex(ValueError, \"0 samples\", _num_samples, y)\n\n    # Test with non-iterable object\n    assert_raises_regex(TypeError, \"Expected sequence or array-like\", _num_samples, 42)\n\n    # Test with an estimator object\n    assert_raises_regex(TypeError, \"Expected sequence or array-like, got estimator RandomForestRegressor\", _num_samples, RandomForestRegressor())\n"], "sample_1067": ["def test_issue_6421():\n    x = Symbol('x')\n    I, p = symbols('I p')\n    a = Wild('a')\n\n    assert (-I*x).match(a*I) == {a: -x}\n    assert (I*x*I).match(a*I) == {a: -x}\n", "def test_issue_6421():\n    from sympy import I\n    x = Symbol('x')\n    a = Wild('a')\n    assert (-I*x).match(a*I) == {a: -x}\n", "def test_issue_7000():\n    x, y, z = symbols('x y z')\n    a, b, c = symbols('a b c', cls=Wild)\n\n    # Test matching with unevaluated expressions\n    assert (x + y*z).match(a + b*c) == {a: x, b: y, c: z}\n    assert (x*y + z).match(a*b + c) == {a: x, b: y, c: z}\n\n    # Test matching with evaluated expressions\n    assert (x + y*z).match(a + b*c, evaluate=True) == {a: x, b: y, c: z}\n    assert (x*y + z).match(a*b + c, evaluate=True) == {a: x, b: y, c: z}\n\n    # Test matching with constants\n    assert (x + 2).match(a + b) == {a: x, b: 2}\n    assert (3*x).match(a*b) == {a: 3, b: x}\n\n    # Test matching with functions\n    f = Function('f')\n    assert (f(x) + y).match(a + b) == {a: f(x), b: y}\n    assert (f(x)*y).match(a*b) == {a: f(x), b: y}\n\n    # Test matching with powers\n    assert (x**2 + y).match(a**2 + b) == {a: x, b: y}\n    assert (x*y**2).match(a*b**2) == {a: x, b: y}\n\n    # Test matching with complex numbers\n    assert ((1 + I)*x).match(a*b) == {a: 1 + I, b: x}\n    assert ((1 + I)*x).match(a*x) == {a: 1 + I}\n\n    # Test matching with derivatives\n    assert (diff(f(x), x) + y).match(a + b) == {a: diff(f(x), x), b: y}\n    assert (diff", "def test_issue_6421():\n    x, y = symbols('x y')\n    p = Wild('p')\n    assert (-I*x).match(p*I) == {p: -x}\n    assert (x*I).match(p*I) == {p: x}\n", "def test_issue_7115():\n    x = Symbol('x')\n    a, b = symbols('a b', cls=Wild, exclude=[x])\n\n    assert (x * log(x)).match(a * log(b)) == {a: x, b: x}\n"], "sample_207": ["def test_deep_lookup_obj_target(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__1={'f': 'g'}),\n        [self.objs[4]],\n    )\n", "def test_key_transform_exact_with_key_transform(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(\n            value__d__1__f=KeyTransform('f', KeyTransform('1', KeyTransform('d', 'value'))),\n        ),\n        [self.objs[4]],\n    )\n", "def test_key_transform_lt(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lt=15),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertIs(NullableJSONModel.objects.filter(value__c__lt=13).exists(), False)\n", "def test_key_transform_text_lookup_mixin_key_transform(self):\n    key_transform = KeyTransform('foo', 'value')\n    lookup = KeyTransformTextLookupMixin(key_transform)\n    self.assertIsInstance(lookup, KeyTransformTextLookupMixin)\n    self.assertIsInstance(lookup.lhs, KeyTextTransform)\n    self.assertEqual(lookup.lhs.key_name, 'foo')\n    self.assertEqual(lookup.lhs.lhs.source_expressions[0].name, 'value')\n", "def test_key_transform_with_cast(self):\n    obj = NullableJSONModel.objects.create(value={'a': 123})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__exact=Cast(123, models.TextField())),\n        [obj],\n    )\n"], "sample_943": ["def test_pep_0420_enabled_separate_private(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.b._private.rst').isfile()\n\n    with open(outdir / 'a.b._private.rst') as f:\n        rst = f.read()\n        assert \".. automodule:: a.b._private\\n\" in rst\n        assert \":private-members:\\n\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n\n    builddir = outdir / '_build' / 'text'\n    assert (builddir / 'a.b._private.txt').isfile()\n\n    with open(builddir / 'a.b._private.txt') as f:\n        txt = f.read()\n        assert \"a.b._private module\\n\" in txt\n", "def test_excludes_subpackage_should_be_skipped_recursive(apidoc):\n    \"\"\"Subpackage exclusion should work recursively.\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.rst').isfile()\n    assert (outdir / 'a.b.rst').isfile()\n    assert not (outdir / 'a.b.c.rst').isfile()  # skipped because of exclusion\n    assert not (outdir / 'a.b.c.d.rst').isfile()  # skipped because of exclusion\n    assert not (outdir / 'a.b.e.f.rst').isfile()  # skipped because 'b/e' subpackage is skipped\n", "def test_pep_0420_enabled_separate_private(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.b.c.rst').isfile()\n    assert (outdir / 'a.b.e.rst').isfile()\n    assert (outdir / 'a.b.e.f.rst').isfile()\n    assert (outdir / 'a.b.x.rst').isfile()\n    assert (outdir / 'a.b.x.y.rst').isfile()\n\n    with open(outdir / 'a.b.c.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 4\\n\\n   a.b.c.d\\n\" in rst\n        assert \":private-members:\" in rst\n\n    with open(outdir / 'a.b.e.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 4\\n\\n   a.b.e.f\\n\" in rst\n        assert \":private-members:\" in rst\n\n    with open(outdir / 'a.b.x.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 4\\n\\n   a.b.x.y\\n\" in rst\n        assert \":private-members:\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n", "def test_implicit_namespace_enabled(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.b.c.rst').isfile()\n\n    with open(outdir / 'a.b.c.rst') as f:\n        rst = f.read()\n        assert \"automodule:: a.b.c.d\\n\" in rst\n        assert \"automodule:: a.b.c\\n\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n\n    builddir = outdir / '_build' / 'text'\n    assert (builddir / 'a.b.c.txt').isfile()\n\n    with open(builddir / 'a.b.c.txt') as f:\n        txt = f.read()\n        assert \"a.b.c package\\n\" in txt\n", "def test_pep_0420_enabled_separate_maxdepth(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.b.c.rst').isfile()\n    assert (outdir / 'a.b.e.rst').isfile()\n    assert not (outdir / 'a.b.e.f.rst').isfile()\n    assert (outdir / 'a.b.x.rst').isfile()\n    assert not (outdir / 'a.b.x.y.rst').isfile()\n\n    with open(outdir / 'a.b.c.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 2\\n\\n   a.b.c.d\\n\" in rst\n\n    with open(outdir / 'a.b.e.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 2\\n\\n   a.b.e.f\\n\" not in rst\n\n    with open(outdir / 'a.b.x.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 2\\n\\n   a.b.x.y\\n\" not in rst\n"], "sample_498": ["def test_legend_markers_from_line2d_with_size():\n    # Test that markers can be copied for legend lines with size (#17960)\n    _markers = ['.', '*', 'v']\n    _sizes = [10, 20, 30]\n    fig, ax = plt.subplots()\n    lines = [mlines.Line2D([0], [0], ls='None', marker=mark, ms=size)\n             for mark, size in zip(_markers, _sizes)]\n    labels = [\"foo\", \"bar\", \"xyzzy\"]\n    markers = [line.get_marker() for line in lines]\n    sizes = [line.get_markersize() for line in lines]\n    legend = ax.legend(lines, labels)\n\n    new_markers = [line.get_marker() for line in legend.get_lines()]\n    new_sizes = [line.get_markersize() for line in legend.get_lines()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert markers == new_markers == _markers\n    assert sizes == new_sizes == _sizes\n    assert labels == new_labels\n", "def test_legend_title_fontproperties_none():\n    # test the title_fontproperties kwarg with None\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend(title='Aardvark', title_fontproperties=None)\n    assert leg.get_title().get_text() == \"Aardvark\"\n    assert leg.get_title().get_visible()\n", "def test_legend_set_draggable():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='shabnams')\n    legend = ax.legend()\n    assert legend.get_draggable() is False\n    legend.set_draggable(True)\n    assert legend.get_draggable() is True\n    legend.set_draggable(False)\n    assert legend.get_draggable() is False\n", "def test_legend_title_fontprop_fontsize_rcparams():\n    # test the title_fontsize rcParams\n    fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n    axes = axes.flat\n    axes[0].plot(range(10))\n    with rc_context({'legend.title_fontsize': 22}):\n        leg0 = axes[0].legend(title='Aardvark')\n    assert leg0.get_title().get_fontsize() == 22\n\n    axes[1].plot(range(10))\n    leg1 = axes[1].legend(title='Aardvark')\n    assert leg1.get_title().get_fontsize() == mpl.rcParams['font.size']\n\n    mpl.rcParams['legend.title_fontsize'] = 20\n    axes[2].plot(range(10))\n    leg2 = axes[2].legend(title='Aardvark')\n    assert leg2.get_title().get_fontsize() == 20\n\n    mpl.rcParams['legend.title_fontsize'] = None\n    axes[3].plot(range(10))\n    leg3 = axes[3].legend(title='Aardvark')\n    assert leg3.get_title().get_fontsize() == mpl.rcParams['font.size']\n", "def test_legend_markers_from_pathcollection():\n    # Test that markers can be copied for legend PathCollection\n    _markers = ['.', '*', 'v']\n    fig, ax = plt.subplots()\n    lines = [plt.scatter([0], [0], marker=mark) for mark in _markers]\n    labels = [\"foo\", \"bar\", \"xyzzy\"]\n    markers = [line._paths[0].get_marker() for line in lines]\n    legend = ax.legend(lines, labels)\n\n    new_markers = [line._paths[0].get_marker() for line in legend.get_lines()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert markers == new_markers == _markers\n    assert labels == new_labels\n"], "sample_517": ["def test_annotation_xycoords_offset():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\"test\", xy=(0.5, 0.5), xytext=(10, 10),\n                      xycoords=\"data\", textcoords=\"offset points\")\n    fig.canvas.draw()\n    bbox = ann.get_window_extent()\n    assert bbox.x0 == ann.xy[0] + 10\n    assert bbox.y0 == ann.xy[1] + 10\n", "def test_text_clip_on_axes():\n    fig, ax = plt.subplots()\n\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n\n    ax.text(1.5, 0.5, 'Clip me out', clip_on=True)\n    ax.text(-0.5, 0.5, 'Clip me out', clip_on=True)\n    ax.text(0.5, 1.5, 'Clip me out', clip_on=True)\n    ax.text(0.5, -0.5, 'Clip me out', clip_on=True)\n\n    ax.text(0.5, 0.5, 'Do not clip me', clip_on=False)\n", "def test_text_stretch(stretch):\n    fig = plt.figure()\n    text = fig.text(0.5, 0.5, 'Test text', stretch=stretch)\n    assert text.get_stretch() == stretch\n", "def test_pdf_chars_beyond_bmp_different_font():\n    plt.rcParams['pdf.fonttype'] = 42\n    plt.rcParams['mathtext.fontset'] = 'cm'\n    plt.figure()\n    plt.figtext(0.1, 0.5, \"Mass $m$ \\U00010308\", size=30)\n", "def test_pdf_unicode():\n    plt.rcParams['pdf.fonttype'] = 42\n    plt.figure()\n    plt.figtext(0.1, 0.5, \"Bj\u00f6rk Gu\u00f0mundsd\u00f3ttir\", size=30)\n"], "sample_703": ["def test_parentheses_precedence() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"false or true and true\", matcher) is True\n    assert evaluate(\"(false or true) and true\", matcher) is True\n    assert evaluate(\"false or (true and true)\", matcher) is True\n    assert evaluate(\"(false or true) and false\", matcher) is False\n", "def test_complex_expressions(expr: str) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is eval(expr, {\"True\": True, \"False\": False})\n", "def test_unbalanced_parentheses() -> None:\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(\"(true and false\", lambda ident: True)\n    assert excinfo.value.column == 13\n    assert excinfo.value.message == \"expected right parenthesis; got end of input\"\n", "def test_operator_precedence() -> None:\n    matcher = {\"a\": True, \"b\": False}.__getitem__\n    assert evaluate(\"a and b or a\", matcher) is True\n    assert evaluate(\"a or b and a\", matcher) is True\n    assert evaluate(\"not a or b\", matcher) is False\n    assert evaluate(\"a or not b\", matcher) is True\n    assert evaluate(\"(not a) or b\", matcher) is False\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n"], "sample_677": ["def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_reserved_words_and_constants(expr: str, expected: bool) -> None:\n    matcher = lambda ident: ident in {\"True\", \"False\", \"None\", \"if\", \"else\", \"while\"}\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True, \"d\": False}.__getitem__\n    assert evaluate(\"(a and b) or (c and d)\", matcher) is False\n    assert evaluate(\"(a or b) and (c or d)\", matcher) is True\n    assert evaluate(\"(a and b) or (not c and d)\", matcher) is False\n    assert evaluate(\"(a or b) and (not c or d)\", matcher) is True\n", "def test_valid_operators() -> None:\n    assert evaluate(\"not True\", {}.__getitem__) is False\n    assert evaluate(\"not False\", {}.__getitem__) is True\n    assert evaluate(\"True and True\", {}.__getitem__) is True\n    assert evaluate(\"True and False\", {}.__getitem__) is False\n    assert evaluate(\"False and True\", {}.__getitem__) is False\n    assert evaluate(\"True or True\", {}.__getitem__) is True\n    assert evaluate(\"True or False\", {}.__getitem__) is True\n    assert evaluate(\"False or False\", {}.__getitem__) is False\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n"], "sample_376": ["def test_empty_messages(self):\n    \"\"\"\n    When no messages are present, an empty list is returned and the cookie is removed.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(list(storage), [])\n    self.assertNotIn(CookieStorage.cookie_name, response.cookies)\n", "def test_message_decoder_with_extra_tags(self):\n    \"\"\"\n    Test MessageDecoder with extra tags.\n    \"\"\"\n    message = Message(constants.INFO, 'Test message', extra_tags='tag')\n    encoded = json.dumps(message, cls=MessageEncoder)\n    decoded = json.loads(encoded, cls=MessageDecoder)\n    self.assertEqual(decoded.extra_tags, 'tag')\n", "def test_message_ordering(self):\n    \"\"\"\n    Messages are stored and retrieved in the order they were added.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Add messages in a specific order\n    messages = ['first', 'second', 'third']\n    for msg in messages:\n        storage.add(constants.INFO, msg)\n\n    storage.update(response)\n    retrieved_messages = list(storage)\n\n    # Check if the messages are retrieved in the same order\n    self.assertEqual([msg.message for msg in retrieved_messages], messages)\n", "def test_message_encoder_decoder_special_cases(self):\n    \"\"\"\n    The MessageEncoder and MessageDecoder handle special cases correctly,\n    such as when a message has extra_tags set to None.\n    \"\"\"\n    message = Message(constants.INFO, 'Test message', extra_tags=None)\n    encoder = MessageEncoder()\n    decoder = MessageDecoder()\n    value = encoder.encode(message)\n    decoded_message = decoder.decode(value)\n    self.assertEqual(message, decoded_message)\n", "def test_update_cookie(self):\n    \"\"\"\n    Test if update_cookie function sets the cookie when data is provided,\n    and deletes the cookie when data is not provided.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test cookie set when data is provided\n    storage._update_cookie(\"test_data\", response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name].value, \"test_data\")\n\n    # Test cookie delete when no data is provided\n    storage._update_cookie(None, response)\n    self.assertNotIn(CookieStorage.cookie_name, response.cookies)\n"], "sample_185": ["def test_get_supported_language_variant_null_strict(self):\n    with self.assertRaises(LookupError):\n        trans_null.get_supported_language_variant('pt', strict=True)\n    with self.assertRaises(LookupError):\n        trans_null.get_supported_language_variant('de', strict=True)\n", "def test_specific_language_codes_with_quality_factor(self):\n    # issue 19919\n    r = self.rf.get('/')\n    r.COOKIES = {}\n    r.META = {'HTTP_ACCEPT_LANGUAGE': 'en-US;q=0.9,bg;q=0.8'}\n    lang = get_language_from_request(r)\n    self.assertEqual('en-us', lang)\n    r = self.rf.get('/')\n    r.COOKIES = {}\n    r.META = {'HTTP_ACCEPT_LANGUAGE': 'bg-bg;q=0.9,en-US;q=0.8'}\n    lang = get_language_from_request(r)\n    self.assertEqual('bg', lang)\n", "def test_localize_templatetag_and_filter_with_l10n_off(self):\n    \"\"\"\n    Test the {% localize %} templatetag and the localize/unlocalize filters with USE_L10N off.\n    \"\"\"\n    context = Context({'int': 1455, 'float': 3.14, 'date': datetime.date(2016, 12, 31)})\n    template = Template(\n        '{% load l10n %}{% localize off %}'\n        '{{ int }}/{{ float }}/{{ date }}{% endlocalize %}'\n    )\n    expected_output = '1455/3.14/December 31, 2016'\n    with self.settings(USE_L10N=False, USE_THOUSAND_SEPARATOR=True, DATE_FORMAT='F j, Y'):\n        self.assertEqual(template.render(context), expected_output)\n", "def test_get_language_info_fallback(self):\n    li = get_language_info('fr-ca')\n    self.assertEqual(li['code'], 'fr')\n    self.assertEqual(li['name_local'], 'fran\u00e7ais')\n    self.assertEqual(li['name'], 'French')\n    self.assertIs(li['bidi'], False)\n", "def test_get_language_from_path_null_with_prefix(self):\n    g = trans_null.get_language_from_path\n    self.assertIsNone(g('/pl/'))\n    self.assertIsNone(g('/pl'))\n    self.assertIsNone(g('/xyz/'))\n\n    with self.settings(PREFIX_DEFAULT_LANGUAGE=False):\n        self.assertIsNone(g('/pl/'))\n        self.assertIsNone(g('/pl'))\n        self.assertIsNone(g('/xyz/'))\n"], "sample_405": ["def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, limit_choices_to={'field': 'value'}),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "def test_references_field_by_m2m_to_field(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ManyToManyField(\"Other\", to_field=\"field\")\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_through_from_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\n            \"Other\", through=models.fields.related.ForeignObject(\n                \"Through\", models.CASCADE, [\"from\"], [\"to\"]\n            )\n        )\n    )\n    self.assertIs(operation.references_field(\"Model\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Through\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Through\", \"to\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"to\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "    def test_references_field_by_order_with_respect_to(self):\n        operation = FieldOperation(\"Model\", \"field\", models.IntegerField())\n        operation.model_name = \"Model\"\n        operation.name = \"order_field\"\n        operation.order_with_respect_to = \"Other\"\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "    def test_references_field_by_limit_choices_to(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\",\n                models.CASCADE,\n                limit_choices_to={\"field\": \"value\"},\n            ),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"other_field\", \"migrations\"), False)\n        self.assertIs(operation.references_field(\"Missing\", \"field\", \"migrations\"), False)\n"], "sample_707": ["def test_node_getparent():\n    parent = nodes.Node(\"parent\")\n    child = nodes.Node(\"child\", parent=parent)\n    grandchild = nodes.Node(\"grandchild\", parent=child)\n\n    assert child.getparent(nodes.Node) == parent\n    assert grandchild.getparent(nodes.Node) == child\n    assert grandchild.getparent(nodes.Item) is None\n", "def test_node_from_parent_fspath_and_path_consistency() -> None:\n    path = Path(\"/test/path\")\n    fspath = legacy_path(\"/test/path\")\n    with pytest.raises(ValueError, match=\"Path.* != .*\\\\nif both path and fspath are given they need to be equal\"):\n        nodes.Node.from_parent(None, path=path, fspath=fspath)\n", "def test_node_add_marker(request):\n    node = nodes.Node.from_parent(request.session, name=\"test_node\")\n    node.add_marker(\"mark1\")\n    assert node.keywords[\"mark1\"] == pytest.mark.mark1\n    assert pytest.mark.mark1.mark in node.own_markers\n\n    node.add_marker(pytest.mark.mark2)\n    assert node.keywords[\"mark2\"] == pytest.mark.mark2\n    assert pytest.mark.mark2.mark in node.own_markers\n\n    with pytest.raises(ValueError, match=\"is not a string or pytest.mark.* Marker\"):\n        node.add_marker(123)\n", "def test_node_listextrakeywords(request):\n    class DummyNode(nodes.Node):\n            super().__init__(name, parent=parent, session=request.session, **kwargs)\n\n    parent_node = DummyNode(\"parent\")\n    parent_node.extra_keyword_matches = {\"parent_keyword\"}\n    child_node = DummyNode(\"child\", parent=parent_node)\n    child_node.extra_keyword_matches = {\"child_keyword\"}\n\n    extra_keywords = child_node.listextrakeywords()\n    assert extra_keywords == {\"parent_keyword\", \"child_keyword\"}\n", "def test_node_listchain_method(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n\n            pass\n\n        class TestClass:\n                pass\n        \"\"\"\n    )\n    items = pytester.getitems(\"\")\n\n    # Testing listchain method for a test function\n    test_parent = [item for item in items if item.name == \"test_parent\"][0]\n    assert len(test_parent.listchain()) == 4  # session, root, file, test_parent\n\n    # Testing listchain method for a test method\n    test_method = [item for item in items if item.name == \"test_method\"][0]\n    assert len(test_method.listchain()) == 5  # session, root, file, TestClass, test_method\n"], "sample_1014": ["def test_mutability():\n    # Test mutability of MutableDenseNDimArray\n    mutable_array = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    mutable_array[0, 0] = 5\n    assert mutable_array[0, 0] == 5\n\n    # Test immutability of ImmutableDenseNDimArray\n    immutable_array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n        immutable_array[0, 0] = 5\n    raises(TypeError, test_immutable)\n\n    # Test conversion between mutable and immutable\n    assert mutable_array.as_immutable() == immutable_array\n    assert immutable_array.as_mutable() == mutable_array\n", "def test_array_copying():\n    original = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    copied = original.copy()\n\n    assert original == copied\n    assert original is not copied\n\n    copied[0, 0] = 5\n\n    assert original != copied\n\n    copied = original.copy(deep=True)\n    copied[0, 0] = 5\n\n    assert original == copied\n", "def test_mutability():\n    from sympy import MutableDenseNDimArray, Symbol\n\n    # Create a mutable array\n    mutable_array = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert mutable_array[0, 0] == 1\n\n    # Change a value in the mutable array\n    mutable_array[0, 0] = 5\n    assert mutable_array[0, 0] == 5\n\n    # Attempt to change a value in the immutable array\n    immutable_array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n        immutable_array[0, 0] = 5\n    raises(TypeError, change_immutable)\n\n    # Change a value in the mutable array to a symbol\n    x = Symbol('x')\n    mutable_array[1, 1] = x\n    assert mutable_array[1, 1] == x\n\n    # Convert the mutable array to an immutable array\n    immutable_from_mutable = mutable_array.as_immutable()\n    assert isinstance(immutable_from_mutable, ImmutableDenseNDimArray)\n    assert immutable_from_mutable[0, 0] == 5\n    assert immutable_from_mutable[1, 1] == x\n\n    # Attempt to change a value in the immutable array from the mutable array\n        mutable_array[0, 0] = 6\n    raises(TypeError, change_immutable_from_mutable)\n", "def test_mutable_array_operations():\n    from sympy import MutableDenseNDimArray\n\n    array = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n\n    # Test setting item\n    array[1, 1] = 5\n    assert array[1, 1] == 5\n\n    # Test getting item\n    assert array[0, 1] == 2\n\n    # Test reshaping\n    array = array.reshape(4)\n    assert array.shape == (4,)\n\n    # Test as_immutable method\n    immutable_array = array.as_immutable()\n    assert isinstance(immutable_array, ImmutableDenseNDimArray)\n    assert immutable_array.shape == array.shape\n    assert immutable_array._array == array._array\n\n    # Test rebuild_mutable_arrays\n    mutable_array = MutableDenseNDimArray(immutable_array)\n    assert isinstance(mutable_array, MutableDenseNDimArray)\n    assert mutable_array.shape == immutable_array.shape\n    assert mutable_array._array == immutable_array._array\n\n    # Test free_symbols property\n    from sympy import symbols\n    x, y = symbols('x y')\n    array = MutableDenseNDimArray([x, y, 3, 4], (2, 2))\n    assert array.free_symbols == {x, y}\n", "def test_ndim_array_sympy_compatibility():\n    from sympy import symbols, sin\n\n    x, y = symbols('x y')\n\n    # Creating a 2D array with symbolic elements\n    symbolic_array = ImmutableDenseNDimArray([x, y, sin(x), sin(y)], (2, 2))\n\n    # Checking the free_symbols property\n    assert symbolic_array.free_symbols == {x, y}\n\n    # Testing substitution\n    substituted_array = symbolic_array.subs({x: 0, y: 0})\n    assert substituted_array == ImmutableDenseNDimArray([0, 0, 0, 0], (2, 2))\n\n    # Testing substitution with another symbolic expression\n    z = symbols('z')\n    substituted_array = symbolic_array.subs({x: z, y: z**2})\n    assert substituted_array == ImmutableDenseNDimArray([z, z**2, sin(z), sin(z**2)], (2, 2))\n"], "sample_402": ["def test_prepend_www_and_append_slash_with_custom_urlconf(self):\n    \"\"\"\n    Test that PREPEND_WWW and APPEND_SLASH work together with a custom URLconf.\n    \"\"\"\n    request = self.rf.get(\"/customurlconf/slash\")\n    request.urlconf = \"middleware.extra_urls\"\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/customurlconf/slash/\")\n", "def test_append_slash_custom_method(self):\n    \"\"\"\n    APPEND_SLASH should append slash to path when request method is not GET.\n    \"\"\"\n    request = self.rf.put(\"/slash\")\n    request.urlconf = \"middleware.extra_urls\"\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://testserver/slash/\")\n", "def test_prepend_www_custom_port(self):\n    request = self.rf.get(\"/customurlconf/path/\")\n    request.urlconf = \"middleware.extra_urls\"\n    request.META[\"SERVER_PORT\"] = \"8000\"\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver:8000/customurlconf/path/\")\n", "def test_append_slash_redirect_custom_urlconf_no_trailing_slash(self):\n    \"\"\"\n    APPEND_SLASH should redirect slashless URLs to a valid pattern even if they\n    don't end with a trailing slash.\n    \"\"\"\n    request = self.rf.get(\"/customurlconf/slash\")\n    request.path_info = \"/customurlconf/slash\"\n    request.urlconf = \"middleware.extra_urls\"\n    r = CommonMiddleware(get_response_404).process_request(request)\n    self.assertIsNotNone(\n        r,\n        \"CommonMiddleware failed to return APPEND_SLASH redirect using request.urlconf\",\n    )\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"/customurlconf/slash/\")\n", "def test_prepend_www_append_slash_have_slash_custom_urlconf_secure_proxy_ssl_header(self):\n    request = self.rf.get(\"/customurlconf/slash/\")\n    request.urlconf = \"middleware.extra_urls\"\n    request.META[\"HTTP_X_PROTO\"] = \"https\"\n    request.META[\"SERVER_PORT\"] = 443\n    with self.settings(SECURE_PROXY_SSL_HEADER=(\"HTTP_X_PROTO\", \"https\")):\n        r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"https://www.testserver/customurlconf/slash/\")\n"], "sample_742": ["def test_intercept_scaling_with_fit_intercept():\n    # Test that intercept_scaling is used when fit_intercept is True\n    clf = LogisticRegression(fit_intercept=True, intercept_scaling=2.0)\n    clf.fit(X, Y1)\n    assert_equal(clf.intercept_, 2.0 * clf.coef_[:, -1])\n", "def test_logistic_regression_l1_coefficients():\n    # Test that L1 regularization results in sparse coefficients\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    lr_l1 = LogisticRegression(penalty=\"l1\", C=0.1, solver='liblinear')\n    lr_l1.fit(X, y)\n    assert_true(np.count_nonzero(lr_l1.coef_) < X.shape[1])\n", "def test_logreg_sample_weight_with_intercept_scaling():\n    # Test that sample weights work correctly when intercept_scaling is not 1.\n    X, y = make_classification(n_samples=20, n_features=5, n_informative=3,\n                               n_classes=2, random_state=0)\n    sample_weight = y + 1\n    intercept_scaling = 2.0\n\n    clf_sw = LogisticRegression(fit_intercept=True, intercept_scaling=intercept_scaling,\n                                random_state=42)\n    clf_sw.fit(X, y, sample_weight=sample_weight)\n\n    clf_no_sw = LogisticRegression(fit_intercept=True, intercept_scaling=intercept_scaling,\n                                   random_state=42)\n    clf_no_sw.fit(X, y)\n\n    assert_array_almost_equal(clf_sw.coef_, clf_no_sw.coef_, decimal=4)\n    assert_array_almost_equal(clf_sw.intercept_, clf_no_sw.intercept_, decimal=4)\n", "def test_intercept_scaling():\n    # Test that intercept_scaling is taken into account in the decision function\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    for intercept_scaling in [1, 2, 10]:\n        clf = LogisticRegression(fit_intercept=True, solver='liblinear',\n                                 intercept_scaling=intercept_scaling)\n        clf.fit(X, y)\n        decision_function = clf.decision_function(X)\n        expected_decision_function = X.dot(clf.coef_[0]) + clf.intercept_[0] * intercept_scaling\n        assert_array_almost_equal(decision_function, expected_decision_function)\n", "def test_logreg_coef_intercept_shape():\n    # Test that the shape of coef_ and intercept_ is correct\n    X, y = iris.data, iris.target\n    y_bin = y.copy()\n    y_bin[y_bin == 2] = 0\n\n    for solver in ['newton-cg', 'liblinear', 'sag', 'saga', 'lbfgs']:\n        for multi_class in ['ovr', 'multinomial']:\n            if solver == 'liblinear' and multi_class == 'multinomial':\n                continue\n            clf = LogisticRegression(tol=1e-2, multi_class=multi_class,\n                                     solver=solver, C=1.,\n                                     random_state=42, max_iter=100)\n            clf.fit(X, y)\n            if multi_class == 'ovr':\n                n_classes = len(np.unique(y))\n            else:\n                n_classes = 1\n            assert_equal(clf.coef_.shape, (n_classes, X.shape[1]))\n            assert_equal(clf.intercept_.shape, (n_classes,))\n"], "sample_442": ["def test_fallback_keys_verification(self):\n    old_signer = signing.Signer(key=\"oldsecret\")\n    signed = old_signer.sign(\"abc\")\n    signer = signing.Signer(key=\"secret\", fallback_keys=[\"oldsecret\"])\n    self.assertEqual(signer.unsign(signed), \"abc\")\n", "def test_sign_unsign_with_fallback_keys(self):\n    \"\"\"\n    Signer should use fallback keys to unsign a signed value.\n    \"\"\"\n    signer = signing.Signer(key=\"secret\", fallback_keys=[\"oldsecret\"])\n    old_signer = signing.Signer(key=\"oldsecret\")\n    signed_value = old_signer.sign(\"abc\")\n    self.assertEqual(signer.unsign(signed_value), \"abc\")\n", "    def test_verify_with_fallback_keys(self):\n        old_signer = signing.Signer(key=\"oldsecret\")\n        new_signer = signing.Signer(key=\"newsecret\", fallback_keys=[\"oldsecret\"])\n        signed = old_signer.sign(\"abc\")\n        self.assertEqual(new_signer.unsign(signed), \"abc\")\n", "def test_sign_unsign_with_different_salt(self):\n    signer1 = signing.Signer(key=\"secret\", salt=\"salt1\")\n    signer2 = signing.Signer(key=\"secret\", salt=\"salt2\")\n    signed = signer1.sign(\"abc\")\n    with self.assertRaises(signing.BadSignature):\n        signer2.unsign(signed)\n", "def test_fallback_keys_verification(self):\n    old_signer = signing.Signer(key=\"oldsecret\")\n    signed = old_signer.sign(\"abc\")\n    signer = signing.Signer(key=\"newsecret\", fallback_keys=[\"oldsecret\"])\n    self.assertEqual(signer.unsign(signed), \"abc\")\n"], "sample_173": ["def test_explain_query_prefix(self):\n    msg = 'This backend does not support explaining query execution.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        self.ops.explain_query_prefix()\n", "def test_distinct_sql_without_fields(self):\n    sql, params = self.ops.distinct_sql(None, None)\n    self.assertEqual(sql, ['DISTINCT'])\n    self.assertEqual(params, [])\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.141592653589793238462643383279502884197169399375105820974944592307816406286')\n    max_digits = 30\n    decimal_places = 28\n    expected_result = '3.1415926535897932384626433833'\n    self.assertEqual(self.ops.adapt_decimalfield_value(value, max_digits, decimal_places), expected_result)\n", "    def test_adapt_ipaddressfield_value(self):\n        value = '192.168.1.1'\n        self.assertEqual(self.ops.adapt_ipaddressfield_value(value), value)\n", "def test_subtract_temporals_supported(self):\n    with connection.features().enable_feature('supports_temporal_subtraction'):\n        duration_field = DurationField()\n        duration_field_internal_type = duration_field.get_internal_type()\n        lhs = ('%s', [1000]),\n        rhs = ('%s', [500]),\n        result = self.ops.subtract_temporals(duration_field_internal_type, lhs, rhs)\n        self.assertEqual(result, ('(%s - %s)', (1000, 500)))\n"], "sample_691": ["def test_config_override(pytester: Pytester) -> None:\n    \"\"\"Test option to override faulthandler_timeout in the command line.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import faulthandler\n        assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.01\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\"-o\", \"faulthandler_timeout=0.02\")\n    assert \"faulthandler_timeout=0.02\" in result.stdout.str()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_faulthandler_disable_after_test(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler is disabled after the test execution.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import faulthandler\n        assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n    assert not faulthandler.is_enabled()\n", "def test_custom_stderr(pytester: Pytester) -> None:\n    \"\"\"Test option to write tracebacks to a custom stderr.\"\"\"\n    custom_stderr = io.StringIO()\n    pytester.makepyfile(\n        \"\"\"\n    import time\n    import sys\n        sys.stderr = custom_stderr\n        time.sleep(0.1)\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.01\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert \"most recent call first\" in custom_stderr.getvalue()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_faulthandler_reconfiguration(pytester: Pytester) -> None:\n    \"\"\"Test that faulthandler can be reconfigured after pytest_configure.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import faulthandler\n        import pytest\n\n        @pytest.hookimpl(tryfirst=True)\n            faulthandler.enable()\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n            import faulthandler\n            assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_no_duplicate_file_descriptor(pytester: Pytester) -> None:\n    \"\"\"Test for not duplicating the file descriptor if faulthandler is not enabled (#1234).\"\"\"\n    import os\n    from _pytest.faulthandler import FaultHandlerHooks\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n            import faulthandler\n            assert not faulthandler.is_enabled()\n    \"\"\"\n    )\n\n    # Mock os.dup to raise an exception when called\n    class MockedOs:\n            raise OSError(\"File descriptor cannot be duplicated\")\n\n    with pytester.plugin.monkeypatch.context() as mp:\n        mp.setattr(os, 'dup', MockedOs().dup)\n\n        # Run the test\n        result = pytester.runpytest_subprocess()\n\n        # Ensure the test passed and no error was raised\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n        assert result.ret == 0\n        assert \"File descriptor cannot be duplicated\" not in result.stderr.str()\n\n    # Check that our mocked os.dup was not called\n    assert MockedOs().dup.call_count == 0\n"], "sample_428": ["    def test_none_and_empty_string(self):\n        self.assertEqual(nformat(None, \".\"), \"\")\n        self.assertEqual(nformat(\"\", \".\"), \"\")\n        self.assertEqual(nformat(None, \".\", decimal_pos=2), \"\")\n        self.assertEqual(nformat(\"\", \".\", decimal_pos=2), \"\")\n        self.assertEqual(nformat(None, \".\", grouping=2, thousand_sep=\",\"), \"\")\n        self.assertEqual(nformat(\"\", \".\", grouping=2, thousand_sep=\",\"), \"\")\n", "    def test_custom_format(self):\n        class CustomDecimal(Decimal):\n            \"\"\"\n            Wrapper for Decimal which formats the number as custom.\n            \"\"\"\n\n                amount = super().__format__(specifier, **kwargs)\n                return \"custom {}\".format(amount)\n\n        self.assertEqual(nformat(CustomDecimal(\"1234.56\"), \".\", decimal_pos=2), \"custom 1234.56\")\n        self.assertEqual(nformat(CustomDecimal(\"1234.56\"), \".\", grouping=3, thousand_sep=\",\", force_grouping=True), \"custom 1,234.56\")\n", "    def test_none_and_empty_string(self):\n        self.assertEqual(nformat(None, \".\"), None)\n        self.assertEqual(nformat(\"\", \".\"), \"\")\n", "    def test_decimal_subclass_with_grouping(self):\n        class EuroDecimal(Decimal):\n            \"\"\"\n            Wrapper for Decimal which prefixes each amount with the \u20ac symbol.\n            \"\"\"\n\n                amount = super().__format__(specifier, **kwargs)\n                return \"\u20ac {}\".format(amount)\n\n        self.assertEqual(\n            nformat(EuroDecimal(\"1234567.89\"), \".\", grouping=3, thousand_sep=\",\", force_grouping=True),\n            \"\u20ac 1,234,567.89\"\n        )\n", "    def test_scientific_notation(self):\n        self.assertEqual(nformat(12345678901234567890123456789, \".\", decimal_pos=2), \"1.23e+27\")\n        self.assertEqual(nformat(12345678901234567890123456789, \".\", decimal_pos=2, thousand_sep=\",\", grouping=3, force_grouping=True), \"1,234,567,890,123,456,789,012,345,678,901,234,567,890.00\")\n        self.assertEqual(nformat(0.00000000000000000000000000001234, \".\", decimal_pos=2), \"1.23e-25\")\n"], "sample_1134": ["def test_latex_Str():\n    from sympy.core.symbol import Str\n    assert latex(Str('x')) == 'x'\n", "def test_latex_StrLength():\n    from sympy.core.symbol import Str\n    assert latex(StrLength(Str('x'))) == r'\\left| x \\right|'\n", "def test_latex_complex_number():\n    assert latex(1 + 2*I) == '1 + 2i'\n    assert latex(1 + 2*I, imaginary_unit='j') == '1 + 2j'\n    assert latex(1 + 2*I, imaginary_unit='foo') == '1 + 2foo'\n", "def test_issue_17808():\n    from sympy import Lambda, Matrix, exp, MatrixSymbol\n    X = MatrixSymbol('X', 3, 3)\n    A = Matrix([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n    f = Lambda(X, exp(X*A))\n    assert latex(f(X)) == r\"e^{X A}\"\n", "def test_latex_printer_tensor_with_matrix():\n    from sympy import Matrix, TensorIndexType, tensor_indices, TensorHead, tensor_heads\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A, B = tensor_heads(\"A B\", [L])\n    H = TensorHead(\"H\", [L, L])\n    K = TensorHead(\"K\", [L, L, L, L])\n    M = MatrixSymbol(\"M\", 3, 3)\n    N = MatrixSymbol(\"N\", 3, 3)\n\n    expr = A(i)*M(i, j)*N(j, k)\n    assert latex(expr) == \"A{}^{i}M{}^{i}_{j}N{}^{j}_{k}\"\n\n    expr = H(i, j)*M(i, k)*N(k, j)\n    assert latex(expr) == \"H{}^{ij}M{}^{i}_{k}N{}^{k}_{j}\"\n\n    expr = K(i, j, k, l)*M(i, k)*N(l, j)\n    assert latex(expr) == \"K{}^{ijkl}M{}^{i}_{k}N{}^{l}_{j}\"\n"], "sample_1190": ["def test_physical_constant_property():\n    assert not meter.is_physical_constant\n    assert not joule.is_physical_constant\n    assert not day.is_physical_constant\n    assert not second.is_physical_constant\n    assert not volt.is_physical_constant\n    assert not ohm.is_physical_constant\n    assert not centimeter.is_physical_constant\n    assert not kilometer.is_physical_constant\n    assert not kilogram.is_physical_constant\n    assert not pebibyte.is_physical_constant\n    assert elementary_charge.is_physical_constant\n    assert speed_of_light.is_physical_constant\n    assert vacuum_permittivity.is_physical_constant\n    assert gravitational_constant.is_physical_constant\n    assert molar_gas_constant.is_physical_constant\n", "def test_physical_constants():\n    assert elementary_charge.convert_to(coulomb) == 1.602176634e-19*coulomb\n    assert molar_gas_constant.convert_to(joule/(mole*kelvin)) == 8.31446261815324*joule/(mole*kelvin)\n    assert vacuum_permittivity.convert_to(coulomb/(volt*meter)) == 8.8541878128e-12*coulomb/(volt*meter)\n", "def test_quantity_comparison():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(20), meter)\n\n    assert u != v\n    assert u < v\n    assert u <= v\n    assert v > u\n    assert v >= u\n\n    u.set_global_relative_scale_factor(S(20), meter)\n    assert u == v\n    assert not u < v\n    assert u <= v\n    assert not v > u\n    assert v >= u\n", "def test_issue_23011():\n    from sympy.physics.units import ampere, coulomb, second, volt, ohm\n    from sympy.physics.units.systems.si import dimsys_SI\n    assert coulomb.convert_to(ampere * second) == 1 * ampere * second\n    assert dimsys_SI.get_dimensional_dependencies(volt) == {length: 2, mass: 1, time: -3, current: 1}\n    assert ohm.convert_to(volt / ampere) == 1 * volt / ampere\n", "def test_physical_constant_property():\n    assert not meter.is_physical_constant\n    assert not joule.is_physical_constant\n    assert not day.is_physical_constant\n    assert not second.is_physical_constant\n    assert not volt.is_physical_constant\n    assert not ohm.is_physical_constant\n    assert not centimeter.is_physical_constant\n    assert not kilogram.is_physical_constant\n    assert not pebibyte.is_physical_constant\n    assert elementary_charge.is_physical_constant\n    assert speed_of_light.is_physical_constant\n"], "sample_719": ["def test_countvectorizer_with_stop_words():\n    # Test CountVectorizer with custom stop words\n    custom_stop_words = ['the', 'and']\n    vectorizer = CountVectorizer(stop_words=custom_stop_words)\n    vectorizer.fit(ALL_FOOD_DOCS)\n    assert 'the' not in vectorizer.vocabulary_\n    assert 'and' not in vectorizer.vocabulary_\n", "def test_vectorizer_lowercase():\n    # Test lowercase parameter\n    text_data = [\"This is a Test\", \"Another test with Uppercase\"]\n    vectorizer = CountVectorizer(lowercase=False)\n    X = vectorizer.fit_transform(text_data)\n    assert_equal(len(vectorizer.vocabulary_), 7)\n\n    vectorizer = CountVectorizer(lowercase=True)\n    X = vectorizer.fit_transform(text_data)\n    assert_equal(len(vectorizer.vocabulary_), 4)\n", "def test_vectorizer_max_features_less_than_one():\n    try:\n        vect = CountVectorizer(max_features=0)\n        assert False, \"max_features must be a positive integer\"\n    except ValueError as e:\n        assert_in(\"max_features must be a positive integer\", str(e))\n\n    try:\n        vect = CountVectorizer(max_features=-1)\n        assert False, \"max_features must be a positive integer\"\n    except ValueError as e:\n        assert_in(\"max_features must be a positive integer\", str(e))\n", "def test_tfidf_vectorizer_fit_transform_copy():\n    # Test that fit_transform makes a copy of the input data when copy=True\n    X = np.array([\"numpy\", \"scipy\", \"sklearn\"])\n    vectorizer = TfidfVectorizer()\n    X_idf = vectorizer.fit_transform(X, copy=True)\n    X[0] = \"new\"\n    X_idf_transformed = vectorizer.transform(X)\n    assert X_idf_transformed[0].sum() != 0  # The first document should still be present\n", "def test_tfidf_transformer_copy():\n    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)\n    X_copy = X.copy()\n\n    X_trans = TfidfTransformer().fit_transform(X, copy=True)\n    assert_allclose_dense_sparse(X, X_copy)\n\n    X_trans_inplace = TfidfTransformer().fit_transform(X, copy=False)\n    assert_allclose_dense_sparse(X, X_trans_inplace)\n"], "sample_1181": ["def test_numpy_sinc():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    from sympy import sinc, pi\n\n    expr = sinc(x)\n    f = lambdify(x, expr, 'numpy')\n\n    x_ = np.linspace(-pi, pi, 100)\n    assert np.allclose(f(x_), np.sinc(x_ / pi))\n", "def test_numpy_print_known_funcs_consts():\n    printer = NumPyPrinter()\n    assert printer._print_sin(sin(x)) == 'numpy.sin(x)'\n    assert printer._print_cos(cos(x)) == 'numpy.cos(x)'\n    assert printer._print_tan(tan(x)) == 'numpy.tan(x)'\n    assert printer._print_exp(exp(x)) == 'numpy.exp(x)'\n    assert printer._print_log(log(x)) == 'numpy.log(x)'\n    assert printer._print_sqrt(sqrt(x)) == 'numpy.sqrt(x)'\n    assert printer._print_abs(Abs(x)) == 'numpy.abs(x)'\n    assert printer._print_Pi(Pi) == 'numpy.pi'\n    assert printer._print_E(E) == 'numpy.e'\n", "def test_scipy_special_functions():\n    if not np or not import_module('scipy.special'):\n        skip(\"SciPy or NumPy not installed\")\n\n    from scipy.special import erf, gamma\n\n    e = erf(x)\n    f = lambdify((x,), e, 'scipy')\n    x_ = np.linspace(-1, 1, 10)\n    assert np.allclose(f(x_), erf(x_))\n\n    g = gamma(x)\n    f = lambdify((x,), g, 'scipy')\n    x_ = np.linspace(1, 10, 10)\n    assert np.allclose(f(x_), gamma(x_))\n", "def test_numpy_print_Min_Max():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    # Test the _print_Min method\n    expr_min = Min(a, b)\n    f_min = lambdify((a, b), expr_min, 'numpy')\n    a_ = np.array([2, 4, 6])\n    b_ = np.array([1, 3, 5])\n    assert np.array_equal(f_min(a_, b_), [1, 3, 5])\n\n    # Test the _print_Max method\n    expr_max = Max(a, b)\n    f_max = lambdify((a, b), expr_max, 'numpy')\n    assert np.array_equal(f_max(a_, b_), [2, 4, 6])\n", "def test_numpy_hadamard_product():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    from sympy import HadamardProduct\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = HadamardProduct(A, B)\n    f = lambdify((A, B), C, \"numpy\")\n\n    A_np = np.array([[1, 2], [3, 4]])\n    B_np = np.array([[5, 6], [7, 8]])\n    expected_result = np.array([[5, 12], [21, 32]])\n\n    assert np.array_equal(f(A_np, B_np), expected_result)\n"], "sample_98": ["def test_wsgi_application_import_error(self):\n    \"\"\"\n    The WSGI application specified in settings.WSGI_APPLICATION\n    should be importable.\n    \"\"\"\n    with self.settings(WSGI_APPLICATION='servers.invalid_module'):\n        with self.assertRaises(ImproperlyConfigured):\n            get_internal_wsgi_application()\n", "    def test_header_underscores(self):\n        \"\"\"\n        The server should strip all headers with underscores in the name before\n        constructing the WSGI environ. This prevents header-spoofing based on\n        ambiguity between underscores and dashes both normalized to underscores\n        in WSGI env vars. Nginx and Apache 2.4+ both do this as well.\n        \"\"\"\n        conn = HTTPConnection(\n            LiveServerHeaders.server_thread.host,\n            LiveServerHeaders.server_thread.port,\n        )\n        try:\n            conn.request('GET', '/environ_view/', headers={'Header_With_Underscore': 'test'})\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertNotIn(b\"HEADER_WITH_UNDERSCORE\", response.read())\n        finally:\n            conn.close()\n", "    def test_https_error_message(self):\n        \"\"\"\n        The server sends a proper error message if it's accessed over HTTPS but\n        SSL is not enabled.\n        \"\"\"\n        with self.assertLogs('django.server', 'ERROR') as cm:\n            with self.assertRaises(HTTPError) as err:\n                urlopen(self.live_server_url + '/example_view/')\n            err.exception.close()\n            self.assertEqual(err.exception.code, 500, 'Expected 500 response')\n            self.assertIn(\n                \"You're accessing the development server over HTTPS, but it only supports HTTP.\",\n                cm.output[0],\n            )\n", "    def test_underscore_header_is_removed(self):\n        \"\"\"\n        Headers with underscores are removed to prevent header spoofing.\n        \"\"\"\n        conn = HTTPConnection(\n            LiveServerHeaderSanitization.server_thread.host,\n            LiveServerHeaderSanitization.server_thread.port,\n            timeout=1,\n        )\n        try:\n            conn.request('GET', '/environ_view/', headers={'Header_With_Underscore': 'spoofed'})\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertNotIn(b\"Header_With_Underscore\", response.read())\n        finally:\n            conn.close()\n", "    def test_thread_safety(self):\n        \"\"\"\n        Single threaded server is thread safe.\n        \"\"\"\n        # Access the server from two threads simultaneously.\n            with self.urlopen('/example_view/') as f:\n                self.assertEqual(f.read(), b'example view')\n\n        # Use a thread pool to execute the requests simultaneously.\n        from concurrent.futures import ThreadPoolExecutor\n        with ThreadPoolExecutor() as executor:\n            # Submit the requests to the thread pool.\n            future_to_request = {executor.submit(access_server): request for request in range(10)}\n            # Wait for the requests to complete.\n            for future in concurrent.futures.as_completed(future_to_request):\n                future.result()  # If an exception is raised, it will be raised here.\n"], "sample_868": ["def test_empty_input(metric):\n    # all metrics should support empty input\n    metric([], [])\n", "def test_empty_labels(metric_name):\n    # All clustering metrics should return 1.0 for empty labels\n    y_true = np.array([])\n    y_pred = np.array([])\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric(y_true, y_pred) == pytest.approx(1.0)\n", "def test_empty_clusters(metric_name):\n    # Metrics should handle empty clusters\n    y_true = [0, 0, 1, 1, 1]\n    y_pred = [0, 0, 2, 2, 2]\n    metric = SUPERVISED_METRICS[metric_name]\n    score = metric(y_true, y_pred)\n    assert score >= 0.0 and score <= 1.0\n", "def test_consistency_with_contingency(metric_name):\n    # Test consistency of supervised metrics with their contingency matrix\n    y_true = [0, 0, 1, 1, 2, 2]\n    y_pred = [0, 1, 0, 1, 2, 3]\n    metric = SUPERVISED_METRICS[metric_name]\n    contingency = np.array([[2, 1, 0, 0],\n                            [1, 1, 0, 0],\n                            [0, 0, 1, 1]])\n    score_direct = metric(y_true, y_pred)\n    score_from_contingency = metric(y_true, y_pred, contingency=contingency)\n    assert_allclose(score_direct, score_from_contingency)\n", "def test_empty_labels(metric):\n    # Test that the metrics return 1.0 when labels are empty\n    assert metric([], []) == 1.0\n"], "sample_636": ["def test_duplicate_code_raw_strings_ignore_imports(self) -> None:\n    \"\"\"Tests the ignore-imports option.\"\"\"\n    path = join(DATA, \"raw_strings_ignore_imports\")\n    self._runtest([path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports\"], code=0)\n", "def test_duplicate_code_raw_strings_disable_line_end_multiple(self) -> None:\n    \"\"\"Tests disabling duplicate-code at a line at the end of multiple pieces of similar code.\"\"\"\n    path = join(DATA, \"raw_strings_disable_line_end_multiple\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_raw_strings_ignore_imports(self) -> None:\n    \"\"\"Tests the ignore-imports option of the duplicate-code checker.\"\"\"\n    path = join(DATA, \"raw_strings_ignore_imports\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_raw_strings_disable_line_within_similarity(self) -> None:\n    \"\"\"Tests disabling duplicate-code at a line within a piece of similar code.\"\"\"\n    path = join(DATA, \"raw_strings_disable_line_within_similarity\")\n    self._runtest([path, \"--disable=all\", \"--enable=duplicate-code\"], code=0)\n", "def test_duplicate_code_raw_strings_disable_imports(self) -> None:\n    \"\"\"Tests disabling duplicate-code for import statements.\"\"\"\n    path = join(DATA, \"raw_strings_disable_imports\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports\"],\n        expected_output=expected_output,\n    )\n"], "sample_500": ["def test_colorbar_ticklocation():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    plt.contourf(data, levels=levels)\n    cbar = plt.colorbar(orientation='horizontal', ticklocation='left')\n    assert cbar.ticklocation == 'left'\n\n    cbar = plt.colorbar(orientation='vertical', ticklocation='top')\n    assert cbar.ticklocation == 'top'\n", "def test_colorbar_custom_ticks():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    plt.contourf(data, levels=levels)\n    cbar = plt.colorbar(orientation='horizontal')\n\n    custom_ticks = [200, 600, 1000]\n    cbar.set_ticks(custom_ticks)\n    assert list(cbar.get_ticks()) == custom_ticks\n", "def test_twoslope_colorbar_different_vcenter():\n    # Test a TwoSlopeNorm with vcenter different from 0\n    fig, ax = plt.subplots()\n\n    norm = mcolors.TwoSlopeNorm(10, -5, 15)\n    pc = ax.pcolormesh(np.arange(1, 11), np.arange(1, 11),\n                       np.arange(100).reshape(10, 10),\n                       norm=norm, cmap='RdBu_r')\n    fig.colorbar(pc)\n", "def test_colorbar_inverted_ticks_with_shrink():\n    fig, axs = plt.subplots(2)\n    pc = axs[0].pcolormesh(10**np.arange(1, 5).reshape(2, 2), norm=LogNorm())\n    cbar = fig.colorbar(pc, ax=axs[0], extend='both', shrink=0.5)\n    ticks = cbar.get_ticks()\n    cbar.ax.invert_yaxis()\n    np.testing.assert_allclose(ticks, cbar.get_ticks())\n\n    ax = axs[1]\n    pc = ax.pcolormesh(np.arange(1, 5).reshape(2, 2))\n    cbar = fig.colorbar(pc, ax=ax, extend='both', shrink=0.8)\n    cbar.minorticks_on()\n    ticks = cbar.get_ticks()\n    minorticks = cbar.get_ticks(minor=True)\n    cbar.ax.invert_yaxis()\n    np.testing.assert_allclose(ticks, cbar.get_ticks())\n    np.testing.assert_allclose(minorticks, cbar.get_ticks(minor=True))\n", "def test_colorbar_log_label():\n    fig, ax = plt.subplots()\n    norm = mcolors.LogNorm(vmin=1e-4, vmax=1e-1)\n    pc = ax.pcolormesh(np.arange(1, 11), np.arange(1, 11),\n                       np.arange(100).reshape(10, 10),\n                       norm=norm, cmap='viridis')\n    cb = fig.colorbar(pc, label='Log Scale')\n"], "sample_75": ["def test_remove_clears_prefetched_objects_with_queryset(self):\n    bookwithyear = BookWithYear.objects.get(pk=self.bookwithyear1.pk)\n    prefetch_related_objects([bookwithyear], 'bookreview_set')\n    self.assertCountEqual(bookwithyear.bookreview_set.all(), [self.bookreview1])\n    bookwithyear.bookreview_set.all().delete()\n    self.assertCountEqual(bookwithyear.bookreview_set.all(), [])\n", "    def test_create_clears_prefetched_objects(self):\n        bookwithyear = BookWithYear.objects.get(pk=self.bookwithyear1.pk)\n        prefetch_related_objects([bookwithyear], 'bookreview_set')\n        self.assertCountEqual(bookwithyear.bookreview_set.all(), [self.bookreview1])\n        new_review = BookReview.objects.create(book=bookwithyear)\n        self.assertNotIn(new_review, bookwithyear.bookreview_set.all())\n", "def test_prefetch_favorites(self):\n    with self.assertNumQueries(3):\n        authors = AuthorWithAge.objects.prefetch_related('favorite_authors')\n        favorites = [list(a.favorite_authors.all()) for a in authors]\n    self.assertEqual(favorites, [[self.author2], []])\n", "def test_custom_queryset_with_limit(self):\n    bookmark = Bookmark.objects.create(url='http://www.djangoproject.com/')\n    django_tag = TaggedItem.objects.create(content_object=bookmark, tag='django')\n    TaggedItem.objects.create(content_object=bookmark, tag='python')\n\n    with self.assertNumQueries(2):\n        bookmark = Bookmark.objects.prefetch_related(\n            Prefetch('tags', TaggedItem.objects.filter(tag='django'), to_attr='limited_tags'),\n        ).get()\n\n    with self.assertNumQueries(0):\n        self.assertEqual(list(bookmark.limited_tags), [django_tag])\n", "def test_prefetch_reverse_one_to_one(self):\n    with self.assertNumQueries(2):\n        author = AuthorWithAge.objects.get(pk=self.author1.pk)\n        prefetch_related_objects([author], 'book_set')\n    with self.assertNumQueries(0):\n        self.assertCountEqual(author.book_set.all(), [self.book1])\n"], "sample_89": ["    def test_tick_notifies_file_changed(self):\n        reloader = StatReloader()\n        reloader.watch_file(self.existing_file)\n        with mock.patch.object(reloader, 'notify_file_changed') as mock_notify:\n            ticker = reloader.tick()\n            next(ticker)\n            self.increment_mtime(self.existing_file)\n            next(ticker)\n        self.assertEqual(mock_notify.call_count, 1)\n        self.assertCountEqual(mock_notify.call_args[0], [self.existing_file])\n", "def test_snapshot_files_duplicate_paths(self):\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.existing_file]):\n        snapshot1 = dict(self.reloader.snapshot_files())\n        self.assertEqual(len(snapshot1), 1)\n        self.assertIn(self.existing_file, snapshot1)\n", "def test_run_calls_update_watches(self):\n    fake_main_thread = mock.MagicMock()\n    fake_main_thread.is_alive.return_value = False\n    fake_apps = mock.MagicMock()\n    fake_apps.ready_event.wait.return_value = True\n\n    with mock.patch('django.apps.apps', fake_apps):\n        with mock.patch('django.urls.get_resolver') as mocked_get_resolver:\n            mocked_get_resolver.return_value.urlconf_module = {}\n            with mock.patch.object(self.reloader, 'update_watches') as mocked_update_watches:\n                self.reloader.run(fake_main_thread)\n\n    mocked_update_watches.assert_called_once()\n", "def test_snapshot_files_does_not_raise_oserror(self):\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n        with mock.patch('pathlib.Path.stat', side_effect=OSError):\n            self.assertEqual(dict(self.reloader.snapshot_files()), {})\n", "def test_nonexistent_glob(self, mocked_modules, notify_mock):\n    nonexistent_file = self.tempdir / 'nonexistent_file.py'\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        # This should not raise a FileNotFoundError or any other exception\n        self.increment_mtime(nonexistent_file)\n    self.assertEqual(notify_mock.call_count, 0)\n"], "sample_847": ["def test_enet_copy_X_False_check_input_True():\n    X, y, _, _ = build_dataset()\n    X = X.copy(order='F')\n\n    original_X = X.copy()\n    enet = ElasticNet(copy_X=False)\n    enet.fit(X, y, check_input=True)\n\n    # No copying, X is overwritten\n    assert np.any(np.not_equal(original_X, X))\n", "def test_multi_task_lasso_and_enet_positive():\n    X, y, _, _ = build_dataset()\n    Y = np.c_[y, y]\n    clf = MultiTaskLasso(alpha=1, tol=1e-8, positive=True)\n    clf.fit(X, Y)\n    assert np.all(clf.coef_ >= 0)\n\n    clf = MultiTaskElasticNet(alpha=1, tol=1e-8, positive=True)\n    clf.fit(X, Y)\n    assert np.all(clf.coef_ >= 0)\n", "def test_sparse_input_shape():\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    csr = sparse.csr_matrix(X)\n    clf = ElasticNet()\n    clf.fit(csr, y)\n    assert clf.coef_.shape == (20,)\n", "def test_multi_task_enet_and_lasso_cv_large_alphas():\n    X, y, _, _ = build_dataset(n_features=50, n_targets=3)\n    large_alphas = np.logspace(0, 2, 5)\n    clf = MultiTaskElasticNetCV(cv=3, alphas=large_alphas).fit(X, y)\n    assert_almost_equal(clf.alpha_, large_alphas[0], 3)\n    clf = MultiTaskLassoCV(cv=3, alphas=large_alphas).fit(X, y)\n    assert_almost_equal(clf.alpha_, large_alphas[0], 3)\n", "def test_lasso_alpha_invalid():\n    X = [[-1], [0], [1]]\n    Y = [-1, 0, 1]       # just a straight line\n\n    clf = Lasso(alpha=0.5, max_iter=1000, precompute=True)\n    clf.fit(X, Y)\n    # alpha=0.5 should not raise a warning\n    assert_warns(None, clf.fit, X, Y)\n\n    clf = Lasso(alpha=-0.5, max_iter=1000, precompute=True)\n    # alpha=-0.5 should raise a warning\n    assert_warns(UserWarning, clf.fit, X, Y)\n"], "sample_692": ["def test_mktemp_unnumbered(pytester: Pytester) -> None:\n    mytemp = pytester.mkdir(\"mytemp\")\n    p = pytester.makepyfile(\n        \"\"\"\n            tmpdir_factory.mktemp('unnumbered', numbered=False)\n        \"\"\"\n    )\n\n    result = pytester.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert result.ret == 0\n    assert mytemp.joinpath(\"unnumbered\").exists()\n", "def test_tmp_path_factory_with_custom_basetemp(pytester: Pytester, monkeypatch) -> None:\n    \"\"\"Test that tmp_path_factory uses the custom basetemp if provided.\"\"\"\n    custom_basetemp = pytester.mkdir(\"custom_basetemp\")\n    monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(custom_basetemp))\n    pytester.makepyfile(\n        \"\"\"\n            assert tmp_path_factory.getbasetemp() == custom_basetemp\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_tmpdir_factory_mktemp(pytester: Pytester) -> None:\n    \"\"\"Test that tmpdir_factory.mktemp returns a py.path.local object\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            tmp = tmpdir_factory.mktemp(\"test_dir\")\n            assert isinstance(tmp, py.path.local)\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_mktemp_with_existing_directory(tmp_path):\n    \"\"\"Test that mktemp raises an error if the directory already exists.\"\"\"\n    existing_dir = tmp_path / \"existing_dir\"\n    existing_dir.mkdir()\n\n    config = cast(Config, FakeConfig(tmp_path))\n    t = TempPathFactory.from_config(config, _ispytest=True)\n\n    with pytest.raises(ValueError):\n        t.mktemp(\"existing_dir\", numbered=False)\n", "def test_tmpdir_factory_relative_basetemp(pytester: Pytester) -> None:\n    \"\"\"Test the case when basetemp is a relative path.\"\"\"\n    mytemp = \"mytemp\"\n    p = pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    pytester.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert Path(mytemp).exists()\n    (Path(mytemp) / \"hello\").touch()\n\n    pytester.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert Path(mytemp).exists()\n    assert not (Path(mytemp) / \"hello\").exists()\n"], "sample_795": ["def test_check_estimator_with_deprecated_fit_method():\n    # Tests that check_estimator works on a class with\n    # a deprecated fit method\n\n    class TestEstimatorWithDeprecatedFitMethod(BaseEstimator):\n        @deprecated(\"Deprecated for the purpose of testing check_estimator\")\n            return self\n\n    check_estimator(TestEstimatorWithDeprecatedFitMethod)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # check that the class weights are computed correctly for linear classifiers\n    from sklearn.linear_model import LinearRegression, LogisticRegression\n    from sklearn.svm import LinearSVC\n\n    check_class_weight_balanced_linear_classifier(\"LinearRegression\", LinearRegression)\n    check_class_weight_balanced_linear_classifier(\"LogisticRegression\", LogisticRegression)\n    check_class_weight_balanced_linear_classifier(\"LinearSVC\", LinearSVC)\n", "def test_check_classifier_data_not_an_array():\n    # Check if classifier throws an exception when fed data that is not an array\n    from sklearn.datasets import make_blobs\n    from sklearn.utils.testing import NotAnArray\n\n    class NotAnArrayClassifier(BaseEstimator, ClassifierMixin):\n            return self\n\n            return np.ones(X.shape[0])\n\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n    X = NotAnArray(np.asarray(X))\n    y = NotAnArray(np.asarray(y))\n\n    estimator = NotAnArrayClassifier()\n    estimator.fit(X, y)\n    estimator.predict(X)\n", "def test_check_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n    from sklearn.datasets import load_boston\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    e = LinearRegression()  # using a regressor to test the check\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, check_classifiers_regression_target, \"estimator\", e)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # check_class_weight_balanced_linear_classifier is a test function, so we can directly call it\n    # here we are using a dummy classifier for testing purposes\n    class DummyClassifier:\n            self.coef_ = coef_\n\n            # In a real classifier, this would be where the model is trained\n            # For this test, we're just assigning a dummy value to coef_\n            self.coef_ = np.array([[1.0, 1.0]])\n            return self\n\n    check_class_weight_balanced_linear_classifier(\"DummyClassifier\", DummyClassifier)\n"], "sample_0": ["def test_self_conversion_via_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        start_uncert.represent_as(UncertClass)\n", "def test_self_conversion_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        start_uncert.represent_as(UncertClass)\n", "def test_self_conversion_via_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        start_uncert.represent_as(UncertClass)\n", "def test_conversion_to_from_stddev_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    stddev_uncert = start_uncert.represent_as(StdDevUncertainty)\n    final_uncert = stddev_uncert.represent_as(UncertClass)\n    assert_allclose(np.sqrt(start_uncert.array), stddev_uncert.array)\n    assert_array_equal(start_uncert.array, final_uncert.array)\n    assert start_uncert.unit == final_uncert.unit\n", "def test_conversion_not_supported_from_variance(UncertClass):\n    var_uncert = VarianceUncertainty([1, 2, 3], unit=u.adu**2)\n    with pytest.raises(TypeError):\n        var_uncert.represent_as(UncertClass)\n"], "sample_559": ["def test_axesgrid_colorbar_log_position():\n    fig = plt.figure()\n    grid = AxesGrid(fig, 111,\n                    nrows_ncols=(1, 1),\n                    ngrids=1,\n                    label_mode=\"L\",\n                    cbar_location=\"top\",\n                    cbar_mode=\"single\",\n                    )\n\n    Z = 10000 * np.random.rand(10, 10)\n    im = grid[0].imshow(Z, interpolation=\"nearest\", norm=LogNorm())\n\n    cax = grid.cbar_axes[0]\n    cax.set_position([0.2, 0.8, 0.6, 0.05])\n    cax.colorbar(im)\n", "def test_rgba_axes_extent():\n    fig = plt.figure()\n    ax = RGBAxes(fig, (0.1, 0.1, 0.8, 0.8), pad=0.1, extent=(0, 10, 0, 10))\n    rng = np.random.default_rng(19680801)\n    r = rng.random((5, 5))\n    g = rng.random((5, 5))\n    b = rng.random((5, 5))\n    ax.imshow_rgb(r, g, b, interpolation='none')\n    assert ax.get_xlim() == (0, 10)\n    assert ax.get_ylim() == (0, 10)\n", "def test_inset_axes_invalid_loc():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=\"Invalid loc parameter\"):\n        inset_axes(ax, width=\"3%\", height=\"70%\", loc=5)\n", "def test_inset_axes_bbox_transform():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3])\n    ax.axis([1, 3, 1, 3])\n    inset_ax = inset_axes(ax, width=\"50%\", height=\"50%\", bbox_transform=ax.transData)\n    inset_ax.axis([1.1, 1.4, 1.1, 1.4])\n    fig.canvas.draw()\n    inset_bbox = inset_ax.get_position().extents\n    expected_bbox = [1.1/3, 1.1/3, 1.4/3, 1.4/3]\n    np.testing.assert_array_almost_equal(inset_bbox, expected_bbox)\n", "def test_inset_locator_zoom_one():\n    fig, ax = plt.subplots(figsize=[5, 4])\n\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n    Z2 = np.zeros((150, 150))\n    ny, nx = Z.shape\n    Z2[30:30+ny, 30:30+nx] = Z\n\n    ax.imshow(Z2, extent=extent, interpolation=\"nearest\",\n              origin=\"lower\")\n\n    axins = zoomed_inset_axes(ax, zoom=1, loc='upper right')\n    axins.imshow(Z2, extent=extent, interpolation=\"nearest\",\n                 origin=\"lower\")\n    axins.yaxis.get_major_locator().set_params(nbins=7)\n    axins.xaxis.get_major_locator().set_params(nbins=7)\n\n    x1, x2, y1, y2 = -1.5, -0.9, -2.5, -1.9\n    axins.set_xlim(x1, x2)\n    axins.set_ylim(y1, y2)\n\n    plt.xticks(visible=False)\n    plt.yticks(visible=False)\n\n    mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n\n    asb = AnchoredSizeBar(ax.transData,\n                          0.5,\n                          '0.5',\n                          loc='lower center',\n                          pad=0.1, borderpad=0.5, sep=5,\n                          frameon=False)\n    ax.add_artist(asb)\n"], "sample_684": ["def test_frame_eval(mocker) -> None:\n        return sys._getframe(0)\n\n    fr1 = Frame(f1(\"a\"))\n    mocker.patch.object(fr1, 'f_locals', {'x': 'b'})\n    assert fr1.eval('x') == 'b'\n", "def test_exception_chain_repr(tw_mock) -> None:\n        raise ValueError(\"ValueError in func1\")\n\n        try:\n            func1()\n        except ValueError as e:\n            raise TypeError(\"TypeError in func2\") from e\n\n    try:\n        func2()\n    except TypeError:\n        exci = ExceptionInfo.from_current()\n\n    repr_chain = exci.getrepr()\n    assert isinstance(repr_chain, ExceptionChainRepr)\n    assert len(repr_chain.chain) == 2\n    repr_chain.toterminal(tw_mock)\n    assert \"ValueError in func1\" in tw_mock.output\n    assert \"TypeError in func2\" in tw_mock.output\n", "def test_frame_eval(monkeypatch):\n        return sys._getframe(0)\n\n    fr = Frame(f(\"test\"))\n\n        return \"mocked_result\"\n\n    monkeypatch.setattr(fr, \"eval\", mock_eval)\n\n    result = fr.eval(\"code\", a=1, b=2)\n    assert result == \"mocked_result\"\n", "def test_exception_info_repr_chain(tw_mock) -> None:\n        try:\n            raise ValueError(\"Nested Exception\")\n        except ValueError as ve:\n            raise TypeError(\"Outer Exception\") from ve\n\n    try:\n        raise_nested_exception()\n    except Exception:\n        exci = ExceptionInfo.from_current()\n\n    repr = exci.getrepr()\n    repr.toterminal(tw_mock)\n\n    # Check if both exceptions are present in the output\n    assert \"ValueError: Nested Exception\" in \"\".join(tw_mock.lines)\n    assert \"TypeError: Outer Exception\" in \"\".join(tw_mock.lines)\n", "    def test_repr_chain(self) -> None:\n        try:\n            raise ValueError(\"first exception\")\n        except ValueError:\n            try:\n                raise TypeError(\"second exception\")\n            except TypeError:\n                excinfo = ExceptionInfo.from_current()\n\n        repr_excinfo = excinfo.getrepr()\n        assert isinstance(repr_excinfo, ExceptionChainRepr)\n        assert len(repr_excinfo.chain) == 2\n        assert repr_excinfo.chain[0][0].style == \"long\"\n        assert repr_excinfo.chain[0][2] == \"During handling of the above exception, another exception occurred:\"\n        assert repr_excinfo.chain[1][0].style == \"long\"\n        assert repr_excinfo.chain[1][2] is None\n"], "sample_393": ["def test_add_location_default(self):\n    \"\"\"makemessages --add-location (no value)\"\"\"\n    management.call_command(\n        \"makemessages\", locale=[LOCALE], verbosity=0, add_location=\"\"\n    )\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    # Comment with source file relative path and line number is present.\n    self.assertLocationCommentPresent(\n        self.PO_FILE, \"Translatable literal #6b\", \"templates\", \"test.html\"\n    )\n", "    def test_no_obsolete_messages(self):\n        \"\"\"\n        --no-obsolete removes obsolete messages.\n        \"\"\"\n        management.call_command(\n            \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n        )\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn(\"This literal should be removed.\", po_contents)\n", "    def test_no_obsolete_messages(self):\n        management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            # Check that obsolete message is removed\n            self.assertNotIn(\"#, fuzzy\", po_contents)\n            self.assertNotIn(\"#~ msgid \\\"Obsolete message\\\"\", po_contents)\n            self.assertNotIn(\"#~ msgstr \\\"Old translation\\\"\", po_contents)\n", "def test_no_obsolete_enabled(self):\n    \"\"\"makemessages --no-obsolete\"\"\"\n    management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n    self.assertNotIn(\"#, fuzzy\", po_contents)\n", "def test_no_obsolete_enabled(self):\n    \"\"\"makemessages --no-obsolete\"\"\"\n    management.call_command(\n        \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n    )\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertNotIn(\"#~\", po_contents)\n"], "sample_477": ["def test_random03(self):\n    output = self.engine.render_to_string(\n        \"random03\", {\"c\": [\"apple\", \"banana\", \"cherry\"]}\n    )\n    self.assertIn(output, [\"apple\", \"banana\", \"cherry\"])\n", "def test_random03(self):\n    output = self.engine.render_to_string(\n        \"random03\", {\"a\": [\"x\", \"y\", \"z\"], \"b\": [\"1\", \"2\", \"3\"]}\n    )\n    self.assertIn(output, [\"x 1\", \"x 2\", \"x 3\", \"y 1\", \"y 2\", \"y 3\", \"z 1\", \"z 2\", \"z 3\"])\n", "def test_random03(self):\n    output = self.engine.render_to_string(\"random03\", {\"a\": [\"test1\", \"test2\", \"test3\"]})\n    self.assertIn(output, [\"test1\", \"test2\", \"test3\"])\n", "def test_random03(self):\n    output = self.engine.render_to_string(\n        \"random03\", {\"a\": [\"a&b\", \"a&b\"], \"b\": [\"c&d\", \"c&d\"]}\n    )\n    self.assertEqual(output, \"a&amp;b c&amp;d\")\n", "def test_random03(self):\n    output = self.engine.render_to_string(\n        \"random03\", {\"c\": [\"c&d\", \"c&d\"], \"d\": [mark_safe(\"c&d\"), mark_safe(\"c&d\")]}\n    )\n    self.assertEqual(output, \"c&amp;d c&d\")\n"], "sample_1139": ["def test_ImageSet_simplification():\n    from sympy.abc import n, m\n    assert imageset(Lambda(n, n), S.Integers) == S.Integers\n    assert imageset(Lambda(n, sin(n)),\n                    imageset(Lambda(m, tan(m)), S.Integers)) == \\\n            imageset(Lambda(m, sin(tan(m))), S.Integers)\n    assert imageset(n, 1 + 2*n, S.Naturals) == Range(3, oo, 2)\n    assert imageset(n, 1 + 2*n, S.Naturals0) == Range(1, oo, 2)\n    assert imageset(n, 1 - 2*n, S.Naturals) == Range(-1, -oo, -2)\n", "def test_issue_17858():\n    assert 1 in Range(-oo, oo)\n    assert 0 in Range(oo, -oo, -1)\n    assert oo not in Range(-oo, oo)\n    assert -oo not in Range(-oo, oo)\n    assert oo not in Range(-oo, oo, 2)\n    assert -oo not in Range(-oo, oo, 2)\n    assert 0 not in Range(-oo, oo, 2)\n    assert 2 in Range(-oo, oo, 2)\n    assert -2 in Range(-oo, oo, 2)\n", "def test_issue_17858_cont():\n    assert 1 not in Range(oo, -oo, -1)\n    assert 0 not in Range(oo, -oo, -1)\n    assert 0 not in Range(-oo, oo)\n    assert -oo not in Range(-oo, oo)\n    assert oo not in Range(-oo, oo, -1)\n    assert -oo not in Range(-oo, oo, -1)\n", "def test_issue_17858():\n    assert 1 in Range(-oo, oo)\n    assert 0 in Range(oo, -oo, -1)\n    assert oo not in Range(-oo, oo)\n    assert -oo not in Range(-oo, oo)\n", "def test_issue_18089():\n    assert imageset(Lambda(x, x**2), S.Naturals0) == Range(0, oo)\n    assert imageset(Lambda(x, x**2), S.Naturals) == Range(1, oo)\n"], "sample_520": ["def test_quiver3d_masked_sparse():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Using mgrid here instead of ogrid because masked_where doesn't\n    # seem to like broadcasting very much...\n    x, y, z = np.mgrid[-1:0.8:10j, -1:0.8:10j, -1:0.6:3j]\n\n    u = np.sin(np.pi * x) * np.cos(np.pi * y) * np.cos(np.pi * z)\n    v = -np.cos(np.pi * x) * np.sin(np.pi * y) * np.cos(np.pi * z)\n    w = (2/3)**0.5 * np.cos(np.pi * x) * np.cos(np.pi * y) * np.sin(np.pi * z)\n    u = np.ma.masked_where((-0.4 < x) & (x < 0.1), u, copy=False)\n    v = np.ma.masked_where((0.1 < y) & (y < 0.7), v, copy=False)\n\n    ax.quiver(x, y, z, u, v, w, length=0.1, pivot='tip', normalize=True, sparse=0.5)\n", "def test_quiver3d_linewidth():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Check that array-like linewidth can be set\n    x, y, z = np.mgrid[-2:2:20j, -2:2:20j, -2:2:20j]\n    u = np.sin(np.pi * x) * np.cos(np.pi * y) * np.cos(np.pi * z)\n    v = -np.cos(np.pi * x) * np.sin(np.pi * y) * np.cos(np.pi * z)\n    w = (2/3)**0.5 * np.cos(np.pi * x) * np.cos(np.pi * y) * np.sin(np.pi * z)\n\n    linewidths = np.full(x.shape, 1)\n    linewidths[::2, ::2, ::2] = 3\n\n    ax.quiver(x, y, z, u, v, w, length=0.1, pivot='tip', normalize=True, linewidths=linewidths)\n", "def test_surface3d_cmap_under_over():\n    X = np.arange(-5, 5, 0.25)\n    Y = np.arange(-5, 5, 0.25)\n    X, Y = np.meshgrid(X, Y)\n    R = np.sqrt(X ** 2 + Y ** 2)\n    Z = np.sin(R)\n\n    cmap = plt.get_cmap('viridis').copy()\n    cmap.set_under('red')\n    cmap.set_over('blue')\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.plot_surface(X, Y, Z, cmap=cmap, vmin=-0.5, vmax=0.5, lw=0, antialiased=False)\n    ax.set_zlim(-1.01, 1.01)\n", "def test_text_3d_modification():\n    # Test that modifying Text3D position after the fact works the same as\n    # setting it directly.\n    fig = plt.figure()\n    ax_test = fig.add_subplot(projection='3d')\n    t = art3d.Text3D(0, 0, 0, 'test')\n    ax_test.add_artist(t)\n    t.set_position_3d((1, 1, 1))\n\n    ax_ref = fig.add_subplot(projection='3d')\n    ax_ref.text(1, 1, 1, 'test')\n\n    # Add more assertions to check if the texts are equivalent\n", "def test_quiver3d_invalid_input():\n    # Test with invalid input for u, v, w\n    x, y, z = np.arange(0, 5), np.arange(0, 5), np.arange(0, 5)\n    u, v, w = np.array([[1, 2, 3], [4, 5, 6]]), np.array([[1, 2], [3, 4]]), np.array([1, 2, 3, 4, 5])\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    with pytest.raises(ValueError):\n        ax.quiver(x, y, z, u, v, w)\n"], "sample_105": ["    def test_get_context_data_deprecation_warning(self):\n        \"\"\"A warning is raised when a generic template view passes kwargs as context.\"\"\"\n        with self.assertWarns(RemovedInDjango40Warning):\n            response = self.client.get('/template/simple/bar/')\n", "def test_template_view_with_kwargs(self):\n    \"\"\"A generic template view with kwargs handles multiple parameters correctly.\"\"\"\n    response = self.client.get('/template/multi/bar1/bar2/bar3/')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context['foo1'], 'bar1')\n    self.assertEqual(response.context['foo2'], 'bar2')\n    self.assertEqual(response.context['foo3'], 'bar3')\n    self.assertIsInstance(response.context['view'], View)\n", "    def test_template_view_get_context_data(self):\n        \"\"\"TemplateView uses get_context_data method to populate context.\"\"\"\n        response = self.client.get('/template/custom_context/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['custom_key'], 'custom_value')\n", "    def test_deprecation_warning(self):\n        \"\"\"\n        A deprecation warning is raised when accessing template view kwargs in\n        the context.\n        \"\"\"\n        with self.assertWarns(RemovedInDjango40Warning):\n            response = self.client.get('/template/simple/bar/')\n            self.assertEqual(response.status_code, 200)\n            # Accessing a kwarg in the context should raise a deprecation warning.\n            self.assertEqual(response.context['foo'], 'bar')\n", "def test_extra_template_params_override(self):\n    \"\"\"A template view's extra context can be overridden by URL kwargs.\"\"\"\n    response = self.client.get('/template/custom/overridden/bar2/')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context['foo1'], 'overridden')\n    self.assertEqual(response.context['foo2'], 'bar2')\n    self.assertEqual(response.context['key'], 'value')\n    self.assertIsInstance(response.context['view'], View)\n"], "sample_988": ["def test_issue_10928():\n    x = symbols('x')\n    assert str(Eq(x, zoo)) == 'Eq(x, zoo)'\n    assert str(Eq(x, -zoo)) == 'Eq(x, -zoo)'\n", "def test_issue_14238():\n    x = symbols('x')\n    assert str(Eq(x, zoo)) == 'Eq(x, zoo)'\n    assert str(Eq(x, -zoo)) == 'Eq(x, -zoo)'\n", "def test_as_set():\n    # Test as_set method for relational objects\n    assert Eq(x, y).as_set() == FiniteSet((x, y))\n    assert Ne(x, y).as_set() == Interval(-oo, x, True, True) + Interval(x, oo, True, True) - FiniteSet(x)\n    assert Gt(x, y).as_set() == Interval(y, oo, True, True)\n    assert Lt(x, y).as_set() == Interval(-oo, y, True, True)\n    assert Ge(x, y).as_set() == Interval(y, oo)\n    assert Le(x, y).as_set() == Interval(-oo, y)\n", "def test_issue_13825():\n    a, b, c, d = symbols('a b c d', positive=True)\n    assert (a*b < c*d) != (a/d > c/b)\n    assert (a/d > c/b) == (a/d > c/b, evaluate=False)\n", "def test_issue_10898():\n    x = symbols('x')\n    assert (x > -oo) != True\n    assert (x < oo) != True\n    assert (x > -oo) is S.true\n    assert (x < oo) is S.true\n"], "sample_1008": ["def test_ang_acc():\n    q1, q2, q3, q4 = dynamicsymbols('q1 q2 q3 q4')\n    q1d, q2d, q3d, q4d = dynamicsymbols('q1 q2 q3 q4', 1)\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    C = B.orientnew('C', 'Axis', [q3, B.y])\n    D = N.orientnew('D', 'Axis', [q4, N.y])\n    u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n    u1d, u2d, u3d = dynamicsymbols('u1 u2 u3', 1)\n    C.set_ang_vel(N, u1*C.x + u2*C.y + u3*C.z)\n    C.set_ang_acc(N, u1d*C.x + u2d*C.y + u3d*C.z)\n    assert C.ang_acc_in(N) == (u1d)*C.x + (u2d)*C.y + (u3d)*C.z\n    assert N.ang_acc_in(C) == (-u1d)*C.x + (-u2d)*C.y + (-u3d)*C.z\n    assert C.ang_acc_in(D) == (u1d)*C.x + (u2d)*C.y + (u3d)*C.z + (-q4d)*D.y\n    assert D.ang_acc_in(C) == (-u1d)*C.x + (-u2d)*C.y + (-u3d)*C.z + (q4d)*D.y\n", "def test_orient_DCM():\n    N = ReferenceFrame('N')\n    dcm_matrix = eye(3)\n    A = N.orientnew('A', 'DCM', dcm_matrix)\n    assert A.dcm(N) == dcm_matrix\n", "def test_ang_acc():\n    q1, q2, q3, q4 = dynamicsymbols('q1 q2 q3 q4')\n    q1d, q2d, q3d, q4d = dynamicsymbols('q1 q2 q3 q4', 1)\n    q1dd, q2dd, q3dd, q4dd = dynamicsymbols('q1 q2 q3 q4', 2)\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    C = B.orientnew('C', 'Axis', [q3, B.y])\n    D = N.orientnew('D', 'Axis', [q4, N.y])\n    u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n    u1d, u2d, u3d = dynamicsymbols('u1 u2 u3', 1)\n    A.set_ang_acc(N, u1*N.x + u2*N.y + u3*N.z)\n    assert A.ang_acc_in(N) == u1*N.x + u2*N.y + u3*N.z\n    assert N.ang_acc_in(A) == -u1*N.x - u2*N.y - u3*N.z\n    assert B.ang_acc_in(N) == u1*N.x + u2*N.y + u3*N.z - q2d*q1d*B.x - 2*q1d*q2d*B.y\n    assert N.ang_acc_in(B) == -u1*N.x - u2*N.y - u3*N.z + q2d*q1d*B.x + 2*q1d*q2d*B.y\n    C.set_ang_acc(N, u1*C.x + u2*C.y + u3*C.z)\n   ", "def test_set_ang_acc():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [dynamicsymbols('q1'), N.z])\n    B = A.orientnew('B', 'Axis', [dynamicsymbols('q2'), A.x])\n\n    A.set_ang_acc(N, 2*N.x)\n    B.set_ang_acc(A, 3*A.y)\n\n    assert A.ang_acc_in(N) == 2*N.x\n    assert B.ang_acc_in(N) == 2*N.x + 3*A.y\n    assert B.ang_acc_in(A) == 3*A.y\n", "def test_dcm_cache_reset():\n    q1, q2, q3, q4 = dynamicsymbols('q1 q2 q3 q4')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    C = N.orientnew('C', 'Axis', [q3, N.y])\n\n    # Test DCM cache before changing orientation\n    assert N.dcm(A) == Matrix([[cos(q1), -sin(q1), 0], [sin(q1), cos(q1), 0], [0, 0, 1]])\n    assert A.dcm(N) == Matrix([[cos(q1), sin(q1), 0], [-sin(q1), cos(q1), 0], [0, 0, 1]])\n    assert A.dcm(B) == eye(3)  # Identity matrix as they are aligned\n    assert B.dcm(A) == eye(3)  # Identity matrix as they are aligned\n\n    # Change orientation of A and check DCM cache reset\n    A.orient(N, 'Axis', [q4, N.y])\n    assert N.dcm(A) == Matrix([[cos(q4), 0, sin(q4)], [0, 1, 0], [-sin(q4), 0, cos(q4)]])\n    assert A.dcm(N) == Matrix([[cos(q4), 0, -sin(q4)], [0, 1, 0], [sin(q4), 0, cos(q4)]])\n    assert A.dcm(B) != eye(3)  # Not identity matrix as they are not aligned\n    assert B.dcm(A) != eye(3)  # Not identity matrix as they are not aligned\n\n    # Change orientation of B and check DCM cache reset\n    B.orient(A, 'Axis', [q2, A.z])\n    assert A.dcm(B) != eye(3)  # Not"], "sample_937": ["def test_unparse_assign(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].targets[0]) == expected\n", "def test_unparse_Subscript_slice_with_Tuple_and_Index():\n    module = ast.parse(\"a[1, 2:3]\")\n    assert ast.unparse(module.body[0].value) == \"a[1, 2:3]\"\n", "def test_unparse_annotation():\n    source = \"a: int\"\n    expected = \"a: int\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value.annotation) == expected\n", "def test_positional_only_arguments():\n    source = \"def f(a, b, /, c): pass\"\n    expected = \"a, b, /, c\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_keyword_arguments():\n    source = \"func(a=1, b=2, *args, c=3, **kwargs)\"\n    expected = \"func(a=1, b=2, *args, c=3, **kwargs)\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n"], "sample_225": ["    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.u2 = User.objects.create_user(username='user', password='secret', email='user@example.com')\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUp(self):\n        self.site = admin.AdminSite(name=\"test_adminsite\")\n        self.site.register(User)\n        self.site.register(Article)\n        self.site.disable_action('delete_selected')\n        request = self.request_factory.get(reverse('test_adminsite:index'))\n        request.user = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        self.ctx = self.site.each_context(request)\n"], "sample_896": ["def test_nmf_verbose(solver):\n    # Check verbose mode of NMF for better coverage.\n    A = np.random.RandomState(0).random_sample((100, 10))\n    nmf = NMF(solver=solver, tol=1e-2, random_state=0, verbose=1)\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    try:\n        nmf.fit(A)\n    finally:\n        sys.stdout = old_stdout\n", "def test_nmf_beta_loss_error(solver):\n    # Check that an error is raised if beta_loss is not supported by the solver\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    with pytest.raises(ValueError, match=\"Invalid beta_loss parameter\"):\n        NMF(solver=solver, beta_loss=-0.5).fit(X)\n", "def test_minibatch_nmf_with_small_batch_size():\n    # Check that MiniBatchNMF works with a small batch size\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(100, 5))\n\n    n_components = 5\n    batch_size = 1\n    max_iter = 2\n\n    mbnmf = MiniBatchNMF(\n        n_components=n_components,\n        init=\"custom\",\n        random_state=0,\n        max_iter=max_iter,\n        batch_size=batch_size,\n        tol=0,\n        max_no_improvement=None,\n        fresh_restarts=False,\n    )\n\n    # Force the same init of H (W is recomputed anyway) to be able to compare results.\n    W, H = nmf._initialize_nmf(\n        X, n_components=n_components, init=\"random\", random_state=0\n    )\n\n    mbnmf.fit(X, W=W, H=H)\n    assert mbnmf.n_steps_ == max_iter * (X.shape[0] // batch_size)\n", "def test_nmf_custom_init_shape_error():\n    # Check that an error is raised if custom H and/or W have incorrect shapes.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((20, 15))\n    H = rng.random_sample((15, 20))\n    W = rng.random_sample((15, 20))\n\n    with pytest.raises(ValueError, match=\"Input H has shape\"):\n        NMF(init=\"custom\").fit(X, H=H, W=W)\n\n    with pytest.raises(ValueError, match=\"Input W has shape\"):\n        NMF(init=\"custom\").fit(X, H=H, W=W)\n", "def test_nmf_shuffle(shuffle):\n    # Test that shuffle parameter in NMF has an effect\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(6, 5))\n    m1 = NMF(solver=\"cd\", shuffle=shuffle, init=\"random\", random_state=0)\n    m2 = NMF(solver=\"cd\", shuffle=shuffle, init=\"random\", random_state=0)\n    m1.fit(A)\n    m2.fit(A)\n    assert m1.n_iter_ != m2.n_iter_\n"], "sample_328": ["    def test_jsonfield_nullable(self):\n        objects = [JSONFieldNullable.objects.create(json_field={'key': 'value'}) for _ in range(10)]\n        for obj in objects:\n            obj.json_field = None\n        JSONFieldNullable.objects.bulk_update(objects, ['json_field'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(json_field__isnull=True), objects)\n", "def test_jsonfield(self):\n    json_models = [\n        JSONFieldNullable.objects.create(data={'key': 'value'})\n        for _ in range(10)\n    ]\n    new_data = {'key': 'new_value'}\n    for model in json_models:\n        model.data = new_data\n    JSONFieldNullable.objects.bulk_update(json_models, ['data'])\n    self.assertCountEqual(JSONFieldNullable.objects.filter(data=new_data), json_models)\n", "def test_json_field_nullable(self):\n    json_field_nullables = [\n        JSONFieldNullable.objects.create(json_data={\"key\": \"value\"})\n        for _ in range(10)\n    ]\n    for json_field_nullable in json_field_nullables:\n        json_field_nullable.json_data = {\"key\": \"new_value\"}\n    JSONFieldNullable.objects.bulk_update(json_field_nullables, ['json_data'])\n    self.assertCountEqual(\n        JSONFieldNullable.objects.values_list('json_data', flat=True),\n        [obj.json_data for obj in json_field_nullables]\n    )\n", "def test_jsonfield_nullable(self):\n    json_nullable_objs = [JSONFieldNullable.objects.create(data=None) for _ in range(10)]\n    new_data = {'key': 'value'}\n    for obj in json_nullable_objs:\n        obj.data = new_data\n    JSONFieldNullable.objects.bulk_update(json_nullable_objs, ['data'])\n    self.assertCountEqual(JSONFieldNullable.objects.filter(data=new_data), json_nullable_objs)\n", "    def test_json_field_nullable(self):\n        json_fields = [JSONFieldNullable.objects.create(json_data={'key': 'value'}) for _ in range(10)]\n        for json_field in json_fields:\n            json_field.json_data = None\n        JSONFieldNullable.objects.bulk_update(json_fields, ['json_data'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(json_data=None), json_fields)\n"], "sample_101": ["def test_limited_stream(self):\n    \"\"\"\n    LimitedStream reads from the underlying stream up to the limit.\n    \"\"\"\n    stream = BytesIO(b'a' * 1024 * 1024)\n    limited_stream = LimitedStream(stream, 512)\n    self.assertEqual(limited_stream.read(), b'a' * 512)\n    self.assertEqual(limited_stream.read(), b'')\n    self.assertEqual(stream.read(), b'a' * (1024 * 1024 - 512))\n", "    def test_limited_stream(self):\n        \"\"\"\n        LimitedStream wraps another stream to disallow reading it past a number of bytes.\n        \"\"\"\n        stream = BytesIO(b\"Hello World!\")\n        limited_stream = LimitedStream(stream, 5)\n\n        self.assertEqual(limited_stream.read(), b\"Hello\")\n        self.assertEqual(limited_stream.read(), b\"\")\n\n        limited_stream = LimitedStream(stream, 20)\n        self.assertEqual(limited_stream.readline(), b\"World!\\n\")\n        self.assertEqual(limited_stream.readline(), b\"\")\n", "def test_script_name_force(self):\n    \"\"\"\n    FORCE_SCRIPT_NAME setting overrides the calculated SCRIPT_NAME.\n    \"\"\"\n    application = get_wsgi_application()\n    environ = self.request_factory._base_environ(\n        PATH_INFO=\"/\",\n        REQUEST_METHOD=\"GET\",\n        SCRIPT_NAME=\"/original_script_name\",\n        HTTP_HOST=\"example.com\"\n    )\n    with override_settings(FORCE_SCRIPT_NAME=\"/forced_script_name\"):\n        request = application.request_class(environ)\n        self.assertEqual(request.environ[\"SCRIPT_NAME\"], \"/forced_script_name\")\n", "    def test_get_request(self):\n        \"\"\"\n        WSGIHandler correctly handles a GET request.\n        \"\"\"\n        handler = WSGIHandler()\n\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/test/\",\n            REQUEST_METHOD=\"GET\"\n        )\n\n        response_data = {}\n\n            response_data[\"status\"] = status\n            response_data[\"headers\"] = headers\n\n        response = handler(environ, start_response)\n\n        self.assertEqual(response_data[\"status\"], \"200 OK\")\n        self.assertEqual(\n            set(response_data[\"headers\"]),\n            {('Content-Length', '0'), ('Content-Type', 'text/html; charset=utf-8')})\n        self.assertEqual(b''.join(response), b'')\n", "    def test_limited_stream(self):\n        \"\"\"\n        LimitedStream correctly limits the amount of data that can be read.\n        \"\"\"\n        data = b'This is some test data.'\n        stream = LimitedStream(BytesIO(data), limit=9)\n        self.assertEqual(stream.read(), b'This is s')\n        self.assertEqual(stream.readline(), b'ome test data.')\n        self.assertEqual(stream.read(), b'')\n"], "sample_982": ["def test_primeomega():\n    assert primeomega(2) == 1\n    assert primeomega(2 * 3) == 2\n    assert primeomega(2 * 3 * 5) == 3\n    assert primeomega(3 * 25) == 4\n    assert [primeomega(p) for p in primerange(1, 10)] == [0, 1, 1, 1]\n    assert primeomega(fac(50)) == 85\n    assert primeomega(2 ** 9941 - 1) == 9940\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2 ** 31 - 1) == 31\n    assert summation(primeomega(n), (n, 2, 30)) == 107\n", "def test_primeomega():\n    assert primeomega(1) == 0\n    assert primeomega(2) == 1\n    assert primeomega(2 * 2) == 2\n    assert primeomega(2 * 2 * 3) == 3\n    assert primeomega(3 * 25) == 3\n    assert [primeomega(p) for p in primerange(1, 10)] == [0, 1, 1, 1, 1]\n    assert primeomega(fac(50)) == 77\n    assert primeomega(2 ** 9941 - 1) == 9940\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2 ** 31 - 1) == 31\n    assert summation(primeomega(n), (n, 2, 30)) == 71\n", "def test_primeomega():\n    assert primeomega(1) == 0\n    assert primeomega(2) == 1\n    assert primeomega(3) == 1\n    assert primeomega(6) == 2\n    assert primeomega(12) == 3\n    assert primeomega(2 * 2 * 3 * 3 * 3 * 5 * 7 * 11 * 13) == 8\n    assert primeomega(fac(50)) == 49\n    assert primeomega(2 ** 9941 - 1) == 9940\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2 ** 31 - 1) == 30\n    assert summation(primeomega(n), (n, 2, 30)) == 27\n", "def test_primeomega():\n    assert primeomega(1) == 0\n    assert primeomega(2) == 1\n    assert primeomega(3) == 1\n    assert primeomega(4) == 2\n    assert primeomega(5) == 1\n    assert primeomega(6) == 2\n    assert primeomega(7) == 1\n    assert primeomega(2 * 3 * 5) == 3\n    assert primeomega(3 * 25) == primeomega(3) + primeomega(25)\n    assert [primeomega(p) for p in primerange(1, 10)] == [0, 1, 1, 2, 1, 1, 1, 1, 1]\n    assert primeomega(fac(50)) == 71\n    assert primeomega(2 ** 9941 - 1) == 1\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2 ** 31 - 1) == 1\n    assert summation(primeomega(n), (n, 2, 30)) == 43\n", "def test_primeomega():\n    assert primeomega(1) == 0\n    assert primeomega(2) == 1\n    assert primeomega(2*3) == 2\n    assert primeomega(2*2*3*5) == 5\n    assert primeomega(2**10) == 10\n    assert primeomega(2**10*3**5) == 15\n    assert primeomega(10**10) == primeomega(2**7 * 5**10)\n    assert primeomega(fac(50)) == 78\n    assert primeomega(2**9941 - 1) == 9941\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2**31 - 1) == 3046\n    assert summation(primeomega(n), (n, 2, 30)) == 45\n"], "sample_621": ["def test_copy_indexes(self, indexes) -> None:\n    new_indexes, new_index_vars = indexes.copy_indexes()\n    assert new_indexes is not indexes._indexes\n    assert new_index_vars is not indexes._variables\n    for k in indexes._indexes:\n        assert new_indexes[k] is not indexes._indexes[k]\n        assert new_indexes[k].equals(indexes._indexes[k])\n        assert new_index_vars[k] is not indexes._variables[k]\n        assert new_index_vars[k].equals(indexes._variables[k])\n", "def test_copy_indexes(self, indexes) -> None:\n    new_indexes, new_index_vars = indexes.copy_indexes()\n    assert set(new_indexes.keys()) == set(indexes.keys())\n    assert set(new_index_vars.keys()) == set(indexes.variables.keys())\n    for key in new_indexes:\n        assert new_indexes[key] is not indexes[key]\n    for key in new_index_vars:\n        assert new_index_vars[key].equals(indexes.variables[key])\n", "def test_copy_indexes(self, indexes) -> None:\n    new_indexes, new_index_vars = indexes.copy_indexes(deep=True)\n    assert all(id(new_indexes[k]) != id(indexes[k]) for k in indexes)\n    assert all(new_indexes[k].equals(indexes[k]) for k in indexes)\n    assert all(new_index_vars[k].equals(indexes.variables[k]) for k in new_index_vars)\n\n    shallow_indexes, shallow_index_vars = indexes.copy_indexes(deep=False)\n    assert all(shallow_indexes[k] is indexes[k] for k in indexes)\n    assert all(shallow_index_vars[k] is indexes.variables[k] for k in shallow_index_vars)\n", "def test_copy_indexes(self, indexes) -> None:\n    new_indexes, new_index_variables = indexes.copy_indexes()\n\n    # Check that new indexes are deep copies\n    for key, new_idx in new_indexes.items():\n        assert new_idx is not indexes[key]\n        if isinstance(indexes[key], pd.MultiIndex):\n            assert not new_idx.equals(indexes[key])\n        else:\n            assert not new_idx.equals(indexes[key].index)\n\n    # Check that new index variables are equal to the original variables\n    for key, new_var in new_index_variables.items():\n        assert new_var.equals(indexes.variables[key])\n", "def test_copy_indexes(self, indexes) -> None:\n    new_indexes, new_index_vars = indexes.copy_indexes()\n\n    # Check that the new indexes are not the same as the original ones\n    assert new_indexes is not indexes._indexes\n    assert new_index_vars is not indexes._variables\n\n    # Check that the new indexes are equal to the original ones\n    for key in indexes._indexes:\n        assert indexes._indexes[key].equals(new_indexes[key])\n\n    # Check that the new index variables are equal to the original ones\n    for key in indexes._variables:\n        assert indexes._variables[key].equals(new_index_vars[key])\n\n    # Check that the copy is deep by default\n    new_indexes_shallow, new_index_vars_shallow = indexes.copy_indexes(deep=False)\n    for key in indexes._indexes:\n        if isinstance(indexes._indexes[key], PandasMultiIndex):\n            assert new_indexes_shallow[key].index is not indexes._indexes[key].index\n        else:\n            assert new_indexes_shallow[key] is indexes._indexes[key]\n\n    for key in indexes._variables:\n        assert new_index_vars_shallow[key] is indexes._variables[key]\n"], "sample_476": ["    def setUp(self):\n        super().setUp()\n        self.PersonModel = type(\n            \"PersonNull\",\n            (models.Model,),\n            {\n                \"mugshot\": ImageField(upload_to=\"test_images\", null=True),\n                \"mugshot_width\": models.PositiveIntegerField(null=True),\n                \"mugshot_height\": models.PositiveIntegerField(null=True),\n            },\n        )\n", "def test_image_resize(self):\n    \"\"\"\n    Tests resizing an image after assignment to an ImageField.\n    \"\"\"\n    p = self.PersonModel(name=\"Joe\")\n    p.mugshot = self.file1\n    p.mugshot.open()\n    p.mugshot.resize((2, 4))\n    p.mugshot.seek(0)\n    p.save()\n\n    # Check dimensions of the resized image\n    p = self.PersonModel.objects.get(name=\"Joe\")\n    self.assertEqual(p.mugshot.width, 2)\n    self.assertEqual(p.mugshot.height, 4)\n", "    def test_null_assignment(self):\n        p = self.PersonModel()\n        self.assertIsNone(p.mugshot)\n\n        p.mugshot = None\n        self.assertIsNone(p.mugshot)\n\n        p.mugshot = self.file1\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n\n        p.mugshot = None\n        self.assertIsNone(p.mugshot)\n        self.assertIsNone(p.mugshot_height)\n", "    def test_save_method(self):\n        p = self.PersonModel(name=\"Joe\")\n        p.mugshot.save(\"mug\", self.file1, save=False)\n        self.assertIs(p._state.adding, True)\n        self.assertIs(p.mugshot._committed, False)\n\n        # Calling save on the model instance should commit the file.\n        p.save()\n        self.assertIs(p.mugshot._committed, True)\n\n        # Reassigning the file should commit it.\n        p.mugshot = self.file2\n        self.assertIs(p.mugshot._committed, False)\n        p.save()\n        self.assertIs(p.mugshot._committed, True)\n\n        # Assigning None should not commit the file.\n        p.mugshot = None\n        p.save()\n        self.assertIs(p.mugshot._committed, False)\n", "    def test_save_and_load(self):\n        p = self.PersonModel(name=\"Alice\")\n        p.mugshot.save(\"shot\", self.file1)\n        p.save()\n\n        # Retrieve the object from the database\n        p_loaded = self.PersonModel.objects.get(name=\"Alice\")\n\n        # Check that the image was saved and loaded correctly\n        self.assertEqual(p_loaded.mugshot.read(), self.file1.read())\n        self.assertEqual(p_loaded.mugshot.name, \"shot\")\n        self.check_dimensions(p_loaded, 4, 8)\n"], "sample_215": ["def test_sensitive_variables_with_non_sensitive_vars(self):\n    \"\"\"\n    Sensitive variables don't leak in the sensitive_variables decorator's\n    frame, when the decorated function doesn't define the sensitive variables.\n    \"\"\"\n    with self.settings(DEBUG=True):\n        self.verify_unsafe_response(sensitive_view)\n        self.verify_unsafe_email(sensitive_view)\n\n    with self.settings(DEBUG=False):\n        self.verify_safe_response(sensitive_view)\n        self.verify_safe_email(sensitive_view)\n", "def test_traceback_hide(self):\n    \"\"\"\n    Test that traceback frames with __traceback_hide__ in locals are hidden\n    \"\"\"\n    try:\n            import django.utils.traceback\n            frame = sys._getframe()\n            frame.f_locals['__traceback_hide__'] = True\n            raise ValueError(\"Hidden traceback\")\n        func_with_hide_traceback()\n    except ValueError:\n        exc_type, exc_value, tb = sys.exc_info()\n    reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n    html = reporter.get_traceback_html()\n    self.assertNotIn(\"func_with_hide_traceback\", html)\n    text = reporter.get_traceback_text()\n    self.assertNotIn(\"func_with_hide_traceback\", text)\n", "    def test_file_encoding(self):\n        \"\"\"\n        The source code of files is loaded with the encoding specified in the\n        file.\n        \"\"\"\n        source = \"def funcName():\\n    raise Error('Whoops')\\nfuncName()\"\n        filename = 'generated_file.py'\n        with open(filename, 'w', encoding='utf-8') as f:\n            f.write(source)\n\n        try:\n            with open(filename, 'r', encoding='utf-8') as f:\n                code = compile(f.read(), filename, 'exec')\n            exec(code, {})\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        finally:\n            os.remove(filename)\n\n        request = self.rf.get('/test_view/')\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n        last_frame = frames[-1]\n        self.assertEqual(last_frame['context_line'], '    raise Error(\\'Whoops\\')')\n        self.assertEqual(last_frame['filename'], filename)\n        self.assertEqual(last_frame['function'], 'funcName')\n        self.assertEqual(last_frame['lineno'], 2)\n", "def test_sensitive_variables_wrapper_function(self):\n    \"\"\"\n    The sensitive_variables decorator works with functions that return another\n    function, and that returned function is the one that gets decorated.\n    \"\"\"\n    with self.settings(DEBUG=True):\n        self.verify_unsafe_response(sensitive_function_wrapper, check_for_POST_params=False)\n        self.verify_unsafe_email(sensitive_function_wrapper, check_for_POST_params=False)\n\n    with self.settings(DEBUG=False):\n        self.verify_safe_response(sensitive_function_wrapper, check_for_POST_params=False)\n        self.verify_safe_email(sensitive_function_wrapper, check_for_POST_params=False)\n", "def test_settings_with_sensitive_keys_in_nested_dicts(self):\n    \"\"\"\n    The debug page should filter out some sensitive information found in\n    nested dict settings.\n    \"\"\"\n    sensitive_settings = [\n        'SECRET_KEY',\n        'PASSWORD',\n        'API_KEY',\n        'AUTH_TOKEN',\n    ]\n    for setting in sensitive_settings:\n        FOOBAR = {\n            'nested': {\n                setting: \"should not be displayed\",\n            },\n            'recursive': {\n                'nested': {setting: \"should not be displayed\"},\n            },\n        }\n        with self.settings(DEBUG=True, FOOBAR=FOOBAR):\n            response = self.client.get('/raises500/')\n            self.assertNotContains(response, 'should not be displayed', status_code=500)\n"], "sample_708": ["def test_getstatementrange_with_syntaxerror_issue7_part2() -> None:\n    source = Source(\"pass\\n:\")\n    pytest.raises(SyntaxError, lambda: source.getstatementrange(1))\n", "def test_getstatementrange_for_loop():\n    source = Source(\n        \"\"\"\\\n        for i in range(10):\n            print(i)\n        \"\"\"\n    )\n    ast, start, end = getstatementrange_ast(1, source)\n    assert start == 0\n    assert end == 2\n", "def test_getstatementrange_empty_lines():\n    source = Source(\n        \"\"\"\\\n\n\n                pass\n\n            x = 3\n        \"\"\"\n    )\n    assert getstatement(2, source).lines == source.lines[2:5]\n", "def test_getstatementrange_ast_issue58_with_indent() -> None:\n    source = Source(\n        \"\"\"\n                for a in [a for a in\n                    CAUSE_ERROR]: pass\n\n            x = 3\n        \"\"\"\n    )\n    assert getstatement(3, source).lines == source.lines[3:4]\n", "def test_source_with_decorators_and_docstring() -> None:\n    \"\"\"Test behavior with Source / Code().source with regard to decorators and docstrings.\"\"\"\n    from _pytest.compat import get_real_func\n\n    @pytest.mark.foo\n        \"\"\"Docstring.\"\"\"\n        assert False\n\n    src = inspect.getsource(deco_mark)\n    assert textwrap.indent(str(Source(deco_mark)), \"    \") + \"\\n\" == src\n    assert src.startswith('    \"\"\"Docstring.')\n    assert src.startswith('    @pytest.mark.foo')\n\n    @pytest.fixture\n        \"\"\"Docstring.\"\"\"\n        assert False\n\n    src = inspect.getsource(deco_fixture)\n    assert src.startswith('    \"\"\"Docstring.')\n    assert src.startswith('    @pytest.fixture')\n    assert str(Source(deco_fixture)).startswith(\"@functools.wraps(function)\")\n    assert (\n        textwrap.indent(str(Source(get_real_func(deco_fixture))), \"    \") + \"\\n\" == src\n    )\n"], "sample_134": ["def test_serialize_complex_numbers(self):\n    complex_number = 1 + 2j\n    self.assertSerializedEqual(complex_number)\n    self.assertSerializedResultEqual(\n        complex_number,\n        (\"complex('1+2j')\", set())\n    )\n", "def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex((1+2j))\", set())\n    )\n\n    field = models.ComplexField(default=complex(3, 4))\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.ComplexField(default=complex((3+4j)))\"\n    )\n", "def test_serialize_deconstructible(self):\n    class TestDeconstructible:\n            self.value = value\n\n            return (\n                'migrations.test_writer.TestDeconstructible',\n                [self.value],\n                {},\n            )\n\n    self.assertSerializedResultEqual(\n        TestDeconstructible('test'),\n        (\"migrations.test_writer.TestDeconstructible('test')\", {'import migrations.test_writer'}),\n    )\n", "def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex((1+2j))\", set())\n    )\n\n    field = models.BinaryField(default=complex(3, 4))\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.BinaryField(default=complex((3+4j)))\"\n    )\n", "def test_serialize_custom_serializer(self):\n    \"\"\"\n    Test a custom serializer.\n    \"\"\"\n    class CustomType:\n            self.value = value\n\n    class CustomSerializer(BaseSerializer):\n            return f'CustomType({self.value})', {'import migrations.test_writer'}\n\n    Serializer.register(CustomType, CustomSerializer)\n\n    custom_obj = CustomType(42)\n    string, imports = MigrationWriter.serialize(custom_obj)\n    self.assertEqual(string, 'CustomType(42)')\n    self.assertEqual(imports, {'import migrations.test_writer'})\n\n    Serializer.unregister(CustomType)\n"], "sample_249": ["    def test_migrate_test_setting_true_migrated(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Migrations run only for new migrations.\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_migrated', '0002_new_migration')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n            # App is not synced.\n            mocked_sync_apps.assert_not_called()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_serialize_db_to_string_non_migrated_app(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n            with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n                # serialize_db_to_string() serializes only migrated apps, so mark\n                # the unmigrated app as not migrated.\n                loader_instance = loader.return_value\n                loader_instance.migrated_apps = {}\n                data = creation.serialize_db_to_string()\n            # The data should be an empty list since the app is not migrated.\n            self.assertEqual(data, \"[]\")\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_serialize_deserialization(self, mocked_sync_apps, *mocked_objects):\n        # serialize_db_to_string() and deserialize_db_from_string() correctly\n        # serializes and deserializes data in the database.\n        obj_1 = Object.objects.create(key='X')\n        obj_2 = Object.objects.create(key='Y', obj_ref=ObjectReference.objects.create(obj=obj_1))\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            # serialize_db_to_string() serializes only migrated apps, so mark\n            # the backends app as migrated.\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n        # Deserialize objects.\n        connection.creation.deserialize_db_from_string(data)\n        obj_1_deserialized = Object.objects.get(key='X')\n        obj_2_deserialized = Object.objects.get(key='Y')\n        self.assertEqual(obj_1_deserialized.obj_ref.obj, obj_2_deserialized)\n", "    def test_no_custom_test_db_name(self):\n        # _get_test_db_name() returns the name of the test DB with the prefix\n        # when no custom test DB name is set.\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['NAME'] = None\n        test_db_name = test_connection.creation_class(test_connection)._get_test_db_name()\n        self.assertEqual(test_db_name, TEST_DATABASE_PREFIX + ':memory:')\n", "    def test_migrate_with_migrated_apps(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Migrations are not run again.\n            mocked_migrate.assert_not_called()\n            # App is not synced.\n            mocked_sync_apps.assert_not_called()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n"], "sample_1192": ["def test_uniquely_named_symbol_with_exprs():\n    F = uniquely_named_symbol\n    x = Symbol('x')\n    y = Symbol('y')\n    exprs = (x + y, x - y)\n    assert str(F('x', exprs)) == 'x0'\n    assert str(F('y', exprs)) == 'y0'\n    assert str(F('z', exprs)) == 'z'\n", "def test_uniquely_named_symbol_with_different_assumptions():\n    x = Symbol('x', real=True)\n    y = Symbol('x', integer=True)\n    assert uniquely_named_symbol('x', (x, y)) != x\n    assert uniquely_named_symbol('x', (x, y)).is_real is None\n    assert uniquely_named_symbol('x', (x, y), real=True).is_real\n    assert uniquely_named_symbol('x', (x, y), integer=True).is_integer\n", "def test_disambiguate():\n    x = Symbol('x')\n    dx = Dummy('x')\n    y = Symbol('y')\n    assert disambiguate(x, dx, y) == (Symbol('x_2'), Symbol('x'), y)\n\n    eqs = Tuple(x / y, dx / y)\n    assert disambiguate(*eqs) == (Symbol('x_1') / y, x / y)\n\n    ix = Symbol('x', integer=True)\n    assert disambiguate(x + ix) == (Symbol('x_1'), ix)\n\n    free = eqs.free_symbols\n    mapping = dict(zip(free, disambiguate(*free)))\n    assert eqs.xreplace(mapping) == (Symbol('x_1') / Symbol('y'), x / Symbol('y'))\n", "def test_disambiguate():\n    x, y = symbols('x, y')\n    d1, d2 = symbols('d:2', cls=Dummy)\n\n    assert disambiguate(x + d1) == (x + d2,)\n    assert disambiguate(x + d1, d1 / y) == (x + d2, d2 / y)\n    assert disambiguate(x + d1, x + d2) == (x + d2, x + d1)\n\n    # Test with symbols that appear the same when printed\n    ix = Symbol('x', integer=True)\n    vx = Symbol('x')\n    assert disambiguate(vx + ix) == (vx + ix.subs(x, Symbol('x_1')),)\n\n    # Test with expressions containing multiple symbols\n    eqs = (Symbol('x') / y, Dummy('x') / y)\n    assert disambiguate(*eqs) == (Symbol('x_1') / y, Symbol('x') / y)\n", "def test_symbol_comparison():\n    a = Symbol(\"a\", real=True)\n    b = Symbol(\"b\", real=False)\n    c = Symbol(\"c\")\n    d = Symbol(\"a\", real=True)\n\n    assert a == d\n    assert a != b\n    assert a != c\n    assert b != c\n    assert a != \"a\"\n    assert a != 1\n"], "sample_1112": ["def test_is_palindromic():\n    assert is_palindromic(121) == True\n    assert is_palindromic(12321) == True\n    assert is_palindromic(12345) == False\n    assert is_palindromic(123321, 10) == True\n    assert is_palindromic(121, 2) == False\n", "def test_is_palindromic():\n    assert is_palindromic(12321) == True\n    assert is_palindromic(12322) == False\n    assert is_palindromic(0xFA, 16) == False\n    assert is_palindromic(0o121, 8) == False\n    assert is_palindromic(121, 8) == True\n", "def test_is_palindromic():\n    assert is_palindromic(121) == True\n    assert is_palindromic(12321) == True\n    assert is_palindromic(12345) == False\n    assert is_palindromic(1234321) == True\n    assert is_palindromic(12321, 2) == True\n    assert is_palindromic(12321, 8) == False\n    assert is_palindromic(81, 8) == True\n    assert is_palindromic(121, 8) == True\n", "def test_is_palindromic():\n    assert is_palindromic(12321, 10) == True\n    assert is_palindromic(12345, 10) == False\n    assert is_palindromic(2552, 10) == True\n    assert is_palindromic(121, 16) == False\n    assert is_palindromic(12321, 2) == False\n    assert is_palindromic(0b101010101, 2) == True\n", "def test_is_palindromic():\n    assert is_palindromic(121)\n    assert not is_palindromic(123)\n    assert is_palindromic(121, 8)\n    assert not is_palindromic(123, 8)\n    assert is_palindromic(0o121, 8)\n    assert not is_palindromic(0o123, 8)\n"], "sample_1000": ["def test_octave_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == 'existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_octave_user_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"my_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n           'my_octave_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_octave_user_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        'custom_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_octave_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"mat_g\"),\n              (lambda x: not x.is_Matrix, \"scalar_g\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == 'custom_f(x) + scalar_g(x) + mat_g([1 x])'\n", "def test_lambertw_with_symbols():\n    assert mcode(LambertW(x, y)) == 'lambertw(y, x)'\n"], "sample_1001": ["def test_issue_14237_expanded():\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    C = MatrixSymbol(\"C\", 3, 3)\n\n    assert latex(A + B) == r\"A + B\"\n    assert latex(A - B) == r\"A - B\"\n    assert latex(A*B) == r\"A B\"\n    assert latex(A*B*C) == r\"A B C\"\n", "def test_issue_13765():\n    from sympy.physics.units import kilogram, meter\n    expr = kilogram * meter\n    assert latex(expr) == r\"\\detokenize {kilogram} \\cdot \\detokenize {meter}\"\n", "def test_issue_13689():\n    x, y, z = symbols('x y z')\n    expr = sqrt(x**2 * y**2 + z**2)\n    assert latex(expr) == r\"\\sqrt{\\left(x^{2} y^{2} + z^{2}\\right)}\"\n", "def test_issue_14649():\n    from sympy.physics.units import joule, newton, meter\n    F = newton\n    x = meter\n    W = joule\n    expr = F*x\n    assert latex(expr) == r\"\\detokenize {newton} \\cdot \\detokenize {meter}\"\n    expr = W/x\n    assert latex(expr) == r\"\\frac{\\detokenize {joule}}{\\detokenize {meter}}\"\n", "def test_LeviCivita_printing():\n    assert latex(LeviCivita(x, y, z)) == r\"\\varepsilon_{x y z}\"\n    assert latex(LeviCivita(x, y, z)**2) == r\"\\left(\\varepsilon_{x y z}\\right)^{2}\"\n    assert latex(LeviCivita(x, y, z + 1)) == r\"\\varepsilon_{x, y, z + 1}\"\n    assert latex(LeviCivita(x, y + 1, z)) == r\"\\varepsilon_{x, y + 1, z}\"\n    assert latex(LeviCivita(x + 1, y, z)) == r\"\\varepsilon_{x + 1, y, z}\"\n"], "sample_870": ["def test_custom_kernel_diag_no_mutation():\n    \"\"\"\n    Check that the input matrix X is not mutated by the custom kernel's diag method.\n    \"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n    kernel = CustomKernel()\n    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)\n    X_copy = X.copy()\n    gpr.kernel_.diag(X)\n    assert_array_equal(X, X_copy)\n", "def test_diag_method():\n    \"\"\"\n    Test that the diag method of the custom kernel returns the first column of the\n    input matrix X and does not mutate X.\n    \"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n\n    kernel = CustomKernel()\n    gpr = GaussianProcessRegressor(kernel=kernel)\n    gpr.fit(X, y)\n\n    X_copy = np.copy(X)\n    diag_values = gpr.kernel_.diag(X)\n\n    assert_array_equal(diag_values, X_copy[:, 0])\n    assert_array_equal(X, X_copy)\n", "def test_custom_kernel_diag_does_not_mutate_input():\n    \"\"\"\n    Check that the input matrix X is not mutated by the custom kernel's diag method.\n    \"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10, 1)\n\n    kernel = CustomKernel(1.0, constant_value_bounds=\"fixed\")\n    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)\n\n    X_copy = X.copy()\n    gpr.kernel_.diag(X_copy)\n    assert_array_equal(X, X_copy)\n", "def test_diag_no_mutation():\n    \"\"\"\n    Check that the diag method does not mutate the input matrix X.\n    This is important for the correctness of the predict method.\n    \"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n\n    model = GaussianProcessRegressor(kernel=CustomKernel())\n    model.fit(X, y)\n\n    X_copy = np.copy(X)\n    model.kernel_.diag(X)\n\n    assert_array_equal(X, X_copy)\n", "def test_custom_kernel_diag():\n    \"\"\"Check that a custom kernel's diag method does not mutate the input matrix X.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n\n    model = GaussianProcessRegressor(kernel=CustomKernel())\n    model.fit(X, y)\n\n    X_copy = np.copy(X)\n    model.predict(X, return_std=True)\n    assert_array_equal(X, X_copy)\n"], "sample_1186": ["def test_array_addition():\n    for ArrayType in array_types:\n        a = ArrayType([1, 2, 3], (3,))\n        b = ArrayType([4, 5, 6], (3,))\n        c = a + b\n        assert c == ArrayType([5, 7, 9], (3,))\n", "def test_array_iteration():\n    for ArrayType in array_types:\n        test_array = ArrayType([1, 2, 3, 4, 5])\n        iterator = iter(test_array)\n        assert next(iterator) == 1\n        assert next(iterator) == 2\n        assert next(iterator) == 3\n        assert next(iterator) == 4\n        assert next(iterator) == 5\n        raises(StopIteration, lambda: next(iterator))\n", "def test_array_operations():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2], [3, 4]])\n        B = ArrayType([[5, 6], [7, 8]])\n\n        assert A + B == ArrayType([[6, 8], [10, 12]])\n        assert A - B == ArrayType([[-4, -4], [-4, -4]])\n        assert A * 2 == ArrayType([[2, 4], [6, 8]])\n        assert 2 * A == ArrayType([[2, 4], [6, 8]])\n        assert A / 2 == ArrayType([[0.5, 1], [1.5, 2]])\n\n        raises(ValueError, lambda: A + ArrayType([[1, 2], [3, 4, 5]]))\n        raises(NotImplementedError, lambda: A + [1, 2, 3, 4])\n        raises(NotImplementedError, lambda: A * ArrayType([[1, 2], [3, 4]]))\n        raises(NotImplementedError, lambda: A * [1, 2, 3, 4])\n        raises(NotImplementedError, lambda: A / 2)\n", "def test_array_slicing():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n        assert test_array[0, :] == Array([1, 2, 3, 4, 5])\n        assert test_array[:, 0] == Array([1, 6])\n        assert test_array[0, 1:4] == Array([2, 3, 4])\n        assert test_array[1, 2:] == Array([8, 9, 10])\n", "def test_issue_19876():\n    for array_type in array_types:\n        A = array_type([1, 2, 3, 4], (2, 2))\n        assert A.tolist() == [[1, 2], [3, 4]]\n\n        B = array_type([[1, 2], [3, 4]])\n        assert B.tolist() == [[1, 2], [3, 4]]\n"], "sample_366": ["def test_parse_duration_iso8601(self):\n    test_values = (\n        ('P1D', timedelta(days=1)),\n        ('P1DT1H', timedelta(days=1, hours=1)),\n        ('P1DT1H1M', timedelta(days=1, hours=1, minutes=1)),\n        ('P1DT1H1M1S', timedelta(days=1, hours=1, minutes=1, seconds=1)),\n        ('P1DT1H1M1.1S', timedelta(days=1, hours=1, minutes=1, seconds=1, milliseconds=100)),\n        ('P1DT1H1M1,1S', timedelta(days=1, hours=1, minutes=1, seconds=1, milliseconds=100)),\n        ('-P1D', timedelta(days=-1)),\n        ('-P1DT1H', timedelta(days=-1, hours=-1)),\n        ('-P1DT1H1M', timedelta(days=-1, hours=-1, minutes=-1)),\n        ('-P1DT1H1M1S', timedelta(days=-1, hours=-1, minutes=-1, seconds=-1)),\n        ('-P1DT1H1M1.1S', timedelta(days=-1, hours=-1, minutes=-1, seconds=-1, milliseconds=-100)),\n        ('-P1DT1H1M1,1S', timedelta(days=-1, hours=-1, minutes=-1, seconds=-1, milliseconds=-100)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_datetime_invalid_tzinfo(self):\n        # Invalid timezone offset\n        self.assertIsNone(parse_datetime('2012-04-23T10:20:30Z05:00'))\n        self.assertIsNone(parse_datetime('2012-04-23T10:20:30+25:00'))\n        self.assertIsNone(parse_datetime('2012-04-23T10:20:30-25:00'))\n", "    def test_parse_datetime_with_microseconds(self):\n        # Test datetime parsing with microseconds\n        self.assertEqual(parse_datetime('2012-04-23T10:20:30.123456-02'), datetime(2012, 4, 23, 10, 20, 30, 123456, get_fixed_timezone(-120)))\n        self.assertEqual(parse_datetime('2012-04-23T10:20:30.123456 +0230'), datetime(2012, 4, 23, 10, 20, 30, 123456, get_fixed_timezone(150)))\n        self.assertEqual(parse_datetime('2012-04-23T10:20:30.123456Z'), datetime(2012, 4, 23, 10, 20, 30, 123456, get_fixed_timezone(0)))\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P1Y2M3DT4H5M6S', timedelta(days=757, hours=4, minutes=5, seconds=6)),\n            ('P1M2DT3H4M5S', timedelta(days=32, hours=3, minutes=4, seconds=5)),\n            ('P1Y2M3D', timedelta(days=424)),\n            ('P1Y2M', timedelta(days=365 * 1 + 30 * 2)),\n            ('P1Y', timedelta(days=365)),\n            ('P1M', timedelta(days=30)),\n            ('P1DT2H', timedelta(days=1, hours=2)),\n            ('PT2H', timedelta(hours=2)),\n            ('PT2H3M', timedelta(hours=2, minutes=3)),\n            ('PT2H3M4S', timedelta(hours=2, minutes=3, seconds=4)),\n            ('P1YT2H3M4S', timedelta(days=365, hours=2, minutes=3, seconds=4)),\n            ('-P1Y2M3DT4H5M6S', timedelta(days=-757, hours=-4, minutes=-5, seconds=-6)),\n            ('-P1M2DT3H4M5S', timedelta(days=-32, hours=-3, minutes=-4, seconds=-5)),\n            ('-P1Y2M3D', timedelta(days=-424)),\n            ('-P1Y2M', timedelta(days=-365 * 1 - 30 * 2)),\n            ('-P1Y', timedelta(days=-365)),\n            ('-P1M', timedelta(days=-30)),\n            ('-PT2H', timedelta(hours=-2)),\n            ('-PT2H3M', timedelta(hours=-2, minutes=-3)),\n", "    def test_parse_duration_iso8601_format(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('P1DT2H3M4.5S', timedelta(days=1, hours=2, minutes=3, seconds=4, milliseconds=500)),\n            ('P1YT2M3S', None),  # Unsupported: year-month intervals\n            ('P1MT2S', None),  # Unsupported: month-day intervals\n            ('-P1D', timedelta(days=-1)),\n            ('-P1DT2H3M4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_251": ["def test_alias_annotation_with_m2m(self):\n    qs = Book.objects.alias(\n        author_age_alias=F('authors__age'),\n    ).annotate(author_age=F('author_age_alias')).filter(pk=self.b1.pk).order_by('author_age')\n    self.assertIs(hasattr(qs.first(), 'author_age_alias'), False)\n    self.assertEqual(qs[0].author_age, 34)\n    self.assertEqual(qs[1].author_age, 35)\n", "def test_aggregate_filter_alias(self):\n    qs = Book.objects.alias(\n        sum_pages=Sum('pages'),\n    ).filter(sum_pages__gt=1000)\n    self.assertIs(hasattr(qs.first(), 'sum_pages'), False)\n    self.assertCountEqual(qs, [self.b3, self.b4])\n", "def test_annotation_with_m2m_through_field(self):\n    qs = Book.objects.annotate(\n        author_age=F('authors__age'),\n        author_name=F('authors__name'),\n        author_count=Count('authors'),\n    ).filter(pk=self.b1.pk).order_by('author_age')\n    self.assertEqual(qs[0].author_age, 34)\n    self.assertEqual(qs[0].author_name, 'Adrian Holovaty')\n    self.assertEqual(qs[0].author_count, 2)\n    self.assertEqual(qs[1].author_age, 35)\n    self.assertEqual(qs[1].author_name, 'Jacob Kaplan-Moss')\n    self.assertEqual(qs[1].author_count, 2)\n", "def test_alias_with_m2m(self):\n    qs = Book.objects.alias(\n        author_age=F('authors__age'),\n    ).filter(pk=self.b1.pk).order_by('author_age')\n    self.assertEqual(qs[0].author_age, 34)\n    self.assertEqual(qs[1].author_age, 35)\n", "def test_annotate_with_m2m_and_f_expression(self):\n    qs = Book.objects.annotate(\n        author_age_plus_10=F('authors__age') + 10,\n    ).filter(\n        pk=self.b1.pk,\n    ).order_by('author_age_plus_10')\n    self.assertEqual(qs[0].author_age_plus_10, 44)\n    self.assertEqual(qs[1].author_age_plus_10, 45)\n"], "sample_737": ["def test_vectorizer_ngram_range_validation():\n    with pytest.raises(ValueError):\n        CountVectorizer(ngram_range=(2, 1))\n", "def test_countvectorizer_with_max_df_lt_min_df():\n    try:\n        CountVectorizer(max_df=0.5, min_df=0.6)\n    except ValueError as e:\n        assert_in(\"max_df corresponds to < documents than min_df\", str(e).lower())\n", "def test_vectorizer_custom_analyzer():\n    # Test CountVectorizer with a custom analyzer\n    analyzer = lambda doc: doc.split(' ')\n    cv = CountVectorizer(analyzer=analyzer)\n    X = cv.fit_transform(ALL_FOOD_DOCS)\n    expected_vocabulary = {\"pizza\": 0, \"beer\": 1, \"the\": 2, \"coke\": 3, \"burger\": 4, \"salad\": 5, \"water\": 6, \"celeri\": 7, \"tomato\": 8, \"sparkling\": 9, \"copyright\": 10}\n    assert_equal(cv.vocabulary_, expected_vocabulary)\n    assert_equal(X.shape, (len(ALL_FOOD_DOCS), len(expected_vocabulary)))\n\n    # Test TfidfVectorizer with a custom analyzer\n    tfidf = TfidfVectorizer(analyzer=analyzer)\n    X_tfidf = tfidf.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(tfidf.vocabulary_, expected_vocabulary)\n    assert_equal(X_tfidf.shape, (len(ALL_FOOD_DOCS), len(expected_vocabulary)))\n", "def test_vectorizer_invalid_params():\n    # Test invalid values for max_df and min_df\n    assert_raises(ValueError, CountVectorizer, max_df=-0.5)\n    assert_raises(ValueError, CountVectorizer, min_df=-1)\n\n    # Test invalid values for max_features\n    assert_raises(ValueError, CountVectorizer, max_features=\"invalid\")\n    assert_raises(ValueError, CountVectorizer, max_features=0)\n\n    # Test invalid value for ngram_range\n    assert_raises(ValueError, CountVectorizer, ngram_range=(2, 1))\n", "def test_vectorizer_vocab_updates():\n    vect_no_vocab = TfidfVectorizer()\n    vect_no_vocab.fit(ALL_FOOD_DOCS)\n    vocab_size_no_vocab = len(vect_no_vocab.vocabulary_)\n\n    vect_vocab = TfidfVectorizer(vocabulary=[\"pizza\", \"burger\"])\n    vect_vocab.fit(ALL_FOOD_DOCS)\n    vocab_size_vocab = len(vect_vocab.vocabulary_)\n\n    assert_not_equal(vocab_size_no_vocab, vocab_size_vocab)\n    assert_true(vocab_size_no_vocab > vocab_size_vocab)\n\n    vect_vocab.vocabulary_.update({\"coke\": len(vect_vocab.vocabulary_)})\n    vect_vocab.fit(ALL_FOOD_DOCS)\n    updated_vocab_size_vocab = len(vect_vocab.vocabulary_)\n\n    assert_equal(updated_vocab_size_vocab, vocab_size_no_vocab)\n"], "sample_26": ["def test_image_write_readonly_compressed(tmp_path):\n    # Test to make sure that we can write out read-only arrays for compressed images\n\n    x = np.array([1.0, 2.0, 3.0])\n    x.setflags(write=False)\n    ghdu = fits.CompImageHDU(data=x)\n\n    filename = tmp_path / \"test3.fits\"\n\n    ghdu.writeto(filename)\n\n    with fits.open(filename) as hdulist:\n        assert_equal(hdulist[1].data, [1.0, 2.0, 3.0])\n", "def test_uint_header_consistency_64bit(self):\n    \"\"\"\n    Test the uint_header_consistency for 64-bit unsigned integers.\n    \"\"\"\n    arr = np.zeros(100, dtype=\"uint64\")\n    arr.fill(18446744073709551615)\n    arr -= np.arange(100, dtype=\"uint64\")\n\n    uint_hdu = fits.PrimaryHDU(data=arr)\n    assert np.all(uint_hdu.data == arr)\n    assert uint_hdu.data.dtype.name == \"uint64\"\n    assert \"BZERO\" in uint_hdu.header\n    assert uint_hdu.header[\"BZERO\"] == 9223372036854775808\n\n    filename = \"uint64.fits\"\n    uint_hdu.writeto(self.temp(filename))\n\n    with fits.open(self.temp(filename), uint=True) as hdul:\n        new_uint_hdu = hdul[0]\n        assert np.all(new_uint_hdu.data == arr)\n        assert new_uint_hdu.data.dtype.name == \"uint64\"\n        assert \"BZERO\" in new_uint_hdu.header\n        assert new_uint_hdu.header[\"BZERO\"] == 9223372036854775808\n", "def test_image_update_header_with_section(self):\n    \"\"\"\n    Regression test for an issue where updating the header of an image HDU with a section\n    would result in incorrect data being written to the file.\n    \"\"\"\n    data = np.arange(100).reshape(10, 10)\n    hdu = fits.ImageHDU(data)\n    hdu.header[\"TEST\"] = 1\n    hdu.data[5:10, 5:10] = 10\n    hdu.header[\"TEST\"] = 2\n    hdu.writeto(self.temp(\"test.fits\"))\n\n    with fits.open(self.temp(\"test.fits\")) as hdul:\n        assert hdul[0].header[\"TEST\"] == 2\n        assert np.all(hdul[0].data[5:10, 5:10] == 10)\n", "def test_image_header_access(self):\n    # Test that we can access the header attribute of ImageHDU\n    hdu = fits.ImageHDU()\n    header = hdu.header\n    assert isinstance(header, fits.Header)\n", "def test_scale_back_compressed_float_assignment(self):\n    \"\"\"\n    Extend fix for #4600 to assignment to data\n\n    Identical to test_scale_back_uint_assignment() but uses a compressed\n    floating-point image.\n\n    Suggested by:\n    https://github.com/astropy/astropy/pull/4602#issuecomment-208713748\n    \"\"\"\n\n    a = np.arange(100, 200, dtype=np.float32)\n    fits.CompImageHDU(a).writeto(self.temp(\"test.fits\"))\n    with fits.open(self.temp(\"test.fits\"), mode=\"update\", scale_back=True) as hdul:\n        hdul[1].data[:] = 0.0\n        assert np.allclose(hdul[1].data, 0.0)\n"], "sample_902": ["def test_pipeline_memory_disabled():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = None\n    transf = DummyTransf()\n    clf = SVC(probability=True, random_state=0)\n    pipe = Pipeline([('transf', transf), ('svc', clf)], memory=cachedir)\n    pipe.fit(X, y)\n    assert_true(hasattr(pipe.named_steps['transf'], 'means_'))\n", "def test_pipeline_memory_with_fit_params():\n    # Test that pipeline passes fit parameters to the transformers when memory is used\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', TransfFitParams()), ('clf', FitParamT())], memory=DummyMemory())\n    pipe.fit(X, y=None, transf__should_get_this=True, clf__should_succeed=True)\n    assert_true(pipe.named_steps['transf'].fit_params['should_get_this'])\n    assert_true(pipe.named_steps['clf'].successful)\n    assert_false('should_succeed' in pipe.named_steps['transf'].fit_params)\n", "def test_pipeline_with_no_cache():\n    # Test that the pipeline works when cache is disabled\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    clf = SVC(probability=True, random_state=0)\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', transf), ('svc', clf)], memory=None)\n    pipe.fit(X, y)\n    assert_true(hasattr(pipe.named_steps['transf'], 'means_'))\n    assert_array_equal(pipe.predict(X), clf.predict(transf.transform(X)))\n", "def test_pipeline_memory_with_string():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        # Test with Transformer + SVC and memory as a string\n        clf = SVC(probability=True, random_state=0)\n        transf = DummyTransf()\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=cachedir)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y)\n        # Get the time stamp of the transformer in the cached pipeline\n        ts = cached_pipe.named_steps['transf'].timestamp_\n        # Check that we are reading the cache while fitting a second time\n        cached_pipe.fit(X, y)\n        # Check that the timestamp is the same, indicating the cache was hit\n        assert_equal(ts, cached_pipe.named_steps['transf'].timestamp_)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_memory_persist():\n    # Test that the memory persists across multiple calls\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        transf = DummyTransf()\n        cached_pipe = Pipeline([('transf', transf), ('svc', SVC())],\n                               memory=memory)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y)\n        ts = cached_pipe.named_steps['transf'].timestamp_\n\n        # Create a new pipeline with the same memory\n        cached_pipe_2 = Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                                 memory=memory)\n\n        # Check that the new pipeline uses the cached transformer\n        cached_pipe_2.fit(X, y)\n        assert_equal(ts, cached_pipe_2.named_steps['transf'].timestamp_)\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_417": ["def test_decimal_input(self):\n    self.assertEqual(floatformat(Decimal(\"1.2345678901234567890123456789\"), 20), \"1.23456789012345678901\")\n", "def test_floatformat_large_number(self):\n    output = self.engine.render_to_string(\n        \"floatformat_large_number\", {\"a\": \"1234567890123456.789\", \"b\": mark_safe(\"1234567890123456.789\")}\n    )\n    self.assertEqual(output, \"1234567890123456.79 1234567890123456.79\")\n", "    def test_decimal_input(self):\n        self.assertEqual(floatformat(Decimal(\"12345.6789\"), 3), \"12345.679\")\n        self.assertEqual(floatformat(Decimal(\"-12345.6789\"), 2), \"-12345.68\")\n        self.assertEqual(floatformat(Decimal(\"1.23456789012345678901\"), 20), \"1.23456789012345678901\")\n", "def test_float_string_conversion(self):\n    self.assertEqual(floatformat(\"1.42\"), \"1.4\")\n    self.assertEqual(floatformat(\"1.400\"), \"1.4\")\n    self.assertEqual(floatformat(\"1.000\"), \"1\")\n    self.assertEqual(floatformat(\"1.000\", 2), \"1.00\")\n    self.assertEqual(floatformat(\"1.000\", -2), \"1.00\")\n    self.assertEqual(floatformat(\"1.2345\", 2), \"1.23\")\n    self.assertEqual(floatformat(\"1.2345\", -2), \"1.23\")\n", "def test_floatformat03(self):\n    with translation.override(\"en\"):\n        output = self.engine.render_to_string(\n            \"floatformat03\", {\"a\": \"6666.6666\", \"b\": \"66666.666\"}\n        )\n        self.assertEqual(output, \"6,666.67 66,666.666\")\n"], "sample_790": ["def test_kernel_pca_inverse_transform_sparse():\n    rng = np.random.RandomState(0)\n    X_fit = sp.csr_matrix(rng.random_sample((5, 4)))\n\n    kpca = KernelPCA(n_components=2, fit_inverse_transform=True)\n    X_transformed = kpca.fit_transform(X_fit)\n    X_reconstructed = kpca.inverse_transform(X_transformed)\n\n    assert_raises(NotImplementedError, X_reconstructed)\n", "def test_kernel_pca_fit_inverse_transform():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n\n    for kernel in (\"linear\", \"rbf\", \"poly\"):\n        kpca = KernelPCA(n_components=4, kernel=kernel, fit_inverse_transform=True)\n        X_fit_transformed = kpca.fit_transform(X_fit)\n        X_fit_inverse = kpca.inverse_transform(X_fit_transformed)\n\n        assert_array_almost_equal(X_fit, X_fit_inverse, decimal=3)\n", "def test_kernel_pca_sparse_inverse_transform():\n    rng = np.random.RandomState(0)\n    X_fit = sp.csr_matrix(rng.random_sample((5, 4)))\n    X_pred = sp.csr_matrix(rng.random_sample((2, 4)))\n\n    for eigen_solver in (\"auto\", \"dense\", \"arpack\"):\n        for kernel in (\"linear\", \"rbf\", \"poly\"):\n            # transform fit data and inverse transform\n            kpca = KernelPCA(4, kernel=kernel, eigen_solver=eigen_solver,\n                             fit_inverse_transform=True)\n            X_fit_transformed = kpca.fit_transform(X_fit)\n            X_fit_inverse = kpca.inverse_transform(X_fit_transformed)\n            assert_equal(X_fit_inverse.shape, X_fit.shape)\n\n            # transform new data and inverse transform\n            X_pred_transformed = kpca.transform(X_pred)\n            X_pred_inverse = kpca.inverse_transform(X_pred_transformed)\n            assert_equal(X_pred_inverse.shape, X_pred.shape)\n", "def test_kernel_pca_fit_inverse_transform():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    kpca = KernelPCA(n_components=2, fit_inverse_transform=True)\n    X_transformed = kpca.fit_transform(X)\n    X_inverse_transformed = kpca.inverse_transform(X_transformed)\n    # Check if the inverse transform approximately returns the original data\n    assert_array_almost_equal(X, X_inverse_transformed, decimal=2)\n", "def test_kernel_pca_gamma_parameter():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    for gamma in [0.1, 1.0, 10.0]:\n        kpca = KernelPCA(kernel=\"rbf\", gamma=gamma)\n        X_transformed = kpca.fit_transform(X_fit)\n        X_pred_transformed = kpca.transform(X_pred)\n\n        # Test that the transformation works with the specified gamma\n        assert X_transformed.shape[1] > 0\n        assert X_pred_transformed.shape[1] > 0\n\n        # Test that the transformation is different for different gamma values\n        if gamma != 0.1:\n            assert not np.allclose(X_transformed, prev_X_transformed)\n        prev_X_transformed = X_transformed\n"], "sample_681": ["def test_logging_emit_error_raised(testdir: Testdir) -> None:\n    \"\"\"\n    If logging is configured to raise errors, pytest\n    propagates errors and fails the test.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            monkeypatch.setattr(logging, 'raiseExceptions', True)\n            with pytest.raises(ValueError):\n                logging.warning('oops', 'first', 2)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_logging_format_multiline(testdir: Testdir) -> None:\n    \"\"\"\n    Test that the logging format supports multiline messages.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(__name__)\n            logger.info('first line\\\\nsecond line')\n            assert caplog.messages == ['first line\\\\nsecond line']\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n", "def test_get_log_level_for_setting(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level = INFO\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.logging import get_log_level_for_setting\n\n            config = request.config\n            assert get_log_level_for_setting(config, \"log_level\") == 20\n\n            config.option.log_level = \"WARNING\"\n            assert get_log_level_for_setting(config, \"log_level\") == 30\n\n            with pytest.raises(pytest.UsageError):\n                config.option.log_level = \"INVALID\"\n                get_log_level_for_setting(config, \"log_level\")\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_log_file_abs_path(testdir):\n    log_file = os.path.join(testdir.tmpdir.strpath, \"pytest.log\")\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level=WARNING\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_handler.baseFilename == log_file\n            logging.getLogger('catchlog').info(\"This log message won't be shown\")\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"test_log_file_abs_path.py PASSED\"])\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"This log message will be shown\" in contents\n        assert \"This log message won't be shown\" not in contents\n", "def test_log_cli_handler_reset(testdir):\n    \"\"\"Test that log_cli_handler is reset at the start of each test.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning('log message from test_first')\n\n            logger = logging.getLogger('test_second')\n            logger.warning('log message from test_second')\n            assert 'log message from test_first' not in logging.getLogger('test_second').handlers[0].stream.getvalue()\n        \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_second::test_second PASSED*\",\n            \"=* 2 passed in *=\",\n        ]\n    )\n"], "sample_904": ["def test_envvar_xref(app):\n    text = (\".. envvar:: ENV_VAR\\n\"\n            \"\\n\"\n            \"This is a reference to :envvar:`ENV_VAR`.\")\n    doctree = restructuredtext.parse(app, text)\n    refnodes = list(doctree.findall(pending_xref))\n    assert_node(refnodes[0], pending_xref, reftarget=\"ENV_VAR\")\n    assert_node(refnodes[1], pending_xref, reftarget=\"ENV_VAR\")\n\n    domain = app.env.get_domain(\"std\")\n    refnode = domain.resolve_xref(app.env, 'index', app.builder, 'envvar', 'ENV_VAR',\n                                  pending_xref(), nodes.paragraph())\n    assert_node(refnode, nodes.reference, refid=\"envvar-ENV-VAR\")\n", "def test_program_option_with_spaces(app):\n    text = \".. program:: prog with spaces\\n\\n.. option:: -o output\"\n    domain = app.env.get_domain('std')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"-o\"],\n                                                    [desc_addname, \" output\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[('pair', 'prog-with-spaces command line option; -o', 'cmdoption-prog-with-spaces-o', '', None)])\n    assert ('prog-with-spaces', '-o') in domain.progoptions\n    assert domain.progoptions[('prog-with-spaces', '-o')] == ('index', 'cmdoption-prog-with-spaces-o')\n", "def test_cmdoption_multiple_options(app):\n    text = (\".. option:: -a, -b ARG\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (index,\n                          [desc, ([desc_signature, ([desc_name, '-a'],\n                                                    [desc_addname, ', '],\n                                                    [desc_name, '-b'],\n                                                    [desc_addname, ' ARG'])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[('pair', 'command line option; -a', 'cmdoption-a', '', None),\n                         ('pair', 'command line option; -b', 'cmdoption-b', '', None),\n                         ('pair', 'arg command line option; ARG', 'cmdoption-arg-ARG', '', None)])\n\n    objects = list(app.env.get_domain(\"std\").get_objects())\n    assert ('-a', '-a', 'cmdoption', 'index', 'cmdoption-a', 1) in objects\n    assert ('-b', '-b', 'cmdoption', 'index', 'cmdoption-b', 1)\n    assert ('ARG', 'ARG', 'cmdoption', 'index', 'cmdoption-arg-ARG', 1) in objects\n", "def test_productionlist_missing_target(app, warning):\n    text = (\".. productionlist:: P3\\n\"\n            \"   A: `:MissingTarget`\\n\")\n    restructuredtext.parse(app, text)\n    warning.seek(0)\n    assert \"WARNING: undefined reference: MissingTarget\" in warning.read()\n", "def test_cmd_option_with_hyphen_in_name(app):\n    text = \".. option:: --foo-bar\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (index,\n                          [desc, ([desc_signature, ([desc_name, '--foo-bar'],\n                                                    [desc_addname, ()])],\n                                  [desc_content, ()])]))\n    objects = list(app.env.get_domain(\"std\").get_objects())\n    assert ('--foo-bar', '--foo-bar', 'cmdoption', 'index', 'cmdoption--foo-bar', 1) in objects\n"], "sample_2": ["def test_wcs_attribute_non_wcs_instance():\n    ccd_data = create_ccd_data()\n    with pytest.raises(TypeError):\n        ccd_data.wcs = 'not a WCS instance'\n", "def test_wcs_attribute_modification(tmpdir):\n    \"\"\"\n    Check that modifications to the WCS attribute are correctly reflected in the header\n    when the CCDData object is converted back to an HDU.\n    \"\"\"\n    ccd_data = create_ccd_data()\n    tmpfile = tmpdir.join('temp.fits')\n    wcs = WCS(naxis=2)\n    wcs.wcs.crpix = np.array(ccd_data.shape) / 2\n    wcs.wcs.cdelt = np.array([-0.066667, 0.066667])\n    wcs.wcs.crval = [0, -90]\n    wcs.wcs.ctype = [\"RA---AIR\", \"DEC--AIR\"]\n    wcs.wcs.set_pv([(2, 1, 45.0)])\n    ccd_data.wcs = wcs\n    ccd_data.write(tmpfile.strpath)\n\n    ccd_new = CCDData.read(tmpfile.strpath)\n    ccd_new.wcs.wcs.cdelt *= 2\n    ccd_new_hdu = ccd_new.to_hdu()[0]\n    assert ccd_new_hdu.header['CDELT1'] == ccd_new.wcs.wcs.cdelt[0]\n    assert ccd_new_hdu.header['CDELT2'] == ccd_new.wcs.wcs.cdelt[1]\n", "def test_wcs_attribute_with_array():\n    \"\"\"\n    Check that an AttributeError is raised when trying to assign an array to the WCS attribute.\n    \"\"\"\n    ccd_data = create_ccd_data()\n    with pytest.raises(TypeError):\n        ccd_data.wcs = np.array([[1, 2], [3, 4]])\n", "def test_stddevuncertainty_compat_descriptor_weakref():\n    # TODO: Remove this test if astropy 1.0 isn't supported anymore\n    # This test might create a Memoryleak on purpose, so the last lines after\n    # the assert are IMPORTANT cleanup.\n    ccd = CCDData(np.ones((10, 10)), unit='')\n    uncert = StdDevUncertainty(np.ones((10, 10)))\n    uncert.parent_nddata = ccd\n    assert uncert.parent_nddata is ccd\n    uncert.parent_nddata = None\n", "def test_wcs_attribute_modification():\n    # Check that if the WCS attribute of a CCDData object is modified, then\n    # the CCDData object is turned back into an hdu, the modified WCS object\n    # overwrites the old WCS information in the header.\n    ccd_data = create_ccd_data()\n    wcs = WCS(naxis=2)\n    wcs.wcs.crpix = np.array(ccd_data.shape) / 2\n    wcs.wcs.cdelt = np.array([-0.066667, 0.066667])\n    wcs.wcs.crval = [0, -90]\n    wcs.wcs.ctype = [\"RA---AIR\", \"DEC--AIR\"]\n    wcs.wcs.set_pv([(2, 1, 45.0)])\n    ccd_data.wcs = wcs\n    ccd_data.header = ccd_data.to_hdu()[0].header\n\n    ccd_data.wcs.wcs.crpix += 1\n    ccd_new_hdu = ccd_data.to_hdu()[0]\n    assert ccd_new_hdu.header['CRPIX1'] == ccd_data.wcs.wcs.crpix[0]\n    assert ccd_new_hdu.header['CRPIX2'] == ccd_data.wcs.wcs.crpix[1]\n"], "sample_135": ["def test_time_format_specifiers(self):\n    my_time = datetime(1984, 8, 7, 13, 30, 15)\n\n    for specifier in ['b', 'c', 'd', 'D', 'E', 'F', 'I', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'S', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n        msg = (\n            \"The format for time objects may not contain date-related \"\n            \"format specifiers (found '%s').\" % specifier\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            dateformat.time_format(my_time.time(), specifier)\n", "def test_time_format_specifiers(self):\n    my_time = datetime(1984, 8, 7, 12, 34, 56)\n\n    for specifier in ['c', 'd', 'D', 'E', 'F', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'S', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n        msg = (\n            \"The format for time objects may not contain date-related \"\n            \"format specifiers (found '%s').\" % specifier\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            dateformat.time_format(my_time, specifier)\n", "    def test_iso_week_number(self):\n        # Test that the week number is calculated correctly for ISO weeks\n        iso_week_1 = datetime(2022, 1, 3)  # This date is in the first week of 2022\n        self.assertEqual(dateformat.format(iso_week_1, 'W'), '1')\n\n        iso_week_53 = datetime(2021, 12, 27)  # This date is in the last week of 2021\n        self.assertEqual(dateformat.format(iso_week_53, 'W'), '53')\n", "def test_timezone_change(self):\n    # Test for datetime objects that have a timezone change (DST).\n    # This test case is used to ensure that no AmbiguousTimeError is raised.\n    dst_start = datetime(2015, 3, 29, 2, 30, 0)  # DST starts in Europe/Copenhagen\n    dst_end = datetime(2015, 10, 25, 2, 30, 0)  # DST ends in Europe/Copenhagen\n\n    self.assertEqual(format(dst_start, 'I'), '1')\n    self.assertEqual(format(dst_start, 'O'), '+0200')\n    self.assertEqual(format(dst_start, 'T'), 'CEST')\n    self.assertEqual(format(dst_start, 'Z'), '7200')\n\n    self.assertEqual(format(dst_end, 'I'), '0')\n    self.assertEqual(format(dst_end, 'O'), '+0100')\n    self.assertEqual(format(dst_end, 'T'), 'CET')\n    self.assertEqual(format(dst_end, 'Z'), '3600')\n", "def test_datetime_with_local_tzinfo_format(self):\n    ltz = get_default_timezone()\n    dt = make_aware(datetime(2009, 5, 16, 5, 30, 30), ltz)\n    self.assertEqual(format(dt, 'D, M j, Y H:i:s O'), 'Sat, May 16, 2009 05:30:30 +0200')\n"], "sample_356": ["    def test_remove_field_with_default(self):\n        \"\"\"Removing a field with a default should not raise an error.\"\"\"\n        changes = self.get_changes([self.author_name_default], [self.author_empty])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n", "def test_add_index_together_with_order_with_respect_to(self):\n    \"\"\"\n    Adding index_together and order_with_respect_to to a model should\n    work correctly.\n    \"\"\"\n    changes = self.get_changes(\n        [self.book, self.author_with_book],\n        [self.book, self.author_with_book_order_wrt_index_together],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterOrderWithRespectTo', 'AlterIndexTogether'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=\"book\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"author\", index_together={('name', '_order')})\n", "def test_alter_model_managers_with_custom_queryset(self):\n    \"\"\"\n    Changing the model managers that use custom queryset adds a new operation.\n    \"\"\"\n    changes = self.get_changes([self.other_pony_no_custom_queryset], [self.other_pony_custom_queryset])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterModelManagers\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"pony\")\n    self.assertEqual([name for name, mgr in changes['otherapp'][0].operations[0].managers],\n                     ['food_qs', 'food_mgr', 'food_mgr_kwargs'])\n    self.assertEqual(changes['otherapp'][0].operations[0].managers[0][1].__class__.__name__, 'CustomQuerySet')\n    self.assertEqual(changes['otherapp'][0].operations[0].managers[0][1].model._meta.label, 'otherapp.Pony')\n    self.assertEqual(changes['otherapp'][0].operations[0].managers[0][1].using, 'custom_db')\n", "def test_alter_many_to_many_through_model(self):\n    changes = self.get_changes(\n        [self.author_with_m2m_through, self.publisher, self.contract],\n        [self.author_with_m2m_through_new, self.publisher, self.contract_new],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"contract\", table=\"new_contract\")\n", "    def test_create_model_with_many_to_many_field(self):\n        author = ModelState('otherapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ])\n        book = ModelState('otherapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n            ('authors', models.ManyToManyField('otherapp.Author')),\n        ])\n        changes = self.get_changes([], [author, book])\n        # Right number of migrations?\n        self.assertEqual(len(changes['otherapp']), 1)\n        # Right number of actions?\n        migration = changes['otherapp'][0]\n        self.assertEqual(len(migration.operations), 2)\n        # Right actions order?\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='Book')\n"], "sample_294": ["def test_https_good_referer_matches_cookie_domain_without_trailing_slash(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted from a\n    subdomain that's allowed by SESSION_COOKIE_DOMAIN and doesn't have a\n    trailing slash.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'https://foo.example.com'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_good_origin_wildcard_trusted_origin_allowed_with_port(self):\n    \"\"\"\n    A POST request with an origin that matches a CSRF_TRUSTED_ORIGINS\n    wildcard is accepted when the origin includes a port.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://foo.example.com:8000'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n    self.assertEqual(mw.allowed_origins_exact, set())\n    self.assertEqual(mw.allowed_origin_subdomains, {'https': ['.example.com']})\n", "def test_bare_secret_cookie_sets_session(self):\n    \"\"\"\n    The session is set from a bare secret cookie.\n    \"\"\"\n    req = self._get_POST_bare_secret_csrf_cookie_request_with_token()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, token_view, (), {})\n    self.assertIsNone(resp)\n    resp = mw(req)\n    self.assertEqual(req.session[CSRF_SESSION_KEY], resp.cookies[settings.CSRF_COOKIE_NAME].value)\n", "def test_https_malformed_host_with_session(self):\n    \"\"\"\n    CsrfViewMiddleware generates a 403 response if it receives an HTTPS\n    request with a bad host when using session-based CSRF tokens.\n    \"\"\"\n    req = self._get_POST_no_csrf_cookie_request()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = '@malformed'\n    req.META['HTTP_REFERER'] = 'https://www.evil.org/somepage'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(token_view)\n    response = mw.process_view(req, token_view, (), {})\n    self.assertEqual(response.status_code, 403)\n", "    def test_bare_secret_accepted_and_updated_in_session(self):\n        \"\"\"\n        The session is updated from a bare secret.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request_with_token()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, token_view, (), {})\n        self.assertIsNone(resp)\n        resp = mw(req)\n        self.assertIn(CSRF_SESSION_KEY, req.session, \"Session was not updated from bare secret\")\n        session_csrf = req.session[CSRF_SESSION_KEY]\n        self.assertEqual(len(session_csrf), CSRF_TOKEN_LENGTH)\n        self._check_token_present(resp, csrf_id=session_csrf)\n"], "sample_699": ["def test_doctest_module_with_fixtures_and_usefixtures_in_ini(self, pytester: Pytester):\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        usefixtures = myfixture\n    \"\"\"\n    )\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        @pytest.fixture\n            monkeypatch.setenv(\"HELLO\", \"WORLD\")\n    \"\"\"\n    )\n\n    p = pytester.makepyfile(\n        \"\"\"\n        '''\n            >>> import os\n            >>> os.environ[\"HELLO\"]\n            'WORLD'\n        '''\n    \"\"\"\n    )\n    reprec = pytester.inline_run(p, \"--doctest-modules\")\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_namespace_fixture_scope_class(self, pytester):\n    \"\"\"\n    Check that inserting something into the namespace works in a\n    class-scoped doctest\n    \"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import contextlib\n\n        @pytest.fixture(autouse=True, scope=\"class\")\n            doctest_namespace['cl'] = contextlib\n    \"\"\"\n    )\n    p = pytester.makepyfile(\n        \"\"\"\n        class TestFoo:\n            '''\n            >>> print(cl.__name__)\n            contextlib\n            '''\n        \"\"\"\n    )\n    reprec = pytester.inline_run(p, \"--doctest-modules\")\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_report_only_first_failure(self, pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n            '''\n            >>> foo()\n               a  b\n            0  1  4\n            1  2  4\n            2  3  6\n            >>> foo()\n               a  b\n            0  1  4\n            1  3  5\n            2  3  6\n            '''\n            print('   a  b\\\\n'\n                  '0  1  4\\\\n'\n                  '1  2  4\\\\n'\n                  '2  3  6')\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\", \"--doctest-report\", \"only_first_failure\")\n    result.stdout.fnmatch_lines(\n        [\n            \"Expected:\",\n            \"   a  b\",\n            \"0  1  4\",\n            \"1  2  4\",\n            \"2  3  6\",\n            \"Got:\",\n            \"   a  b\",\n            \"0  1  4\",\n            \"1  2  4\",\n            \"2  3  6\",\n        ]\n    )\n    # Check that only the first failure is reported\n    result.stdout.no_fnmatch_line(\"Expected:\")\n", "def test_doctest_module_with_namespace_fixture(self, pytester: Pytester):\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"module\")\n            return {'custom_function': lambda x: x**2}\n        \"\"\"\n    )\n    p = pytester.makepyfile(\n        \"\"\"\n            '''\n            >>> custom_function(2)\n            4\n            '''\n        \"\"\"\n    )\n    reprec = pytester.inline_run(p, \"--doctest-modules\")\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_continue_on_failure_multiple_failures(self, pytester: Pytester):\n    pytester.maketxtfile(\n        test_something=\"\"\"\n        >>> i = 5\n        >>> def foo():\n        ...     raise ValueError('error1')\n        >>> foo()\n        >>> i + 2\n        7\n        >>> i + 1\n        7\n    \"\"\"\n    )\n    result = pytester.runpytest(\n        \"--doctest-modules\", \"--doctest-continue-on-failure\"\n    )\n    result.assert_outcomes(passed=0, failed=1)\n    # The lines that contain the failure are 4, 6, and 8.  The first one\n    # is a stack trace and the other two are mismatches.\n    result.stdout.fnmatch_lines(\n        [\"*4: UnexpectedException*\", \"*6: DocTestFailure*\", \"*8: DocTestFailure*\"]\n    )\n"], "sample_1048": ["def test_parabola_intersection():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    p2 = Parabola(Point(0, 0), Line(Point(8, 5), Point(8, 7)))\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    e1 = Ellipse(Point(0, 0), 2, 5)\n    s1 = Segment((-12, -65), (14, -68))\n\n    assert p1.intersection(p2) == [p2]\n    assert p1.intersection(Point(0, 16)) == []\n    assert p1.intersection(Point(4, 3)) == [Point2D(4, 3)]\n    assert p1.intersection(l1) == [Point2D(-4, 3), Point2D(4, 3)]\n    assert p1.intersection(e1) == [Point2D(-2, 0), Point2D(2, 0)]\n    assert p1.intersection(s1) == []\n    raises(TypeError, lambda: p1.intersection(Line(Point(1, 2, 3), Point(4, 5, 6))))\n    raises(TypeError, lambda: p1.intersection(Point(1, 2, 3)))\n", "def test_parabola_intersection():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    e1 = Ellipse(Point(0, 0), 2, 5)\n    s1 = Segment2D(Point(-12, -65), Point(14, -68))\n\n    assert p1.intersection(e1) == [Point2D(-2, 0), Point2D(2, 0)]\n    assert p1.intersection(l1) == [Point2D(-4, -2), Point2D(4, -2)]\n    assert p1.intersection(s1) == []\n    assert p1.intersection(Parabola(Point(0, 4), Line(Point(0, 0), Point(0, 8)))) == [p1]\n    assert p1.intersection(Point2D(2, 4)) == []\n    assert p1.intersection(Point2D(0, 0)) == [Point2D(0, 0)]\n", "def test_parabola_intersection():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    p2 = Parabola(Point(2, 0), Line(Point(5, 8), Point(7, 8)))\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    e1 = Ellipse(Point(0, 0), 2, 5)\n    s1 = Segment2D(Point(-12, -65), Point(14, -68))\n\n    assert p1.intersection(p2) == [p2]\n    assert p1.intersection(Point(0, 16)) == []\n    assert p1.intersection(Point(4, 3)) == [Point(4, 3)]\n    assert p1.intersection(l1) == [Point(-4, 3), Point(4, 3)]\n    assert p1.intersection(e1) == [Point(-2, 0), Point(2, 0)]\n    assert p1.intersection(s1) == []\n\n    raises(TypeError, lambda: p1.intersection(Line(Point(1, 2, 3), Point(4, 5, 6))))\n    raises(TypeError, lambda: p1.intersection(\"wrong type\"))\n", "def test_parabola_intersection():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    p2 = Parabola(Point(0, 0), Line(Point(0, -1), Point(0, 1)))\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    e1 = Ellipse(Point(0, 0), 2, 5)\n    s1 = Segment((-12, -65), (14, -68))\n\n    assert p1.intersection(p2) == [p2]\n    assert p1.intersection(l1) == [Point2D(-4, -2), Point2D(4, -2)]\n    assert p1.intersection(e1) == [Point2D(-2, 0), Point2D(2, 0)]\n    assert p1.intersection(s1) == []\n\n    raises(TypeError, lambda: p1.intersection(Line(Point(1, 2, 3), Point(4, 5, 6))))\n    raises(TypeError, lambda: p1.intersection(\"invalid type\"))\n", "def test_parabola_intersection():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    e1 = Ellipse(Point(0, 0), 2, 5)\n    s1 = Segment2D((-12, -65), (14, -68))\n\n    assert p1.intersection(l1) == [Point2D(-4, -2), Point2D(4, -2)]\n    assert p1.intersection(e1) == [Point2D(-2, 0), Point2D(2, 0)]\n    assert p1.intersection(s1) == []\n    assert p1.intersection(p1) == [p1]\n"], "sample_1165": ["def test_quaternion_division():\n    q1 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    q2 = Quaternion(1, 2, 3, 5)\n    q3 = Quaternion(1, 1, 1, y)\n\n    assert q1 / 2 == Quaternion((3 + 4*I) / 2, (2 + 5*I) / 2, 0, (7 + 8*I) / 2)\n    assert q2 / q3 == q2 * q3.inverse()\n\n    z = symbols('z', complex=True)\n    z_quat = Quaternion(re(z), im(z), 0, 0)\n    q = Quaternion(*symbols('q:4', real=True))\n\n    assert q / z == q * z_quat.inverse()\n    assert z / q == z_quat * q.inverse()\n", "def test_quaternion_complex_multiplication():\n    q1 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    q2 = Quaternion(1, 2, 3, 5)\n    z = 2 + 3*I\n\n    assert q1 * z == Quaternion((2 + 3*I)*(3 + 4*I), (2 + 3*I)*(2 + 5*I), 0, (2 + 3*I)*(7 + 8*I))\n    assert z * q1 == Quaternion((2 + 3*I)*(3 + 4*I), (2 + 3*I)*(2 + 5*I), 0, (2 + 3*I)*(7 + 8*I))\n    assert q2 * z == Quaternion(2 + 3*I, 4 + 6*I, 6 + 9*I, 10 + 15*I)\n    assert z * q2 == Quaternion(2 + 3*I, 4 + 6*I, 6 + 9*I, 10 + 15*I)\n", "def test_quaternion_division():\n    q1 = Quaternion(3, 2, 5, 7)\n    q2 = Quaternion(1, 2, 3, 4)\n\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / 2 == Quaternion(3/2, 1, 5/2, 7/2)\n    assert 2 / q1 == Quaternion(2/3, -1/3, -5/18, -7/18)\n", "def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / 2 == Quaternion(0.5, 1, 1.5, 2)\n    assert 2 / q1 == Quaternion(2, -1, -1.5, -2)\n\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field = False)\n    q4 = Quaternion(1, 2, 3, 4)\n\n    assert q3 / q4 == q3 * q4.inverse()\n    assert q3 / 2 == Quaternion((3 + 4*I)/2, (2 + 5*I)/2, 0, (7 + 8*I)/2)\n    assert 2 / q3 == Quaternion(2/q3.norm() * conjugate(q3))\n", "def test_quaternion_normalization():\n    q1 = Quaternion(1, 2, 3, 4)\n    q1_norm = q1.norm()\n    q1_normalized = q1.normalize()\n\n    assert q1_normalized.norm() == 1\n    assert q1_normalized == q1 / q1_norm\n\n    q2 = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q2.normalize())\n"], "sample_784": ["def test_calibration_multilabel():\n    \"\"\"Test calibration for multilabel\"\"\"\n    X, y = make_classification(n_samples=100, n_features=2, n_classes=3,\n                               n_clusters_per_class=1, n_informative=2,\n                               random_state=42)\n    y = (np.arange(3) == y[:, None]).astype(float)\n\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf = LinearSVC()\n    clf.fit(X_train, y_train)\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        cal_clf.fit(X_train, y_train)\n        probas = cal_clf.predict_proba(X_test)\n        assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test)))\n\n        # Check that log-loss of calibrated classifier is smaller than\n        # log-loss of naively turned OvR decision function to probabilities\n        # via softmax\n        uncalibrated_log_loss = log_loss(y_test, clf.decision_function(X_test))\n        calibrated_log_loss = log_loss(y_test, probas)\n        assert_greater_equal(uncalibrated_log_loss, calibrated_log_loss)\n", "def test_calibration_multilabel():\n    \"\"\"Test calibration for multilabel targets\"\"\"\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                               n_redundant=0, n_classes=3, n_clusters_per_class=1,\n                               random_state=42, n_labels=2)\n\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n    clf_probs = clf.predict_proba(X_test)\n    loss = log_loss(y_test, clf_probs)\n\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        cal_clf.fit(X_train, y_train)\n        cal_clf_probs = cal_clf.predict_proba(X_test)\n        cal_loss = log_loss(y_test, cal_clf_probs)\n        assert_greater(loss, cal_loss)\n", "def test_calibration_multilabel():\n    \"\"\"Test calibration for multilabel case\"\"\"\n    n_samples = 50\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6, n_classes=3,\n                               random_state=42, n_informative=3, n_clusters_per_class=1)\n    y = (y[:, np.newaxis] == np.unique(y)).astype(int)  # convert to multilabel\n\n    X_train, y_train = X[:n_samples], y[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n    clf_probs = clf.predict_proba(X_test)\n    loss = log_loss(y_test, clf_probs)\n\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        cal_clf.fit(X_train, y_train)\n        cal_clf_probs = cal_clf.predict_proba(X_test)\n        cal_loss = log_loss(y_test, cal_clf_probs)\n        assert_greater(loss, cal_loss)\n", "def test_calibration_multi_label():\n    \"\"\"Test calibration for multi-label\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3,\n                               n_informative=2, n_redundant=2,\n                               random_state=42, n_clusters_per_class=1)\n    y = label_binarize(y, classes=[0, 1, 2])\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n    clf_probs = clf.predict_proba(X_test)\n    loss = log_loss(y_test, clf_probs)\n\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        cal_clf.fit(X_train, y_train)\n        cal_clf_probs = cal_clf.predict_proba(X_test)\n        cal_loss = log_loss(y_test, cal_clf_probs)\n        assert_greater(loss, cal_loss)\n", "def test_calibration_multilabel():\n    \"\"\"Test calibration for multilabel classification\"\"\"\n    n_samples = 100\n    n_features = 20\n    n_labels = 3\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               n_classes=n_labels, n_informative=n_features,\n                               n_redundant=0, random_state=42)\n    y = (y > 0).astype(np.int64)  # Make y multilabel\n\n    base_estimator = RandomForestClassifier(random_state=42)\n\n    for method in ['isotonic', 'sigmoid']:\n        calibrated_clf = CalibratedClassifierCV(base_estimator, method=method, cv=3)\n        calibrated_clf.fit(X, y)\n        probas = calibrated_clf.predict_proba(X)\n\n        # Check that each predicted probability vector sums to 1\n        assert_array_almost_equal(np.sum(probas, axis=1), np.ones(n_samples))\n"], "sample_230": ["def test_disabled_field_to_python(self):\n    field = JSONField(disabled=True)\n    value = '{\"a\": \"b\"}'\n    self.assertEqual(field.to_python(value), value)\n", "def test_widget_attrs(self):\n    field = JSONField(widget=TextInput(attrs={'class': 'custom-class'}))\n    self.assertEqual(field.widget_attrs(field.widget), {'class': 'custom-class'})\n", "def test_json_field_with_disabled_widget(self):\n    field = JSONField(disabled=True)\n    self.assertEqual(field.clean('{\"a\": \"b\"}'), '{\"a\": \"b\"}')\n    self.assertFalse(field.has_changed({'a': 'b'}, '{\"a\": \"c\"}'))\n", "def test_formfield_error_messages(self):\n    class JSONForm(Form):\n        json_field = JSONField(error_messages={'invalid': 'Invalid JSON input.'})\n\n    form = JSONForm({'json_field': 'invalid json'})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors['json_field'], ['Invalid JSON input.'])\n", "def test_custom_encoder_decoder_failure(self):\n    class CustomDecoder(json.JSONDecoder):\n            return super().__init__(object_hook=self.as_uuid, *args, **kwargs)\n\n            if 'uuid' in dct:\n                dct['uuid'] = 'invalid_uuid'\n            return dct\n\n    value = {'uuid': 'c141e152-6550-4172-a784-05448d98204b'}\n    encoded_value = '{\"uuid\": \"c141e152-6550-4172-a784-05448d98204b\"}'\n    field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid UUID.'):\n        field.clean(encoded_value)\n"], "sample_644": ["def test_check_import_as_rename(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_as_rename\", REGR_DATA)\n    import_node = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"useless-import-alias\",\n        node=import_node,\n        confidence=UNDEFINED,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=21,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n\n    import_node = module.body[1]\n    msg = MessageTest(\n        msg_id=\"consider-using-from-import\",\n        node=import_node,\n        args=(\"os\", \"path\"),\n        confidence=UNDEFINED,\n        line=2,\n        col_offset=0,\n        end_line=2,\n        end_col_offset=36,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n", "def test_check_import_as_rename(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_as_rename\", REGR_DATA)\n    import_node = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"useless-import-alias\",\n        node=import_node,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=19,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n\n    import_node = module.body[1]\n    msg = MessageTest(\n        msg_id=\"consider-using-from-import\",\n        node=import_node,\n        args=(\"collections\", \"defaultdict\"),\n        line=2,\n        col_offset=0,\n        end_line=2,\n        end_col_offset=25,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n", "def test_check_toplevel(self) -> None:\n    module = astroid.parse(\"from os import path\")\n    import_node = module.body[0]\n    # Set the scope of the import_node to be a function scope\n    import_node.parent = astroid.nodes.FunctionDef(\"test_func\")\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=import_node,\n        args=\"os.path\",\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=17,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_importfrom(import_node)\n", "def test_relative_import_level(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"relative_import\", REGR_DATA)\n    import_from = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"relative-beyond-top-level\",\n        node=import_from,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=25,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_importfrom(import_from)\n\n    import_from = module.body[1]\n    with self.assertNoMessages():\n        self.checker.visit_importfrom(import_from)\n", "def test_reimport(capsys: CaptureFixture[str]) -> None:\n    Run(\n        [\n            f\"{os.path.join(REGR_DATA, 'reimport')}\",\n            \"-d all\",\n            \"-e reimported,shadowed-import\",\n        ],\n        exit=False,\n    )\n    output, errors = capsys.readouterr()\n    assert \"Reimport 'module' (imported line 1)\" in output\n    assert \"Shadowed 'alias' (imported line 1)\" in output\n    assert errors == \"\"\n"], "sample_227": ["def test_emptylistfieldfilter_reverse_relationships_ordering(self):\n    class UserAdminReverseRelationship(UserAdmin):\n        list_filter = (\n            ('books_contributed', EmptyFieldListFilter),\n        )\n\n    self.guitar_book.employee = self.john\n    self.guitar_book.save()\n    self.bio_book.employee = self.jack\n    self.bio_book.save()\n\n    user_admin = UserAdminReverseRelationship(User, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = user_admin.get_changelist_instance(request)\n    filterspec = changelist.get_filters(request)[0][0]\n    expected = [(self.bob.pk, 'bob'), (self.lisa.pk, 'lisa'), (self.alfred.pk, 'alfred')]\n    self.assertEqual(filterspec.lookup_choices, expected)\n", "def test_emptylistfieldfilter_reverse_relationships_no_related_objects(self):\n    class UserAdminReverseRelationship(UserAdmin):\n        list_filter = (\n            ('books_contributed', EmptyFieldListFilter),\n        )\n\n    Book.objects.all().delete()\n    user_admin = UserAdminReverseRelationship(User, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = user_admin.get_changelist_instance(request)\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(len(filterspec.lookup_choices), 0)\n", "def test_emptylistfieldfilter_ordering(self):\n    \"\"\"EmptyFieldListFilter ordering respects ModelAdmin.ordering.\"\"\"\n    class BookAdminWithOrdering(ModelAdmin):\n        ordering = ('title',)\n        list_filter = [('title', EmptyFieldListFilter)]\n\n    empty_title = Book.objects.create(title='', author=self.alfred)\n    modeladmin = BookAdminWithOrdering(Book, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    filterspec = changelist.get_filters(request)[0][0]\n    expected_choices = [('All', '?', False), ('Empty', '?title__isempty=1', True), ('Not empty', '?title__isempty=0', False)]\n    self.assertEqual([(c['display'], c['query_string'], c['selected']) for c in filterspec.choices(changelist)], expected_choices)\n", "def test_emptylistfieldfilter_reverse_one_to_one_relationship(self):\n    class BookAdminWithEmptyFieldListFilter(ModelAdmin):\n        list_filter = [('improvedbook', EmptyFieldListFilter)]\n\n    ImprovedBook.objects.create(book=self.guitar_book)\n\n    modeladmin = BookAdminWithEmptyFieldListFilter(Book, site)\n\n    tests = [\n        ({'improvedbook__isempty': '1'}, [self.django_book, self.bio_book, self.djangonaut_book]),\n        ({'improvedbook__isempty': '0'}, [self.guitar_book]),\n    ]\n    for query_string, expected_result in tests:\n        with self.subTest(query_string=query_string):\n            request = self.request_factory.get('/', query_string)\n            request.user = self.alfred\n            changelist = modeladmin.get_changelist_instance(request)\n            queryset = changelist.get_queryset(request)\n            self.assertCountEqual(queryset, expected_result)\n", "def test_emptylistfieldfilter_non_editable_field(self):\n    class BookAdminWithNonEditableField(ModelAdmin):\n        list_filter = [('year', EmptyFieldListFilter)]\n\n    modeladmin = BookAdminWithNonEditableField(Book, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    msg = (\n        \"The list filter 'EmptyFieldListFilter' cannot be used with field \"\n        \"'year' which is not editable.\"\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        modeladmin.get_changelist_instance(request)\n"], "sample_228": ["    def test_can_delete_extra_true(self):\n        ChoiceFormSet = formset_factory(Choice, can_delete=True, can_delete_extra=True, extra=2)\n        formset = ChoiceFormSet()\n        self.assertEqual(len(formset), 2)\n        self.assertIn('DELETE', formset.forms[0].fields)\n        self.assertIn('DELETE', formset.forms[1].fields)\n", "def test_formset_with_deletion_and_empty_forms(self):\n    \"\"\"FormSets with deletion and empty forms are valid.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True)\n    initial = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n\n    data = {\n        'choices-TOTAL_FORMS': '4',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '2',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-0-DELETE': '',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-1-DELETE': '',\n        'choices-2-choice': '',\n        'choices-2-votes': '',\n        'choices-2-DELETE': '',\n        'choices-3-choice': '',\n        'choices-3-votes': '',\n        'choices-3-DELETE': '',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.forms],\n        [\n            {'votes': 100, 'DELETE': False, 'choice': 'Calexico'},\n            {'votes': 900, 'DELETE': False, 'choice': 'Fergie'},\n            {},\n            {},\n        ],\n    )\n    self.assertEqual(formset.deleted_forms, [])\n", "def test_formset_with_deletion_extra_forms(self):\n    \"\"\"\n    FormSets with can_delete=True and extra forms don't have a delete field\n    until they're filled.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, extra=1)\n    formset = ChoiceFormSet()\n    self.assertNotIn('DELETE', formset.empty_form.fields)\n    self.assertNotIn('DELETE', formset.forms[0].fields)\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '1',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-0-DELETE': '',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertIn('DELETE', formset.forms[0].fields)\n    self.assertIn('DELETE', formset.forms[1].fields)\n", "def test_invalid_initial_form_with_extra(self):\n    \"\"\"\n    Initial forms are validated and don't count toward the extra forms when\n    validating min_num.\n    \"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '3',\n        'choices-INITIAL_FORMS': '1',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': '',\n        'choices-1-votes': '',\n        'choices-2-choice': '',\n        'choices-2-votes': '',\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=1, min_num=2, validate_min=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices', initial=[{'choice': 'Zero', 'votes': 0}])\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at least 2 forms.'])\n", "def test_formset_empty_permitted(self):\n    \"\"\"Empty forms should be permitted when initial_form_count is reached.\"\"\"\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '2',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '2',\n        'form-0-name': 'Drink 1',\n        'form-1-name': 'Drink 2',\n    }\n    formset = FavoriteDrinksFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertTrue(formset.forms[0].empty_permitted)\n    self.assertTrue(formset.forms[1].empty_permitted)\n"], "sample_370": ["def test_nested_prefetch_with_duplicate_prefetcher(self):\n    houses_2 = House.objects.prefetch_related(Prefetch('rooms'))\n    persons = Person.objects.prefetch_related(Prefetch('houses', queryset=houses_2))\n    houses = House.objects.prefetch_related(Prefetch('occupants', queryset=persons))\n    list(houses)  # queryset must be evaluated once to reproduce the bug.\n    self.assertEqual(\n        houses.all()[0].occupants.all()[0].houses.all()[1].rooms.all()[0],\n        self.room\n    )\n", "def test_prefetch_through_multiple_m2m_relations(self):\n    with self.assertNumQueries(4):\n        rooms = Room.objects.prefetch_related('house__occupants__favorite_authors')\n        for room in rooms:\n            for occupant in room.house.occupants.all():\n                list(occupant.favorite_authors.all())\n", "    def test_nested_prefetch(self):\n        with self.assertNumQueries(3):\n            # House -> Room, Room -> House\n            room = Room.objects.prefetch_related('house__rooms').get(pk=self.room.pk)\n\n        with self.assertNumQueries(0):\n            self.assertEqual(room.house.rooms.count(), 1)\n            self.assertEqual(room.house.rooms.first(), room)\n", "def test_prefetch_related_on_prefetched_instance(self):\n    # Test that prefetch_related() can be used on prefetched instances.\n    houses = House.objects.prefetch_related('rooms')\n    with self.assertNumQueries(2):\n        # One query for houses, one for rooms.\n        prefetch_related_objects([houses[0]], 'rooms__house')\n    with self.assertNumQueries(0):\n        self.assertEqual(self.room.house.name, 'Big house')\n", "    def test_prefetch_nested_through_model(self):\n        book = Book.objects.create(title='Poems')\n        author = Author.objects.create(name='Jane', first_book=book)\n        bookreview = BookReview.objects.create(book=book)\n        FavoriteAuthors.objects.create(author=author, likes_author=author)\n\n        with self.assertNumQueries(4):\n            rooms = Room.objects.prefetch_related(\n                Prefetch(\n                    'house__owner__authors__books__bookreview_set',\n                    queryset=BookReview.objects.all(),\n                )\n            )\n            for room in rooms:\n                self.assertCountEqual(room.house.owner.authors.all()[0].books.all()[0].bookreview_set.all(), [bookreview])\n"], "sample_954": ["def test_nested_inline(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert r'\\fBfoo=\\fP\\fI1\\fP\\fB&bar=\\fP\\fI2\\fP' in content\n", "def test_custom_nodes(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # test custom desc_name node\n    assert r'\\fBcustom_node_name\\en\\fP' in content\n\n    # test custom desc_addname node\n    assert r'\\fBadditional_name\\en\\fP' in content\n\n    # test custom desc_type node\n    assert r'\\fBtype_name\\en\\fP' in content\n\n    # test custom desc_returns node\n    assert r'->' in content\n\n    # test custom desc_parameterlist node\n    assert r'(' in content\n    assert r')' in content\n\n    # test custom desc_parameter node\n    assert r'param' in content\n\n    # test custom desc_optional node\n    assert r'[' in content\n    assert r']' in content\n\n    # test custom desc_annotation node\n    assert r'annotation' in content\n", "def test_section_title_handling(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # test section title handling\n    assert '.SH SECTION TITLE' in content\n    assert '.SH SUBSECTION TITLE' in content\n    assert 'SUBSUBSECTION TITLE' not in content  # not a manpage section title\n", "def test_custom_nodes(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'custom-nodes.1').read_text()\n\n    # test custom nodes\n    assert '\\n.B custom_node_name\\n' in content\n    assert '\\ncustom_node_content\\n' in content\n    assert '\\n\\\\fBemphasized_text\\\\fP\\n' in content\n    assert '\\n\\\\fBstrong_text\\\\fP\\n' in content\n", "def test_desc_signature(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # desc_signature with desc_name, desc_addname, desc_type, and desc_returns\n    assert '\\n.B function\\\\fP \\\\fI(arg1\\\\fP, arg2)\\\\fP -> \\\\fIreturn\\\\fP\\n' in content\n"], "sample_340": ["def test_loading_pyc_without_py(self):\n    \"\"\"\n    To support frozen environments, MigrationLoader loads .pyc migrations\n    even if the corresponding .py file is missing.\n    \"\"\"\n    with self.temporary_migration_module(module='migrations.test_migrations') as migration_dir:\n        # Compile .py files to .pyc files and delete .py files.\n        compileall.compile_dir(migration_dir, force=True, quiet=1, legacy=True)\n        for name in os.listdir(migration_dir):\n            if name.endswith('.py'):\n                os.remove(os.path.join(migration_dir, name))\n        loader = MigrationLoader(connection)\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n", "def test_loading_bytecode_only(self):\n    \"\"\"\n    To support frozen environments, MigrationLoader loads bytecode-only migrations.\n    \"\"\"\n    with self.temporary_migration_module(module='migrations.test_migrations') as migration_dir:\n        # Compile .py files to .pyc files and delete .py files.\n        compileall.compile_dir(migration_dir, force=True, quiet=1, legacy=True)\n        for name in os.listdir(migration_dir):\n            if name.endswith('.py'):\n                os.remove(os.path.join(migration_dir, name))\n        loader = MigrationLoader(connection)\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n", "def test_run_before_cycle(self):\n    \"\"\"\n    Makes sure the loader raises an error when there's a cycle in run_before dependencies.\n    \"\"\"\n    with self.assertRaises(NodeNotFoundError) as context:\n        migration_loader = MigrationLoader(connection)\n    self.assertEqual(str(context.exception), \"Migration migrations.0002_second depends on a nonexistent parent node ('migrations', '0003_third').\")\n", "def test_loading_replacing_migration(self):\n    \"\"\"\n    Tests loading a replacing migration\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Load with nothing applied: both migrations are present\n    loader.build_graph()\n    self.assertIn(('migrations', '0001_initial'), loader.graph.nodes)\n    self.assertIn(('migrations', '0002_replacing'), loader.graph.nodes)\n\n    # Fake-apply the replacing migration: it's used instead of the initial migration\n    self.record_applied(recorder, 'migrations', '0002_replacing')\n    loader.build_graph()\n    self.assertNotIn(('migrations', '0001_initial'), loader.graph.nodes)\n    self.assertIn(('migrations', '0002_replacing'), loader.graph.nodes)\n\n    # Fake-unapply the replacing migration: the initial migration is used instead\n    recorder.record_unapplied('migrations', '0002_replacing')\n    loader.build_graph()\n    self.assertIn(('migrations', '0001_initial'), loader.graph.nodes)\n    self.assertNotIn(('migrations', '0002_replacing'), loader.graph.nodes)\n", "def test_no_migration_class(self):\n    \"Tests when a migration file does not contain a Migration class\"\n    with self.assertRaises(BadMigrationError):\n        MigrationLoader(connection)\n"], "sample_419": ["def test_extra_forms_empty_permitted(self):\n    \"\"\"\n    Extra forms are allowed to be empty unless they're part of the minimum forms.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=3, min_num=1)\n    formset = ChoiceFormSet()\n    self.assertTrue(formset.forms[0].empty_permitted)\n    self.assertTrue(formset.forms[1].empty_permitted)\n    self.assertTrue(formset.forms[2].empty_permitted)\n\n    formset = ChoiceFormSet(initial=[{\"choice\": \"Zero\", \"votes\": \"1\"}])\n    self.assertFalse(formset.forms[0].empty_permitted)\n    self.assertTrue(formset.forms[1].empty_permitted)\n    self.assertTrue(formset.forms[2].empty_permitted)\n", "def test_formset_with_deletion_and_min_num(self):\n    \"\"\"FormSets with deletion and min_num validate correctly.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, min_num=1, can_delete=True)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-0-votes\" value=\"100\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-0-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-1-choice\" value=\"Fergie\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-1-votes\" value=\"900\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-1-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-2-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-2-votes\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-2-DELETE\"></li>',\n    )\n    # Let's delete Fergie, but we need to keep at least one form.\n    data = {\n        \"choices-TOTAL_FORMS\": \"3\",  # the number of forms rendered\n        \"choices-INITIAL_FORMS\": \"2\",  # the number of forms with initial data\n        \"choices-MIN_NUM_FORMS\": \"1\",  # min", "def test_formset_prefix(self):\n    \"\"\"The formset prefix is used correctly.\"\"\"\n    formset = self.make_choiceformset(prefix=\"custom_prefix\")\n    self.assertEqual(formset.management_form.prefix, \"custom_prefix\")\n", "def test_formset_with_different_error_message(self):\n    \"\"\"\n    Test formset with a different error message for management form errors.\n    \"\"\"\n    data = {\n        \"form-TOTAL_FORMS\": \"two\",\n        \"form-INITIAL_FORMS\": \"one\",\n    }\n    error_message = \"This is a custom error message for management form errors.\"\n    formset = ArticleFormSet(data, error_messages={\"missing_management_form\": error_message})\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(formset.non_form_errors(), [error_message])\n", "def test_formset_with_custom_renderer_override(self):\n    \"\"\"\n    A custom renderer passed to a formset_factory() can be overridden by the\n    individual form.\n    \"\"\"\n    from django.forms.renderers import Jinja2\n\n    class CustomRendererForm(Form):\n        renderer = None\n\n    renderer = Jinja2()\n    data = {\n        \"choices-TOTAL_FORMS\": \"1\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"0\",\n    }\n    ChoiceFormSet = formset_factory(CustomRendererForm, renderer=renderer)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    formset.forms[0].renderer = \"custom\"\n    self.assertEqual(formset.renderer, renderer)\n    self.assertEqual(formset.forms[0].renderer, \"custom\")\n    self.assertEqual(formset.management_form.renderer, renderer)\n    self.assertEqual(formset.non_form_errors().renderer, renderer)\n    self.assertEqual(formset.empty_form.renderer, renderer)\n"], "sample_963": ["def test_stringify_type_hints_special_form():\n    from typing import Any, NoReturn\n    assert stringify(Any) == \"Any\"\n    assert stringify(NoReturn) == \"NoReturn\"\n", "def test_restify_type_hints_special_forms():\n    assert restify(Any) == \":py:obj:`~typing.Any`\"\n    assert restify(Union) == \":py:obj:`~typing.Union`\"\n    assert restify(Optional) == \":py:obj:`~typing.Optional`\"\n    assert restify(ClassVar) == \":py:obj:`~typing.ClassVar`\"\n    assert restify(Final) == \":py:obj:`~typing.Final`\"\n", "def test_restify_type_NamedTuple():\n    from typing import NamedTuple\n\n    class Point(NamedTuple):\n        x: int\n        y: int\n\n    assert restify(Point) == \":py:class:`tests.test_util_typing.Point`\"\n    assert restify(Point[int, int]) == \":py:class:`tests.test_util_typing.Point`\"\n", "def test_restify_type_union_operator():\n    assert restify(int | None) == \":py:class:`int` | :py:obj:`None`\"  # type: ignore\n    assert restify(int | str) == \":py:class:`int` | :py:class:`str`\"  # type: ignore\n    assert restify(int | str | None) == (\":py:class:`int` | :py:class:`str` | \"  # type: ignore\n                                         \":py:obj:`None`\")\n", "def test_stringify_type_hints_broken_type_hints():\n    assert stringify(BrokenType) == \"tests.test_util_typing.BrokenType\"\n"], "sample_1090": ["def test_mul():\n    with evaluate(False):\n        p = oo * oo\n        assert p == oo\n        p = -oo * oo\n        assert p == -oo\n        p = oo * -oo\n        assert p == -oo\n        p = -oo * -oo\n        assert p == oo\n        p = oo * 5\n        assert p == oo\n        p = 5 * oo\n        assert p == oo\n        p = -oo * 5\n        assert p == -oo\n        p = 5 * -oo\n        assert p == -oo\n        p = oo * 0\n        assert p == nan\n        p = 0 * oo\n        assert p == nan\n        p = -oo * 0\n        assert p == nan\n        p = 0 * -oo\n        assert p == nan\n\n    with evaluate(False):\n        expr = x * x\n        assert isinstance(expr, Mul)\n        assert expr.args == (x, x)\n\n        with evaluate(True):\n            assert (x * x).args == (x**2,)\n\n        assert (x * x).args == (x, x)\n\n    assert isinstance(x * x, Pow)\n\n    with evaluate(False):\n        assert S(2) * 2 == Mul(2, 2)\n        assert 2 * S(2) == Mul(2, 2)\n\n        assert S(2) / 2 == Mul(2, S.One / 2)\n        assert S.One / 2 * 2 == Mul(S.One / 2, 2)\n", "def test_sub():\n    with evaluate(False):\n        p = oo - oo\n        assert isinstance(p, Add) and p.args == (oo, -oo)\n        p = 5 - oo\n        assert isinstance(p, Add) and p.args == (-oo, 5)\n        p = oo - 5\n        assert isinstance(p, Add) and p.args == (oo, -5)\n        p = oo + 5\n        assert isinstance(p, Add) and p.args == (oo, 5)\n        p = 5 + oo\n        assert isinstance(p, Add) and p.args == (oo, 5)\n        p = -oo + 5\n        assert isinstance(p, Add) and p.args == (-oo, 5)\n        p = -5 - oo\n        assert isinstance(p, Add) and p.args == (-oo, -5)\n\n    with evaluate(False):\n        expr = x - x\n        assert isinstance(expr, Add)\n        assert expr.args == (x, -x)\n\n        with evaluate(True):\n            assert (x - x).args == (0,)\n\n        assert (x - x).args == (x, -x)\n\n    assert isinstance(x - x, Mul)\n\n    with evaluate(False):\n        assert S(4) - S(4) == 0\n        assert S(4) - 3 == 1\n        assert -3 + S(4) == 1\n", "def test_nan_operations():\n    assert nan + 1 == nan\n    assert 1 + nan == nan\n    assert nan - 1 == nan\n    assert 1 - nan == nan\n    assert nan * 1 == nan\n    assert 1 * nan == nan\n    assert nan / 1 == nan\n    assert 1 / nan == nan\n    assert nan ** 2 == nan\n    assert 2 ** nan == nan\n    assert nan % 1 == nan\n    assert 1 % nan == nan\n", "def test_mul():\n    with evaluate(False):\n        p = oo * oo\n        assert isinstance(p, Mul) and p.args == (oo, oo)\n        p = 5 * oo\n        assert isinstance(p, Mul) and p.args == (oo, 5)\n        p = oo * 5\n        assert isinstance(p, Mul) and p.args == (oo, 5)\n        p = -oo * 5\n        assert isinstance(p, Mul) and p.args == (-oo, 5)\n        p = 5 * -oo\n        assert isinstance(p, Mul) and p.args == (-oo, 5)\n\n    with evaluate(False):\n        expr = x * x\n        assert isinstance(expr, Mul)\n        assert expr.args == (x, x)\n\n        with evaluate(True):\n            assert (x * x).args == (x, x)\n\n        assert (x * x).args == (x, x)\n\n    assert isinstance(x * x, Pow)\n\n    with evaluate(False):\n        assert S.One * 2 == Mul(1, 2)\n        assert 2 * S.One == Mul(2, 1)\n\n        assert S(4) * 3 == Mul(4, 3)\n        assert 3 * S(4) == Mul(3, 4)\n\n        assert S(2) ** 4 == Pow(2, 4)\n        assert S(4) ** 2 == Pow(4, 2)\n\n        assert S(2) / 4 == Mul(2, S.One / 4)\n        assert S.One / 4 * 2 == Mul(S.One / 4, 2)\n\n        assert S(2) / 3 * 4 == Mul(S(2) / 3, 4)\n        assert 4 * S(2) / 3 == Mul(4, S(2) / 3)\n\n        assert S(6) / 3 == Mul(6, S.One / 3)\n        assert S.One / 3 * 6 == Mul(S.One / 3, 6", "def test_mul():\n    with evaluate(False):\n        p = oo * oo\n        assert p == oo * oo\n        p = 5 * oo\n        assert p == oo\n        p = oo * 5\n        assert p == oo\n        p = oo * 0\n        assert p == nan\n        p = 0 * oo\n        assert p == nan\n        p = -oo * 5\n        assert p == -oo\n        p = 5 * -oo\n        assert p == -oo\n        p = -oo * -5\n        assert p == oo\n        p = -5 * -oo\n        assert p == oo\n\n    with evaluate(False):\n        expr = x * x\n        assert isinstance(expr, Mul)\n        assert expr.args == (x, x)\n\n        with evaluate(True):\n            assert (x * x).args == (x**2,)\n\n        assert (x * x).args == (x, x)\n\n    assert isinstance(x * x, Pow)\n\n    with evaluate(False):\n        assert S.One * 2 == Mul(1, 2)\n        assert 2 * S.One == Mul(2, 1)\n\n        assert S(4) / 2 == Mul(4, S.One / 2)\n        assert S.One / 2 * 4 == Mul(S.One / 2, 4)\n\n        assert sqrt(2) * sqrt(2) == Pow(2, S.One / 2) * Pow(2, S.One / 2)\n"], "sample_268": ["def test_check_server_status_with_inner_exception(self):\n    class TestException(Exception):\n        pass\n\n    with mock.patch.object(self.reloader.client, 'query') as mocked_query:\n        mocked_query.side_effect = TestException()\n        with self.assertRaises(autoreload.WatchmanUnavailable):\n            self.reloader.check_server_status(TestException())\n", "    def test_notify_file_changed(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.utils.autoreload.file_changed.send') as mocked_send:\n            reloader.notify_file_changed('test.py')\n            self.assertEqual(mocked_send.call_count, 1)\n            self.assertEqual(mocked_send.call_args[1]['file_path'], 'test.py')\n", "    def test_notify_file_changed(self):\n        reloader = autoreload.BaseReloader()\n        test_file = Path(__file__)\n        with mock.patch.object(autoreload, 'file_changed') as mock_file_changed:\n            mock_file_changed.return_value = [(mock.Mock(), True)]\n            reloader.notify_file_changed(test_file)\n            mock_file_changed.send.assert_called_once_with(sender=reloader, file_path=test_file)\n\n        with mock.patch.object(autoreload, 'file_changed') as mock_file_changed:\n            mock_file_changed.return_value = [(mock.Mock(), False)]\n            with mock.patch.object(autoreload, 'trigger_reload') as mock_trigger_reload:\n                reloader.notify_file_changed(test_file)\n                mock_file_changed.send.assert_called_once_with(sender=reloader, file_path=test_file)\n                mock_trigger_reload.assert_called_once_with(test_file)\n", "def test_glob_nested_directory(self, mocked_modules, notify_mock):\n    inner_py_file = self.ensure_file(self.tempdir / 'dir' / 'inner_dir' / 'file.py')\n    self.reloader.watch_dir(self.tempdir, '**/*.py')\n    with self.tick_twice():\n        self.increment_mtime(inner_py_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [inner_py_file])\n", "def test_tick_triggers_on_file_change(self, mock_notify_file_changed):\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        self.increment_mtime(self.existing_file)\n        next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n"], "sample_63": ["def test_app_dirs_with_loaders(self):\n    msg = \"app_dirs must not be set when loaders is defined.\"\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        Engine.get_default()\n", "    def test_template_loaders_configuration(self):\n        loaders = [\n            ('django.template.loaders.cached.Loader', [\n                'django.template.loaders.filesystem.Loader',\n            ]),\n        ]\n        engine = Engine(dirs=[TEMPLATE_DIR], loaders=loaders)\n        self.assertEqual(len(engine.template_loaders), 1)\n        self.assertIsInstance(engine.template_loaders[0], CachedLoader)\n        self.assertEqual(len(engine.template_loaders[0].loaders), 1)\n        self.assertIsInstance(engine.template_loaders[0].loaders[0], FilesystemLoader)\n", "def test_custom_libraries_configured(self):\n    self.assertIn('custom_library', Engine.get_default().template_libraries)\n", "def test_app_dirs_with_loaders(self):\n    msg = 'app_dirs must not be set when loaders is defined.'\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        Engine.get_default()\n", "    def test_additional_builtins(self):\n        builtins = ['django.template.defaulttags', 'path.to.custom.builtins']\n        engine = Engine(dirs=[TEMPLATE_DIR], builtins=builtins)\n        # Add a test that verifies the custom builtins are loaded correctly\n        # This could involve rendering a template that uses the custom builtins\n"], "sample_1039": ["def test_presentation_mathml_matrix_symbol():\n    A = MatrixSymbol('A', 1, 2)\n    mml = mpp._print(A)\n    assert mml.nodeName == 'mi'\n    assert mml.childNodes[0].nodeValue == 'A'\n    assert mml.getAttribute('mathvariant') == ''\n\n    mml = mpp._print(A, mat_symbol_style=\"bold\")\n    assert mml.nodeName == 'mi'\n    assert mml.childNodes[0].nodeValue == 'A'\n    assert mml.getAttribute('mathvariant') == 'bold'\n", "def test_print_random_symbol():\n    X = RandomSymbol('X', Real)\n    assert mpp.doprint(X) == '<mi>X</mi>'\n    assert mp.doprint(X) == '<ci>X</ci>'\n    assert mathml(X, printer='presentation', mat_symbol_style=\"bold\" )== '<mi mathvariant=\"bold\">X</mi>'\n    assert mathml(X, mat_symbol_style=\"bold\" )== '<ci>X</ci>' # No effect in content printer\n", "def test_presentation_mathml_negative_infinity():\n    mml = mpp._print(-oo)\n    assert mml.nodeName == 'mrow'\n    nodes = mml.childNodes\n    assert len(nodes) == 2\n    assert nodes[0].nodeName == 'mo'\n    assert nodes[0].childNodes[0].nodeValue == '-'\n    assert nodes[1].nodeName == 'mi'\n    assert nodes[1].childNodes[0].nodeValue == '&#x221E;'\n", "def test_presentation_mathml_matrix_symbol():\n    A = MatrixSymbol('A', 1, 2)\n    mml = mpp._print(A)\n    assert mml.nodeName == 'mi'\n    assert mml.childNodes[0].nodeValue == 'A'\n\n    mml = mpp._print(A, mat_symbol_style=\"bold\")\n    assert mml.nodeName == 'mi'\n    assert mml.childNodes[0].nodeValue == 'A'\n    assert mml.getAttribute('mathvariant') == 'bold'\n\n    mml = mp._print(A)\n    assert mml.nodeName == 'ci'\n    assert mml.childNodes[0].nodeValue == 'A'\n\n    mml = mp._print(A, mat_symbol_style=\"bold\")\n    assert mml.nodeName == 'ci'\n    assert mml.childNodes[0].nodeValue == 'A'  # No effect in content printer\n", "def test_presentation_mathml_matrix_symbol():\n    A = MatrixSymbol('A', 1, 2)\n    mml = mpp._print(A)\n    assert mml.nodeName == 'mi'\n    assert mml.childNodes[0].nodeValue == 'A'\n    assert mml.getAttribute('mathvariant') == ''\n\n    mpp_bold = MathMLPresentationPrinter({'mat_symbol_style': 'bold'})\n    mml_bold = mpp_bold._print(A)\n    assert mml_bold.nodeName == 'mi'\n    assert mml_bold.childNodes[0].nodeValue == 'A'\n    assert mml_bold.getAttribute('mathvariant') == 'bold'\n"], "sample_361": ["def test_urlize_nofollow(self):\n    value = 'Check out this link: example.com'\n    output = 'Check out this link: <a href=\"http://example.com\" rel=\"nofollow\">example.com</a>'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_nofollow(self):\n    value = 'Visit https://example.com for more information.'\n    output = 'Visit <a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a> for more information.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "    def test_urlize_trailing_punctuation(self):\n        tests = (\n            ('Search for google.com!', 'Search for <a href=\"http://google.com/\">google.com</a>!'),\n            ('Search for google.com.', 'Search for <a href=\"http://google.com/\">google.com</a>.'),\n            ('Search for google.com,', 'Search for <a href=\"http://google.com/\">google.com</a>,'),\n            ('Search for google.com:', 'Search for <a href=\"http://google.com/\">google.com</a>:'),\n            ('Search for google.com;', 'Search for <a href=\"http://google.com/\">google.com</a>;'),\n            ('Search for google.com/?q=1!', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>!'),\n            ('Search for google.com/?q=1.', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>.'),\n            ('Search for google.com/?q=1,', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>,'),\n            ('Search for google.com/?q=1:', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>:'),\n            ('Search for google.com/?q=1;', 'Search for <a href=\"http://google.com/?q=1\">google.com/?q=1</a>;'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), output)\n", "def test_urlize_truncate_url(self):\n    tests = (\n        ('Search for averylongurl.com/?q=1234567890 and see.', 10, 'Search for <a href=\"http://averylongurl.com/?q=1234567890\">averylongu...</a> and see.'),\n        (lazystr('Search for averylongurl.com/?q=1234567890'), 15, 'Search for <a href=\"http://averylongurl.com/?q=1234567890\">averylongurl...</a>'),\n    )\n    for value, trim_url_limit, output in tests:\n        with self.subTest(value=value, trim_url_limit=trim_url_limit):\n            self.assertEqual(urlize(value, trim_url_limit=trim_url_limit), output)\n", "def test_urlize_truncate_urls(self):\n    # Test that URLs are truncated to a specified limit and appended with an ellipsis\n    tests = (\n        (\n            'Visit https://www.example.com/very-long-path for more information.',\n            10,\n            'Visit <a href=\"https://www.example.com/very-long-path\">https://\u2026</a> for more information.'\n        ),\n        (\n            'Check out www.example.com/another-long-path/file.html?param=value#section for more.',\n            30,\n            'Check out <a href=\"http://www.example.com/another-long-path/file.html?param=value#section\">'\n            'www.example.com/another-long-pa\u2026</a> for more.'\n        ),\n        (\n            'Email me at user@example.com if you have questions.',\n            10,\n            'Email me at <a href=\"mailto:user@example.com\">user@example.com</a> if you have questions.'\n        ),\n    )\n    for value, limit, output in tests:\n        with self.subTest(value=value, limit=limit):\n            self.assertEqual(urlize(value, trim_url_limit=limit), output)\n"], "sample_240": ["def test_token_with_different_password(self):\n    \"\"\"A valid token can be created with a different password.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    user.set_password('newpassword')\n    user.save()\n    p0 = PasswordResetTokenGenerator()\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    user.set_password('testpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk0), False)\n", "    def test_token_with_different_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with a different algorithm by using the\n    PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'sha512'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n", "    def test_token_with_different_algorithm(self):\n        \"\"\"\n        A valid token can be created with a hashing algorithm other than\n        the default by using the PasswordResetTokenGenerator.algorithm attribute.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        new_algorithm = 'sha512'\n        # Create and check a token with a different hashing algorithm.\n        p0 = PasswordResetTokenGenerator()\n        p0.algorithm = new_algorithm\n        tk0 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk0), True)\n        # Create and check a token with the default hashing algorithm.\n        p1 = PasswordResetTokenGenerator()\n        self.assertNotEqual(p1.algorithm, new_algorithm)\n        tk1 = p1.make_token(user)\n        # Tokens created with a different hashing algorithm don't validate.\n        self.assertIs(p0.check_token(user, tk1), False)\n        self.assertIs(p1.check_token(user, tk0), False)\n"], "sample_651": ["def test_warns_context_manager_with_invalid_arguments(self) -> None:\n    with pytest.raises(TypeError) as excinfo:\n        with pytest.warns(\"UserWarning\"):  # type: ignore\n            pass\n    assert \"exceptions must be derived from Warning\" in str(excinfo.value)\n", "def test_re_emit_non_match_multiple(self) -> None:\n    with pytest.warns(UserWarning, match=\"v1 warning\"):\n        with pytest.warns(UserWarning, match=\"v2 warning\"):\n            warnings.warn(\"v1 warning\", UserWarning)\n            warnings.warn(\"v2 warning\", UserWarning)\n            warnings.warn(\"non-matching v3 warning\", UserWarning)\n", "def test_warns_context_manager_with_match_and_no_warning(self) -> None:\n    with pytest.raises(pytest.fail.Exception):\n        with pytest.warns(UserWarning, match=\"user warning\"):\n            warnings.warn(\"no warning\", RuntimeWarning)\n", "def test_warns_context_manager_with_match_regex(self) -> None:\n    with pytest.warns(UserWarning, match=r\"warning \\d+\"):\n        warnings.warn(\"warning 1\", UserWarning)\n        warnings.warn(\"warning 2\", UserWarning)\n\n    with pytest.raises(pytest.fail.Exception, match=\"DID NOT WARN\"):\n        with pytest.warns(UserWarning, match=r\"warning \\d+\"):\n            warnings.warn(\"non-matching warning\", UserWarning)\n", "def test_warns_context_manager_with_incorrect_arguments(self) -> None:\n    with pytest.raises(TypeError) as excinfo:\n        with pytest.warns(UserWarning, \"invalid_argument\"):  # type: ignore\n            pass\n    assert \"positional arguments\" in str(excinfo.value)\n"], "sample_112": ["def test_cell_count(self):\n    \"\"\"\n    cell_count filter should count the number of cells used in a tabular inline.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:admin_views_question_change', args=[self.question.pk]))\n    request.user = self.superuser\n    admin = ModelAdmin(Question, site)\n    response = admin.change_view(request, str(self.question.pk))\n    inline_admin_formset = response.context_data['inline_admin_formsets'][0]\n    inline_admin_form = inline_admin_formset.forms[0]\n    cell_count_result = cell_count(inline_admin_form)\n    expected_count = 1 + len(inline_admin_form.fieldsets) + int(inline_admin_formset.can_delete)\n    self.assertEqual(cell_count_result, expected_count)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should create prepopulated_fields_json correctly.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[self.article.pk]))\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.change_view(request, str(self.article.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    prepopulated_fields_json = template_context['prepopulated_fields_json']\n    self.assertIsInstance(prepopulated_fields_json, str)\n    prepopulated_fields_json = json.loads(prepopulated_fields_json)\n    self.assertIsInstance(prepopulated_fields_json, list)\n    self.assertEqual(len(prepopulated_fields_json), len(self.article._meta.prepopulated_fields))\n    for field, field_json in zip(self.article._meta.prepopulated_fields, prepopulated_fields_json):\n        self.assertEqual(field['field'].name, field_json['name'])\n        self.assertEqual([\"#%s\" % dependency.auto_id for dependency in field[\"dependencies\"]], field_json['dependency_ids'])\n        self.assertEqual([dependency.name for dependency in field[\"dependencies\"]], field_json['dependency_list'])\n", "def test_cell_count(self):\n    \"\"\"\n    cell_count template filter should return the correct number of cells for an inline form.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[1]))\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.change_view(request, '1')\n\n    inline_admin_form = response.context_data['inline_admin_formsets'][0].form\n    expected_count = 2  # Hidden cell with hidden 'id' field and one field in the form\n    self.assertEqual(cell_count(inline_admin_form), expected_count)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should create a JSON string of prepopulated fields data.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:admin_views_article_add'))\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.add_view(request)\n    prepopulated_fields_js(response.context_data)\n    prepopulated_fields_json = response.context_data['prepopulated_fields_json']\n    self.assertIsInstance(prepopulated_fields_json, str)\n    prepopulated_fields = json.loads(prepopulated_fields_json)\n    self.assertIsInstance(prepopulated_fields, list)\n    for field in prepopulated_fields:\n        self.assertIn('id', field)\n        self.assertIn('name', field)\n        self.assertIn('dependency_ids', field)\n        self.assertIn('dependency_list', field)\n        self.assertIn('maxLength', field)\n        self.assertIn('allowUnicode', field)\n", "    def test_cell_count(self):\n        \"\"\"\n        Test cell_count function for tabular inlines.\n        \"\"\"\n        from django.contrib.admin import TabularInline\n\n        class QuestionInline(TabularInline):\n            model = Question\n\n        request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[self.article.pk]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        admin.inlines = [QuestionInline]\n        response = admin.change_view(request, str(self.article.pk))\n        inline_admin_form = response.context_data['inline_admin_formsets'][0].forms[0]\n        count = cell_count(inline_admin_form)\n        self.assertEqual(count, 2)  # Hidden cell with hidden 'id' field + 1 Question field\n"], "sample_900": ["def test_invscaling_learning_rate():\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n    clf = MLPClassifier(tol=0.5, max_iter=3000, solver='sgd', learning_rate='invscaling')\n    clf.fit(X, y)\n    assert clf.max_iter > clf.n_iter_\n    assert clf.learning_rate_init / pow(clf.n_iter_, clf.power_t) == clf._optimizer.learning_rate\n", "def test_hidden_layer_sizes_input_types():\n    # Test that hidden_layer_sizes can be a single integer or a tuple of integers\n    X = [[3, 2], [1, 6], [5, 6], [-2, -4]]\n    y = [1, 1, 1, 0]\n    mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=4, max_iter=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp.fit(X, y)\n\n    mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=(4,), max_iter=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp.fit(X, y)\n", "def test_invscaling_learning_rate():\n    # Test invscaling learning rate.\n    # It should decrease the learning rate according to the inverse scaling formula.\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n    power_t = 0.5\n    max_iter = 5\n    clf = MLPClassifier(max_iter=max_iter, solver='sgd', learning_rate='invscaling',\n                        power_t=power_t)\n    clf.fit(X, y)\n    learning_rates = [clf.learning_rate_init / pow(i + 1, power_t) for i in range(max_iter)]\n    assert_array_equal(clf.loss_curve_, learning_rates)\n", "def test_learning_rate_adaptive():\n    # Tests the adaptive learning rate for sgd solver\n    X = [[3, 2], [1, 6], [5, 6], [-2, -4]]\n    y = [1, 1, 1, 0]\n    for learning_rate in [\"invscaling\", \"constant\"]:\n        mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=4,\n                            learning_rate=learning_rate, max_iter=5,\n                            power_t=0.25, warm_start=True, tol=1e-10)\n        with ignore_warnings(category=ConvergenceWarning):\n            mlp.fit(X, y)\n            prev_eta = mlp._optimizer.learning_rate\n            prev_loss = mlp.loss_\n            mlp.fit(X, y)\n            post_eta = mlp._optimizer.learning_rate\n            post_loss = mlp.loss_\n\n        if learning_rate == 'constant':\n            assert prev_eta == post_eta\n            assert prev_loss > post_loss\n        elif learning_rate == 'invscaling':\n            assert (mlp.learning_rate_init / pow(8 + 5, mlp.power_t) ==\n                         post_eta)\n            assert prev_loss > post_loss\n", "def test_adaptive_learning_rate_regression(X, y):\n    # Test adaptive learning rate in regression\n    mlp = MLPRegressor(solver='sgd', learning_rate='adaptive', max_iter=3000, tol=0.01, random_state=1)\n    mlp.fit(X, y)\n    assert mlp.max_iter > mlp.n_iter_\n    assert 1e-6 > mlp._optimizer.learning_rate\n"], "sample_533": ["def test_contour_invalid_algorithm():\n    x = np.arange(10)\n    y = np.arange(9)\n    x, y = np.meshgrid(x, y)\n    z = np.random.random((9, 10))\n\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError):\n        ax.contour(x, y, z, algorithm='invalid')\n", "def test_contourf_with_hatches():\n    # Test that contourf with hatches is drawn correctly\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    cs = ax.contourf(data, hatches=['//', '\\\\\\\\', '||', '--'])\n    assert len(cs.collections) == 4\n    for collection, hatch in zip(cs.collections, ['//', '\\\\\\\\', '||', '--']):\n        assert collection.get_hatch() == hatch\n", "def test_inline_spacing():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n    for ax, inline_spacing in zip(axs, [0, 5, 10]):\n        CS = ax.contour(X, Y, Z, 6, colors='k')\n        ax.clabel(CS, inline=True, inline_spacing=inline_spacing)\n        ax.set_title(f'Inline spacing = {inline_spacing}')\n", "def test_contour_extend_min():\n    x = np.arange(10)\n    y = np.arange(9)\n    x, y = np.meshgrid(x, y)\n    z = np.sin(x) * np.cos(y)\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(x, y, z, extend='min')\n    assert cs.extend == 'min'\n    assert cs.norm.clip is False\n\n    # Check that the under value is used for values below the minimum level\n    cmin = cs.norm(cs.levels[0])\n    cmap = cs.cmap\n    assert np.allclose(cmap(cmin), cmap.get_under())\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, extend='min')\n    assert cs.extend == 'min'\n    assert cs.norm.clip is False\n", "def test_contour_log_min_zero():\n    # Test that contour handles the case where z contains zeros but log scale is used\n    z = np.array([[0.0, 0.0], [1.0, 1.0]])\n    fig, ax = plt.subplots()\n    with pytest.warns(UserWarning, match=\"Log scale: values of z <= 0 have been masked\"):\n        cs = ax.contour(z, levels=np.logspace(-1, 1, 3), norm=LogNorm())\n    # Check that only the positive values are contoured\n    assert_array_almost_equal(cs.levels, [0.1, 1.0])\n"], "sample_578": ["def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == baseline + y[i]\n", "def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == baseline + y[i]\n", "def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == pytest.approx(baseline)\n        assert verts[3, 1] == pytest.approx(y[i] + baseline)\n", "def test_baseline_position(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == baseline + y[i]\n", "def test_baseline_mapped(self, x, y):\n\n    baseline = np.array([2, 3, 4, 5, 6])\n    p = Plot(x, y, baseline=baseline).add(Bar()).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == pytest.approx(baseline[i])\n"], "sample_126": ["def test_remove_blank_textfield_and_charfield(self):\n    \"\"\"\n    #23405 - Removing a NOT NULL and blank `CharField` or `TextField`\n    without default should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_with_biography_blank], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveField\", \"RemoveField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"biography\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"nickname\")\n", "def test_add_blank_datetime_field(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Adding a NOT NULL and blank `DateTimeField` without default\n    should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_birthdate_blank])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"birthdate\")\n", "def test_alter_fk_on_delete(self):\n    \"\"\"\n    #23422 - Altering on_delete on a ForeignKey should work.\n    \"\"\"\n    changes = self.get_changes([self.author_with_publisher], [self.author_with_publisher_set_null])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"publisher\")\n    self.assertEqual(changes['testapp'][0].operations[0].field.remote_field.on_delete, models.SET_NULL)\n", "def test_add_field_with_db_column(self):\n    \"\"\"\n    #24138 - Adding a field with db_column set should not remove and re-add\n    the field.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_name_db_column])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, db_column=\"author_name\")\n", "def test_add_nullable_textfield_and_charfield(self):\n    \"\"\"\n    #23405 - Adding a NULL `CharField` or `TextField`\n    should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_biography_nullable])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n"], "sample_262": ["def test_lazy_modulo(self):\n    \"\"\"\n    % works correctly for Promises.\n    \"\"\"\n    lazy_obj = lazy(lambda: 'abc', str)\n    self.assertEqual(lazy_obj() % 'def', 'abcdef')\n", "def test_classproperty(self):\n    \"\"\"classproperty works correctly.\"\"\"\n    class Foo:\n        @classproperty\n            return 123\n\n    self.assertEqual(Foo.bar, 123)\n", "def test_classproperty_without_getter(self):\n    class Foo:\n        @classproperty\n            return 123\n\n    self.assertEqual(Foo.bar, 123)\n    foo = Foo()\n    self.assertEqual(foo.bar, 123)\n", "    def test_classproperty_multiple_getters(self):\n        class Foo:\n            foo_attr = 123\n\n            @classproperty\n                return cls.foo_attr\n\n            @foo.getter\n                return 456\n\n        with self.assertRaises(AssertionError):\n            Foo.foo\n", "    def test_lazy_comparison(self):\n        \"\"\"\n        Comparisons work correctly for Promises.\n        \"\"\"\n        lazy_a = lazy(lambda: 4, int)\n        lazy_b = lazy(lambda: 4, int)\n        lazy_c = lazy(lambda: 5, int)\n\n        self.assertTrue(lazy_a() == lazy_b())\n        self.assertTrue(lazy_a() < lazy_c())\n        self.assertTrue(lazy_c() > lazy_a())\n        self.assertTrue(lazy_a() != lazy_c())\n        self.assertTrue(lazy_a() <= lazy_b())\n        self.assertTrue(lazy_c() >= lazy_b())\n"], "sample_148": ["def test_display_for_value_boolean(self):\n    self.assertEqual(\n        display_for_value(True, self.empty_value, boolean=True),\n        '<img src=\"/static/admin/img/icon-yes.svg\" alt=\"True\">'\n    )\n    self.assertEqual(\n        display_for_value(False, self.empty_value, boolean=True),\n        '<img src=\"/static/admin/img/icon-no.svg\" alt=\"False\">'\n    )\n", "def test_quote_and_unquote(self):\n    \"\"\"\n    Test quote and unquote functions\n    \"\"\"\n    s = 'test_string:/?_#@&='\n    quoted_s = quote(s)\n    unquoted_s = unquote(quoted_s)\n    self.assertEqual(unquoted_s, s)\n", "def test_display_for_field_file(self):\n    \"\"\"\n    Test display_for_field with FileField\n    \"\"\"\n    test_file = SimpleUploadedFile(\"test_file.txt\", b\"file_content\")\n    display_value = display_for_field(test_file, models.FileField(), self.empty_value)\n    expected = '<a href=\"%stest_file.txt\">test_file.txt</a>' % settings.MEDIA_URL\n    self.assertHTMLEqual(display_value, expected)\n", "def test_quote_and_unquote(self):\n    \"\"\"\n    Test quote and unquote functions.\n    \"\"\"\n    test_string = 'test_string:with_special/characters_and_numbers123'\n    quoted_string = quote(test_string)\n    unquoted_string = unquote(quoted_string)\n    self.assertEqual(unquoted_string, test_string)\n", "def test_quote_unquote(self):\n    \"\"\"\n    Tests for quote() and unquote() functions.\n    \"\"\"\n    self.assertEqual(quote(\"foo\"), \"foo\")\n    self.assertEqual(quote(\":/_#?;@&=+$,\"), \"_3A__2F__5F__23__3F__40__26__3D__2B__24__2C\")\n    self.assertEqual(quote(\"[]<>%\\n\\\\'\"), \"_5B__5D__3C__3E__25__0A__5C__27\")\n\n    self.assertEqual(unquote(\"foo\"), \"foo\")\n    self.assertEqual(unquote(\"_3A__2F__5F__23__3F__40__26__3D__2B__24__2C\"), \":/_#?;@&=+$,\")\n    self.assertEqual(unquote(\"_5B__5D__3C__3E__25__0A__5C__27\"), \"[]<>%\\n\\\\'\")\n"], "sample_722": ["def test_k_means_small_batch_size():\n    # Test with a batch size smaller than the number of samples\n    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10, random_state=42)\n    mb_k_means.fit(X)\n    _check_fitted_model(mb_k_means)\n", "def test_minibatch_inertia():\n    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10, random_state=42)\n    mb_k_means.fit(X)\n    inertia = mb_k_means.inertia_\n    assert_greater(inertia, 0.0)\n", "def test_k_means_with_small_cluster_count():\n    # Check that an error is raised when n_clusters is greater than n_samples\n    X_small = np.array([[1.1, 1.1], [-7.5, -7.5], [-1.1, -1.1], [7.5, 7.5]])\n    assert_raises(ValueError, KMeans, n_clusters=5).fit, X_small\n", "def test_minibatch_k_means_init_multiple_runs_with_explicit_centers_sparse():\n    mb_k_means = MiniBatchKMeans(init=centers.copy(), n_clusters=n_clusters,\n                                 random_state=42, n_init=10)\n    assert_warns(RuntimeWarning, mb_k_means.fit, X_csr)\n", "def test_k_means_single_init_multiple_runs_with_explicit_centers():\n    km = KMeans(init=centers.copy(), n_clusters=n_clusters, random_state=42, n_init=10)\n    assert_warns(RuntimeWarning, km.fit, X)\n"], "sample_989": ["def test_Float_numpy_to_float():\n    from sympy.utilities.pytest import skip\n    from sympy.external import import_module\n    np = import_module('numpy')\n    if not np:\n        skip('numpy not installed. Abort numpy tests.')\n\n        x = Float(npval)\n        y = Float(ratval, precision=x._prec)\n        assert abs((x - y)/y) < 2**(-(x._prec+1))\n\n    check_prec_and_relerr(np.float16(2)/3, S(2)/3)\n    check_prec_and_relerr(np.float32(2)/3, S(2)/3)\n    check_prec_and_relerr(np.float64(2)/3, S(2)/3)\n    # extended precision, on some arch/compilers:\n    x = np.longdouble(2)/3\n    check_prec_and_relerr(x, S(2)/3)\n", "def test_Rational_powers():\n    \"\"\"Test Rational._eval_power\"\"\"\n    # check infinity\n    assert Rational(1, 2) ** S.Infinity == 0\n    assert Rational(3, 2) ** S.Infinity == S.Infinity\n    assert Rational(-1, 2) ** S.Infinity == 0\n    assert Rational(-3, 2) ** S.Infinity == S.ComplexInfinity\n\n    # check Nan\n    assert Rational(3, 4) ** S.NaN == S.NaN\n    assert Rational(-2, 3) ** S.NaN == S.NaN\n\n    # exact roots on numerator\n    assert sqrt(Rational(4, 3)) == 2 * sqrt(3) / 3\n    assert Rational(4, 3) ** Rational(3, 2) == 8 * sqrt(3) / 9\n    assert sqrt(Rational(-4, 3)) == I * 2 * sqrt(3) / 3\n    assert Rational(-4, 3) ** Rational(3, 2) == - I * 8 * sqrt(3) / 9\n    assert Rational(27, 2) ** Rational(1, 3) == 3 * (2 ** Rational(2, 3)) / 2\n    assert Rational(5**3, 8**3) ** Rational(4, 3) == Rational(5**4, 8**4)\n\n    # exact root on denominator\n    assert sqrt(Rational(1, 4)) == Rational(1, 2)\n    assert sqrt(Rational(1, -4)) == I * Rational(1, 2)\n    assert sqrt(Rational(3, 4)) == sqrt(3) / 2\n    assert sqrt(Rational(3, -4)) == I * sqrt(3) / 2\n    assert Rational(5, 27) ** Rational(1, 3) == (5 ** Rational(1, 3)) / 3\n\n    # not exact roots\n    assert", "def test_Integer_subs():\n    x = Symbol('x')\n    assert Integer(3).subs(x, 2) == Integer(3)\n    assert Integer(3).subs(3, 2) == Integer(2)\n", "def test_Float_comparison_with_other_numbers():\n    # Test Float comparison with other Number instances\n    assert Float('1.0') == Integer(1)\n    assert Float('1.0') == Rational(1, 1)\n    assert Float('1.0') != Rational(2, 1)\n    assert Float('1.0') > Rational(0, 1)\n    assert Float('1.0') < Rational(2, 1)\n    assert Float('1.0') >= Rational(1, 1)\n    assert Float('1.0') <= Rational(1, 1)\n\n    # Test Float comparison with other non-Number instances\n    assert Float('1.0') != '1.0'\n    assert Float('1.0') != 1.0\n    assert Float('1.0') != None\n", "def test_Float_precision():\n    # Make sure Float inputs for keyword args work\n    assert Float('1.0', dps=Float(15))._prec == 53\n    assert Float('1.0', precision=Float(15))._prec == 15\n    assert type(Float('1.0', precision=Float(15))._prec) == int\n    assert sympify(srepr(Float('1.0', precision=15))) == Float('1.0', precision=15)\n"], "sample_695": ["def test_fs_collector_from_parent() -> None:\n    \"\"\"Test the from_parent method of the FSCollector class.\"\"\"\n    parent = nodes.Node.from_parent(None, name=\"root\", nodeid=\"\")\n    fspath = legacy_path(\"/path/to/test.py\")\n    collector = nodes.FSCollector.from_parent(parent, fspath=fspath)\n    assert collector.path == Path(\"/path/to/test.py\")\n    assert collector.name == \"test.py\"\n    assert collector.nodeid == \"test.py\"\n", "def test_node_addfinalizer(pytester: Pytester) -> None:\n    \"\"\"Test the addfinalizer method of the Node class.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            finalizers = []\n                finalizers.append(\"finalized\")\n            request.node.addfinalizer(finalizer)\n            assert finalizers == []\n            request.node.session._setupstate.finalize_phase()\n            assert finalizers == [\"finalized\"]\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_fs_collector_from_parent_disallowed_arguments() -> None:\n    with pytest.raises(TypeError, match=\"session is\"):\n        nodes.FSCollector.from_parent(None, session=None)  # type: ignore[arg-type]\n    with pytest.raises(TypeError, match=\"config is\"):\n        nodes.FSCollector.from_parent(None, config=None)  # type: ignore[arg-type]\n", "def test_node_getparent() -> None:\n    class TestCollector(nodes.Collector):\n            return []\n\n    class TestItem(nodes.Item):\n            pass\n\n    class TestSession(pytest.Session):\n            pass\n\n    collector = TestCollector(name=\"parent\", session=TestSession())\n    item = TestItem(name=\"child\", parent=collector)\n\n    assert item.getparent(TestCollector) == collector\n    assert item.getparent(nodes.Item) == item\n    assert item.getparent(nodes.Node) is None\n", "def test_fs_collector_from_parent_disallowed_arguments(tmp_path: Path) -> None:\n    with pytest.raises(TypeError, match=\"session is\"):\n        nodes.FSCollector.from_parent(None, session=None)  # type: ignore[arg-type]\n    with pytest.raises(TypeError, match=\"config is\"):\n        nodes.FSCollector.from_parent(None, config=None)  # type: ignore[arg-type]\n    with pytest.raises(TypeError, match=\"nodeid is\"):\n        nodes.FSCollector.from_parent(None, nodeid=None)  # type: ignore[arg-type]\n    with pytest.raises(TypeError, match=\"path is\"):\n        nodes.FSCollector.from_parent(None, path=tmp_path)  # type: ignore[arg-type]\n    with pytest.raises(TypeError, match=\"fspath is\"):\n        nodes.FSCollector.from_parent(None, fspath=str(tmp_path))  # type: ignore[arg-type]\n"], "sample_139": ["def test_get_edited_object_ids_with_prefix_without_dash(self):\n    a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n    Swallow.objects.create(origin='Swallow B', load=2, speed=2)\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '2',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '1000',\n        'form-0-uuid': str(a.pk),\n        'form-0-load': '10',\n        '_save': 'Save',\n    }\n    superuser = self._create_superuser('superuser')\n    self.client.force_login(superuser)\n    changelist_url = reverse('admin:admin_changelist_swallow_changelist')\n    m = SwallowAdmin(Swallow, custom_site)\n    request = self.factory.post(changelist_url, data=data)\n    pks = m._get_edited_object_pks(request, prefix='form')\n    self.assertEqual(pks, [str(a.pk)])\n", "def test_custom_list_display_method(self):\n    \"\"\"\n    Regression test for #13196: output of functions should be localized\n    in the changelist.\n    \"\"\"\n    event = Event.objects.create(date=datetime.date.today())\n    m = EventAdmin(Event, custom_site)\n    request = self._mocked_authenticated_request('/event/', self.superuser)\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_custom_list_display_values(event), {\n        'date_display': formats.localize(event.date),\n    })\n", "def test_ordering_with_empty_queryset(self):\n    m = BandAdmin(Band, custom_site)\n    request = self.factory.get('/band/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    cl.queryset = Band.objects.none()\n    cl.get_ordering_field_columns()\n    # No error should be raised, even though the queryset is empty\n", "def test_get_admin_log_with_user(self):\n    user = User(username='jondoe', password='secret', email='super@example.com')\n    user.save()\n    ct = ContentType.objects.get_for_model(User)\n    LogEntry.objects.log_action(user.pk, ct.pk, user.pk, repr(user), 1)\n    t = Template(\n        '{% load log %}'\n        '{% get_admin_log 100 as admin_log for_user user %}'\n        '{% for entry in admin_log %}'\n        '{{ entry|safe }}'\n        '{% endfor %}'\n    )\n    self.assertEqual(t.render(Context({'user': user})), 'Added \u201c<User: jondoe>\u201d.')\n", "def test_admin_site_with_custom_app_index_template(self):\n    \"\"\"AdminSite with custom app_index_template displays custom template.\"\"\"\n    custom_site = admin.AdminSite(name='custom_admin')\n    custom_site.app_index_template = 'admin_changelist/custom_app_index.html'\n    custom_site.register(Group, GroupAdmin)\n\n    self.client.login(username='super', password='secret')\n    response = self.client.get('/custom_admin/')\n\n    self.assertTemplateUsed(response, 'admin_changelist/custom_app_index.html')\n"], "sample_938": ["def test_footnotes_in_manpage(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert 'Footnotes\\n--------' in content\n    assert r'\\[1] This is a footnote.' in content\n", "def test_admonition_directive(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert '.SH \"NOTE\"' in content\n    assert '.INDENT 0.0' in content\n    assert '.INDENT 3.5' in content\n    assert 'This is a note' in content\n    assert '.UNINDENT' in content\n", "def test_multiple_authors(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert r'\\fBAuthor\\fR\\n' in content\n    assert 'Author 1\\nAuthor 2\\n' in content\n", "def test_figure_directive(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert ('\\n.IP \\\\fBtest\\\\fP 0\\n' in content and\n            '.sp\\n'\n            '.in +0.5i\\n'\n            '.sp\\n'\n            '.nf\\n'\n            '.ft C\\n'\n            'import antigravity\\n'\n            '.ft P\\n'\n            '.fi\\n'\n            '.sp\\n'\n            '.in -0.5i\\n' in content)\n", "def test_footnotes_in_manpage(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Check if footnote is included in the content\n    assert 'Footnotes' in content\n    assert 'This is a footnote.' in content\n"], "sample_7": ["def test_column_with_non_string_name():\n    \"\"\"Test that a column can be created with a non-string name.\"\"\"\n    c = table.Column(data=[1, 2, 3], name=123)\n    assert c.name == '123'\n", "def test_column_searchsorted():\n    \"\"\"\n    Test the searchsorted method of Column.\n    \"\"\"\n    c = table.Column([b'abc', b'def', b'ghi'])\n    assert c.searchsorted(b'def') == 1\n    assert np.all(c.searchsorted([b'abc', b'ghi']) == [0, 2])\n    assert c.searchsorted(b'xyz', side='right') == 3\n", "def test_masked_column_dtype_conversion():\n    mc = table.MaskedColumn([1., 2., 3.], mask=[True, False, True])\n    mc_int = mc.astype(int)\n    assert mc_int.dtype == int\n    assert mc_int.data.dtype == int\n    assert mc_int.mask.dtype == bool\n    assert np.all(mc_int.data.mask == mc.mask)\n", "def test_column_copy_with_indices():\n    \"\"\"\n    Test that the copy method of a column also copies the indices.\n    \"\"\"\n    data = [1, 2, 3]\n    indices = ['a', 'b', 'c']\n    col = table.Column(data=data, name='test')\n    col.info.indices = indices\n\n    col_copy = col.copy()\n    assert col_copy.info.indices == indices\n    assert col_copy.info.indices is not indices\n", "def test_column_unit_conversion_on_init(Column):\n    \"\"\"Test that the unit of a column is correctly converted to a Quantity when initializing\"\"\"\n    data = np.array([1, 2, 3])\n    c = Column(data=data, unit='m')\n    assert isinstance(c.quantity, u.Quantity)\n    assert np.all(c.quantity.value == data)\n    assert c.quantity.unit == u.m\n"], "sample_503": ["def test_get_set_markerfacecoloralt():\n    line = mlines.Line2D([], [])\n    line.set_markerfacecoloralt('green')\n    assert line.get_markerfacecoloralt() == 'green'\n    line.set_markerfacecoloralt(None)\n    assert line.get_markerfacecoloralt() == 'auto'\n", "def test_custom_capstyle():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], linestyle='--', dash_capstyle='round')\n    ax.plot([0, 1], [1, 0], linestyle='--', dash_capstyle='projecting')\n", "def test_line_dashes_with_custom_offset():\n    fig, ax = plt.subplots()\n\n    ax.plot(range(10), linestyle=(2, (3, 3)), lw=5)\n\n    # Check if the dash offset is set correctly\n    assert ax.lines[0].get_dashes() == (2, [3, 3])\n", "def test_clip_path_transform(fig_test, fig_ref):\n    fig_test.subplots().plot([0, 1], [0, 1], clip_path=Path([[0.2, 0.2], [0.8, 0.8]]),\n                             transform=mtransforms.Affine2D().translate(0.5, 0.5))\n    fig_ref.subplots().plot([0.2, 0.8], [0.2, 0.8], clip_path=Path([[0.2, 0.2], [0.8, 0.8]]))\n", "def test_capstyle_joinstyle(fig_test, fig_ref):\n    fig_test.add_subplot().plot([1, 2], dash_capstyle='round', dash_joinstyle='round')\n    fig_ref.add_subplot().plot([1, 2], dash_capstyle='round', dash_joinstyle='round')\n"], "sample_632": ["def test_ignore_signatures_and_imports():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", \"--ignore-imports\", SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_docstrings_multiline():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-docstrings\", SIMILAR3, SIMILAR4])\n    assert ex.value.code == 0\n    assert \"TOTAL lines=50 duplicates=0 percent=0.00\" in output.getvalue()\n", "def test_ignore_signatures_with_async():\n    output = StringIO()\n    async_file1 = str(INPUT / \"similar_async_func_1.py\")\n    async_file2 = str(INPUT / \"similar_async_func_2.py\")\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([async_file1, async_file2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_signatures_empty_functions_fail_with_different_imports():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            '''", "def test_ignore_signatures_docstrings_fail():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-docstrings\", SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            '''"], "sample_372": ["    def test_include_lazy(self):\n        lazy_include = include(self.url_patterns)\n        self.assertIsInstance(lazy_include, tuple)\n        self.assertEqual(lazy_include, (self.url_patterns, None, None))\n", "    def test_namespace_match(self):\n        tests = [\n            ('testapp:urlobject-view', 'test-ns1', '/default/inner/'),\n            ('inc-ns1:testapp:urlobject-view', 'inc-ns1:test-ns3', '/ns-included1/test3/inner/'),\n            ('inc-ns1:inc-ns4:inc-ns1:testapp:urlobject-view', 'inc-ns1:inc-ns4:inc-ns1:test-ns3', '/ns-included1/ns-included4/ns-included1/test3/inner/'),\n        ]\n        for name, namespace, expected in tests:\n            with self.subTest(name=name, namespace=namespace):\n                match = resolve(reverse(name))\n                self.assertEqual(match.namespace, namespace)\n", "    def test_nested_app_lookup_with_namespace(self):\n        \"\"\"\n        A nested current_app should be split in individual namespaces\n        when namespace is given (#24904).\n        \"\"\"\n        test_urls = [\n            ('inc-ns1:testapp:urlobject-view', [], {}, 'inc-ns1', '/ns-included1/test4/inner/'),\n            ('inc-ns1:testapp:urlobject-view', [37, 42], {}, 'inc-ns1', '/ns-included1/test4/inner/37/42/'),\n            ('inc-ns1:testapp:urlobject-view', [], {'arg1': 42, 'arg2': 37}, 'inc-ns1', '/ns-included1/test4/inner/42/37/'),\n            ('inc-ns1:testapp:urlobject-special-view', [], {}, 'inc-ns1', '/ns-included1/test4/inner/+%5C$*/'),\n            ('inc-ns1:testapp:urlobject-view', [], {}, 'inc-ns1:test-ns3', '/ns-included1/test3/inner/'),\n            ('inc-ns1:testapp:urlobject-view', [37, 42], {}, 'inc-ns1:test-ns3', '/ns-included1/test3/inner/37/42/'),\n            ('inc-ns1:testapp:urlobject-view', [], {'arg1': 42, 'arg2': 37}, 'inc-ns1:test-ns3', '/ns-included1/test3/inner/42/37/'),\n            ('inc-ns1:testapp:urlobject-special-view', [], {}, 'inc-ns1:test-ns3', '/ns-included1/test3/inner/+%5C$*/'),\n        ]\n        for name, args, kwargs, current_app", "    def test_valid_resolve(self):\n        test_urls = [\n            '/ends-with-slash/',\n        ]\n        for test_url in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).url_name, 'ends-with-slash')\n", "    def test_lookahead_resolve_extra_chars(self):\n        test_urls = [\n            '/lookahead+/a-city/extra/',\n            '/lookbehind+/a-city/extra/',\n            '/lookahead-/a-city/extra/',\n            '/lookbehind-/a-city/extra/',\n        ]\n        for test_url in test_urls:\n            with self.subTest(url=test_url):\n                with self.assertRaises(Resolver404):\n                    resolve(test_url)\n"], "sample_1046": ["def test_tensor_anti_symmetrization():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    H = tensorhead(\"H\", [L, L], [[2]])\n\n    expr = H(i, j) - H(j, i)\n    repl = {H(i, j): [[0, 1], [-1, 0]]}\n    assert expr.replace_with_arrays(repl, [i, j]) == Array([[0, 2], [-2, 0]])\n    assert expr.replace_with_arrays(repl, [j, i]) == Array([[0, -2], [2, 0]])\n", "def test_valued_tensor_indices():\n    (A, B, AB, BA, C, Lorentz, E, px, py, pz, LorentzD, mu0, mu1, mu2, ndm, n0, n1,\n     n2, NA, NB, NC, minkowski, ba_matrix, ndm_matrix, i0, i1, i2, i3, i4) = _get_valued_base_test_variables()\n\n    # test the indices of the valued tensors\n    assert A(i0).indices == (i0,)\n    assert B(i0).indices == (i0,)\n    assert AB(i0, i1).indices == (i0, i1)\n    assert BA(i0, i1).indices == (i0, i1)\n    assert C(mu0).indices == (mu0,)\n    assert NA(n0).indices == (n0,)\n    assert NB(n0, n1).indices == (n0, n1)\n    assert NC(n0, n1, n2).indices == (n0, n1, n2)\n", "compilation error", "def test_tensor_replacement_with_indices_not_in_expression():\n    L = TensorIndexType(\"L\")\n    i, j = tensor_indices(\"i j\", L)\n    A, B = tensorhead(\"A B\", [L], [[1]])\n\n    expr = A(i) * B(j)\n    repl = {A(i): [1, 2], B(j): [3, 4]}\n    assert expr.replace_with_arrays(repl, [i, j]) == Array([[3, 4], [6, 8]])\n\n    # Replacement indices not in expression\n    repl = {A(i): [1, 2], B(k): [3, 4]}  # k is not in expr\n    raises(ValueError, lambda: expr.replace_with_arrays(repl, [i, j]))\n", "def test_tensor_index_type_commutation():\n    L = TensorIndexType(\"L\")\n    i, j = tensor_indices(\"i j\", L)\n    A, B = tensorhead(\"A B\", [L], [[1]])\n\n    # Test commutation of indices\n    expr = A(i)*B(j)\n    assert expr.replace_with_arrays({A(i): [1, 2], B(j): [3, 4]}, [i, j]) == Array([[3, 4], [6, 8]])\n    assert expr.replace_with_arrays({A(i): [1, 2], B(j): [3, 4]}, [j, i]) == Array([[3, 6], [4, 8]])\n\n    # Test non-commutation of indices\n    L.comm = 1  # Set non-commutation\n    expr = A(i)*B(j)\n    assert expr.replace_with_arrays({A(i): [1, 2], B(j): [3, 4]}, [i, j]) == Array([[3, 6], [4, 8]])\n    assert expr.replace_with_arrays({A(i): [1, 2], B(j): [3, 4]}, [j, i]) == Array([[3, 4], [6, 8]])\n    L.comm = 0  # Reset commutation\n"], "sample_412": ["def test_urlize_nofollow(self):\n    value = \"Check out this link: example.com\"\n    output = 'Check out this link: <a href=\"http://example.com\" rel=\"nofollow\">example.com</a>'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_autoescape(self):\n    tests = (\n        (\n            \"Search for google.com/?q=! and see.\",\n            'Search for <a href=\"http://google.com/?q=&quot;\">google.com/?q=</a>! and '\n            'see.',\n        ),\n        (\n            \"foo@example.com\",\n            '<a href=\"mailto:foo@example.com\">foo@example.com</a>',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n", "def test_urlize_nofollow(self):\n    tests = (\n        (\n            \"Check out this site: google.com\",\n            'Check out this site: <a href=\"http://google.com\" rel=\"nofollow\">'\n            'google.com</a>',\n        ),\n        (\n            lazystr(\"Don't forget to visit example.org\"),\n            'Don\\'t forget to visit <a href=\"http://example.org\" rel=\"nofollow\">'\n            'example.org</a>',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_truncate_url(self):\n    value = \"Search for a very long url https://www.example.com/some/very/long/path/that/needs/to/be/truncated.html\"\n    output = 'Search for a very long url <a href=\"https://www.example.com/some/very/long/path/that/needs/to/be/truncated.html\">https://www.example.com/some/very/long/path/th...</a>'\n    self.assertEqual(urlize(value, trim_url_limit=40), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        (\"Search for google.com/?q=!.\", 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!.'),\n        (\"Search for google.com/?q=1&lt!.\", 'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>!.'),\n        (\"Search for google.com/?q=!,\", 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!,'),\n        (\"Search for google.com/?q=1&lt!,\", 'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>!,'),\n        (\"foo@example.com.\", '<a href=\"mailto:foo@example.com\">foo@example.com</a>.'),\n        (\"foo@example.com,\", '<a href=\"mailto:foo@example.com\">foo@example.com</a>,'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_408": ["def test_alter_field_with_db_column(self):\n    \"\"\"Changing a field's db_column should make a change.\"\"\"\n    changes = self.get_changes([self.author_name], [self.author_name_db_column])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"name\", db_column=\"author_name\"\n    )\n", "    def test_add_model_with_base_model(self):\n        before = [\n            ModelState(\n                \"app\",\n                \"base\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"app\",\n                \"base\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n            ModelState(\n                \"app\",\n                \"derived\",\n                [\n                    (\"description\", models.TextField()),\n                ],\n                bases=(\"app.base\",),\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"derived\")\n", "def test_alter_field_with_choices(self):\n    author_initial = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"gender\", models.CharField(max_length=10, choices=[(\"M\", \"Male\"), (\"F\", \"Female\")])),\n        ],\n    )\n    author_altered = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"gender\", models.CharField(max_length=10, choices=[(\"M\", \"Male\"), (\"F\", \"Female\"), (\"O\", \"Other\")])),\n        ],\n    )\n    changes = self.get_changes([author_initial], [author_altered])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        model_name=\"author\",\n        name=\"gender\",\n        field=author_altered.fields[1][1],\n    )\n", "def test_remove_indexes_and_constraints(self):\n    \"\"\"Test change detection of removed indexes and constraints.\"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.book_indexes_constraints],\n        [self.author_empty, self.book],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"RemoveIndex\", \"RemoveConstraint\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, model_name=\"book\", name=\"book_title_author_idx\"\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"book_title_contains_bob\"\n    )\n", "def test_add_field_with_db_column(self):\n    \"\"\"\n    #23527 - Adding a field with a db_column in an existing model should work.\n    \"\"\"\n    changes = self.get_changes([self.author_name], [self.author_name_with_db_column])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name_alias\")\n    self.assertEqual(changes[\"testapp\"][0].operations[0].field.db_column, \"name_column\")\n"], "sample_1178": ["def test_Token_subclassing():\n    class MyToken(Token):\n        __slots__ = ('a', 'b')\n\n    mt = MyToken(1, 2)\n    assert mt.a == 1\n    assert mt.b == 2\n    assert mt == MyToken(1, 2)\n    assert mt != MyToken(2, 1)\n    assert mt.func(*mt.args) == mt\n", "def test_ComplexType():\n    assert c64.cast_check(0.5 + 0.3j) == 0.5 + 0.3j\n    assert abs(c64.cast_check(3.7 + 1.2j) - (3.7 + 1.2j) - 4.9e-8) < 1e-8\n    assert c128.cast_check(Float('0.123456789012345670499') + 1e-20j) == Float('0.123456789012345670499') + 1e-20j\n    v19 = Float('0.1234567890123456749') + 1j*Float('0.1234567890123456749')\n    raises(ValueError, lambda: c128.cast_check(v19))\n", "def test_FunctionPrototype_and_FunctionDefinition_with_defaults():\n    vx = Variable(x, type=real, value=1.0)\n    vn = Variable(n, type=integer, value=2)\n    fp1 = FunctionPrototype(real, 'power', [vx, vn])\n    body = [Assignment(x, x**n), Return(x)]\n    fd1 = FunctionDefinition(real, 'power', [vx, vn], body)\n    fp2 = FunctionPrototype(real, 'power', [x, n], attrs=[value_const])\n    fd2 = FunctionDefinition(real, 'power', [x, n], body, attrs=[value_const])\n    assert fp2 == FunctionPrototype.from_FunctionDefinition(fd1)\n    assert fd2 == FunctionDefinition.from_FunctionPrototype(fp1, body)\n", "def test_ComplexType():\n    val16 = 123.456789049 + 0.123456789049j\n    raises(ValueError, lambda: c128.cast_check(0.12345678949 + 0.12345678949j))\n    assert abs(val16 - c128.cast_check(val16) - 4.9e-8) < 1e-8\n\n    dcm21 = Float('0.123456789012345670499') + 1e-20j  # 21 decimals\n    assert abs(dcm21 - c128.cast_check(dcm21) - 4.99e-19) < 1e-19\n    v19 = Float('0.1234567890123456749') + 1j*Float('0.1234567890123456749')\n    raises(ValueError, lambda: c128.cast_check(v19))\n\n    assert c64.cast_nocheck(1 + 1j) == 1 + 1j\n    assert c128.cast_nocheck(1 + 1j) == 1 + 1j\n", "def test_ComplexType():\n    z = 0.123456789012345678901 + 0.987654321098765432109j\n    z_c64 = c64.cast_check(z)\n    assert isinstance(z_c64, complex)\n    assert abs(z_c64.real - 0.123456789012345678901) < 1e-6\n    assert abs(z_c64.imag - 0.987654321098765432109) < 1e-6\n\n    z_c128 = c128.cast_check(z)\n    assert isinstance(z_c128, complex)\n    assert abs(z_c128.real - 0.123456789012345678901) < 1e-18\n    assert abs(z_c128.imag - 0.987654321098765432109) < 1e-18\n"], "sample_354": ["def test_validate_fk_via_option_non_interactive(self):\n    email = Email.objects.create(email='mymail@gmail.com')\n    Group.objects.all().delete()\n    nonexistent_group_id = 1\n    msg = f'group instance with id {nonexistent_group_id} does not exist.'\n\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=email.pk,\n            email=email.email,\n            group=nonexistent_group_id,\n            verbosity=0,\n        )\n", "def test_fields_with_fk_non_interactive(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username=email.pk,\n        email=email.email,\n        group=group.pk,\n        stdout=new_io,\n    )\n\n    command_output = new_io.getvalue().strip()\n    self.assertEqual(command_output, 'Superuser created successfully.')\n    u = CustomUserWithFK._default_manager.get(email=email)\n    self.assertEqual(u.username, email)\n    self.assertEqual(u.group, group)\n", "def test_fields_with_m2m_non_interactive(self):\n    new_io = StringIO()\n    org_id_1 = Organization.objects.create(name='Organization 1').pk\n    org_id_2 = Organization.objects.create(name='Organization 2').pk\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username='joe',\n        email='joe@example.com',\n        first_name='John',\n        date_of_birth='1980-01-01',\n        orgs=[org_id_1, org_id_2],\n        stdout=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertEqual(command_output, 'Superuser created successfully.')\n    user = CustomUser._default_manager.get(username='joe')\n    self.assertEqual(user.email, 'joe@example.com')\n    self.assertEqual(user.first_name, 'John')\n    self.assertEqual(user.date_of_birth, date(1980, 1, 1))\n    self.assertEqual(user.orgs.count(), 2)\n", "def test_create_superuser_with_database_option_and_fk(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username=email.pk,\n        email=email.email,\n        group=group.pk,\n        database='other',\n        stdout=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertEqual(command_output, 'Superuser created successfully.')\n    u = CustomUserWithFK._default_manager.using('other').get(email=email)\n    self.assertEqual(u.username, email)\n    self.assertEqual(u.group, group)\n", "def test_fields_with_fk_environment_variable_non_interactive(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    with mock.patch.dict(\n        os.environ,\n        {'DJANGO_SUPERUSER_USERNAME': str(email.pk), 'DJANGO_SUPERUSER_EMAIL': email.email, 'DJANGO_SUPERUSER_GROUP': str(group.pk)},\n    ):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = CustomUserWithFK._default_manager.get(email=email)\n        self.assertEqual(u.username, email)\n        self.assertEqual(u.group, group)\n"], "sample_789": ["def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work with sample_weights in the base estimator\n    The sample weighting is done internally in the _boost method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0])\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator())\n    clf.fit(X, y_class, sample_weight=np.ones(len(y_class)))\n    assert len(clf.estimators_) > 0\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work with sample_weights in the base estimator\n    The base estimator should support sample weighting\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0])\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator())\n    clf.fit(X, y_class, sample_weight=np.ones(len(y_class)))\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work with sample_weights in the base estimator\n    The weighted sampling is done internally in the _boost method in\n    AdaBoostClassifier, but the base estimator should also accept sample_weights\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            assert sample_weight is not None, \"sample_weight should not be None\"\n\n            return np.zeros(X.shape[0])\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator())\n    clf.fit(iris.data, iris.target)\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should raise a ValueError if the base estimator\n    does not support sample weights.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0])\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator())\n    with pytest.raises(ValueError):\n        clf.fit(X, y_class)\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work with sample_weights in the base estimator\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            assert sample_weight is not None\n            pass\n\n            return np.zeros(X.shape[0])\n\n    X, y = datasets.make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n    sample_weight = np.random.RandomState(42).rand(100)\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator(), random_state=42)\n    clf.fit(X, y, sample_weight=sample_weight)\n"], "sample_567": ["def test_text_antialiased_off_default_vs_manual_with_math(fig_test, fig_ref):\n    fig_test.text(0.5, 0.5, r\"OutsideMath $I\\'m \\sqrt{2}$\", antialiased=False)\n\n    mpl.rcParams['text.antialiased'] = False\n    fig_ref.text(0.5, 0.5, r\"OutsideMath $I\\'m \\sqrt{2}$\")\n", "def test_annotation_arrowstyle():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\n        \"hello\", xy=(.4, .4), xytext=(.6, .6), arrowprops={\"arrowstyle\": \"->\"})\n    fig.canvas.draw()\n    assert ann.arrow_patch.arrowstyle == \"->\"\n", "def test_set_rotation(rotation):\n    txt = Text(.5, .5, \"foo\")\n    txt.set_rotation(rotation)\n    if rotation is None:\n        assert txt._rotation == 0.\n    elif rotation == 'horizontal':\n        assert txt._rotation == 0.\n    elif rotation == 'vertical':\n        assert txt._rotation == 90.\n", "def test_annotation_rotation(rotation):\n    fig, ax = plt.subplots()\n    annot = ax.annotate(\"text\", xy=(0.5, 0.5), xytext=(0.6, 0.6), rotation=rotation)\n    fig.canvas.draw()\n    assert annot.get_rotation() == rotation\n", "def test_rotation_mode(rotation_mode, ha, va, text_rotation, expected_position):\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Text\", ha=ha, va=va, rotation=text_rotation, rotation_mode=rotation_mode)\n    fig.canvas.draw()\n    assert_almost_equal(text.get_position(), expected_position)\n"], "sample_543": ["def test_polygon_selector_clear(ax):\n    # Create a polygon\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.verts == verts\n\n    # Clear the polygon\n    do_event(tool, 'on_key_press', key='escape')\n    assert tool.verts == [(0, 0)]\n", "def test_polygon_selector_box_handle_props(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with custom box handle props\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_handle_props={'markersize': 10})\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that box handle props were set correctly\n    for handle in tool._box._corner_handles.artists:\n        assert handle.get_markersize() == 10\n", "def test_polygon_selector_set_handle_props(ax):\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n\n    tool.set_handle_props(markeredgecolor='r', alpha=0.3)\n    for artist in tool._handles_artists:\n        assert artist.get_markeredgecolor() == 'r'\n        assert artist.get_alpha() == 0.3\n", "def test_polygon_selector_box_props(ax, draw_bounding_box):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box,\n                                   box_props=dict(facecolor='yellow', alpha=0.5))\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check the box properties\n    box_artist = tool._box._selection_artist\n    assert box_artist.get_facecolor() == mcolors.to_rgba('yellow', alpha=0.5)\n", "def test_polygon_selector_ignore_outside(ax, draw_bounding_box):\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    tool = widgets.PolygonSelector(ax, onselect, draw_bounding_box=draw_bounding_box)\n    # Draw a polygon\n    event_sequence = [\n        *polygon_place_vertex(10, 10),\n        *polygon_place_vertex(100, 10),\n        *polygon_place_vertex(100, 100),\n        *polygon_place_vertex(10, 100),\n        *polygon_place_vertex(10, 10),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Press-release event outside the polygon to clear the polygon\n    do_event(tool, 'press', xdata=130, ydata=130, button=1)\n    do_event(tool, 'release', xdata=130, ydata=130, button=1)\n\n    assert onselect.call_count == 1\n    assert tool._selection_completed is False\n\n    # Set ignore_event_outside to True\n    tool.ignore_event_outside = True\n\n    # Press-release event outside the polygon ignored\n    do_event(tool, 'press', xdata=130, ydata=130, button=1)\n    do_event(tool, 'release', xdata=130, ydata=130, button=1)\n\n    assert onselect.call_count == 1\n    assert tool._selection_completed is False\n"], "sample_344": ["def test_abstract_model_children_constraints(self):\n    class Abstract(models.Model):\n        size = models.IntegerField()\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n            constraints = [models.CheckConstraint(check=models.Q(size__gt=1), name='size_gt_1')]\n\n    class Child1(Abstract):\n        pass\n\n    class Child2(Abstract):\n        pass\n\n    child1_state = ModelState.from_model(Child1)\n    child2_state = ModelState.from_model(Child2)\n    constraint_names = [constraint.name for constraint in child1_state.options['constraints']]\n    self.assertEqual(constraint_names, ['size_gt_1'])\n    constraint_names = [constraint.name for constraint in child2_state.options['constraints']]\n    self.assertEqual(constraint_names, ['size_gt_1'])\n\n    # Modifying the state doesn't modify the constraint on the model.\n    child1_state.options['constraints'][0].name = 'bar'\n    self.assertEqual(Child1._meta.constraints[0].name, 'size_gt_1')\n", "def test_abstract_model_children_inherit_constraints(self):\n    class Abstract(models.Model):\n        size = models.IntegerField()\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n            constraints = [models.CheckConstraint(check=models.Q(size__gt=0), name='size_gt_0')]\n\n    class Child1(Abstract):\n        pass\n\n    class Child2(Abstract):\n        pass\n\n    child1_state = ModelState.from_model(Child1)\n    child2_state = ModelState.from_model(Child2)\n    constraint_names = [constraint.name for constraint in child1_state.options['constraints']]\n    self.assertEqual(constraint_names, ['migrations__size_gt_0'])\n    constraint_names = [constraint.name for constraint in child2_state.options['constraints']]\n    self.assertEqual(constraint_names, ['migrations__size_gt_0_1'])\n\n    # Modifying the state doesn't modify the constraint on the model.\n    child1_state.options['constraints'][0].name = 'bar'\n    self.assertEqual(Child1._meta.constraints[0].name, 'migrations__size_gt_0')\n", "def test_model_state_from_model_excludes_rels(self):\n    class Author(models.Model):\n        name = models.CharField(max_length=255)\n        book = models.ForeignKey('Book', models.CASCADE)\n\n        class Meta:\n            app_label = 'migrations'\n\n    class Book(models.Model):\n        author = models.ForeignKey('Author', models.CASCADE)\n\n        class Meta:\n            app_label = 'migrations'\n\n    author_state = ModelState.from_model(Author, exclude_rels=True)\n    self.assertNotIn('book', author_state.fields)\n    self.assertNotIn('unique_together', author_state.options)\n    self.assertNotIn('index_together', author_state.options)\n\n    book_state = ModelState.from_model(Book, exclude_rels=True)\n    self.assertNotIn('author', book_state.fields)\n    self.assertNotIn('unique_together', book_state.options)\n    self.assertNotIn('index_together', book_state.options)\n", "def test_abstract_model_inherit_unique_together(self):\n    class Abstract(models.Model):\n        name = models.CharField(max_length=50)\n        value = models.IntegerField()\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n            unique_together = [['name', 'value']]\n\n    class Child1(Abstract):\n        pass\n\n    class Child2(Abstract):\n        pass\n\n    child1_state = ModelState.from_model(Child1)\n    child2_state = ModelState.from_model(Child2)\n    self.assertEqual(\n        child1_state.options['unique_together'],\n        {('name', 'value')}\n    )\n    self.assertEqual(\n        child2_state.options['unique_together'],\n        {('name', 'value')}\n    )\n", "def test_custom_manager_default_and_base(self):\n    \"\"\"\n    Tests making a ProjectState from unused models with custom default and base managers\n    \"\"\"\n    new_apps = Apps(['migrations'])\n\n    class Food(models.Model):\n        food_mgr = FoodManager('a', 'b')\n        food_qs = FoodQuerySet.as_manager()\n        food_no_mgr = NoMigrationFoodManager('x', 'y')\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n            default_manager_name = 'food_qs'\n            base_manager_name = 'food_no_mgr'\n\n    food_state = ModelState.from_model(Food)\n\n    # The default manager is used in migrations\n    self.assertEqual([name for name, mgr in food_state.managers], ['food_no_mgr', 'food_qs'])\n    self.assertEqual(food_state.managers[0][1].__class__, NoMigrationFoodManager)\n    self.assertEqual(food_state.managers[1][1].__class__, models.Manager)\n    self.assertEqual(food_state.options['default_manager_name'], 'food_qs')\n    self.assertEqual(food_state.options['base_manager_name'], 'food_no_mgr')\n"], "sample_307": ["def test_invalid_date_format_specifiers(self):\n    my_time = datetime.time(14, 30)\n\n    for specifier in ['b', 'c', 'd', 'D', 'E', 'F', 'I', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'S', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n        msg = (\n            \"The format for time objects may not contain date-related \"\n            \"format specifiers (found '%s').\" % specifier\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            dateformat.time_format(my_time, specifier)\n", "def test_datetime_with_utc_tzinfo(self):\n    dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=utc)\n    self.assertEqual(datetime.fromtimestamp(int(format(dt, 'U'))), dt)\n    self.assertEqual(datetime.fromtimestamp(int(format(dt, 'U')), utc), dt)\n", "    def test_time_format_on_date_object(self):\n        my_birthday = date(1984, 8, 7)\n\n        with self.assertRaises(TypeError):\n            dateformat.time_format(my_birthday, 'H:i')\n", "def test_time_formats_without_leading_zero(self):\n    my_birthday = datetime(1979, 7, 8, 9, 5)\n\n    self.assertEqual(dateformat.format(my_birthday, 'g'), '9')\n    self.assertEqual(dateformat.format(my_birthday, 'G'), '9')\n    self.assertEqual(dateformat.format(my_birthday, 'i'), '05')\n", "def test_dst_transition(self):\n    # DST transition (spring forward) at 2 AM\n    dst_spring_forward = datetime(2021, 3, 8, 2, 00)\n    # DST transition (fall back) at 2 AM\n    dst_fall_back = datetime(2021, 11, 7, 2, 00)\n\n    self.assertEqual(dateformat.format(dst_spring_forward, 'O'), '+0500')\n    self.assertEqual(dateformat.format(dst_spring_forward, 'T'), 'EDT')\n    self.assertEqual(dateformat.format(dst_spring_forward, 'I'), '0')\n\n    self.assertEqual(dateformat.format(dst_fall_back, 'O'), '+0400')\n    self.assertEqual(dateformat.format(dst_fall_back, 'T'), 'EST')\n    self.assertEqual(dateformat.format(dst_fall_back, 'I'), '1')\n"], "sample_613": ["def test_ds_resample_apply_func_kwargs(self):\n        return kwargs[\"ds\"].mean(\"time\") + kwargs[\"arg1\"] + kwargs[\"arg2\"]\n\n    times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n    ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n    expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n    actual = ds.resample(time=\"D\").map(func, arg1=1.0, arg2=1.0)\n    assert_identical(expected, actual)\n", "def test_resample_apply_func_args_dataset():\n        return arg1.mean(\"time\") + arg2 + arg3\n\n    times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n    ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n    expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n    actual = ds.resample(time=\"D\").apply(func, args=(1.0,), arg3=1.0)\n    assert_identical(expected, actual)\n", "def test_ds_resample_apply_func_returns_dataset(self):\n        return xr.Dataset({\"bar\": group.mean(\"time\")})\n\n    times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n    ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n    expected = xr.Dataset({\"bar\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n    actual = ds.resample(time=\"D\").map(func)\n    assert_identical(expected, actual)\n", "def test_groupby_last_and_first(self):\n    ds = xr.Dataset({\"foo\": (\"x\", [1, 2, 3, 4, 5])}, coords={\"x\": np.arange(5)})\n    by = xr.DataArray([0, 0, 1, 1, 1], dims=\"x\", name=\"ab\")\n\n    expected_first = xr.DataArray([1, 3], coords={\"ab\": [0, 1]})\n    actual_first = ds.groupby(by).first()\n    assert_identical(expected_first, actual_first)\n\n    expected_last = xr.DataArray([2, 5], coords={\"ab\": [0, 1]})\n    actual_last = ds.groupby(by).last()\n    assert_identical(expected_last, actual_last)\n", "def test_ds_resample_apply_func_errors(self):\n        return arg1.mean(\"time\") + arg2 + arg3\n\n    times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n    ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n\n    with pytest.raises(TypeError, match=\"missing 1 required positional argument: 'arg2'\"):\n        ds.resample(time=\"D\").map(func)\n\n    with pytest.raises(TypeError, match=\"func() got multiple values for argument 'arg2'\"):\n        ds.resample(time=\"D\").map(func, args=(1.0,), arg2=2.0)\n\n    with pytest.raises(TypeError, match=\"func() got an unexpected keyword argument 'arg4'\"):\n        ds.resample(time=\"D\").map(func, args=(1.0,), arg4=2.0)\n"], "sample_966": ["def test_pyclassmethod_with_return_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:classmethod:: meth\\n\"\n            \"      :rtype: int\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class class method)', 'Class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, (\"classmethod\", desc_sig_space)],\n                                                     [desc_name, \"meth\"],\n                                                     [desc_parameterlist, ()],\n                                                     [desc_returns, pending_xref, \"int\"])],\n                                   [desc_content, ()]))\n    assert 'Class.meth' in domain.objects\n    assert domain.objects['Class.meth'] == ('index', 'Class.meth', 'method', False)\n", "def test_pyclassmethod_signature_with_prefix(app):\n    text = (\".. py:class:: Namespace\\n\"\n            \"\\n\"\n            \"   .. py:classmethod:: Namespace.meth\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Namespace\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Namespace class method)', 'Namespace.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, (\"classmethod\", desc_sig_space)],\n                                                     [desc_addname, \"Namespace.\"],\n                                                     [desc_name, \"meth\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n    assert 'Namespace.meth' in domain.objects\n    assert domain.objects['Namespace.meth'] == ('index', 'Namespace.meth', 'method', False)\n", "def test_pyfunction_signature_with_meta(app):\n    text = (\".. py:function:: func(name: str) -> str\\n\"\n            \"   :meta private:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][1][1],\n                ([nodes.field_name, \"meta\"],\n                 [nodes.field_body, nodes.paragraph, \"private\"]))\n", "def test_pyclass_subclass_signature(app):\n    text = (\".. py:class:: ParentClass\\n\"\n            \".. py:class:: ChildClass\\n\"\n            \"   :noindex:\\n\"\n            \"   :subclass: ParentClass\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"ParentClass\"])],\n                                  [desc_content, ()])],\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"ChildClass\"],\n                                                    [desc_annotation, (desc_sig_space,\n                                                                       [desc_sig_punctuation, '('],\n                                                                       [desc_name, \"ParentClass\"],\n                                                                       [desc_sig_punctuation, ')'])])],\n                                  [desc_content, ()])]))\n    assert 'ParentClass' in domain.objects\n    assert domain.objects['ParentClass'] == ('index', 'ParentClass', 'class', False)\n    assert 'ChildClass' in domain.objects\n    assert domain.objects['ChildClass'] == ('index', 'ChildClass', 'class', False)\n", "def test_py_typing_imports(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   from typing import List, Tuple, Union\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: List[Union[int, str]]\\n\"\n            \"   .. py:method:: meth(arg: Tuple[int, ...]) -> None:\\n\"\n            \"      :noindex:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (nodes.comment,\n                                                  addnodes.index,\n                                                  desc,\n                                                  desc)])]))\n    assert_node(doctree[1][1][2], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, ([desc_sig_punctuation, ':'],\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"List\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"Union\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"int\"],\n                                                                        [desc_sig_punctuation, \",\"],\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \"]\"],\n                                                                        [desc_sig_punctuation, \"]\"])])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][2][0][1][2], pending_xref"], "sample_282": ["def test_partially_required_field(self):\n    form = PartiallyRequiredForm({'f_0': 'required', 'f_1': ''})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['f'], 'required,')\n", "def test_required_field(self):\n    form = PartiallyRequiredForm({'f_0': 'required'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['f'], 'required,')\n", "def test_partially_required_field(self):\n    data = {'f_0': 'first', 'f_1': ''}\n    form = PartiallyRequiredForm(data)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['f'], 'first')\n\n    data = {'f_0': '', 'f_1': ''}\n    form = PartiallyRequiredForm(data)\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {'f': ['This field is required.']})\n", "    def test_partially_required_field(self):\n        form = PartiallyRequiredForm({'f_0': 'required'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'required,')\n", "    def test_form_initial_data(self):\n        form = ComplexFieldForm(initial={\n            'field1': ['initial text', ['J', 'P'], ['2007-04-25', '06:24:00']],\n        })\n        self.assertHTMLEqual(\n            form.as_table(),\n            \"\"\"\n            <tr><th><label for=\"id_field1_0\">Field1:</label></th>\n            <td><input type=\"text\" name=\"field1_0\" value=\"initial text\" id=\"id_field1_0\" required>\n            <select multiple name=\"field1_1\" id=\"id_field1_1\" required>\n            <option value=\"J\" selected>John</option>\n            <option value=\"P\" selected>Paul</option>\n            <option value=\"G\">George</option>\n            <option value=\"R\">Ringo</option>\n            </select>\n            <input type=\"text\" name=\"field1_2_0\" value=\"2007-04-25\" id=\"id_field1_2_0\" required>\n            <input type=\"text\" name=\"field1_2_1\" value=\"06:24:00\" id=\"id_field1_2_1\" required></td></tr>\n            \"\"\",\n        )\n"], "sample_1077": ["def test_Rationals_as_relational():\n    assert S.Rationals.as_relational(x) == And(Eq(floor(x), x), -oo < x, x < oo)\n", "def test_ComplexRegion_attributes():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(Union(a*b, b*a))\n\n    assert c1.sets == a*b\n    assert c1.psets == (a*b,)\n    assert c1.a_interval == a\n    assert c1.b_interval == b\n    assert c1.polar is False\n    assert c1._measure == 12\n\n    assert c2.sets == Union(a*b, b*a)\n    assert c2.psets == (a*b, b*a)\n    assert c2.a_interval == Union(a, b)\n    assert c2.b_interval == Union(b, a)\n    assert c2.polar is False\n    assert c2._measure == 20\n", "def test_ComplexRegion_boundary():\n    a, b = Interval(2, 5), Interval(4, 8)\n    theta1, theta2 = Interval(0, 2*S.Pi), Interval(0, S.Pi)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(Union(a*theta1, b*theta2), polar=True)\n\n    assert c1.boundary == Union(Interval(2, 5)*FiniteSet(4, 8), Interval(4, 5)*FiniteSet(2, 5), FiniteSet(2 + 4*I, 5 + 4*I, 2 + 8*I, 5 + 8*I))\n    assert c2.boundary == Union(ImageSet(Lambda(theta, 2*exp(I*theta)), Interval(0, 2*S.Pi)), ImageSet(Lambda(theta, 5*exp(I*theta)), Interval(0, 2*S.Pi)), ImageSet(Lambda(r, r*exp(I*0)), Interval(2, 8)), ImageSet(Lambda(r, r*exp(I*S.Pi)), Interval(2, 5)), ImageSet(Lambda(r, r*exp(I*S.Pi)), Interval(4, 8)))\n", "def test_ComplexRegion_properties():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c = ComplexRegion(a*b)\n\n    assert c.sets == ProductSet(a, b)\n    assert c.psets == (ProductSet(a, b),)\n    assert c.a_interval == a\n    assert c.b_interval == b\n    assert c.polar is False\n    assert c._measure == a._measure * b._measure\n    assert ComplexRegion.from_real(a) == ComplexRegion(a * FiniteSet(0))\n", "def test_ComplexRegion_sympy_functions():\n    from sympy import sin, cos, tan, log, exp, Abs\n    from sympy.abc import x, y\n\n    # Test ComplexRegion with sympy functions\n    a = Interval(0, 1)\n    theta = Interval(0, 2*S.Pi)\n\n    # sin function\n    c_sin = ComplexRegion(a*theta, polar=True, expr=sin(theta))\n    assert (sin(0.5) + 6*I/10) in c_sin\n    assert (sin(S.Half) + 6*I/10) in c_sin\n\n    # cos function\n    c_cos = ComplexRegion(a*theta, polar=True, expr=cos(theta))\n    assert (cos(0.5) + 6*I/10) in c_cos\n    assert (cos(S.Half) + 6*I/10) in c_cos\n\n    # tan function\n    c_tan = ComplexRegion(a*theta, polar=True, expr=tan(theta))\n    assert (tan(0.5) + 6*I/10) in c_tan\n    assert (tan(S.Half) + 6*I/10) in c_tan\n\n    # log function\n    c_log = ComplexRegion(a*theta, polar=True, expr=log(a))\n    assert (log(0.5) + 6*I/10) in c_log\n    assert (log(S.Half) + 6*I/10) in c_log\n\n    # exp function\n    c_exp = ComplexRegion(a*theta, polar=True, expr=exp(theta))\n    assert (exp(0.5) + 6*I/10) in c_exp\n    assert (exp(S.Half) + 6*I/10) in c_exp\n\n    # Abs function\n    c_abs = ComplexRegion(a*theta, polar=True, expr=Abs(a))\n    assert (Abs(0.5) + 6*I/10) in c"], "sample_58": ["def test_renderer_attribute_class_instance(self):\n    custom = CustomRenderer()\n\n    class CustomForm(Form):\n        default_renderer = custom\n\n    form = CustomForm()\n    self.assertEqual(form.renderer, custom)\n", "def test_form_prefix_custom_widget_id_for_label(self):\n    class CustomIdForLabelTextInput(TextInput):\n            return 'custom_' + id\n\n    class SomeForm(Form):\n        custom = CharField(widget=CustomIdForLabelTextInput)\n\n    form = SomeForm(prefix='test')\n    self.assertHTMLEqual(form['custom'].label_tag(), '<label for=\"custom_test-custom\">Custom:</label>')\n", "    def test_field_named_data(self):\n        class DataForm(Form):\n            data = CharField(max_length=10)\n\n        f = DataForm({'data': 'xyzzy'})\n        self.assertTrue(f.is_valid())\n        self.assertEqual(f.cleaned_data, {'data': 'xyzzy'})\n\n        # Test with an empty string\n        f = DataForm({'data': ''})\n        self.assertFalse(f.is_valid())\n        self.assertEqual(f.errors, {'data': ['This field is required.']})\n\n        # Test with a value longer than the max_length\n        f = DataForm({'data': 'abcdefghijkl'})\n        self.assertFalse(f.is_valid())\n        self.assertEqual(f.errors, {'data': ['Ensure this value has at most 10 characters (it has 11).']})\n", "    def test_filefield_with_large_data(self):\n        large_file_data = 'a' * (settings.FILE_UPLOAD_MAX_MEMORY_SIZE + 1)\n        file1 = SimpleUploadedFile('large_file.txt', large_file_data.encode())\n        f = self.FileForm(data={}, files={'file1': file1}, auto_id=False)\n        self.assertFalse(f.is_valid())\n        self.assertEqual(f.errors['file1'], ['Upload a file smaller than 10.0MB.'])\n", "def test_clean_field_name(self):\n    class MyForm(Form):\n        invalid_field_name = CharField()\n\n            self.cleaned_data['invalid_field_name'] = 'cleaned_value'\n            self.cleaned_data['new_field'] = 'new_value'\n            return self.cleaned_data\n\n    form = MyForm({'invalid_field_name': 'original_value'})\n    form.full_clean()\n    self.assertEqual(form.cleaned_data, {'invalid_field_name': 'cleaned_value', 'new_field': 'new_value'})\n"], "sample_401": ["def test_formset_with_empty_data(self):\n    \"\"\"\n    A FormSet with empty data is not valid and has no cleaned_data attribute.\n    \"\"\"\n    formset = self.make_choiceformset(formset_data=None)\n    self.assertFalse(formset.is_valid())\n    with self.assertRaises(AttributeError):\n        formset.cleaned_data\n", "def test_delete_extra_initial_data(self):\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, can_delete_extra=True)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    # To delete something, set that form's special delete field to 'on'.\n    # Let's go ahead and delete the extra form.\n    data = {\n        \"choices-TOTAL_FORMS\": \"3\",  # the number of forms rendered\n        \"choices-INITIAL_FORMS\": \"2\",  # the number of forms with initial data\n        \"choices-MIN_NUM_FORMS\": \"0\",  # min number of forms\n        \"choices-MAX_NUM_FORMS\": \"0\",  # max number of forms\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-0-DELETE\": \"\",\n        \"choices-1-choice\": \"Fergie\",\n        \"choices-1-votes\": \"900\",\n        \"choices-1-DELETE\": \"\",\n        \"choices-2-choice\": \"\",\n        \"choices-2-votes\": \"\",\n        \"choices-2-DELETE\": \"on\",\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.forms],\n        [\n            {\"votes\": 100, \"DELETE\": False, \"choice\": \"Calexico\"},\n            {\"votes\": 900, \"DELETE\": False, \"choice\": \"Fergie\"},\n            {},\n        ],\n    )\n    self.assertEqual(\n        [form.cleaned_data for form in formset.deleted_forms],\n        [{}", "def test_empty_formset_errors(self):\n    \"\"\"Form errors are available on empty formset.\"\"\"\n\n    class RequiredFieldForm(Form):\n        required_field = CharField(required=True)\n\n    RequiredFieldFormSet = formset_factory(RequiredFieldForm, extra=0)\n    data = {\"form-TOTAL_FORMS\": \"0\", \"form-INITIAL_FORMS\": \"0\"}\n    formset = RequiredFieldFormSet(data, prefix=\"form\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.errors, [])\n    self.assertEqual(formset.non_form_errors(), [\"Please submit at least 1 form.\"])\n", "def test_formset_with_min_num(self):\n    \"\"\"Formsets with min_num.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"2\",\n        \"choices-MAX_NUM_FORMS\": \"1000\",\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-1-choice\": \"Fergie\",\n        \"choices-1-votes\": \"900\",\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.forms],\n        [\n            {\"votes\": 100, \"choice\": \"Calexico\"},\n            {\"votes\": 900, \"choice\": \"Fergie\"},\n        ],\n    )\n\n    # Test validation for min_num\n    data[\"choices-TOTAL_FORMS\"] = \"1\"\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(), [\"Please submit at least 2 forms.\"]\n    )\n", "def test_validate_min_excludes_deleted_forms(self):\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"0\",\n        \"choices-0-DELETE\": \"on\",\n        \"choices-1-choice\": \"\",\n        \"choices-1-votes\": \"\",\n        \"choices-1-DELETE\": \"\",\n    }\n    ChoiceFormSet = formset_factory(Choice, min_num=1, validate_min=True, can_delete=True)\n    formset = ChoiceFormSet(data, prefix=\"choices\")\n    self.assertFalse(formset.has_changed())\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [])\n"], "sample_573": ["def test_order_greater_than_unique_values(self, df):\n    df = pd.DataFrame(dict(x=[1, 2, 3], y=[4, 5, 6]))\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=3, gridsize=100)(df, groupby, \"x\", {})\n\n    assert_frame_equal(res, pd.DataFrame(columns=[\"x\", \"y\"]))\n", "def test_order_greater_than_unique_values(self, df):\n\n    df[\"x\"] = np.repeat(np.arange(5), 20)  # x has only 5 unique values\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=2, gridsize=100)(df, groupby, \"x\", {})\n\n    assert res.empty, \"Result should be empty as order is greater than unique values of x\"\n", "def test_non_numeric_data(self, df):\n    df[\"x\"] = \"non_numeric\"\n    groupby = GroupBy([\"group\"])\n    with pytest.raises(TypeError):\n        PolyFit(order=1, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n", "def test_unique_values_less_than_order(self, df):\n\n    df[\"x\"] = np.repeat([1, 2, 3], df.shape[0] // 3)\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=5)(df, groupby, \"x\", {})\n\n    assert res.empty\n", "def test_order_gt_unique_x(self, df):\n\n    df = pd.DataFrame(dict(x=[1, 2, 3], y=[1, 2, 3]))\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=3)(df, groupby, \"x\", {})\n\n    assert res.empty\n"], "sample_474": ["def test_substring(self):\n    authors = Author.objects.annotate(alias_substr=Substr(\"alias\", 1, 2))\n    self.assertCountEqual(authors.filter(alias_substr=\"mi\"), [self.john])\n    self.assertCountEqual(authors.exclude(alias_substr=\"mi\"), [self.elena, self.rhonda])\n", "    def test_transform_with_f_expression(self):\n        with register_lookup(IntegerField, Chr):\n            authors = Author.objects.annotate(first_initial_code_point=Ord(Left(F('name'), 1)))\n            self.assertCountEqual(\n                authors.filter(first_initial_code_point__chr=Chr(ord(\"J\"))), [self.john]\n            )\n            self.assertCountEqual(\n                authors.exclude(first_initial_code_point__chr=Chr(ord(\"J\"))),\n                [self.elena, self.rhonda],\n            )\n", "def test_null_value(self):\n    authors = Author.objects.annotate(first_initial=Left(F(\"alias\"), 1))\n    self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"J\"))), [self.john])\n    self.assertCountEqual(\n        authors.exclude(first_initial=Chr(ord(\"J\"))), [self.elena]\n    )\n    # Test with None value in the alias field\n    self.rhonda.alias = None\n    self.rhonda.save()\n    self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"J\"))), [self.john])\n    self.assertCountEqual(\n        authors.exclude(first_initial=Chr(ord(\"J\"))), [self.elena, self.rhonda]\n    )\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.smith = Author.objects.create(name=\"Smith\", alias=\"smith\")\n        cls.johnsmith = Author.objects.create(name=\"John Smith\", alias=\"johnsmith\")\n", "def test_null_values(self):\n    authors = Author.objects.annotate(first_initial=Left(\"alias\", 1))\n    self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"J\"))), [self.john])\n    self.assertCountEqual(\n        authors.filter(first_initial=Chr(None)), [self.rhonda]\n    )\n"], "sample_468": ["    def test_bind_template(self):\n        class MockTemplate:\n            class MockEngine:\n                template_context_processors = []\n\n            engine = MockEngine()\n\n        request = self.request_factory.get(\"/\")\n        context = RequestContext(request, {})\n\n        with context.bind_template(MockTemplate()):\n            self.assertEqual(context.template, MockTemplate())\n        self.assertIsNone(context.template)\n", "def test_context_processor_none(self):\n    \"\"\"\n    #26185 -- Context processors can return None.\n    \"\"\"\n    request = self.request_factory.get(\"/\")\n    ctx = RequestContext(request, {}, processors=[context_process_returning_none])\n    self.assertEqual(len(ctx.dicts), 4)\n", "def test_context_processor_returns_none(self):\n    \"\"\"\n    Context processors returning None should be handled correctly.\n    \"\"\"\n    engine = Engine(\n        context_processors=[\n            context_process_returning_none,\n        ],\n        loaders=[\n            (\n                \"django.template.loaders.locmem.Loader\",\n                {\n                    \"test\": '{{ none_processor|default:\"none\" }}',\n                },\n            ),\n        ],\n    )\n    request = self.request_factory.get(\"/\")\n    ctx = RequestContext(request, {})\n    self.assertEqual(engine.get_template(\"test\").render(ctx), \"none\")\n", "def test_render_context_push_state(self):\n    test_context = RenderContext({\"fruit\": \"apple\"})\n\n    with test_context.push_state(None):\n        test_context[\"vegetable\"] = \"carrot\"\n        self.assertEqual(list(test_context), [\"vegetable\"])\n\n    self.assertNotIn(\"vegetable\", test_context)\n    self.assertEqual(test_context[\"fruit\"], \"apple\")\n", "def test_request_context_processors(self):\n    request = self.request_factory.get(\"/\")\n\n        return {\"one\": 1}\n\n        return {\"two\": 2}\n\n    ctx = RequestContext(request, processors=[process_one, process_two])\n    self.assertEqual(ctx[\"one\"], 1)\n    self.assertEqual(ctx[\"two\"], 2)\n\n    # Test that processors are cleared out after rendering\n    with ctx.bind_template(Template(\"\")):\n        pass\n    self.assertNotIn(\"one\", ctx)\n    self.assertNotIn(\"two\", ctx)\n\n    # Test that processors are re-applied after rendering\n    with ctx.bind_template(Template(\"\")):\n        pass\n    self.assertEqual(ctx[\"one\"], 1)\n    self.assertEqual(ctx[\"two\"], 2)\n"], "sample_939": ["def test_unparse_AugAssign():\n    source = \"a += 1\"\n    expected = \"a += 1\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_Starred_Subscript():\n    source = \"a[1, 2, *b]\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == \"a[1, 2, *b]\"\n", "def test_unparse_complex():\n    source = \"1j + 2\"\n    expected = \"1j + 2\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value.targets[0].elts, source) == expected\n", "def test_unparse_Dict_with_none_key():\n    source = \"{'key1': 'value1', None: 'value2'}\"\n    expected = \"{'key1': 'value1', None: 'value2'}\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n"], "sample_1023": ["def test_sieve_extend():\n    s = Sieve()\n    s.extend(100)\n    assert s._list == _array('l', [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97])\n    s.extend(200)\n    assert s._list == _array('l', [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199])\n", "def test_sieve_reset():\n    sieve._reset()\n    assert sieve._list == _array('l', [2, 3, 5, 7, 11, 13])\n    assert sieve._tlist == _array('l', [0, 1, 1, 2, 2, 4])\n    assert sieve._mlist == _array('l', [0, 1, -1, -1, 0, -1])\n\n    sieve._reset(prime=False)\n    assert sieve._list == _array('l', [])\n    assert sieve._tlist == _array('l', [0, 1, 1, 2, 2, 4])\n    assert sieve._mlist == _array('l', [0, 1, -1, -1, 0, -1])\n\n    sieve._reset(totient=False)\n    assert sieve._list == _array('l', [])\n    assert sieve._tlist == _array('l', [])\n    assert sieve._mlist == _array('l', [0, 1, -1, -1, 0, -1])\n\n    sieve._reset(mobius=False)\n    assert sieve._list == _array('l', [])\n    assert sieve._tlist == _array('l', [])\n    assert sieve._mlist == _array('l', [])\n\n    sieve._reset()\n    assert sieve._list == _array('l', [2, 3, 5, 7, 11, 13])\n    assert sieve._tlist == _array('l', [0, 1, 1, 2, 2, 4])\n    assert sieve._mlist == _array('l', [0, 1, -1, -1, 0, -1])\n", "def test_sieve_reset():\n    from sympy.ntheory.generate import Sieve\n\n    s = Sieve()\n    s.extend(100)\n    assert len(s._list) > 25\n    assert len(s._tlist) > 100\n    assert len(s._mlist) > 100\n\n    s._reset(prime=True)\n    assert len(s._list) <= 25\n    assert len(s._tlist) > 100\n    assert len(s._mlist) > 100\n\n    s._reset(totient=True)\n    assert len(s._list) <= 25\n    assert len(s._tlist) <= 25\n    assert len(s._mlist) > 100\n\n    s._reset(mobius=True)\n    assert len(s._list) <= 25\n    assert len(s._tlist) <= 25\n    assert len(s._mlist) <= 25\n\n    s._reset()\n    assert len(s._list) <= 25\n    assert len(s._tlist) <= 25\n    assert len(s._mlist) <= 25\n", "def test_primetest_mr():\n    assert mr(2, [2]) is True\n    assert mr(3, [2]) is True\n    assert mr(4, [2]) is False\n    assert mr(17, [2]) is True\n    assert mr(341, [2]) is True\n    assert mr(341, [2, 3]) is True\n    assert mr(341, [2, 3, 5]) is True\n    assert mr(341, [2, 3, 5, 7]) is True\n    assert mr(341, [2, 3, 5, 7, 11]) is True\n    assert mr(341, [2, 3, 5, 7, 11, 13]) is True\n    assert mr(341, [2, 3, 5, 7, 11, 13, 17]) is True\n    assert mr(341, [2, 3, 5, 7, 11, 13, 17, 19]) is True\n    assert mr(341, [2, 3, 5, 7, 11, 13, 17, 19, 23]) is True\n    assert mr(341, [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]) is True\n    assert mr(341, [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31]) is True\n    assert mr(341, [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]) is True\n    assert mr(341, [2, 3, 5, 7, 11, 13, 17", "def test_sieve_reset():\n    from sympy.ntheory.generate import sieve\n    sieve.extend(100)\n    sieve._reset()\n    assert sieve._list == _array('l', [2, 3, 5, 7, 11, 13])\n    assert sieve._tlist == _array('l', [0, 1, 1, 2, 2, 4])\n    assert sieve._mlist == _array('l', [0, 1, -1, -1, 0, -1])\n    sieve._reset(prime=False, totient=False, mobius=False)\n    assert len(sieve._list) == 6\n    assert len(sieve._tlist) == 6\n    assert len(sieve._mlist) == 6\n"], "sample_777": ["def test_gradient_boosting_init_with_parameters():\n    # Check that GradientBoostingRegressor works when init is a sklearn\n    # estimator with parameters\n\n    X, y = make_regression()\n    init_est = OneHotEncoder(handle_unknown='ignore')\n    gb = GradientBoostingRegressor(init=init_est)\n    gb.fit(X, y)\n    gb.predict(X)\n", "def test_gradient_boosting_loss_not_supported_by_init():\n    # Make sure error is raised if init estimators don't support the loss\n\n    message = (\"The init parameter must support the loss function 'huber'\")\n    with pytest.raises(ValueError, match=message):\n        GradientBoostingRegressor(init=DummyRegressor(), loss='huber').fit(X, y)\n", "def test_gradient_boosting_loss_functions():\n    # Test that different loss functions work as expected for classification and regression\n\n    X, y = make_classification()\n    gbc_deviance = GradientBoostingClassifier(loss='deviance').fit(X, y)\n    gbc_exponential = GradientBoostingClassifier(loss='exponential').fit(X, y)\n    assert not np.allclose(gbc_deviance.predict(X), gbc_exponential.predict(X))\n\n    X, y = make_regression()\n    gbr_ls = GradientBoostingRegressor(loss='ls').fit(X, y)\n    gbr_lad = GradientBoostingRegressor(loss='lad').fit(X, y)\n    assert not np.allclose(gbr_ls.predict(X), gbr_lad.predict(X))\n", "def test_gradient_boosting_n_iter_no_change_validation_fraction():\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=100,\n                                     n_iter_no_change=10,\n                                     validation_fraction=0.1,\n                                     learning_rate=0.1, max_depth=3,\n                                     random_state=42)\n\n    # Check if n_iter_no_change and validation_fraction are set correctly\n    assert gbc.n_iter_no_change == 10\n    assert gbc.validation_fraction == 0.1\n\n    # Check if error is raised when validation_fraction is not in [0, 1]\n    with pytest.raises(ValueError, match=\"validation_fraction must be between 0 and 1\"):\n        gbc.set_params(validation_fraction=1.2)\n\n    # Check if error is raised when n_iter_no_change is not integer\n    with pytest.raises(ValueError, match=\"n_iter_no_change should either be None or an integer.\"):\n        gbc.set_params(n_iter_no_change='10')\n", "def test_gradient_boosting_sparse_input():\n    # Test gradient boosting with sparse input\n    X, y = make_classification(n_samples=1000, n_features=20, random_state=0)\n    X_sparse = csr_matrix(X)\n\n    gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n    gbc_sparse = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n\n    gbc.fit(X, y)\n    gbc_sparse.fit(X_sparse, y)\n\n    assert_array_almost_equal(gbc.predict(X), gbc_sparse.predict(X_sparse))\n    assert_array_almost_equal(gbc.feature_importances_, gbc_sparse.feature_importances_)\n"], "sample_505": ["def test_change_converter_and_interval_multiples():\n    plt.rcParams['date.converter'] = 'concise'\n    plt.rcParams['date.interval_multiples'] = False\n    dates = np.arange('2020-01-01', '2020-05-01', dtype='datetime64[D]')\n    fig, ax = plt.subplots()\n\n    ax.plot(dates, np.arange(len(dates)))\n    fig.canvas.draw()\n    assert ax.get_xticklabels()[0].get_text() == 'Jan'\n    assert ax.get_xticklabels()[1].get_text() == '05'\n\n    plt.rcParams['date.converter'] = 'auto'\n    plt.rcParams['date.interval_multiples'] = True\n    fig, ax = plt.subplots()\n\n    ax.plot(dates, np.arange(len(dates)))\n    fig.canvas.draw()\n    assert ax.get_xticklabels()[0].get_text() == 'Jan 01 2020'\n    assert ax.get_xticklabels()[1].get_text() == 'Feb 01 2020'\n", "def test_rrule_dtypes():\n    # Test that rrulewrapper correctly handles numpy datetimes and datetimes\n    # with timezones.\n    dtstart = np.datetime64('2017-04-01T00:00:00')\n    dtend = np.datetime64('2017-04-04T00:00:00')\n\n    rule = mdates.rrulewrapper(freq=dateutil.rrule.DAILY, dtstart=dtstart)\n\n    act = rule.between(dtstart, dtend)\n    exp = [np.datetime64('2017-04-01T00:00:00'),\n           np.datetime64('2017-04-02T00:00:00')]\n\n    np.testing.assert_array_equal(act, exp)\n\n    tz = mdates.UTC\n    dtstart = datetime.datetime(2017, 4, 1, 0, 0, tzinfo=tz)\n    dtend = datetime.datetime(2017, 4, 4, 0, 0, tzinfo=tz)\n\n    rule = mdates.rrulewrapper(freq=dateutil.rrule.DAILY, dtstart=dtstart)\n\n    act = rule.between(dtstart, dtend)\n    exp = [datetime.datetime(2017, 4, 1, 0, 0, tzinfo=tz),\n           datetime.datetime(2017, 4, 2, 0, 0, tzinfo=tz)]\n\n    assert act == exp\n", "def test_microsecondlocator():\n    # Test MicrosecondLocator with non-singular data\n    t0 = datetime.datetime(2000, 1, 1, 0, 0, 0, 123456)\n    tf = datetime.datetime(2000, 1, 1, 0, 0, 0, 789012)\n    fig, ax = plt.subplots()\n    ax.plot([t0, tf], [0, 1])\n    ax.xaxis_date()\n    locator = mdates.MicrosecondLocator(interval=1000)\n    ax.xaxis.set_major_locator(locator)\n    fig.canvas.draw()\n    ticklabels = [tl.get_text() for tl in ax.get_xticklabels()]\n    expected = ['00:00:00.123456', '00:00:00.124456', '00:00:00.125456',\n                '00:00:00.126456', '00:00:00.127456', '00:00:00.128456']\n    assert ticklabels == expected\n\n    # Test MicrosecondLocator with singular data\n    fig, ax = plt.subplots()\n    ax.plot([t0], [0])\n    ax.xaxis_date()\n    locator = mdates.MicrosecondLocator(interval=1000)\n    ax.xaxis.set_major_locator(locator)\n    fig.canvas.draw()\n    ticklabels = [tl.get_text() for tl in ax.get_xticklabels()]\n    expected = ['00:00:00.000000']\n    assert ticklabels == expected\n", "def test_num2date_NaT():\n    dn = mdates.date2num('NaT')\n    assert np.isnan(dn)\n    dt = mdates.num2date(dn)\n    assert dt is np.datetime64('NaT')\n", "def test_drange_with_tzinfo():\n    start = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC)\n    end = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC)\n    delta = datetime.timedelta(hours=1)\n    # We expect 24 values in drange(start, end, delta), because drange returns\n    # dates from an half open interval [start, end)\n    assert len(mdates.drange(start, end, delta)) == 24\n\n    # if end is a little bit later, we expect the range to contain one element\n    # more\n    end = end + datetime.timedelta(microseconds=1)\n    assert len(mdates.drange(start, end, delta)) == 25\n\n    # reset end\n    end = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC)\n\n    # and tst drange with \"complicated\" floats:\n    # 4 hours = 1/6 day, this is an \"dangerous\" float\n    delta = datetime.timedelta(hours=4)\n    daterange = mdates.drange(start, end, delta)\n    assert len(daterange) == 6\n    assert mdates.num2date(daterange[-1]) == (end - delta)\n"], "sample_899": ["def test_check_estimator_no_check_in_predict():\n    # check that a warning is raised when calling predict\n    # without checking the input\n    msg = \"Estimator doesn't check for NaN and inf in predict\"\n    assert_warns(Warning, check_estimator, NoCheckinPredict)\n", "def test_check_estimator_not_fitted():\n    # check that a NotFittedError is raised when calling fit_transform\n    # on an unfitted estimator\n    msg = \"The unfitted estimator .* does not raise an error when \" \\\n          \"fit_transform is called. Perhaps use check_is_fitted \" \\\n          \"in fit_transform.\"\n    assert_raises_regex(AssertionError, msg, check_estimators_unfitted,\n                        \"estimator\", SparseTransformer())\n", "def test_check_estimator_infinite_values():\n    # check that check_estimator() raises an error when the estimator doesn't\n    # check for infinite values in fit and predict\n    class NonConformantEstimatorInfiniteValues(BaseEstimator):\n            return self\n            return np.ones(X.shape[0])\n\n    msg = \"Estimator doesn't check for NaN and inf in fit.\"\n    assert_raises_regex(AssertionError, msg, check_estimator, NonConformantEstimatorInfiniteValues())\n", "def test_check_estimator_transformer_mixin():\n    # check that TransformerMixin is required for transformer tests to run\n    class BadTransformerWithoutFitTransform(BaseEstimator):\n            return X\n\n    msg = \".*fit_transform.*\"\n    assert_raises_regex(AttributeError, msg, check_estimator, BadTransformerWithoutFitTransform())\n", "def test_check_estimator_sparsify_coefficients():\n    # check that check_estimator() works on estimator with sparsify_coefficients\n    est = LinearRegression()\n    check_estimator(est)\n"], "sample_721": ["def test_check_memory():\n    # Test with None\n    memory = check_memory(None)\n    assert isinstance(memory, Memory)\n\n    # Test with str\n    memory = check_memory('test_cache_dir')\n    assert isinstance(memory, Memory)\n    assert memory.location == 'test_cache_dir'\n\n    # Test with object with joblib.Memory interface\n    class CustomMemory:\n            return func\n\n    memory = check_memory(CustomMemory())\n    assert isinstance(memory, CustomMemory)\n\n    # Test with invalid input\n    with pytest.raises(ValueError, match=r\"'memory' should be None, a string or have the same interface as sklearn\\.externals\\.joblib\\.Memory\\. Got memory='invalid' instead\\.\"):\n        check_memory('invalid')\n", "def test_check_memory():\n    memory = check_memory(None)\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    memory = check_memory('/tmp/')\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    memory = DummyMemory()\n    assert check_memory(memory) == memory\n\n    invalid_memory = 'invalid'\n    with pytest.raises(ValueError, match=\"'memory' should be None, a string or have the same interface as sklearn.externals.joblib.Memory\"):\n        check_memory(invalid_memory)\n", "def test_check_memory():\n    # Test that check_memory returns a Memory instance when given None or str\n    mem = check_memory(None)\n    assert isinstance(mem, Memory)\n    mem = check_memory('cache_dir')\n    assert isinstance(mem, Memory)\n    assert mem.cachedir == 'cache_dir'\n\n    # Test that check_memory returns the same object when given a Memory instance\n    mem = Memory(cachedir='cache_dir')\n    assert check_memory(mem) is mem\n\n    # Test that check_memory raises a ValueError when given an invalid input\n    with pytest.raises(ValueError):\n        check_memory(123)\n", "def test_check_memory():\n    memory = check_memory(None)\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    memory = check_memory('/tmp/cache')\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    memory = DummyMemory()\n    assert check_memory(memory) is memory\n\n    try:\n        check_memory(42)\n    except ValueError as e:\n        assert str(e) == \"'memory' should be None, a string or have the same interface as sklearn.externals.joblib.Memory. Got memory='42' instead.\"\n", "def test_check_memory():\n    memory = check_memory(None)\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    memory = check_memory('cache_dir')\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    memory = DummyMemory()\n    assert check_memory(memory) == memory\n\n    invalid_memory = 'invalid'\n    msg = (\"'memory' should be None, a string or have the same interface as sklearn.externals.joblib.Memory. Got memory='{}' instead.\".format(invalid_memory))\n    assert_raise_message(ValueError, msg, check_memory, invalid_memory)\n"], "sample_915": ["def test_getdoc_inherited_decorated_method():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n        @functools.wraps(Foo.meth)\n            pass\n\n    assert inspect.getdoc(Bar.meth, allow_inherited=True, cls=Bar, name='meth') == \"docstring.\"\n", "def test_getdoc_partial_method():\n        \"\"\"docstring.\"\"\"\n        pass\n\n    p = functools.partial(func, 10, c=11)\n\n    doc = inspect.getdoc(p)\n    assert doc == \"docstring.\"\n", "def test_getdoc_builtin_class_method():\n    doc = inspect.getdoc(int.__init__)\n    assert doc == int.__init__.__doc__\n", "def test_getdoc_inherited_and_decorated_method():\n    class Parent:\n            \"\"\"Docstring of the parent method.\"\"\"\n            pass\n\n    class Child(Parent):\n        @functools.wraps(Parent.meth)\n            pass\n\n    assert inspect.getdoc(Child().meth, allow_inherited=True, cls=Child, name='meth') == \"Docstring of the parent method.\"\n", "def test_getdoc_partial_function():\n        \"\"\"docstring.\"\"\"\n        pass\n\n    func2 = functools.partial(func, 1)\n    func3 = functools.partial(func2, 2)  # nested partial object\n\n    assert inspect.getdoc(func2) == \"docstring.\"\n    assert inspect.getdoc(func3) == \"docstring.\"\n"], "sample_649": ["def test_log_file_cli_subdirectories_do_not_exist(pytester: Pytester) -> None:\n    path = pytester.makepyfile(\"\"\" def test_logger(): pass \"\"\")\n    expected = os.path.join(os.path.dirname(str(path)), \"nonexistent\", \"logf.log\")\n    result = pytester.runpytest(f\"--log-file={expected}\")\n    assert \"logf.log\" in os.listdir(os.path.dirname(expected))\n    assert result.ret == ExitCode.OK\n", "def test_log_format_and_date_format(pytester: Pytester) -> None:\n    custom_format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    custom_date_format = \"%Y-%m-%d %H:%M:%S\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info('test message')\n        \"\"\"\n    )\n    result = pytester.runpytest(f\"--log-cli-format='{custom_format}'\", f\"--log-cli-date-format='{custom_date_format}'\")\n    result.stdout.fnmatch_lines(\n        [\"*test_log_format_and_date_format.py - root - INFO - test message*\"]\n    )\n", "def test_logger_level_restoration(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_level(logging.ERROR, 'test')\n            logger = logging.getLogger('test')\n            assert logger.level == logging.ERROR\n            caplog._finalize()\n            assert logger.level == logging.WARNING  # default level\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.OK\n", "def test_log_format(pytester: Pytester) -> None:\n    custom_format = \"%(levelname)s - %(message)s\"\n    pytester.makeini(\n        f\"\"\"\n        [pytest]\n        log_cli = true\n        log_cli_format = {custom_format}\n        \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n            logging.getLogger('test_log_format').warning(\"Custom format test\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-s\")\n    result.stdout.fnmatch_lines([\"*WARNING - Custom format test*\"])\n    assert result.ret == 0\n", "def test_live_logging_with_verbose(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n            logging.info(\"log message from test_log_verbose\")\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"-v\", \"--log-cli-level=INFO\")\n    result.stdout.fnmatch_lines(\n        [\n            \"test_live_logging_with_verbose.py::test_log_verbose \",\n            \"*-- live log call --*\",\n            \"INFO *test_live_logging_with_verbose.py* log message from test_log_verbose*\",\n            \"PASSED*\",\n        ]\n    )\n"], "sample_680": ["def test_xfail_imperative_in_teardown_function(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(\"hello\")\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n    result = testdir.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_this*\", \"*reason:*hello*\"])\n    result = testdir.runpytest(p, \"--runxfail\")\n    result.stdout.fnmatch_lines([\"*1 pass*\"])\n", "def test_marked_skipif_with_invalid_syntax(testdir) -> None:\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"invalid syntax\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_imperativeskip_on_skipif_test(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"False\")\n            assert 1\n\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n            pytest.skip(\"abc\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rsxX\")\n    result.stdout.fnmatch_lines_random(\n        \"\"\"\n        *SKIP*abc*\n        *1 passed*1 skipped*\n    \"\"\"\n    )\n", "def test_importorskip_success(testdir):\n    \"\"\"\n    Verify that importorskip() doesn't raise an exception when the import is successful\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.importorskip(\"os\")\n\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_marked_skipif_with_boolean_with_reason(self, testdir) -> None:\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(False, reason=\"test skip\")\n            pass\n        \"\"\"\n    )\n    skipped = evaluate_skip_marks(item)\n    assert skipped\n    assert skipped.reason == \"test skip\"\n"], "sample_551": ["def test_scatter3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    z = np.linspace(0, 1, 100)\n    x = z * np.sin(20 * z)\n    y = z * np.cos(20 * z)\n    ax.scatter3D(x, y, z, c=z, cmap='Greens')\n", "def test_patch_collection_depthshade():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    x = np.arange(10)\n    y = x**2\n    z = x\n    colors = np.random.rand(len(x), 3)\n    ax.scatter(x, y, z, c=colors, depthshade=True)\n    ax.set_title('3D Scatter Plot with Depth Shading')\n", "def test_text_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter([0, 1, 2], [0, 1, 2], [0, 1, 2], c='r', marker='o')\n    ax.text(0, 0, 0, \"Origin\", 'z', color='b')\n    ax.text(1, 1, 1, \"Middle\", 'x', color='g')\n    ax.text(2, 2, 2, \"End\", 'y', color='k')\n", "def test_3d_polygon_with_shading():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    verts = [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0), (0.5, 0.5, 1)]\n    poly = art3d.Poly3DCollection([verts], shade=True, lightsource=None)\n    ax.add_collection3d(poly)\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_zlim(0, 1)\n\n    ax.view_init(elev=20, azim=30)\n", "def test_text3d_orientation():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Generate some sample data\n    x, y, z = np.random.rand(3, 100)\n\n    # Create 3D text labels\n    for i in range(len(x)):\n        text = f'Point {i}'\n        ax.text(x[i], y[i], z[i], text, zdir='x')\n\n    # Set axis limits and labels\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.set_zlim([0, 1])\n\n    # Set the viewing angle\n    ax.view_init(elev=20, azim=30)\n"], "sample_839": ["def test_vectorizer_idf_attribute(Estimator):\n    data = [\"this is a test\", \"this is another test\"]\n    vectorizer = Estimator(use_idf=True)\n    vectorizer.fit(data)\n    assert hasattr(vectorizer, 'idf_')\n    assert len(vectorizer.idf_) == len(vectorizer.vocabulary_)\n", "def test_vectorizer_lowercase_option():\n    data = ['Hello World', 'Good Bye']\n    vec = CountVectorizer(lowercase=True)\n    X = vec.fit_transform(data)\n    feature_names = vec.get_feature_names()\n    assert 'hello' in feature_names\n    assert 'Hello' not in feature_names\n    assert 'world' in feature_names\n    assert 'good' in feature_names\n    assert 'bye' in feature_names\n", "def test_tfidfvectorizer_max_df_min_df_max_features():\n    test_data = ['abc', 'dea', 'eat']\n    vect = TfidfVectorizer(analyzer='char', max_df=0.6, min_df=0.4, max_features=2)\n    vect.fit(test_data)\n    assert 'a' not in vect.vocabulary_.keys()  # {bcdt} ignored\n    assert len(vect.vocabulary_.keys()) == 2    # {ae} remain\n    assert 'a' in vect.stop_words_\n    assert len(vect.stop_words_) == 2\n", "def test_callable_analyzer_no_warning(Estimator, input_type):\n    data = ['this is text, not file or filename']\n    with pytest.warns(None) as record:\n        Estimator(analyzer=lambda x: x.split(),\n                  input=input_type).fit_transform(data)\n    assert len(record) == 0\n", "def test_tfidfvectorizer_max_df_min_df():\n    # Test that max_df and min_df are correctly handled\n    docs = [\"This is a test document.\",\n            \"Another test document with words.\",\n            \"Test document words test test.\"]\n    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2)\n    X = vectorizer.fit_transform(docs)\n    assert \"document\" not in vectorizer.vocabulary_  # Should be removed by max_df\n    assert \"is\" not in vectorizer.vocabulary_  # Should be removed by min_df\n    assert \"test\" in vectorizer.vocabulary_  # Should be kept\n"], "sample_782": ["def test_column_transformer_callable_specifier_with_pandas():\n    # assert that function gets the full DataFrame\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n    X_res_first = np.array([[0, 1, 2]]).T\n\n        assert_array_equal(X, X_df)\n        return ['first']\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == ['first']\n", "def test_column_transformer_named_transformers_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', 'passthrough', [1])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert isinstance(ct.named_transformers_['trans1'], StandardScaler)\n    assert 'trans2' not in ct.named_transformers_  # 'passthrough' is not added\n    assert isinstance(ct.named_transformers_['remainder'], StandardScaler)\n", "def test_column_transformer_empty_transformers():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([], remainder='drop')\n    assert_raises(ValueError, ct.fit_transform, X_array)\n    assert_raises(ValueError, ct.fit, X_array)\n", "def test_column_transformer_empty_dataframe():\n    # test case that ensures the ColumnTransformer works with an empty dataframe\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame()\n\n    ct = ColumnTransformer([('trans', Trans(), [])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df), np.array([]).reshape(0, 0))\n    assert_array_equal(ct.fit(X_df).transform(X_df), np.array([]).reshape(0, 0))\n    assert len(ct.transformers_) == 2  # including remainder\n    assert isinstance(ct.transformers_[0][1], Trans)\n", "def test_column_transformer_callable_specifier_pandas():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n    X_res_first = np.array([[0, 1, 2]]).T\n\n        assert_array_equal(X.columns, X_df.columns)\n        assert_array_equal(X.values, X_df.values)\n        return ['first']\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == ['first']\n"], "sample_945": ["def test_pyfunction_signature_with_default_values(app):\n    text = \".. py:function:: hello(name: str = 'World') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"str\"],\n                                                      \" \",\n                                                      [desc_sig_operator, \"=\"],\n                                                      \" \",\n                                                      [nodes.inline, \"'World'\"])])\n", "def test_pyproperty_with_union_type_operator(app):\n    text = \".. py:property:: prop -> int | None\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0],\n                ([desc_annotation, \"property \"],\n                 [desc_name, \"prop\"],\n                 [desc_returns, ([pending_xref, \"int\"],\n                                 \" \",\n                                 [desc_sig_punctuation, \"|\"],\n                                 \" \",\n                                 [pending_xref, \"None\"])])\n                )\n", "def test_pyexception_signature_with_module_option(app):\n    text = (\".. py:exception:: IOError\\n\"\n            \"   :module: exceptions\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n    assert 'exceptions.IOError' in domain.objects\n    assert domain.objects['exceptions.IOError'] == ('index', 'exceptions.IOError', 'exception', False)\n", "def test_resolve_xref_for_properties_with_module_option(app, status, warning):\n    app.build()\n\n    content = (app.outdir / 'module_option.html').read_text()\n    assert ('Link to <a class=\"reference internal\" href=\"#test.extra.B.prop\"'\n            ' title=\"test.extra.B.prop\">'\n            '<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">'\n            'prop</span> <span class=\"pre\">attribute</span></code></a>' in content)\n    assert ('Link to <a class=\"reference internal\" href=\"#test.extra.B.prop\"'\n            ' title=\"test.extra.B.prop\">'\n            '<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">'\n            'prop</span> <span class=\"pre\">method</span></code></a>' in content)\n", "def test_pydata_signature_type_annotation(app):\n    text = (\".. py:data:: version\\n\"\n            \"   :type: typing.Union[int, str]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0],\n                ([desc_name, \"version\"],\n                 [desc_annotation, (\": \",\n                                    [pending_xref, \"typing.Union\"],\n                                    [desc_sig_punctuation, \"[\"],\n                                    [pending_xref, \"int\"],\n                                    [desc_sig_punctuation, \", \"],\n                                    [pending_xref, \"str\"],\n                                    [desc_sig_punctuation, \"]\"])]))\n"], "sample_571": ["def test_regplot_line_kws(self):\n\n    f, ax = plt.subplots()\n    color = 'g'\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, line_kws={'color': color})\n    assert ax.lines[0].get_color() == color\n\n    f, ax = plt.subplots()\n    linestyle = '--'\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, line_kws={'linestyle': linestyle})\n    assert ax.lines[0].get_linestyle() == linestyle\n", "def test_regplot_no_data(self):\n    with pytest.raises(TypeError):\n        # keyword argument `data` is required\n        lm.regplot(x=\"x\", y=\"y\")\n", "def test_regplot_color(self):\n\n    color = (0.5, 0.2, 0.8)\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n    scatter_color = ax.collections[0].get_facecolors()[0, :3]\n    line_color = ax.lines[0].get_color()\n    npt.assert_array_equal(scatter_color, line_color)\n    npt.assert_array_equal(line_color, color)\n", "def test_regplot_color(self):\n\n    color = (0.2, 0.8, 0.5)\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n\n    scatter_color = ax.collections[0].get_facecolors()[0, :3]\n    line_color = ax.lines[0].get_color()\n\n    npt.assert_array_equal(scatter_color, color)\n    npt.assert_array_equal(line_color, color)\n", "def test_regplot_color(self):\n\n    f, ax = plt.subplots()\n    color = (1, 0, 0)\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, color=color)\n    assert ax.lines[0].get_color() == color\n    assert ax.collections[0].get_facecolors()[0, :3] == color\n"], "sample_163": ["def test_logout_then_login_with_custom_login_redirect_url(self):\n    self.login()\n    req = HttpRequest()\n    req.method = \"POST\"\n    csrf_token = get_token(req)\n    req.COOKIES[settings.CSRF_COOKIE_NAME] = csrf_token\n    req.POST = {\"csrfmiddlewaretoken\": csrf_token}\n    req.session = self.client.session\n    with self.settings(LOGIN_URL=\"/custom_login/\"):\n        response = logout_then_login(req)\n        self.confirm_logged_out()\n        self.assertRedirects(response, \"/custom_login/\", fetch_redirect_response=False)\n", "def test_logout_then_login_redirect_url_setting(self):\n    self.login()\n    req = HttpRequest()\n    req.method = \"POST\"\n    csrf_token = get_token(req)\n    req.COOKIES[settings.CSRF_COOKIE_NAME] = csrf_token\n    req.POST = {\"csrfmiddlewaretoken\": csrf_token}\n    req.session = self.client.session\n    response = logout_then_login(req)\n    self.confirm_logged_out()\n    self.assertRedirects(response, \"/custom/\", fetch_redirect_response=False)\n", "    def test_extra_email_context_available_in_template(self):\n        response = self.client.post(\n            \"/password_reset_extra_email_context/\",\n            {\"email\": \"staffmember@example.com\"},\n        )\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertIn('Email email context: \"Hello!\"', mail.outbox[0].body)\n        self.assertIn(\"http://custom.example.com/reset/\", mail.outbox[0].body)\n", "    def test_login_view_extra_context(self):\n        response = self.client.get(reverse(\"login\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(\"extra_context_key\", response.context)\n        self.assertEqual(response.context[\"extra_context_key\"], \"extra_context_value\")\n", "def test_password_reset_confirm_invalid_token_message(self):\n    # Arrange\n    url, path = self._test_confirm_start()\n    path = path[:-5] + (\"0\" * 4) + path[-1]  # Invalidate the token\n\n    # Act\n    response = self.client.get(path)\n\n    # Assert\n    self.assertContains(response, \"The password reset link was invalid.\")\n"], "sample_637": ["def test_fixme_with_disable_pragma(self) -> None:\n    code = \"\"\"a = 1\n              # pylint: disable=fixme\n              # FIXME message\n              \"\"\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_notes_rgx(self) -> None:\n    code = \"\"\"a = 1\n            # FOO\n            # FIXME\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"FOO\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_regex_codetag(self) -> None:\n    code = \"\"\"a = 1\n            #???\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"???\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_regex_codetag(self) -> None:\n    code = \"\"\"a = 1\n            # CODETAG\n            # CODENOTE\n            # FIXME\n            # TODO\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=3, args=\"CODENOTE\", col_offset=17),\n        MessageTest(msg_id=\"fixme\", line=4, args=\"FIXME\", col_offset=17),\n        MessageTest(msg_id=\"fixme\", line=5, args=\"TODO\", col_offset=17),\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_regex_notes(self) -> None:\n    code = \"\"\"a = 1\n                # HACK this should trigger a fixme\n                \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"HACK this should trigger a fixme\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n"], "sample_667": ["def test_tmpdir_factory_unique_directories(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture(scope='session')\n            return tmpdir_factory.mktemp('data')\n        @pytest.fixture(scope='session')\n            return tmpdir_factory.mktemp('data')\n            assert session_dir1 != session_dir2\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_tmp_path_factory_with_custom_basetemp(testdir):\n    mytemp = testdir.tmpdir.mkdir(\"mytemp\")\n    p = testdir.makepyfile(\n        \"\"\"\n            assert tmp_path_factory.getbasetemp().resolve() == mytemp.resolve()\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert result.ret == 0\n", "def test_tmp_path_factory_custom_removes_old(testdir):\n    mytemp = testdir.tmpdir.join(\"xyz\")\n    p = testdir.makepyfile(\n        \"\"\"\n            tmp_path_factory.mktemp('hello', numbered=False)\n        \"\"\"\n    )\n    testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n    mytemp.check()\n    mytemp.ensure(\"hello\")\n\n    testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n    mytemp.check()\n    assert not mytemp.join(\"hello\").check()\n", "def test_tmppath_factory_no_basetemp(tmp_path, monkeypatch):\n    \"\"\"Test TempPathFactory with no basetemp configured.\"\"\"\n    from _pytest.tmpdir import TempPathFactory\n\n    monkeypatch.delenv(\"PYTEST_DEBUG_TEMPROOT\", raising=False)\n    config = FakeConfig(None)\n    t = TempPathFactory.from_config(config)\n    tmp = t.mktemp(\"test\")\n    assert tmp.exists()\n", "def test_make_numbered_dir_with_cleanup(tmp_path):\n    from _pytest.pathlib import make_numbered_dir_with_cleanup, LOCK_TIMEOUT\n    from _pytest.tmpdir import TempPathFactory\n\n    prefix = \"test-\"\n    root = tmp_path\n    keep = 3\n    dirs = [make_numbered_dir_with_cleanup(prefix, root, keep, LOCK_TIMEOUT) for _ in range(5)]\n\n    assert len(os.listdir(root)) == keep\n    for i in range(keep - 1, -1, -1):\n        assert os.path.isdir(os.path.join(root, f\"{prefix}{i}\"))\n\n    t = TempPathFactory(given_basetemp=root, trace=lambda *k: None)\n    t.getbasetemp()  # This should trigger cleanup\n    assert len(os.listdir(root)) == 1\n    assert os.path.isdir(os.path.join(root, f\"{prefix}{keep-1}\"))\n"], "sample_337": ["def test_custom_csrf_cookie_name(self):\n    \"\"\"\n    CSRF cookie name can be set using settings.CSRF_COOKIE_NAME.\n    \"\"\"\n    req = self._get_request()\n\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    self.assertIn('mycsrfcookie', resp.cookies)\n", "def test_https_good_referer_behind_proxy_with_forwarded_port(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted when\n    USE_X_FORWARDED_PORT=True and a different port is used.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'https://example.com/'\n    req.META['SERVER_PORT'] = '8443'\n    req.META['HTTP_X_FORWARDED_HOST'] = 'example.com'\n    req.META['HTTP_X_FORWARDED_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_https_good_referer_subdomain_different_case(self):\n    \"\"\"\n    A POST HTTPS request with a good referer is accepted even if the subdomain\n    has a different case.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_REFERER'] = 'https://FoO.ExAmPlE.CoM'\n    mw = CsrfViewMiddleware(post_form_view)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_https_good_referer_custom_header(self):\n    \"\"\"\n    A POST HTTPS request with a good referer is accepted even if it uses a custom header.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_REFERER'] = 'https://www.example.com/somepage'\n    req.META['HTTP_X_CSRFTOKEN_CUSTOM'] = self._csrf_id_token\n    with self.settings(CSRF_HEADER_NAME='HTTP_X_CSRFTOKEN_CUSTOM'):\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "def test_masked_unmasked_cookies(self):\n    \"\"\"\n    Masked and unmasked cookies are allowed both as POST and as the\n    X-CSRFToken header.\n    \"\"\"\n    cases = [\n        # Bare secrets are not allowed when CSRF_USE_SESSIONS=True.\n        (TEST_SECRET, None, None),\n        (MASKED_TEST_SECRET1, None, None),\n        (None, TEST_SECRET, None),\n        (None, MASKED_TEST_SECRET2, None),\n    ]\n    for args in cases:\n        with self.subTest(args=args):\n            cookie, post_token, meta_token = args\n            req = self._get_POST_csrf_cookie_request(\n                cookie=cookie, post_token=post_token, meta_token=meta_token,\n            )\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_request(req)\n            resp = mw.process_view(req, token_view, (), {})\n            self.assertIsNone(resp)\n"], "sample_59": ["def test_model_save_and_delete(self):\n    # Test model instance save and delete methods\n    worker = Worker(name=\"John Doe\", department_id=1)\n    worker.save()\n    self.assertIsNotNone(worker.id)\n\n    worker_id = worker.id\n    worker.delete()\n    with self.assertRaises(Worker.DoesNotExist):\n        Worker.objects.get(id=worker_id)\n", "def test_foreign_key_null(self):\n    # Saving a null value in a ForeignKey field is allowed\n    d = Department.objects.create(id=10, name=\"IT\")\n    w = Worker.objects.create(name=\"Full-time\")\n    w.department = None\n    w.save()\n    self.assertIsNone(w.department)\n", "def test_get_next_or_previous_in_order(self):\n    Event.objects.create(when=datetime.datetime(2000, 1, 1, 12, 0, 0), _order=1)\n    Event.objects.create(when=datetime.datetime(2000, 1, 1, 13, 0, 0), _order=2)\n    e = Event.objects.create(when=datetime.datetime(2000, 1, 1, 14, 0, 0), _order=3)\n    self.assertEqual(\n        e.get_next_in_order().when, datetime.datetime(2000, 1, 1, 13, 0, 0)\n    )\n    self.assertEqual(\n        e.get_previous_in_order().when, datetime.datetime(2000, 1, 1, 12, 0, 0)\n    )\n", "def test_model_save_base(self):\n    \"\"\"\n    Test the _save_base method of the Model class.\n    \"\"\"\n    # Create a new instance of the Worker model\n    worker = Worker(name=\"Test Worker\")\n\n    # Save the instance using the _save_base method\n    worker.save_base()\n\n    # Check that the instance was saved correctly\n    self.assertIsNotNone(worker.pk)\n    self.assertEqual(Worker.objects.count(), 1)\n    saved_worker = Worker.objects.get(pk=worker.pk)\n    self.assertEqual(saved_worker.name, \"Test Worker\")\n\n    # Update the instance using the _save_base method\n    worker.name = \"Updated Worker\"\n    worker.save_base()\n\n    # Check that the instance was updated correctly\n    saved_worker = Worker.objects.get(pk=worker.pk)\n    self.assertEqual(saved_worker.name, \"Updated Worker\")\n", "def test_related_object_cache(self):\n    \"\"\"\n    Regression test for #19935: If a related object is deleted, the cache\n    should be invalidated.\n    \"\"\"\n    d = Department.objects.create(id=10, name=\"IT\")\n    w = Worker.objects.create(department=d, name=\"Full-time\")\n    self.assertEqual(w.department, d)\n    d.delete()\n    with self.assertRaises(Department.DoesNotExist):\n        w.department\n"], "sample_1085": ["def test_Float_floor_ceiling():\n    a = Float(2.7)\n    b = Float(-2.7)\n\n    assert a.floor() == Float(2)\n    assert a.ceiling() == Float(3)\n    assert b.floor() == Float(-3)\n    assert b.ceiling() == Float(-2)\n", "def test_Float_ceiling_floor():\n    a = Float(4.2)\n    b = Float(4.8)\n\n    assert a.floor() == 4.0\n    assert b.floor() == 4.0\n    assert a.ceiling() == 5.0\n    assert b.ceiling() == 5.0\n", "def test_Float_power():\n    a = Float(2)\n    b = Float(3)\n    assert a**b == Float(8)\n    assert b**a == Float(9)\n    assert a**Float(0.5) == Float(1.4142135623730951)\n    assert Float(0.5)**b == Float(0.125)\n    assert a**S.NaN is S.NaN\n    assert Float(0)**Float(0) is S.NaN\n    assert Float(0)**Float(-1) is S.ComplexInfinity\n", "def test_issue_11676():\n    assert comp(Float(1, 2), 1, 0.1)\n    assert not comp(Float(1, 2), 1, 0.001)\n", "def test_Zero_power():\n    assert S.Zero**12 is S.Zero\n    assert S.Zero**(-12) is zoo\n    assert S.Zero**0 is S.NaN\n"], "sample_54": ["def test_file_from_contentfile_response(self):\n    content = ContentFile(b'binary content', name='test.txt')\n    response = FileResponse(content)\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"test.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_contentfile(self):\n    content = b'test content'\n    f = ContentFile(content)\n    response = FileResponse(f, as_attachment=True, filename='test.txt')\n    self.assertEqual(response['Content-Length'], str(len(content)))\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n    self.assertEqual(list(response), [content])\n", "def test_file_from_disk_with_filename(self):\n    filename = \"custom_file.txt\"\n    with tempfile.NamedTemporaryFile(delete=False) as temp:\n        temp.write(b\"file content\")\n        temp_name = temp.name\n    response = FileResponse(open(temp_name, 'rb'), filename=filename)\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(temp_name)))\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{filename}\"')\n    response.close()\n    os.unlink(temp_name)\n", "def test_file_from_disk_with_custom_filename(self):\n    custom_filename = \"custom_name.py\"\n    response = FileResponse(open(__file__, 'rb'), filename=custom_filename)\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{custom_filename}\"')\n    response.close()\n", "def test_file_from_contentfile_response(self):\n    content = b'binary content'\n    response = FileResponse(ContentFile(content))\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertFalse(response.has_header('Content-Disposition'))\n    self.assertEqual(list(response), [content])\n"], "sample_491": ["def test_cyclic_context_boundfield_legend_tag(self):\n    class FirstNameForm(Form):\n        first_name = CharField()\n        template_name_label = \"forms_tests/cyclic_context_boundfield_render.html\"\n\n    f = FirstNameForm()\n    try:\n        f['first_name'].legend_tag()\n    except RecursionError:\n        self.fail(\"Cyclic reference in BoundField.legend_tag().\")\n", "def test_errorlist_custom_template_name(self):\n    class CommentForm(Form):\n        name = CharField(max_length=50, required=False)\n        email = EmailField()\n        comment = CharField()\n\n    data = {\"email\": \"invalid\"}\n    f = CommentForm(data, auto_id=False)\n    error_list = f.errors.as_ul(error_class=ErrorList, template_name=\"forms_tests/error.html\")\n    self.assertHTMLEqual(\n        error_list,\n        '<div class=\"errorlist\">'\n        '<div class=\"error\">Enter a valid email address.</div>'\n        '<div class=\"error\">This field is required.</div></div>',\n    )\n", "def test_template_name_label_override(self):\n    class Person(Form):\n        first_name = CharField()\n        template_name_label = \"forms_tests/custom_label.html\"\n\n    f = Person(auto_id=False)\n    self.assertHTMLEqual(\n        f[\"first_name\"].label_tag(),\n        '<label for=\"id_first_name\">Custom Label:</label>',\n    )\n", "    def test_custom_error_message(self):\n        class MyForm(Form):\n            username = CharField(max_length=10, error_messages={'required': 'Custom required message'})\n\n        f = MyForm()\n        self.assertHTMLEqual(\n            f.as_p(),\n            '<ul class=\"errorlist\"><li>Custom required message</li></ul>'\n            '<p><label for=\"id_username\">Username:</label>'\n            '<input id=\"id_username\" maxlength=\"10\" name=\"username\" type=\"text\" required></p>',\n        )\n", "    def test_custom_boundfield(self):\n        class CustomBoundField(BoundField):\n            template_name = \"forms_tests/custom_boundfield.html\"\n\n        class CustomField(CharField):\n            bound_field_class = CustomBoundField\n\n        class CustomForm(Form):\n            custom_field = CustomField()\n\n        form = CustomForm()\n        self.assertHTMLEqual(\n            form[\"custom_field\"].render(),\n            '<label for=\"id_custom_field\">Custom field:</label>'\n            '<p>Custom BoundField<p>'\n            '<input type=\"text\" name=\"custom_field\" required id=\"id_custom_field\">',\n        )\n"], "sample_327": ["def test_valid_json_string(self):\n    field = JSONField()\n    value = field.clean('\"foo\"')\n    self.assertEqual(value, 'foo')\n", "def test_invalid_json_input(self):\n    field = JSONField()\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean(field.bound_data('{some badly formed: json}', None))\n", "def test_invalid_json_string(self):\n    field = JSONField()\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean('{\"key\": \"value\\\\\"}')\n", "def test_custom_validator(self):\n        if 'error' in value:\n            raise ValidationError('Error found in JSON data')\n\n    field = JSONField(required=True, validators=[validate_json])\n    with self.assertRaisesMessage(ValidationError, 'Error found in JSON data'):\n        field.clean('{\"error\": \"This is an error\"}')\n", "def test_custom_widget_instance(self):\n    \"\"\"The widget can be overridden with an instance.\"\"\"\n    field = JSONField(widget=TextInput())\n    self.assertIsInstance(field.widget, TextInput)\n"], "sample_450": ["def test_get_admin_log_tag(self):\n    \"\"\"\n    Test the get_admin_log template tag.\n    \"\"\"\n    # Create more log entries for testing\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something else\",\n    )\n    # Render a template with the get_admin_log tag\n    template = Template(\"{% load admin_utils %}{% get_admin_log 2 as admin_log for_user user %}\")\n    context = Context({\"log_entries\": LogEntry.objects.all(), \"user\": self.user})\n    rendered = template.render(context)\n    # Check that the rendered output is as expected\n    self.assertEqual(len(context[\"admin_log\"]), 2)\n    self.assertEqual(context[\"admin_log\"][0].user, self.user)\n    self.assertEqual(context[\"admin_log\"][1].user, self.user)\n", "def test_get_admin_log_template_tag(self):\n    \"\"\"\n    Test the 'get_admin_log' template tag.\n    \"\"\"\n    # Create some additional log entries\n    user2 = User.objects.create_user(username=\"user2\", password=\"password2\")\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        user2.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something by user2\",\n    )\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something by super\",\n    )\n\n    # Render a template with the 'get_admin_log' tag\n    template = Template(\"{% load admin_utils %}{% get_admin_log 2 as admin_log for_user user %}\")\n    context = Context({\"log_entries\": LogEntry.objects.all(), \"user\": user2})\n    rendered = template.render(context)\n\n    # Check the rendered output\n    self.assertIn(\"Changed something by user2\", rendered)\n    self.assertNotIn(\"Changed something by super\", rendered)\n", "def test_get_admin_log_tag_filtering_user(self):\n    \"\"\"\n    Test the 'get_admin_log' template tag with filtering by user.\n    \"\"\"\n    template = Template(\"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user user.pk %}\")\n    context = Context({\"log_entries\": LogEntry.objects.all(), \"user\": self.user})\n    template.render(context)\n    self.assertEqual(context['admin_log'].count(), LogEntry.objects.filter(user=self.user).count())\n", "def test_admin_log_template_tag(self):\n    \"\"\"\n    Test the custom template tag 'get_admin_log' which populates a template\n    variable with the admin log for the given criteria.\n    \"\"\"\n    # Create some additional log entries for testing\n    User.objects.create_user(username=\"user1\", password=\"secret\", email=\"user1@example.com\")\n    User.objects.create_user(username=\"user2\", password=\"secret\", email=\"user2@example.com\")\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        User.objects.get(username=\"user1\").pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something by user1\",\n    )\n    LogEntry.objects.log_action(\n        User.objects.get(username=\"user2\").pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something by user2\",\n    )\n\n    # Render a template with the custom template tag\n    template = Template(\"{% load admin_log %}{% get_admin_log 2 as admin_log for_user user1 %}\")\n    context = Context({\"log_entries\": LogEntry.objects.all(), \"user1\": User.objects.get(username=\"user1\")})\n    template.render(context)\n\n    # Check the result\n    self.assertEqual(len(context[\"admin_log\"]), 2)\n    self.assertEqual(context[\"admin_log\"][0].user, User.objects.get(username=\"user1\"))\n    self.assertEqual(context[\"admin_log\"][1].user, User.objects.get(username=\"super\"))\n", "def test_get_admin_log_tag(self):\n    \"\"\"\n    Test the get_admin_log template tag.\n    \"\"\"\n    # Create a few more log entries for the user\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    for i in range(5):\n        LogEntry.objects.log_action(\n            self.user.pk,\n            content_type_pk,\n            self.a1.pk,\n            repr(self.a1),\n            CHANGE,\n            change_message=f\"Changed something {i}\",\n        )\n\n    # Test getting the log entries for the user\n    response = self.client.get(reverse(\"admin_utils:test_template\") + \"?user=1\")\n    self.assertContains(response, \"Changed something 0\")\n    self.assertContains(response, \"Changed something 1\")\n    self.assertNotContains(response, \"Changed something 5\")  # Limit is 3\n\n    # Test getting the log entries for a different user\n    response = self.client.get(reverse(\"admin_utils:test_template\") + \"?user=2\")\n    self.assertNotContains(response, \"Changed something 0\")\n    self.assertNotContains(response, \"Changed something 1\")\n"], "sample_856": ["def test_leave_p_out_empty_trainset():\n    cv = LeavePOut(p=2)\n    X, y = [[1]], [0]  # 1 sample\n    with pytest.raises(\n            ValueError,\n            match='Cannot perform LeavePOut with p=2 and n_samples=1'):\n        next(cv.split(X, y))\n", "def test_leave_p_out_empty_trainset():\n    cv = LeavePOut(p=2)\n    X, y = [[1]], [0]  # 1 sample\n    with pytest.raises(\n            ValueError,\n            match='Cannot have number of samples n_samples=1 smaller than p=2'):\n        next(cv.split(X, y))\n", "def test_group_shuffle_split_empty_trainset():\n    groups = [1]  # 1 sample, 1 unique group\n    X, y = [[1]], [0]\n    cv = GroupShuffleSplit(test_size=.99)\n    with pytest.raises(\n            ValueError,\n            match='With n_samples=1, test_size=0.99 and train_size=None, '\n            'the resulting train set will be empty'):\n        next(cv.split(X, y, groups))\n", "def test_leave_one_p_group_out_empty_trainset():\n    cv = LeaveOneGroupOut()\n    X, y, groups = [[1]], [0], [0]  # 1 sample, 1 group\n    with pytest.raises(\n            ValueError,\n            match='The groups parameter contains fewer than 2 unique groups.'):\n        next(cv.split(X, y, groups))\n\n    cv = LeavePGroupsOut(n_groups=2)\n    X, y, groups = [[1], [1]], [0, 0], [0, 1]  # 2 samples, 2 groups\n    with pytest.raises(\n            ValueError,\n            match='The groups parameter contains fewer than (or equal to) n_groups'):\n        next(cv.split(X, y, groups))\n", "def test_shuffle_split_large_testset(CVSplitter):\n    cv = CVSplitter(test_size=1000)\n    X, y = np.ones((5, 2)), np.ones(5)  # 5 samples\n    with pytest.raises(\n            ValueError,\n            match='test_size=1000 should be either positive and smaller than '\n            'the number of samples 5 or a float in the \\(0, 1\\) range'):\n        next(cv.split(X, y, groups=np.ones(5)))\n"], "sample_875": ["def test_precision_recall_fscore_support_binary(y_true, y_pred, average, expected):\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=average)\n    if average is None:\n        assert_array_almost_equal(p, expected)\n        assert_array_almost_equal(r, expected)\n        assert_array_almost_equal(", "def test_balanced_accuracy_score_sample_weight():\n    y_true = np.array([\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"])\n    y_pred = np.array([\"a\", \"b\", \"a\", \"b\", \"c\", \"c\"])\n    sample_weight = np.array([1, 2, 1, 2, 1, 2])\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    expected = (1 * 2 + 2 * 2) / (2 * 3 + 2 * 3)\n    assert balanced == pytest.approx(expected)\n", "def test_balanced_accuracy_score_multilabel():\n    # Test balanced_accuracy_score with multilabel-indicator inputs\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [0, 0, 1]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [1, 0, 1]])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == pytest.approx(0.4444, abs=1e-4)\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    chance = balanced_accuracy_score(y_true, np.zeros_like(y_true))\n    assert adjusted == (balanced - chance) / (1 - chance)\n", "def test_balanced_accuracy_score_with_sample_weight():\n    y_true = np.array([\"a\", \"b\", \"a\", \"b\"])\n    y_pred = np.array([\"a\", \"a\", \"a\", \"b\"])\n    sample_weight = np.array([0.5, 1, 0.5, 1])\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    expected_balanced = (0.5 * 0.5 + 1) / 2  # Manually calculated expected balanced accuracy\n    assert balanced == pytest.approx(expected_balanced)\n", "def test_balanced_accuracy_score_weighted(y_true, y_pred, sample_weight, average, expected):\n    score = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight, average=average)\n    assert score == pytest.approx(expected)\n"], "sample_980": ["def test_commutator():\n    p = Permutation([0, 2, 3, 1])\n    x = Permutation([2, 0, 3, 1])\n    c = p.commutator(x)\n    assert c == Permutation([2, 1, 3, 0])\n    assert c == ~x*~p*x*p\n\n    I = Permutation(3)\n    p = [I + i for i in range(6)]\n    for i in range(len(p)):\n        for j in range(len(p)):\n            c = p[i].commutator(p[j])\n            if p[i]*p[j] == p[j]*p[i]:\n                assert c == I\n            else:\n                assert c != I\n", "def test_commutes_with():\n    p = Permutation([1, 5, 2, 0, 3, 6, 4])\n    q = Permutation([[1, 2, 3, 5, 6], [0, 4]])\n    assert p.commutes_with(q) == False\n    assert q.commutes_with(p) == False\n    assert p.commutes_with(p) == True\n    assert q.commutes_with(q) == True\n", "def test_printing_array():\n    Permutation.print_cyclic = False\n    p1 = Permutation([0, 2, 1])\n    assert repr(p1) == 'Permutation([0, 2, 1])'\n    assert str(p1) == 'Permutation([0, 2, 1])'\n    p2 = Permutation()\n    assert repr(p2) == 'Permutation([])'\n    assert str(p2) == 'Permutation([])'\n    p3 = Permutation([1, 2, 0, 3])\n    assert repr(p3) == 'Permutation([1, 2, 0, 3])'\n    assert str(p3) == 'Permutation([1, 2, 0, 3])'\n", "def test_from_sequence_with_duplicates():\n    assert Permutation.from_sequence('SymPySymPy') == Permutation(4)(0, 1, 3)(8, 9)\n    assert Permutation.from_sequence('SymPySymPy', key=lambda x: x.lower()) == Permutation(4)(0, 2)(1, 3)(8, 10)\n", "def test_printing_non_cyclic():\n    Permutation.print_cyclic = False\n    p1 = Permutation([0, 2, 1])\n    assert repr(p1) == 'Permutation([0, 2, 1])'\n    assert str(p1) == 'Permutation([0, 2, 1])'\n    p2 = Permutation()\n    assert repr(p2) == 'Permutation([])'\n    assert str(p2) == 'Permutation([])'\n    p3 = Permutation([1, 2, 0, 3])\n    assert repr(p3) == 'Permutation([1, 2, 0, 3])'\n    assert str(p3) == 'Permutation([1, 2, 0, 3])'\n"], "sample_824": ["def test_check_mismatched_dimensions():\n    # Ensure an error is raised if the dimensions are different.\n    XA = np.resize(np.arange(45), (5, 9))\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n", "def test_check_different_sparsity():\n    # Ensure an error is raised if the sparsity is different.\n    XA = np.resize(np.arange(45), (5, 9))\n    XB_sparse = csr_matrix(XA)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB_sparse)\n", "def test_check_boolean_input():\n    # Ensures that checks return valid boolean arrays.\n    XA = np.array([[True, False], [True, True]])\n    XB = np.array([[False, True], [False, False]])\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype=bool)\n    assert_array_equal(XA_checked, XA)\n    assert_array_equal(XB_checked, XB)\n", "def test_check_dtype():\n    # Ensures that type float32 is preserved.\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype=np.float64)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    # Test that the warning is raised when the dtype is bool and the input is not\n    XA_bool = (XA > 0.5).astype(np.bool)\n    XB_bool = (XB > 0.5).astype(np.bool)\n\n    with pytest.warns(DataConversionWarning, match=\"Data was converted to boolean\"):\n        check_pairwise_arrays(XA_bool, XB_bool, dtype=np.bool)\n\n    # Test that the warning is not raised when the dtype is bool and the input is already bool\n    with pytest.warns(None) as records:\n        check_pairwise_arrays(XA_bool, XB_bool, dtype=np.bool)\n    assert len(records) == 0\n", "def test_check_different_shapes():\n    # Ensure an error is raised if the shapes are different.\n    XA = np.resize(np.arange(45), (5, 9))\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, XB)\n"], "sample_438": ["def test_get_object_cache_with_multiple_objects(self):\n    question1 = Question.objects.create(text=\"Who?\")\n    question2 = Question.objects.create(text=\"What?\")\n    post1 = Post.objects.create(title=\"Answer 1\", parent=question1)\n    post2 = Post.objects.create(title=\"Answer 2\", parent=question2)\n\n    question1_pk = question1.pk\n    question2_pk = question2.pk\n    Question.objects.all().delete()\n\n    post1 = Post.objects.get(pk=post1.pk)\n    post2 = Post.objects.get(pk=post2.pk)\n    with self.assertNumQueries(2):\n        self.assertEqual(post1.object_id, question1_pk)\n        self.assertIsNone(post1.parent)\n        self.assertEqual(post2.object_id, question2_pk)\n        self.assertIsNone(post2.parent)\n", "def test_clear_cached_generic_relation_after_update(self):\n    question = Question.objects.create(text=\"What is your favorite color?\")\n    answer = Answer.objects.create(text=\"Red\", question=question)\n    old_entity = answer.question\n    question.text = \"What is your favorite food?\"\n    question.save()\n    answer.refresh_from_db()\n    new_entity = answer.question\n    self.assertIsNot(old_entity, new_entity)\n", "def test_get_content_type_with_instance(self):\n    question = Question.objects.create(text=\"Who?\")\n    answer = Answer.objects.create(text=\"Answer\", question=question)\n    content_type = Answer.question.get_content_type(instance=answer)\n    self.assertEqual(content_type, question.content_type)\n", "def test_generic_foreign_key_multiple_inheritance(self):\n    class ParentModel(models.Model):\n        content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n        object_id = models.PositiveIntegerField()\n        content_object = GenericForeignKey('content_type', 'object_id')\n\n    class ChildModel(ParentModel):\n        extra_field = models.CharField(max_length=100)\n\n    child_obj = ChildModel.objects.create(extra_field='test')\n    parent_obj = ParentModel.objects.get(id=child_obj.id)\n\n    self.assertEqual(parent_obj.content_object, child_obj)\n", "def test_generic_relation_with_json_field(self):\n    question = Question.objects.create(text=\"How are you?\")\n    data = {\"question_id\": question.id, \"content_type_id\": question.content_type.id}\n    Answer.objects.create(text=\"I'm fine\", content_object=json.dumps(data))\n\n    question = Question.objects.get(id=question.id)\n    self.assertEqual(question.answers.count(), 1)\n    self.assertEqual(question.answers.first().text, \"I'm fine\")\n"], "sample_671": ["def test_skip_allow_module_level_with_error(testdir):\n    \"\"\"\n    Verify that using pytest.skip(allow_module_level=True) with an error raises an error\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.skip(\"skip_module_level\", allow_module_level=True, error=True)\n\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*TypeError:*['error']*\"])\n", "def test_skip_no_reason_with_allow_module_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.skip(allow_module_level=True)\n\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines([\"*SKIP*unconditional skip*\"])\n", "def test_xfail_strict_with_condition_keyword(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(condition=True, reason='unsupported feature', strict=True)\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rxX\")\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n    assert result.ret == 0\n", "def test_skip_with_allow_module_level_parameter(testdir):\n    \"\"\"\n    Verify that using pytest.skip(allow_module_level=True) at module level is allowed\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.skip(\"skip_module_level\", allow_module_level=True)\n\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines([\"*SKIP*skip_module_level\"])\n", "def test_xfail_imperative_reason(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(\"reason\")\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_this*\", \"*reason*\"])\n"], "sample_564": ["def test_mutating_input_arrays_x_and_z(fig_test, fig_ref):\n    \"\"\"\n    Test to see if the `x` axis does not get mutated\n    after a call to `Axes3D.plot`\n    \"\"\"\n    ax1 = fig_test.add_subplot(111, projection='3d')\n    x = [0.0, 0.0, 0.0]\n    y = [1, 2, 3]\n    z = [1, 2, 3]\n    ax1.plot(x, y, z, 'o-')\n\n    # mutate x to get a nontrivial line\n    x[:] = [1, 2, 3]\n\n    # draw the same plot without mutating x and z\n    ax2 = fig_ref.add_subplot(111, projection='3d')\n    x = [0.0, 0.0, 0.0]\n    y = [1, 2, 3]\n    z = [1, 2, 3]\n    ax2.plot(x, y, z, 'o-')\n", "def test_panecolor_property():\n    fig = plt.figure(figsize=(1, 1))\n    ax = fig.add_subplot(projection='3d')\n    ax.xaxis.set_pane_color('r')\n    ax.yaxis.set_pane_color('g')\n    ax.zaxis.set_pane_color('b')\n", "def test_contourf3d_extend3d_regression():\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1, projection='3d')\n    X, Y, Z = axes3d.get_test_data(0.05)\n    ax.contourf(X, Y, Z, zdir='z', offset=-100, cmap=cm.coolwarm, extend3d=True)\n    ax.set_xlim(-30, 30)\n    ax.set_ylim(-20, 40)\n    ax.set_zlim(-80, 80)\n", "def test_panecolor_method():\n    fig = plt.figure(figsize=(1, 1))\n    ax = fig.add_subplot(projection='3d')\n    ax.w_xaxis.set_pane_color((1.0, 0.0, 0.0))\n    ax.w_yaxis.set_pane_color((0.0, 1.0, 0.0))\n    ax.w_zaxis.set_pane_color((0.0, 0.0, 1.0))\n", "def test_set_box_aspect_with_data_range():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.scatter([0, 1], [0, 1], [0, 1])\n    ax.set_box_aspect((1, 1, 2))\n    assert ax.get_box_aspect() == (1, 1, 2)\n\n    # Check that setting box aspect with data range does not change limits\n    x_lim = ax.get_xlim3d()\n    y_lim = ax.get_ylim3d()\n    z_lim = ax.get_zlim3d()\n    ax.set_box_aspect((1, 1, 1), zoom=1.25)\n    assert ax.get_xlim3d() == x_lim\n    assert ax.get_ylim3d() == y_lim\n    assert ax.get_zlim3d() == z_lim\n"], "sample_1078": ["def test_Indexed_derivative():\n    A = IndexedBase(\"A\")\n    i, j, k = symbols(\"i,j,k\")\n    x, y, z = symbols(\"x,y,z\")\n    f = Function(\"f\")\n\n    assert Derivative(A[i], A[j]).doit() == KroneckerDelta(i, j)\n    assert Derivative(A[i], A[i]).doit() == 1\n    assert Derivative(A[i], x).doit() == 0\n    assert Derivative(x, A[i]).doit() == KroneckerDelta(x, A[i])\n    assert Derivative(f(A[i]), A[j]).doit() == Derivative(f(A[i]), A[j])\n    assert Derivative(A[i]*x, A[j]).doit() == x*KroneckerDelta(i, j)\n    assert Derivative(A[i]*x, x).doit() == A[i]\n    assert Derivative(A[i]*x, A[i]).doit() == x + A[i]*KroneckerDelta(x, A[i])\n", "def test_Indexed_broadcasting():\n    A = IndexedBase('A', shape=(3, 4))\n    B = IndexedBase('B', shape=(1, 4))\n    C = IndexedBase('C', shape=(3, 1))\n    D = IndexedBase('D', shape=(2, 2))\n    i, j = symbols('i j', integer=True)\n\n    assert A[i, j].shape == B[i, j].shape == C[i, j].shape\n    assert A[i, j].shape != D[i, j].shape\n\n    raises(IndexException, lambda: A[i])\n    raises(IndexException, lambda: B[i, j, j])\n    raises(IndexException, lambda: C[i, j, j])\n    raises(IndexException, lambda: D[i])\n    raises(IndexException, lambda: D[i, j, j])\n", "def test_Indexed_broadcasting():\n    A = IndexedBase('A', shape=(2, 3))\n    B = IndexedBase('B', shape=(3,))\n    C = IndexedBase('C')\n\n    # Test broadcasting with compatible shapes\n    expr1 = A[0, :] + B[:]\n    assert expr1.shape == (2, 3)\n\n    # Test broadcasting with incompatible shapes\n    raises(IndexException, lambda: A[0, :] + B[0])\n\n    # Test broadcasting with undefined shape\n    expr2 = A[0, :] + C[:]\n    assert expr2.shape == (2, 3)\n\n    # Test broadcasting with undefined shape and incompatible rank\n    raises(IndexException, lambda: A[0] + C[:])\n", "def test_indexed_array_like_behavior():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A', shape=(3, 3))\n    assert A[0, 0] == A[0][0]\n    assert A[0, 1:] == A[0][1:]\n    assert A[:, 0] == A[:, 0]\n    assert A[0:2, 0:2] == A[0:2][:, 0:2]\n    assert A[i, j].subs(A, Array([[1, 2], [3, 4]])) == Array([[1, 2], [3, 4]])[i, j]\n", "def test_Indexed_derivative_simplification():\n    i, j, k = symbols('i j k', cls=Idx)\n    A = IndexedBase('A')\n    expr = A[i] * A[j]\n    assert expr.diff(A[k]) == A[j] * KroneckerDelta(i, k) + A[i] * KroneckerDelta(j, k)\n"], "sample_1131": ["def test_log1p():\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x + 1)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log(x + 1)'\n", "def test_cosm1():\n    from sympy import cosm1\n\n    expr = cosm1(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.cosm1(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.cos(x) - 1'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.cos(x) - 1'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.cosm1(x)'\n", "def test_log1p():\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log(x+1)'\n", "def test_log1p():\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x + 1)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log1p(x)'\n", "def test_log1p():\n    from sympy import log1p\n\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(x + 1)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x + 1)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log(x + 1)'\n"], "sample_787": ["def test_balanced_accuracy_score_multiclass():\n    # Check balanced_accuracy_score function for multiclass case\n    y_true = np.array([0, 1, 2, 2, 0, 1])\n    y_pred = np.array([0, 1, 1, 2, 0, 0])\n    true_score = (0.5 + 0.5 + 0.5) / 3\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), true_score)\n    assert_raises(ValueError, balanced_accuracy_score, y_true, y_pred[1:])\n", "def test_balanced_accuracy_score_binary():\n    # Test balanced accuracy score for binary classification task\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    score = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(score, 0.71875, decimal=4)\n", "def test_balanced_accuracy_score_multiclass():\n    # Test balanced_accuracy_score for multiclass classification task\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute score with default labels introspection\n    score = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(score, 0.483, 2)\n\n    # compute score with explicit label ordering\n    score = balanced_accuracy_score(y_true, y_pred, labels=[0, 2, 1])\n    assert_almost_equal(score, 0.483, 2)\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = np.array([0, 1, 2, 2, 1, 0])\n    y_pred = np.array([0, 1, 2, 0, 1, 2])\n    score = balanced_accuracy_score(y_true, y_pred)\n    # Replace the following assert with the correct expected score\n    assert_almost_equal(score, 0.5)\n", "def test_brier_score_loss_multiclass():\n    # Check brier_score_loss function for multiclass\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.2, 0.3, 0.5], [0.7, 0.1, 0.2], [0.3, 0.5, 0.2],\n                       [0.6, 0.1, 0.3], [0.4, 0.5, 0.1], [0.3, 0.4, 0.3]])\n    true_score = np.mean(np.sum((y_pred - (y_true[:, None] == np.arange(3)) ** 2), axis=1))\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred[1:])\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred + 1.)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred - 1.)\n    assert_raises(ValueError, brier_score_loss, y_true, np.array([[0.2, 0.3], [0.7, 0.1], [0.3, 0.5]]))\n"], "sample_1108": ["def test_is_palindromic():\n    assert is_palindromic('')\n    assert is_palindromic('x')\n    assert is_palindromic('xx')\n    assert is_palindromic('xyx')\n    assert not is_palindromic('xy')\n    assert not is_palindromic('xyzx')\n    assert is_palindromic('xxyzzyx', 1)\n    assert not is_palindromic('xxyzzyx', 2)\n    assert is_palindromic('xxyzzyx', 2, -1)\n    assert is_palindromic('xxyzzyx', 2, 6)\n    assert is_palindromic('xxyzyx', 1)\n    assert not is_palindromic('xxyzyx', 2)\n    assert is_palindromic('xxyzyx', 2, 2 + 3)\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3), (1, -2, 3), (1, 2, -3), (1, -2, -3), (-1, 2, 3), (-1, -2, 3), (-1, 2, -3), (-1, -2, -3)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n    assert list(permute_signs((0,))) == [(0,)]\n    assert list(permute_signs(())) == [()]\n", "def test_is_palindromic_with_given_range():\n    assert is_palindromic('xxyzzyx', 2, 6) is True\n    assert is_palindromic('xxyzzyx', 2, 5) is False\n", "def test_is_palindromic_with_start_end():\n    assert is_palindromic('xyx', start=1, end=3)\n    assert not is_palindromic('xyx', start=0, end=2)\n    assert is_palindromic('xyx', start=0, end=3)\n    assert not is_palindromic('xyx', start=1, end=2)\n    assert is_palindromic('xyxyx', start=1, end=4)\n    assert not is_palindromic('xyxyx', start=0, end=4)\n    assert not is_palindromic('xyxyx', start=1, end=3)\n", "def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n"], "sample_820": ["def test_transform_hard():\n    \"\"\"Check transform method of VotingClassifier with hard voting on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    assert_array_equal(eclf.transform(X).shape, (4, 3))\n    assert_array_equal(eclf.transform(X), np.array([[1, 1, 1], [1, 1, 1], [2, 1, 1], [2, 2, 2]]))\n", "def test_sample_weight_regressor():\n    \"\"\"Tests sample_weight parameter of VotingRegressor\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    reg3 = DummyRegressor(strategy='quantile', quantile=.2)\n    ereg1 = VotingRegressor([('lr', reg1), ('rf', reg2), ('dr', reg3)], weights=[1, 2, 1])\n    ereg1.fit(X_r, y_r, sample_weight=np.ones((len(y_r),)))\n    ereg2 = VotingRegressor([('lr', reg1), ('rf', reg2), ('dr', reg3)], weights=[1, 2, 1])\n    ereg2.fit(X_r, y_r)\n    assert_array_equal(ereg1.predict(X_r), ereg2.predict(X_r))\n\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y_r),))\n    ereg3 = VotingRegressor([('lr', reg1)], weights=[1])\n    ereg3.fit(X_r, y_r, sample_weight)\n    reg1.fit(X_r, y_r, sample_weight)\n    assert_array_equal(ereg3.predict(X_r), reg1.predict(X_r))\n\n    # check that an error is raised and indicative if sample_weight is not\n    # supported.\n    reg4 = KNeighborsRegressor()\n    ereg3 = VotingRegressor([('lr', reg1), ('rf', reg2), ('knn', reg4)], weights=[1, 2, 1])\n", "def test_sample_weight_regressor():\n    \"\"\"Tests sample_weight parameter of VotingRegressor\"\"\"\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = RandomForestRegressor(random_state=123)\n    ereg1 = VotingRegressor([('dr', reg1), ('rf', reg2)], weights=[1, 2])\n    ereg2 = VotingRegressor([('dr', reg1), ('rf', reg2)], weights=[1, 2])\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y_r),))\n    ereg1.fit(X_r, y_r, sample_weight)\n    ereg2.fit(X_r, y_r)\n    assert_array_almost_equal(ereg1.predict(X_r), ereg2.predict(X_r), decimal=2)\n\n    # check that an error is raised and indicative if sample_weight is not\n    # supported.\n    reg3 = LinearRegression()\n    ereg3 = VotingRegressor([('dr', reg1), ('rf', reg2), ('lr', reg3)], weights=[1, 2, 1])\n    msg = ('Underlying estimator LinearRegression does not support '\n           'sample weights.')\n    with pytest.raises(ValueError, match=msg):\n        ereg3.fit(X_r, y_r, sample_weight)\n\n    # check that _parallel_fit_estimator will raise the right error\n    # it should raise the original error if this is not linked to sample_weight\n    class RegressorErrorFit(BaseEstimator, RegressorMixin):\n            raise TypeError('Error unrelated to sample_weight.')\n    reg = RegressorErrorFit()\n    with pytest.raises(TypeError, match='Error unrelated to sample_weight'):\n        reg.fit(X_r, y_r, sample_weight=sample_weight)\n", "def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=50, random_state=1)\n    X_r = np.array([[1.1], [2.2], [3.3], [4.4]])\n    y_r = np.array([2, 3, 4, 5])\n\n    ereg1 = VotingRegressor([('lr', reg1), ('rf', reg2)]).fit(X_r, y_r)\n    ereg2 = VotingRegressor([('lr', reg1), ('rf', reg2)]).fit(X_r, y_r)\n\n    assert_array_equal(ereg1.transform(X_r).shape, (2, 4))\n    assert_array_equal(ereg1.transform(X_r), ereg2.transform(X_r))\n", "def test_sample_weight_regressor():\n    \"\"\"Tests sample_weight parameter of VotingRegressor\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    ereg1 = VotingRegressor([('lr', reg1), ('rf', reg2)], weights=[1, 2]).fit(X_r, y_r, sample_weight=np.ones((len(y_r),)))\n    ereg2 = VotingRegressor([('lr', reg1), ('rf', reg2)], weights=[1, 2]).fit(X_r, y_r)\n    assert_array_equal(ereg1.predict(X_r), ereg2.predict(X_r))\n\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y_r),))\n    ereg3 = VotingRegressor([('lr', reg1)], weights=[1])\n    ereg3.fit(X_r, y_r, sample_weight)\n    reg1.fit(X_r, y_r, sample_weight)\n    assert_array_equal(ereg3.predict(X_r), reg1.predict(X_r))\n"], "sample_221": ["def test_annotation_with_function(self):\n    # Test annotation with a custom function\n    from django.db.models.functions import Upper\n    qs = Happening.objects.annotate(upper_name=Upper('name'))\n    self.assert_pickles(qs)\n", "def test_annotation_values_with_filter(self):\n    qs = Happening.objects.values('name').annotate(latest_time=models.Max('when')).filter(id=self.happening.id)\n    reloaded = Happening.objects.all()\n    reloaded.query = pickle.loads(pickle.dumps(qs.query))\n    self.assertEqual(\n        reloaded.get(),\n        {'name': 'test', 'latest_time': self.happening.when},\n    )\n", "def test_select_related_with_queryset_annotation(self):\n    group = Group.objects.create(name='test')\n    event = Event.objects.create(title='test event', group=group)\n    event.edition_set.create()\n    events = Event.objects.annotate(edition_count=models.Count('edition')).select_related('group')\n    self.assert_pickles(events)\n", "def test_filter_with_datetime_field(self):\n    dt = datetime.datetime(2022, 1, 1)\n    happening = Happening.objects.create(name='Test Event', when=dt)\n    filtered = Happening.objects.filter(when=dt)\n    self.assertSequenceEqual(filtered, [happening])\n    self.assert_pickles(filtered)\n", "def test_filtered_relation_with_values(self):\n    e = Event.objects.create(title='Event 1', group=Group.objects.create(name='Group 1'))\n    qs = Event.objects.filter(group__name__in=['Group 1']).values('title', 'group__name')\n    self.assert_pickles(qs)\n"], "sample_999": ["def test_Quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    result = q1 * q2\n    assert latex(result) == \"-60 + 12 i + 24 j + 44 k\"\n", "def test_Quaternion_commutation():\n    i = Quaternion(0, 1, 0, 0)\n    j = Quaternion(0, 0, 1, 0)\n    k = Quaternion(0, 0, 0, 1)\n    assert i*j == k\n    assert j*k == i\n    assert k*i == j\n", "def test_issue_14862():\n    q = Quaternion(0, 0, 0, 0)\n    assert latex(q) == \"0\"\n", "def test_issue_14931():\n    from sympy import symbols, Derivative, latex\n    x, y = symbols('x y', real=True)\n    f = Function('f')\n    g = Function('g')\n    expr = Derivative(f(x)*g(y), x)\n    assert latex(expr) == r\"\\frac{d}{d x} \\left(f{\\left (x \\right )} g{\\left (y \\right )}\\right)\"\n", "def test_issue_14117():\n    # Test case for issue #14117\n    from sympy.printing.latex import latex\n    from sympy.physics.quantum.dagger import Dagger\n    from sympy import Symbol\n    x = Symbol('x')\n    assert latex(Dagger(x)) == r'x^{\\dagger}'\n    assert latex(Dagger(x**2)) == r'\\left(x^{2}\\right)^{\\dagger}'\n"], "sample_821": ["def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_sparse = af.fit_predict(X_sparse)\n    labels_dense = af.fit_predict(X)\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    X_csr = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_csr = af.fit_predict(X_csr)\n    labels = af.fit_predict(X)\n    assert_array_equal(labels, labels_csr)\n", "def test_affinity_propagation_preference_float():\n    # Test with preference as float\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference)\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    labels_precomputed = af.fit(S).labels_\n\n    assert_array_equal(labels, labels_precomputed)\n", "def test_affinity_propagation_with_large_max_iter():\n    # Test AffinityPropagation with max_iter larger than the number of iterations needed for convergence\n    af = AffinityPropagation(preference=np.median(S) * 10, max_iter=300, verbose=True)\n    labels = af.fit(X).labels_\n    assert_equal(af.n_iter_, 200)  # The algorithm should still converge in 200 iterations\n", "def test_affinity_propagation_convergence_iter():\n    # Test that convergence_iter parameter works as expected\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    S = -euclidean_distances(X, squared=True)\n\n    # With default convergence_iter = 15\n    _, _, n_iter = affinity_propagation(S, preference=-10, return_n_iter=True)\n    assert n_iter > 15\n\n    # With convergence_iter = 5\n    _, _, n_iter = affinity_propagation(S, preference=-10, convergence_iter=5, return_n_iter=True)\n    assert n_iter <= 5\n"], "sample_284": ["def test_post_processing_failure_with_missing_file(self):\n    \"\"\"\n    post_processing indicates the origin of the error when it fails due to a missing file.\n    \"\"\"\n    finders.get_finder.cache_clear()\n    err = StringIO()\n    with self.assertRaises(Exception):\n        call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n    self.assertIn(\"The file 'missing.css' could not be found with\", err.getvalue())\n    self.assertPostCondition()\n", "def test_post_processing_order(self):\n    \"\"\"\n    The correct hashed name is used for referenced files, regardless of\n    the order in which the files are post-processed.\n    \"\"\"\n    # Create two files that will be post-processed, but reference each other.\n    file1_path = self._get_filename_path('file1.css')\n    file2_path = self._get_filename_path('file2.css')\n    with open(file1_path, 'w') as f:\n        f.write('body { background: url(\"file2.png\"); }')\n    with open(file2_path, 'w') as f:\n        f.write('body { color: url(\"file1.png\"); }')\n\n    # Post-process the files in reverse order.\n    collectstatic_args = {\n        'interactive': False,\n        'verbosity': 0,\n        'link': False,\n        'clear': False,\n        'dry_run': False,\n        'post_process': True,\n        'use_default_ignore_patterns': True,\n        'ignore_patterns': ['*.ignoreme'],\n        'paths': [file2_path, file1_path],\n    }\n\n    collectstatic_cmd = CollectstaticCommand()\n    collectstatic_cmd.set_options(**collectstatic_args)\n    stats = collectstatic_cmd.collect()\n\n    # Check that the correct hashed name is used for the referenced file.\n    relpath = self.hashed_file_path(\"file1.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"file2.png\", content)\n        self.assertIn(b\"file2.hashed_name.png\", content)\n\n    relpath = self.hashed_file_path(\"file2.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"file1.png\", content)\n        self.assertIn(", "def test_hashed_name_with_existing_file(self):\n    filename = 'cached/styles.css'\n    hashed_filename = storage.staticfiles_storage.hashed_name(filename)\n    self.assertEqual(hashed_filename, 'cached/styles.5e0040571e1a.css')\n    with storage.staticfiles_storage.open(hashed_filename) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"cached/other.css\", content)\n        self.assertIn(b\"other.d41d8cd98f00.css\", content)\n\n    # Now, change the content of the original file and check if the hashed name changes\n    with open(self._get_filename_path(filename), 'w') as f:\n        f.write('/* Modified content */')\n    new_hashed_filename = storage.staticfiles_storage.hashed_name(filename)\n    self.assertNotEqual(new_hashed_filename, hashed_filename)\n    with storage.staticfiles_storage.open(new_hashed_filename) as relfile:\n        content = relfile.read()\n        self.assertIn(b'/* Modified content */', content)\n", "def test_template_tag_with_unicode(self):\n    relpath = self.hashed_file_path(\"unicode/snowman.css\")\n    self.assertEqual(relpath, \"unicode/snowman.c645f201970f.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertIn(b'url(\"snowman.png?\\xe2\\x98\\x83\")', content)\n    self.assertPostCondition()\n", "def test_post_processing_order(self):\n    \"\"\"\n    Files referenced from CSS use the correct final hashed name regardless of\n    the order in which the files are post-processed.\n    \"\"\"\n    # Create temporary files\n    original_css_filename = self._get_filename_path('original.css')\n    with open(original_css_filename, 'w') as f:\n        f.write('url(\"reference.css\")')\n\n    referenced_css_filename = self._get_filename_path('reference.css')\n    with open(referenced_css_filename, 'w') as f:\n        f.write('/* This is the referenced file. */')\n\n    # Collect static files with shuffled order\n    original_collectstatic_args = storage.staticfiles_storage.post_process.func_closure[0].cell_contents['collectstatic_args']\n    collectstatic_args = original_collectstatic_args.copy()\n    collectstatic_args['dirs'] = [os.path.join(self._temp_dir, 'test')]\n    collectstatic_args['paths'] = shuffle(['original.css', 'reference.css'])\n    collectstatic_cmd = CollectstaticCommand()\n    collectstatic_cmd.set_options(**collectstatic_args)\n    collectstatic_cmd.collect()\n\n    # Check that the referenced file has been correctly hashed in the original file\n    with storage.staticfiles_storage.open('original.css') as f:\n        self.assertIn(b'url(\"reference.{}.css\")'.format(storage.staticfiles_storage.hashed_files['reference.css'].encode()), f.read())\n"], "sample_349": ["def test_render_options_with_custom_to_field(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    album = Album.objects.create(name='Rubber Soul', band=beatles, release_date='2022-01-01')\n    release_event = ReleaseEvent.objects.create(name='Test Target', album=album)\n    # Test with a custom 'to_field'\n    form = VideoStreamForm(initial={'release_event': album.release_date})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>2022-01-01</option>' % album.release_date\n    self.assertIn(selected_option, output)\n", "def test_render_options_related_field_with_to_field(self):\n    \"\"\"Related field with a to_field different from the primary key.\"\"\"\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    rubber_soul = Album.objects.create(name='Rubber Soul', band=beatles, release_date='1965-08-06')\n    release_event = ReleaseEvent.objects.create(name='Test Target', album=rubber_soul)\n    form = VideoStreamForm(initial={'release_event': rubber_soul.release_date})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>1965-08-06</option>' % rubber_soul.release_date\n    self.assertIn(selected_option, output)\n", "def test_render_options_fk_as_non_pk(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    rubber_soul = Album.objects.create(name='Rubber Soul', band=beatles, year=1965)\n    release_event = ReleaseEvent.objects.create(name='Test Target', album=rubber_soul)\n    form = VideoStreamForm(initial={'release_event': release_event.year})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>Test Target</option>' % release_event.year\n    self.assertIn(selected_option, output)\n", "def test_render_options_fk_with_to_field(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    rubber_soul = Album.objects.create(name='Rubber Soul', band=beatles)\n    release_event = ReleaseEvent.objects.create(name='Test Target', album=rubber_soul, location='London')\n    form = VideoStreamForm(initial={'release_event': release_event.location})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>London</option>' % release_event.location\n    self.assertIn(selected_option, output)\n", "def test_render_options_fk_as_non_pk(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    rubber_soul = Album.objects.create(name='Rubber Soul', band=beatles, year=1965)\n    release_event = ReleaseEvent.objects.create(name='Test Target', album=rubber_soul)\n    form = VideoStreamForm(initial={'release_event': rubber_soul.year})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>1965</option>' % rubber_soul.year\n    self.assertIn(selected_option, output)\n"], "sample_960": ["def test_pyclassmethod_with_annotation(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:classmethod:: meth\\n\"\n            \"      :type: (int, str) -> bool\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class class method)', 'Class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"classmethod \"],\n                                                     [desc_name, \"meth\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [desc_sig_punctuation, \"(\"],\n                                                                        [pending_xref, \"int\"],\n                                                                        [desc_sig_punctuation, \", \"],\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \")\"],\n                                                                        \" \",\n                                                                        [desc_sig_operator, \"->\"],\n                                                                        \" \",\n                                                                        [pending_xref, \"bool\"])])],\n                                   [desc_content, ()]))\n    assert 'Class.meth' in domain.objects\n    assert domain.objects['Class.meth'] == ('index', 'Class.meth', 'method', False)\n", "def test_python_use_unqualified_type_names_in_class_signature(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Bar.Name\" title=\"foo.Bar.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Bar.Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Bar.Name\" '\n            'title=\"foo.Bar.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.Bar.Age</em>) \u2013 blah blah</p>' in content\n", "def test_pyfunction_with_underscore_prefix(app):\n    text = \".. py:function:: _internal_func(arg: int) -> int\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"_internal_func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"int\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"arg\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"int\"])])\n", "def test_python_python_use_unqualified_type_names_priority(app, status, warning):\n    app.build()\n    content = (app.outdir / 'priority.html').read_text()\n    assert '<span class=\"n\"><span class=\"pre\">Name</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert '<p><strong>name</strong> (<em>Name</em>) \u2013 blah blah</p>' in content\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n", "def test_pyexception_hierarchy(app):\n    text = (\".. py:exception:: builtins.IOError\\n\"\n            \"   :base-exception: Exception\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"builtins.\"],\n                                                    [desc_name, \"IOError\"])],\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Bases\"],\n                                      [nodes.field_body, nodes.paragraph,\n                                       [pending_xref, nodes.emphasis, \"Exception\"]]))\n    assert_node(doctree[1][1][0][0][1][0][0], pending_xref,\n                refdomain=\"py\", reftype=\"exc\", reftarget=\"Exception\")\n"], "sample_4": ["def test_readwrite_html_table_units(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n    \"\"\"Test cosmology -> ascii.html -> cosmology with units.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_table_units.html\"\n\n    # To Table with units\n    write(fp, format=\"ascii.html\")\n\n    # Read Table with units\n    got = read(fp, format=\"ascii.html\")\n    assert got == cosmo\n\n    # Check units\n    assert got.H0.unit == u.km / u.s / u.Mpc\n    assert got.Om0.unit == u.dimensionless_unscaled\n    assert got.Ode0.unit == u.dimensionless_unscaled\n    assert got.Tcmb0.unit == u.K\n    assert got.Neff.unit == u.dimensionless_unscaled\n    assert got.m_nu.unit == u.eV\n    assert got.Ob0.unit == u.dimensionless_unscaled\n    assert got.w0.unit == u.dimensionless_unscaled\n    assert got.wa.unit == u.dimensionless_unscaled\n    assert got.wz.unit == u.dimensionless_unscaled\n    assert got.wp.unit == u.dimensionless_unscaled\n    assert got.zp.unit == u.dimensionless_unscaled\n", "def test_readwrite_html_table_bad_format(self, write, tmp_path):\n    \"\"\"Test if argument ``format`` is incorrect\"\"\"\n    fp = tmp_path / \"test_readwrite_html_table_bad_format.html\"\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not csv\"):\n        write(fp, format=\"csv\")\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not ascii\"):\n        write(fp, format=\"ascii\")\n", "def test_readwrite_html_format(self, cosmo, read, write, tmp_path, add_cu):\n    \"\"\"Test if the format is not 'ascii.html'.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_format.html\"\n\n    # Test writing with an incorrect format\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not csv\"):\n        write(fp, format=\"csv\")\n\n    # Test reading with an incorrect format\n    write(fp, format=\"ascii.html\")\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not csv\"):\n        read(fp, format=\"csv\")\n", "def test_readwrite_html_table_unitless_parameter(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n    \"\"\"Test cosmology -> ascii.html -> cosmology with unitless parameter.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_table_unitless_parameter.html\"\n\n    # Add a unitless parameter to the cosmology\n    cosmo.meta[\"unitless_param\"] = 10\n\n    # ------------\n    # To Table\n\n    write(fp, format=\"ascii.html\")\n\n    # some checks on the saved file\n    tbl = QTable.read(fp)\n    assert tbl[\"unitless_param\"] == 10\n\n    # ------------\n    # From Table\n\n    got = read(fp, format=\"ascii.html\")\n    assert got.__class__ is cosmo_cls\n    assert got.name == cosmo.name\n    assert got.meta[\"unitless_param\"] == 10\n", "def test_readwrite_html_table_units(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n    \"\"\"Test that units are correctly saved and loaded.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_table_units.html\"\n\n    # Write cosmology to HTML\n    write(fp, format=\"ascii.html\")\n\n    # Read the cosmology back from the HTML\n    got = read(fp, format=\"ascii.html\")\n\n    # Check that the parameters with units are equal to the original cosmology\n    for param in cosmo_cls.__parameters__:\n        original_param = getattr(cosmo, param)\n        if hasattr(original_param, \"unit\") and original_param.unit is not None:\n            assert getattr(got, param) == original_param\n"], "sample_464": ["def test_content_length_filelike_object_with_getbuffer(self):\n    class TestFileLike:\n            self._file = open(path, *args, **kwargs)\n\n            return self._file.read(n_bytes)\n\n            return self._file.getbuffer()\n\n            return self._file.tell()\n\n            if self._file:\n                self._file.close()\n                self._file = None\n\n            return self\n\n            self.close()\n\n    file = TestFileLike(__file__, \"rb\")\n    response = FileResponse(file)\n    response.close()\n    self.assertEqual(response.headers[\"Content-Length\"], str(os.path.getsize(__file__)))\n", "def test_content_length_non_file_object(self):\n    class TestObj:\n            self.content = content\n            self.index = 0\n\n            if n_bytes == -1 or self.index + n_bytes > len(self.content):\n                result = self.content[self.index:]\n            else:\n                result = self.content[self.index:self.index + n_bytes]\n            self.index += len(result)\n            return result\n\n    test_obj = TestObj(b\"binary content\")\n    response = FileResponse(test_obj)\n    self.assertEqual(response.headers[\"Content-Length\"], \"14\")\n", "def test_file_response_close(self):\n    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n        tmp.write(b\"binary content\")\n        tmp_name = tmp.name\n\n    response = FileResponse(open(tmp_name, \"rb\"))\n    self.assertFalse(response.closed)\n    response.close()\n    self.assertTrue(response.closed)\n\n    # Ensure file is closed after response is closed.\n    with self.assertRaises(ValueError):\n        open(tmp_name, \"rb\")\n", "def test_streaming_content_with_generator(self):\n        yield b\"Hello, \"\n        yield b\"World!\"\n\n    response = FileResponse(data_generator())\n    self.assertEqual(list(response), [b\"Hello, \", b\"World!\"])\n    self.assertFalse(response.has_header(\"Content-Length\"))\n", "def test_content_type_file_with_explicit_content_type(self):\n    explicit_content_type = \"text/plain\"\n    response = FileResponse(open(__file__, \"rb\"), content_type=explicit_content_type)\n    response.close()\n    self.assertEqual(response.headers[\"Content-Type\"], explicit_content_type)\n"], "sample_1055": ["def test_encipher_bifid():\n    raises(ValueError, lambda: encipher_bifid('abc', 'b', 'abcde'))\n    assert encipher_bifid('abc', 'b', 'abcd') == 'bdb'\n    raises(ValueError, lambda: decipher_bifid('bdb', 'b', 'abcde'))\n    assert decipher_bifid('bdb', 'b', 'abcd') == 'abc'\n    raises(ValueError, lambda: bifid_square('abcde'))\n", "def test_bg_public_key():\n    assert 161 == bg_public_key(13, 11)[0]\n    assert 461 == bg_public_key(23, 21)[0]\n    raises(ValueError, lambda: bg_public_key(13, 15))\n", "def test_bg_public_key():\n    assert 1611 == bg_public_key(23, 31)\n    assert 961  == bg_public_key(31, 31)\n    raises(ValueError, lambda: bg_public_key(16, 31))\n", "def test_bg_public_key():\n    assert 161 == bg_public_key(13, 11)[0]\n    assert 799 == bg_public_key(19, 41)[0]\n    raises(ValueError, lambda: bg_public_key(8, 16))\n    raises(ValueError, lambda: bg_public_key(8, 8))\n", "def test_encipher_decipher_elgamal():\n    p, g, a = 1031, 14, 347\n    b = 723\n    prk = (p, g, a)\n    ek = elgamal_public_key(prk)\n    msg = 1234\n    enc = encipher_elgamal(msg, ek)\n    dec = decipher_elgamal(enc, prk)\n    assert dec == msg\n"], "sample_1070": ["def test_log_as_leading_term():\n    x = Symbol('x')\n    assert log(x).as_leading_term(x) == log(x)\n    assert log(x**2).as_leading_term(x) == 2*log(x)\n    assert log(1/x).as_leading_term(x) == -log(x)\n    assert log(2 + x).as_leading_term(x) == log(2)\n", "def test_exp_pow():\n    x = symbols('x')\n    assert exp(x)**2 == exp(2*x)\n    assert exp(x)**(S(1)/2) == exp(x/2)\n    assert exp(x)**(-1) == exp(-x)\n", "def test_exp_power():\n    x, y = symbols('x y')\n    assert exp(x)**y == exp(x*y)\n    assert exp(x)**(-y) == exp(-x*y)\n    assert exp(x)**(y/z) == exp(x*y/z)\n    assert exp(x)**(y*z) == exp(x*y*z)\n    assert exp(x)**0 == 1\n    assert exp(x)**1 == exp(x)\n    assert exp(x)**(-1) == exp(-x)\n", "def test_exp_lambertw():\n    x = Symbol(\"x\", positive=True)\n    assert exp(LambertW(x)) == x * LambertW(x) * exp(LambertW(x))\n    assert exp(LambertW(x, k=0)) == x * LambertW(x, k=0) * exp(LambertW(x, k=0))\n    assert exp(LambertW(x, k=1)) == x * LambertW(x, k=1) * exp(LambertW(x, k=1))\n", "def test_exp_power():\n    x, y = symbols('x y')\n    assert exp(x)**y == exp(x*y)\n    assert exp(x)**2 == exp(2*x)\n    assert exp(x)**(-1) == exp(-x)\n    assert exp(x)**0 == 1\n    assert exp(x)**1 == exp(x)\n"], "sample_1127": ["def test_coset_table_symmetric():\n    G = SymmetricPermutationGroup(5)\n    H = PermutationGroup(Permutation(0,1,2), Permutation(0,2,3))\n    assert G.coset_table(H) == [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 0, 1, 2], [15, 16, 17, 18, 19], [20, 21, 22, 23, 3]]\n", "def test_orbit_stabilizer_theorem():\n    S = SymmetricGroup(5)\n    for i in range(5):\n        orbit = S.orbit(i)\n        stabilizer = S.stabilizer(i)\n        assert len(orbit) == S.order() // stabilizer.order()\n", "def test_coset_table_symmetric():\n    G = SymmetricPermutationGroup(5)\n    H = PermutationGroup(Permutation(0,1,2), Permutation(1,2))\n    assert G.coset_table(H) == [\n        [0, 0, 0, 1, 2, 3],\n        [4, 5, 4, 0, 6, 6],\n        [5, 4, 5, 2, 0, 7],\n        [3, 6, 7, 3, 7, 0],\n        [2, 1, 6, 7, 3, 4],\n        [1, 2, 7, 6, 4, 5]\n    ]\n", "def test_subgroups():\n    S = SymmetricGroup(3)\n    subgroups = S.subgroups()\n    expected = [PermutationGroup(Permutation(size=3)),\n                PermutationGroup(Permutation(0, 1), Permutation(0, 2), Permutation(1, 2)),\n                PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2, 1)),\n                PermutationGroup(Permutation(0, 1, 2))]\n    assert len(subgroups) == len(expected)\n    for sg in subgroups:\n        assert sg in expected\n\n    G = PermutationGroup(Permutation(1, 2, 3), Permutation(0, 1))\n    subgroups = G.subgroups()\n    expected = [PermutationGroup(Permutation(size=4)),\n                PermutationGroup(Permutation(0, 1), Permutation(2, 3)),\n                PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2, 1)),\n                PermutationGroup(Permutation(0, 1, 2, 3), Permutation(0, 2, 3, 1), Permutation(0, 3, 1, 2)),\n                PermutationGroup(Permutation(0, 1, 2, 3))]\n    assert len(subgroups) == len(expected)\n    for sg in subgroups:\n        assert sg in expected\n", "def test_random_stab_with_pairs():\n    S = SymmetricGroup(5)\n    _random_el = Permutation([1, 3, 2, 0, 4])\n    _random_prec = {'rand': _random_el}\n    g = S.random_stab(2, _random_prec=_random_prec, pairs=True)\n    assert g == ([1, 3, 2, 0, 4], Permutation([1, 3, 2, 0, 4]))\n    h = S.random_stab(1, pairs=True)\n    assert h[0](1) == 1\n"], "sample_518": ["def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n", "def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n", "def test_fancyarrowpatch_path():\n    path = mpath.Path([(0, 0), (1, 1), (2, 0)])\n    arrow = FancyArrowPatch(path=path)\n    assert arrow._posA_posB is None\n    assert arrow._path_original == path\n", "def test_patch_default_linewidth():\n    patch = Patch()\n    assert patch.get_linewidth() == rcParams['lines.linewidth']\n", "def test_polygon_setters():\n\n    fig, ax = plt.subplots()\n    poly = Polygon([[0.5, 0.5], [0.7, 0.5], [0.6, 0.7]], closed=True, fc='g')\n    ax.add_patch(poly)\n    ax.set_aspect('equal')\n\n    poly.xy = [[0.2, 0.2], [0.4, 0.2], [0.3, 0.4]]\n    poly.closed = False\n    poly.fc = 'm'\n    poly.ec = 'b'\n    poly.alpha = 0.5\n    poly.hatch = 'xxx'\n"], "sample_654": ["def test_call_fixture_function_error(fix):\n    with pytest.raises(RuntimeError):\n        fix()\n", "def test_call_fixture_function_error(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 1\n\n            with pytest.raises(ValueError, match=\"A fixture function can only be used in a test function\"):\n                fix()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_call_fixture_function_directly(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 1\n\n            with pytest.raises(ValueError):\n                fix()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*fixture functions can't be called directly*\"])\n", "def test_call_fixture_function_error(request):\n    with pytest.raises(FixtureLookupError) as excinfo:\n        fix(request)\n    assert str(excinfo.value) == \"Fixture 'fix' not found.\"\n", "def test_fixture_func_direct_call(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 1\n\n            with pytest.raises(ValueError) as exc_info:\n                fix()\n            assert \"fixture 'fix' not found\" in str(exc_info.value)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_574": ["def test_label_concise(self, t):\n    s = Temporal().label(concise=True)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(10, 1000)\n    label, = a.major.formatter.format_ticks([100])\n    assert \"1970\" in label\n", "def test_label_concise(self, t):\n\n    s = Temporal().label(concise=True)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(10, 1000)\n    label, = a.major.formatter.format_ticks([100])\n    assert label != \"\"\n", "def test_tick_locator_input_check(self, t):\n\n    err = \"Tick locator must be an instance of .*?, not <class 'str'>.\"\n    with pytest.raises(TypeError, match=err):\n        Temporal().tick(\"invalid_locator\")\n", "def test_color_list_palette_numeric_data(self, y):\n\n    cs = color_palette(\"crest\", 3)\n    s = Temporal(cs)._setup(y, Color())\n    normed = (y - y.min()) / (y.max() - y.min())\n    assert_array_equal(s(y), cs[normed.map(lambda x: int(x * (len(cs) - 1)))])\n", "def test_label_concise(self, t):\n\n    s = Temporal().label(concise=True)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(t.min(), t.max())\n    label, = a.major.formatter.format_ticks([t.mean()])\n    assert label in [\"Sep 1974\", \"Jun 1975\", \"Dec 1978\"]  # The exact label may vary based on the environment's locale\n"], "sample_648": ["def test_mark_decorator_with_fixture(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pytest.mark.a\n\n        @pytest.mark.b\n            pass\n\n        @pytest.mark.c\n            pass\n        \"\"\"\n    )\n    items, rec = pytester.inline_genitems(p)\n    self.assert_markers(items, test_with_fixture=(\"b\",), test_without_fixture=(\"c\",))\n", "def test_mark_generator_id_access(pytester: Pytester) -> None:\n    \"\"\"Test accessing the id attribute on a MarkGenerator instance.\"\"\"\n    mark = MarkGenerator(_ispytest=True)\n    with pytest.raises(AttributeError) as excinfo:\n        mark.id  # type: ignore[attr-defined]\n    (msg,) = excinfo.value.args\n    assert msg == \"'MarkGenerator' object has no attribute 'id'\"\n", "def test_mark_decorator_subclass_propagates_to_base(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class Base(object): pass\n\n        @pytest.mark.a\n        class Test1(Base):\n\n        class Test2(Base):\n            @pytest.mark.b\n        \"\"\"\n    )\n    items, rec = pytester.inline_genitems(p)\n    assert_markers(items, test_foo=(\"a\",), test_bar=(\"b\",))\n", "def test_pytest_param_id_allows_string_or_float_or_int_or_bool() -> None:\n    assert pytest.param(id=\"hello\")\n    assert pytest.param(id=123)\n    assert pytest.param(id=123.456)\n    assert pytest.param(id=True)\n", "def test_mark_with_lambda_default_args(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [None], ids=lambda arg: str(arg) or \"None\")\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n    result.stdout.fnmatch_lines([\"test_func[None]\"])\n"], "sample_596": ["def test_concat_data_vars_list(self):\n    data = Dataset({\"foo\": (\"x\", np.random.randn(10)), \"bar\": (\"x\", np.random.randn(10))})\n    objs = [data.isel(x=slice(5)), data.isel(x=slice(5, None))]\n    actual = concat(objs, dim=\"x\", data_vars=[\"foo\"])\n    expected = data.drop_vars(\"bar\")\n    assert_identical(expected, actual)\n", "def test_concat_with_positions():\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3])}, {\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"a\": (\"x\", [4, 5, 6])}, {\"x\": [3, 4, 5]})\n    expected = Dataset({\"a\": (\"x\", [1, 2, 3, 4, 5, 6])}, {\"x\": [0, 1, 2, 3, 4, 5]})\n    actual = concat([ds1, ds2], dim=\"x\", positions=[[0, 1, 2], [3, 4, 5]])\n    assert_identical(actual, expected)\n", "def test_concat_coords_different_compat_override():\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [1, 2, 3], \"y\": 1})\n    ds2 = Dataset({\"a\": (\"x\", [4, 5, 6])}, coords={\"x\": [4, 5, 6], \"y\": 2})\n\n    expected = Dataset({\"a\": (\"x\", [1, 2, 3, 4, 5, 6])}, coords={\"x\": [1, 2, 3, 4, 5, 6], \"y\": 1})\n\n    actual = concat([ds1, ds2], \"x\", coords=\"different\", compat=\"override\")\n    assert_identical(actual, expected)\n", "def test_concat_existing_dim_different_coords():\n    da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1})\n    da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6], \"y\": 2})\n\n    expected = DataArray([1, 2, 3, 4, 5, 6], dims=\"x\", coords={\"x\": [1, 2, 3, 4, 5, 6], \"y\": [1, 1, 1, 2, 2, 2]})\n\n    actual = concat([da1, da2], \"x\", coords=\"different\")\n    assert_identical(actual, expected)\n", "def test_concat_merge_diff_non_dim_coords():\n    da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1})\n    da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6], \"y\": 2})\n\n    with raises_regex(ValueError, \"coordinates in some datasets but not others\"):\n        concat([da1, da2], dim=\"x\", coords=\"different\")\n\n    with raises_regex(ValueError, \"conflicting values\"):\n        concat([da1, da2], dim=\"x\", coords=\"minimal\")\n\n    da3 = DataArray([7, 8, 9], dims=\"x\", coords={\"x\": [7, 8, 9], \"y\": 1})\n    expected = DataArray(range(1, 10), dims=\"x\", coords={\"x\": range(1, 10)})\n\n    actual = concat([da1, da2, da3], dim=\"x\", coords=\"all\")\n    assert_identical(actual, expected)\n"], "sample_891": ["def test_label_ranking_avg_precision_score_should_allow_csc_matrix_for_y_true_input():\n    # Test that label_ranking_avg_precision_score accept sparse y_true in csc format.\n    y_true = csc_matrix([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(2 / 3)\n", "def test_ranking_metric_sample_weight_types(metric, sample_weight):\n    \"\"\"Check that the metric works with different types of `sample_weight`.\n\n    We can expect `sample_weight` to be None, an array of ones, an array of integers, and an array of floats.\n    No error should be raised for those types.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    n_samples = 10\n    y_true = rng.choice([0, 1], size=n_samples, replace=True)\n    y_proba = rng.rand(n_samples)\n    result = metric(y_true, y_proba, sample_weight=sample_weight)\n    if isinstance(result, float):\n        assert not np.isnan(result)\n    else:\n        metric_1, metric_2, thresholds = result\n        assert not np.isnan(metric_1).any()\n        assert not np.isnan(metric_2).any()\n        assert not np.isnan(thresholds).any()\n", "def test_label_ranking_average_precision_score_should_allow_different_shapes_for_y_true_and_y_score():\n    # Test that label_ranking_avg_precision_score accept different shapes for y_true and y_score.\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([0.5, 0.9, 0.6, 0, 0, 1])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(2 / 3)\n", "def test_top_k_accuracy_score_error_with_labels(y_true, y_score, k, labels, msg):\n    with pytest.raises(ValueError, match=msg):\n        top_k_accuracy_score(y_true, y_score, k=k, labels=labels)\n", "def test_ranking_metric_negative_label_types(metric, classes):\n    \"\"\"Check that the metric works with negative labels.\n\n    We can expect `pos_label` to be a bool, an integer, a float, a string.\n    No error should be raised for those types.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    n_samples, neg_label = 10, classes[0]\n    y_true = rng.choice(classes, size=n_samples, replace=True)\n    y_proba = rng.rand(n_samples)\n    result = metric(y_true, y_proba, pos_label=neg_label)\n    if isinstance(result, float):\n        assert not np.isnan(result)\n    else:\n        metric_1, metric_2, thresholds = result\n        assert not np.isnan(metric_1).any()\n        assert not np.isnan(metric_2).any()\n        assert not np.isnan(thresholds).any()\n"], "sample_229": ["def test_difference_with_subcompound_qs(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.union(Number.objects.filter(num__gt=5))\n    self.assertEqual(qs1.difference(qs2).count(), 5)\n", "def test_union_with_single_field_values(self):\n    qs1 = Number.objects.filter(num=1).values('num')\n    qs2 = Number.objects.filter(num=2).values('num')\n    self.assertCountEqual(qs1.union(qs2), [{'num': 1}, {'num': 2}])\n", "def test_union_with_values_list_and_multiple_fields(self):\n    qs1 = Number.objects.filter(num=1).values_list('num', 'other_num')\n    qs2 = Number.objects.filter(num=2).values_list('num', 'other_num')\n    self.assertCountEqual(qs1.union(qs2), [(1, 9), (2, 8)])\n", "def test_union_with_values_and_exclude(self):\n    ReservedName.objects.create(name='a', order=2)\n    ReservedName.objects.create(name='b', order=3)\n    qs1 = ReservedName.objects.all().values('name')\n    qs2 = ReservedName.objects.exclude(name='a').values('name')\n    self.assertQuerysetEqual(qs1.union(qs2), ['a', 'b'], operator.itemgetter('name'), ordered=False)\n", "def test_union_with_ordering_and_distinct(self):\n    qs1 = Number.objects.filter(num__lte=1).order_by('num')\n    qs2 = Number.objects.filter(num__gte=2, num__lte=3).order_by('-num')\n    self.assertNumbersEqual(qs1.union(qs2, all=True).distinct(), [3, 2, 1, 0], ordered=True)\n"], "sample_535": ["def test_table_contains():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    table.add_cell(0, 0, 1, 1)\n    table.add_cell(1, 1, 1, 1)\n\n    class MockMouseEvent:\n            self.x = x\n            self.y = y\n\n    # Test that the contains method returns True for a mouseevent within the table\n    mouseevent = MockMouseEvent(0.5, 0.5)\n    inside, info = table.contains(mouseevent)\n    assert inside is True\n\n    # Test that the contains method returns False for a mouseevent outside the table\n    mouseevent = MockMouseEvent(2, 2)\n    inside, info = table.contains(mouseevent)\n    assert inside is False\n", "def test_table_cell_properties():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    cell = table.add_cell(0, 0, text='Test', edgecolor='red', facecolor='blue')\n\n    assert cell.get_text() == 'Test'\n    assert cell.get_edgecolor() == 'red'\n    assert cell.get_facecolor() == 'blue'\n\n    cell.set_text_props(text='Changed', edgecolor='green', facecolor='yellow')\n\n    assert cell.get_text() == 'Changed'\n    assert cell.get_edgecolor() == 'green'\n    assert cell.get_facecolor() == 'yellow'\n", "def test_table_bbox():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    bbox = [0.25, 0.25, 0.5, 0.5]\n    t = ax.table(cellText=[['Data']], bbox=bbox)\n    assert np.allclose(t.get_position(), bbox)\n", "def test_custom_cell_text():\n    fig = plt.figure()\n\n    # Create a table with custom cell text properties\n    ax = fig.add_subplot(1, 1, 1)\n    ax.axis('off')\n    tb = ax.table(\n        cellText=[['Fit Text', 2],\n                  ['Custom Styles', 1]],\n        rowLabels=[\"A\", \"B\"],\n        colLabels=[\"Col1\", \"Col2\"],\n        loc=\"center\")\n    tb.auto_set_font_size(False)\n    tb.set_fontsize(12)\n\n    # Change text properties of the second cell\n    tb[(1, 0)].get_text().set_fontweight('bold')\n    tb[(1, 0)].get_text().set_color('red')\n", "def test_edgecolor():\n    fig = plt.figure()\n\n    ax = fig.add_subplot(1, 1, 1)\n    ax.axis('off')\n    tb = ax.table(\n        cellText=[['Fit Text', 2],\n                  ['Long text', 1]],\n        rowLabels=[\"A\", \"B\"],\n        colLabels=[\"Col1\", \"Col2\"],\n        loc=\"center\",\n        edges='horizontal')\n    tb.auto_set_font_size(False)\n    tb.set_fontsize(12)\n\n    for (i, j), cell in tb.get_celld().items():\n        if i == 0 or j == -1:\n            cell.set_edgecolor('blue')\n        else:\n            cell.set_edgecolor('red')\n"], "sample_286": ["def test_refresh_with_deferred_fields(self):\n    a = Article.objects.create(headline='Deferred article', pub_date=datetime.now())\n    deferred_a = Article.objects.only('headline').get(pk=a.pk)\n    a.headline = 'Updated headline'\n    a.save()\n    deferred_a.refresh_from_db()\n    self.assertEqual(deferred_a.headline, 'Updated headline')\n", "def test_refresh_from_db_with_deferred_fields(self):\n    a = Article.objects.create(pub_date=datetime.now())\n    deferred_article = Article.objects.defer('pub_date').get(pk=a.pk)\n    with self.assertNumQueries(1):\n        deferred_article.refresh_from_db(fields=['pub_date'])\n        self.assertIsNotNone(deferred_article.pub_date)\n", "def test_refresh_from_db_with_defer(self):\n    pub_date = datetime.now()\n    a = Article.objects.create(pub_date=pub_date)\n    a2 = Article.objects.defer('headline').get(pk=a.pk)\n    with self.assertNumQueries(0):\n        a2.refresh_from_db()\n    self.assertEqual(a2.pub_date, pub_date)\n    self.assertEqual(a2._state.db, \"default\")\n    with self.assertNumQueries(1):\n        _ = a2.headline\n", "def test_refresh_defer_fields(self):\n    a = Article.objects.create(pub_date=datetime.now(), headline='Test Headline')\n    deferred_a = Article.objects.defer('headline').get(pk=a.pk)\n    a.headline = 'New Test Headline'\n    a.save()\n    deferred_a.refresh_from_db()\n    with self.assertRaises(Article.headline.field.RelatedObjectDoesNotExist):\n        deferred_a.headline\n", "def test_refresh_m2m(self):\n    a1 = Article.objects.create(pub_date=datetime.now())\n    a2 = Article.objects.create(pub_date=datetime.now())\n    fa1 = FeaturedArticle.objects.create(article=a1)\n    fa1.related_articles.add(a2)\n    fa1_copy = FeaturedArticle.objects.get(pk=fa1.pk)\n    fa1_copy.related_articles.remove(a2)\n    with self.assertNumQueries(1):\n        fa1_copy.refresh_from_db()\n    self.assertEqual(list(fa1_copy.related_articles.all()), [a2])\n"], "sample_471": ["def test_integerfield_invalid_input(self):\n    f = IntegerField()\n    with self.assertRaises(ValidationError):\n        f.clean(\"invalid\")\n", "def test_integerfield_step_size_max_value(self):\n    f = IntegerField(step_size=3, max_value=10)\n    self.assertWidgetRendersTo(\n        f,\n        '<input name=\"f\" max=\"10\" step=\"3\" type=\"number\" id=\"id_f\" required>',\n    )\n    msg = (\n        \"Ensure this value is a multiple of step size 3, up to 10, e.g. 0, 3, 6, \"\n        \"and 9.\"\n    )\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"11\")\n    self.assertEqual(f.clean(\"9\"), 9)\n    self.assertEqual(f.clean(\"0\"), 0)\n    self.assertEqual(f.step_size, 3)\n", "def test_integerfield_negative_numbers(self):\n    f = IntegerField()\n    self.assertEqual(-1, f.clean(-1))\n    self.assertEqual(-1, f.clean(\"-1\"))\n    self.assertEqual(-1, f.clean(\" -1 \"))\n    self.assertEqual(-123, f.clean(-123))\n    self.assertEqual(-123, f.clean(\"-123\"))\n    self.assertEqual(-123, f.clean(\" -123 \"))\n", "def test_integerfield_negative_number(self):\n    f = IntegerField()\n    self.assertEqual(-5, f.clean(\"-5\"))\n    self.assertEqual(-5, f.clean(\"-5.0\"))\n    self.assertEqual(-5, f.clean(\"-5.000\"))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n        f.clean(\"-5.5\")\n    self.assertEqual(-5, f.clean(float(\"-5.0\")))\n", "def test_integerfield_initial_value(self):\n    f = IntegerField(initial=42)\n    self.assertWidgetRendersTo(\n        f, '<input type=\"number\" name=\"f\" value=\"42\" id=\"id_f\" required>'\n    )\n    self.assertEqual(42, f.clean(\"\"))\n"], "sample_426": ["def test_years_edge(self):\n    t = datetime.datetime(2020, 1, 1)\n    tests = [\n        (datetime.datetime(2020, 12, 31), \"11\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2021, 1, 1), \"1\\xa0year\"),\n        (datetime.datetime(2021, 12, 31), \"1\\xa0year, 11\\xa0months\"),\n        (datetime.datetime(2022, 1, 1), \"2\\xa0years\"),\n        (datetime.datetime(2022, 12, 31), \"2\\xa0years, 11\\xa0months\"),\n        (datetime.datetime(2023, 1, 1), \"3\\xa0years\"),\n        (datetime.datetime(2023, 12, 31), \"3\\xa0years, 11\\xa0months\"),\n        (datetime.datetime(2024, 1, 1), \"4\\xa0years\"),\n        (datetime.datetime(2024, 12, 31), \"4\\xa0years, 11\\xa0months\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value), expected)\n", "def test_zero_depth(self):\n    \"\"\"\n    If depth is set to 0, a ValueError should be raised.\n    \"\"\"\n    with self.assertRaises(ValueError):\n        timesince(self.t, self.t + self.oneweek, depth=0)\n", "def test_february_leap_year(self):\n    t = datetime.datetime(2020, 2, 29)\n    tests = [\n        (datetime.datetime(2020, 3, 1), \"1\\xa0month\"),\n        (datetime.datetime(2020, 3, 31), \"1\\xa0month, 3\\xa0weeks, 3\\xa0days\"),\n        (datetime.datetime(2020, 4, 1), \"2\\xa0months\"),\n        (datetime.datetime(2020, 4, 30), \"2\\xa0months, 4\\xa0weeks, 3\\xa0days\"),\n        (datetime.datetime(2021, 2, 28), \"1\\xa0year\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value), expected)\n", "    def test_time_strings(self):\n        \"\"\"Custom time strings should be used.\"\"\"\n        custom_time_strings = {\n            \"year\": \"{} year ago\",\n            \"month\": \"{} month ago\",\n            \"week\": \"{} week ago\",\n            \"day\": \"{} day ago\",\n            \"hour\": \"{} hour ago\",\n            \"minute\": \"{} minute ago\",\n        }\n        t = datetime.datetime(2022, 1, 1)\n        tests = [\n            (datetime.datetime(2021, 1, 1), \"1 year ago\"),\n            (datetime.datetime(2021, 12, 1), \"1 month ago\"),\n            (datetime.datetime(2021, 12, 25), \"1 week ago\"),\n            (datetime.datetime(2021, 12, 26), \"6 days ago\"),\n            (datetime.datetime(2021, 12, 27, 12, 0), \"1 day ago\"),\n            (datetime.datetime(2021, 12, 27, 13, 0), \"1 hour ago\"),\n            (datetime.datetime(2021, 12, 27, 13, 30), \"30 minutes ago\"),\n        ]\n        for value, expected in tests:\n            with self.subTest():\n                self.assertEqual(\n                    timesince(t, value, time_strings=custom_time_strings), expected\n                )\n", "    def test_leap_year_february(self):\n        start_date = datetime.date(2020, 2, 28)\n        self.assertEqual(timeuntil(start_date + self.oneday, start_date), \"1\\xa0day\")\n        self.assertEqual(timesince(start_date, start_date + self.oneday), \"1\\xa0day\")\n"], "sample_801": ["def test_basic_with_changed_params():\n    # Basic pprint test with changed params\n    lr = LogisticRegression(C=0.5, penalty='l1', solver='liblinear')\n    expected = \"\"\"", "def test_nested_pipeline():\n    # Render a pipeline with a nested pipeline\n    pipeline = make_pipeline(\n        make_pipeline(StandardScaler(), PCA()),\n        LogisticRegression(C=999)\n    )\n    expected = \"\"\"", "def test_numpy_array():\n    # Check that the representation of numpy arrays is correct\n\n    lr = LogisticRegression(Cs=np.array([0.1, 1]))\n    expected = \"\"\"", "def test_changed_only_with_nested_estimators():\n    # Test the changed_only param with nested estimators\n    set_config(print_changed_only=True)\n    pipeline = make_pipeline(StandardScaler(with_std=False), LogisticRegression(C=999, solver='liblinear'))\n    expected = \"\"\"", "def test_imputer():\n    # Render a SimpleImputer object\n    imputer = SimpleImputer(missing_values=np.nan, strategy='mean', fill_value=None, verbose=0, copy=True)\n    expected = \"\"\""], "sample_283": ["def test_no_dbname_or_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {},\n        )\n    )\n", "    def test_missing_dbname_with_service(self):\n        \"\"\"\n        If both NAME and service are missing, the default 'postgres' db is used.\n        \"\"\"\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\n                'USER': 'someuser',\n                'PASSWORD': 'somepassword',\n                'HOST': 'somehost',\n                'PORT': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'PASSWORD': 'somepassword',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_runshell_sigint_handler_restored(self, mock_super):\n    \"\"\"The original SIGINT handler is restored after runshell().\"\"\"\n    orig_handler = signal.getsignal(signal.SIGINT)\n    try:\n        DatabaseClient().runshell([])\n        self.assertEqual(signal.getsignal(signal.SIGINT), orig_handler)\n    finally:\n        signal.signal(signal.SIGINT, orig_handler)\n", "def test_no_dbname_and_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'postgres',\n            'HOST': 'localhost',\n            'PORT': '5432',\n        }), (\n            ['psql', '-U', 'postgres', '-h', 'localhost', '-p', '5432', 'postgres'],\n            {},\n        )\n    )\n"], "sample_733": ["def test_vectorizer_custom_preprocessor():\n        return s.replace(' ', '_')\n\n    vect = CountVectorizer(preprocessor=custom_preprocessor)\n    X = vect.fit_transform(['hello world', 'hello hello']).toarray()\n    assert_array_equal(X.ravel(), [1, 2, 1, 0])\n", "def test_countvectorizer_invalid_ngram_range():\n    invalid_ngram_range = (3, 2)\n    assert_raises(ValueError, CountVectorizer, ngram_range=invalid_ngram_range)\n", "def test_countvectorizer_transform_error():\n    # Test transform on unfitted vectorizer with empty vocabulary\n    v3 = CountVectorizer(vocabulary=None)\n    assert_raises(ValueError, v3.transform, ALL_FOOD_DOCS)\n", "def test_countvectorizer_max_features_none():\n    # Test that CountVectorizer with max_features=None doesn't limit the number of features\n    cv = CountVectorizer(max_features=None)\n    X = cv.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(len(cv.vocabulary_), len(set(word for doc in ALL_FOOD_DOCS for word in doc.split())))\n", "def test_strip_tags_function():\n    # Test the strip_tags function with HTML and XML tags\n    text_with_html_tags = \"<html><body><p>This is a <b>test</b> with HTML tags.</p></body></html>\"\n    expected_output = \"This is a test with HTML tags.\"\n    assert_equal(strip_tags(text_with_html_tags), expected_output)\n\n    text_with_xml_tags = \"<root><tag1>Value1</tag1><tag2>Value2</tag2></root>\"\n    expected_output = \"Value1Value2\"\n    assert_equal(strip_tags(text_with_xml_tags), expected_output)\n\n    # Test the strip_tags function with non-tag characters\n    text_without_tags = \"This is a test without any tags.\"\n    assert_equal(strip_tags(text_without_tags), text_without_tags)\n\n    # Test the strip_tags function with empty string\n    empty_string = \"\"\n    assert_equal(strip_tags(empty_string), empty_string)\n"], "sample_716": ["def test_ridge_classifier_cv_no_support_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n", "def test_ridge_classifier_sample_weights():\n    X, y = make_classification(n_samples=100, random_state=0)\n    sample_weight = np.random.rand(100)\n    ridge_classifier = RidgeClassifier()\n    ridge_classifier.fit(X, y, sample_weight=sample_weight)\n    assert_equal(len(ridge_classifier.coef_.shape), 2)\n    assert_equal(type(ridge_classifier.intercept_), np.ndarray)\n", "def test_ridge_regression_n_samples_equals_n_features():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 5, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n\n    for solver in (\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\"):\n        coefs = ridge_regression(X, y, alpha=1.0, solver=solver)\n        assert_equal(coefs.shape, (n_features,))\n", "def test_ridge_classifier_sample_weights():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0, n_labels=2)\n    sample_weight = np.random.rand(10)\n    ridge_classifier = RidgeClassifier(class_weight='balanced')\n    ridge_classifier.fit(X, y, sample_weight)\n    y_pred = ridge_classifier.predict(X)\n    assert_true(np.mean(y == y_pred) >= 0.8)\n", "def test_ridge_classifier_cv_no_support_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n"], "sample_833": ["def test_logistic_regression_path_coefs_ovr():\n    # Make sure that the returned coefs by logistic_regression_path when\n    # multi_class='ovr' don't override each other (used to be a bug).\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,\n                               n_redundant=0, n_clusters_per_class=1,\n                               random_state=0, n_features=2)\n    Cs = [.00001, 1, 10000]\n    coefs, _, _ = _logistic_regression_path(X, y, penalty='l1', Cs=Cs,\n                                            solver='saga', random_state=0,\n                                            multi_class='ovr')\n\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(coefs[0], coefs[1], decimal=1)\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(coefs[0], coefs[2], decimal=1)\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(coefs[1], coefs[2], decimal=1)\n", "def test_logistic_regression_path_sparse():\n    # Test logistic_regression_path with sparse input\n    X_sparse = sparse.csr_matrix(X)\n    coefs, _, _ = _logistic_regression_path(X_sparse, Y1, solver='lbfgs', multi_class='ovr')\n    assert coefs.shape[1] == X.shape[1]\n", "def test_logistic_regression_path_dual(solver):\n    # Make sure that dual=True raises a ValueError for solvers that don't support it\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n    Cs = [1.0]\n    msg = \"Solver %s supports only dual=False, got dual=True\" % solver\n    assert_raise_message(ValueError, msg, _logistic_regression_path, X, y, Cs=Cs, dual=True, solver=solver)\n", "def test_logistic_regression_path_dual():\n    # Test that an error is raised when dual=True is passed to logistic_regression_path\n    msg = \"dual=True is not supported for multinomial solvers.\"\n    assert_raise_message(ValueError, msg, _logistic_regression_path, X, Y2, dual=True, multi_class='multinomial')\n", "def test_LogisticRegression_C_penalty_multiclass(C, penalty, multi_class):\n    # Test LogisticRegression with different values of C, penalty, and multi_class\n    # Check that the model can be fitted and the coefficients are not all zero\n\n    X, y = make_classification(random_state=0)\n\n    lr = LogisticRegression(C=C, penalty=penalty, multi_class=multi_class,\n                            solver='saga', random_state=0)\n    lr.fit(X, y)\n\n    assert not np.allclose(lr.coef_, np.zeros_like(lr.coef_))\n"], "sample_986": ["def test_evalf_complex_parts():\n    a = re(1 + I*2, evaluate=False)\n    b = im(1 + I*2, evaluate=False)\n    assert a.evalf() == 1\n    assert b.evalf() == 2\n    assert a.evalf(subs={x: 3}) == 3\n    assert b.evalf(subs={x: 3}) == 0\n", "def test_evalf_Piecewise():\n    assert NS(Piecewise((x, x > 0), (0, True)), subs={x: 3}) == '3.00000000000000'\n    assert NS(Piecewise((x, x > 0), (0, True)), subs={x: -3}) == '0.000000000000000'\n    raises(NotImplementedError, lambda: Piecewise((x, x > 0), (y, True)).evalf(subs={x: 3, y: 2}))\n", "def test_issue_11876():\n    assert ((pi + I)**2).evalf() == -1\n", "def test_evalf_complex_zero_detection():\n    a = exp(10**10) * sin(1) + I * exp(-10**10)\n    b = exp(10**10) * sin(1)\n    assert a.evalf() == b.evalf()\n    assert a.evalf(chop=True) == b.evalf()\n    raises(PrecisionExhausted, lambda: a.evalf(strict=True))\n", "def test_issue_11403():\n    from sympy import Function, I, pi, sqrt\n    f = Function('f')\n    assert NS(f(pi) + I*f(pi), subs={f: sin}) == '0.999987711756967 + 0.999987711756967*I'\n"], "sample_120": ["def test_serialize_model_fields(self):\n    # Test serializing model fields\n    field = models.CharField(max_length=255)\n    self.assertSerializedResultEqual(\n        field,\n        (\"models.CharField(max_length=255)\", {\"from django.db import models\"})\n    )\n\n    # Test serializing a field with a custom deconstruct method\n    class CustomField(models.CharField):\n            name, path, args, kwargs = super().deconstruct()\n            kwargs['custom_arg'] = 'custom_value'\n            return name, path, args, kwargs\n\n    field = CustomField(max_length=255)\n    self.assertSerializedResultEqual(\n        field,\n        (\"models.CharField(max_length=255, custom_arg='custom_value')\", {\"from django.db import models\"})\n    )\n", "def test_serialize_complex(self):\n    complex_number = complex(1, 2)\n    self.assertSerializedEqual(complex_number)\n    self.assertSerializedResultEqual(complex_number, (\"complex('1+2j')\", set()))\n", "def test_serialize_model_fields(self):\n    \"\"\"\n    Test serialization of Model fields.\n    \"\"\"\n    # Create a model field\n    field = models.CharField(max_length=200)\n\n    # Serialize the field\n    string, imports = MigrationWriter.serialize(field)\n\n    # Check the serialized string\n    self.assertEqual(string, \"models.CharField(max_length=200)\")\n\n    # Check the imported modules\n    self.assertEqual(imports, {\"from django.db import models\"})\n\n    # Deserialize the field and compare with the original field\n    new_field = self.serialize_round_trip(field)\n    self.assertEqual(field.__class__, new_field.__class__)\n    self.assertEqual(field.max_length, new_field.max_length)\n", "def test_serialize_complex_numbers(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex((1+2j))\", set())\n    )\n", "def test_serialize_custom_field(self):\n    class CustomField(models.Field):\n            path = 'migrations.test_writer.CustomField'\n            args = []\n            kwargs = {}\n            return path, args, kwargs\n\n    field = CustomField()\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(string, \"migrations.test_writer.CustomField()\")\n"], "sample_594": ["def test_last_item_empty_array():\n    array = np.array([])\n    result = formatting.last_item(array)\n    assert result == []\n", "def test_inline_variable_array_repr_dask_array():\n    import dask.array as da\n\n    value = da.from_array(np.array([20, 40]), chunks=2)\n    variable = xr.Variable(\"x\", value)\n\n    max_width = 30\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    expected = \"dask.array<chunksize=(2), meta=np.ndarray>\"\n    assert actual == expected\n", "def test_inline_variable_array_repr_custom_array_function():\n    class CustomArrayFunction:\n            self.value = value\n\n            return NotImplemented\n\n        @property\n            return self.value.shape\n\n        @property\n            return self.value.dtype\n\n        @property\n            return self.value.ndim\n\n    value = CustomArrayFunction(np.array([20, 40]))\n    variable = xr.Variable(\"x\", value)\n\n    max_width = 10\n    actual = formatting.inline_variable_array_repr(variable, max_width=10)\n\n    expected = repr(value).replace(\"\\n\", \" \")\n    assert len(actual) <= max_width\n    assert expected.startswith(actual)\n", "def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\"),\n        (pd.Timedelta(\"1 hour 30 minutes\"), \"0 days 01:30:00\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n\n    actual = formatting.format_timedelta(pd.Timedelta(\"10 days 1 hour\"), \"date\")\n    expected = \"10 days\"\n    assert expected == actual\n\n    actual = formatting.format_timedelta(pd.Timedelta(\"10 days 1 hour\"), \"time\")\n    expected = \"01:00:00\"\n    assert expected == actual\n", "def test_diff_coords_repr_with_multiindex(self):\n    ds_a = xr.Dataset(\n        coords={\n            \"x\": pd.MultiIndex.from_arrays(\n                [np.array([\"a\", \"b\"], dtype=\"U1\"), np.array([1, 2], dtype=\"int64\")],\n                names=(\"letter\", \"number\"),\n            ),\n        }\n    )\n\n    ds_b = xr.Dataset(\n        coords={\n            \"x\": pd.MultiIndex.from_arrays(\n                [np.array([\"a\", \"c\"], dtype=\"U1\"), np.array([1, 3], dtype=\"int64\")],\n                names=(\"letter\", \"number\"),\n            ),\n        }\n    )\n\n    expected = dedent(\n        \"\"\"\\\n    Differing coordinates:\n    L * x        (x) MultiIndex\n        letter   (x) <U1 'a' 'b'\n        number   (x) int64 1 2\n    R * x        (x) MultiIndex\n        letter   (x) <U1 'a' 'c'\n        number   (x) int64 1 3\n    \"\"\"\n    ).strip()\n\n    actual = formatting.diff_coords_repr(ds_a.coords, ds_b.coords, \"identical\")\n    assert expected == actual\n"], "sample_1200": ["def test_physical_constant_property():\n    assert not meter.is_physical_constant\n    assert not second.is_physical_constant\n    assert not volt.is_physical_constant\n    assert not ohm.is_physical_constant\n    assert not elementary_charge.is_physical_constant  # Quantity\n    assert not speed_of_light.is_physical_constant  # Number\n    assert not molar_gas_constant.is_physical_constant  # PhysicalConstant\n    assert vacuum_permittivity.is_physical_constant\n    assert gravitational_constant.is_physical_constant\n", "def test_physical_constant_property():\n    assert not meter.is_physical_constant\n    assert not kilogram.is_physical_constant\n    assert not second.is_physical_constant\n    assert not volt.is_physical_constant\n    assert not ohm.is_physical_constant\n    assert not speed_of_light.is_physical_constant\n    assert not elementary_charge.is_physical_constant\n    assert not vacuum_permittivity.is_physical_constant\n    assert not gravitational_constant.is_physical_constant\n    assert not molar_gas_constant.is_physical_constant\n    assert not planck.is_physical_constant\n    assert not boltzmann.is_physical_constant\n    assert not stefan_boltzmann.is_physical_constant\n    assert not avogadro.is_physical_constant\n    assert speed_of_light.physical_constant is PhysicalConstant(\"speed_of_light\")\n    assert elementary_charge.physical_constant is PhysicalConstant(\"elementary_charge\")\n    assert vacuum_permittivity.physical_constant is PhysicalConstant(\"vacuum_permittivity\")\n    assert gravitational_constant.physical_constant is PhysicalConstant(\"gravitational_constant\")\n    assert molar_gas_constant.physical_constant is PhysicalConstant(\"molar_gas_constant\")\n    assert planck.physical_constant is PhysicalConstant(\"planck\")\n    assert boltzmann.physical_constant is PhysicalConstant(\"boltzmann\")\n    assert stefan_boltzmann.physical_constant is PhysicalConstant(\"stefan_boltzmann\")\n    assert avogadro.physical_constant is PhysicalConstant(\"avogadro\")\n", "def test_prefixed_quantity_operations():\n    u = centimeter * 2\n    v = meter * 3\n    assert u + v == meter + 20 * centimeter\n    assert u - v == -meter + 20 * centimeter\n    assert u * v == 60 * centimeter**2\n    assert u / v == Rational(1, 3)\n    assert u ** 2 == 4 * centimeter**2\n    assert sqrt(u) == 10 * centimeter\n", "def test_issue_24402():\n    from sympy.physics.units import length, time, frequency, hertz\n    f = Quantity('f')\n    SI.set_quantity_dimension(f, frequency)\n    SI.set_quantity_scale_factor(f, 1 / second)\n    l = Quantity('l')\n    SI.set_quantity_dimension(l, length)\n    SI.set_quantity_scale_factor(l, 1 * meter)\n    expr = f * l\n    dim = SI._collect_factor_and_dimension(expr)[1]\n    assert dim == length / time\n    assert SI.get_dimension_system().equivalent_dims(dim, hertz * meter)\n", "def test_get_quantity_scale_factor():\n    assert SI.get_quantity_scale_factor(meter) == 1\n    assert SI.get_quantity_scale_factor(second) == 1\n    assert SI.get_quantity_scale_factor(kg) == 1\n    assert SI.get_quantity_scale_factor(coulomb) == elementary_charge\n    assert SI.get_quantity_scale_factor(molar_gas_constant) == 8.31446261815324\n    assert SI.get_quantity_scale_factor(vacuum_permittivity) == 8.8541878128e-12 * farad / meter\n"], "sample_13": ["def test_angle_from_invalid_input():\n    \"\"\"\n    Test that an error is raised when creating an Angle with invalid input.\n    \"\"\"\n    with pytest.raises(ValueError):\n        Angle('invalid input')\n", "def test_angle_wrap_at_360(unit, value):\n    angle = Angle(value, unit=unit)\n    wrapped_angle = angle.wrap_at(360 * u.degree)\n    assert wrapped_angle == Angle(value % 360, unit=u.degree)\n", "def test_angle_string_parsing(angle_str, expected_degree):\n    angle = Angle(angle_str)\n    assert_allclose(angle.degree, expected_degree)\n", "def test_latitude_boundaries(angle, unit, expected):\n    if expected is None:\n        lat = Latitude(angle, unit=unit)\n        assert lat.value == angle\n    else:\n        with pytest.raises(expected):\n            Latitude(angle, unit=unit)\n", "def test_longitude_wrap_angle():\n    \"\"\"\n    Test setting the wrap_angle attribute of a Longitude object.\n    \"\"\"\n    lon = Longitude(['370d', '88d'], wrap_angle='180d')\n    assert np.all(lon == Longitude(['170d', '-92d']))\n\n    # Test changing the wrap_angle attribute\n    lon.wrap_angle = '90d'\n    assert np.all(lon == Longitude(['80d', '-172d']))\n\n    # Test setting wrap_angle to a value out of the range -360d to 360d\n    with pytest.raises(ValueError):\n        lon.wrap_angle = '361d'\n    with pytest.raises(ValueError):\n        lon.wrap_angle = '-361d'\n"], "sample_921": ["def test_is_builtin_class_method_int():\n    assert is_builtin_class_method(int, \"__new__\") is True\n    assert is_builtin_class_method(int, \"__init__\") is True\n    assert is_builtin_class_method(int, \"my_method\") is False\n", "def test_is_builtin_class_method_for_int():\n    assert is_builtin_class_method(int, '__init__') is True\n    assert is_builtin_class_method(int, 'my_method') is False\n\n    class MyInt(int):\n            pass\n\n    assert is_builtin_class_method(MyInt, '__init__') is True\n    assert is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.register) is True\n    assert inspect.is_singledispatch_function(func.dispatch) is False\n    assert inspect.is_singledispatch_function(lambda x: x) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @fun.register(int)\n        pass\n\n    assert inspect.is_singledispatch_function(fun) is True\n    assert inspect.is_singledispatch_function(fun.register) is False\n\n        pass\n\n    assert inspect.is_singledispatch_function(not_singledispatch) is False\n", "def test_signature_annotations_with_forward_refs():\n    from typing import ForwardRef\n\n        pass\n\n    sig = inspect.signature(func)\n    assert stringify_signature(sig) == '(a: A, b: B)'\n"], "sample_456": ["def test_formset_with_custom_deletion_field_name(self):\n    \"\"\"\n    Test that the formset correctly uses a custom deletion field name.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\", delete_field_name=\"REMOVE\")\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-0-votes\" value=\"100\"></li>'\n        '<li>Remove: <input type=\"checkbox\" name=\"choices-0-REMOVE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-1-choice\" value=\"Fergie\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-1-votes\" value=\"900\"></li>'\n        '<li>Remove: <input type=\"checkbox\" name=\"choices-1-REMOVE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-2-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-2-votes\"></li>'\n        '<li>Remove: <input type=\"checkbox\" name=\"choices-2-REMOVE\"></li>',\n    )\n    # To delete something, set that form's special delete field to 'on'.\n    # Let's go ahead and delete Fergie.\n    data = {\n        \"choices-TOTAL_FORMS\": \"3\",  # the number of forms rendered\n        \"choices-INITIAL_FORMS\": \"2\",  # the", "def test_formset_validate_min_excludes_initial_forms(self):\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"1\",\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=2, min_num=1, validate_min=True, can_delete=True)\n    formset = ChoiceFormSet(data, prefix=\"choices\", initial=[{\"choice\": \"Initial\", \"votes\": \"1\"}])\n    self.assertFalse(formset.has_changed())\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [\"Please submit at least 1 form.\"])\n", "def test_formset_validate_max_deleted_forms(self):\n    \"\"\"\n    validate_max should ignore forms marked for deletion.\n    \"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"3\",\n        \"choices-INITIAL_FORMS\": \"2\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"2\",\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-0-DELETE\": \"on\",  # deleted\n        \"choices-1-choice\": \"Fergie\",\n        \"choices-1-votes\": \"900\",\n        \"choices-1-DELETE\": \"\",\n        \"choices-2-choice\": \"The Decemberists\",\n        \"choices-2-votes\": \"500\",\n        \"choices-2-DELETE\": \"\",\n    }\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, max_num=2, validate_max=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [])\n    self.assertEqual(len(formset.deleted_forms), 1)\n", "def test_custom_deletion_widget(self):\n    \"\"\"Custom deletion widget is used when provided.\"\"\"\n    CustomDeletionWidget = HiddenInput(attrs={\"class\": \"custom-deletion-widget\"})\n    CustomDeletionFormSet = formset_factory(Choice, can_delete=True, deletion_widget=CustomDeletionWidget)\n    formset = CustomDeletionFormSet(initial=[{\"choice\": \"Zero\", \"votes\": 0}])\n    self.assertIn('class=\"custom-deletion-widget\"', str(formset.forms[0][\"DELETE\"]))\n", "def test_formset_with_deletion_non_empty_form(self):\n    \"\"\"\n    A formset with can_delete and extra forms doesn't require the extra\n    forms to be filled out when deleting an initial form.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, extra=1)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n    ]\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",  # the number of forms rendered\n        \"choices-INITIAL_FORMS\": \"1\",  # the number of forms with initial data\n        \"choices-MIN_NUM_FORMS\": \"0\",  # min number of forms\n        \"choices-MAX_NUM_FORMS\": \"0\",  # max number of forms\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-0-DELETE\": \"on\",\n        \"choices-1-choice\": \"\",\n        \"choices-1-votes\": \"\",\n        \"choices-1-DELETE\": \"\",\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\", initial=initial)\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.deleted_forms],\n        [{\"votes\": 100, \"DELETE\": True, \"choice\": \"Calexico\"}],\n    )\n"], "sample_585": ["def test_da_groupby_apply_shortcut():\n\n        return array * 2\n\n    array = xr.DataArray([1, 2, 3], [('x', [1, 2, 3])])\n    expected = xr.DataArray([2, 4, 6], [('x', [1, 2, 3])])\n    actual = array.groupby('x').apply(func, shortcut=True)\n    assert_identical(expected, actual)\n", "def test_da_groupby_empty_group_variable():\n\n    array = xr.DataArray([1, 2, 3], [('x', ['a', 'b', 'b'])])\n\n    with pytest.raises(ValueError):\n        array.groupby(xr.DataArray([], name='y'))\n", "def test_groupby_empty_dimension():\n    # Test groupby with an empty dimension\n    array = xr.DataArray([1, 2, 3], coords={'x': ('x', [1, 2, 3]), 'y': ('y', [])}, dims=('x', 'y'))\n    with pytest.raises(ValueError):\n        array.groupby('y')\n", "def test_da_groupby_empty_coord():\n\n    empty_coord = xr.DataArray([], dims='coord')\n    array = xr.DataArray([1, 2, 3], [('dim', [1, 2, 3]), ('coord', [])])\n\n    with pytest.raises(ValueError):\n        array.groupby(empty_coord)\n", "def test_groupby_empty_coord():\n    # regression test for GH2180\n    array = xr.DataArray([1, 2, 3], coords={'x': ('y', [1, 2, 3])}, dims='y')\n    expected = xr.DataArray([6], coords={'x': ('z', [2])}, dims='z')\n    actual = array.groupby('x').sum()\n    assert_identical(expected, actual)\n"], "sample_147": ["def test_union_with_different_models(self):\n    Number.objects.create(num=100, other_num=100)\n    ReservedName.objects.create(name='100', order=100)\n    qs1 = Number.objects.filter(num=100).values_list('num', flat=True)\n    qs2 = ReservedName.objects.filter(order=100).values_list('order', flat=True)\n    self.assertEqual(list(qs1.union(qs2).order_by('num')), [100, 100])\n", "def test_union_with_different_numbers_of_fields(self):\n    qs1 = Number.objects.filter(num=1).values('num', 'other_num')\n    qs2 = Number.objects.filter(num=2).values('num')\n    msg = 'Merging QuerySet classes must involve the same values in each case.'\n    with self.assertRaisesMessage(TypeError, msg):\n        list(qs1.union(qs2))\n", "def test_union_with_values_list_and_distinct(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=1),\n        ReservedName(name='rn2', order=2),\n        ReservedName(name='rn1', order=3),\n    ])\n    qs1 = ReservedName.objects.values_list('name', flat=True).distinct()\n    qs2 = ReservedName.objects.values_list('order', flat=True).distinct()\n    self.assertCountEqual(qs1.union(qs2), ['rn1', 'rn2', 1, 2, 3])\n", "def test_union_with_distinct_and_order_by(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=5),\n        ReservedName(name='rn3', order=7),\n        ReservedName(name='rn4', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=5)\n    qs2 = ReservedName.objects.filter(order__lt=5)\n    union_qs = qs1.union(qs2)\n    for qs, expected_result in (\n        (union_qs.order_by('-pk').distinct('order').values_list('order', flat=True), [7, 5, -1]),\n        (union_qs.order_by('pk').distinct('order').values_list('order', flat=True), [5, 7, -1]),\n        (union_qs.values_list('order', flat=True).distinct('order').order_by('-pk'), [7, 5, -1]),\n        (union_qs.values_list('order', flat=True).distinct('order').order_by('pk'), [5, 7, -1]),\n    ):\n        with self.subTest(qs=qs):\n            self.assertEqual(list(qs), expected_result)\n", "def test_union_with_different_types(self):\n    qs1 = Number.objects.filter(num__lte=1).values_list('num', flat=True)\n    qs2 = ReservedName.objects.values_list('order', flat=True)\n    result = list(qs1.union(qs2))\n    self.assertCountEqual(result, [0, 1, 2])\n"], "sample_160": ["    def test_force_decimal_position_with_zero(self):\n        self.assertEqual(nformat(0, '.', decimal_pos=2), '0.00')\n", "def test_scientific_notation(self):\n    self.assertEqual(nformat(1e10, '.'), '10000000000')\n    self.assertEqual(nformat(1e10, '.', decimal_pos=2), '10000000000.00')\n    self.assertEqual(nformat(1e200, '.'), '1e+200')\n    self.assertEqual(nformat(-1e200, '.'), '-1e+200')\n", "    def test_decimal_subnormal(self):\n        class SubnormalDecimal(Decimal):\n            \"\"\"\n            Wrapper for Decimal which considers numbers less than 1e-6 to be \"subnormal\".\n            \"\"\"\n                if abs(self) < Decimal('1e-6'):\n                    return 'subnormal'\n                return super().__format__(specifier, **kwargs)\n\n        value = SubnormalDecimal('1e-7')\n        self.assertEqual(nformat(value, '.'), 'subnormal')\n        value = SubnormalDecimal('1e-5')\n        self.assertEqual(nformat(value, '.'), '0.00001')\n", "    def test_grouping_sequence(self):\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '123,45,6789')\n        self.assertEqual(nformat(123456789, '.', decimal_pos=2, grouping=(3, 2, 0), thousand_sep=','), '123,45,6789.00')\n        self.assertEqual(nformat(-123456789, '.', decimal_pos=1, grouping=(3, 2, 0), thousand_sep=','), '-123,45,6789.0')\n", "    def test_number_with_custom_locale(self):\n        # Testing with a custom locale\n        self.assertEqual(nformat(123456789, ',', grouping=(3, 2), decimal_pos=2, thousand_sep='.', use_l10n=False), '12.34.56.789,00')\n        # Testing with grouping as a single value\n        self.assertEqual(nformat(123456789, ',', grouping=3, decimal_pos=2, thousand_sep='.', use_l10n=False), '123.456.789,00')\n        # Testing with force_grouping and decimal_pos\n        self.assertEqual(nformat(123456789, ',', decimal_pos=2, thousand_sep='.', force_grouping=True, use_l10n=False), '123.456.789,00')\n"], "sample_1197": ["def test_prefixed_conversion():\n    assert convert_to(kilometer, meter) == 1000*meter\n    assert convert_to(centimeter, meter) == Rational(1, 100)*meter\n    assert convert_to(pebibyte, byte) == 2**50*byte\n", "def test_derived_units():\n    derived_unit = Quantity('derived_unit')\n    SI.set_quantity_dimension(derived_unit, length * area)\n    SI.set_quantity_scale_factor(derived_unit, 1)\n\n    assert derived_unit in SI.derived_units\n    assert SI.derived_units[length * area] == derived_unit\n", "def test_physical_constant_property():\n    assert not meter.is_physical_constant\n    assert not joule.is_physical_constant\n    assert not day.is_physical_constant\n    assert not second.is_physical_constant\n    assert not volt.is_physical_constant\n    assert not ohm.is_physical_constant\n    assert not centimeter.is_physical_constant\n    assert not kilometer.is_physical_constant\n    assert not kilogram.is_physical_constant\n    assert not pebibyte.is_physical_constant\n    assert vacuum_permittivity.is_physical_constant\n    assert elementary_charge.is_physical_constant\n    assert molar_gas_constant.is_physical_constant\n    assert gravitational_constant.is_physical_constant\n", "def test_get_quantity_dimension_and_scale_factor():\n    q = Quantity(\"q\")\n    SI.set_quantity_dimension(q, length)\n    SI.set_quantity_scale_factor(q, 10)\n    assert SI.get_quantity_dimension(q) == length\n    assert SI.get_quantity_scale_factor(q) == 10\n    assert q.convert_to(meter) == 10*meter\n", "def test_prefixed_quantities():\n    assert (centimeter * centimeter).convert_to(meter) == 0.01*meter\n    assert (centimeter / meter).convert_to(1) == 0.01\n    assert (centimeter ** 2).convert_to(meter**2) == 0.0001*meter**2\n    assert (centimeter ** 3).convert_to(meter**3) == 0.000001*meter**3\n    assert (kibibyte * kibibyte).convert_to(byte) == 1024**2*byte\n    assert (kibibyte / byte).convert_to(1) == 1024\n    assert (kibibyte ** 2).convert_to(byte**2) == 1024**2*byte**2\n    assert (kibibyte ** 3).convert_to(byte**3) == 1024**3*byte**3\n"], "sample_751": ["def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work without sample_weights in the base estimator\n\n    The random weighted sampling is done internally in the _boost method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(len(X))\n\n    X, y = datasets.make_classification(n_samples=200, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, n_classes=2, random_state=0)\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator(), random_state=0)\n    clf.fit(X, y)\n    assert clf.score(X, y) >= 0.5\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work without sample_weights in the base estimator\n\n    The random weighted sampling is done internally in the _boost method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n    X, y = datasets.make_classification(n_samples=200, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=0)\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator(), random_state=0)\n    clf.fit(X, y)\n", "def test_base_estimator_sample_weight_support():\n    # Test that base estimators support sample_weight.\n    from sklearn.dummy import DummyRegressor\n\n    X, y = datasets.make_regression(n_samples=100, n_features=5, random_state=42)\n    sample_weight = np.random.RandomState(42).rand(100)\n\n    clf = AdaBoostRegressor(DummyRegressor(strategy=\"mean\"))\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    # Check that sample_weight was passed to the base estimator.\n    for estimator in clf.estimators_:\n        assert hasattr(estimator, \"sample_weight_\")\n        assert_array_equal(estimator.sample_weight_, sample_weight)\n", "def test_adaboost_regressor_with_sample_weight():\n    # Check regression with sample_weight input.\n    X, y = datasets.make_regression(n_samples=15, n_features=5, n_targets=1, random_state=42)\n    sample_weight = np.random.RandomState(0).rand(len(X))\n\n    clf = AdaBoostRegressor(random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_equal(clf.estimators_samples_[0].sum(), len(X))\n    assert_array_equal(clf.estimator_weights_.shape, (clf.n_estimators,))\n    assert_array_equal(clf.estimator_errors_.shape, (clf.n_estimators,))\n    assert_equal(clf.classes_, None)\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work without sample_weights in the base estimator\n\n    The random weighted sampling is done internally in the _boost_real method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.ones((X.shape[0], 2)) / 2\n\n    X, y = datasets.make_classification(n_samples=100, n_features=20, n_informative=2,\n                                        n_redundant=10, n_classes=2, random_state=42)\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator(), random_state=42)\n    clf.fit(X, y)\n\n    assert hasattr(clf, 'estimators_')\n    assert len(clf.estimators_) > 0\n"], "sample_892": ["def test_adaboost_classifier_string_classes():\n    # Test that AdaBoostClassifier works with string class labels\n    clf = AdaBoostClassifier()\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n", "def test_adaboost_classifier_single_class():\n    # Test that AdaBoostClassifier raises an error when there's only a single class in the target.\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 1])  # All instances have the same class\n    model = AdaBoostClassifier()\n\n    err_msg = \"y contains only one label\"\n    with pytest.raises(ValueError, match=err_msg):\n        model.fit(X, y)\n", "def test_adaboost_classifier_feature_importance_error():\n    # Test that it gives proper exception when feature_importances_ is called on a\n    # base_estimator that doesn't have it.\n    class DummyEstimator(BaseEstimator):\n            pass\n\n            return np.zeros(X.shape[0])\n\n    clf = AdaBoostClassifier(DummyEstimator())\n    clf.fit(X, y_class)\n\n    msg = \"Unable to compute feature importances since estimator does not have a feature_importances_ attribute\"\n    with pytest.raises(AttributeError, match=msg):\n        clf.feature_importances_\n", "def test_adaboost_with_custom_loss_function():\n    \"\"\"Check that a custom loss function can be used with AdaBoostRegressor.\"\"\"\n\n        return np.abs(y_true - y_pred) ** 2\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    model = AdaBoostRegressor(loss=custom_loss, random_state=42)\n    model.fit(X, y)\n    pred = model.predict(X)\n\n    assert len(pred) == len(y)\n    assert isinstance(pred, np.ndarray)\n", "def test_adaboost_classifier_samme_without_predict_proba():\n    # Test that SAMME.R fails if base_estimator does not implement predict_proba\n    X, y = iris.data, iris.target\n    estimator = NoSampleWeightWrapper(DummyClassifier(strategy=\"most_frequent\"))\n    clf = AdaBoostClassifier(estimator=estimator, algorithm=\"SAMME.R\")\n    err_msg = \"AdaBoostClassifier with algorithm='SAMME.R' requires that the weak learner supports the calculation of class probabilities with a predict_proba method.\"\n    with pytest.raises(TypeError, match=err_msg):\n        clf.fit(X, y)\n"], "sample_860": ["def test_check_X_y_multi_output():\n    X = np.ones((2, 2))\n    y = np.array([[1, 2], [3, 4]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_allclose_dense_sparse(y_checked, y)\n", "def test_check_array_dtype_preserved(dtype):\n    X = np.arange(9).reshape(3, 3).astype(dtype)\n    X_checked = check_array(X, dtype=dtype)\n    assert X_checked.dtype == dtype\n    assert np.allclose(X, X_checked)\n", "def test_check_array_with_large_sparse_matrix():\n    # Create a large sparse matrix with int64 indices\n    X = sp.rand(int(1e6), 10, format='csr', dtype=np.float64)\n    X.indices = X.indices.astype(np.int64)\n    X.indptr = X.indptr.astype(np.int64)\n\n    # Check that the function raises a ValueError when accept_large_sparse is False\n    with pytest.raises(ValueError, match=\"Only sparse matrices with 32-bit integer indices are accepted.\"):\n        check_array(X, accept_sparse=True, accept_large_sparse=False)\n\n    # Check that the function does not raise an error when accept_large_sparse is True\n    check_array(X, accept_sparse=True, accept_large_sparse=True)\n", "def test_allclose_dense_sparse_close(toarray):\n    base = np.arange(9).reshape(3, 3)\n    x, y = toarray(base), toarray(base + 1e-10)\n    assert _allclose_dense_sparse(x, y)\n", "def test_allclose_dense_sparse_different_types(toarray):\n    base = np.arange(9).reshape(3, 3)\n    x, y = toarray(base), toarray(base.astype(float))\n    with pytest.raises(ValueError, match=\"Can only compare two sparse matrices, not a sparse matrix and an array\"):\n        _allclose_dense_sparse(x, y)\n"], "sample_995": ["def test_issue_10020_complex_exponent():\n    assert oo**I is S.ComplexInfinity\n    assert oo**(1 + I) is S.ComplexInfinity\n    assert oo**(-1 + I) is S.Zero\n    assert (-oo)**I is S.ComplexInfinity\n    assert (-oo)**(-1 + I) is S.Zero\n", "def test_NumberSymbol_float_conversion():\n    assert float(pi) == mpmath.pi\n    assert float(E) == mpmath.e\n    assert float(GoldenRatio) == (1 + mpmath.sqrt(5)) / 2\n    assert float(EulerGamma) == mpmath.euler\n    raises(TypeError, lambda: float(oo))\n    raises(TypeError, lambda: float(-oo))\n    raises(TypeError, lambda: float(zoo))\n    raises(TypeError, lambda: float(nan))\n    raises(TypeError, lambda: float(I))\n", "def test_Float_repr():\n    assert repr(Float('1.0', dps=15)) == \"Float('1.0', precision=15)\"\n    assert repr(Float('1.0', precision=15)) == \"Float('1.0', precision=15)\"\n    assert repr(Float('1.0', dps=15, precision=15)) == \"Float('1.0', precision=15)\"\n    assert repr(Float('1.0')) == \"Float('1.0')\"\n    assert repr(Float('1.0', dps=15, precision=20)) == \"Float('1.0', precision=20)\"\n", "def test_Float_sub():\n    assert Float('1.5') - Float('0.5') == Float('1.0')\n    assert Float('1.5') - 0.5 == Float('1.0')\n    assert Float('1.5') - Float('0.5', 3) == Float('1.0')\n    assert Float('1.5') - 0.5j == Float('1.5') - Float('0.5j')\n    assert Float('1.5') - '0.5' == Float('1.0')\n    assert Float('1.5') - S.Half == Float('1.0')\n    raises(TypeError, lambda: Float('1.5') - cos)\n", "def test_Float_conversion():\n    assert Float(0.5) == Rational(1, 2)\n    assert Float('0.5') == Rational(1, 2)\n    assert Float('0.5', precision=10) == Rational(1, 2)\n    assert Float('0.5', dps=1) == Float('0.5', precision=1)\n    assert Float(0.5, precision=10) == Rational(1, 2)\n    assert Float(0.5, dps=1) == Float('0.5', precision=1)\n"], "sample_1205": ["def test_PolyElement_sqf_part():\n    _, x = ring(\"x\", ZZ)\n\n    f = x**5 - x**3 - x**2 + 1\n    p = x**4 + x**3 - x - 1\n\n    assert f.sqf_part() == p\n", "def test_PolyElement_sqf_list_all():\n    _, x = ring(\"x\", ZZ)\n\n    f = x**5 - x**3 - x**2 + 1\n    g = x**3 + 2*x**2 + 2*x + 1\n    h = x - 1\n    p = x**4 + x**3 - x - 1\n\n    assert f.sqf_list(all=True) == [(1, [(g, 1), (h, 2)]), (1, [(p, 1)])]\n", "def test_PolyElement_factor_list_with_fractions():\n    _, x = ring(\"x\", QQ)\n\n    f = x**2 - QQ(2, 3)\n\n    u = x - QQ(sqrt(2), 2)\n    v = x + QQ(sqrt(2), 2)\n\n    assert f.factor_list() == (1, [(u, 1), (v, 1)])\n", "def test_PolyElement_imul_num():\n    R, x = ring(\"x\", ZZ)\n    f = 3*x**2 + 4*x + 2\n\n    f.imul_num(5)\n    assert f == 15*x**2 + 20*x + 10\n\n    g = R(1)\n    h = g.imul_num(5)\n    assert h == 5 and g is not h\n", "def test_PolyElement_trunc():\n    R, x = ring(\"x\", ZZ)\n\n    f = 3*x**2 - 2*x + 5\n    assert f.trunc(3) == 3*x**2\n\n    R, x, y = ring(\"x,y\", ZZ)\n    f = 2*x**3*y - 3*x*y**2 + 4\n    assert f.trunc(2) == 2*x**2*y\n"], "sample_198": ["def test_expression_wrapper_as_sql(self):\n    expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n    sql, params = expr.as_sql(compiler=None, connection=None)\n    self.assertEqual(sql, '3')\n    self.assertEqual(params, [])\n", "def test_equal_output_field(self):\n    expr = ExpressionWrapper(Value(1), output_field=IntegerField())\n    same_expr = ExpressionWrapper(Value(1), output_field=IntegerField())\n    other_expr = ExpressionWrapper(Value(1), output_field=CharField())\n    self.assertEqual(expr, same_expr)\n    self.assertNotEqual(expr, other_expr)\n", "def test_expression_wrapper_with_col(self):\n    expr = ExpressionWrapper(Col('alias', 'field'), output_field=IntegerField())\n    self.assertEqual(expr.get_group_by_cols(alias=None), [Ref(alias='alias', target=Col('alias', 'field'))])\n", "def test_expression_wrapper_source_expression(self):\n    source_expr = Value(10)\n    wrapper_expr = ExpressionWrapper(source_expr, output_field=IntegerField())\n    self.assertEqual(wrapper_expr.source_expression, source_expr)\n\n    new_source_expr = Value(20)\n    wrapper_expr.set_source_expressions([new_source_expr])\n    self.assertEqual(wrapper_expr.source_expression, new_source_expr)\n", "def test_expression_wrapper_output_field(self):\n    expr = ExpressionWrapper(F('name'), output_field=CharField())\n    self.assertEqual(expr.output_field, CharField())\n"], "sample_191": ["def test_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "def test_tick_triggers_on_file_change(self, mock_notify_file_changed):\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        self.increment_mtime(self.existing_file)\n        next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n        self.assertEqual(mock_notify_file_changed.call_args[0][0], self.existing_file)\n", "def test_watch_dir_with_unresolvable_path_in_glob(self, mocked_modules, notify_mock):\n    path = Path('unresolvable_directory')\n    self.reloader.watch_dir(path, '**/*.mo')\n    self.assertEqual(list(self.reloader.directory_globs), [])\n", "def test_file_removed(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        self.existing_file.unlink()\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "def test_watch_glob_uses_existing_directories(self):\n    with mock.patch.object(self.reloader, '_subscribe') as mocked_subscribe:\n        self.reloader._watch_glob(self.tempdir, ['*'])\n    self.assertSequenceEqual(\n        mocked_subscribe.call_args[0],\n        [\n            self.tempdir, 'glob:%s' % self.tempdir,\n            ['anyof', ['match', '*', 'wholename']]\n        ]\n    )\n"], "sample_629": ["def test_expand_modules():\n    files_or_modules = [\"test_module.py\"]\n    ignore_list = [\"ignored_module.py\"]\n    ignore_list_re = [re.compile(\"unittest_.*\")]\n    ignore_list_paths_re = [re.compile(\".*tests/.*\")]\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert len(result) == 1\n    assert result[0][\"name\"] == \"test_module\"\n    assert len(errors) == 0\n", "def test_expand_modules():\n    files_or_modules = ['src/module1.py', 'src/module2/__init__.py', 'module3']\n    ignore_list = ['module1.py']\n    ignore_list_re = [re.compile('.*__init__.py')]\n    ignore_list_paths_re = []\n\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n\n    assert len(result) == 1\n    assert result[0]['name'] == 'module3'\n    assert len(errors) == 0\n", "def test_expand_modules_with_file():\n    files_or_modules = [\"test_file.py\"]\n    ignore_list = [\"ignored.py\"]\n    ignore_list_re = [re.compile(\".*enchilada.*\")]\n    ignore_list_paths_re = [re.compile(\".*tests/.*\")]\n\n    result, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n\n    assert len(result) == 1\n    assert len(errors) == 0\n    assert result[0][\"name\"] == \"test_file\"\n    assert result[0][\"isarg\"] is True\n", "def test_expand_modules():\n    files_or_modules = [\"test_module.py\", \"ignore_module.py\"]\n    ignore_list = [\"ignore_module.py\"]\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert len(result) == 1\n    assert len(errors) == 0\n    assert result[0]['name'] == 'test_module'\n", "def test_expand_modules():\n    files_or_modules = [\"module1\", \"tests/test_module2.py\"]\n    ignore_list = [\"tests\"]\n    ignore_list_re = [re.compile(\"unittest_.*\")]\n    ignore_list_paths_re = [re.compile(\".*__pycache__.*\")]\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert len(result) > 0\n    assert len(errors) == 0\n    # Add more assertions based on the expected output\n"], "sample_537": ["def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    assert_allclose(P, Su, atol=1e-06)\n", "def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    assert_allclose(P, Su, atol=1e-06)\n", "def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    Su_2side = np.fft.fftshift(Su)\n    assert_allclose(P, Su_2side, atol=1e-06)\n", "def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    assert_allclose(P, Su, atol=1e-06)\n", "def test_specgram_onesided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f, t = mlab.specgram(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                            detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                            scale_by_freq=None,\n                            sides='onesided')\n    Su_1side = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n    assert_allclose(np.mean(P, axis=1), Su_1side, atol=1e-06)\n"], "sample_607": ["def test_get_backend_valid_engine():\n    backend = plugins.get_backend(\"dummy\")\n    assert isinstance(backend, DummyBackendEntrypoint1)\n", "def test_guess_engine_success():\n    DummyBackendEntrypoint1.guess_can_open = mock.MagicMock(return_value=True)\n    engine = plugins.guess_engine(\"valid\")\n    assert engine == \"dummy\"\n", "def test_guess_engine_no_installed_engine():\n    with pytest.raises(ValueError, match=r\"xarray is unable to open this file\"):\n        plugins.guess_engine(\"not-valid\")\n", "def test_unrecognized_engine():\n    with pytest.raises(ValueError, match=r\"unrecognized engine\"):\n        plugins.get_backend(\"unknown\")\n", "def test_guess_engine_warning():\n    dummy_backend = mock.MagicMock(return_value=DummyBackendEntrypointArgs())\n    dummy_backend.guess_can_open.side_effect = Exception()\n    plugins.BACKEND_ENTRYPOINTS[\"dummy\"] = dummy_backend\n\n    with pytest.warns(RuntimeWarning, match=\"'dummy' fails while guessing\"):\n        plugins.guess_engine(\"not-valid\")\n"], "sample_164": ["    def test_server_formatter_custom_time_format(self):\n        custom_time_format = '%Y-%m-%d %H:%M'\n        server_time = '2016-09-25 10:20'\n        log_msg = 'log message'\n        logger = logging.getLogger('django.server')\n        logger.handlers[0].setFormatter(ServerFormatter(datefmt=custom_time_format))\n\n        @contextmanager\n            old_stream = logger.handlers[0].stream\n            new_stream = StringIO()\n            logger.handlers[0].stream = new_stream\n            yield new_stream\n            logger.handlers[0].stream = old_stream\n\n        with patch_django_server_logger() as logger_output:\n            logger.info(log_msg, extra={'server_time': server_time})\n            self.assertEqual('[%s] %s\\n' % (server_time, log_msg), logger_output.getvalue())\n", "    def test_middleware_exception_creates_log_message(self):\n        self.assertLogsRequest(\n            url='/middleware_exception/',\n            level='ERROR',\n            msg='Internal Server Error: /middleware_exception/',\n            status_code=500,\n            logger='django.request',\n            exc_class=Exception,\n        )\n", "    def test_emit_without_request(self):\n        handler = AdminEmailHandler()\n        record = self.logger.makeRecord('name', logging.ERROR, 'function', 'lno', 'message', None, None)\n        handler.emit(record)\n        self.assertEqual(len(mail.outbox), 1)\n        msg = mail.outbox[0]\n        self.assertEqual(msg.subject, \"[Django] ERROR: message\")\n        self.assertIn(\"ERROR: message\\n\\n\", msg.body)\n", "    def test_page_found_no_info(self):\n        self.client.get('/innocent/')\n        self.assertEqual(self.logger_output.getvalue(), '')\n", "    def test_server_formatter_server_time_not_set(self):\n        formatter = ServerFormatter()\n        record = logging.makeLogRecord({'msg': 'log message', 'status_code': 200})\n        record.server_time = None\n        self.assertRegex(formatter.format(record), r'^\\[[/:,\\w\\s\\d]+\\] .*')\n"], "sample_106": ["def test_cache_key_varies_by_accept_encoding(self):\n    \"\"\"\n    get_cache_key keys differ by Accept-Encoding header value\n    \"\"\"\n    request1 = self.factory.get(self.path, HTTP_ACCEPT_ENCODING='gzip')\n    learn_cache_key(request1, HttpResponse())\n    request2 = self.factory.get(self.path, HTTP_ACCEPT_ENCODING='identity')\n    learn_cache_key(request2, HttpResponse())\n    self.assertNotEqual(get_cache_key(request1), get_cache_key(request2))\n", "def test_has_vary_header(self):\n    response = HttpResponse()\n    response['Vary'] = 'Accept-Language, Cookie'\n    self.assertTrue(has_vary_header(response, 'Accept-Language'))\n    self.assertTrue(has_vary_header(response, 'cookie'))\n    self.assertFalse(has_vary_header(response, 'Accept-Encoding'))\n    self.assertFalse(has_vary_header(response, '*'))\n    response['Vary'] = '*'\n    self.assertTrue(has_vary_header(response, 'Accept-Language'))\n", "def test_cache_control_header(self):\n    request = self.factory.get(self.path)\n    response = HttpResponse()\n    response['Cache-Control'] = 'max-age=60'\n    learn_cache_key(request, response)\n    self.assertIn('max-age=60', get_cache_key(request))\n", "def test_invalid_backend(self):\n    invalid_backend_params = {\n        'BACKEND': 'django.core.cache.backends.invalid_backend.InvalidBackend',\n    }\n    with self.assertRaises(InvalidCacheBackendError):\n        caches_setting_for_tests(base=invalid_backend_params)\n", "def test_cache_versioning_add_without_version(self):\n    cache.add('answer1', 42)\n    self.assertEqual(cache.get('answer1'), 42)\n    self.assertIsNone(cache.add('answer1', 37))\n    self.assertEqual(cache.get('answer1'), 42)\n"], "sample_16": ["def test_matmul(self):\n    q1 = np.arange(9.0).reshape(3, 3) * u.m\n    q2 = np.arange(9.0).reshape(3, 3) / u.s\n    out = np.matmul(q1, q2)\n    expected = np.matmul(q1.value, q2.value) * u.m / u.s\n    assert np.all(out == expected)\n", "def test_matrix_inv_power_non_square():\n    q = np.arange(9.0).reshape(3, 2) << u.m\n    with pytest.raises(np.linalg.LinAlgError):\n        np.linalg.matrix_power(q, -1)\n", "def test_inner_with_complex_numbers():\n    q1 = np.array([1j, 2j, 3j]) * u.m\n    q2 = np.array([4j, 5j, 6j]) / u.s\n    o = np.inner(q1, q2)\n    expected = np.inner(q1.value, q2.value) * u.m / u.s\n    assert np.allclose(o, expected)\n", "def test_merge_arrays_fill_value(self):\n    arr = rfn.merge_arrays((self.q_pv, self.q_pv_t), fill_value=-1.0 * u.m)\n    expected = np.array([(self.q_pv[0], self.q_pv_t[0]), (self.q_pv[1], self.q_pv_t[1]), (self.q_pv[2], (-1.0 * u.m, -1.0 * u.m))], dtype=[('f0', [('p', '<f8'), ('v', '<f8')]), ('f1', [('pv', [('pp', '<f8'), ('vv', '<f8')]), ('t', '<f8')])])\n    assert_array_equal(arr, expected)\n", "def test_merge_arrays_fill_value(self):\n    # Merge arrays with fill_value\n    arr = rfn.merge_arrays((self.q_pv[\"p\"], self.q_pv_t[\"pv\"]), fill_value=-1)\n    expected_p = self.q_pv[\"p\"].value\n    expected_pp = self.q_pv_t[\"pv\"][\"pp\"].value\n    expected_vv = np.full_like(expected_p, -1)\n    assert_array_equal(arr[\"f0\"], expected_p << u.km)\n    assert_array_equal(arr[\"f1\"][\"pp\"], expected_pp << u.km)\n    assert_array_equal(arr[\"f1\"][\"vv\"], expected_vv << u.km)\n"], "sample_897": ["def test_partial_dependence_display_kind_centered_single_feature(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_wrong_kind_value(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_kind_centered_interaction_error(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_centered_and_subsample(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_kind_centered_single_feature(\n    pyplot,\n    kind,\n    clf_diabetes,\n    diabetes,"], "sample_618": ["def test_polyval_errors():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs, degree_dim=\"invalid\")\n", "def test_polyval_invalid_degree_dim():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\")\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs, degree_dim=\"invalid_dim\")\n", "def test_polyval_invalid_degree_dim():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"invalid_dim\": [0, 1, 2]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs)\n", "def test_polyval_fill_value():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 5]})\n    expected = xr.DataArray([9, 2 + 6 + 16, 2 + 9 + 4 * 3**5], dims=\"x\")\n    actual = xr.polyval(coord=x, coeffs=coeffs)\n    xr.testing.assert_allclose(actual, expected)\n", "def test_polyval_errors():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs, degree_dim=\"wrong_dim\")\n"], "sample_992": ["def test_CustomPrintedObject():\n    p = SciPyPrinter()\n    obj = CustomPrintedObject()\n    assert p.doprint(obj) == 'numpy'\n    p = MpmathPrinter()\n    assert p.doprint(obj) == 'mpmath'\n", "def test_CustomPrintedObject():\n    obj = CustomPrintedObject()\n    assert pycode(obj) == 'numpy'\n    assert NumPyPrinter().doprint(obj) == 'numpy'\n    assert MpmathPrinter().doprint(obj) == 'mpmath'\n", "def test_CustomPrintedObject():\n    obj = CustomPrintedObject()\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(obj) == 'CustomPrintedObject()'\n    assert 'numpy' not in prntr.module_imports\n    prntr = NumPyPrinter()\n    assert prntr.doprint(obj) == 'numpy'\n    assert 'numpy' in prntr.module_imports\n    prntr = MpmathPrinter()\n    assert prntr.doprint(obj) == 'mpmath'\n    assert 'mpmath' in prntr.module_imports\n", "def test_CustomPrintedObject():\n    p = NumPyPrinter()\n    obj = CustomPrintedObject()\n    assert p.doprint(obj) == 'numpy'\n\n    p = MpmathPrinter()\n    obj = CustomPrintedObject()\n    assert p.doprint(obj) == 'mpmath'\n", "def test_CustomPrintedObject():\n    obj = CustomPrintedObject()\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(obj) == 'CustomPrintedObject()'\n    prntr = NumPyPrinter()\n    assert prntr.doprint(obj) == 'numpy'\n    prntr = MpmathPrinter()\n    assert prntr.doprint(obj) == 'mpmath'\n"], "sample_541": ["def test_polygon_selector_box_props(ax):\n    # Create a simple square shape\n    verts = [(20, 20), (40, 20), (40, 40), (20, 40)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with custom box properties\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_props={'facecolor': 'r', 'alpha': 0.5})\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the box properties are set correctly\n    assert tool._box._selection_artist.get_facecolor() == mcolors.to_rgba('r', alpha=0.5)\n", "def test_polygon_selector_box_handle_props(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with custom handle props\n    handle_props = dict(color='red', size=10)\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True, box_handle_props=handle_props)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the handle props were set correctly\n    for artist in tool._box._handles_artists:\n        assert artist.get_color() == handle_props['color']\n        assert artist.get_sizes()[0] == handle_props['size']\n", "def test_polygon_selector_rotate(ax):\n    # Create a rectangle\n    verts = [(10, 10), (30, 10), (30, 30), (10, 30)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Rotate the rectangle\n    tool.add_state('rotate')\n    do_event(tool, 'onmove', xdata=20, ydata=20)\n    do_event(tool, 'press', xdata=20, ydata=20, button=1)\n    do_event(tool, 'onmove', xdata=25, ydata=25)\n    do_event(tool, 'release', xdata=25, ydata=25, button=1)\n\n    # Check that the vertices have been rotated\n    rotated_verts = [(15.0, 15.0), (35.0, 15.0), (35.0, 35.0), (15.0, 35.0)]\n    np.testing.assert_allclose(tool.verts, rotated_verts)\n", "def test_polygon_selector_scale_bounding_box(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # In order to trigger the correct callbacks, trigger events on the canvas\n    # instead of the individual tools\n    t = ax.transData\n    canvas = ax.figure.canvas\n\n    # Scale to double size using the bottom left corner of the bounding box\n    MouseEvent(\n        \"button_press_event\", canvas, *t.transform((20, 0)), 1)._process()\n    MouseEvent(\n        \"motion_notify_event\", canvas, *t.transform((0, 40)))._process()\n    MouseEvent(\n        \"button_release_event\", canvas, *t.transform((0, 40)), 1)._process()\n    np.testing.assert_allclose(\n        tool.verts, [(0, 0), (-20, 20), (0, 40), (20, 20)])\n", "def test_polygon_selector_box_style(ax):\n    # Create a simple shape\n    verts = [(20, 0), (0, 20), (20, 40)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with custom box style\n    box_handle_props = dict(markersize=10, markeredgecolor='red',\n                            markerfacecolor='yellow')\n    box_props = dict(edgecolor='green', facecolor='lightgreen', alpha=0.5)\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_handle_props=box_handle_props,\n                                   box_props=box_props)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the box style is correctly applied\n    box = tool._box._selection_artist\n    assert box.get_edgecolor() == 'green'\n    assert box.get_facecolor() == 'lightgreen'\n    assert box.get_alpha() == 0.5\n\n    handles = tool._box._corner_handles.artists\n    for handle in handles:\n        assert handle.get_markersize() == 10\n        assert handle.get_markeredgecolor() == 'red'\n        assert handle.get_markerfacecolor() == 'yellow'\n"], "sample_330": ["    def test_sql_keywords_as_table_name(self):\n        reporter = Reporter.objects.create(first_name='Test', last_name='User')\n        sql_keyword_obj = SQLKeywordsModel.objects.create(reporter=reporter)\n        self.assertEqual(SQLKeywordsModel.objects.get(pk=sql_keyword_obj.pk), sql_keyword_obj)\n", "def test_last_executed_query_with_parameters(self):\n    \"\"\"\n    last_executed_query should return a string with the parameters\n    interpolated if they are provided.\n    \"\"\"\n    with connection.cursor() as cursor:\n        sql = \"SELECT * FROM backends_article WHERE id = %s\"\n        params = (1,)\n        cursor.execute(sql, params)\n        last_sql = cursor.db.ops.last_executed_query(cursor, sql, params)\n        self.assertEqual(last_sql, \"SELECT * FROM backends_article WHERE id = 1\")\n", "    def test_foreign_key_constraint_violation(self):\n        with self.assertRaises(IntegrityError):\n            ObjectReference.objects.create(obj_id=99999)  # An id that doesn't exist in Object model\n", "    def setUp(self):\n        self.obj = Object.objects.create()\n        self.ref = ObjectReference.objects.create(obj=self.obj)\n", "    def test_cannot_set_non_existent_object(self):\n        non_existent_obj_id = 12345\n        self.assertFalse(Object.objects.filter(id=non_existent_obj_id).exists())\n        with self.assertRaises(ValueError):\n            ObjectReference.objects.create(obj_id=non_existent_obj_id)\n"], "sample_481": ["def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": mark_safe(\" & \")}\n    )\n    self.assertEqual(output, \"alpha & beta & me\")\n", "def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": \"<br>\"}\n    )\n    self.assertEqual(output, \"alpha<br>beta & me\")\n", "def test_join09(self):\n    output = self.engine.render_to_string(\"join09\", {\"a\": [\"apple\", \"banana\", \"cherry\"]})\n    self.assertEqual(output, \"apple | banana | cherry\")\n", "def test_join_autoescape_on(self):\n    var_list = [\"<p>Hello World!</p>\", \"beta & me\", \"<script>Hi!</script>\"]\n    context = {\"var_list\": var_list, \"var_joiner\": \"<br/>\"}\n    output = self.engine.render_to_string(\"join_autoescape_on\", context)\n    expected_result = \"&lt;p&gt;Hello World!&lt;/p&gt;&lt;br/&gt;beta &amp; me&lt;br/&gt;&lt;script&gt;Hi!&lt;/script&gt;\"\n    self.assertEqual(output, expected_result)\n", "def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"alpha\", \"beta\", \"gamma\"], \"var\": \", \"}\n    )\n    self.assertEqual(output, \"alpha, beta, gamma\")\n"], "sample_499": ["def test_legend_title_fontprop_dict():\n    # test the title_fontproperties kwarg\n    plt.plot(range(10))\n    leg = plt.legend(title='Aardvark', title_fontproperties={'family': 'serif', 'size': 22})\n    assert leg.get_title().get_fontproperties().get_family() == 'serif'\n    assert leg.get_title().get_fontproperties().get_size() == 22\n", "def test_legend_text_color(color):\n    # test text color for a single color\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), np.arange(10)*1, label='#1')\n    ax.plot(np.arange(10), np.arange(10)*2, label='#2')\n    ax.plot(np.arange(10), np.arange(10)*3, label='#3')\n\n    leg = ax.legend(text_color=color)\n    for text in leg.get_texts():\n        assert mpl.colors.same_color(text.get_color(), color)\n", "def test_legend_fontsize():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='hello world')\n    legend = ax.legend(fontsize=12)\n    assert legend.prop.get_size() == 12\n", "def test_legend_title_edgecolor_inherit():\n    # Test that legend title edgecolor inherits from axes when 'inherit'\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    ax.set_facecolor('w')\n    ax.set_edgecolor('k')\n    leg = ax.legend(title='Aardvark', title_edgecolor='inherit')\n    assert leg.get_title().get_edgecolor() == ax.spines['left'].get_edgecolor()\n", "def test_legend_handler_update():\n    # Test the update method for custom legend handler.\n    class CustomHandler(HandlerTuple):\n            legend_handle[0].update_from(orig_handle[0])\n            legend_handle[1].update_from(orig_handle[1])\n\n    fig, ax = plt.subplots()\n    p1, = ax.plot([1, 2, 3], '-o')\n    p2, = ax.plot([2, 3, 4], '-x')\n    ax.legend([(p1, p2)], ['custom'], handler_map={(p1, p2): CustomHandler()})\n\n    legend = ax.get_legend()\n    handler = legend._legend_handler_map[(p1, p2)]\n    legend_handle = legend.legendHandles[0]\n    orig_handle = (p1, p2)\n\n    handler.update_prop(legend_handle, orig_handle, legend)\n    assert legend_handle[0].get_color() == orig_handle[0].get_color()\n    assert legend_handle[1].get_marker() == orig_handle[1].get_marker()\n"], "sample_858": ["def test_voting_regressor_with_sample_weight():\n    \"\"\"Tests sample_weight parameter of VotingRegressor\"\"\"\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = DummyRegressor(strategy='median')\n    ereg1 = VotingRegressor([('mean', reg1), ('median', reg2)], weights=[1, 2]).fit(X_r, y_r, sample_weight=np.ones((len(y_r),)))\n    ereg2 = VotingRegressor([('mean', reg1), ('median', reg2)], weights=[1, 2]).fit(X_r, y_r)\n    assert_array_equal(ereg1.predict(X_r), ereg2.predict(X_r))\n\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y_r),))\n    ereg3 = VotingRegressor([('mean', reg1), ('median', reg2)], weights=[1, 2])\n    ereg3.fit(X_r, y_r, sample_weight)\n    reg1.fit(X_r, y_r, sample_weight)\n    assert_array_equal(ereg3.predict(X_r), reg1.predict(X_r))\n", "def test_transform_hard_voting():\n    \"\"\"Check transform method of VotingClassifier with hard voting on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    expected_transform = np.array([[1, 1, 1],\n                                   [1, 1, 1],\n                                   [2, 2, 2],\n                                   [2, 2, 2]])\n\n    assert_array_equal(eclf.transform(X), expected_transform)\n", "def test_predict_shape():\n    \"\"\"Check predict shape of VotingClassifier and VotingRegressor.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=5, random_state=123)\n\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)])\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)])\n\n    eclf.fit(X, y)\n    ereg.fit(X_r, y_r)\n\n    assert eclf.predict(X).shape == (len(X),)\n    assert ereg.predict(X_r).shape == (len(X_r),)\n", "def test_predict_on_unfitted_error():\n    \"\"\"Check that calling predict on unfitted VotingClassifier raises an error.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1)], voting='hard')\n    msg = \"This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    with pytest.raises(NotFittedError, match=msg):\n        eclf.predict(X)\n", "def test_weights_input_validation():\n    \"\"\"Check weights input validation in VotingClassifier and VotingRegressor.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = DummyRegressor(strategy='median')\n\n    # Too few weights\n    with pytest.raises(ValueError, match='Number of `estimators` and weights must be equal'):\n        VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], weights=[1]).fit(X, y)\n\n    # Too many weights\n    with pytest.raises(ValueError, match='Number of `estimators` and weights must be equal'):\n        VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], weights=[1, 2, 3]).fit(X, y)\n\n    # Invalid weight type\n    with pytest.raises(ValueError, match='Weights must be numbers'):\n        VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], weights=['a', 2]).fit(X, y)\n\n    # Negative weight\n    with pytest.raises(ValueError, match='Weights must be non-negative'):\n        VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], weights=[1, -2]).fit(X, y)\n\n    # Validation for VotingRegressor\n    with pytest.raises(ValueError, match='Number of `estimators` and weights must be equal'):\n        VotingRegressor(estimators=[('reg1', reg1), ('reg2', reg2)], weights=[1]).fit(X_r, y_r)\n"], "sample_1121": ["def test_div_modulo():\n    assert divmod(x, y) == (x // y, x % y)\n    assert divmod(x, 3) == (x // 3, x % 3)\n    assert divmod(3, x) == (3 // x, 3 % x)\n", "def test_Mul_expand():\n    assert expand_mul(x*y) == x*y\n    assert expand_mul(x*(y+z)) == x*y + x*z\n    assert expand_mul((x+y)*z) == x*z + y*z\n    assert expand_mul((x+y)*(z+w)) == x*z + x*w + y*z + y*w\n", "def test_div():\n    e = a/b\n    assert e == a*b**(-1)\n    e = a/b + c/2\n    assert e == a*b**(-1) + Rational(1)/2*c\n    e = (1 - b)/(b - 1)\n    assert e == (1 + -b)*((-1) + b)**(-1)\n\n    # Test for divisibility\n    assert (x / y).is_divisible(z) == (x % z == 0 and y % z == 0)\n\n    # Test for non-divisibility\n    assert not (x / y).is_divisible(z) == (x % z != 0 or y % z != 0)\n\n    # Test for divisibility when y is 1\n    assert (x / 1).is_divisible(z) == (x % z == 0)\n\n    # Test for non-divisibility when y is 1\n    assert not (x / 1).is_divisible(z) == (x % z != 0)\n", "def test_divmod_symbolic():\n    x, y = symbols('x y', real=True, positive=True)\n    assert divmod(x, y) == (floor(x/y), x % y)\n    assert divmod(x, 3) == (floor(x/3), x % 3)\n    assert divmod(3, x) == (floor(3/x), 3 % x)\n\n    # Symbolic division and modulo should return expressions\n    assert divmod(x, y)[0].is_real is True\n    assert divmod(x, y)[1].is_real is True\n    assert divmod(x, 3)[0].is_real is True\n    assert divmod(x, 3)[1].is_real is True\n    assert divmod(3, x)[0].is_real is True\n    assert divmod(3, x)[1].is_real is True\n\n    # Test with zero\n    x = symbols('x', real=True)\n    assert divmod(x, 0) == (nan, nan)\n    assert divmod(0, x) == (0, 0)\n    assert divmod(0, 0) == (nan, nan)\n\n    # Test with negative numbers\n    x, y = symbols('x y', real=True)\n    assert divmod(x, -y) == (floor(x/-y), x % -y)\n    assert divmod(-x, y) == (floor(-x/y), -x % y)\n    assert divmod(-x, -y) == (floor(-x/-y), -x % -y)\n\n    # Test with complex numbers\n    x, y = symbols('x y', complex=True)\n    assert divmod(x, y)[0].is_complex is True\n    assert divmod(x, y)[1].is_complex is True\n    assert divmod(x, 3)[0].is_complex is True\n    assert divmod(x, 3)[1].is_complex is True\n    assert divmod(3, x)[0].is_complex is True\n    assert divmod(3, x)[1].is_complex is True\n", "def test_div():\n    e = a/b\n    assert e == a*b**(-1)\n    e = a/b + c/2\n    assert e == a*b**(-1) + Rational(1)/2*c\n    e = (1 - b)/(b - 1)\n    assert e == (1 + -b)*((-1) + b)**(-1)\n\n    # Test with non-commutative symbols\n    A, B, C = symbols('A B C', commutative=False)\n    e = A/B\n    assert e == A*B**(-1)\n    e = A/B + C/2\n    assert e == A*B**(-1) + Rational(1)/2*C\n    e = (1 - B)/(B - 1)\n    assert e != (1 + -B)*((-1) + B)**(-1)  # A/B != B**(-1)*A\n\n    # Test with integers\n    assert (5/2).is_integer is False\n    assert (4/2).is_integer is True\n\n    # Test with rationals\n    assert (Rational(3, 4)/Rational(1, 2)).is_rational is True\n    assert (Rational(3, 4)/Rational(1, 3)).is_rational is True\n    assert (Rational(3, 4)/Rational(1, 3)).is_integer is False\n\n    # Test with complex numbers\n    assert (1 + I)/(1 - I) == 2*I/2\n    assert (1 + I)/(1 + I) == 1\n\n    # Test with symbols\n    x, y = symbols('x y')\n    assert (x/y).is_rational is None\n    assert (x/y).is_integer is None\n    assert (x/1).is_integer is None\n    assert (x/2).is_integer is None\n\n    # Test with zero\n    assert (0/x).is_zero is True\n    assert (0/y).is_zero is True\n    with raises(ZeroDivisionError):\n        x/0\n    with raises"], "sample_406": ["def test_save_with_update_fields(self):\n    pub_date = datetime.now()\n    a = Article.objects.create(headline=\"Original\", pub_date=pub_date)\n    new_headline = \"Updated\"\n    a.headline = new_headline\n    with self.assertNumQueries(1):\n        a.save(update_fields=[\"headline\"])\n    a.refresh_from_db()\n    self.assertEqual(a.headline, new_headline)\n    self.assertEqual(a.pub_date, pub_date)\n", "    def test_related_manager_queryset(self):\n        article = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        featured = FeaturedArticle.objects.create(article=article)\n        self.assertEqual(article.featuredarticles.get(), featured)\n", "    def test_save_updates_fields(self):\n        pub_date = datetime.now()\n        a = Article.objects.create(headline=\"Original headline\", pub_date=pub_date)\n        new_pub_date = pub_date + timedelta(days=10)\n        a.headline = \"New headline\"\n        a.pub_date = new_pub_date\n        with self.assertNumQueries(1):\n            a.save(update_fields=[\"headline\"])\n        a.refresh_from_db()\n        self.assertEqual(a.headline, \"New headline\")\n        self.assertEqual(a.pub_date, pub_date)\n", "def test_save_with_select_on_save_and_custom_manager(self):\n    class CustomManager(BaseManager.from_queryset(models.QuerySet)):\n            return super().get_queryset().filter(headline__startswith=\"Custom\")\n\n    class CustomArticle(Article):\n        objects = CustomManager()\n        class Meta:\n            proxy = True\n\n    CustomArticle.objects.create(headline=\"Custom Article\", pub_date=datetime.now())\n    asos = CustomArticle.objects.get(headline=\"Custom Article\")\n    with self.assertNumQueries(2):\n        asos.save()\n    with self.assertNumQueries(1):\n        asos.save(force_update=True)\n    CustomArticle.objects.all().delete()\n    with self.assertRaisesMessage(\n        DatabaseError, \"Forced update did not affect any rows.\"\n    ):\n        with self.assertNumQueries(1):\n            asos.save(force_update=True)\n", "def test_defer_fields(self):\n    pub_date = datetime.now()\n    a = Article.objects.create(headline=\"Test Article\", pub_date=pub_date)\n    a_defer = Article.objects.defer(\"headline\").get(id=a.id)\n    self.assertTrue(hasattr(a_defer, \"id\"))\n    self.assertTrue(hasattr(a_defer, \"pub_date\"))\n    self.assertFalse(hasattr(a_defer, \"headline\"))\n"], "sample_1196": ["def test_contains_non_set():\n    x = Symbol('x')\n    class NonSet:\n        pass\n    raises(TypeError, lambda: Contains(x, NonSet()))\n", "def test_contains_non_set():\n    x = Symbol('x')\n    class NotASet:\n        pass\n    raises(TypeError, lambda: Contains(x, NotASet()))\n", "def test_contains_with_sets():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Contains(x, S.EmptySet) is S.false\n    assert Contains(x, FiniteSet(x)) is S.true\n    assert Contains(x, FiniteSet(y)) == Contains(x, FiniteSet(y), evaluate=False)\n", "def test_contains_with_non_sets():\n    x = Symbol('x')\n    raises(TypeError, lambda: Contains(x, 1))\n    raises(TypeError, lambda: Contains(x, 'a'))\n    raises(TypeError, lambda: Contains(x, [1, 2, 3]))\n", "def test_eval_method():\n    x = Symbol('x')\n    y = Symbol('y', integer=True)\n\n    assert Contains(x, S.Reals).eval() is True\n    assert Contains(y, S.Naturals).eval() == Contains(y, S.Naturals)\n    assert Contains(y, S.Integers).eval() is True\n\n    raises(TypeError, lambda: Contains(x, 'Not a Set'))\n"], "sample_93": ["def test_annotate_case_when(self):\n    books = Book.objects.annotate(\n        price_category=Case(\n            When(price__lt=Decimal('30.00'), then=Value('Cheap')),\n            When(price__lt=Decimal('50.00'), then=Value('Affordable')),\n            default=Value('Expensive'),\n        )\n    ).order_by('pk')\n    self.assertQuerysetEqual(\n        books, [\n            ('The Definitive Guide to Django: Web Development Done Right', 'Cheap'),\n            ('Sams Teach Yourself Django in 24 Hours', 'Cheap'),\n            ('Practical Django Projects', 'Cheap'),\n            ('Python Web Development with Django', 'Affordable'),\n            ('Artificial Intelligence: A Modern Approach', 'Expensive'),\n            ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 'Expensive')\n        ],\n        lambda b: (b.name, b.price_category)\n    )\n", "def test_annotate_filter_on_aggregate(self):\n    authors = Author.objects.annotate(book_count=Count('book')).filter(book_count__gt=1)\n    self.assertEqual(authors.count(), 6)\n\n    authors = Author.objects.annotate(book_count=Count('book')).filter(book_count=1)\n    self.assertEqual(authors.count(), 3)\n\n    authors = Author.objects.annotate(book_count=Count('book')).filter(book_count__lt=2)\n    self.assertEqual(authors.count(), 3)\n", "def test_complex_filter_with_aggregation(self):\n    qs = Publisher.objects.annotate(\n        total_pages=Sum('book__pages'),\n        total_books=Count('book')\n    ).filter(\n        total_pages__gt=F('total_books') * 300\n    ).order_by('num_awards')\n    self.assertQuerysetEqual(\n        qs, [3, 7, 9], lambda v: v.num_awards\n    )\n", "def test_subquery_annotation_in_group_by(self):\n    \"\"\"\n    Subquery annotations are included in the GROUP BY if they are explicitly\n    grouped against.\n    \"\"\"\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).values('publisher').annotate(avg_price=Avg('price')).values('avg_price')\n    publishers = Publisher.objects.annotate(avg_book_price=Subquery(subquery)).values('avg_book_price').annotate(count=Count('id')).order_by('count')\n    self.assertEqual(\n        list(publishers), [\n            {'avg_book_price': None, 'count': 1},\n            {'avg_book_price': Approximate(Decimal('60.83'), places=2), 'count': 2},\n            {'avg_book_price': Approximate(Decimal('44.77'), places=2), 'count': 3},\n        ]\n    )\n", "def test_filter_aggregate_on_annotation(self):\n    # Test filtering on an annotation\n    books = Book.objects.annotate(total_authors=Count('authors')).filter(total_authors__gt=2)\n    self.assertQuerysetEqual(books, ['Python Web Development with Django'], lambda b: b.name)\n"], "sample_810": ["def test_pipeline_with_fit_params():\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', TransfFitParams()), ('clf', Mult())])\n    pipe.fit(X, y=None, transf__should_get_this=True)\n    assert pipe.named_steps['transf'].fit_params['should_get_this']\n    assert 'should_get_this' not in pipe.named_steps['clf'].fit_params\n", "def test_pipeline_memory_with_fit_params():\n    X = np.array([[1, 2]])\n    y = np.array([0])\n    transf = TransfFitParams()\n    clf = FitParamT()\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        pipe = Pipeline([('transf', transf), ('clf', clf)], memory=memory)\n        pipe.fit(X, y, transf__should_get_this=True, clf__should_succeed=True)\n        assert transf.fit_params['should_get_this']\n        assert clf.successful\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_memory_invalid_fit_params():\n    # Test that an error is raised when fit_params are used with memory\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())],\n                    memory='/tmp/sklearn_cache')\n    assert_raises_regex(ValueError,\n                        \"Pipeline with memory does not support fit_params\",\n                        pipe.fit, X, y=None, transf__invalid_param=True)\n", "def test_pipeline_with_no_fit():\n    # Test that a pipeline with a transformer that doesn't implement fit\n    # can be created but not used for fitting or transforming\n    pipe = Pipeline([('transf', NoFit()), ('clf', FitParamT())])\n    assert_raises_regex(AttributeError,\n                        \"'NoFit' object has no attribute 'fit'\",\n                        pipe.fit, X=None, y=None)\n    assert_raises_regex(AttributeError,\n                        \"'NoFit' object has no attribute 'transform'\",\n                        pipe.transform, X=None)\n    assert_raises_regex(AttributeError,\n                        \"'NoFit' object has no attribute 'fit_transform'\",\n                        pipe.fit_transform, X=None, y=None)\n", "def test_pipeline_with_no_fit_transform():\n    # Test that a pipeline works when the final step does not have a fit_transform method\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', NoFitTransform())])\n    pipe.fit(X, y=None)\n    assert_raises(AttributeError, pipe.fit_transform, X, y=None)\n"], "sample_433": ["def test_add_model_index_together(self):\n    author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n    )\n    book = ModelState(\n        \"testapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"author\", \"title\")},\n        },\n    )\n    changes = self.get_changes([author], [author, book])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"Book\",\n        options={\"index_together\": {(\"author\", \"title\")}},\n    )\n", "def test_add_model_with_indexes_order_with_respect_to_index_together(self):\n    author = ModelState(\n        \"otherapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"create_model_with_indexes_idx\")\n            ]\n        },\n    )\n    book_with_author = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        options={\n            \"order_with_respect_to\": \"author\",\n            \"index_together\": {(\"title\", \"_order\")},\n        },\n    )\n    changes = self.get_changes([], [author, book_with_author])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"CreateModel\", \"CreateModel\", \"AddIndex\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        name=\"Book\",\n        options={\n            \"order_with_respect_to\": \"author\",\n            \"index_together\": {(\"title\", \"_order\")},\n        },\n    )\n    added_index = models.Index(fields=[\"name\"], name=\"create_model_with_indexes_idx\")\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        2,\n        model_name=\"author\",\n        index=added_index,\n    )\n", "    def test_add_model_with_indexes_and_constraints(self):\n        author = ModelState(\n            \"otherapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                \"indexes\": [\n                    models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n                ],\n                \"constraints\": [\n                    models.CheckConstraint(check=models.Q(name__contains=\"Author\"), name=\"name_contains_author\"),\n                ],\n            },\n        )\n        changes = self.get_changes([], [author])\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\", \"AddIndex\", \"AddConstraint\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"Author\")\n        added_index = models.Index(fields=[\"name\"], name=\"author_name_idx\")\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 1, model_name=\"author\", index=added_index)\n        added_constraint = models.CheckConstraint(check=models.Q(name__contains=\"Author\"), name=\"name_contains_author\")\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 2, model_name=\"author\", constraint=added_constraint)\n", "def test_custom_deconstructible_with_custom_deconstruct(self):\n    \"\"\"\n    Custom deconstructible objects with a custom deconstruct method return the\n    correct values.\n    \"\"\"\n    class CustomDeconstructible:\n            self.value = value\n\n            return (\n                \"test_module.CustomDeconstructible\",\n                [],\n                {\"value\": self.value},\n            )\n\n    class ModelWithCustomDeconstructible(models.Model):\n        custom_field = CustomDeconstructible(\"test_value\")\n\n    before = self.make_project_state([ModelWithCustomDeconstructible])\n    after = before.clone()\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    self.assertEqual(changes, {})\n", "    def test_rename_index_together_to_index_extra_options(self):\n        changes = self.get_changes(\n            [AutodetectorTests.author_empty, self.book_index_together],\n            [AutodetectorTests.author_empty, self.book_partial_index],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"AlterIndexTogether\", \"AddIndex\"],\n        )\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 0, name=\"book\", index_together=set()\n        )\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"book_title_author_idx\"\n        )\n"], "sample_83": ["    def test_library_init(self):\n        library = Library()\n        self.assertEqual(library.filters, {})\n        self.assertEqual(library.tags, {})\n", "    def setUp(self):\n        self.library = Library()\n        self.context = {}\n", "    def test_import_library(self):\n        register = import_library('django.templatetags.i18n')\n        self.assertIsInstance(register, Library)\n        self.assertIn('yesno', register.filters)\n", "def test_tag_wrapped(self):\n    @self.library.tag\n    @functools.lru_cache(maxsize=32)\n        return Node()\n    func_wrapped = self.library.tags['func'].__wrapped__\n    self.assertIs(func_wrapped, func)\n    self.assertTrue(hasattr(func_wrapped, 'cache_info'))\n", "    def setUp(self):\n        self.library = Library()\n"], "sample_43": ["def test_ncp_prior_calculation(rseed=0):\n    rng = np.random.RandomState(rseed)\n    t = rng.rand(100)\n    x = np.ones_like(t)\n\n    # Test with gamma\n    gamma = 0.5\n    ncp_prior1 = bayesian_blocks(t, x, fitness='events', gamma=gamma).ncp_prior\n    assert_allclose(ncp_prior1, -np.log(gamma))\n\n    # Test with p0\n    p0 = 0.01\n    ncp_prior2 = bayesian_blocks(t, x, fitness='events', p0=p0).ncp_prior\n    fitfunc = Events(p0=p0)\n    assert_allclose(ncp_prior2, fitfunc.p0_prior(len(t)))\n\n    # Test with explicit ncp_prior\n    ncp_prior = 2.0\n    ncp_prior3 = bayesian_blocks(t, x, fitness='events', ncp_prior=ncp_prior).ncp_prior\n    assert_allclose(ncp_prior3, ncp_prior)\n", "def test_fitness_func_init():\n    # Test initialization of FitnessFunc with p0, gamma, and ncp_prior\n    p0 = 0.01\n    gamma = 2.0\n    ncp_prior = 3.0\n    fitfunc = FitnessFunc(p0=p0, gamma=gamma, ncp_prior=ncp_prior)\n    assert fitfunc.p0 == p0\n    assert fitfunc.gamma == gamma\n    assert fitfunc.ncp_prior == ncp_prior\n\n    # Test initialization of FitnessFunc with only p0\n    p0 = 0.05\n    fitfunc = FitnessFunc(p0=p0)\n    assert fitfunc.p0 == p0\n    assert fitfunc.gamma is None\n    assert fitfunc.ncp_prior is None\n\n    # Test initialization of FitnessFunc with only gamma\n    gamma = 1.5\n    fitfunc = FitnessFunc(gamma=gamma)\n    assert fitfunc.p0 is None\n    assert fitfunc.gamma == gamma\n    assert fitfunc.ncp_prior is None\n\n    # Test initialization of FitnessFunc with only ncp_prior\n    ncp_prior = 2.5\n    fitfunc = FitnessFunc(ncp_prior=ncp_prior)\n    assert fitfunc.p0 is None\n    assert fitfunc.gamma is None\n    assert fitfunc.ncp_prior == ncp_prior\n", "def test_p0_vs_ncp_prior(rseed=0):\n    rng = np.random.RandomState(rseed)\n    t = rng.rand(100)\n\n    # Test that specifying p0 and ncp_prior gives the same result\n    bins1 = bayesian_blocks(t, fitness='events', p0=0.05)\n    ncp_prior = -np.log(0.05)\n    bins2 = bayesian_blocks(t, fitness='events', ncp_prior=ncp_prior)\n\n    assert_allclose(bins1, bins2)\n", "def test_gamma_and_ncp_prior():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n    x = np.ones_like(t)\n    x[:20] += 1\n\n    gamma = 1.0\n    ncp_prior = -np.log(gamma)\n\n    bins1 = bayesian_blocks(t, x, fitness='events', gamma=gamma)\n    bins2 = bayesian_blocks(t, x, fitness='events', ncp_prior=ncp_prior)\n\n    assert_allclose(bins1, bins2)\n", "def test_multiple_change_points(rseed=0):\n    rng = np.random.RandomState(rseed)\n    x = np.concatenate([rng.rand(100),\n                        1 + rng.rand(200),\n                        2 + rng.rand(150)])\n\n    bins = bayesian_blocks(x)\n\n    assert (len(bins) == 4)\n    assert_allclose(bins[1], 1, rtol=0.02)\n    assert_allclose(bins[2], 2, rtol=0.02)\n"], "sample_861": ["def test_grid_search_bad_cv():\n    # Use global X, y\n\n    class BrokenKFold(KFold):\n            return 1\n\n    # create bad cv\n    cv = BrokenKFold(n_splits=3)\n\n    train_size = 100\n    grid = GridSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]}, cv=cv)\n\n    # assert that this raises an error\n    with pytest.raises(ValueError,\n                       match='cv.split and cv.get_n_splits returned '\n                             'inconsistent results. Expected \\\\d+ '\n                             'splits, got \\\\d+'):\n        grid.fit(X[:train_size], y[:train_size])\n", "def test_grid_search_groups_error():\n    # Check if ValueError is raised when groups is not None but cv object does not support groups\n    rng = np.random.RandomState(0)\n\n    X, y = make_classification(n_samples=15, n_classes=2, random_state=0)\n    groups = rng.randint(0, 3, 15)\n\n    clf = LinearSVC(random_state=0)\n    grid = {'C': [1]}\n\n    non_group_cvs = [StratifiedKFold(), StratifiedShuffleSplit()]\n    for cv in non_group_cvs:\n        gs = GridSearchCV(clf, grid, cv=cv)\n        # Should raise an error when groups is not None\n        assert_raise_message(ValueError,\n                             \"The 'groups' parameter should not be None.\",\n                             gs.fit, X, y, groups=groups)\n", "def test_random_search_cv_results_multimetric_refit():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    n_search_iter = 30\n    scoring = {'accuracy': make_scorer(accuracy_score),\n               'recall': make_scorer(recall_score)}\n\n    # Scipy 0.12's stats dists do not accept seed, hence we use param grid\n    params = dict(C=np.logspace(-4, 1, 3),\n                  gamma=np.logspace(-5, 0, 3, base=0.1))\n    for iid in (True, False):\n        for refit in (True, False, 'accuracy', 'recall'):\n            random_search = RandomizedSearchCV(SVC(probability=True, random_state=42),\n                                               n_iter=n_search_iter,\n                                               cv=n_splits, iid=iid,\n                                               param_distributions=params,\n                                               scoring=scoring,\n                                               refit=refit, random_state=0)\n            random_search.fit(X, y)\n\n            if isinstance(refit, bool):\n                if refit:\n                    assert random_search.refit == 'accuracy'\n                else:\n                    assert not hasattr(random_search, 'best_index_')\n                    assert not hasattr(random_search, 'best_score_')\n                    assert not hasattr(random_search, 'best_params_')\n            else:\n                assert random_search.refit == refit\n                assert hasattr(random_search, 'best_index_')\n                assert hasattr(random_search, 'best_score_')\n                assert hasattr(random_search, 'best_params_')\n\n            cv_results = random_search.cv_results_\n            assert all(np.all(cv_results[k] <= 1) for k in", "def test_random_search_cv_results_bad_cv():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    class BrokenKFold(KFold):\n            return 1\n\n    cv = BrokenKFold(n_splits=3)\n    n_search_iter = 30\n\n    params = [{'kernel': ['rbf'], 'C': expon(scale=10),\n               'gamma': expon(scale=0.1)},\n              {'kernel': ['poly'], 'degree': [2, 3]}]\n    param_keys = ('param_C', 'param_degree', 'param_gamma', 'param_kernel')\n    score_keys = ('mean_test_score', 'mean_train_score',\n                  'rank_test_score',\n                  'split0_test_score', 'split1_test_score',\n                  'split2_test_score',\n                  'split0_train_score', 'split1_train_score',\n                  'split2_train_score',\n                  'std_test_score', 'std_train_score',\n                  'mean_fit_time', 'std_fit_time',\n                  'mean_score_time', 'std_score_time')\n    n_cand = n_search_iter\n\n    search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,\n                                cv=cv, param_distributions=params,\n                                return_train_score=True)\n    with pytest.raises(ValueError,\n                       match='cv.split and cv.get_n_splits returned '\n                             'inconsistent results. Expected \\\\d+ '\n                             'splits, got \\\\d+'):\n        search.fit(X, y)\n", "def test_grid_search_multimetric_refit():\n    # Test that GridSearchCV can refit on the best parameter for a specific metric\n    X, y = make_classification(n_samples=100, n_classes=2, flip_y=.2, random_state=0)\n    clf = LinearSVC(random_state=0)\n    grid = {'C': [.1, 1, 10]}\n\n    search_no_scoring = GridSearchCV(clf, grid, scoring=None, refit='accuracy')\n    search_no_scoring.fit(X, y)\n\n    search_multi_scoring = GridSearchCV(clf, grid, scoring={'acc': 'accuracy', 'prec': 'precision'}, refit='prec')\n    search_multi_scoring.fit(X, y)\n\n    # Check that the best parameters are equal\n    assert search_multi_scoring.best_params_ == search_no_scoring.best_params_\n\n    # Check that the best score for precision is equal to the score of the best estimator\n    assert search_multi_scoring.best_score_ == search_multi_scoring.best_estimator_.score(X, y)\n"], "sample_711": ["def test_node_location_property(pytester: Pytester) -> None:\n    \"\"\"Test the location property of the Node class.\"\"\"\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    assert item.location == (str(pytester.path), 3, \"test_function\")\n", "def test_node_warning_location(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            import pytest\n            pytest.fail(\"Failure message\")\n    \"\"\"\n    )\n    with pytest.warns(PytestWarning) as rec:\n        items[0].runtest()\n    warning = rec.pop(PytestWarning)\n    assert str(warning.filename) == items[0].fspath\n    assert warning.lineno == 3\n", "def test_node_repr_failure(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert False, 'This test intentionally fails'\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(failed=1)\n    for report in result.getreports(\"pytest_runtest_logreport\"):\n        if report.failed:\n            failure_representation = nodes.Node.repr_failure(report.node, report.longrepr)\n            assert 'This test intentionally fails' in str(failure_representation)\n", "def test_node_from_parent_path_and_fspath_consistency():\n    fspath = legacy_path(\"/path/to/file.py\")\n    path = Path(\"/path/to/file.py\")\n    node = nodes.Node.from_parent(None, path=path, fspath=fspath)\n    assert node.path == path\n\n    inconsistent_fspath = legacy_path(\"/different/path/to/file.py\")\n    with pytest.raises(ValueError, match=\"Path\\\\(\\\\'/different/path/to/file.py\\\\'\\\\) != Path\\\\(\\\\'/path/to/file.py\\\\'\\\\)\"):\n        nodes.Node.from_parent(None, path=path, fspath=inconsistent_fspath)\n", "def test_fs_collector_from_parent(request, tmp_path: Path) -> None:\n    class CustomFile(nodes.FSCollector):\n            super().__init__(*args, **kwargs)\n\n    parent = request.session\n    path = tmp_path / \"test.py\"\n    path.touch()\n    collector = CustomFile.from_parent(parent, path=path)\n\n    assert collector.path == path\n    assert collector.name == path.name\n    assert collector.parent == parent\n    assert collector.session == parent.session\n    assert collector.config == parent.config\n"], "sample_658": ["def test_is_mocked(self):\n    obj = Broken()\n    assert _is_mocked(obj) is True\n\n    obj = object()\n    assert _is_mocked(obj) is False\n", "def test_is_mocked(self, mocker):\n    \"\"\"\n    Test that _is_mocked returns True for a mocked object.\n    \"\"\"\n    mocked_obj = mocker.MagicMock()\n    assert _is_mocked(mocked_obj) is True\n", "def test_is_mocked(self):\n    \"\"\"\n    Test the _is_mocked function with various objects.\n    \"\"\"\n    # Test with a non-mocked object\n    obj = object()\n    assert _is_mocked(obj) is False\n\n    # Test with a mocked object\n    mock_obj = Mock()\n    assert _is_mocked(mock_obj) is True\n\n    # Test with an object that has a pytest_mock_example_attribute_that_shouldnt_exist attribute\n    class TestObj:\n        pytest_mock_example_attribute_that_shouldnt_exist = 1\n    test_obj = TestObj()\n    assert _is_mocked(test_obj) is True\n\n    # Test with an object that doesn't have a pytest_mock_example_attribute_that_shouldnt_exist attribute\n    class TestObj2:\n        pass\n    test_obj2 = TestObj2()\n    assert _is_mocked(test_obj2) is False\n", "def test_doctest_mock_objects_dont_recurse_broken(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        class Example(object):\n            '''\n            >>> broken_obj = Broken()\n            >>> broken_obj.attr\n            Traceback (most recent call last):\n            ...\n            KeyError: 'This should be an AttributeError'\n            '''\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"* 1 passed *\"])\n", "def test_doctest_mock_objects_dont_recurse_missbehaved_broken_object(self, testdir, mock_module):\n    pytest.importorskip(mock_module)\n    testdir.makepyfile(\n        \"\"\"\n        from {mock_module} import MagicMock\n        class BrokenExample(object):\n            '''\n            >>> obj = MagicMock(spec=Broken)\n            >>> obj.method()\n            ''\n        \"\"\".format(\n            mock_module=mock_module\n        )\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"* 1 failed *\"])\n"], "sample_663": ["def test_collect_sub_with_symlinks_and_collect_ignore(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Symlink that gets collected.\n    sub.join(\"test_symlink.py\").mksymlinkto(\"test_file.py\")\n\n    # Create a conftest.py to ignore the symlink.\n    sub.ensure(\"conftest.py\").write(\"collect_ignore = ['test_symlink.py']\")\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines([\"sub/test_file.py::test_file PASSED*\", \"*1 passed in*\"])\n", "def test_collect_sub_with_symlinks_out_of_tree(testdir):\n    \"\"\"Test collection of symlink via out-of-tree rootdir.\"\"\"\n    sub = testdir.tmpdir.join(\"sub\")\n    real = sub.join(\"test_real.py\")\n    real.write(\"def test_real(): pass\", ensure=True)\n\n    out_of_tree = testdir.tmpdir.join(\"out_of_tree\").ensure(dir=True)\n    symlink_to_sub = out_of_tree.join(\"symlink_to_sub\")\n    symlink_to_sub.mksymlinkto(sub)\n    symlink_to_real = symlink_to_sub.join(\"test_symlink.py\")\n    symlink_to_real.mksymlinkto(real)\n\n    sub.chdir()\n    result = testdir.runpytest(\"-vs\", \"--rootdir=%s\" % sub, symlink_to_sub)\n    result.stdout.fnmatch_lines([\"sub/test_real.py::test_real PASSED*\", \"*1 passed in*\"])\n    assert result.ret == 0\n", "def test_collect_confcutdir(self, testdir):\n    subdir = testdir.mkdir(\"sub\")\n    confcutdir = testdir.mkdir(\"confcutdir\")\n    confcutdir.ensure(\"conftest.py\").write(\"def pytest_collect_file(path, parent): raise ValueError()\")\n    subdir.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    result = testdir.runpytest(\"--confcutdir\", str(confcutdir), str(sub))\n    result.stdout.fnmatch_lines([\"sub/test_file.py::test_file PASSED*\", \"*1 passed in*\"])\n", "def test_collect_protocol_module(self, testdir):\n    p = testdir.makepyfile(\"def test_func(): pass\")\n    id = p.basename\n\n    items, hookrec = testdir.inline_genitems(id)\n    assert len(items) == 1\n    assert items[0].name == \"test_func\"\n    newid = items[0].nodeid\n    assert newid == id + \"::test_func\"\n    hookrec.assert_contains(\n        [\n            (\"pytest_collectstart\", \"collector.fspath == p\"),\n            (\"pytest_make_collect_report\", \"collector.fspath == p\"),\n            (\"pytest_pycollect_makeitem\", \"name == 'test_func'\"),\n            (\"pytest_collectreport\", \"report.result[0].name == 'test_func'\"),\n        ]\n    )\n    # ensure we are reporting the collection of the single test item (#2464)\n    assert [x.name for x in self.get_reported_items(hookrec)] == [\"test_func\"]\n", "def test_collect_sub_with_symlinks_multiple_args(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Symlink that gets collected.\n    sub.join(\"test_symlink.py\").mksymlinkto(\"test_file.py\")\n\n    result = testdir.runpytest(\"-v\", str(sub.join(\"test_file.py\")), str(sub.join(\"test_symlink.py\")))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub/test_file.py::test_file PASSED*\",\n            \"sub/test_symlink.py::test_file PASSED*\",\n            \"*2 passed in*\",\n        ]\n    )\n"], "sample_1198": ["def test_parser_mathematica_additional_translations():\n    additional_translations = {\n        'CustomFunction[x]': 'custom_func(x)',\n        'CustomFunction2[x, y]': 'custom_func2(x, y)',\n    }\n    parser = MathematicaParser(additional_translations)\n\n    assert parser.parse('CustomFunction[x]') == sympify('custom_func(x)')\n    assert parser.parse('CustomFunction2[x, y]') == sympify('custom_func2(x, y)')\n\n    # Testing that the original translations are still available\n    assert parser.parse('Sin[x]') == sin(x)\n", "def test_parser_mathematica_complex_expressions():\n    parser = MathematicaParser()\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # Complex expressions\n    assert chain(\"Sin[x] + Cos[y]^2\") == [\"Plus\", [\"Sin\", \"x\"], [\"Power\", [\"Cos\", \"y\"], \"2\"]]\n    assert chain(\"Sin[x]^2 + Cos[y]^2\") == [\"Plus\", [\"Power\", [\"Sin\", \"x\"], \"2\"], [\"Power\", [\"Cos\", \"y\"], \"2\"]]\n    assert chain(\"Sin[x]^2 - Cos[y]^2\") == [\"Plus\", [\"Power\", [\"Sin\", \"x\"], \"2\"], [\"Times\", \"-1\", [\"Power\", [\"Cos\", \"y\"], \"2\"]]]\n    assert chain(\"2*Sin[x]^2 + 3*Cos[y]^2\") == [\"Plus\", [\"Times\", \"2\", [\"Power\", [\"Sin\", \"x\"], \"2\"]], [\"Times\", \"3\", [\"Power\", [\"Cos\", \"y\"], \"2\"]]]\n    assert chain(\"Sin[x]^2*Cos[y]^2\") == [\"Times\", [\"Power\", [\"Sin\", \"x\"], \"2\"], [\"Power\", [\"Cos\", \"y\"], \"2\"]]\n    assert chain(\"Sin[x]^2*Cos[y] + Sin[y]^2*Cos[x]\") == [\"Plus\", [\"Times\", [\"Power\", [\"Sin\", \"x\"], \"2\"], [\"Cos\", \"y\"]], [\"Times\", [\"Power\", [\"Sin\", \"y\"], \"2\"], [\"Cos\", \"x\"]]]\n", "def test_parser_mathematica_derivatives():\n    parser = MathematicaParser()\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # Derivatives\n    assert chain(\"D[f[x], x]\") == [\"Derivative\", \"f\", \"x\"]\n    assert chain(\"D[f[x, y], x]\") == [\"Derivative\", \"f\", \"x\"]\n    assert chain(\"D[f[x, y], y]\") == [\"Derivative\", \"f\", \"y\"]\n    assert chain(\"D[f[x, y], x, y]\") == [\"Derivative\", [\"Derivative\", \"f\", \"x\"], \"y\"]\n    assert chain(\"D[f[x, y], x, x]\") == [\"Derivative\", [\"Derivative\", \"f\", \"x\"], \"x\"]\n    assert chain(\"D[f[x, y], x, y, x]\") == [\"Derivative\", [\"Derivative\", [\"Derivative\", \"f\", \"x\"], \"y\"], \"x\"]\n    assert chain(\"D[f[x, y, z], x, y, z]\") == [\"Derivative\", [\"Derivative\", [\"Derivative\", \"f\", \"x\"], \"y\"], \"z\"]\n    assert chain(\"f'[x]\") == [\"Derivative\", \"f\", \"x\"]\n    assert chain(\"f''[x]\") == [\"Derivative\", [\"Derivative\", \"f\", \"x\"], \"x\"]\n    assert chain(\"f''[x, y]\") == [\"Derivative\", [\"Derivative\", \"f\", \"x\"], \"y\"]\n    assert chain(\"f'''[x]\") == [\"Derivative\", [\"Derivative\", [\"Derivative\", \"f\", \"x\"], \"x\"], \"x\"]\n    assert chain(\"f''''[x, y, z]\") == [\"Derivative\", [\"Derivative\", [\"Derivative\", [\"Derivative\", \"f\", \"x\"], \"y\"], \"z\"], \"z\"]\n", "def test_parser_mathematica_fullformlist():\n    parser = MathematicaParser()\n\n    # Testing the function for converting FullForm to FullFormList\n    assert parser._from_fullform_to_fullformlist(\"List[a,List[b,c],d]\") == [\"List\", \"a\", [\"List\", \"b\", \"c\"], \"d\"]\n    assert parser._from_fullform_to_fullformlist(\"Times[2,Plus[a,b],3]\") == [\"Times\", \"2\", [\"Plus\", \"a\", \"b\"], \"3\"]\n    assert parser._from_fullform_to_fullformlist(\"Power[Sin[x],2]\") == [\"Power\", [\"Sin\", \"x\"], \"2\"]\n\n    # Testing the function for converting FullFormList to SymPy expression\n    assert parser._from_fullformlist_to_sympy([\"Plus\", \"a\", \"b\"]) == a + b\n    assert parser._from_fullformlist_to_sympy([\"Times\", \"2\", [\"Plus\", \"a\", \"b\"], \"3\"]) == 2 * (a + b) * 3\n    assert parser._from_fullformlist_to_sympy([\"Power\", [\"Sin\", \"x\"], \"2\"]) == sin(x)**2\n", "def test_parser_mathematica_complex_expressions():\n    parser = MathematicaParser()\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # Complex expressions\n    assert chain(\"Sin[x]^2 + Cos[x]^2\") == [\"Plus\", [\"Power\", [\"Sin\", \"x\"], \"2\"], [\"Power\", [\"Cos\", \"x\"], \"2\"]]\n    assert chain(\"Sin[x]^2 + Cos[x]^2 == 1\") == [\"Equal\", [\"Plus\", [\"Power\", [\"Sin\", \"x\"], \"2\"], [\"Power\", [\"Cos\", \"x\"], \"2\"]], \"1\"]\n    assert chain(\"Exp[I*Pi] + 1\") == [\"Plus\", [\"Exp\", [\"Times\", \"I\", \"Pi\"]], \"1\"]\n    assert chain(\"Sin[x] == 0 /; x > 0\") == [\"Condition\", [\"Equal\", [\"Sin\", \"x\"], \"0\"], [\"Greater\", \"x\", \"0\"]]\n    assert chain(\"f[x] /; x > 0\") == [\"Condition\", [\"f\", \"x\"], [\"Greater\", \"x\", \"0\"]]\n    assert chain(\"f[x_, y_] := x^2 + y^2\") == [\"SetDelayed\", [\"f\", [\"Pattern\", \"x\", [\"Blank\"]], [\"Pattern\", \"y\", [\"Blank\"]]], [\"Plus\", [\"Power\", \"x\", \"2\"], [\"Power\", \"y\", \"2\"]]]\n    assert chain(\"f[x_] := x^2 /; x > 0\") == [\"SetDelayed\", [\"f\", [\"Pattern\", \"x\", [\"Blank\"]]], [\"Condition\", [\"Power\", \"x\", \"2\"], [\"Greater\", \"x\", \"0\"]]]\n    assert chain(\"Integrate[Sin[x], x]\") == [\"Integrate\", [\"Sin\", \"x\"], \"x\"]\n"], "sample_1017": ["def test_as_Boolean_equality():\n    x = symbols('x')\n    assert as_Boolean(Equality(x, True)) == x\n    assert as_Boolean(Equality(x, False)) == Not(x)\n", "def test_binary_symbols_with_relational():\n    assert And(x > 0, y < 1).binary_symbols == set([x, y])\n    assert Or(x >= 0, y <= 1).binary_symbols == set([x, y])\n    assert Not(x <= 0).binary_symbols == set([x])\n    assert (x > 0).binary_symbols == set([x])\n    assert (x >= 0).binary_symbols == set([x])\n    assert (x < 0).binary_symbols == set([x])\n    assert (x <= 0).binary_symbols == set([x])\n    assert (x == y).binary_symbols == set([x, y])\n    assert (x != y).binary_symbols == set([x, y])\n", "def test_boolean_simplify():\n    x, y, z = symbols('x y z')\n    expr = And(x, Or(y, z))\n    simplified = expr.simplify()\n    assert isinstance(simplified, And)\n    assert len(simplified.args) == 3\n    assert x in simplified.args\n    assert y in simplified.args\n    assert z in simplified.args\n", "def test_issue_15739():\n    assert Not(A & B & C).equals(Or(~A, ~B, ~C)) is True\n    assert Not(A | B | C).equals(And(~A, ~B, ~C)) is True\n    assert Not(A ^ B ^ C).equals(A ^ B ^ C) is True\n    assert Not(A >> B).equals(A & ~B) is True\n    assert Not(A << B).equals(~A | B) is True\n    assert Not(Equivalent(A, B)).equals(A ^ B) is True\n    assert Not(Implies(A, B)).equals(A & ~B) is True\n", "def test_issue_14392():\n    assert Or(And(Eq(x, 1), Eq(y, 2)), And(Eq(x, 3), Eq(y, 4))).simplify() == Or(And(Eq(x, 1), Eq(y, 2)), And(Eq(x, 3), Eq(y, 4)))\n    assert Or(And(Eq(x, 1), Eq(y, 1)), And(Eq(x, 3), Eq(y, 4))).simplify() == Or(And(Eq(x, 1), Eq(y, 1)), And(Eq(x, 3), Eq(y, 4)))\n"], "sample_1015": ["def test_ccode_Float():\n    f32_printer = C99CodePrinter(dict(type_aliases={real: float32}))\n    f64_printer = C99CodePrinter(dict(type_aliases={real: float64}))\n    f80_printer = C99CodePrinter(dict(type_aliases={real: float80}))\n    assert f32_printer.doprint(Float(2.1)) == '2.1F'\n    assert f64_printer.doprint(Float(2.1)) == '2.1000000000000001'\n    assert f80_printer.doprint(Float('2.0')) == '2.0L'\n", "def test_ccode_Float():\n    f32_printer = C99CodePrinter(dict(type_aliases={real: float32}))\n    f64_printer = C99CodePrinter(dict(type_aliases={real: float64}))\n    f80_printer = C99CodePrinter(dict(type_aliases={real: float80}))\n    assert f32_printer.doprint(Float(2.1)) == '2.1F'\n    assert f64_printer.doprint(Float(2.1)) == '2.1'\n    assert f80_printer.doprint(Float(2.1)) == '2.1L'\n", "def test_ccode_Print():\n    assert ccode(Print('test')) == 'printf(\"test\")'\n    assert ccode(Print('test %d', x)) == 'printf(\"test %d\", x)'\n    assert ccode(Print('test %d %d', x, y)) == 'printf(\"test %d %d\", x, y)'\n", "def test_ccode_user_functions_lambda():\n    # Test user_functions with lambda function\n    custom_functions = {\n        \"Abs\": lambda x: \"abs\" if x.is_integer else \"fabs\",\n    }\n    assert ccode(Abs(x), user_functions=custom_functions) == \"fabs(x)\"\n    assert ccode(Abs(n), user_functions=custom_functions) == \"abs(n)\"\n", "def test_ccode_Type_aliasing():\n    assert ccode(real) == 'double'\n    assert ccode(complex_) == 'double complex'\n    assert ccode(integer) == 'int'\n    assert ccode(bool_) == 'bool'\n    assert ccode(int8) == 'int8_t'\n    assert ccode(int16) == 'int16_t'\n    assert ccode(int32) == 'int32_t'\n    assert ccode(int64) == 'int64_t'\n    assert ccode(uint8) == 'int8_t'\n    assert ccode(uint16) == 'int16_t'\n    assert ccode(uint32) == 'int32_t'\n    assert ccode(uint64) == 'int64_t'\n"], "sample_1149": ["def test_singleton_registry():\n    assert S(5) == Rational(5)\n    assert S(\"x**2\") == S.Symbol(\"x\")**2\n", "def test_S_sympify():\n    assert S(5) == 5\n    assert S(\"x\") == Symbol(\"x\")\n    assert S(1)/2 == Rational(1, 2)\n", "def test_singleton_registry():\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert S(1) == Rational(1)\n    assert S(\"1/2\") == Rational(1, 2)\n\n    assert not hasattr(S, \"TestSingleton\")\n    assert S.TestSingleton is TestSingleton()\n    assert hasattr(S, \"TestSingleton\")\n", "def test_singleton_registry():\n    # Test that the singleton registry behaves as expected\n    assert S(1) == S.One\n    assert S(1)/2 == S.Half\n    assert S(\"x\") == S.Symbol(\"x\")\n\n    # Test that sympify is called correctly\n    assert isinstance(S(3.14), Rational)\n    assert S(3.14) == Rational(314, 100)\n", "def test_singleton_registry():\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n\n    # Check if the singleton instance is registered in the registry\n    assert hasattr(S, 'TestSingleton')\n\n    # Check if the registry returns the same instance as the class\n    assert S.TestSingleton is TestSingleton()\n\n    # Check if the registry raises AttributeError for unregistered classes\n    with pytest.raises(AttributeError):\n        S.NonExistentClass\n\n    # Check if the registry overwrites the old instance when a class is registered again\n    old_instance = S.TestSingleton\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n    assert S.TestSingleton is not old_instance\n"], "sample_323": ["def test_unapply_all_replaced_marks_replacement_as_unapplied(self):\n    \"\"\"\n    Unapplying all replaced migrations marks replacement as unapplied (#24628).\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    # Record the replacement migration as applied\n    recorder.record_applied(\"migrations\", \"0001_squashed_0002\")\n    executor = MigrationExecutor(connection)\n    executor.migrate([(\"migrations\", None)])\n\n    # Because we've now unapplied 0001 and 0002 both, their squashed\n    # replacement should be marked as unapplied.\n    self.assertNotIn(\n        (\"migrations\", \"0001_squashed_0002\"),\n        recorder.applied_migrations(),\n    )\n", "def test_minimize_rollbacks_circular(self):\n    r\"\"\"\n    Minimize rollbacks when target has circular in-app children.\n\n    a: 1 <---- 3 <--\\\n              \\ \\- 2 <-/\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a1)\n    graph.add_dependency(None, a2, a3)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n        a3: a3_impl,\n    })\n\n    plan = executor.migration_plan({a1})\n\n    should_be_rolled_back = [a2_impl, a3_impl]\n    exp = [(m, True) for m in should_be_rolled_back]\n    self.assertEqual(plan, exp)\n", "    def test_minimize_rollbacks_unapplied_children(self):\n        \"\"\"\n        Minimize rollbacks for unapplied children in connected apps.\n\n        When you say \"./manage.py migrate appA 0002\", rather than migrating to\n        just after appA-0002 in the linearized migration plan (which could roll\n        back migrations in other apps that depend on appA 0002, but don't need\n        to be rolled back since we're not rolling back appA 0002), we should\n        not roll back any migrations in appA that are not yet applied.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a2})\n\n        self.assertEqual(plan, [(a3_impl, True), (a2_impl, False)])\n", "def test_detect_soft_applied_add_field_charfield(self):\n    \"\"\"\n    executor.detect_soft_applied() detects CharField columns from an AddField operation.\n    \"\"\"\n    tables = [\n        \"migrations_project\",\n    ]\n    executor = MigrationExecutor(connection)\n    # Create the tables for the migration but make it look like the migration hasn't been applied.\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    executor.migrate([(\"migrations\", None)], fake=True)\n    for table in tables:\n        self.assertTableExists(table)\n    # Table detection sees the migration is applied.\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n\n    # Leave the table but remove the column. That missing column should cause detect_soft_applied() to return False.\n    with connection.schema_editor() as editor:\n        editor.execute(editor.sql_delete_column % {\"table\": tables[0], \"column\": \"added_field\"})\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n\n    # Cleanup by removing the table.\n    with connection.schema_editor() as editor:\n        editor.execute(editor.sql_delete_table % {\"table\": tables[0]})\n    for table in tables:\n        self.assertTableNotExists(table)\n", "def test_unapply_migration_without_replaces(self):\n    \"\"\"\n    Test unapply_migration() without replaces attribute.\n    \"\"\"\n    class FakeMigration(migrations.Migration):\n        app_label = 'fake_app'\n        name = '0001_initial'\n        atomic = True\n\n        operations = [\n            migrations.CreateModel('FakeModel', [\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n            ])\n        ]\n\n            pass\n\n    executor = MigrationExecutor(connection)\n    fake_migration = FakeMigration('fake_app', '0001_initial')\n    state = ProjectState()\n\n    executor.unapply_migration(state, fake_migration)\n\n    self.assertNotIn(('fake_app', '0001_initial'), executor.recorder.applied_migrations())\n"], "sample_766": ["def test_sparse_encode_parallel():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    for algo in ('lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'):\n        code = sparse_encode(X, V, algorithm=algo, n_jobs=2)\n        assert_equal(code.shape, (n_samples, n_components))\n", "def test_dict_learning_online_partial_fit_readonly_data():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    V.setflags(write=False)\n    dict2 = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                        n_iter=1, dict_init=V,\n                                        random_state=0)\n    for sample in X:\n        dict2.partial_fit(sample[np.newaxis, :])\n\n    assert not np.all(sparse_encode(X, dict2.components_, alpha=1) == 0)\n", "def test_dict_learning_lars_max_iter_parameter():\n    n_components = 5\n    alpha = 1\n    transform_max_iter = 2\n    err_msg = \"Maximum number of iterations reached\"\n    with pytest.raises(ConvergenceWarning, match=err_msg):\n        dict_learning(X, n_components, alpha, transform_max_iter=transform_max_iter)\n", "def test_sparse_encode_shapes_parallel():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    for algo in ('lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'):\n        code = sparse_encode(X, V, algorithm=algo, n_jobs=2)\n        assert_equal(code.shape, (n_samples, n_components))\n", "def test_dict_learning_online_partial_fit_positivity():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X),\n                                        batch_size=1, alpha=1, shuffle=False,\n                                        dict_init=V, random_state=0,\n                                        positive_dict=True, positive_code=True).fit(X)\n    dict2 = MiniBatchDictionaryLearning(n_components, alpha=1, n_iter=1,\n                                        dict_init=V, random_state=0,\n                                        positive_dict=True, positive_code=True)\n    for i in range(10):\n        for sample in X:\n            dict2.partial_fit(sample[np.newaxis, :])\n\n    assert (dict1.components_ >= 0).all()\n    assert (dict2.components_ >= 0).all()\n"], "sample_886": ["def test__wrap_in_pandas_container_keep_index():\n    \"\"\"Check that _wrap_in_pandas_container keeps the index if input is a DataFrame.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame([[1, 0, 3], [0, 0, 1]], columns=[\"a\", \"b\", \"c\"], index=[10, 12])\n    new_columns = np.asarray([\"f0\", \"f1\", \"f2\"], dtype=object)\n\n    new_df = _wrap_in_pandas_container(X_df, columns=new_columns)\n    assert_array_equal(new_df.columns, new_columns)\n    assert_array_equal(new_df.index, X_df.index)\n", "def test_set_output_index():\n    \"\"\"Check that transform can change the index of the output.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame({\"feat1\": [1, 2, 3], \"feat2\": [3, 4, 5]})\n    est = EstimatorWithSetOutputIndex().fit(X)\n\n    est.set_output(transform=\"pandas\")\n\n    X_trans_pd = est.transform(X)\n    assert isinstance(X_trans_pd, pd.DataFrame)\n    assert list(X_trans_pd.index) == [f\"s{i}\" for i in range(X.shape[0])]\n", "def test_set_output_pandas_container_with_index():\n    \"\"\"Check that set_output works correctly when a new index is provided.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame([[1, 0, 3], [0, 0, 1]], columns=[\"a\", \"b\", \"c\"])\n    est = EstimatorWithSetOutputIndex().fit(X)\n\n    est.set_output(transform=\"pandas\")\n\n    X_trans_pd = est.transform(X)\n    assert isinstance(X_trans_pd, pd.DataFrame)\n    assert_array_equal(X_trans_pd.index, [\"s0\", \"s1\"])\n", "def test_set_output_index():\n    \"\"\"Check that the output index is changed.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame([[1, 0, 3], [0, 0, 1]], index=[\"a\", \"b\"])\n    est = EstimatorWithSetOutputIndex().fit(X)\n\n    est.set_output(transform=\"pandas\")\n\n    X_trans_pd = est.transform(X)\n    assert isinstance(X_trans_pd, pd.DataFrame)\n    assert_array_equal(X_trans_pd.index, [\"s0\", \"s1\"])\n", "def test__wrap_in_pandas_container_with_dataframe():\n    \"\"\"Check _wrap_in_pandas_container with DataFrame input.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame([[1, 0, 3], [0, 0, 1]], columns=[\"a\", \"b\", \"c\"])\n\n    # When DataFrame is input and columns is None, columns should remain the same\n    dense_named = _wrap_in_pandas_container(X_df, columns=None)\n    assert isinstance(dense_named, pd.DataFrame)\n    assert_array_equal(dense_named.columns, X_df.columns)\n\n    # When DataFrame is input and columns is not None, columns should be updated\n    new_columns = np.asarray([\"f0\", \"f1\", \"f2\"], dtype=object)\n    dense_named = _wrap_in_pandas_container(X_df, columns=new_columns)\n    assert_array_equal(dense_named.columns, new_columns)\n"], "sample_557": ["def test_change_layout_engine():\n    fig = plt.figure()\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n", "def test_change_constrained_layout():\n    fig = plt.figure(constrained_layout=True)\n    fig.set_constrained_layout_pads(w_pad=2.0, h_pad=1.5)\n    w_pad, h_pad, _, _ = fig.get_constrained_layout_pads()\n    assert w_pad == 2.0\n    assert h_pad == 1.5\n", "def test_savefig_pil_kwargs(fmt):\n    fig = Figure()\n    fig.text(0.5, 0.5, \"Test\", fontsize=30)\n    buffer = io.BytesIO()\n    fig.savefig(buffer, format=fmt, pil_kwargs={\"optimize\": True, \"quality\": 85})\n    buffer.seek(0)\n    img = Image.open(buffer)\n    img.verify()\n", "def test_subfigure_with_colorbar():\n    fig = plt.figure(layout='constrained')\n    subfigs = fig.subfigures(1, 2)\n    axs = subfigs[0].subplots(1, 1)\n    im = axs.imshow(np.random.randn(10, 10))\n    subfigs[0].colorbar(im)\n    assert len(subfigs[0].axes) == 2  # 1 axes + 1 colorbar\n    assert len(subfigs[1].axes) == 1  # No colorbar\n", "def test_figure_pickle():\n    fig = Figure(figsize=(4, 4), dpi=100)\n    fig.add_subplot(111).plot([1, 2, 3], [4, 5, 6])\n    fig.suptitle('Test Figure')\n\n    pickled_fig = pickle.dumps(fig)\n    unpickled_fig = pickle.loads(pickled_fig)\n\n    assert fig.get_figwidth() == unpickled_fig.get_figwidth()\n    assert fig.get_figheight() == unpickled_fig.get_figheight()\n    assert fig.dpi == unpickled_fig.dpi\n    assert fig.axes[0].lines[0].get_xdata().tolist() == unpickled_fig.axes[0].lines[0].get_xdata().tolist()\n    assert fig.axes[0].lines[0].get_ydata().tolist() == unpickled_fig.axes[0].lines[0].get_ydata().tolist()\n    assert fig.suptitle.get_text() == unpickled_fig.suptitle.get_text()\n"], "sample_1146": ["def test_latex_fraction():\n    x = Symbol('x')\n    assert latex(Rational(1, 2)) == r'\\frac{1}{2}'\n    assert latex(Rational(1, x)) == r'\\frac{1}{x}'\n    assert latex(Rational(x, 2)) == r'\\frac{x}{2}'\n    assert latex(Rational(x, y)) == r'\\frac{x}{y}'\n", "def test_latex_precedence():\n    # Test that the precedence is correctly handled in latex printing\n    expr1 = (x + y) * z\n    expr2 = x * z + y * z\n    assert latex(expr1) == r\"z \\left(x + y\\right)\"\n    assert latex(expr2) == r\"x z + y z\"\n", "def test_latex_functions():\n    x = Symbol('x')\n    assert latex(sin(x)) == r'\\sin{\\left(x \\right)}'\n    assert latex(cos(x)) == r'\\cos{\\left(x \\right)}'\n    assert latex(tan(x)) == r'\\tan{\\left(x \\right)}'\n    assert latex(log(x)) == r'\\log{\\left(x \\right)}'\n    assert latex(Ei(x)) == r'\\operatorname{Ei}{\\left(x \\right)}'\n    assert latex(zeta(x)) == r'\\zeta{\\left(x \\right)}'\n    assert latex(Chi(x)) == r'\\operatorname{Chi}{\\left(x \\right)}'\n    assert latex(beta(x)) == r'\\operatorname{B}{\\left(x \\right)}'\n    assert latex(gamma(x)) == r'\\Gamma{\\left(x \\right)}'\n    assert latex(lowergamma(x, y)) == r'\\gamma{\\left(x, y \\right)}'\n    assert latex(KroneckerDelta(x, y)) == r'\\delta_{x y}'\n    assert latex(DiracDelta(x)) == r'\\delta\\left(x\\right)'\n", "def test_issue_18441():\n    # Test case for issue #18441\n    x = Symbol('x')\n    y = Symbol('y')\n    expr = Derivative(x**y, x)\n    assert latex(expr) == r\"\\frac{d}{d x} x^{y}\"\n", "def test_latex_as_content():\n    from sympy import latex\n    from sympy.printing.latex import LatexPrinter\n\n    printer = LatexPrinter()\n    content = printer.doprint(2*x + 3*y)\n    assert latex(content) == r\"2 x + 3 y\"\n    assert latex(content, as_content=False) == r\"$2 x + 3 y$\"\n"], "sample_358": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'],\n            lambda column: column.upper(),\n            col_suffixes=['ASC', 'DESC'],\n            opclasses=['text_pattern_ops', 'varchar_pattern_ops']\n        )\n", "    def setUp(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.editor = connection.schema_editor()\n        self.index_columns = IndexColumns(\n            table=Person._meta.db_table,\n            columns=['first_name', 'last_name'],\n            quote_name=self.editor.quote_name,\n            col_suffixes=['ASC', 'DESC'],\n            opclasses=['varchar_pattern_ops', 'text_pattern_ops'],\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'],\n            lambda column: column.upper(),\n            ['ASC', 'DESC'],\n            ['REGCLASS', 'TEXT_PATTERN_OP']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            opclasses=['varchar_ops', 'int_ops']\n        )\n", "    def setUp(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.editor = connection.schema_editor()\n        self.expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                Cast(F('age'), output_field=IntegerField()),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n"], "sample_69": ["def test_non_existing_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.nonexistent_file)\n    with self.tick_twice():\n        self.increment_mtime(self.nonexistent_file)\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_directory_glob_recursive_with_nonexistent_base(self, mocked_modules, notify_mock):\n    non_existent_dir = self.tempdir / 'nonexistent'\n    py_file = non_existent_dir / 'dir' / 'file.py'\n    self.reloader.watch_dir(non_existent_dir, '**/*.py')\n    with self.tick_twice():\n        self.ensure_file(py_file)\n        self.increment_mtime(py_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [py_file])\n", "def test_nonexistent_glob(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.nonexistent')\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_snapshot_files_duplicate_paths(self):\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.existing_file]):\n        snapshot1 = list(self.reloader.snapshot_files())\n        self.assertEqual(len(snapshot1), 1)\n        self.increment_mtime(self.existing_file)\n        snapshot2 = list(self.reloader.snapshot_files())\n        self.assertEqual(len(snapshot2), 1)\n        self.assertNotEqual(snapshot1[0][1], snapshot2[0][1])\n", "def test_check_subscription_handles_missing_file(self):\n    # Arrange\n    result = {'subscription': 'glob:/tmp/test', 'files': ['test.py']}\n    self.reloader.client.getSubscription = mock.MagicMock(return_value=[result])\n    self.reloader.notify_file_changed = mock.MagicMock()\n\n    # Act\n    self.reloader._check_subscription('glob:/tmp/test')\n\n    # Assert\n    self.reloader.notify_file_changed.assert_called_once_with(Path('/tmp/test/test.py'))\n"], "sample_515": ["def test_colorbar_labels():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    im = ax.contourf(data, levels=levels)\n    cbar = fig.colorbar(im, orientation='horizontal')\n    cbar.set_label('Custom Label')\n    assert cbar.ax.get_xlabel() == 'Custom Label'\n", "def test_colorbar_extend_alpha_overunder():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]], alpha=0.3, interpolation=\"none\")\n    fig.colorbar(im, extend='both', boundaries=[-0.5, 0.5, 1.5, 2.5], over=2.6, under=0)\n", "def test_colorbar_inverted_axis():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    im = ax.contourf(data, levels=levels)\n    cb = fig.colorbar(im, ax=ax, orientation='vertical')\n    cb.ax.invert_yaxis()\n    fig.canvas.draw()\n    # Add assertions to check the y-axis inversion\n", "def test_colorbar_with_subplotspec():\n    # Test the colorbar with subplotspec and various orientations and locations\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n\n    # Vertical colorbar on the right\n    ax1 = fig.add_subplot(gs[0, 0])\n    cb_ax1 = fig.add_subplot(gs[0, 1])\n    im1 = ax1.imshow(np.arange(100).reshape((10, 10)))\n    fig.colorbar(im1, cax=cb_ax1)\n\n    # Horizontal colorbar at the bottom\n    ax2 = fig.add_subplot(gs[1, 0])\n    cb_ax2 = fig.add_subplot(gs[1, :])\n    im2 = ax2.imshow(np.arange(100).reshape((10, 10)))\n    fig.colorbar(im2, cax=cb_ax2, orientation='horizontal')\n\n    # Vertical colorbar on the left\n    ax3 = fig.add_subplot(gs[:, 0])\n    cb_ax3 = fig.add_subplot(gs[:, 1])\n    im3 = ax3.imshow(np.arange(100).reshape((10, 10)))\n    fig.colorbar(im3, cax=cb_ax3, location='left')\n", "def test_colorbar_update_normal():\n    # Test the update_normal function in the Colorbar class\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    im = ax.imshow(data)\n    cb = fig.colorbar(im)\n\n    # Change the data and norm of the mappable\n    new_data = np.random.rand(10, 10)\n    new_norm = mcolors.PowerNorm(gamma=0.5)\n    im.set_array(new_data)\n    im.set_norm(new_norm)\n    cb.update_normal(im)\n\n    # Check that the norm of the colorbar is updated\n    assert cb.norm == new_norm\n    # Check that the boundaries of the colorbar are updated\n    np.testing.assert_array_almost_equal(cb._boundaries, new_norm(np.linspace(0, 1, new_data.size)))\n"], "sample_510": ["def test_subplot_mosaic():\n    fig = plt.figure()\n    ax_dict = fig.subplot_mosaic(\n        [['A panel', 'A panel', 'edge'],\n         ['C panel', '.',       'edge']]\n    )\n    assert len(ax_dict) == 6\n    assert isinstance(ax_dict['A panel'], plt.Axes)\n    assert isinstance(ax_dict['C panel'], plt.Axes)\n    assert isinstance(ax_dict['edge'], plt.Axes)\n    assert isinstance(ax_dict['.'], plt.Axes)\n", "def test_subplot_sharex_sharey():\n    fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n    assert ax1.get_shared_x_axes().joined(ax1, ax2)\n    assert not ax1.get_shared_y_axes().joined(ax1, ax2)\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n    assert ax1.get_shared_y_axes().joined(ax1, ax2)\n    assert not ax1.get_shared_x_axes().joined(ax1, ax2)\n\n    fig, ax = plt.subplots(sharex='col', sharey='row')\n    assert isinstance(ax, np.ndarray)\n    assert ax[0, 0].get_shared_x_axes().joined(ax[0, 0], ax[0, 1])\n    assert ax[0, 0].get_shared_y_axes().joined(ax[0, 0], ax[1, 0])\n    assert not ax[0, 0].get_shared_x_axes().joined(ax[0, 0], ax[1, 0])\n    assert not ax[0, 0].get_shared_y_axes().joined(ax[0, 0], ax[0, 1])\n", "def test_subplot_mosaic_dict():\n    # Test subplot_mosaic with a dictionary input\n    fig, ax_dict = plt.subplot_mosaic({'top': 'A', 'bottom': ['B', 'C']})\n    assert isinstance(ax_dict, dict)\n    assert 'top' in ax_dict\n    assert 'bottom' in ax_dict\n    assert isinstance(ax_dict['top'], plt.Axes)\n    assert isinstance(ax_dict['bottom'], np.ndarray)\n    assert ax_dict['bottom'].shape == (2,)\n    plt.close(fig)\n", "def test_subplot_mosaic():\n    # Check that subplot_mosaic returns the correct number of axes\n    fig, ax_dict = plt.subplot_mosaic({'a': [0, 0], 'b': [0, 1], 'c': [1, 0], 'd': [1, 1]})\n    assert len(ax_dict) == 4\n\n    # Check that subplot_mosaic returns the correct axes positions\n    assert ax_dict['a'].get_position().bounds == (0.0, 0.5, 0.5, 0.5)\n    assert ax_dict['b'].get_position().bounds == (0.5, 0.5, 0.5, 0.5)\n    assert ax_dict['c'].get_position().bounds == (0.0, 0.0, 0.5, 0.5)\n    assert ax_dict['d'].get_position().bounds == (0.5, 0.0, 0.5, 0.5)\n\n    # Check that subplot_mosaic raises a ValueError when the input is invalid\n    with pytest.raises(ValueError):\n        plt.subplot_mosaic('a b c d')\n", "def test_subplot_mosaic():\n    fig, ax_dict = plt.subplot_mosaic({'a': [0, 0], 'b': [0, 1]})\n    assert isinstance(ax_dict['a'], plt.Axes)\n    assert isinstance(ax_dict['b'], plt.Axes)\n"], "sample_341": ["def test_can_delete_initial_formset_forms(self):\n    ChoiceFormFormset = formset_factory(form=Choice, can_delete=True)\n    formset = ChoiceFormFormset(initial=[{'choice': 'Zero', 'votes': '1'}])\n    self.assertEqual(len(formset), 1)\n    self.assertIn('DELETE', formset.forms[0].fields)\n\n    formset = ChoiceFormFormset(data={\n        'form-0-choice': 'Zero',\n        'form-0-votes': '1',\n        'form-0-DELETE': 'on',\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '1',\n    }, initial=[{'choice': 'Zero', 'votes': '1'}])\n    self.assertEqual(formset.cleaned_data, [])\n    self.assertIs(formset._should_delete_form(formset.forms[0]), True)\n", "def test_formset_with_ordering_deletion_and_empty_forms(self):\n    \"\"\"FormSets with ordering, deletion, and empty forms.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True, extra=2)\n    initial = [\n        {'choice': 'Calexico', 'votes': 100},\n        {'choice': 'Fergie', 'votes': 900},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    # Let's delete Fergie and leave the last form empty.\n    data = {\n        'choices-TOTAL_FORMS': '3',\n        'choices-INITIAL_FORMS': '2',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-0-ORDER': '1',\n        'choices-0-DELETE': '',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-1-ORDER': '2',\n        'choices-1-DELETE': 'on',\n        'choices-2-choice': '',\n        'choices-2-votes': '',\n        'choices-2-ORDER': '',\n        'choices-2-DELETE': '',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.ordered_forms],\n        [\n            {'votes': 100, 'DELETE': False, 'ORDER': 1, 'choice': 'Calexico'},\n        ],\n    )\n    self.assertEqual(\n        [form.cleaned_data for form in formset.deleted_forms],\n        [", "def test_min_num_validation(self):\n    data = {\n        'choices-TOTAL_FORMS': '1',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n    }\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at least 2 forms.'])\n", "    def test_formset_with_minimum_forms(self):\n        \"\"\"\n        FormSet should raise an error if the minimum number of forms\n        are not submitted.\n        \"\"\"\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '0',\n            'form-MIN_NUM_FORMS': '3',  # minimum number of forms\n        }\n        FavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, min_num=3, validate_min=True)\n        formset = FavoriteDrinkFormSet(data, prefix='form')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Please submit at least 3 forms.'])\n", "    def test_custom_error_messages(self):\n        class CustomChoice(Choice):\n            error_messages = {\n                'missing_choice': 'This field is required (custom message).',\n                'missing_votes': 'This field is required (custom message).',\n            }\n\n        CustomChoiceFormSet = formset_factory(CustomChoice)\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '0',\n        }\n        formset = CustomChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{'choice': ['This field is required (custom message).'],\n                                           'votes': ['This field is required (custom message).']}])\n"], "sample_223": ["    def test_ticket_24605_extra(self):\n        \"\"\"\n        Subquery table names should be quoted with extra select.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.extra(select={'is_alive': 'alive'}).filter(\n                Q(is_alive=False), Q(related_individual__isnull=True)\n            ),\n            [i4]\n        )\n        self.assertSequenceEqual(\n            Individual.objects.extra(select={'is_alive': 'alive'}).exclude(\n                Q(is_alive=False), Q(related_individual__isnull=True)\n            ).order_by('pk'),\n            [i1, i2, i3]\n        )\n", "    def test_ticket_24605_additional(self):\n        \"\"\"\n        Additional test for subquery table names should be quoted.\n        \"\"\"\n        i5 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i5)\n\n        self.assertSequenceEqual(\n            Individual.objects.exclude(Q(alive=True), Q(related_individual__isnull=True)).order_by('pk'),\n            []\n        )\n", "def test_in_query_with_related_name(self):\n    apple = Food.objects.create(name=\"apple\")\n    pear = Food.objects.create(name=\"pear\")\n    lunch_apple = Eaten.objects.create(food=apple, meal=\"lunch\")\n    lunch_pear = Eaten.objects.create(food=pear, meal=\"dinner\")\n\n    self.assertEqual(\n        set(Food.objects.filter(eaten_meals__in=[lunch_apple, lunch_pear])),\n        {apple, pear}\n    )\n", "    def test_ticket_24709(self):\n        \"\"\"\n        Subquery table names should be quoted.\n        \"\"\"\n        p1 = Parent.objects.create(name='p1')\n        c1 = Child.objects.create(name='c1', parent=p1)\n        c2 = Child.objects.create(name='c2', parent=p1)\n\n        self.assertSequenceEqual(Parent.objects.filter(child__name__in=Child.objects.values('name')), [p1])\n        self.assertSequenceEqual(Parent.objects.exclude(child__name__in=Child.objects.values('name')), [])\n", "    def test_additional_subquery_table_names(self):\n        \"\"\"\n        Test additional cases for subquery table names being quoted.\n        \"\"\"\n        p1 = Paragraph.objects.create()\n        p2 = Paragraph.objects.create()\n        c1 = Chapter.objects.create(paragraph=p1)\n        c2 = Chapter.objects.create(paragraph=p2)\n        b1 = Book.objects.create(chapter=c1)\n        b2 = Book.objects.create(chapter=c2)\n\n        self.assertSequenceEqual(\n            Book.objects.filter(Q(chapter__paragraph__page__isnull=True), Q(chapter__paragraph=p1)),\n            [b1]\n        )\n        self.assertSequenceEqual(\n            Book.objects.exclude(Q(chapter__paragraph__page__isnull=True), Q(chapter__paragraph=p2)).order_by('pk'),\n            [b1]\n        )\n"], "sample_1162": ["def test_MatMul_kind():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatMul(A, B).kind is MatrixKind(NumberKind)\n", "def test_MatMul_kind():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatMul(A, B).kind is MatrixKind(NumberKind)\n", "def test_MatMul_kind():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatMul(A, B).kind is MatrixKind(NumberKind)\n", "def test_MatMul_kind():\n    A = MatrixSymbol('A', 2,2)\n    B = MatrixSymbol('B', 2,2)\n    assert MatMul(A, B).kind is MatrixKind(NumberKind)\n", "def test_MatMul_kind():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatMul(A, B).kind is MatrixKind(NumberKind)\n"], "sample_146": ["def test_consistent_language_settings(self):\n    for tag in self.valid_tags:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag, LANGUAGES=[(tag, tag)]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    for tag in self.valid_tags:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    for tag in ['en', 'en-US', 'en-CA']:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        for tag in self.valid_tags:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag, LANGUAGES=[(tag, tag)]):\n                self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_291": ["def test_resolve_redirect_view(self):\n    match = resolve('/redirect/')\n    self.assertIs(match.func.view_class, RedirectView)\n    self.assertEqual(match.func.view_initkwargs['url'], '/destination/')\n", "def test_direct_instantiation_template_view(self):\n    \"\"\"\n    It should be possible to use the TemplateView by directly instantiating it\n    without going through .as_view() (#21564).\n    \"\"\"\n    view = TemplateView()\n    view.template_name = 'generic_views/about.html'\n    response = view.dispatch(self.rf.get('/'))\n    self._assert_about(response)\n", "def test_resolve_redirect_view(self):\n    match = resolve('/redirect/')\n    self.assertIs(match.func.view_class, RedirectView)\n    self.assertEqual(match.func.view_initkwargs['url'], '/new_url/')\n", "def test_redirect_DELETE_with_url(self):\n    \"\"\"\n    DELETE redirects to a specified URL.\n    \"\"\"\n    response = RedirectView.as_view(url='/bar/')(self.rf.delete('/foo/'))\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, '/bar/')\n", "def test_get_template_names_required(self):\n    \"\"\"\n    A template view using a response class that requires a template name\n    will raise an exception if the template name is not provided.\n    \"\"\"\n    class CustomTemplateResponse(TemplateResponse):\n            if not kwargs.get('template'):\n                raise ValueError(\"TemplateResponse requires a template name.\")\n            super().__init__(*args, **kwargs)\n\n    class CustomTemplateView(TemplateView):\n        response_class = CustomTemplateResponse\n\n    msg = \"TemplateResponse requires a template name.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        CustomTemplateView.as_view()(self.rf.get('/'))\n"], "sample_754": ["def test_scaling_invariance():\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)\n    spca_lars = SparsePCA(n_components=3, method='lars', alpha=1, random_state=rng, normalize_components=True)\n    results_original = spca_lars.fit_transform(Y)\n    Y_scaled = Y * 10\n    results_scaled = spca_lars.fit_transform(Y_scaled)\n    assert_allclose(results_original, results_scaled)\n", "def test_sparse_pca_with_different_alphas(norm_comp):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array\n    spca_lars_low_alpha = SparsePCA(n_components=3, method='lars', alpha=0.1,\n                                    random_state=rng, normalize_components=norm_comp)\n    spca_lars_high_alpha = SparsePCA(n_components=3, method='lars', alpha=10,\n                                     random_state=rng, normalize_components=norm_comp)\n    spca_lars_low_alpha.fit(Y)\n    spca_lars_high_alpha.fit(Y)\n    # Check that with higher alpha, components become sparser\n    assert np.count_nonzero(spca_lars_high_alpha.components_) < np.count_nonzero(spca_lars_low_alpha.components_)\n", "def test_spca_with_different_alphas(norm_comp):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    spca_alpha1 = SparsePCA(n_components=3, method='lars', alpha=1, random_state=0, normalize_components=norm_comp)\n    spca_alpha2 = SparsePCA(n_components=3, method='lars', alpha=2, random_state=0, normalize_components=norm_comp)\n    spca_alpha1.fit(Y)\n    spca_alpha2.fit(Y)\n    assert not np.allclose(spca_alpha1.components_, spca_alpha2.components_)\n", "def test_spca_with_zero_variance_feature():\n    X = np.zeros((100, 10))\n    X[:, 0] = 1.0  # all zeros except for one feature\n\n    spca = SparsePCA(n_components=2, random_state=0)\n    spca.fit(X)\n\n    # The first component should be the constant feature\n    assert_array_almost_equal(spca.components_[0], np.ones(10))\n\n    # The second component should be a sparse vector with zeros everywhere except for the first feature\n    assert_array_almost_equal(spca.components_[1, 1:], np.zeros(9))\n    assert_true(spca.components_[1, 0] != 0)\n", "def test_input_type(norm_comp):\n    # Test that the input data type is valid\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng, normalize_components=norm_comp)\n\n    # Test with valid input type (numpy array)\n    spca.fit(X)\n\n    # Test with invalid input type (list)\n    with pytest.raises(ValueError):\n        spca.fit(X.tolist())\n"], "sample_29": ["def test_write_latex_valid_format(self, write, tmp_path):\n    \"\"\"Test passing an unsupported format\"\"\"\n    fp = tmp_path / \"test_write_latex_valid_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex', not\"):\n        write(fp, format=\"csv\")\n", "def test_write_latex_custom_kwargs(self, write, tmp_path):\n    \"\"\"Test passing custom kwargs to write_latex\"\"\"\n    fp = tmp_path / \"test_write_latex_custom_kwargs.tex\"\n    write(fp, format=\"latex\", longtable=True)\n    tbl = QTable.read(fp)\n    # asserts that the table was written using the custom kwargs\n    assert \"longtable\" in str(tbl)\n", "def test_latex_output_format(self, write, tmp_path):\n    fp = tmp_path / \"test_latex_output_format.tex\"\n    write(fp, format=\"latex\")\n    with open(fp, \"r\") as f:\n        content = f.read()\n        assert \"\\\\documentclass{article}\" in content\n        assert \"\\\\begin{document}\" in content\n        assert \"\\\\end{document}\" in content\n        assert \"\\\\begin{tabular}\" in content\n        assert \"\\\\end{tabular}\" in content\n", "def test_write_latex_different_format(self, write, tmp_path):\n    \"\"\"Test to write a LaTeX file with a different format specified\"\"\"\n    fp = tmp_path / \"test_write_latex_different_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex', not csv\"):\n        write(fp, format=\"csv\")\n", "def test_write_latex_with_kwargs(self, write, tmp_path):\n    \"\"\"Test writing a LaTeX file with additional keyword arguments\"\"\"\n    fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n    write(fp, format=\"latex\", overwrite=True, latex_names=False, caption=\"Test Cosmology Parameters\")\n    tbl = QTable.read(fp)\n    # Check if caption is added correctly in the LaTeX file\n    with open(fp, 'r') as file:\n        data = file.read().replace('\\n', '')\n        assert \"\\\\caption{Test Cosmology Parameters}\" in data\n"], "sample_799": ["def test_score_with_scorer_callable():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n    X_train, y_train = X[train], y[train]\n    X_test, y_test = X[test], y[test]\n\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    expected_score = accuracy_score(y_test, y_pred)\n\n        y_pred = estimator.predict(X)\n        return accuracy_score(y, y_pred)\n\n    score = _score(clf, X_test, y_test, scorer)\n    assert_equal(score, expected_score)\n", "def test_score_with_large_number():\n    # Test _score function with a scorer that returns a large number\n        return 1e100\n\n    fit_and_score_args = [None, None, None, large_scorer]\n    score = _score(*fit_and_score_args)\n    assert score == 1e100\n", "def test_score_with_multimetric_scorer():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n\n        return {\"score1\": 0.8, \"score2\": 0.9}\n\n    fit_and_score_args = [clf, X, y, multimetric_scorer, train, test, 0, None, None]\n    result = _score(*fit_and_score_args)\n    assert result == {\"score1\": 0.8, \"score2\": 0.9}\n\n    # Test with incorrectly formatted multimetric scorer\n        return [(\"score1\", 0.8), (\"score2\", 0.9)]\n\n    fit_and_score_args = [clf, X, y, incorrect_multimetric_scorer, train, test, 0, None, None]\n    with pytest.raises(ValueError):\n        _score(*fit_and_score_args)\n", "def test_score_with_scorer_callable():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n\n    # test score with scorer as a callable\n    scorer = make_scorer(accuracy_score)\n    fit_and_score_args = [clf, X[train], y[train], scorer, X[test], y[test]]\n    score = _score(*fit_and_score_args)\n    assert isinstance(score, float)\n\n    # test score with scorer as a string\n    scorer = 'accuracy'\n    fit_and_score_args = [clf, X[train], y[train], scorer, X[test], y[test]]\n    score = _score(*fit_and_score_args)\n    assert isinstance(score, float)\n\n    # test error message for invalid scorer\n    scorer = 'invalid_scorer'\n    fit_and_score_args = [clf, X[train], y[train], scorer, X[test], y[test]]\n    error_message = \"Unknown scoring: invalid_scorer\"\n    assert_raise_message(ValueError, error_message, _score, *fit_and_score_args)\n", "def test_fit_and_score_multimetric():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n    scorer = {'accuracy': make_scorer(accuracy_score),\n              'precision': make_scorer(precision_score, average='micro')}\n    fit_and_score_args = [clf, X, y, scorer, train, test, 0]\n    fit_and_score_kwargs = {'return_train_score': True}\n    result = _fit_and_score(*fit_and_score_args, **fit_and_score_kwargs)\n    assert isinstance(result[-1], dict)\n"], "sample_114": ["def test_add_blank_null_textfield_and_charfield(self):\n    \"\"\"\n    #23405 - Adding a NULL and blank `CharField` or `TextField`\n    without default should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_biography_blank_null])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_add_date_fields_with_auto_now_add_not_asking_for_default(self, mocked_ask_method):\n    changes = self.get_changes([self.author_empty], [self.author_dates_of_birth_auto_now_add])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\", \"AddField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now_add=True)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 1, auto_now_add=True)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 2, auto_now_add=True)\n", "def test_rename_model_with_renamed_rel_field_circular(self):\n    \"\"\"\n    Tests autodetection of renamed models while simultaneously renaming one\n    of the fields that relate to the renamed model in a circular dependency.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_book, self.book],\n        [self.author_renamed_with_book, self.book_with_field_and_author_renamed],\n        MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n    # Right number/type of migrations for related field rename?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"RenameField\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, old_name=\"author\", new_name=\"writer\")\n", "def test_alter_foreign_key(self):\n    \"\"\"Tests altering a ForeignKey.\"\"\"\n    changes = self.get_changes([self.author_with_publisher], [self.author_with_publisher_null])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"publisher\")\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, null=True)\n", "def test_alter_field_to_textfield(self):\n    changes = self.get_changes([self.author_empty], [self.author_with_biography_textfield])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"biography\")\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, max_length=None)\n"], "sample_804": ["def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 2, 55], ['def', 1, 55], ['def', 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1], [1, 0], [1, 1]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 1, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1], [1, 0], [1, 1]]\n    assert_array_equal(trans, exp)\n    assert_array_equal(enc.drop_idx_, np.array([0, 1]))\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 0], [1, 0, 1], [1, 0, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature] for cat, feature in zip(enc.categories_, enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 2, 55])\n    assert_array_equal(np.array(X, dtype=object), enc.inverse_transform(trans))\n", "def test_one_hot_encoder_specified_categories_drop_first(X, X2, cats, cat_dtype):\n    enc = OneHotEncoder(categories=cats, drop='first')\n    exp = np.array([[0., 1.],\n                    [0., 0.],\n                    [0., 0.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n    assert list(enc.categories[0]) == list(cats[0])\n    assert enc.categories_[0].tolist() == list(cats[0])\n    # manually specified categories should have same dtype as\n    # the data when coerced from lists\n    assert enc.categories_[0].dtype == cat_dtype\n\n    # when specifying categories manually, unknown categories should already\n    # raise when fitting\n    enc = OneHotEncoder(categories=cats, handle_unknown='ignore', drop='first')\n    exp = np.array([[0., 1.],\n                    [0., 0.],\n                    [0., 0.]])\n    assert_array_equal(enc.fit(X2).transform(X2", "def test_one_hot_encoder_sparse_output(sparse):\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OneHotEncoder(sparse=sparse)\n    X_tr = enc.fit_transform(X)\n    if sparse:\n        assert sparse.isspmatrix_csr(X_tr)\n    else:\n        assert isinstance(X_tr, np.ndarray)\n"], "sample_509": ["def test_date_ticker_factory_numticks():\n    span = 1\n    numticks = 3\n    locator, _ = mdates.date_ticker_factory(span, numticks=numticks)\n    assert len(locator()) == numticks\n", "def test_date_ticker_factory_numticks():\n    locator, _ = mdates.date_ticker_factory(365, numticks=3)\n    assert isinstance(locator, mdates.MonthLocator)\n", "def test_date_ticker_factory_interval_multiples():\n    locator, _ = mdates.date_ticker_factory(365, interval_multiples=False)\n    assert isinstance(locator, mdates.DayLocator)\n    assert locator.interval == 1\n\n    locator, _ = mdates.date_ticker_factory(365, interval_multiples=True)\n    assert isinstance(locator, mdates.DayLocator)\n    assert locator.interval != 1\n", "def test_microsecond_locator():\n    locator = mdates.MicrosecondLocator(interval=1000)\n    locator.create_dummy_axis()\n    locator.axis.set_data_interval(*mdates.date2num([\"2022-01-10 12:00:00.001000\", \"2022-01-10 12:00:00.003000\"]))\n    assert locator.tick_values(mdates.date2num(\"2022-01-10 12:00:00.001000\"), mdates.date2num(\"2022-01-10 12:00:00.003000\")) == [19002.0000005, 19002.000001]\n", "def test_microsecondlocator():\n    locator = mdates.MicrosecondLocator(interval=1000)\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(mdates.date2num(datetime.datetime(2022, 1, 1)),\n                                   mdates.date2num(datetime.datetime(2022, 1, 2)))\n    t = np.array([738368.0, 738368.000001, 738368.000002])\n    np.testing.assert_equal(locator(), t)\n    locator.set_view_interval(mdates.date2num(datetime.datetime(1970, 1, 1)),\n                              mdates.date2num(datetime.datetime(1970, 1, 2)))\n    with pytest.warns(UserWarning, match=\"Plotting microsecond time intervals\"):\n        locator()\n"], "sample_872": ["def test_top_k_accuracy_score_sparse_input():\n    # Test that top_k_accuracy_score accepts sparse y_true.\n    y_true = csr_matrix([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n    result = top_k_accuracy_score(y_true, y_score, k=1)\n    assert result == pytest.approx(0.5)\n", "def test_label_ranking_avg_precision_score_should_accept_y_true_y_score_in_csr_format():\n    # Test that label_ranking_avg_precision_score accept sparse y_true and y_score.\n    y_true = csr_matrix([[1, 0, 0], [0, 0, 1]])\n    y_score = csr_matrix([[0.5, 0.9, 0.6], [0, 0, 1]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(2 / 3)\n", "def test_top_k_accuracy_score_multilabel():\n    # Test when labels and y_score are multilabel.\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\n    y_score = np.array(\n        [\n            [0.8, 0.2, 0.9],\n            [0.1, 0.9, 0.2],\n            [0.7, 0.2, 0.8],\n            [0.3, 0.8, 0.1],\n        ]\n    )\n    true_score = 0.75\n\n    score = top_k_accuracy_score(y_true, y_score, k=2)\n    assert score == pytest.approx(true_score)\n", "def test_top_k_accuracy_score_multilabel():\n    \"\"\"Test when labels and y_score are multilabel.\"\"\"\n    y_true = np.array([[1, 0, 0], [0, 1, 1], [1, 1, 0], [0, 0, 1]])\n    y_score = np.array(\n        [\n            [0.4, 0.3, 0.2],\n            [0.1, 0.5, 0.4],\n            [0.4, 0.6, 0.2],\n            [0.3, 0.2, 0.5],\n        ]\n    )\n    labels = [0, 1, 2]\n\n    true_score = 0.5\n    score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n    assert score == pytest.approx(true_score)\n", "def test_label_ranking_avg_precision_score_should_accept_list_of_list_for_y_true_input():\n    # Test that label_ranking_avg_precision_score accept list of list for y_true.\n    y_true = [[1, 0, 0], [0, 0, 1]]\n    y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(2 / 3)\n"], "sample_1079": ["def test_projection():\n    a = Point(1, 2)\n    b = Point(2, 5)\n    z = a.origin\n    p = Point.project(a, b)\n    assert Line(p, a).is_perpendicular(Line(p, b))\n    assert Point.is_collinear(z, p, b)\n", "def test_affine_rank():\n    assert Point.affine_rank() == -1\n    assert Point.affine_rank(Point(1, 2)) == 0\n    assert Point.affine_rank(Point(1, 2), Point(3, 4)) == 1\n    assert Point.affine_rank(Point(1, 2), Point(3, 4), Point(5, 6)) == 2\n    assert Point.affine_rank(Point(1, 2), Point(3, 4), Point(5, 6), Point(7, 8)) == 2\n    assert Point.affine_rank(Point(1, 2, 3)) == 0\n    assert Point.affine_rank(Point(1, 2, 3), Point(4, 5, 6)) == 1\n    assert Point.affine_rank(Point(1, 2, 3), Point(4, 5, 6), Point(7, 8, 9)) == 2\n    assert Point.affine_rank(Point(1, 2, 3), Point(4, 5, 6), Point(7, 8, 9), Point(10, 11, 12)) == 3\n", "def test_canberra_distance():\n    p1 = Point(1, 1)\n    p2 = Point(3, 3)\n    p3 = Point(0, 0)\n    p4 = Point(3, 0)\n    assert p1.canberra_distance(p2) == 1\n    assert p3.canberra_distance(p4) == 2\n    raises(ValueError, lambda: Point(0, 0).canberra_distance(Point(0, 0)))\n", "def test_affine_rank():\n    p1 = Point(1, 2)\n    p2 = Point(2, 4)\n    p3 = Point(3, 6)\n    assert Point.affine_rank(p1, p2) == 1\n    assert Point.affine_rank(p1, p2, p3) == 1\n    p4 = Point(2, 5)\n    assert Point.affine_rank(p1, p2, p4) == 2\n    p5 = Point(1, 2, 3)\n    p6 = Point(2, 4, 6)\n    assert Point.affine_rank(p1, p2, p5, p6) == 2\n    assert Point.affine_rank() == -1\n", "def test_project():\n    p1 = Point(3, 4)\n    p2 = Point(5, 12)\n    assert Point.project(p1, p2) == Point(5*13/26, 12*13/26)\n    assert Point.project(p2, p1) == Point(3*25/25, 4*25/25)\n\n    # Test project raises ValueError when projecting to the zero vector\n    p3 = Point(0, 0)\n    raises(ValueError, lambda: Point.project(p1, p3))\n"], "sample_1194": ["def test_julia_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_julia_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert julia_code(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n           \"existing_julia_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])\"\n", "def test_julia_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"mat_fcn_g\"),\n              (lambda x: not x.is_Matrix, \"scalar_fcn_g\")]\n    }\n    mat = Matrix([[1, x]])\n    expected = \"custom_fcn_f(x) + scalar_fcn_g(x) + mat_fcn_g([1 x])\"\n    assert julia_code(f(x) + g(x) + g(mat), user_functions=custom_functions) == expected\n", "def test_julia_HadamardPower():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = HadamardProduct(A, B)\n    assert julia_code(C**x) == \"(A .* B) .^ x\"\n", "def test_julia_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_julia_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert julia_code(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n           'existing_julia_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_julia_hadamard_power():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert julia_code(A**B) == \"A .^ B\"\n    assert julia_code(A**0.5) == \"A .^ 0.5\"\n    assert julia_code(A**x) == \"A .^ x\"\n    assert julia_code(A**(x*y)) == \"A .^ (x .* y)\"\n    assert julia_code(A**(B*x)) == \"A .^ (B * x)\"\n"], "sample_176": ["    def test_mti_inheritance_model_addition(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([Animal], [Animal, ModelState('app', 'Dog', [], bases=('app.Animal',))])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "    def test_mti_inheritance_field_addition(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, ModelState('app', 'Dog', [('breed', models.CharField(max_length=100))], bases=('app.Animal',))])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='breed', model_name='dog')\n", "    def test_m2m_w_through_multistep_add(self):\n        \"\"\"\n        A model with a m2m field that specifies a \"through\" model cannot be\n        added in the same migration as that through model as the schema will\n        pass through an inconsistent state. The autodetector should produce two\n        migrations to avoid this issue.\n        \"\"\"\n        changes = self.get_changes([], [self.author_with_m2m_through, self.publisher])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"CreateModel\", \"AddField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Publisher\")\n        self.assertOperationAttributes(changes, \"testapp\", 1, 0, name=\"Contract\")\n        self.assertOperationAttributes(changes, \"testapp\", 1, 1, model_name=\"author\", name=\"publishers\")\n", "def test_add_non_required_textfield(self):\n    \"\"\"\n    #23405 - Adding a non-required `TextField` should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_bio])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"bio\")\n", "def test_remove_fk_and_mti_model(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"animal_ptr\", models.OneToOneField(Animal, on_delete=models.CASCADE, parent_link=True, primary_key=True)),\n        (\"breed\", models.CharField(max_length=200)),\n    ], bases=('app.Animal',))\n    Owner = ModelState('app', 'Owner', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"pet\", models.ForeignKey(\"app.Dog\", models.CASCADE)),\n    ])\n    changes = self.get_changes([Animal, Dog, Owner], [Animal])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'DeleteModel', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='pet', model_name='owner')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n    self.assertOperationAttributes(changes, 'app', 0, 2, name='Owner')\n"], "sample_805": ["def test_regression_multioutput_uniform_average():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n\n    mse = mean_squared_error(y_true, y_pred, multioutput='uniform_average')\n    mae = mean_absolute_error(y_true, y_pred, multioutput='uniform_average')\n    r = r2_score(y_true, y_pred, multioutput='uniform_average')\n    evs = explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n\n    assert_almost_equal(mse, 0.5125, decimal=2)\n    assert_almost_equal(mae, 0.4375, decimal=2)\n    assert_almost_equal(r, 0.94, decimal=2)\n    assert_almost_equal(evs, 0.94, decimal=2)\n", "def test_regression_multioutput_invalid_multioutput():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n    invalid_multioutput = 'invalid_value'\n    expected_message = (\"Allowed 'multioutput' string values are.+\"\n                        \"You provided multioutput={!r}\".format(\n                            invalid_multioutput))\n    assert_raises_regex(ValueError, expected_message,\n                        mean_squared_error, y_true, y_pred,\n                        multioutput=invalid_multioutput)\n\n    assert_raises_regex(ValueError, expected_message,\n                        mean_absolute_error, y_true, y_pred,\n                        multioutput=invalid_multioutput)\n\n    assert_raises_regex(ValueError, expected_message,\n                        r2_score, y_true, y_pred,\n                        multioutput=invalid_multioutput)\n\n    assert_raises_regex(ValueError, expected_message,\n                        explained_variance_score, y_true, y_pred,\n                        multioutput=invalid_multioutput)\n\n    assert_raises_regex(ValueError, expected_message,\n                        mean_squared_log_error, y_true, y_pred,\n                        multioutput=invalid_multioutput)\n", "def test_regression_multioutput_uniform_average():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n\n    mse = mean_squared_error(y_true, y_pred, multioutput='uniform_average')\n    mae = mean_absolute_error(y_true, y_pred, multioutput='uniform_average')\n    r = r2_score(y_true, y_pred, multioutput='uniform_average')\n    evs = explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n\n    assert_almost_equal(mse, 0.7225, decimal=2)\n    assert_almost_equal(mae, 0.4375, decimal=2)\n    assert_almost_equal(r, 0.94, decimal=2)\n    assert_almost_equal(evs, 0.94, decimal=2)\n\n    # Testing for uniform_average with mean_squared_log_error\n    y_true = np.array([[0.5, 1], [1, 2], [7, 6]])\n    y_pred = np.array([[0.5, 2], [1, 2.5], [8, 8]])\n    msle = mean_squared_log_error(y_true, y_pred, multioutput='uniform_average')\n    msle2 = mean_squared_error(np.log(1 + y_true), np.log(1 + y_pred),\n                               multioutput='uniform_average')\n    assert_almost_equal(msle, msle2, decimal=2)\n", "def test_regression_multioutput_invalid_input():\n    y_true = [[1, 2], [2.5, -1]]\n    y_pred = [[1, 1], [2]]  # Invalid input: different number of outputs\n    with pytest.raises(ValueError, match=\"y_true and y_pred have different number of output\"):\n        _check_reg_targets(y_true, y_pred, None)\n", "def test_regression_metrics_with_sample_weight():\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([1, 2, 3, 5])\n    sample_weight = np.array([0.5, 0.3, 0.2, 1])\n\n    mse = mean_squared_error(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(mse, 0.11, decimal=2)\n\n    mae = mean_absolute_error(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(mae, 0.2, decimal=2)\n\n    r2 = r2_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(r2, 0.982, decimal=3)\n\n    evs = explained_variance_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(evs, 0.982, decimal=3)\n"], "sample_885": ["def test_real_not_int_constraint():\n    \"\"\"Check that the 'real_not_int' type constraint works as expected.\"\"\"\n    constraint = Interval('real_not_int', 0, 1, closed=\"both\")\n    assert constraint.is_satisfied_by(0.5)\n    assert not constraint.is_satisfied_by(0)\n    assert not constraint.is_satisfied_by(1)\n\n    # check that the string representation is as expected\n    assert str(constraint) == \"a float in the range [0, 1]\"\n", "def test_interval_errors_real_not_int():\n    \"\"\"Check that an informative error is raised when an invalid value is passed for the 'type' parameter\"\"\"\n    with pytest.raises(ValueError, match=\"type must be either numbers.Integral, numbers.Real or 'real_not_int'\"):\n        Interval(\"not a valid type\", 0, 1, closed=\"both\")\n", "def test_real_not_int_interval():\n    \"\"\"Check that the \"real_not_int\" interval constraint accepts real numbers that are not integers.\"\"\"\n    constraint = Interval(\"real_not_int\", 0, 1, closed=\"both\")\n    assert constraint.is_satisfied_by(0.5)\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(0)\n    assert not constraint.is_satisfied_by(1)\n", "def test_interval_errors_real_not_int():\n    \"\"\"Check that the 'real_not_int' type is allowed for intervals.\"\"\"\n    # Test that no error is raised when 'real_not_int' is used as the type\n    interval = Interval(\"real_not_int\", 0, 1, closed=\"left\")\n    assert interval.is_satisfied_by(0.5)\n    assert not interval.is_satisfied_by(1)\n", "def test_real_not_int_constraint():\n    \"\"\"Check that the \"real_not_int\" type is valid for real numbers excluding integers.\"\"\"\n    constraint = Interval(\"real_not_int\", -1.5, 1.5, closed=\"both\")\n    assert constraint.is_satisfied_by(-1.0)\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(0)\n    assert not constraint.is_satisfied_by(2.0)\n    assert not constraint.is_satisfied_by(\"not a number\")\n"], "sample_168": ["    def test_include_stale_apps_true(self):\n        \"\"\"\n        --include-stale-apps option deletes stale content types even if the\n        app is not in INSTALLED_APPS.\n        \"\"\"\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n        self.assertIn(\"Deleting stale content type\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_contenttypes_removed_for_uninstalled_app(self):\n    \"\"\"ContentTypes are removed for an uninstalled app.\"\"\"\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', verbosity=2)\n    self.assertIn(\n        \"Deleting stale content type 'contenttypes_tests | Fake'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_include_stale_apps_true(self):\n    \"\"\"\n    With --include-stale-apps option, stale content types from uninstalled apps are deleted.\n    \"\"\"\n    ContentType.objects.create(app_label='empty_models', model='Fake 1')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n    self.assertIn(\n        \"Deleting stale content type 'empty_models | Fake 1'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_include_stale_apps_option(self):\n    \"\"\"The --include-stale-apps option deletes content types from uninstalled apps.\"\"\"\n    ContentType.objects.create(app_label='no_models', model='Fake')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n    self.assertIn(\n        \"Deleting stale content type 'no_models | Fake'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_include_stale_apps_flag(self):\n    \"\"\"The --include-stale-apps flag deletes stale content types in uninstalled apps.\"\"\"\n    ContentType.objects.create(app_label='empty_models', model='Fake 1')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n    self.assertIn(\n        \"Deleting stale content type 'empty_models | Fake 1'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n"], "sample_348": ["    def test_custom_permissions_has_method(self):\n        @admin.action(permissions=['custom'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_custom_permissions_defined(self):\n        @admin.action(permissions=['custom'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_unique_with_different_attributes(self):\n        @admin.action\n            pass\n\n        @admin.action(permissions=['custom'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_unique_with_different_modules(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        class SongAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsValid(BandAdmin, Band)\n        self.assertIsValid(SongAdmin, Song)\n", "def test_custom_permissions_not_string(self):\n    @admin.action(permissions=None)\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action,)\n\n    self.assertIsInvalid(\n        BandAdmin, Band,\n        'The \"permissions\" attribute of the action \"action\" must be a list or a tuple.',\n        id='admin.E131',\n    )\n"], "sample_377": ["    def test_view(self, request):\n        return HttpResponse()\n", "    def test_non_str_in_multivalue_dict(self):\n        request = self.rf.post(\"/some_url/\", {\"non_str\": 123})\n        request.sensitive_post_parameters = [\"non_str\"]\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_post = reporter_filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleansed_post[\"non_str\"], reporter_filter.cleansed_substitute)\n", "    def test_sensitive_variables_with_args(self):\n        @sensitive_variables(\"password\")\n            pass\n\n        self.assertEqual(test_func.__wrapped__.__name__, \"test_func\")\n        self.assertEqual(test_func.sensitive_variables, {\"password\"})\n", "    def setUp(self):\n        self.rf.cookies.load({\n            \"sensitive-cookie\": \"sensitive-value\",\n            \"safe-cookie\": \"safe-value\",\n        })\n", "def test_sensitive_post_parameters_view(self):\n    request = self.rf.post(\"/some_url/\", self.breakfast_data)\n    request.sensitive_post_parameters = [\"sausage-key\", \"bacon-key\"]\n\n    with self.settings(DEBUG=True):\n        response = sensitive_view(request)\n        # Sensitive POST parameters are not shown.\n        self.assertNotContains(response, \"sausage-value\", status_code=500)\n        self.assertNotContains(response, \"bacon-value\", status_code=500)\n\n    with self.settings(DEBUG=False):\n        response = sensitive_view(request)\n        # Sensitive POST parameters are replaced with stars.\n        self.assertContains(response, \"sausage-key\", status_code=500)\n        self.assertContains(response, \"***\", status_code=500)\n        self.assertContains(response, \"bacon-key\", status_code=500)\n        self.assertContains(response, \"***\", status_code=500)\n"], "sample_1043": ["def test_Exp():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(exp(2*x)) == \"Exp[2*x]\"\n    assert mcode(exp(x)*exp(y)) == \"Exp[x]*Exp[y]\"\n", "def test_Log():\n    assert mcode(log(x)) == \"Hold[Log[x]]\"\n    assert mcode(log(x, y)) == \"Hold[Log[x, y]]\"\n", "def test_user_functions():\n        return 2*x + y\n    settings = {'user_functions': {'user_func': 'CustomFunction'}}\n    assert mcode(user_func(x, y), **settings) == \"CustomFunction[x, y]\"\n", "def test_Exp():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(exp(x) + exp(y)) == \"Exp[x] + Exp[y]\"\n    assert mcode(exp(x*y)) == \"Exp[x*y]\"\n", "def test_InverseTrigFunctions():\n    assert mcode(sin(x).asin()) == \"ArcSin[Sin[x]]\"\n    assert mcode(cos(x).acos()) == \"ArcCos[Cos[x]]\"\n    assert mcode(tan(x).atan()) == \"ArcTan[Tan[x]]\"\n    assert mcode(sinh(x).asinh()) == \"ArcSinh[Sinh[x]]\"\n    assert mcode(cosh(x).acosh()) == \"ArcCosh[Cosh[x]]\"\n    assert mcode(tanh(x).atanh()) == \"ArcTanh[Tanh[x]]\"\n"], "sample_1123": ["def test_CondSet_as_relational():\n    input_conditionset = ConditionSet(x, x**2 > 4, Interval(1, 4, False, False))\n    assert input_conditionset.as_relational(5) == S.true\n    assert input_conditionset.as_relational(2) == S.false\n    assert input_conditionset.as_relational(w) == And(w**2 > 4, And(w >= 1, w < 4))\n", "def test_CondSet_as_relational():\n    c = ConditionSet(x, x > 5, Interval(1, 7))\n    assert c.as_relational(6) == And(6 > 5, Contains(6, Interval(1, 7)))\n    assert c.as_relational(8) == And(8 > 5, Contains(8, Interval(1, 7)))\n    assert c.as_relational(w) == And(w > 5, Contains(w, Interval(1, 7)))\n", "def test_CondSet_union():\n    set1 = ConditionSet(x, x > 0, Interval(0, 5))\n    set2 = ConditionSet(x, x < 3, Interval(2, 6))\n    union_set = Union(set1, set2)\n    assert union_set == Interval(0, 6)\n    assert 4 in union_set\n    assert -1 not in union_set\n", "def test_CondSet_contains_unsolved_equations():\n    c = ConditionSet((x, y), Eq(x + y, 0), S.Reals)\n    assert c.contains((-1, 1))\n    assert c.contains((2, -2)) is S.false\n    assert c.contains((x, y)) == Eq(x + y, 0)\n", "def test_CondSet_contains_expr():\n    assert ConditionSet(x, x**2 > 4, S.Reals).contains(x + 2) == And(x + 2 > 2, x + 2 < -2)\n    assert ConditionSet(x, x**2 > 4, S.Reals).contains(2*x) == And(2*x > 2, 2*x < -2)\n    assert ConditionSet(x, x**2 > 4, S.Reals).contains(x**2) == x**2 > 4\n    assert ConditionSet(x, x**2 > 4, S.Reals).contains(sin(x)) == And(sin(x) > 2, sin(x) < -2)\n"], "sample_1107": ["def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_has_variety():\n    assert has_variety([1, 2, 1]) is True\n    assert has_variety([1, 1, 1]) is False\n", "def test_permutations():\n    assert list(permutations([])) == [()]\n    assert list(permutations([1])) == [(1,)]\n    assert list(permutations([1, 2])) == [(1, 2), (2, 1)]\n    assert list(permutations([1, 2, 3])) == [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n", "def test_permutations():\n    assert list(permutations([])) == [()]\n    assert list(permutations([1])) == [(1,)]\n    assert list(permutations([1, 2])) == [(1, 2), (2, 1)]\n    assert list(permutations([1, 2, 3])) == [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n    assert len(list(permutations(range(5)))) == factorial(5)\n", "def test_permutations():\n    assert list(permutations([1, 2, 3], 0)) == [()]\n    assert list(permutations([1, 2, 3], 1)) == [(1,), (2,), (3,)]\n    assert list(permutations([1, 2, 3], 2)) == [(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)]\n    assert list(permutations([1, 2, 3], 3)) == [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n    l = list(range(4))\n    assert list(permutations(l, 0)) == [()]\n    assert list(permutations(l, 1)) == [(0,), (1,), (2,), (3,)]\n    assert len(list(permutations(l, 2))) == 12\n    assert len(list(permutations(l, 3))) == 24\n    assert len(list(permutations(l, 4))) == 24\n\n    assert list(permutations(l[:2], 3)) == []\n    assert list(permutations(l[:2], 3, repetition=True)) == [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1),\n                                                             (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]\n    assert list(permutations([1, 2], repetition=True)) == [(1, 1), (1, 2), (2, 1), (2, 2)]\n    assert list(permutations([1, 2], repetition=False)) == [(1, 2), ("], "sample_129": ["def test_floatformat03(self):\n    output = self.engine.render_to_string('floatformat03', {\"a\": \"13.1031\"})\n    self.assertEqual(output, \"13.103\")\n", "def test_floatformat03(self):\n    with localcontext(Decimal) as ctx:\n        ctx.prec = 2\n        output = self.engine.render_to_string('floatformat03', {\"a\": Decimal('1.42')})\n        self.assertEqual(output, \"1.42\")\n", "def test_floatformat03(self):\n    with localcontext() as ctx:\n        ctx.prec = 2\n        self.assertEqual(floatformat(Decimal('1.2345')), '1.23')\n", "    def test_large_numbers(self):\n        self.assertEqual(floatformat(1234567890.123456789, 6), '1234567890.123457')\n        self.assertEqual(floatformat(1234567890123456789.123456789, 6), '1234567890123456789.123457')\n        self.assertEqual(floatformat(12345678901234567890.123456789, 6), '12345678901234567890.123457')\n        self.assertEqual(floatformat(123456789012345678901.123456789, 6), '123456789012345678901.123457')\n", "    def test_decimal_values(self):\n        self.assertEqual(floatformat(Decimal('1.5'), 1), '1.5')\n        self.assertEqual(floatformat(Decimal('1.50000'), 1), '1.5')\n        self.assertEqual(floatformat(Decimal('1.5000000001'), 1), '1.5')\n        self.assertEqual(floatformat(Decimal('1.5'), -1), '2')\n        self.assertEqual(floatformat(Decimal('1.50000'), -1), '2')\n        self.assertEqual(floatformat(Decimal('1.5000000001'), -1), '2')\n        self.assertEqual(floatformat(Decimal('1.0000000001'), 10), '1.0000000001')\n"], "sample_189": ["def test_cache_control_no_store(self):\n    \"\"\"Responses with 'Cache-Control: no-store' are not cached.\"\"\"\n    view_with_no_store_cache = cache_page(3)(cache_control(no_store=True)(hello_world_view))\n    request = self.factory.get('/view/')\n    response = view_with_no_store_cache(request, '1')\n    self.assertEqual(response.content, b'Hello World 1')\n    response = view_with_no_store_cache(request, '2')\n    self.assertEqual(response.content, b'Hello World 2')\n", "    def test_get_with_version(self):\n        cache.set('key', 'value', version=2)\n        self.assertIsNone(cache.get('key'))\n        self.assertEqual(cache.get('key', version=2), 'value')\n", "def test_invalid_backend(self):\n    # Attempting to retrieve an invalid backend should raise an error.\n    with self.assertRaises(InvalidCacheBackendError):\n        caches['invalid_backend']\n", "def test_cache_get_or_set_with_timeout(self):\n    \"\"\"Test that cache.get_or_set() respects the timeout argument.\"\"\"\n    key = 'test_key'\n    value = 'test_value'\n    timeout = 1  # 1 second\n    self.assertIsNone(cache.get(key))\n    cache.get_or_set(key, value, timeout)\n    self.assertEqual(cache.get(key), value)\n    time.sleep(timeout + 0.1)  # Wait for the timeout to expire\n    self.assertIsNone(cache.get(key))\n", "def test_get_or_set_timeout(self):\n    \"\"\"get_or_set() honors the timeout parameter.\"\"\"\n    cache.set('key1', 'value1', timeout=2)\n    self.assertEqual(cache.get_or_set('key1', 'value2', timeout=2), 'value1')\n    time.sleep(3)\n    self.assertEqual(cache.get_or_set('key1', 'value2', timeout=2), 'value2')\n"], "sample_136": ["    def test_httprequest_headers(self):\n        request = HttpRequest()\n        request.META = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'HTTP_USER_AGENT': 'python-requests/1.2.0',\n        }\n        self.assertEqual(dict(request.headers), {\n            'Content-Type': 'text/html',\n            'Content-Length': '100',\n            'Host': 'example.com',\n            'User-Agent': 'python-requests/1.2.0',\n        })\n", "def test_set_encoding_clears_GET_invalid_encoding(self):\n    request = WSGIRequest({\n        'REQUEST_METHOD': 'GET',\n        'wsgi.input': '',\n        'QUERY_STRING': 'name=Hello%20G%C3%BCnter',\n    })\n    self.assertEqual(request.GET, {'name': ['Hello G\u00fcnter']})\n    request.encoding = 'invalid-encoding'\n    self.assertEqual(request.GET, {})\n", "def test_wsgirequest_path_with_non_ascii_chars(self):\n    \"\"\"\n    The request's path is correctly assembled, regardless of whether or not\n    the PATH_INFO contains non-ASCII characters (#20951).\n    \"\"\"\n    # With non-ASCII characters\n    request = WSGIRequest({\n        'PATH_INFO': '/path/with/\u00e9/',\n        'REQUEST_METHOD': 'get',\n        'wsgi.input': BytesIO(b''),\n    })\n    self.assertEqual(request.path, '/path/with/\u00e9/')\n", "def test_wsgirequest_https(self):\n    # Test WSGIRequest with HTTPS\n    request = WSGIRequest({\n        'HTTPS': 'on',\n        'REQUEST_METHOD': 'get',\n        'wsgi.input': BytesIO(b''),\n        'SERVER_NAME': 'internal.com',\n        'SERVER_PORT': 443,\n    })\n    self.assertTrue(request.is_secure())\n    self.assertEqual(request.scheme, 'https')\n", "def test_wsgirequest_content_type_with_invalid_charset(self):\n    \"\"\"\n    Test a POST with a charset parameter that isn't valid.\n    \"\"\"\n    payload = FakePayload(urlencode({'key': 'Espa\u00f1a'.encode('latin-1')}))\n    request = WSGIRequest({\n        'REQUEST_METHOD': 'POST',\n        'CONTENT_LENGTH': len(payload),\n        'CONTENT_TYPE': 'application/x-www-form-urlencoded; charset=invalid-charset',\n        'wsgi.input': payload,\n    })\n    with self.assertRaises(LookupError):\n        request.POST\n"], "sample_447": ["def test_annotation_with_aggregate_filter(self):\n    qs = Book.objects.annotate(\n        total_pages=Sum(\"pages\"),\n    ).filter(total_pages__gt=1000)\n    self.assertCountEqual(qs, [self.b3, self.b4])\n", "def test_chaining_alias_annotation(self):\n    qs = (\n        Publisher.objects.alias(multiplier=Value(3))\n        .annotate(multiplied_value_sum=Sum(F(\"multiplier\") * F(\"num_awards\")))\n        .order_by()\n    )\n    self.assertCountEqual(\n        qs,\n        [\n            {\"multiplied_value_sum\": 9, \"name\": \"Apress\"},\n            {\"multiplied_value_sum\": 0, \"name\": \"Jonno's House of Books\"},\n            {\"multiplied_value_sum\": 27, \"name\": \"Morgan Kaufmann\"},\n            {\"multiplied_value_sum\": 21, \"name\": \"Prentice Hall\"},\n            {\"multiplied_value_sum\": 3, \"name\": \"Sams\"},\n        ],\n    )\n", "def test_annotation_with_foreign_key(self):\n    qs = Book.objects.alias(\n        contact_name=F(\"contact__name\"),\n    ).annotate(\n        contact_first_name=F(\"contact_name\").split()[0],\n    )\n    book = qs.get(pk=self.b1.pk)\n    self.assertIs(hasattr(book, \"contact_name\"), False)\n    self.assertEqual(book.contact_first_name, \"Adrian\")\n", "def test_alias_with_m2m(self):\n    qs = Book.objects.alias(\n        author_age=F(\"authors__age\"),\n    ).filter(pk=self.b1.pk).order_by(\"author_age\")\n    self.assertIs(hasattr(qs.first(), \"author_age\"), False)\n    self.assertSequenceEqual(\n        qs,\n        [self.b1, self.b1],\n        lambda b: b.authors.values_list(\"age\", flat=True),\n    )\n", "def test_annotation_with_m2m_and_filter(self):\n    books = (\n        Book.objects.annotate(author_age=F(\"authors__age\"))\n        .filter(authors__age__gt=30)\n        .filter(pk=self.b1.pk)\n        .order_by(\"author_age\")\n    )\n    self.assertEqual(books[0].author_age, 35)\n"], "sample_80": ["def test_related_isnull(self):\n    query = Query(Author)\n    where = query.build_where(Q(item__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.field, Author._meta.get_field('item'))\n", "def test_related_isnull(self):\n    query = Query(Author)\n    where = query.build_where(Q(books__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.field.related_name, 'books')\n", "def test_related_isnull(self):\n    query = Query(ObjectC)\n    where = query.build_where(Q(parent__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.target, ObjectC._meta.get_field('parent'))\n", "def test_related_isnull(self):\n    query = Query(ObjectC)\n    where = query.build_where(~Q(related__isnull=True))\n    self.assertTrue(where.negated)\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.field.name, 'related')\n", "def test_related_isnull(self):\n    query = Query(Author)\n    where = query.build_where(Q(books__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('books'))\n"], "sample_257": ["def test_key_transform_nested_in(self):\n    obj = NullableJSONModel.objects.create(value={'a': {'b': [1, 2, 3]}})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__b__in=[2, 3]),\n        [obj],\n    )\n", "def test_key_transform_lookup_with_raw_sql(self):\n    raw_sql = self.raw_sql % \"'{\\\"x\\\": \\\"bar\\\"}'\"\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=RawSQL(raw_sql)),\n        [self.objs[7]],\n    )\n", "    def test_contains_with_none_value(self):\n        self.assertIs(NullableJSONModel.objects.filter(value__j=None).exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__contains={'j': None}).exists(), True)\n", "def test_key_transform_gt(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__gt=4),\n        [self.objs[4]],\n    )\n", "def test_contains_with_key_transform(self):\n    tests = [\n        ('value__baz__contains', 'd'),\n        ('value__baz__contains', {'a': 'b'}),\n        ('value__contains', KeyTransform('bar', 'value')),\n        ('value__contains', F('value__bar')),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertIs(NullableJSONModel.objects.filter(\n                **{lookup: value},\n            ).exists(), True)\n"], "sample_1030": ["def test_closest_and_farthest_points():\n    points = [Point2D(0, 0), Point2D(1, 0), Point2D(1, 1), Point2D(0, 1)]\n    assert closest_points(*points) == {(Point2D(0, 0), Point2D(1, 0)), (Point2D(0, 0), Point2D(0, 1)), (Point2D(1, 0), Point2D(1, 1)), (Point2D(0, 1), Point2D(1, 1))}\n    assert farthest_points(*points) == {(Point2D(0, 0), Point2D(1, 1))}\n", "def test_closest_and_farthest_points():\n    points = [(1, 1), (4, 2), (3, 6), (-2, -3), (0, 0)]\n    assert closest_points(*points) == {(Point2D(0, 0), Point2D(1, 1))}\n    assert farthest_points(*points) == {(Point2D(-2, -3), Point2D(3, 6))}\n", "def test_closest_points():\n    points = [Point2D(1, 1), Point2D(1, 2), Point2D(3, 1), Point2D(-5, 2), Point2D(15, 4)]\n    assert closest_points(*points) == {(Point2D(1, 1), Point2D(1, 2))}\n", "def test_closest_points():\n    points = [Point2D(1, 1), Point2D(4, 1), Point2D(2, 2), Point2D(5, 4)]\n    assert closest_points(*points) == {(Point2D(1, 1), Point2D(2, 2))}\n", "def test_closest_points():\n    points = [(0, 0), (1, 1), (2, 2), (3, 3)]\n    assert closest_points(*points) == {(Point2D(0, 0), Point2D(1, 1))}\n    points = [(0, 0), (1, 1), (2, 3), (3, 3)]\n    assert closest_points(*points) == {(Point2D(1, 1), Point2D(2, 3))}\n"], "sample_895": ["def test_column_transformer_fit_transform_then_transform(remainder):\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])], remainder=remainder)\n    X_fit_transform = ct.fit_transform(X_array)\n    X_transform = ct.transform(X_array)\n    assert_array_equal(X_fit_transform, X_transform)\n", "def test_column_transformer_empty_selection_array():\n    \"\"\"Check that column transformer handles empty selection with array input.\"\"\"\n    X = np.array([[1.0, 2.2], [3.0, 1.0]])\n    ct = ColumnTransformer(\n        [\n            (\"categorical\", \"passthrough\", []),\n            (\"numerical\", StandardScaler(), [0, 1]),\n        ]\n    )\n    X_out = ct.fit_transform(X)\n    assert_array_equal(X_out, StandardScaler().fit_transform(X))\n", "def test_column_transformer_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # Test transformer_weights with single transformer\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])], transformer_weights={\"trans\": 0.1})\n    exp = 0.1 * X_array\n    assert_array_equal(ct.fit_transform(X_array), exp)\n    assert_array_equal(ct.fit(X_array).transform(X_array), exp)\n\n    # Test transformer_weights with multiple transformers\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])],\n        transformer_weights={\"trans1\": 0.1, \"trans2\": 10},\n    )\n    exp = np.vstack([0.1 * X_array[0], 10 * X_array[1]])\n    assert_array_equal(ct.fit_transform(X_array), exp)\n    assert_array_equal(ct.fit(X_array).transform(X_array), exp)\n", "def test_column_transformer_with_list_of_features():\n    # Test case where the input to ColumnTransformer is a list of features\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_both = X_array\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0])])\n    assert_array_equal(ct.fit_transform(X_array.tolist()), X_res_first)\n    assert_array_equal(ct.fit(X_array.tolist()).transform(X_array.tolist()), X_res_first)\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array.tolist()), X_res_both)\n    assert_array_equal(ct.fit(X_array.tolist()).transform(X_array.tolist()), X_res_both)\n", "def test_column_transformer_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_res_both = X_array.copy()\n\n    # second and third columns are doubled when remainder = DoubleTrans\n    X_res_both[:, 1:3] *= 2\n\n    transformer_weights = {\"trans1\": 1, \"remainder\": 2}\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0])], remainder=DoubleTrans(), transformer_weights=transformer_weights)\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n"], "sample_199": ["def test_annotation_with_boolean_expression(self):\n    books = Book.objects.annotate(\n        is_high_rated=ExpressionWrapper(Q(rating__gt=4), output_field=BooleanField())\n    )\n    for book in books:\n        self.assertEqual(book.is_high_rated, book.rating > 4)\n", "def test_annotation_with_duration_field(self):\n    from django.db.models import DurationField\n    duration = datetime.timedelta(days=5, hours=12, minutes=30)\n    Ticket.objects.create(duration=duration)\n    ticket = Ticket.objects.annotate(\n        double_duration=ExpressionWrapper(F('duration') * 2, output_field=DurationField())\n    ).first()\n    self.assertEqual(ticket.double_duration, duration * 2)\n", "def test_annotation_with_subquery(self):\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).order_by().values('pages')[:1]\n    publisher_books_qs = Publisher.objects.annotate(\n        first_book_page=Subquery(subquery, output_field=IntegerField()),\n    ).values('name', 'first_book_page')\n    expected_books = [\n        {'name': 'Apress', 'first_book_page': 447},\n        {'name': 'Sams', 'first_book_page': 528},\n        {'name': 'Prentice Hall', 'first_book_page': 350},\n        {'name': 'Morgan Kaufmann', 'first_book_page': 946},\n        {'name': \"Jonno's House of Books\", 'first_book_page': None},\n    ]\n    self.assertCountEqual(publisher_books_qs, expected_books)\n", "def test_annotation_filter_with_subquery_and_aggregate(self):\n    long_books_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n    ).values('publisher').annotate(count=Count('pk')).values('count')\n    publisher_books_qs = Publisher.objects.annotate(\n        total_books=Count('book'),\n        long_books_count=Subquery(long_books_qs, output_field=IntegerField()),\n    ).filter(\n        total_books__gt=Subquery(long_books_qs, output_field=IntegerField()),\n    ).values('name')\n    self.assertCountEqual(publisher_books_qs, [{'name': 'Apress'}, {'name': 'Prentice Hall'}])\n", "def test_annotation_filter_with_case_when(self):\n    from_authors = Author.objects.annotate(\n        has_many_books=Case(\n            When(book__count__gt=1, then=True),\n            default=False,\n            output_field=BooleanField(),\n        )\n    ).filter(has_many_books=True).values('name')\n    self.assertCountEqual(from_authors, [{'name': 'Jeffrey Forcier'}, {'name': 'Paul Bissex'}])\n"], "sample_292": ["def test_https_good_referer_matches_cookie_domain_with_trailing_slash(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted from a\n    subdomain that's allowed by SESSION_COOKIE_DOMAIN with a trailing slash.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'https://foo.example.com/'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_https_good_referer_matches_cookie_domain_with_subdirectory(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted from a\n    subdomain that's allowed by CSRF_COOKIE_DOMAIN and a subdirectory.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'https://foo.example.com/subdirectory/'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_https_good_origin_csrf_trusted_origin_allowed(self):\n    \"\"\"\n    A POST request with an origin that matches a CSRF_TRUSTED_ORIGINS\n    wildcard is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://foo.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n    self.assertEqual(mw.allowed_origins_exact, set())\n    self.assertEqual(mw.allowed_origin_subdomains, {'https': ['.example.com']})\n", "def test_https_good_referer_no_port(self):\n    \"\"\"\n    A POST HTTPS request with a good referer without a port is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_REFERER'] = 'https://www.example.com'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n", "def test_process_view_token_too_short(self):\n    \"\"\"\n    If the token is shorter than expected, it is ignored and a new token is\n    created.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    req.COOKIES[settings.CSRF_COOKIE_NAME] = 'x' * 31\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n    self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n"], "sample_460": ["def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n    user = User.objects.create_user(\n        username=\"user\",\n        password=\"secret\",\n        email=\"user@example.com\",\n        is_staff=False,\n    )\n    self.client.force_login(user)\n    known_url = reverse(\"admin10:admin_views_article_changelist\")\n    response = self.client.get(known_url[:-1])\n    self.assertRedirects(\n        response,\n        \"/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article\",\n    )\n", "def test_unknown_url_no_trailing_slash_if_auth_without_final_catch_all_view(self):\n    superuser = User.objects.create_superuser(\n        username=\"super\",\n        password=\"secret\",\n        email=\"super@example.com\",\n    )\n    self.client.force_login(superuser)\n    url = reverse(\"admin10:article_extra_json\")[:-1]\n    response = self.client.get(url)\n    self.assertEqual(response.status_code, 404)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.c1 = City.objects.create(name=\"New York\")\n", "    def test_add_view_with_invalid_data(self):\n        post_data = {\"name\": \"\"}  # name field is required\n        response = self.client.post(\n            reverse(\"admin:admin_views_explicitlyprovidedpk_add\"), post_data\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"This field is required.\")\n", "def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username=\"staff\",\n        password=\"secret\",\n        email=\"staff@example.com\",\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse(\"admin10:admin_views_article_changelist\")\n    response = self.client.get(known_url[:-1], SCRIPT_NAME=\"/prefix/\")\n    self.assertRedirects(\n        response,\n        \"/prefix\" + known_url,\n        status_code=301,\n        fetch_redirect_response=False,\n    )\n"], "sample_769": ["def test_balanced_accuracy_score_multiclass():\n    # Test balanced accuracy score for multiclass classification task\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute score with default labels introspection\n    score = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(score, 0.46, 2)\n\n    # compute score with explicit label ordering\n    score = balanced_accuracy_score(y_true, y_pred, labels=[0, 2, 1])\n    assert_almost_equal(score, 0.46, 2)\n", "def test_hinge_loss_binary_string_labels():\n    y_true = np.array([\"no\", \"yes\", \"yes\", \"no\"])\n    pred_decision = np.array([-8.5, 0.5, 1.5, -0.3])\n    assert_equal(hinge_loss(y_true, pred_decision), 1.2 / 4)\n", "def test_balanced_accuracy_score_zero_division():\n    # Test balanced_accuracy_score when there are no occurrences of a class in y_true\n    assert_warns_message(UserWarning, 'y_true contains no occurrences of class 1',\n                         balanced_accuracy_score, [0, 0, 0], [0, 0, 1])\n", "def test_balanced_accuracy_score_binary():\n    # Test binary balanced accuracy score\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    score = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(score, 0.7142857, decimal=6)\n\n    score = balanced_accuracy_score(y_true, y_true)\n    assert_almost_equal(score, 1.0, decimal=6)\n", "def test_hinge_loss_multilabel_indicator():\n    y_true = np.array([[0, 1], [1, 0], [1, 1]])\n    pred_decision = np.array([[0.5, -0.5], [-0.5, 0.5], [-0.5, 0.5]])\n    loss = hinge_loss(y_true, pred_decision)\n    assert_almost_equal(loss, 1.0)\n"], "sample_36": ["def test_biweight_location_with_M_parameter():\n    \"\"\"Test biweight_location with the M parameter.\"\"\"\n    with NumpyRNGContext(12345):\n        ny = 100\n        nx = 200\n        data = normal(5, 2, (ny, nx))\n        M = np.median(data, axis=0)\n\n        bw = biweight_location(data, M=M, axis=0)\n        bwi = []\n        for i in range(nx):\n            bwi.append(biweight_location(data[:, i], M=M[i]))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n\n        bw = biweight_location(data, M=M, axis=1)\n        bwi = []\n        for i in range(ny):\n            bwi.append(biweight_location(data[i, :], M=M[i]))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n", "def test_biweight_midcorrelation_symmetry():\n    \"\"\"\n    Test that biweight_midcorrelation is symmetric.\n    \"\"\"\n\n    rng = np.random.RandomState(1)\n    x = rng.normal(0, 2, size=500)\n    y = rng.normal(0, 2, size=500)\n    assert_allclose(biweight_midcorrelation(x, y), biweight_midcorrelation(y, x))\n", "def test_biweight_midcovariance_small_sample_size():\n    \"\"\"Test biweight midcovariance with small sample size.\"\"\"\n    x = np.array([1, 2, 3, 4, 5])\n    y = np.array([2, 3, 4, 5, 6])\n    expected_cov = np.array([[2.5, 2.5], [2.5, 2.5]])\n    cov = biweight_midcovariance([x, y], modify_sample_size=True)\n    assert_allclose(cov, expected_cov)\n", "def test_biweight_midvariance_with_initial_guess():\n    \"\"\"\n    Test biweight_midvariance with initial guess for the location.\n    \"\"\"\n    with NumpyRNGContext(12345):\n        data = normal(5, 2, 100)\n        var = biweight_midvariance(data)\n        var_with_guess = biweight_midvariance(data, M=5)\n        assert_allclose(var, var_with_guess)\n\n        M = np.median(data)\n        var_with_guess = biweight_midvariance(data, M=M)\n        assert_allclose(var, var_with_guess)\n", "def test_biweight_location_M_input():\n    \"\"\"Test the M input for biweight_location.\"\"\"\n    data = np.array([1, 2, 3, 4, 5])\n    M = 3.0\n    result = biweight_location(data, M=M)\n    expected_result = 3.0\n    assert_allclose(result, expected_result)\n\n    M = np.array([2.0, 2.0, 2.0, 2.0, 2.0])\n    result = biweight_location(data, M=M)\n    assert_allclose(result, expected_result)\n\n    with pytest.raises(ValueError):\n        biweight_location(data, M=np.array([1.0, 2.0]))\n\n    with pytest.raises(ValueError):\n        biweight_location(data, M=np.array([[1.0, 2.0], [3.0, 4.0]]))\n"], "sample_768": ["def test_group_shuffle_split_reproducible():\n    # Check that iterating twice on the GroupShuffleSplit gives the same\n    # sequence of train-test when the random_state is given\n    gss = GroupShuffleSplit(random_state=21)\n    assert_array_equal(list(a for a, b in gss.split(X, y, groups)),\n                       list(a for a, b in gss.split(X, y, groups)))\n", "def test_stratified_shuffle_split_reproducible():\n    # Check that iterating twice on the StratifiedShuffleSplit gives the same\n    # sequence of train-test when the random_state is given\n    sss = StratifiedShuffleSplit(test_size=0.2, random_state=21)\n    y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n    assert_array_equal(list(a for a, b in sss.split(X, y)),\n                       list(a for a, b in sss.split(X, y)))\n", "def test_check_cv_default_repr():\n    # Test that repr is being returned\n    assert_equal(\"KFold(n_splits=5, random_state=None, shuffle=False)\", repr(check_cv(5)))\n", "def test_group_kfold_no_shuffle():\n    # Manually check that GroupKFold preserves the data ordering as much\n    # as possible on toy datasets in order to avoid hiding sample dependencies\n    # when possible\n    X = np.ones(7)\n    y = [0] * 3 + [1] * 4\n    groups = [0, 1, 1, 0, 1, 1, 0]\n    splits = GroupKFold(2).split(X, y, groups)\n    train, test = next(splits)\n    assert_array_equal(test, [0, 2, 3, 5, 6])\n    assert_array_equal(train, [1, 4])\n\n    train, test = next(splits)\n    assert_array_equal(test, [1, 4])\n    assert_array_equal(train, [0, 2, 3, 5, 6])\n\n    # Check if get_n_splits returns the number of folds\n    assert_equal(5, GroupKFold(5).get_n_splits(X, y, groups))\n", "def test_stratified_shuffle_split_overlap_train_test_bug_with_groups():\n    # Test for the bug reported in https://github.com/scikit-learn/scikit-learn/issues/6121\n    # but with groups parameter\n    y = [0, 1, 2, 3] * 3 + [4, 5] * 5\n    groups = [0] * 12 + [1] * 10\n    X = np.ones_like(y)\n\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n\n    train, test = next(sss.split(X=X, y=y, groups=groups))\n\n    # no overlap\n    assert_array_equal(np.intersect1d(train, test), [])\n\n    # complete partition\n    assert_array_equal(np.union1d(train, test), np.arange(len(y)))\n"], "sample_235": ["def test_hook_with_savepoint(self):\n    with transaction.atomic():\n        with transaction.atomic():\n            self.do(1)\n            transaction.on_commit(lambda: self.notify('sp1'))\n        self.do(2)\n    self.assertDone([1, 2])\n    self.assertNotified(['sp1'])\n", "def test_savepoint_with_atomic(self):\n    with transaction.atomic():\n        self.do(1)\n        sid = connection.savepoint()\n        self.do(2)\n        connection.savepoint_rollback(sid)\n\n    self.assertDone([1])\n", "def test_hook_in_savepoint(self):\n        t = Thing.objects.create(num=i)\n        self.notify(t.num)\n\n    with transaction.atomic():\n        with transaction.atomic():\n            transaction.on_commit(lambda: on_commit(1))\n        transaction.on_commit(lambda: on_commit(2))\n\n    self.assertDone([1, 2])\n", "def test_savepoint_rollback_with_nested_commit(self):\n    with transaction.atomic():\n        self.do(1)\n        with transaction.atomic():\n            self.do(2)\n            transaction.commit()\n        self.do(3)\n        transaction.savepoint_rollback(self.notified.pop())\n\n    self.assertDone([1])\n", "def test_runs_hooks_after_nested_rollback(self):\n    with transaction.atomic():\n        self.do(1)\n        try:\n            with transaction.atomic():\n                self.do(2)\n                raise ForcedError()\n        except ForcedError:\n            pass\n        self.do(3)\n\n    self.assertDone([1, 3])\n"], "sample_646": ["def test_unittest_skip_class(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        @unittest.skip(\"skipping class due to reasons\")\n        class MySkippedTestCase(unittest.TestCase):\n                self.fail()\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"-rs\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *SKIP*[1]*skipping class due to reasons*\n        *1 skipped*\n    \"\"\"\n    )\n", "def test_traceback_pruning_no_crash(pytester: Pytester) -> None:\n    \"\"\"Regression test for #9610 - doesn't crash during traceback pruning.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                unittest.TestCase.__init__(self, test_method)\n\n        class TestIt(MyTestCase):\n                pass\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 1\n    assert failed == 0\n    assert reprec.ret == 0\n", "def test_setup_skipped_class(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        @pytest.mark.skip(\"skipping class\")\n        class MyTestCase(unittest.TestCase):\n                self.foo = 1\n\n                assert self.foo == 1\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(skipped=1)\n", "def test_unittest_skip_class_issue148(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        @unittest.skip(\"class skip\")\n        class MyTestCase(unittest.TestCase):\n            @classmethod\n                xxx\n                pass\n            @classmethod\n                xxx\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(skipped=1)\n", "def test_teardown_issue1649_with_gc_disabled(pytester: Pytester) -> None:\n    \"\"\"\n    Are TestCase objects cleaned up when gc.disable() is called?\n\n    This test checks if TestCase objects are cleaned up when gc.disable() is called\n    during the execution of the test methods.\n    \"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import gc\n        class TestCaseObjectsShouldBeCleanedUp(unittest.TestCase):\n                self.an_expensive_object = 1\n                gc.disable()\n                pass\n                gc.enable()\n\n    \"\"\"\n    )\n    pytester.inline_run(\"-s\", testpath)\n    gc.collect()\n    for obj in gc.get_objects():\n        assert type(obj).__name__ != \"TestCaseObjectsShouldBeCleanedUp\"\n"], "sample_33": ["def test_shaped_like_ndarray():\n    class TestShape(misc.ShapedLikeNDArray):\n            self._data = np.array(data)\n\n        @property\n            return self._data.shape\n\n            return TestShape(getattr(self._data, method)(*args, **kwargs))\n\n    t = TestShape([[1, 2, 3], [4, 5, 6]])\n    assert t.shape == (2, 3)\n    assert t.size == 6\n    assert t.ndim == 2\n    assert t.isscalar == False\n    assert len(t) == 2\n    assert bool(t) == True\n    assert t[0, 1] == 2\n    assert [i for i in t] == [1, 2, 3, 4, 5, 6]\n    assert t.copy().shape == (2, 3)\n    assert t.reshape(3, 2).shape == (3, 2)\n    assert t.ravel().shape == (6,)\n    assert t.flatten().shape == (6,)\n    assert t.transpose().shape == (3, 2)\n    assert t.T.shape == (3, 2)\n    assert t.swapaxes(0, 1).shape == (3, 2)\n    assert t.diagonal().shape == (2,)\n    assert t.squeeze().shape == (2, 3)\n    assert t.take([0, 1]).shape == (2,)\n", "def test_shaped_like_ndarray():\n    class ArrayLike(misc.ShapedLikeNDArray):\n            self._data = np.array(data)\n\n        @property\n            return self._data.shape\n\n            new_data = getattr(self._data, method)(*args, **kwargs)\n            return ArrayLike(new_data)\n\n    a = ArrayLike([[1, 2, 3], [4, 5, 6]])\n    assert a.shape == (2, 3)\n    assert a.size == 6\n    assert a.ndim == 2\n    assert a.isscalar is False\n    assert len(a) == 2\n    assert bool(a) is True\n    assert a[1, 2] == 6\n    assert list(a) == [ArrayLike([1, 2, 3]), ArrayLike([4, 5, 6])]\n    assert a.reshape(3, 2).shape == (3, 2)\n    assert a.ravel().shape == (6,)\n    assert a.flatten().shape == (6,)\n    assert a.transpose().shape == (3, 2)\n    assert a.T.shape == (3, 2)\n    assert a.swapaxes(0, 1).shape == (3, 2)\n    assert a.diagonal().shape == (2,)\n    assert a.squeeze().shape == (2, 3)\n    assert a.take([0, 1]).shape == (2,)\n", "def test_indent():\n    s = \"Hello\\nWorld\"\n    indented = misc.indent(s)\n    assert indented == \"    Hello\\n    World\"\n    assert misc.indent(s, width=2) == \"  Hello\\n  World\"\n    assert misc.indent(s, shift=2, width=2) == \"      Hello\\n      World\"\n    assert misc.indent(\"\") == \"\"\n    assert misc.indent(\"Hello\\nWorld\\n\", shift=0) == \"Hello\\nWorld\\n\"\n", "def test_ordered_descriptor_container():\n    class ExampleDecorator(misc.OrderedDescriptor):\n        _class_attribute_ = '_examples_'\n        _name_attribute_ = 'name'\n        name = '<unbound>'\n\n            super().__init__()\n            self.type = type\n\n    class Point2D(metaclass=misc.OrderedDescriptorContainer):\n        x = ExampleDecorator(float)\n        y = ExampleDecorator(int)\n\n    assert 'x' in Point2D._examples_\n    assert 'y' in Point2D._examples_\n    assert isinstance(Point2D._examples_['x'], ExampleDecorator)\n    assert isinstance(Point2D._examples_['y'], ExampleDecorator)\n    assert Point2D._examples_['x'].name == 'x'\n    assert Point2D._examples_['y'].name == 'y'\n    assert Point2D._examples_['x'].type == float\n    assert Point2D._examples_['y'].type == int\n\n    class Point3D(Point2D):\n        _inherit_descriptors_ = (ExampleDecorator,)\n        z = ExampleDecorator(complex)\n\n    assert 'x' in Point3D._examples_\n    assert 'y' in Point3D._examples_\n    assert 'z' in Point3D._examples_\n    assert Point3D._examples_['z'].type == complex\n", "def test_ordered_descriptor_container():\n    class ExampleDecorator(misc.OrderedDescriptor):\n        _class_attribute_ = '_examples_'\n        _name_attribute_ = 'name'\n        name = '<unbound>'\n\n            super().__init__()\n            self.value = value\n\n    class ExampleContainer(metaclass=misc.OrderedDescriptorContainer):\n        a = ExampleDecorator('first')\n        b = ExampleDecorator('second')\n\n    assert ExampleContainer._examples_ == {'a': ExampleContainer.a, 'b': ExampleContainer.b}\n    assert ExampleContainer.a.name == 'a'\n    assert ExampleContainer.b.name == 'b'\n\n    class SubExampleContainer(ExampleContainer):\n        _inherit_descriptors_ = (ExampleDecorator,)\n        c = ExampleDecorator('third')\n\n    assert SubExampleContainer._examples_ == {'a': ExampleContainer.a, 'b': ExampleContainer.b, 'c': SubExampleContainer.c}\n    assert SubExampleContainer.c.name == 'c'\n"], "sample_87": ["def test_watch_dir_without_absolute(self):\n    with self.assertRaisesMessage(ValueError, 'test.py must be absolute.'):\n        self.reloader.watch_dir('test.py', '*.py')\n", "def test_nonexistent_file_not_watched(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.nonexistent_file)\n    with self.tick_twice():\n        self.increment_mtime(self.nonexistent_file)\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_nonexistent_glob(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, 'nonexistent*.py')\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_notify_file_changed_no_handlers(self):\n    with mock.patch('django.utils.autoreload.file_changed') as mocked_file_changed:\n        mocked_file_changed.send.return_value = [(mock.sentinel.RECEIVER, False)]\n        with self.assertRaises(SystemExit):\n            self.reloader.notify_file_changed(self.existing_file)\n        self.assertEqual(mocked_file_changed.send.call_count, 1)\n        self.assertCountEqual(mocked_file_changed.send.call_args[1], {'sender': self.reloader, 'file_path': self.existing_file})\n", "def test_snapshot_files_ignores_duplicate_paths(self):\n    # Create a duplicate file with a different path\n    dup_file = self.ensure_file(self.tempdir / 'dup_test.py')\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, dup_file]):\n        # Make both files identical\n        with open(self.existing_file, 'wb') as f1, open(dup_file, 'rb') as f2:\n            f1.write(f2.read())\n        # Check that only one file is returned\n        self.assertEqual(len(dict(self.reloader.snapshot_files())), 1)\n"], "sample_931": ["def test_pyattribute_without_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :value: ''\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \" = ''\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_pydecoratormethod_signature_noindex(app):\n    text = \".. py:decoratormethod:: deco\\n   :noindex:\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, [desc], desctype=\"method\", domain=\"py\", objtype=\"method\", noindex=True)\n    assert 'deco' not in domain.objects\n", "def test_pyattribute_without_type_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],)],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_pyfunction_signature_return_annotation(app):\n    text = \".. py:function:: hello(name: str) -> Optional[str]\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"Optional\"],\n                                                    [desc_sig_punctuation, \"[\"],\n                                                    [pending_xref, \"str\"],\n                                                    [desc_sig_punctuation, \"]\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][2], [desc_returns, pending_xref, \"Optional\"])\n    assert_node(doctree[1][0][2][1], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Optional\")\n    assert_node(doctree[1][0][3], desc_sig_punctuation, astext=\"[\")\n    assert_node(doctree[1][0][4], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n    assert_node(doctree[1][0][5], desc_sig_punctuation, astext=\"]\")\n", "def test_pyfunction_signature_with_defaults(app):\n    text = \".. py:function:: hello(name='world') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"'world'\"])])\n"], "sample_1167": ["def test_latex_MatrixExpr():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A * B + B * A\n    assert latex(expr) == r\"A B + B A\"\n", "def test_latex_div():\n    assert latex(x / y) == r'\\frac{x}{y}'\n", "def test_latex_settings_propagation():\n    # Test that settings are propagated to sub-expressions\n    x, y = symbols('x y')\n    expr = sin(x) + cos(y)\n    assert latex(expr, fold_func_brackets=True) == r\"\\sin x + \\cos y\"\n", "def test_latex_exponent():\n    # Test that exponents are printed correctly\n    assert latex(x**2) == r'x^{2}'\n    assert latex(x**-1) == r'\\frac{1}{x}'\n    assert latex(x**(1/2)) == r'\\sqrt{x}'\n    assert latex(x**(3/2)) == r'\\sqrt{x^{3}}'\n    assert latex(x**(-3/2)) == r'\\frac{1}{\\sqrt{x^{3}}}'\n", "def test_issue_17913():\n    from sympy.physics.quantum.circuitplot import CircuitPlot\n    from sympy.physics.quantum import Qubit\n    q1 = Qubit('1')\n    q2 = Qubit('2')\n    plot = CircuitPlot([q1, q2], [{'target': q1, 'control': q2}])\n    assert latex(plot) == \\\n        r'\\includegraphics[scale=0.5]{generated_latex_issue_17913.pdf}'\n"], "sample_44": ["def test_inplace_multiplication_division_unit_checks(self):\n    lu1 = u.mag(u.Jy)\n    lq1 = u.Magnitude(np.arange(1., 10.), lu1)\n    with pytest.raises(u.UnitsError):\n        lq1 *= u.m\n\n    assert np.all(lq1.value == np.arange(1., 10.))\n    assert lq1.unit == lu1\n\n    with pytest.raises(u.UnitsError):\n        lq1 /= u.m\n\n    assert np.all(lq1.value == np.arange(1., 10.))\n    assert lq1.unit == lu1\n", "def test_neg_pos_inplace(self):\n    lq = u.Magnitude(np.arange(1., 10.)*u.Jy)\n    lq_neg = lq.copy()\n    lq_neg.neg()\n    assert np.all(lq_neg.value == -lq.value)\n    assert lq_neg.unit == lq.unit\n\n    lq_pos = -lq_neg.copy()\n    lq_pos.pos()\n    assert np.all(lq_pos == lq)\n", "def test_rtruediv_with_dimensionless(self):\n    lq = u.Magnitude(np.arange(1., 10.))\n    result = 1. / lq\n    assert np.allclose(result.value, 1. / lq.value)\n    assert result.unit == u.mag**-1\n", "def test_neg_pos_quantity(self):\n    lq = u.Magnitude(np.arange(1., 4.)*u.Jy)\n    neg_lq = -lq\n    assert neg_lq.unit.physical_unit == u.Jy**-1\n    assert np.all(neg_lq.value == -lq.value)\n    pos_lq = +lq\n    assert pos_lq is not lq\n    assert np.all(pos_lq.value == lq.value)\n", "def test_neg_pos_quantity(self):\n    lq = u.Magnitude(np.arange(1., 10.)*u.Jy)\n    neg_lq = -lq\n    assert neg_lq != lq\n    assert neg_lq.unit.physical_unit == u.Jy**-1\n    assert -neg_lq == lq\n    pos_lq = +lq\n    assert pos_lq is not lq\n    assert pos_lq == lq\n"], "sample_866": ["def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix(X)\n    af_sparse = AffinityPropagation(preference=np.median(S) * 10)\n    labels_sparse = af_sparse.fit_predict(X_sparse)\n\n    af_dense = AffinityPropagation(preference=np.median(S) * 10)\n    labels_dense = af_dense.fit_predict(X)\n\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_precomputed_affinity():\n    # Test AffinityPropagation with precomputed affinity matrix\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    labels = af.fit_predict(S)\n\n    # Test if labels match when using precomputed affinity matrix\n    af_direct = AffinityPropagation(preference=preference)\n    labels_direct = af_direct.fit_predict(X)\n    assert_array_equal(labels, labels_direct)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(preference=np.median(X_sparse.multiply(X_sparse).sum(axis=1).A1) * 10)\n    labels_sparse = af.fit_predict(X_sparse)\n\n    af_dense = AffinityPropagation(preference=np.median(X @ X.T) * 10)\n    labels_dense = af_dense.fit_predict(X)\n\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix(X)\n\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X_sparse)\n\n    # Check that labels are the same for sparse and dense input\n    labels_dense = af.fit_predict(X)\n    assert_array_equal(labels, labels_dense)\n", "def test_affinity_propagation_sparse_input():\n    # Test that AffinityPropagation works with sparse input\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X_sparse)\n    labels2 = af.predict(X_sparse)\n    assert_array_equal(labels, labels2)\n"], "sample_342": ["def test_custom_get_search_results(self):\n    q = Question.objects.create(question='Is this a question?')\n\n    class CustomQuestionAdmin(QuestionAdmin):\n            queryset, use_distinct = super().get_search_results(request, queryset, search_term)\n            queryset = queryset.filter(id=q.id)  # Custom filter to return only the specific question\n            return queryset, use_distinct\n\n    with model_admin(Question, CustomQuestionAdmin):\n        request = self.factory.get(self.url, {'term': 'is', **self.opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q.pk), 'text': q.question}],\n            'pagination': {'more': False},\n        })\n", "def test_custom_paginator(self):\n    \"\"\"\n    ModelAdmin.paginator is used for the autocomplete view's pagination.\n    \"\"\"\n    class CustomPaginatorQuestionAdmin(QuestionAdmin):\n            from django.core.paginator import Paginator\n            return Paginator(queryset, per_page // 2, orphans=orphans, allow_empty_first_page=allow_empty_first_page)\n\n    Question.objects.bulk_create(Question(question=str(i)) for i in range(PAGINATOR_SIZE + 10))\n    request = self.factory.get(self.url, {'term': '', **self.opts})\n    request.user = self.superuser\n    with model_admin(Question, CustomPaginatorQuestionAdmin):\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(len(data['results']), PAGINATOR_SIZE // 2)\n", "def test_non_admin_user_permissions(self):\n    \"\"\"\n    Non-admin users require the view permission for the related model to the\n    autocomplete view for it.\n    \"\"\"\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.user\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    p = Permission.objects.get(\n        content_type=ContentType.objects.get_for_model(Question),\n        codename='view_question',\n    )\n    self.user.user_permissions.add(p)\n    request.user = User.objects.get(pk=self.user.pk)\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n", "def test_autocomplete_pagination(self):\n    # Create more questions than the paginator size\n    for i in range(PAGINATOR_SIZE + 10):\n        Question.objects.create(question=f'Question {i}')\n\n    request = self.factory.get(self.url, {'term': 'Question', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n\n    self.assertEqual(len(data['results']), PAGINATOR_SIZE)\n    self.assertTrue(data['pagination']['more'])\n\n    # Request the next page of results\n    request = self.factory.get(self.url, {'term': 'Question', 'page': '2', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n\n    self.assertEqual(len(data['results']), 10)\n    self.assertFalse(data['pagination']['more'])\n", "def test_inline_delete_widgets(self):\n    from selenium.webdriver.support import expected_conditions as EC\n    from selenium.common.exceptions import NoSuchElementException\n\n    # Autocomplete works in rows present when the page loads.\n    self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_book_add'))\n    rows = self.selenium.find_elements_by_css_selector('.dynamic-authorship_set')\n    self.assertEqual(len(rows), 3)\n\n    # Add an author to be deleted later\n    Author.objects.create(name='Delete Me')\n    self.selenium.refresh()\n\n    # Select the author in the first row\n    elem = rows[0].find_element_by_css_selector('.select2-selection')\n    elem.click()  # Open the autocomplete dropdown.\n    search = self.selenium.find_element_by_css_selector('.select2-search__field')\n    search.send_keys('Delete Me')\n    search.send_keys(Keys.RETURN)\n\n    # Delete the author in the first row\n    rows[0].find_element_by_css_selector('.inline-deletelink').click()\n    self.assertTrue(EC.alert_is_present()(self.selenium))\n    self.selenium.switch_to.alert.accept()\n\n    # Check that the deleted row is no longer present\n    with self.assertRaises(NoSuchElementException):\n        rows[0].find_element_by_css_selector('.select2-selection')\n"], "sample_61": ["    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', 'abc123', 'user.name', 'user+name', 'user-name', 'user_name', 'user@name']\n        invalid_usernames = ['\u0623\u062d\u0645\u062f', 'zerowidth\\u200Bspace', 'nonbreaking\\u00A0space', 'en\\u2013dash']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'John123', 'user.name', 'user@domain', 'user+name', 'user-name', 'user_name']\n        invalid_usernames = ['Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', '\u0623\u062d\u0645\u062f', 'zerowidth\\u200Bspace', 'nonbreaking\\u00A0space']\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', '12345', 'joe.doe', 'joe@example', 'joe+doe', 'joe-doe', 'joe_doe']\n        invalid_usernames = ['joe-doe-', 'joe_doe_', 'joe..doe', 'joe@@example', 'joe++doe', 'joe--doe', 'joe__doe', ' joe', 'joe ', 'joe\\n', '\u30b8\u30e7\u30fc', '\uff4a\uff4f\uff45']\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', 'abc123', 'user.name', 'user@example.com', 'user+name', 'user-name', 'user_name']\n        invalid_usernames = ['joe\ud83c\udf0e', 'Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', '\u0623\u062d\u0645\u062f']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', '12345', 'joe.smith', 'joe_smith', 'joe+smith', 'joe-smith', 'joe@smith']\n        invalid_usernames = ['joe\u00e9', '\uff2a\uff4f\uff45', 'joe\\u200B', 'joe\\u00A0', 'joe\\u2013', 'joe\\u000A']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_1179": ["def test_MatPow():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert str(A**-1) == \"A**(-1)\"\n    assert str(A**2) == \"A**2\"\n", "def test_MatrixDeterminant():\n    A = Matrix([[1, 2], [3, 4]])\n    assert str(A.det()) == \"A.det()\"\n", "def test_MatrixTranspose():\n    A = Matrix([[1, 2], [3, 4]])\n    assert str(A.T) == \"Matrix([\\n[1, 3],\\n[2, 4]])\"\n", "def test_MatPow():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert str(MatPow(A, 2)) == \"A**2\"\n    assert str(MatPow(A, -1)) == \"A**(-1)\"\n    assert str(MatPow(A + B, 3)) == \"(A + B)**3\"\n", "def test_NDimArray_str_printer():\n    A = NDimArray([[1, 2], [3, 4]])\n    assert str(A) == \"Array([[1, 2], [3, 4]])\"\n    assert sstr(A) == \"Array([\\n[1, 2],\\n[3, 4]])\"\n\n    B = NDimArray([[5, 6], [7, 8]])\n    assert str(A + B) == \"Array([[1, 2], [3, 4]]) + Array([[5, 6], [7, 8]])\"\n    assert sstr(A + B) == \"Array([\\n[6, 8],\\n[10, 12]])\"\n"], "sample_201": ["def test_legacy_hash(self):\n    \"\"\"\n    The legacy hash function generates a hash that can be verified by\n    the legacy decode function.\n    \"\"\"\n    storage = self.get_storage()\n    value = 'test message'\n    hash_ = storage._legacy_hash(value)\n    data = f'{hash_}${value}'\n    decoded = storage._legacy_decode(data)\n    self.assertEqual(decoded, value)\n", "def test_legacy_hash(self):\n    \"\"\"\n    The legacy hash function generates a valid hash for a given value.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    value = 'test_value'\n    hash_ = storage._legacy_hash(value)\n    self.assertTrue(constant_time_compare(hash_, salted_hmac('django.contrib.messages', value).hexdigest()))\n", "def test_safedata_persistence(self):\n    \"\"\"\n    A message containing SafeData is persisted correctly in the cookie and\n    retrieves its safe status when retrieved from the message storage.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Add a message containing SafeData\n    message = Message(constants.DEBUG, mark_safe('safe message'))\n    storage.add(message.level, message.message)\n    storage.update(response)\n\n    # Simulate the message being set in the cookie\n    set_cookie_data(storage, [message])\n\n    # Check the message retrieves its safe status\n    retrieved_messages = list(storage)\n    self.assertEqual(len(retrieved_messages), 1)\n    self.assertIsInstance(retrieved_messages[0].message, SafeData)\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    If the data exceeds what is allowed in a cookie, the 'not_finished'\n    sentinel value is appended to the data.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 3 messages into the cookie, but not 4.\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(4):\n        storage.add(constants.INFO, str(i) * msg_size)\n    unstored_messages = storage.update(response)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, '3' * msg_size)\n\n    # Decode the data to check for the 'not_finished' sentinel\n    data = storage._decode(response.cookies[storage.cookie_name].value)\n    self.assertEqual(data[-1], CookieStorage.not_finished)\n", "def test_legacy_hash(self):\n    \"\"\"\n    Test the legacy hash function used for backward compatibility.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    # Test the hash function with a known input-output pair\n    value = 'test_value'\n    expected_hash = '2a0052f0753850e0824001d11539d22114966731'\n    self.assertEqual(storage._legacy_hash(value), expected_hash)\n"], "sample_429": ["def test_prohibit_null_characters_validator_equality(self):\n    self.assertEqual(\n        ProhibitNullCharactersValidator(),\n        ProhibitNullCharactersValidator(),\n    )\n    self.assertEqual(\n        ProhibitNullCharactersValidator(message=\"Custom error message\"),\n        ProhibitNullCharactersValidator(message=\"Custom error message\"),\n    )\n    self.assertEqual(\n        ProhibitNullCharactersValidator(code=\"custom_code\"),\n        ProhibitNullCharactersValidator(code=\"custom_code\"),\n    )\n    self.assertNotEqual(\n        ProhibitNullCharactersValidator(),\n        ProhibitNullCharactersValidator(message=\"Custom error message\"),\n    )\n    self.assertNotEqual(\n        ProhibitNullCharactersValidator(),\n        ProhibitNullCharactersValidator(code=\"custom_code\"),\n    )\n    self.assertNotEqual(\n        ProhibitNullCharactersValidator(message=\"Custom error message\"),\n        ProhibitNullCharactersValidator(code=\"custom_code\"),\n    )\n", "def test_prohibit_null_characters_validator_equality(self):\n    self.assertEqual(\n        ProhibitNullCharactersValidator(),\n        ProhibitNullCharactersValidator(),\n    )\n    self.assertNotEqual(\n        ProhibitNullCharactersValidator(message=\"Custom message\"),\n        ProhibitNullCharactersValidator(),\n    )\n    self.assertEqual(\n        ProhibitNullCharactersValidator(message=\"Custom message\", code=\"custom_code\"),\n        ProhibitNullCharactersValidator(message=\"Custom message\", code=\"custom_code\"),\n    )\n    self.assertNotEqual(\n        ProhibitNullCharactersValidator(message=\"Custom message\", code=\"custom_code\"),\n        ProhibitNullCharactersValidator(message=\"Different message\", code=\"custom_code\"),\n    )\n    self.assertNotEqual(\n        ProhibitNullCharactersValidator(message=\"Custom message\", code=\"custom_code\"),\n        ProhibitNullCharactersValidator(message=\"Custom message\", code=\"different_code\"),\n    )\n", "def test_prohibit_null_characters_validator_attributes(self):\n    custom_msg = \"Custom null character error message\"\n    custom_code = \"custom_null_char_code\"\n    validator = ProhibitNullCharactersValidator(message=custom_msg, code=custom_code)\n    self.assertEqual(validator.message, custom_msg)\n    self.assertEqual(validator.code, custom_code)\n", "compilation error", "def test_url_validator_with_extended_schemes(self):\n    validator = URLValidator(schemes=EXTENDED_SCHEMES)\n    for url in VALID_URLS:\n        with self.subTest(url=url):\n            self.assertEqual(None, validator(url))\n\n    for url in INVALID_URLS:\n        with self.subTest(url=url):\n            with self.assertRaises(ValidationError):\n                validator(url)\n"], "sample_540": ["def test_extra_args_warning(anim):\n    with pytest.warns(UserWarning, match='Passing in values for arguments fps, codec, bitrate, extra_args, or metadata is not supported when writer is an existing MovieWriter instance.'):\n        anim.save('unused.null', writer=NullMovieWriter(), fps=30, codec='h264', bitrate=5000, extra_args=['-vcodec', 'libx264'], metadata={'title': 'Test Animation'})\n", "def test_blit_draw_and_clear(anim):\n    # test _blit_draw and _blit_clear methods\n    anim._blit = True\n    anim._setup_blit()\n\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n    artists = [line]\n\n    # test _blit_draw method\n    anim._blit_draw(artists)\n\n    # test _blit_clear method\n    anim._blit_clear(artists)\n", "def test_animation_pause_resume(anim):\n    # Test pausing and resuming an animation\n    anim.pause()\n    assert not anim.event_source.running()\n    anim.resume()\n    assert anim.event_source.running()\n", "def test_save_count_override_warnings_no_length(anim):\n    save_count = 5\n    frames = lambda: (i for i in range(2))\n    match_target = (\n        f\"{frames=!r} which we can infer the length of, \"\n        \"did not pass an explicit *save_count* \"\n        f\"and passed {anim._cache_frame_data=}.  To avoid a possibly \"\n        \"unbounded cache, frame data caching has been disabled. \"\n        \"To suppress this warning either pass \"\n        \"`cache_frame_data=False` or `save_count=MAX_FRAMES`.\"\n    )\n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames, 'save_count': save_count}\n        )\n    assert anim._save_count == save_count\n    anim._init_draw()\n", "def test_anim_func_args(anim):\n    # Test passing args to the func of FuncAnimation\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i + constant)\n        line.set_data(x, y)\n        return line,\n\n    line, = plt.plot([], [])\n    anim = animation.FuncAnimation(fig=plt.gcf(), func=animate, init_func=lambda: line, fargs=(2,))\n    # Call _step() to simulate the animation\n    anim._step()\n    # Check if the data is updated correctly\n    assert np.array_equal(line.get_xdata(), np.linspace(0, 10, 100))\n    assert np.array_equal(line.get_ydata(), np.sin(np.linspace(0, 10, 100) + 2))\n"], "sample_395": ["def test_template_dirs_ignore_django_path(self, mock_is_django_path):\n    self.assertEqual(autoreload.get_template_directories(), set())\n", "def test_template_dirs_empty_list(self):\n    self.assertEqual(autoreload.get_template_directories(), set())\n", "def test_django_path_not_included(self):\n    self.assertNotIn(\n        Path(\"/path/to/django/templates\"),\n        autoreload.get_template_directories(),\n    )\n", "def test_template_dirs_ignore_non_django_backends(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            ROOT / \"django_templates\",\n        },\n    )\n", "    def test_django_paths_are_ignored(self, mock_reset):\n        with mock.patch(\"django.utils.autoreload.is_django_path\") as mock_is_django:\n            mock_is_django.return_value = True\n            self.assertFalse(autoreload.template_changed(None, EXTRA_TEMPLATES_DIR / \"index.html\"))\n            mock_reset.assert_not_called()\n"], "sample_1191": ["def test_hermite_normal_form():\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    raises(DMDomainError, lambda: _hermite_normal_form(DM([[1]], ZZ[x])))\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(DM([[1]], ZZ[x]), 5))\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(DM([[1]], ZZ), 'a'))\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(DM([[1, 2], [3, 4]], ZZ), 5))\n", "def test_hermite_normal():\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert _hermite_normal_form(m).to_dense() == hnf\n\n    raises(DMDomainError, lambda: _hermite_normal_form(DM([[1]], ZZ[x])))\n\n    D = 30\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 0], [0, 15, 0], [0, 0, 2]], ZZ)\n    assert _hermite_normal_form_modulo_D(m, D).to_dense() == hnf\n\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(DM([[1]], ZZ[x]), D))\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(m, ZZ[x]))\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(DM([[1, 2], [3, 4]], ZZ), D))\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], QQ)\n    raises(DMDomainError, lambda: hermite_normal_form(m))\n\n    D = 30\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 0], [0, 15, 0],", "def test_hermite_normal():\n    # Test hermite_normal_form function\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Test _hermite_normal_form function\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert _hermite_normal_form(m).to_dense() == hnf\n\n    # Test _hermite_normal_form_modulo_D function\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    D = 100\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert _hermite_normal_form_modulo_D(m, D).to_dense() == hnf\n\n    # Test hermite_normal_form function with D\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    D = 100\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m, D=D).to_dense() == hnf\n\n    # Test hermite_normal_form function with check", "def test_hermite_normal():\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    D = 30\n    hnf_mod_D = DM([[10, 0, 1], [0, 15, 0], [0, 0, 30]], ZZ)\n    assert _hermite_normal_form_modulo_D(m, D).to_dense() == hnf_mod_D\n\n    m_rank_deficient = DM([[12, 6, 4], [3, 9, 6], [6, 18, 12]], ZZ)\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(m_rank_deficient, D))\n\n    m_non_integer = DM([[12, 6, 4], [3, 9, 6], [2, 16.5, 14]], QQ)\n    raises(DMDomainError, lambda: _hermite_normal_form(m_non_integer))\n", "def test_hermite_normal():\n    A = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(A).to_dense() == hnf\n\n    # Test with D parameter\n    A = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    D = 90\n    hnf_mod_D = DM([[5, 0, 2], [0, 5, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(A, D=D).to_dense() == hnf_mod_D\n\n    # Test with D parameter and check_rank\n    A = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    D = 90\n    hnf_mod_D = DM([[5, 0, 2], [0, 5, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(A, D=D, check_rank=True).to_dense() == hnf_mod_D\n\n    # Test with non-square matrix\n    A = DM([[12, 6, 4, 8], [3, 9, 6, 12], [2, 16, 14, 28]], ZZ)\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(A, D=90))\n\n    # Test with non-integer matrix\n    A = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], QQ)\n    raises"], "sample_877": ["def test_isotonic_regression_increasing_false():\n    # Test the case when increasing is set to False\n    y = np.array([10, 9, 8, 7, 6, 5])\n    x = np.arange(len(y))\n    y_true = [10, 9, 8, 7, 6, 5]  # expected output when increasing is False\n\n    ir = IsotonicRegression(increasing=False)\n    y_pred = ir.fit_transform(x, y)\n\n    assert_array_equal(y_pred, y_true)\n", "def test_isotonic_regression_sample_weight_zero():\n    # Test case where all sample weights are zero\n    X = np.arange(10)\n    y = np.arange(10)\n    sample_weight = np.zeros(10)\n\n    # Should not raise an error, and result should be the midpoint of y\n    expected_y = np.mean(y)\n    y_transformed = IsotonicRegression().fit_transform(X, y, sample_weight=sample_weight)\n    assert_allclose(y_transformed, expected_y)\n", "def test_isotonic_regression_feature_names_out():\n    \"\"\"Check `get_feature_names_out` for `IsotonicRegression` with different input shapes.\"\"\"\n    X_1d = np.arange(10)\n    X_2d = X_1d.reshape(-1, 1)\n    y = np.arange(10)\n\n    iso_1d = IsotonicRegression().fit(X_1d, y)\n    iso_2d = IsotonicRegression().fit(X_2d, y)\n\n    names_1d = iso_1d.get_feature_names_out()\n    names_2d = iso_2d.get_feature_names_out()\n\n    assert_array_equal(names_1d, names_2d)\n", "def test_isotonic_regression_increasing_decreasing():\n    # Test increasing and decreasing cases with unique and repeated values\n    y = np.array([5, 4, 3, 3, 2, 1])\n    y_increasing = np.array([1, 2, 3, 3, 4, 5])\n    y_decreasing = np.array([5, 4, 3, 3, 2, 1])\n\n    assert_array_equal(isotonic_regression(y, increasing=True), y_increasing)\n    assert_array_equal(isotonic_regression(y, increasing=False), y_decreasing)\n", "def test_isotonic_regression_2darray_input():\n    # Ensure IsotonicRegression can handle 2darray with only 1 feature\n    X = np.arange(10).reshape(-1, 1)\n    y = np.arange(10)\n\n    iso_reg = IsotonicRegression().fit(X, y)\n    y_pred = iso_reg.predict(X)\n\n    assert y_pred.shape == (10,)\n"], "sample_920": ["    def test_no_arg_in_returns_section(self):\n        docstring = \"\"\"", "    def test_code_block_in_parameters_section(self):\n        docstring = \"\"\"", "    def test_parameter_with_array_type(self):\n        docstring = \"\"\"\n        Parameters\n        ----------\n        arr : numpy.ndarray\n            A 2D array of shape (M, N) where M is the number of samples and N is the number of features.\n        \"\"\"\n\n        expected = \"\"\"\n        :Parameters: **arr** (*numpy.ndarray*) -- A 2D array of shape (M, N) where M is the number of samples and N is the number of features.\n        \"\"\"\n\n        config = Config(napoleon_use_param=True)\n        app = mock.Mock()\n        actual = str(NumpyDocstring(docstring, config, app, \"function\"))\n\n        self.assertEqual(expected, actual)\n", "    def test_usage_section(self):\n        docstring = \"\"\"\n        Single line summary\n\n        Usage:\n          Example usage of the function:\n\n              result = my_function(arg1, arg2)\n\n        \"\"\"\n        expected = \"\"\"\n        Single line summary\n\n        .. rubric:: Usage:\n\n        .. code-block:: python\n\n            result = my_function(arg1, arg2)\n\n        \"\"\"\n        actual = str(GoogleDocstring(dedent(docstring)))\n        self.assertEqual(expected, actual)\n", "    def test_custom_sections(self):\n        docstring = \"\"\""], "sample_299": ["    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path',\n            },\n        }):\n            msg = \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\"\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(msg, id='caches.W003'),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path/cache',\n            },\n        }):\n            warning_message = \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\"\n            self.assertEqual(check_file_based_cache_is_absolute(None), [Warning(warning_message, id='caches.W003')])\n", "    def test_cache_path_relative(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\", id='caches.W003'),\n            ])\n", "def test_relative_path(self):\n    with self.settings(CACHES={\n        'default': {\n            'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n            'LOCATION': 'relative/path/cache',\n        },\n    }):\n        self.assertEqual(\n            check_file_based_cache_is_absolute(None),\n            [Warning(\"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\", id='caches.W003')]\n        )\n"], "sample_73": ["    def test_manifest_missing_version(self):\n        # Create a manifest with a missing version\n        filename = storage.staticfiles_storage.manifest_name\n        path = storage.staticfiles_storage.path(filename)\n        with open(path, 'w') as f:\n            f.write('{\"paths\": {\"cached/styles.css\": \"cached/styles.5e0040571e1a.css\"}}')\n\n        # Loading the manifest should raise a ValueError\n        with self.assertRaisesMessage(ValueError, \"Couldn't load manifest 'staticfiles.json' (version 1.0)\"):\n            storage.staticfiles_storage.load_manifest()\n", "def test_nested_import_loop(self):\n    finders.get_finder.cache_clear()\n    err = StringIO()\n    with self.assertRaisesMessage(RuntimeError, 'Max post-process passes exceeded'):\n        call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n    self.assertEqual(\"Post-processing 'All' failed!\\n\\n\", err.getvalue())\n    self.assertPostCondition()\n", "    def test_cache_persistence_across_instances(self):\n        \"\"\"\n        The hashed file name cache persists across storage instances.\n        \"\"\"\n        # Get the initial hashed file name from the first storage instance.\n        storage_instance_1 = storage.staticfiles_storage\n        cached_name_1 = storage_instance_1.hashed_files['cached/styles.css']\n\n        # Create a new storage instance and check that the cached name is the same.\n        storage_instance_2 = storage.staticfiles_storage\n        cached_name_2 = storage_instance_2.hashed_files['cached/styles.css']\n\n        self.assertEqual(cached_name_1, cached_name_2)\n", "def test_cache_persistence(self):\n    original_hashed_files = storage.staticfiles_storage.hashed_files\n\n    # Simulate a server restart by creating a new storage instance\n    new_storage = storage.ManifestStaticFilesStorage()\n\n    # Check if the hashed files are loaded from the manifest on a new instance\n    self.assertEqual(new_storage.hashed_files, original_hashed_files)\n\n    # Simulate changing a file\n    file_path = \"cached/styles.css\"\n    with open(file_path, 'a') as file:\n        file.write(\"/* Add a comment to change the file */\")\n\n    # Collect static files again to update the manifest\n    self.run_collectstatic()\n\n    # Create a new storage instance after the file change\n    new_storage_after_change = storage.ManifestStaticFilesStorage()\n\n    # Check if the hashed files are updated in the new instance\n    self.assertNotEqual(new_storage_after_change.hashed_files, original_hashed_files)\n", "    def test_post_processing_order(self):\n        # Create a temporary CSS file that references another CSS file\n        filename = 'temp.css'\n        filepath = self._get_filename_path(filename)\n        with open(filepath, 'w') as f:\n            f.write('body { background: url(\"other.css\"); }')\n\n        # Add the temporary directory to STATICFILES_DIRS\n        self.patched_settings = self.settings(\n            STATICFILES_DIRS=settings.STATICFILES_DIRS + [self._temp_dir],\n        )\n        self.patched_settings.enable()\n        self.addCleanup(self.patched_settings.disable)\n\n        # Run collectstatic to post-process the files\n        self.run_collectstatic()\n\n        # Check that the 'temp.css' file has been correctly post-processed\n        relpath = self.hashed_file_path(filename)\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            # The 'other.css' file should have been correctly hashed\n            self.assertIn(b'body { background: url(\"other.deploy12345.css\"); }', content)\n\n        # Clean up the temporary file\n        os.unlink(filepath)\n"], "sample_808": ["def test_iforest_contamination_auto():\n    X_train = [[1, 1], [1, 2], [2, 1], [10, 10]]\n    clf = IsolationForest(contamination='auto').fit(X_train)\n    assert_array_equal(clf.predict([[10, 10]]), [-1])\n    assert_array_equal(clf.predict([[2, 2]]), [1])\n", "def test_iforest_random_state():\n    \"\"\"Test random_state parameter consistency.\"\"\"\n    X = [[0.0], [1.0]]\n    clf1 = IsolationForest(random_state=42)\n    clf2 = IsolationForest(random_state=42)\n    clf1.fit(X)\n    clf2.fit(X)\n    assert_array_equal(clf1.predict(X), clf2.predict(X))\n", "def test_iforest_contamination_param():\n    X_train = [[1, 1], [1, 2], [2, 1], [10, 10]]  # the last sample is an outlier\n    clf = IsolationForest(contamination=0.1).fit(X_train)\n    assert clf.predict([[10, 10]]) == [-1]  # the outlier is detected\n\n    clf = IsolationForest(contamination=0.0).fit(X_train)\n    assert clf.predict([[10, 10]]) == [1]  # no outliers are expected\n", "def test_iforest_max_features():\n    \"\"\"Test that max_features is correctly applied.\"\"\"\n    X = np.random.RandomState(42).rand(10, 10)\n    n_estimators = 10\n    max_features = 5\n    clf = IsolationForest(n_estimators=n_estimators, max_features=max_features, random_state=42)\n    clf.fit(X)\n    assert all(len(features) == max_features for features in clf.estimators_features_)\n", "def test_iforest_threshold_attribute_warning():\n    X_train = [[1, 1], [1, 2], [2, 1], [10, 10]]\n    clf = IsolationForest(behaviour='old', contamination=0.2).fit(X_train)\n    with pytest.warns(DeprecationWarning, match=\"threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\"):\n        threshold = clf.threshold_\n"], "sample_548": ["def test_colorbar_set_label():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_label('Test Label')\n    fig.draw_without_rendering()\n    assert cb.ax.get_ylabel() == 'Test Label'\n\n    cb.set_label('New Label')\n    fig.draw_without_rendering()\n    assert cb.ax.get_ylabel() == 'New Label'\n", "def test_colorbar_labelsize():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, label='cbar', labelsize=12)\n    assert cbar.ax.yaxis.label.get_size() == 12\n\n    cbar.set_label('cbar 2', size=14)\n    assert cbar.ax.yaxis.label.get_size() == 14\n", "def test_colorbar_lines():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    cb.add_lines([1.5], colors=['red'], linewidths=[2])\n", "def test_colorbar_extend_lines_inverted_axis(orientation, extend):\n    \"\"\"Test extend lines with an inverted axis\"\"\"\n    data = np.arange(12).reshape(3, 4)\n    fig, ax = plt.subplots()\n    im = ax.imshow(data)\n    cbar = fig.colorbar(im, orientation=orientation, extend=extend)\n    if orientation == \"horizontal\":\n        cbar.ax.invert_xaxis()\n    else:\n        cbar.ax.invert_yaxis()\n    assert len(cbar.lines) == 1\n    assert len(cbar.lines[0].get_segments()) == 1\n", "def test_colorbar_alpha_none():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, alpha=None)\n    assert cb.alpha is None\n"], "sample_950": ["def test_pyfunction_with_default_values(app):\n    text = \".. py:function:: hello(name: str = 'World') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"str\"],\n                                                        \" \",\n                                                        [desc_sig_operator, \"=\"],\n                                                        \" \",\n                                                        [nodes.inline, \"'World'\"])])])\n", "def test_pyclassmethod_with_return_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:classmethod:: meth() -> str\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class class method)', 'Class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"classmethod \"],\n                                                     [desc_name, \"meth\"],\n                                                     [desc_parameterlist, ()],\n                                                     [desc_returns, pending_xref, \"str\"])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][3], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.meth' in domain.objects\n    assert domain.objects['Class.meth'] == ('index', 'Class.meth', 'method', False)\n", "def test_pyfunction_with_default_argument(app):\n    text = \".. py:function:: hello(name='World')\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    nodes.Text)],\n                                  desc_content)]))\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"'World'\"])])\n", "def test_python_classmethod_noindexentry(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"   .. py:classmethod:: my_method()\\n\"\n            \"      :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'MyClass (built-in class)', 'MyClass', '', None)])\n    assert_node(doctree[1][1], desc, noindexentry=True)\n", "def test_pyfunction_signature_with_optional_param(app):\n    text = \".. py:function:: func(a, b=None)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, desc_sig_name, \"a\",\n                 desc_parameter, (desc_sig_name, \"b\", desc_sig_operator, \"=\", nodes.inline, \"None\")])\n"], "sample_1094": ["def test_eval_subs():\n    x, y, z = symbols('x y z')\n    expr = x + y\n    assert expr._eval_subs(x, z) == y + z\n    assert expr._eval_subs(y, z) == x + z\n    assert expr._eval_subs(x + y, z) is None\n", "def test_count_ops():\n    x, y, z = symbols('x y z')\n    expr = sin(x) + cos(y) + z**2\n    assert expr.count_ops() == 6\n    assert expr.count_ops(visual=True) == 4\n", "def test_subs_simultaneous():\n    x, y = symbols('x y')\n    expr = (x + y)/y\n    assert expr.subs([(x, y), (y, x + y)], simultaneous=True) == y/(x + y)\n    assert expr.subs([(x, y), (y, x + y)], simultaneous=False) == (x + y + y)/(x + y)\n", "def test_find():\n    x, y, z = symbols('x y z')\n    expr = z + w*(x + y)\n    assert expr.find(x) == {x}\n    assert expr.find(y) == {y}\n    assert expr.find(z) == {z}\n    assert expr.find(w) == {w}\n    assert expr.find(x + y) == {x + y}\n    assert expr.find(x, group=True) == {x: 1}\n    assert expr.find(x, y, group=True) == {x: 1, y: 1}\n    assert expr.find(x, y, z, group=True) == {x: 1, y: 1, z: 1}\n    assert expr.find(x, y, z, w, group=True) == {x: 1, y: 1, z: 1, w: 1}\n    assert expr.find(x + y, group=True) == {x + y: 1}\n    assert expr.find(x, y, z, w, x + y, group=True) == {x: 2, y: 2, z: 1, w: 1, x + y: 1}\n", "def test_is_hypergeometric():\n    from sympy import hyper, pi\n    assert hyper((1, 1), (2,), z).is_hypergeometric(1)\n    assert not hyper((1, 1), (2,), z).is_hypergeometric(2)\n    assert not hyper((), (1,), z).is_hypergeometric(1)\n    assert not hyper((1,), (1,), z).is_hypergeometric(1)\n    assert not hyper((1, 1), (), z).is_hypergeometric(1)\n    assert not hyper((1, 1, 1), (2,), z).is_hypergeometric(1)\n    assert not hyper((1, 1), (2, 2), z).is_hypergeometric(1)\n    assert not hyper((1, 1), (2,), z**2).is_hypergeometric(1)\n    assert not hyper((1, 1), (2,), pi).is_hypergeometric(1)\n"], "sample_822": ["def test_check_preserve_type_sparse():\n    # Ensures that type float32 is preserved for sparse matrices.\n    XA = csr_matrix(np.resize(np.arange(40), (5, 8)).astype(np.float32))\n    XB = csr_matrix(np.resize(np.arange(40), (5, 8)).astype(np.float32))\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, None)\n    assert_equal(XA_checked.dtype, np.float32)\n\n    # both float32\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked.dtype, np.float32)\n    assert_equal(XB_checked.dtype, np.float32)\n\n    # mismatched A\n    XA_checked, XB_checked = check_pairwise_arrays(XA.astype(np.float), XB)\n    assert_equal(XA_checked.dtype, np.float)\n    assert_equal(XB_checked.dtype, np.float)\n\n    # mismatched B\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB.astype(np.float))\n    assert_equal(XA_checked.dtype, np.float)\n    assert_equal(XB_checked.dtype, np.float)\n", "def test_check_XB_returned_sparse():\n    # Ensure that if XA and XB are given correctly, they return as equal.\n    # Check that if XB is not None, it is returned equal.\n    # Note that the second dimension of XB is the same as XA.\n    XA = np.resize(np.arange(40), (5, 8))\n    XB = np.resize(np.arange(32), (4, 8))\n    XA_sparse = csr_matrix(XA)\n    XB_sparse = csr_matrix(XB)\n    XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB_sparse)\n    assert issparse(XA_checked)\n    assert issparse(XB_checked)\n    assert_array_equal(XA_sparse.todense(), XA_checked.todense())\n    assert_array_equal(XB_sparse.todense(), XB_checked.todense())\n", "def test_check_invalid_dtypes():\n    # Ensure an error is raised if the dtypes are not floating point.\n    XA = np.arange(45).reshape(9, 5).astype(int)\n    XB = np.arange(32).reshape(4, 8).astype(int)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n", "def test_pairwise_kernels_valid_sparse_input():\n    # Test that the kernels that support sparse input do not raise errors\n    # when sparse matrices are given as input\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((2, 4))\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n\n    for metric in PAIRWISE_KERNEL_FUNCTIONS:\n        if metric in [\"chi2\", \"additive_chi2\"]:\n            # these don't support sparse matrices yet\n            continue\n        K1 = pairwise_kernels(X_sparse, Y=Y_sparse, metric=metric)\n        K2 = pairwise_kernels(X, Y, metric=metric)\n        assert_array_almost_equal(K1, K2)\n", "def test_pairwise_distances_callable_metric():\n    # Test the pairwise_distances helper function\n    # with a callable function, with given keywords.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((2, 4))\n\n        return np.sum(np.abs(x - y))\n\n    K1 = pairwise_distances(X, Y=Y, metric=custom_metric)\n    K2 = np.array([[custom_metric(x, y) for y in Y] for x in X])\n    assert_array_almost_equal(K1, K2)\n\n    # Callable function, X=Y\n    K1 = pairwise_distances(X, Y=X, metric=custom_metric)\n    K2 = np.array([[custom_metric(x, y) for y in X] for x in X])\n    assert_array_almost_equal(K1, K2)\n"], "sample_664": ["def test_funcargnames_is_deprecated(testdir):\n    \"\"\"Test that using funcargnames is deprecated.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*The `funcargnames` attribute was an alias for `fixturenames`*\"]\n    )\n", "def test_funcargnames_is_deprecated(testdir, argnames, warning):\n    testdir.makepyfile(\n        f\"\"\"\n        import pytest\n\n        @pytest.fixture({argnames}='test_arg')\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n\n    if warning:\n        result.stdout.fnmatch_lines([str(warning)])\n    else:\n        result.stdout.no_fnmatch_line(str(deprecated.FUNCARGNAMES))\n", "def test_fixture_positional_arguments_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('function')\n            pass\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them \"\n            \"as a keyword argument instead.*\",\n        ]\n    )\n", "def test_deprecation_warnings(testdir, warning, message):\n    testdir.makepyfile(\n        f\"\"\"\n        import warnings\n        import pytest\n        from _pytest import deprecated\n\n        warnings.simplefilter('always', {warning.__class__.__name__})\n\n            with pytest.warns({warning.__class__.__name__}, match=r'{message}'):\n                warning\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_deprecation_warnings(testdir, fixturename):\n    testdir.makepyfile(\n        f\"\"\"\n        import pytest\n        from _pytest.deprecated import {fixturename.upper()}\n\n        @pytest.fixture\n        def {fixturename}():\n            pass\n\n        def test_{fixturename}({fixturename}):\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--verbose\")\n    if fixturename == \"funcargnames\":\n        result.stdout.fnmatch_lines([\"*The `funcargnames` attribute was an alias for `fixturenames`*\"])\n    elif fixturename == \"result_log\":\n        result.stdout.fnmatch_lines([\"*--result-log is deprecated and scheduled for removal in pytest 6.0*\"])\n    elif fixturename == \"fixture_positional_arguments\":\n        result.stdout.fnmatch_lines([\"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\"])\n    elif fixturename == \"junit_xml_default_family\":\n        result.stdout.fnmatch_lines([\"*The 'junit_family' default value will change to 'xunit2' in pytest 6.0*\"])\n"], "sample_1086": ["def test_Transpose_printing():\n    A = MatrixSymbol(\"A\", 2, 3)\n    assert str(Transpose(A)) == \"A.T\"\n", "def test_issue_14160():\n    e = -2*x*y**(-1)\n    assert str(e) == '-2*x*y**(-1)'\n", "def test_Subs_printing_with_dict():\n    e = Subs(x, {x: 1})\n    assert str(e) == 'Subs(x, x, 1)'\n", "def test_MatPow():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert str(A**2) == \"A**2\"\n    assert str((A*A)**3) == \"(A*A)**3\"\n    assert str((A**2)**(1/2)) == \"(A**2)**(1/2)\"\n", "def test_issue_15975():\n    e = log(x**2 + y**2)\n    assert str(e.expand(complex=True)) == 'log(x**2 + y**2)'\n    assert str(e.expand(complex=False)) == 'log(x**2 + y**2)'\n"], "sample_624": ["def test_inline_dask_repr():\n    import dask.array as da\n    array = da.from_array(np.arange(12).reshape(3, 4), chunks=(2, 2))\n    expected = \"dask.array<chunksize=(2, 2), meta=np.ndarray>\"\n    assert formatting.inline_dask_repr(array) == expected\n", "def test_inline_dask_repr() -> None:\n    import dask.array as da\n\n    # Test for dask array with meta information\n    array = da.from_array(np.array([[1, 2], [3, 4]]), chunks=(2, 2))\n    actual = formatting.inline_dask_repr(array)\n    expected = \"dask.array<chunksize=(2, 2), meta=np.ndarray>\"\n    assert actual == expected\n\n    # Test for dask array without meta information\n    array_no_meta = da.from_array(np.array([[1, 2], [3, 4]]), chunks=(2, 2))\n    array_no_meta = array_no_meta.rechunk((1, 1))  # This removes the meta information\n    actual = formatting.inline_dask_repr(array_no_meta)\n    expected = \"dask.array<chunksize=(1, 1)>\"\n    assert actual == expected\n", "def test_short_data_repr():\n    # Test with a numpy array\n    arr = np.array([1, 2, 3])\n    expected_output = 'array([1, 2, 3], dtype=int64)'\n    assert formatting.short_data_repr(arr) == expected_output\n\n    # Test with an xarray Variable\n    var = xr.Variable('x', [1, 2, 3])\n    expected_output = 'array([1, 2, 3], dtype=int64)'\n    assert formatting.short_data_repr(var) == expected_output\n\n    # Test with an xarray DataArray with large size\n    da = xr.DataArray(np.arange(1000), dims='x')\n    expected_output = '[1000 values with dtype=int64]'\n    assert formatting.short_data_repr(da) == expected_output\n", "def test_lazy_array_wont_compute_html() -> None:\n    from xarray.core.indexing import LazilyIndexedArray\n\n    class LazilyIndexedArrayNotComputable(LazilyIndexedArray):\n            raise NotImplementedError(\"Computing this array is not possible.\")\n\n    arr = LazilyIndexedArrayNotComputable(np.array([1, 2]))\n    var = xr.DataArray(arr)\n\n    # These will crash if var.data are converted to numpy arrays:\n    var._repr_html_()\n", "def test_format_item_bytes_behavior() -> None:\n    cases = [\n        (b\"\\x00\\x01\\x02\\x03\", \"b'\\\\x00\\\\x01\\\\x02\\\\x03'\"),\n        (b\"\\x04\\x05\\x06\\x07\\x08\", \"b'\\\\x04\\\\x05\\\\x06\\\\x07\\\\x08'\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_item(item, quote_strings=False)\n        assert expected == actual\n"], "sample_214": ["def test_key_exact(self):\n    self.assertIs(NullableJSONModel.objects.filter(value__foo__exact='bar').exists(), True)\n    self.assertIs(NullableJSONModel.objects.filter(value__foo__exact='\"bar\"').exists(), False)\n", "def test_key_transform_exact(self):\n    self.assertIs(NullableJSONModel.objects.filter(value__foo=KeyTransform('foo', 'value')).exists(), True)\n", "def test_key_transform_lt(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lt=15),\n        [self.objs[3], self.objs[4]],\n    )\n", "def test_deep_lookup_with_integer_key(self):\n    obj = NullableJSONModel.objects.create(value={'data': {1: 'foo'}})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__data__1='foo'),\n        [obj],\n    )\n", "def test_key_transform_exact(self):\n    tests = [\n        (Q(value__baz__a__exact='b'), self.objs[7]),\n        (Q(value__baz__exact={'a': 'b', 'c': 'd'}), self.objs[7]),\n        (Q(value__d__1__exact={'f': 'g'}), self.objs[4]),\n        (Q(value__exact={'n': [None]}), self.objs[4]),\n        (Q(value__exact={'j': None}), self.objs[4]),\n    ]\n    for condition, expected in tests:\n        with self.subTest(condition=condition):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(condition),\n                [expected],\n            )\n"], "sample_1033": ["def test_Mul_does_not_distribute_negative_infinity():\n    a, b = symbols('a b')\n    assert ((1 + I)*(-oo)).is_Mul\n    assert ((a + b)*(-oo)).is_Mul\n    assert ((a + 1)*(-zoo)).is_Mul\n    assert ((1 + I)*(-oo)).is_finite is False\n    z = (1 + I)*(-oo)\n    assert ((1 - I)*z).expand() is -oo\n", "def test_Mul_does_not_distribute_nan():\n    a, b = symbols('a b')\n    assert ((1 + I)*nan).is_Mul\n    assert ((a + b)*nan).is_Mul\n    assert ((a + 1)*nan).is_Mul\n    assert ((1 + I)*nan).is_nan\n    z = (1 + I)*nan\n    assert ((1 - I)*z).expand() is nan\n", "def test_Mul_is_zero():\n    x, y = symbols('x y', zero=True)\n    assert (x * y).is_zero\n\n    z = symbols('z', zero=False)\n    assert (x * z).is_zero\n    assert (z * x).is_zero\n\n    a, b = symbols('a b', nonzero=True)\n    assert (a * b).is_zero is False\n", "def test_Add_is_real():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=False, complex=True)\n    z = Symbol('z', zero=True)\n\n    e = z + x\n    assert e.is_real is True\n\n    e = y + x\n    assert e.is_real is False\n\n    e = z + y\n    assert e.is_real is False\n\n    e = z + z\n    assert e.is_real is True\n", "def test_Add_is_zero_with_symbol():\n    x = Symbol('x')\n    assert (x + (-x)).is_zero\n    assert (x + (-x) + x).is_zero is False\n    assert (x + (-x) + x + (-x)).is_zero\n"], "sample_1093": ["def test_log2_log1p():\n    from sympy import log2, log1p\n\n    expr1 = log2(x)\n    expr2 = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(x)/numpy.log(2)'\n    assert prntr.doprint(expr2) == 'numpy.log(x + 1)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log2(x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log(x)/math.log(2)'\n    assert prntr.doprint(expr2) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log(x, 2)'\n    assert prntr.doprint(expr2) == 'mpmath.log(x + 1)'\n", "def test_NumPyPrinter_print_Pow():\n    n = NumPyPrinter()\n\n    assert n._print_Pow(x**(-2)) == 'x**(-2.0)'\n", "def test_PythonCodePrinter_rational():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(Rational(1, 3)) == '1/3'\n    assert prntr.doprint(Rational(2, 5)) == '2/5'\n", "def test_PythonCodePrinter_user_functions():\n        return 2 * x\n\n    prntr = PythonCodePrinter({'user_functions': {'my_func': 'mymodule.my_func'}})\n\n    assert prntr.doprint(my_func(x)) == 'mymodule.my_func(x)'\n    assert prntr.module_imports == {'mymodule': {'my_func'}}\n\n    prntr = PythonCodePrinter({'user_functions': {'my_func': 'my_func'}})\n\n    assert prntr.doprint(my_func(x)) == 'my_func(x)'\n    assert not prntr.module_imports\n", "def test_NumPyPrinter_print_MatrixBase():\n    n = NumPyPrinter()\n\n    # Test with a custom MatrixBase subclass\n    class CustomMatrix(MatrixSymbol):\n        pass\n\n    C = CustomMatrix(\"C\", 2, 2)\n    assert n.doprint(C) == 'numpy.array(C.tolist())'\n"], "sample_728": ["def test_make_multilabel_classification_sparse():\n    X, Y = make_multilabel_classification(n_samples=25, n_features=20,\n                                          n_classes=3, random_state=0,\n                                          sparse=True)\n    assert_equal(X.shape, (25, 20), \"X shape mismatch\")\n    assert_true(sp.issparse(X))\n    assert_equal(Y.shape, (25, 3), \"Y shape mismatch\")\n", "def test_make_classification_random_state():\n    X1, y1 = make_classification(n_samples=100, random_state=0)\n    X2, y2 = make_classification(n_samples=100, random_state=0)\n    assert_array_equal(X1, X2, \"Random state does not produce consistent output\")\n    assert_array_equal(y1, y2, \"Random state does not produce consistent output\")\n\n    X3, y3 = make_classification(n_samples=100, random_state=1)\n    assert_false(np.array_equal(X1, X3), \"Different random states should produce different output\")\n    assert_false(np.array_equal(y1, y3), \"Different random states should produce different output\")\n", "def test_make_classification_redundant_features():\n    \"\"\"Test the construction of redundant features in make_classification\"\"\"\n    # Create dataset with redundant features and check that they are linearly\n    # dependent on the informative features\n    n_samples = 100\n    n_features = 20\n    n_informative = 5\n    n_redundant = 10\n    n_repeated = 0\n    n_classes = 3\n    n_clusters_per_class = 1\n    class_sep = 1.0\n\n    make = partial(make_classification, class_sep=class_sep,\n                   n_repeated=n_repeated, flip_y=0, shift=0, scale=1,\n                   shuffle=False)\n\n    X, y = make(n_samples=n_samples, n_classes=n_classes,\n                n_features=n_features, n_informative=n_informative,\n                n_redundant=n_redundant, n_clusters_per_class=n_clusters_per_class,\n                hypercube=True, random_state=0)\n\n    assert_equal(X.shape, (n_samples, n_features))\n    assert_equal(y.shape, (n_samples,))\n\n    # Check that redundant features are linearly dependent on informative features\n    informative_features = X[:, :n_informative]\n    redundant_features = X[:, n_informative:n_informative+n_redundant]\n\n    for i in range(n_redundant):\n        coef = np.linalg.lstsq(informative_features, redundant_features[:, i], rcond=None)[0]\n        assert_array_almost_equal(redundant_features[:, i], np.dot(informative_features, coef))\n", "def test_make_classification_random_state():\n    \"\"\"Test the deterministic nature of make_classification with random_state\"\"\"\n    X1, y1 = make_classification(n_samples=100, n_features=20, random_state=0)\n    X2, y2 = make_classification(n_samples=100, n_features=20, random_state=0)\n    assert_array_equal(X1, X2, err_msg=\"Different samples generated with the same random_state\")\n    assert_array_equal(y1, y2, err_msg=\"Different labels generated with the same random_state\")\n", "def test_make_classification_shift_scale():\n    \"\"\"Test `shift` and `scale` parameters in make_classification\n\n    Also tests `flip_y`.\n    \"\"\"\n    # Test that default (shift, scale) is within (-class_sep, class_sep)\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                               n_redundant=0, n_repeated=0, n_classes=2,\n                               n_clusters_per_class=1, hypercube=True,\n                               class_sep=1.0, shift=None, scale=None,\n                               flip_y=0.0, random_state=0)\n\n    assert_array_less(X.min(axis=0), [1.0] * 2)\n    assert_array_less([-1.0] * 2, X.max(axis=0))\n\n    # Test that we can move the centroid to (20, 20)\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                               n_redundant=0, n_repeated=0, n_classes=2,\n                               n_clusters_per_class=1, hypercube=True,\n                               class_sep=1.0, shift=(20.0, 20.0), scale=None,\n                               flip_y=0.0, random_state=0)\n\n    assert_array_less(X.min(axis=0), [21.0] * 2)\n    assert_array_less([19.0] * 2, X.max(axis=0))\n\n    # Test that we can scale the data by 10\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                               n_redundant=0, n_repeated=0, n_classes=2,\n                               n_clusters_per_class=1"], "sample_702": ["def test_testdir_makefile_ext_empty_string_raises_value_error(testdir) -> None:\n    \"\"\"For backwards compat #8192\"\"\"\n    with pytest.raises(ValueError):\n        testdir.makefile(\"\", \"content\")\n", "def test_testdir_makefile_ext_no_dot_prefixes_extension_with_warning(testdir) -> None:\n    with pytest.raises(\n        ValueError,\n        match=\"pytester.makefile expects a file extension, try .foo instead of foo\",\n    ):\n        testdir.makefile(\"foo\", \"\")\n", "def test_pytester_makefile_ext_empty_string_makes_file(pytester: Pytester) -> None:\n    \"\"\"Test makefile with an empty string extension\"\"\"\n    p1 = pytester.makefile(\"\", \"\")\n    assert \"test_pytester_makefile_ext_empty_string_makes_file.py\" in str(p1)\n", "def test_pytester_run_with_stdin(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\"def test_stdin(capsys): import sys; print(sys.stdin.read())\")\n    result = pytester.runpytest_subprocess(testfile, stdin=b\"test input\")\n    assert result.ret == ExitCode.OK\n    assert result.stdout.str().strip() == \"test input\"\n", "def test_pytester_parse_summary_nouns(pytester: Pytester) -> None:\n    p1 = pytester.makepyfile(\"def test_pass(): assert True\")\n    p2 = pytester.makepyfile(\"def test_fail(): assert False\")\n    p3 = pytester.makepyfile(\"def test_skip(): pytest.skip()\")\n    p4 = pytester.makepyfile(\"def test_warning(): warnings.warn('warning')\")\n    p5 = pytester.makepyfile(\"def test_error(): raise Exception('error')\")\n    result = pytester.runpytest(str(p1), str(p2), str(p3), str(p4), str(p5))\n    assert result.parseoutcomes() == {\"passed\": 1, \"failed\": 1, \"skipped\": 1, \"warnings\": 1, \"errors\": 1}\n"], "sample_1020": ["def test_Sum():\n    assert mcode(Sum(x**k, (k, 0, n))) == \"Hold[Sum[x^k, {k, 0, n}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**i, (i, 1, n))) == \"Hold[Sum[x^i, {i, 1, n}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**n, (n, 1, 10))) == \"Hold[Sum[x^n, {n, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**n, (n, 0, oo))) == \"Hold[Sum[x^n, {n, 0, Infinity}]]\"\n    assert mcode(Sum(x**n, (n, 1, 5))) == \"Hold[Sum[x^n, {n, 1, 5}]]\"\n    assert mcode(Sum(x**n*y**n, (n, 0, oo))) == \"Hold[Sum[x^n*y^n, {n, 0, Infinity}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**k, (k, 1, n))) == \"Hold[Sum[x^k, {k, 1, n}]]\"\n"], "sample_237": ["def test_is_anonymous_authenticated_attributes(self):\n    \"\"\"\n    <User Model>.is_anonymous/is_authenticated should be attributes, not methods.\n    \"\"\"\n    class CustomUserWithMethods(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        is_anonymous = False\n        is_authenticated = True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_is_staff_method(self):\n    \"\"\"\n    <User Model>.is_staff must be a method.\n    \"\"\"\n    class CustomUserMethodIsStaff(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        @property\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Critical(\n            '%s.is_staff must be a method rather than a property or attribute. '\n            'Ignoring this is a security issue as staff users will be treated '\n            'as non-staff!' % CustomUserMethodIsStaff,\n            obj=CustomUserMethodIsStaff,\n            id='auth.C011',\n        ),\n    ])\n", "def test_valid_user_model(self):\n    \"\"\"Test a valid user model.\"\"\"\n    class CustomUserValid(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        date_of_birth = models.DateField()\n\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = ['date_of_birth']\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_required_fields_not_method(self):\n    \"\"\"REQUIRED_FIELDS should not be a method.\"\"\"\n    class CustomUserMethodRequiredFields(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        date_of_birth = models.DateField()\n\n        USERNAME_FIELD = 'username'\n            return ['date_of_birth']\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserMethodRequiredFields,\n            id='auth.E001',\n        ),\n    ])\n", "def test_is_active_method(self):\n    \"\"\"\n    <User Model>.is_active must be an attribute or property rather than a method.\n    \"\"\"\n    class CustomUserWithMethodIsActive(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Critical(\n            '%s.is_active must be an attribute or property rather than a '\n            'method. Ignoring this is a security issue as inactive users will '\n            'be treated as active!' % CustomUserWithMethodIsActive,\n            obj=CustomUserWithMethodIsActive,\n            id='auth.C011',\n        ),\n    ])\n"], "sample_978": ["def test_multiplicity_degree_2():\n    d = 2\n    knots = [0, 0, 0, 1, 2, 3, 3, 3]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((x**2/2, Interval(0, 1).contains(x)),\n                   (0, True))\n    b1 = Piecewise((-x**2 + 3*x - Rational(3, 2), Interval(1, 2).contains(x)),\n                   (0, True))\n    b2 = Piecewise((x**2/2 - 3*x + 9/2, Interval(2, 3).contains(x)),\n                   (0, True))\n    assert splines[0] == b0\n    assert splines[1] == b1\n    assert splines[2] == b2\n", "def test_discontinuous_degree_2():\n    d = 2\n    knots = [0, 0, 0, 2, 3, 3, 4, 5, 5, 5]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((x**2/2, Interval(0, 2).contains(x)),\n                   (0, True))\n    b1 = Piecewise((Rational(9, 2) - 3*x + x**2/2, Interval(2, 3).contains(x)),\n                   (0, True))\n    b2 = Piecewise((0, True))\n    b3 = Piecewise((Rational(32, 3) - 8*x + x**2/2, Interval(3, 4).contains(x)),\n                   (0, True))\n    assert splines[0] == b0\n    assert splines[1] == b1\n    assert splines[2] == b2\n    assert splines[3] == b3\n", "def test_basic_degree_2_with_fractions():\n    d = 2\n    knots = [0, Rational(1, 2), 1]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((x**2, And(x >= 0, x <= Rational(1, 2))),\n                   (Rational(3, 4) - x**2, And(x > Rational(1, 2), x <= 1)),\n                   (0, True))\n    assert splines[0] == b0\n", "def test_zero_degree_with_fractions():\n    d = 0\n    knots = [0, Rational(1, 2), 1]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((1, And(x >= 0, x <= Rational(1, 2))), (0, True))\n    assert splines[1] == Piecewise((1, And(x >= Rational(1, 2), x <= 1)), (0, True))\n", "def test_edge_cases():\n    # Test degree 0 with one knot\n    d = 0\n    knots = [0]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == 0  # There should be no basis functions\n\n    # Test degree 1 with two knots\n    d = 1\n    knots = [0, 1]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((1, Interval(0, 1).contains(x)), (0, True))\n\n    # Test degree 2 with three knots\n    d = 2\n    knots = [0, 1, 2]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == 1  # There should be only one basis function\n    assert splines[0] == Piecewise((x, Interval(0, 1).contains(x)), (0, True))\n\n    # Test degree 0 with non-integer knots\n    d = 0\n    knots = [0.5, 1.5, 2.5]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((1, Interval(0.5, 1.5).contains(x)), (0, True))\n    assert splines[1] == Piecewise((1, Interval(1.5, 2.5).contains(x)), (0, True))\n"], "sample_66": ["def test_get_host_without_allowed_hosts(self):\n    request = HttpRequest()\n    request.META = {\n        'HTTP_HOST': 'example.com',\n        'SERVER_NAME': 'internal.com',\n        'SERVER_PORT': 80,\n    }\n    with self.assertRaises(DisallowedHost):\n        request.get_host()\n", "    def test_parsing_header_name(self):\n        self.assertEqual(HttpHeaders.parse_header_name('HTTP_USER_AGENT'), 'User-Agent')\n        self.assertEqual(HttpHeaders.parse_header_name('HTTP_CONTENT_TYPE'), 'Content-Type')\n        self.assertEqual(HttpHeaders.parse_header_name('CONTENT_TYPE'), 'Content-Type')\n        self.assertEqual(HttpHeaders.parse_header_name('CONTENT_LENGTH'), 'Content-Length')\n        self.assertEqual(HttpHeaders.parse_header_name('HTTP_NOT_A_HEADER'), None)\n        self.assertEqual(HttpHeaders.parse_header_name('NOT_A_HEADER'), None)\n", "def test_build_absolute_uri_with_allowed_host(self):\n    request = HttpRequest()\n    request.get_host = lambda: 'example.com'\n    request.path = '/somepath/'\n    self.assertEqual(\n        request.build_absolute_uri(location='/path/with:colons'),\n        'http://example.com/path/with:colons'\n    )\n", "def test_http_headers_parsing(self):\n    environ = {\n        'HTTP_ACCEPT': '*',\n        'HTTP_USER_AGENT': 'python-requests/1.2.0',\n        'HTTP_X_FORWARDED_FOR': '127.0.0.1',\n        'HTTP_X_CUSTOM_HEADER': 'custom-value',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(headers['Accept'], '*')\n    self.assertEqual(headers['User-Agent'], 'python-requests/1.2.0')\n    self.assertEqual(headers['X-Forwarded-For'], '127.0.0.1')\n    self.assertEqual(headers['X-Custom-Header'], 'custom-value')\n", "def test_non_ascii_headers(self):\n    environ = {\n        'HTTP_USER_AGENT': 'Python-urllib/3.x',\n        'HTTP_ACCEPT_LANGUAGE': 'en-US,en;q=0.8,fr;q=0.6',\n        'HTTP_ACCEPT_ENCODING': 'gzip, deflate, br',\n        'HTTP_REFERER': 'https://www.djangoproject.com/',\n        'HTTP_COOKIE': 'sessionid=abc123; csrftoken=xyz789',\n        'HTTP_ACCEPT': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'HTTP_ACCEPT_CHARSET': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n        'HTTP_CACHE_CONTROL': 'max-age=0',\n        'HTTP_CONNECTION': 'keep-alive',\n        'HTTP_UPGRADE_INSECURE_REQUESTS': '1',\n        'HTTP_X_REQUESTED_WITH': 'XMLHttpRequest',\n        'HTTP_SEC_FETCH_DEST': 'document',\n        'HTTP_SEC_FETCH_MODE': 'navigate',\n        'HTTP_SEC_FETCH_SITE': 'none',\n        'HTTP_SEC_FETCH_USER': '?1',\n        'HTTP_IF_NONE_MATCH': 'W/\"53b1-14e5b603557\"',\n        'HTTP_IF_MODIFIED_SINCE': 'Thu, 31 Dec 2015 23:59:59 GMT',\n        'HTTP_TE': 'trailers',\n        'HTTP_DNT': '1',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(headers['User-Agent'], 'Python-urllib/3.x')\n    self.assertEqual(headers['Accept-Language'], 'en"], "sample_24": ["def test_trim_zeros(self):\n    a = np.array([0, 0, 1, 2, 0, 0, 0])\n    mask_a = np.array([True, False, False, False, False, True, False])\n    ma = Masked(a, mask=mask_a)\n    out = np.trim_zeros(ma)\n    expected = np.trim_zeros(a)\n    expected_mask = np.trim_zeros(mask_a)\n    assert_array_equal(out.unmasked, expected)\n    assert_array_equal(out.mask, expected_mask)\n", "def test_nan_to_num_out_of_place(self):\n    o = np.nan_to_num(self.ma, copy=True)\n    assert o is not self.ma\n    assert_masked_equal(o, Masked([0.0, 1.0], mask=[True, False]))\n", "def test_pad(self):\n    ma = self.ma.copy()\n    pad_width = ((1, 1), (0, 0))\n    constant_values = Masked(-1, mask=True)\n    out = np.pad(ma, pad_width, mode='constant', constant_values=constant_values)\n    expected = np.pad(self.a, pad_width, mode='constant', constant_values=-1)\n    expected_mask = np.pad(self.mask_a, pad_width, mode='constant', constant_values=True)\n    assert_array_equal(out.unmasked, expected)\n    assert_array_equal(out.mask, expected_mask)\n", "def test_trim_zeros(self):\n    o = np.trim_zeros(self.ma)\n    expected = np.trim_zeros(self.a)\n    assert_array_equal(o.unmasked, expected)\n    assert_array_equal(o.mask, np.zeros(expected.shape, dtype=bool))\n\n    o2 = np.trim_zeros(self.ma, trim='b')\n    expected2 = np.trim_zeros(self.a, trim='b')\n    assert_array_equal(o2.unmasked, expected2)\n    assert_array_equal(o2.mask, np.zeros(expected2.shape, dtype=bool))\n", "def test_trim_zeros(self):\n    a = np.array([0, 0, 1, 2, 0, 0])\n    mask_a = np.array([True, False, False, False, False, True])\n    ma = Masked(a, mask=mask_a)\n    out = np.trim_zeros(ma)\n    expected = np.trim_zeros(a)\n    expected_mask = np.trim_zeros(mask_a)\n    assert_array_equal(out.unmasked, expected)\n    assert_array_equal(out.mask, expected_mask)\n"], "sample_218": ["def test_trunc_time_func_with_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            extracted=TruncTime('start_datetime', tzinfo=melb)\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, start_datetime.astimezone(melb).time()),\n            (end_datetime, end_datetime.astimezone(melb).time()),\n        ],\n        lambda m: (m.start_datetime, m.extracted)\n    )\n    self.assertEqual(DTModel.objects.filter(start_datetime__time=TruncTime('start_datetime', tzinfo=melb)).count(), 2)\n", "def test_trunc_func_with_timezone_and_output_field(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb).astimezone(timezone.utc)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb).astimezone(timezone.utc))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('year')\n    test_datetime_kind('quarter')\n    test_datetime_kind('month')\n    test_datetime_kind('week')\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n    test_datetime_kind('minute')\n    test_datetime_kind('second')\n", "def test_extract_time_and_trunc_time_with_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    # Test ExtractTime\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            extracted_hour=ExtractHour('start_datetime', tzinfo=melb),\n            extracted_minute=ExtractMinute('start_datetime', tzinfo=melb),\n            extracted_second=ExtractSecond('start_datetime', tzinfo=melb),\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, start_datetime.astimezone(melb).hour, start_datetime.astimezone(melb).minute, start_datetime.astimezone(melb).second),\n            (end_datetime, end_datetime.astimezone(melb).hour, end_datetime.astimezone(melb).minute, end_datetime.astimezone(melb).second),\n        ],\n        lambda m: (m.start_datetime, m.extracted_hour, m.extracted_minute, m.extracted_second)\n    )\n\n    # Test TruncTime\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            truncated_time=TruncTime('start_datetime', tzinfo=melb),\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, truncate_to(start_datetime.astimezone(melb).time(), 'minute', melb)),\n            (end", "def test_trunc_func_with_invalid_tzinfo(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    invalid_tzinfo = 'Invalid/Timezone'\n    msg = 'Invalid timezone'\n    with self.assertRaisesMessage(pytz.UnknownTimeZoneError, msg):\n        list(DTModel.objects.annotate(truncated=Trunc('start_datetime', 'year', output_field=DateTimeField(), tzinfo=invalid_tzinfo)))\n", "def test_extract_timezone_aware_datetime(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321, tzinfo=pytz.utc)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123, tzinfo=pytz.utc)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            extracted=Extract('start_datetime', 'hour', tzinfo=melb)\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, (start_datetime.astimezone(melb)).hour),\n            (end_datetime, (end_datetime.astimezone(melb)).hour),\n        ],\n        lambda m: (m.start_datetime, m.extracted)\n    )\n"], "sample_813": ["def test_ard_scores():\n    \"\"\"Check scores attribute shape\"\"\"\n    X = np.array([[1], [2], [3]])\n    Y = np.array([1, 2, 3])\n    clf = ARDRegression(compute_score=True)\n    clf.fit(X, Y)\n\n    assert clf.scores_[-1] == clf._log_marginal_likelihood(\n        n_samples=X.shape[0],\n        n_features=X.shape[1],\n        eigen_vals=np.linalg.eigvalsh(np.dot(X.T, X)),\n        alpha_=clf.alpha_,\n        lambda_=clf.lambda_,\n        coef=clf.coef_,\n        rmse=np.sum((Y - np.dot(X, clf.coef_)) ** 2)\n    )\n", "def test_bayesian_ridge_normalize():\n    # Test BayesianRidge with normalize=True\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([2, 4, 6, 8, 10])\n    clf = BayesianRidge(normalize=True)\n    clf.fit(X, y)\n\n    # Check that the model could approximately learn the linear function\n    test = [[11, 12], [13, 14]]\n    assert_array_almost_equal(clf.predict(test), [12, 14], 1)\n", "def test_ard_regression_scores():\n    \"\"\"Check scores attribute shape\"\"\"\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([1, 2, 6, 8, 10])\n\n    clf = ARDRegression(compute_score=True)\n    clf.fit(X, y)\n\n    assert len(clf.scores_) == clf.n_iter\n", "def test_bayesian_ridge_prediction_with_std():\n    # Test the prediction with return_std option for BayesianRidge\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(compute_score=True)\n    clf.fit(X, Y)\n\n    # Check the shape of the output when return_std is True\n    test = [[1], [3], [4]]\n    y_pred, y_std = clf.predict(test, return_std=True)\n    assert_equal(y_pred.shape, (3,))\n    assert_equal(y_std.shape, (3,))\n", "def test_zero_variance_input():\n    # Test BayesianRidge and ARDRegression for edge case of zero variance input\n    n_samples = 10\n    n_features = 5\n    X = np.zeros((n_samples, n_features))\n    y = np.ones(n_samples)\n    expected_coef = np.zeros(n_features)\n    expected_intercept = np.mean(y)\n\n    for clf in [BayesianRidge(), ARDRegression()]:\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.coef_, expected_coef)\n        assert_almost_equal(clf.intercept_, expected_intercept)\n"], "sample_736": ["def test_saga_vs_liblinear_intercept():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X = np.concatenate([X] * 10)\n    y = np.concatenate([y] * 10)\n\n    X_bin = X[y <= 1]\n    y_bin = y[y <= 1] * 2 - 1\n\n    X_sparse, y_sparse = make_classification(n_samples=50, n_features=20,\n                                             random_state=0)\n    X_sparse = sparse.csr_matrix(X_sparse)\n\n    for (X, y) in ((X_bin, y_bin), (X_sparse, y_sparse)):\n        for penalty in ['l1', 'l2']:\n            n_samples = X.shape[0]\n            # alpha=1e-3 is time consuming\n            for alpha in np.logspace(-1, 1, 3):\n                saga = LogisticRegression(\n                    C=1. / (n_samples * alpha),\n                    solver='saga',\n                    multi_class='ovr',\n                    max_iter=200,\n                    fit_intercept=True,\n                    penalty=penalty, random_state=0, tol=1e-24)\n\n                liblinear = LogisticRegression(\n                    C=1. / (n_samples * alpha),\n                    solver='liblinear',\n                    multi_class='ovr',\n                    max_iter=200,\n                    fit_intercept=True,\n                    penalty=penalty, random_state=0, tol=1e-24)\n\n                saga.fit(X, y)\n                liblinear.fit(X, y)\n                # Convergence for alpha=1e-3 is very slow\n                assert_array_almost_equal(saga.coef_, liblinear.coef_, 3)\n                assert", "def test_saga_vs_liblinear_multinomial():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X = np.concatenate([X] * 10)\n    y = np.concatenate([y] * 10)\n\n    for penalty in ['l1', 'l2']:\n        n_samples = X.shape[0]\n        # alpha=1e-3 is time consuming\n        for alpha in np.logspace(-1, 1, 3):\n            saga = LogisticRegression(\n                C=1. / (n_samples * alpha),\n                solver='saga',\n                multi_class='multinomial',\n                max_iter=200,\n                fit_intercept=False,\n                penalty=penalty, random_state=0, tol=1e-24)\n\n            lbfgs = LogisticRegression(\n                C=1. / (n_samples * alpha),\n                solver='lbfgs',\n                multi_class='multinomial',\n                max_iter=200,\n                fit_intercept=False,\n                penalty=penalty, random_state=0, tol=1e-24)\n\n            saga.fit(X, y)\n            lbfgs.fit(X, y)\n            # Convergence for alpha=1e-3 is very slow\n            assert_array_almost_equal(saga.coef_, lbfgs.coef_, 3)\n", "def test_logreg_predict_proba_multinomial_binary():\n    X, y = make_classification(n_samples=10, n_features=20, random_state=0,\n                               n_classes=2, n_informative=10)\n\n    clf_multi = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n    clf_multi.fit(X, y)\n    clf_multi_loss = log_loss(y, clf_multi.predict_proba(X))\n    clf_ovr = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\")\n    clf_ovr.fit(X, y)\n    clf_ovr_loss = log_loss(y, clf_ovr.predict_proba(X))\n    assert_almost_equal(clf_ovr_loss, clf_multi_loss, decimal=5)\n", "def test_logreg_intercept_scaling_value():\n    # Test that intercept_scaling affects the value of the intercept\n\n    clf = LogisticRegression(fit_intercept=True, intercept_scaling=2.0)\n    clf.fit(X, Y1)\n    assert_equal(clf.intercept_[0], 2.0 * clf.coef_[0][-1])\n\n    clf = LogisticRegression(fit_intercept=True, intercept_scaling=0.5)\n    clf.fit(X, Y1)\n    assert_equal(clf.intercept_[0], 0.5 * clf.coef_[0][-1])\n", "def test_logreg_intercept_scaling_effect():\n    # Test that intercept_scaling has the expected effect on the intercept.\n    # Here, we test that a large intercept_scaling results in a larger\n    # absolute value for the intercept compared to a smaller intercept_scaling.\n\n    clf_large = LogisticRegression(fit_intercept=True, intercept_scaling=1000)\n    clf_large.fit(X, Y1)\n    clf_small = LogisticRegression(fit_intercept=True, intercept_scaling=1)\n    clf_small.fit(X, Y1)\n\n    assert_true(np.abs(clf_large.intercept_) > np.abs(clf_small.intercept_))\n"], "sample_110": ["def test_filter_with_nested_in_lookup(self):\n    groups = Group.objects.filter(id__in=[1, 2])\n    events = Event.objects.filter(group__in=groups)\n    self.assert_pickles(events)\n", "def test_filter_with_q_object(self):\n    g1 = Group.objects.create(name=\"Group 1\")\n    g2 = Group.objects.create(name=\"Group 2\")\n    Event.objects.create(title=\"Event 1\", group=g1)\n    Event.objects.create(title=\"Event 2\", group=g2)\n\n    qs = Event.objects.filter(Q(group=g1) | Q(group=g2))\n    self.assert_pickles(qs)\n", "def test_pickle_window_queryset_not_evaluated(self):\n    group = Group.objects.create(name='group')\n    Event.objects.create(title='event1', group=group)\n    Event.objects.create(title='event2', group=group)\n    events = Event.objects.annotate(\n        event_rank=models.Window(\n            expression=models.RowNumber(),\n            order_by=models.F('title').asc(),\n        ),\n    )\n    list(events)  # evaluate QuerySet.\n    with self.assertNumQueries(0):\n        self.assert_pickles(events)\n", "def test_window_expression(self):\n    event1 = Event.objects.create(title='Event 1', group=Group.objects.first(), rank=1)\n    event2 = Event.objects.create(title='Event 2', group=Group.objects.first(), rank=2)\n    event3 = Event.objects.create(title='Event 3', group=Group.objects.first(), rank=3)\n    events = Event.objects.annotate(\n        rank_over_group=models.Window(\n            expression=models.F('rank'),\n            partition_by=models.F('group'),\n            order_by=models.F('rank'),\n        ),\n    ).order_by('rank')\n    self.assert_pickles(events)\n", "def test_window_expression(self):\n    Happening.objects.create(name=\"Event 1\", when=datetime.datetime(2022, 1, 1))\n    Happening.objects.create(name=\"Event 2\", when=datetime.datetime(2022, 1, 2))\n\n    qs = Happening.objects.annotate(\n        prev_event=models.Window(\n            expression=models.F('name'),\n            order_by=models.F('when').desc(),\n            frame=models.RowRange(start=-1, end=-1)\n        )\n    )\n\n    with self.assertNumQueries(0):\n        pickle.loads(pickle.dumps(qs))\n\n    with self.assertNumQueries(1):\n        self.assertEqual(list(qs), list(Happening.objects.all()))\n"], "sample_166": ["def test_get_random_string_with_length(self):\n    length = 22\n    random_string = get_random_string(length)\n    self.assertEqual(len(random_string), length)\n", "def test_get_random_string_length_provided(self):\n    length = 22\n    random_string = get_random_string(length)\n    self.assertEqual(len(random_string), length)\n", "    def test_get_random_string_length(self):\n        for length in range(1, 20):\n            with self.subTest(length=length):\n                self.assertEqual(len(get_random_string(length)), length)\n", "    def test_get_random_string_length_argument(self):\n        length = 22\n        random_string = get_random_string(length)\n        self.assertEqual(len(random_string), length)\n", "def test_get_random_string_length(self):\n    length = 22\n    result = get_random_string(length=length)\n    self.assertEqual(len(result), length)\n"], "sample_209": ["def test_model_with_custom_manager(self):\n    \"\"\"\n    You can filter by objects that have a custom manager\n    \"\"\"\n    class CustomManager(models.Manager):\n        pass\n\n    class CustomModel(models.Model):\n        name = models.CharField(max_length=255)\n        objects = CustomManager()\n\n    custom_obj = CustomModel.objects.create(name='abc')\n    CustomModel.objects.filter(name=custom_obj.name)\n", "def test_model_with_evaluate_method_queryset_exclusion(self):\n    \"\"\"\n    You can exclude objects from queryset that have an 'evaluate' attr\n    \"\"\"\n    dept1 = Department.objects.create(pk=1, name='abc')\n    dept1.evaluate = 'abc'\n    dept2 = Department.objects.create(pk=2, name='def')\n    dept2.evaluate = 'def'\n    queryset = Worker.objects.all()\n    filtered_queryset = queryset.exclude(department=dept1)\n    self.assertNotIn(dept1, filtered_queryset.values_list('department', flat=True))\n", "def test_model_copy(self):\n    # Test that copying a model instance works correctly\n    orig = Worker.objects.create(name=\"Original\", department=Department.objects.create(name=\"HR\"))\n    copy = copy.copy(orig)\n    self.assertIsNot(orig, copy)\n    self.assertEqual(orig.name, copy.name)\n    self.assertEqual(orig.department, copy.department)\n    self.assertIsNone(copy.pk)\n", "def test_get_next_prev_in_order_unsaved(self):\n    msg = 'get_next/get_previous cannot be used on unsaved objects.'\n    with self.assertRaisesMessage(ValueError, msg):\n        Article().get_next_in_order()\n    with self.assertRaisesMessage(ValueError, msg):\n        Article().get_previous_in_order()\n", "def test_ordering_with_transform_lookup(self):\n    # Regression test for #21879: ordering with transform lookups\n    Event.objects.create(when=datetime.datetime(2000, 1, 1, 16, 0, 0))\n    Event.objects.create(when=datetime.datetime(2000, 1, 1, 6, 1, 1))\n    Event.objects.create(when=datetime.datetime(2000, 1, 1, 13, 1, 1))\n    events = Event.objects.order_by('when__hour')\n    self.assertEqual(\n        [e.when.hour for e in events],\n        [6, 13, 16]\n    )\n"], "sample_277": ["def test_combine_or_with_and(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q3 = Q(rating__gt=F('min_rating'))\n    q = q1 | (q2 & q3)\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price__gt', F('discounted_price')),\n        (\n            ('price', F('discounted_price')),\n            ('rating__gt', F('min_rating')),\n        ),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n", "def test_combine_different_connectors(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q3 = Q(price__lt=F('discounted_price'))\n    q = (q1 | q2) & q3\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        (\n            ('price__gt', F('discounted_price')),\n            ('price', F('discounted_price')),\n        ),\n        ('price__lt', F('discounted_price')),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n", "def test_combine_nested_or_and_empty(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q = (q1 | q2) & Q()\n    self.assertEqual(q, q1 | q2)\n\n    q = Q() & (q1 | q2)\n    self.assertEqual(q, q1 | q2)\n", "def test_combine_and_or(self):\n    q1 = Q(x=1)\n    q2 = Q(y=2)\n    q3 = Q(z=3)\n    q = (q1 & q2) | q3\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, ((('x', 1), ('y', 2)), ('z', 3)))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n", "def test_combine_and_or_precedence(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q3 = Q(quantity__gt=5)\n    q = q1 & q2 | q3\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price', F('discounted_price')),\n        ('price__gt', F('discounted_price')),\n        ('quantity__gt', 5),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n"], "sample_41": ["def test_unit_division_with_string():\n    \"\"\"Check that division with strings produces the correct unit.\"\"\"\n    u1 = u.cm\n    us = 'kg'\n    assert u1 / us == u1 / u.Unit(us)\n", "def test_unit_summary_physical_type():\n    \"\"\"\n    Test for a few units that the unit summary table correctly reports\n    the physical type of that unit.\n\n    Regression test for https://github.com/astropy/astropy/issues/3835\n    \"\"\"\n\n    from astropy.units import astrophys\n\n    for summary in utils._iter_unit_summary(astrophys.__dict__):\n        unit, _, _, physical_type, _ = summary\n\n        if unit.name == 'lyr':\n            assert physical_type == 'length'\n        elif unit.name == 'pc':\n            assert physical_type == 'length'\n        elif unit.name == 'barn':\n            assert physical_type == 'area'\n        elif unit.name == 'cycle':\n            assert physical_type == 'dimensionless'\n        elif unit.name == 'vox':\n            assert physical_type == 'volume'\n", "def test_unit_summary_symbols():\n    \"\"\"\n    Test for a few units that the unit summary table correctly reports\n    the symbol for that unit.\n\n    Regression test for https://github.com/astropy/astropy/issues/3835\n    \"\"\"\n\n    from astropy.units import astrophys\n\n    for summary in utils._iter_unit_summary(astrophys.__dict__):\n        unit, symbol, _, _, _ = summary\n\n        if unit.name == 'lyr':\n            assert symbol == 'lyr'\n        elif unit.name == 'pc':\n            assert symbol == 'pc'\n        elif unit.name == 'barn':\n            assert symbol == 'b'\n        elif unit.name == 'cycle':\n            assert symbol == 'cycle'\n        elif unit.name == 'vox':\n            assert symbol == 'vox'\n", "def test_unit_summary_dimensionless():\n    \"\"\"\n    Test that the unit summary table correctly reports that dimensionless units\n    have no prefixes.\n\n    Regression test for https://github.com/astropy/astropy/issues/3835\n    \"\"\"\n\n    from astropy.units import si\n\n    for summary in utils._iter_unit_summary(si.__dict__):\n        unit, _, _, _, prefixes = summary\n\n        if unit.physical_type == 'dimensionless':\n            assert prefixes == 'No'\n", "def test_unit_summary_duplicate_unit_names():\n    \"\"\"\n    Test that the unit summary table correctly handles units with\n    duplicate names.\n\n    Regression test for https://github.com/astropy/astropy/issues/3835\n    \"\"\"\n\n    from astropy.units import astrophys\n\n    # Create a duplicate unit name\n    astrophys.duplicate_unit = astrophys.pc\n\n    for summary in utils._iter_unit_summary(astrophys.__dict__):\n        unit, _, _, _, _ = summary\n\n        if unit.name == 'duplicate_unit':\n            assert unit == astrophys.pc\n"], "sample_592": ["def test_inline_dask_repr():\n    import dask.array as da\n\n    array = da.ones((3, 3), chunks=(1, 1))\n    expected = \"dask.array<chunksize=(1, 1)>\"\n    actual = formatting.inline_dask_repr(array)\n    assert expected == actual\n\n    array_with_meta = da.ones((3, 3), chunks=(1, 1), meta=np.ndarray)\n    expected = \"dask.array<chunksize=(1, 1), meta=np.ndarray>\"\n    actual = formatting.inline_dask_repr(array_with_meta)\n    assert expected == actual\n", "def test_inline_dask_repr():\n    import dask.array as da\n\n    # Test with a simple dask array\n    arr = da.from_array(np.array([1, 2, 3]), chunks=2)\n    assert formatting.inline_dask_repr(arr) == \"dask.array<chunksize=(2,)>\"\n\n    # Test with a dask array having a meta attribute\n    arr = da.from_array(pd.DataFrame([1, 2, 3]), chunks=2)\n    assert formatting.inline_dask_repr(arr) == \"dask.array<chunksize=(2,), meta=DataFrame>\"\n", "def test_summarize_coord_levels(self):\n    coord = xr.DataArray([1, 2], dims=\"x\").to_xarray().coords[\"x\"]\n    coord._level_names = (\"level1\", \"level2\")\n    coord._level_coords = {\"level1\": xr.Variable(\"x\", [3, 4])}\n    expected = \"      * level1 (x) int64 3 4\\n      level2 (x) int64 1 2\"\n    actual = formatting._summarize_coord_levels(coord, 10, \"*\")\n    assert actual == expected\n", "def test_diff_coords_repr_with_multiindex():\n    coord_a = xr.DataArray(\n        [1, 2], dims=\"x\", coords={\"x\": [\"a\", \"b\"]}, name=\"coord_a\"\n    )\n    coord_b = xr.DataArray(\n        [3, 4],\n        dims=\"y\",\n        coords={\"y\": [\"c\", \"d\"]},\n        name=\"coord_b\",\n    )\n    ds_a = xr.Dataset(coords={\"multi\": coord_a})\n    ds_b = xr.Dataset(coords={\"multi\": coord_b})\n\n    expected = dedent(\n        \"\"\"\\\n    Differing coordinates:\n    L * multi    (x) int64 1 2\n    R * multi    (y) int64 3 4\n    \"\"\"\n    ).strip()\n    actual = formatting.diff_coords_repr(ds_a.coords, ds_b.coords, \"equals\")\n    assert expected == actual\n", "def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\", \"date\", \"time\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\", \"date\", \"-24:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\", \"0 hours\", \"03:00:00\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\", \"NaT\", \"NaT\"),\n        (pd.Timedelta(\"1000 seconds\"), \"0 days 00:16:40\", \"0 days\", \"00:16:40\"),\n        (pd.Timedelta(\"500 milliseconds\"), \"0 days 00:00:00.500000\", \"0 days\", \"00:00:00.500000\"),\n    ]\n    for item, expected, date_expected, time_expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n        actual = formatting.format_timedelta(item, timedelta_format=\"date\")\n        assert date_expected == actual\n        actual = formatting.format_timedelta(item, timedelta_format=\"time\")\n        assert time_expected == actual\n"], "sample_526": ["def test_num2date_roundoff_smaller():\n    assert mdates.num2date(100000.0000578699) == datetime.datetime(\n        2243, 10, 17, 0, 0, 4, 999980, tzinfo=datetime.timezone.utc)\n", "def test_num2timedelta_array():\n    x = np.array([1, 1.5])\n    dt = mdates.num2timedelta(x)\n    expected = np.array([datetime.timedelta(days=1), datetime.timedelta(days=1.5)])\n    np.testing.assert_array_equal(dt, expected)\n", "def test_num2date_roundoff_larger():\n    assert mdates.num2date(100000.0000578704) == datetime.datetime(\n        2243, 10, 17, 0, 0, 5, 20000, tzinfo=datetime.timezone.utc)\n    # Even larger, steps of 40 microseconds\n    assert mdates.num2date(100000.0000578706) == datetime.datetime(\n        2243, 10, 17, 0, 0, 5, 40000, tzinfo=datetime.timezone.utc)\n", "def test_num2date_small_values():\n    # Test for small values near the epoch\n    assert mdates.num2date(0.0000578703) == datetime.datetime(\n        1970, 1, 1, 0, 0, 0, 57870, tzinfo=datetime.timezone.utc)\n", "def test_num2date_negative():\n    assert mdates.num2date(-100000) == datetime.datetime(\n        1036, 9, 3, 0, 0, 4, 999980, tzinfo=datetime.timezone.utc)\n    # Slightly larger, steps of 20 microseconds\n    assert mdates.num2date(-100000.0000000001) == datetime.datetime(\n        1036, 9, 3, 0, 0, 5, tzinfo=datetime.timezone.utc)\n"], "sample_289": ["    def test_setlistdefault(self):\n        x = MultiValueDict({'a': [1, 2]})\n        a = x.setlistdefault('a')\n        b = x.setlistdefault('b', [3])\n        self.assertEqual(a, [1, 2])\n        self.assertEqual(b, [3])\n        self.assertEqual(list(x.lists()), [('a', [1, 2]), ('b', [3])])\n", "    def test_setdefault_with_default(self):\n        x = CaseInsensitiveMapping({'a': '1'})\n        a = x.setdefault('A', '2')\n        b = x.setdefault('b', '3')\n        self.assertEqual(a, '1')\n        self.assertEqual(b, '3')\n        self.assertEqual(dict(x), {'a': '1', 'b': '3'})\n", "    def test_update_with_mapping(self):\n        x = MultiValueDict({'a': [1], 'b': [2]})\n        x.update({'a': 3, 'c': 4})\n        self.assertEqual(list(x.lists()), [('a', [1, 3]), ('b', [2]), ('c', [4])])\n", "    def test_setlistdefault(self):\n        x = MultiValueDict({'a': [1]})\n        a = x.setlistdefault('a', [4, 5])\n        b = x.setlistdefault('b', [3])\n        self.assertEqual(a, [1])\n        self.assertEqual(b, [3])\n        self.assertEqual(list(x.lists()), [('a', [1]), ('b', [3])])\n", "def test_setlist(self):\n    x = MultiValueDict({'a': [1], 'b': [2], 'c': [3]})\n    x.setlist('a', [4, 5])\n    x.setlist('d', [6])\n    self.assertEqual(list(x.lists()), [('a', [4, 5]), ('b', [2]), ('c', [3]), ('d', [6])])\n"], "sample_470": ["    def test_lazy_bytes_cast_text_result_type(self):\n        lazy_value = lazy(lambda: \"test\", str)()\n        self.assertEqual(bytes(lazy_value), b\"test\")\n", "def test_classproperty_decorator(self):\n    \"\"\"\n    Test that classproperty decorator works correctly.\n    \"\"\"\n    class Foo:\n        @classproperty\n            return 'bar'\n\n    self.assertEqual(Foo.bar, 'bar')\n", "def test_classproperty_getter_method_override(self):\n    class Foo:\n        @classproperty\n            return \"Original\"\n\n        foo = foo.getter(lambda cls: \"Overridden\")\n\n    self.assertEqual(Foo.foo, \"Overridden\")\n", "    def test_classproperty_getter_override(self):\n        class Foo:\n            foo_attr = 123\n\n                self.foo_attr = 456\n\n            @classproperty\n                return cls.foo_attr\n\n            @foo.getter\n                return cls.foo_attr * 2\n\n        self.assertEqual(Foo.foo, 246)\n", "def test_lazy_bytes_str_interaction(self):\n    \"\"\"\n    Test that a lazy object which can return both bytes and str doesn't\n    have its __bytes__ or __str__ methods overwritten.\n    \"\"\"\n    lazy_obj = lazy(lambda: \"test\", str, bytes)()\n    self.assertEqual(str(lazy_obj), \"test\")\n    self.assertEqual(bytes(lazy_obj), b\"test\")\n"], "sample_121": ["def test_check_constraints_with_supported_database(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n\n        class Meta:\n            constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n            required_db_features = {'supports_table_check_constraints': True}\n\n    errors = Model.check()\n    self.assertEqual(errors, [])\n", "def test_check_constraints_supported(self):\n    with self.settings(DATABASES={'default': {'ENGINE': 'django.db.backends.postgresql'}}):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n        errors = Model.check()\n        self.assertCountEqual(errors, [])\n", "def test_check_constraints_with_database(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n\n        class Meta:\n            constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n    with connection.cursor() as cursor:\n        cursor.execute(\"CREATE TABLE invalid_models_tests_model (id INTEGER PRIMARY KEY, age INTEGER)\")\n        cursor.execute(\"ALTER TABLE invalid_models_tests_model ADD CONSTRAINT is_adult CHECK (age >= 18)\")\n\n    errors = Model.check()\n    self.assertEqual(errors, [])\n\n    with connection.cursor() as cursor:\n        cursor.execute(\"DROP TABLE invalid_models_tests_model\")\n", "def test_check_constraints_supported(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n\n        class Meta:\n            constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n            required_db_features = {'supports_table_check_constraints': True}\n\n    errors = Model.check()\n    self.assertEqual(errors, [])\n", "def test_check_constraint_expression(self):\n    class Model(models.Model):\n        width = models.IntegerField()\n        height = models.IntegerField()\n\n        class Meta:\n            constraints = [\n                models.CheckConstraint(\n                    check=models.Q(width__gt=models.F('height')),\n                    name='width_gt_height'\n                ),\n            ]\n\n    # Check the model for any errors\n    errors = Model.check()\n\n    # If the database supports check constraints, no errors are expected\n    expected_errors = [] if connection.features.supports_table_check_constraints else [\n        Warning(\n            '%s does not support check constraints.' % connection.display_name,\n            hint=(\n                \"A constraint won't be created. Silence this warning if \"\n                \"you don't care about it.\"\n            ),\n            obj=Model,\n            id='models.W027',\n        )\n    ]\n\n    self.assertCountEqual(errors, expected_errors)\n"], "sample_1206": ["def test_negation_of_Float_zero():\n    assert -Float(0) is S.Zero\n", "def test_modulus():\n    assert S(10) % S(3) == 1\n    assert S(10) % Float(3.0) == 1.0\n    assert S(10.0) % S(3) == 1.0\n    assert Float(10.0) % S(3) == 1.0\n", "def test_Integer_mod():\n    assert Integer(10) % Integer(3) == 1\n    assert Integer(10) % Integer(5) == 0\n    assert Integer(10) % Integer(11) == 10\n    assert Integer(10) % Integer(0) == zoo\n", "def test_infinity_operations():\n    assert oo + oo is oo\n    assert oo - oo is nan\n    assert oo * oo is oo\n    assert oo / oo is nan\n    assert oo * I is zoo\n    assert oo / I is zoo\n", "def test_issue_17074():\n    assert Integer(0).div(Integer(0)) is zoo\n"], "sample_929": ["def test_pyfunction_signature_defaults(app):\n    text = \".. py:function:: hello(name='World') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'World'\"])])])\n", "def test_pyfunction_async_with_module(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:function:: async_func\\n\"\n            \"   :async:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"async \"],\n                                                    [desc_addname, \"example.\"],\n                                                    [desc_name, \"async_func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[3][0][2][1], pending_xref, **{\"py:module\": \"example\"})\n    assert 'example.async_func' in domain.objects\n    assert domain.objects['example.async_func'] == ('index', 'example.async_func', 'function')\n", "def test_pyfunction_signature_with_generics(app):\n    text = \".. py:function:: func(a: List[int]) -> List[str]\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"List\"],\n                                                    [desc_sig_punctuation, \"[\"],\n                                                    [pending_xref, \"str\"],\n                                                    [desc_sig_punctuation, \"]\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"a\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"List\"],\n                                                      [desc_sig_punctuation, \"[\"],\n                                                      [pending_xref, \"int\"],\n                                                      [desc_sig_punctuation, \"]\"])])\n", "def test_pyfunction_signature_with_defaults(app):\n    text = \".. py:function:: hello(name='world') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'world'\"])])])\n", "def test_pyfunction_signature_noindex(app):\n    text = \".. py:function:: hello(name: str) -> str\\n   :noindex:\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, [desc])\n    assert_node(doctree[0], addnodes.desc, desctype=\"function\", domain=\"py\", objtype=\"function\", noindex=True)\n"], "sample_674": ["def test_node_repr_failure(testdir):\n    item = testdir.getitem(\"test_fail.py\", \"\"\"\n        import pytest\n            assert False, \"Test failed\"\n    \"\"\")\n    excinfo = item.runtest()\n    result = item.repr_failure(excinfo)\n    assert \"Test failed\" in result\n", "def test_node_listextrakeywords():\n    parent = nodes.Node(\"parent\")\n    child = nodes.Node(\"child\", parent=parent)\n    parent.extra_keyword_matches.add(\"parent_keyword\")\n    child.extra_keyword_matches.add(\"child_keyword\")\n\n    assert child.listextrakeywords() == {\"parent_keyword\", \"child_keyword\"}\n", "def test_node_add_marker():\n    node = nodes.Node(\"test_node\")\n    node.add_marker(\"mark1\")\n    assert len(node.own_markers) == 1\n    assert node.own_markers[0].name == \"mark1\"\n    assert len(list(node.iter_markers(\"mark1\"))) == 1\n    node.add_marker(\"mark2\", append=False)\n    assert len(node.own_markers) == 2\n    assert node.own_markers[0].name == \"mark2\"\n    assert len(list(node.iter_markers(\"mark2\"))) == 1\n", "def test_get_fslocation_from_item(testdir):\n    item = testdir.getitems(\"def test_func():\\n    pass\")[0]\n    path, lineno = nodes.get_fslocation_from_item(item)\n    assert str(path).endswith(\"test_func.py\")\n    assert lineno == 1\n", "def test_node_init_with_nodeid():\n    node = nodes.Node(name=\"test_node\", nodeid=\"custom_nodeid\")\n    assert node.nodeid == \"custom_nodeid\"\n\n    parent_node = nodes.Node(name=\"parent_node\", nodeid=\"parent_nodeid\")\n    child_node = nodes.Node(name=\"child_node\", parent=parent_node)\n    assert child_node.nodeid == \"parent_nodeid::child_node\"\n\n    child_node_with_custom_name = nodes.Node(name=\"custom_child_node\", parent=parent_node)\n    assert child_node_with_custom_name.nodeid == \"parent_nodeid::custom_child_node\"\n"], "sample_887": ["def test_calibration_with_non_sample_aligned_sample_weight(data):\n    \"\"\"Check that CalibratedClassifierCV does not enforce sample alignment\n    for sample_weight.\"\"\"\n\n    class TestClassifier(LogisticRegression):\n            assert sample_weight is not None\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    CalibratedClassifierCV(estimator=TestClassifier()).fit(\n        *data, sample_weight=np.ones(len(data[1]) + 1)\n    )\n", "def test_calibration_with_non_sample_aligned_fit_param_cv_prefit(data):\n    \"\"\"Check that CalibratedClassifierCV does not enforce sample alignment\n    for fit parameters when cv is 'prefit'.\"\"\"\n\n    class TestClassifier(LogisticRegression):\n            assert fit_param is not None\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    test_clf = TestClassifier()\n    test_clf.fit(*data)\n    CalibratedClassifierCV(estimator=test_clf, cv='prefit').fit(\n        *data, fit_param=np.ones(len(data[1]) + 1)\n    )\n", "def test_calibration_with_fit_params_in_cv(data):\n    \"\"\"Tests that fit_params are passed to the underlying base estimator in\n    cross-validation.\n    \"\"\"\n    X, y = data\n    fit_params = {\"a\": np.ones(len(y)), \"b\": np.ones(len(y))}\n\n    class ClfWithFitParams(CheckingClassifier):\n            assert \"a\" in fit_params\n            assert \"b\" in fit_params\n            return super().fit(X, y, **fit_params)\n\n    clf = ClfWithFitParams()\n    pc_clf = CalibratedClassifierCV(clf, cv=2)\n\n    pc_clf.fit(X, y, **fit_params)\n", "def test_calibration_empty_data(method):\n    \"\"\"Check that calibration fails with empty dataset\"\"\"\n    clf = LinearSVC(random_state=7)\n    X = np.empty((0, 10))\n    y = np.empty((0,))\n\n    cal_clf = CalibratedClassifierCV(clf, method=method, cv=5)\n    with pytest.raises(ValueError, match=\"requires at least one sample\"):\n        cal_clf.fit(X, y)\n", "def test_calibration_with_sparse_input(data):\n    \"\"\"Test that calibration works with sparse input\"\"\"\n    X, y = data\n    X_sparse = sparse.csr_matrix(X)\n    clf = LinearSVC(random_state=7)\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3, ensemble=False)\n    cal_clf.fit(X_sparse, y)\n    cal_probas = cal_clf.predict_proba(X_sparse)\n\n    # Get probas manually\n    unbiased_preds = cross_val_predict(clf, X_sparse, y, cv=3, method=\"decision_function\")\n    calibrator = _SigmoidCalibration()\n    calibrator.fit(unbiased_preds, y)\n    # Use `clf` fit on all data\n    clf.fit(X_sparse, y)\n    clf_df = clf.decision_function(X_sparse)\n    manual_probas = calibrator.predict(clf_df)\n    assert_allclose(cal_probas[:, 1], manual_probas)\n"], "sample_957": ["def test_stringify_type_union_with_none():\n    assert stringify(Union[int, None]) == \"Optional[int]\"\n", "def test_restify_type_hints_Annotated():\n    from typing import Annotated  # type: ignore\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":class:`str`\"  # NOQA\n", "def test_restify_type_Literal():\n    from typing import Literal  # type: ignore\n    assert restify(Literal[1, \"2\", \"\\r\"]) == \":obj:`~typing.Literal`\\\\ [1, '2', '\\\\r']\"\n", "def test_restify_type_types_Union():\n    from types import UnionType\n    assert restify(UnionType[int, None]) == \":obj:`~typing.Optional`\\\\ [:class:`int`]\"\n    assert restify(UnionType[int, str]) == \":class:`int` | :class:`str`\"\n", "def test_restify_type_Annotated():\n    from typing import Annotated\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":class:`str`\"\n"], "sample_25": ["def test_header_iteritems(self):\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n    for a, b in zip(header.items(), [(\"A\", \"B\"), (\"C\", \"D\")]):\n        assert a == b\n", "def test_subclass_update(self):\n    \"\"\"Check that subclasses don't get ignored on updating.\"\"\"\n\n    class MyHeader(fits.Header):\n            if isinstance(value, tuple) and len(value) == 2:\n                # Just for our checks we add a comment if there is none.\n                value += (\"updated comment\",)\n\n            super().__setitem__(key, value)\n\n    my_header = MyHeader(\n        (\n            (\"a\", 1.0, \"first\"),\n            (\"b\", 2.0, \"second\"),\n        )\n    )\n\n    my_header[\"b\"] = (3.0,)\n    assert my_header.comments[\"b\"] == \"updated comment\"\n\n    my_header[\"b\"] = (4.0, \"new comment\")\n    assert my_header.comments[\"b\"] == \"new comment\"\n", "def test_header_insert_before_non_existent_keyword(self):\n    \"\"\"\n    Test that a ValueError is raised when trying to insert a new keyword before a non-existent keyword.\n    \"\"\"\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n    with pytest.raises(ValueError):\n        header.insert(before=\"Z\", keyword=\"E\", value=\"F\")\n", "def test_header_from_list_of_cards(self):\n    \"\"\"Test creating a header from a list of cards.\"\"\"\n\n    cards = [fits.Card(\"A\", 1), fits.Card(\"B\", 2)]\n    header = fits.Header(cards)\n    assert list(header) == [\"A\", \"B\"]\n    assert header[\"A\"] == 1\n    assert header[\"B\"] == 2\n", "def test_assign_with_comment(self):\n    \"\"\"\n    Regression test for https://github.com/spacetelescope/PyFITS/issues/42\n\n    Ensure that assigning a value to a keyword with a comment specified in the\n    value argument works as expected.\n    \"\"\"\n\n    h = fits.Header()\n    h['TEST'] = ('VALUE', 'COMMENT')\n    assert h['TEST'] == 'VALUE'\n    assert h.comments['TEST'] == 'COMMENT'\n"], "sample_151": ["def test_add_non_null_textfield_and_charfield(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Adding a NOT NULL and non-blank `CharField` or `TextField`\n    without default should prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_biography_non_null])\n    self.assertEqual(mocked_ask_method.call_count, 2)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_add_blank_null_textfield_and_charfield(self):\n    \"\"\"\n    #23405 - Adding a NULL and blank `CharField` or `TextField`\n    without default should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_biography_blank_null])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_alter_field_to_charfield_without_default(self):\n    \"\"\"\n    #23405 - Altering a NOT NULL `CharField` to `TextField` without default\n    should not prompt for a default.\n    \"\"\"\n    author_charfield = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200, null=False, blank=True)),\n    ])\n    author_textfield = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.TextField(null=False, blank=True)),\n    ])\n    changes = self.get_changes([author_charfield], [author_textfield])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n", "def test_add_booleanfield(self):\n    \"\"\"\n    #23405 - Adding a NOT NULL BooleanField without default should prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_is_active])\n    self.assertEqual(self.questioner.ask_not_null_addition.call_count, 1)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_alter_field_with_default(self):\n    \"\"\"\n    #23405 - Altering a field with a default should not prompt for a new default.\n    \"\"\"\n    changes = self.get_changes([self.author_name_default], [self.author_name_new_default])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\", preserve_default=False)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=\"New Default\")\n"], "sample_697": ["def test_tmp_path_factory_create_directory_with_correct_owner(\n    tmp_path: Path, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_handles_non_existent_basetemp(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_error_handling(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test the error handling when the base temporary directory is not owned by the current user.\"\"\"\n    fake_basetemp = tmp_path.joinpath(\"not_owned_by_user\")\n    fake_basetemp.mkdir(mode=0o755)  # Create a directory with world-readable permissions\n\n    # Patch the _given_basetemp attribute to return the fake basetemp\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", fake_basetemp)\n\n    # Patch the os.getuid function to return a fake user ID\n    monkeypatch.setattr(\"os.getuid\", lambda: 1000)\n\n    with pytest.raises(OSError, match=f\"The temporary directory {fake_basetemp} is not owned by the current user. Fix this and try again.\"):\n        tmp_path_factory.getbasetemp()\n", "def test_tmp_path_factory_custom_basetemp_permission(monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Verify that pytest raises an error when given a custom basetemp that is not owned by the current user.\"\"\"\n    custom_basetemp = Path(\"/tmp/custom_basetemp\")\n    custom_basetemp.mkdir(mode=0o755)\n\n    # Mock the os.getuid() function to return a different UID than the actual one\n    monkeypatch.setattr(\"os.getuid\", lambda: 1234)\n\n    # Create a TempPathFactory instance with the custom basetemp\n    tmp_factory = TempPathFactory(custom_basetemp, lambda *args: None, _ispytest=True)\n\n    # Attempt to get the basetemp, which should raise an OSError\n    with pytest.raises(OSError) as excinfo:\n        tmp_factory.getbasetemp()\n\n    # Check that the error message is correct\n    assert str(excinfo.value) == f\"The temporary directory {custom_basetemp} is not owned by the current user. Fix this and try again.\"\n", "def test_tmp_path_factory_basetemp_exists_but_not_owned(\n    tmp_path: Path, monkeypatch: MonkeyPatch"], "sample_844": ["def test_min_samples_float():\n    # Test that min_samples can be specified as a float\n    n_samples = 100\n    X = np.random.rand(n_samples, 2)\n    min_samples = 0.1  # 10% of the total samples\n    expected_min_samples = int(min_samples * n_samples)\n\n    clust = OPTICS(min_samples=min_samples).fit(X)\n    assert clust.min_samples_ == expected_min_samples\n", "def test_extract_xi_noise():\n    # Testing the extraction of noise using the 'xi' method\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 10\n    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)\n    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)\n    Noise = 20 * rng.randn(10, 2)\n    X = np.vstack((C1, C2, C3, C4, C5, Noise))\n    expected_labels = np.r_[[2] * 10, [0] * 10, [1] * 10, [3] * 10, [1] * 10, [-1] * 10]\n    X, expected_labels = shuffle(X, expected_labels, random_state=rng)\n\n    clust = OPTICS(min_samples=3, min_cluster_size=2, max_eps=20, cluster_method='xi', xi=0.4).fit(X)\n    assert_array_equal(clust.labels_, expected_labels)\n", "def test_min_samples_float():\n    C1 = [[0, 0], [0, 0.1], [0, -.1], [0.1, 0]]\n    C2 = [[10, 10], [10, 9], [10, 11], [9, 10]]\n    C3 = [[100, 100], [100, 90], [100, 110], [90, 100]]\n    X = np.vstack((C1, C2, C3))\n\n    expected_labels = np.r_[[0] * 4, [1] * 4, [2] * 4]\n    clust = OPTICS(min_samples=0.25, max_eps=7, cluster_method='xi', xi=0.04).fit(X)\n    assert_array_equal(clust.labels_, expected_labels)\n", "def test_predecessor_correction():\n    # Test the predecessor correction feature\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    # Create a scenario where predecessor correction is necessary\n    # to ensure correct clustering\n    clust = OPTICS(min_samples=2, min_cluster_size=2,\n                   max_eps=np.inf, cluster_method='xi',\n                   xi=0.04, predecessor_correction=True).fit(X)\n    expected_labels = np.array([0, 0, 0, 0, 0])\n    assert_array_equal(clust.labels_, expected_labels)\n\n    # Test the scenario without predecessor correction\n    clust = OPTICS(min_samples=2, min_cluster_size=2,\n                   max_eps=np.inf, cluster_method='xi',\n                   xi=0.04, predecessor_correction=False).fit(X)\n    # In this case, without predecessor correction, the result may not be correct\n    # as it might not consider the overall structure of the data\n    assert not np.array_equal(clust.labels_, expected_labels)\n", "def test_extract_xi_with_predecessor_correction():\n    # Testing an easy xi case with predecessor correction\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 20\n    C1 = [-5, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .2 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, 2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3, C4))\n\n    clust = OPTICS(cluster_method='xi', xi=0.2, predecessor_correction=True).fit(X)\n    assert_array_equal(np.sort(np.unique(clust.labels_)), [0, 1, 2, 3])\n"], "sample_890": ["def test_cv_object_with_groups(direction):\n    \"\"\"Check that SequentialFeatureSelector works with a grouped cross-validation object\"\"\"\n    X, y, groups = make_classification(n_samples=20, n_features=10, n_informative=5, random_state=0)\n    cv = LeaveOneGroupOut()\n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(n_neighbors=1),\n        n_features_to_select=3,\n        direction=direction,\n        cv=cv,\n    )\n    sfs.fit(X, y, groups=groups)\n    assert sfs.transform(X).shape[1] == 3\n", "def test_feature_names_in_support():\n    # Make sure feature names are properly handled\n\n    X, y = make_classification(n_features=5, n_informative=3, random_state=0)\n    feature_names = np.array(['f1', 'f2', 'f3', 'f4', 'f5'])\n    X = pd.DataFrame(X, columns=feature_names)\n    knn = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n    sfs.fit(X, y)\n    assert_array_equal(sfs.feature_names_in_, feature_names)\n    assert_array_equal(sfs.get_support(), [True, True, True, False, False])\n", "def test_scoring_methods(scoring):\n    # Test different scoring methods to ensure they work correctly\n\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction=\"forward\", scoring=scoring, cv=2)\n    sfs.fit(X, y)\n    assert sfs.transform(X).shape[1] == 5\n", "def test_scoring(direction, scoring):\n    # Make sure that different scoring methods are supported\n\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=5,\n        direction=direction,\n        scoring=scoring,\n        cv=2,\n    )\n    sfs.fit(X, y)\n    assert sfs.transform(X).shape[1] == 5\n", "def test_scoring_parameter(scoring):\n    # Test the scoring parameter with different options\n\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction=\"forward\", scoring=scoring, cv=2)\n    sfs.fit(X, y)\n\n    assert sfs.get_support(indices=True).shape[0] == 5\n    assert sfs.transform(X).shape[1] == 5\n"], "sample_599": ["def test_CFMaskCoder_decode_multiple_fill_values():\n    original = xr.Variable((\"x\",), [0, -1, 1], {\"missing_value\": [-1, 2]})\n    with pytest.warns(variables.SerializationWarning):\n        encoded = variables.CFMaskCoder().decode(original)\n    expected = xr.Variable((\"x\",), [0, np.nan, 1])\n    assert_identical(expected, encoded)\n", "def test_scaling_converts_to_float64(dtype):\n    original = xr.Variable(\n        (\"x\",), np.arange(10, dtype=dtype), encoding=dict(scale_factor=10)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float64\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == np.float64\n", "def test_UnsignedIntegerCoder_decode():\n    original = xr.Variable((\"x\",), [0, 1, 2], {\"_Unsigned\": \"true\"})\n    expected = xr.Variable((\"x\",), [0, 1, 2], dtype=np.uint8)\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.decode(original)\n    assert_identical(expected, encoded)\n\n    # Test with non-integer data\n    original = xr.Variable((\"x\",), [0.0, 1.0, 2.0], {\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    with pytest.warns(variables.SerializationWarning):\n        encoded = coder.decode(original)\n    assert_identical(original, encoded)\n", "def test_UnsignedIntegerCoder_decode():\n    original = xr.Variable((\"x\",), [0, 1, 2], {\"_Unsigned\": \"true\"}, dtype=np.int8)\n    expected = xr.Variable((\"x\",), [0, 1, 2], dtype=np.uint8)\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.decode(original)\n    assert_identical(expected, encoded)\n", "def test_scaling_converts_to_float64(dtype):\n    original = xr.Variable(\n        (\"x\",), np.arange(10, dtype=dtype), encoding=dict(scale_factor=10)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float64\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert roundtripped.dtype == np.float64\n"], "sample_1018": ["def test_fcode_Infinity():\n    x = symbols('x')\n    assert fcode(S.Infinity, standard=95, source_format='free') == '(huge(0d0) + 1)'\n", "def test_fcode_Infinity():\n    x = symbols('x')\n    assert fcode(S.Infinity) == '(huge(0d0) + 1)'\n    assert fcode(S.NegativeInfinity) == '-(huge(0d0) + 1)'\n", "def test_fcode_While():\n    x = symbols('x')\n    w = While(x < 5, [Assignment(x, x + 1)])\n    sol = fcode(w)\n    assert sol == (\"      do while (x < 5)\\n\"\n                   \"         x = x + 1\\n\"\n                   \"      end do\")\n", "def test_fcode_AugmentedAssignment():\n    x = symbols('x')\n    y = symbols('y')\n    assert fcode(AugmentedAssignment(x, '+', y), source_format='free') == 'x = x + y'\n    assert fcode(AugmentedAssignment(x, '-', y), source_format='free') == 'x = x - y'\n    assert fcode(AugmentedAssignment(x, '*', y), source_format='free') == 'x = x * y'\n    assert fcode(AugmentedAssignment(x, '/', y), source_format='free') == 'x = x / y'\n    assert fcode(AugmentedAssignment(x, '**', y), source_format='free') == 'x = x ** y'\n", "def test_fcode_Stream():\n    x = symbols('x')\n    assert fcode(Print(x), standard=2003) == 'print *, x'\n    assert fcode(Print(x, stream='stderr'), standard=2003) == 'print *, x, ADVANCE=\"NO\"'\n    assert fcode(Print(x, stream='stdout'), standard=2003) == 'print *, x'\n    assert fcode(Print(x, stream='*'), standard=2003) == 'print *, x'\n    assert fcode(Print(x, stream='some_unit'), standard=2003) == 'print *, x, UNIT=some_unit'\n"], "sample_138": ["def test_source_map_url(self):\n    relpath = self.hashed_file_path('cached/source_map_url.js')\n    self.assertEqual(relpath, 'cached/source_map_url.1234567890ab.js')\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertIn(b'//# sourceMappingURL=source_map.js.map', content)\n        self.assertNotIn(\n            b'//# sourceMappingURL=source_map.js.99914b932bd3.map',\n            content,\n        )\n    self.assertPostCondition()\n", "def test_manifest_storage_is_staticfiles_storage_by_default(self):\n    self.assertIs(self.staticfiles_storage.manifest_storage, self.staticfiles_storage._wrapped)\n", "def test_post_processing_ordering(self):\n    # Create two CSS files with URLs to each other\n    with open(self._get_filename_path('first.css'), 'w') as f:\n        f.write('body { background: url(\"second.css\"); }')\n    with open(self._get_filename_path('second.css'), 'w') as f:\n        f.write('body { background: url(\"first.css\"); }')\n\n    # Run collectstatic\n    self.run_collectstatic()\n\n    # Check that the hashed filenames are correct\n    first_hashed = self.hashed_file_path('test/first.css')\n    second_hashed = self.hashed_file_path('test/second.css')\n    with storage.staticfiles_storage.open(first_hashed) as f:\n        content = f.read()\n        self.assertNotIn(b'second.css', content)\n        self.assertIn(second_hashed.encode(), content)\n    with storage.staticfiles_storage.open(second_hashed) as f:\n        content = f.read()\n        self.assertNotIn(b'first.css', content)\n        self.assertIn(first_hashed.encode(), content)\n", "def test_hashed_name_with_custom_template(self):\n    custom_template = '\"%(url)s?v=%(version)s\"'\n    storage.staticfiles_storage.patterns = (\n        (\"*.css\", (\n            (\n                r\"\"\"(?P<matched>url\\(['\"]{0,1}\\s*(?P<url>.*?)[\"']{0,1}\\))\"\"\",\n                custom_template,\n            ),\n        )),\n    )\n    relpath = self.hashed_file_path(\"cached/styles.css\")\n    self.assertEqual(relpath, \"cached/styles.5e0040571e1a.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"cached/other.css\", content)\n        self.assertIn(b'\"other.d41d8cd98f00.css?v=%(version)s\"', content)\n    self.assertPostCondition()\n", "def test_template_tag_url_encoding(self):\n    relpath = self.hashed_file_path(\"cached/url_encoding.css\")\n    self.assertEqual(relpath, \"cached/url_encoding.e896e57a5804.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertIn(b\"url('%C3%A4%C3%B6%C3%BC%C3%9F.png')\", content)\n        self.assertNotIn(b\"url('%C3%A4%C3%B6%C3%BC%C3%9F.acae32e4532b.png')\", content)\n        self.assertIn(b\"url('/static/%C3%A4%C3%B6%C3%BC%C3%9F.acae32e4532b.png')\", content)\n    self.assertPostCondition()\n"], "sample_750": ["def test_omp_return_n_iter():\n    _, n_iter = orthogonal_mp(X, y, n_nonzero_coefs=5, return_n_iter=True)\n    assert_equal(n_iter, 5)\n    _, n_iter = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5, return_n_iter=True)\n    assert_equal(n_iter, 5)\n", "def test_omp_cv_multi_target():\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y)\n    assert_equal(ompcv.coef_.shape, (n_targets, n_features))\n    assert_true(np.count_nonzero(ompcv.coef_) <= n_targets * n_nonzero_coefs)\n", "def test_omp_multi_output():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n    omp.fit(X, y)\n    assert_equal(omp.coef_.shape, (n_targets, n_features))\n    assert_true(np.count_nonzero(omp.coef_) <= n_targets * n_nonzero_coefs)\n\n    coef_normalized = omp.coef_.copy()\n    omp.set_params(fit_intercept=True, normalize=False)\n    omp.fit(X, y)\n    assert_array_almost_equal(coef_normalized, omp.coef_)\n\n    omp.set_params(fit_intercept=False, normalize=False)\n    omp.fit(X, y)\n    assert_true(np.count_nonzero(omp.coef_) <= n_targets * n_nonzero_coefs)\n    assert_equal(omp.coef_.shape, (n_targets, n_features))\n    assert_equal(omp.intercept_, 0)\n", "def test_omp_cv_with_intercept():\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=True,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y)\n    assert_equal(ompcv.n_nonzero_coefs_, n_nonzero_coefs)\n    assert_array_almost_equal(ompcv.coef_, gamma.T)\n    omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=True,\n                                    n_nonzero_coefs=ompcv.n_nonzero_coefs_)\n    omp.fit(X, y)\n    assert_array_almost_equal(ompcv.coef_, omp.coef_)\n", "def test_omp_with_intercept():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs, fit_intercept=True)\n    omp.fit(X, y[:, 0])\n    lin_reg = LinearRegression(fit_intercept=True)\n    lin_reg.fit(X, y[:, 0])\n    assert_array_almost_equal(omp.coef_ + omp.intercept_, lin_reg.coef_)\n"], "sample_1002": ["def test_Float_abs():\n    assert abs(Float('-1.23')) == Float('1.23')\n    assert abs(Float('1.23')) == Float('1.23')\n    assert abs(Float('0')) == Float('0')\n", "def test_Float_abs():\n    assert abs(Float('-1.5')) == Float('1.5')\n    assert abs(Float('1.5')) == Float('1.5')\n    assert abs(Float('-1.5', dps=10)) == Float('1.5', dps=10)\n    assert abs(Float('1.5', dps=10)) == Float('1.5', dps=10)\n", "def test_Float_to_integer():\n    assert Float(2.0).to_integer() == 2\n    assert Float(2.5).to_integer() == 2\n    assert Float(-2.5).to_integer() == -2\n    assert Float('2.0').to_integer() == 2\n    assert Float('2.5').to_integer() == 2\n    assert Float('-2.5').to_integer() == -2\n    assert Float('inf').to_integer() is None\n    assert Float('-inf').to_integer() is None\n    assert Float('nan').to_integer() is None\n", "def test_Float_to_Rational():\n    assert Float('0.5').to_rational() == Rational(1, 2)\n    assert Float('1.0').to_rational() == Rational(1, 1)\n    assert Float('2.3').to_rational() == Rational(23, 10)\n    assert Float('3.141592653589793').to_rational() == Rational(355, 113)\n    raises(ValueError, lambda: Float('0.1234567890123456789').to_rational())\n", "def test_Float_comparisons():\n    f1 = Float(0.1, 10)\n    f2 = Float(0.2, 10)\n    assert f1 < f2\n    assert f1 <= f2\n    assert f2 > f1\n    assert f2 >= f1\n    assert not f1 > f2\n    assert not f1 >= f2\n    assert not f2 < f1\n    assert not f2 <= f1\n    assert not f1 == f2\n    assert f1 != f2\n\n    f3 = Float(0.1, 5)\n    f4 = Float(0.10001, 5)\n    assert f3 < f4\n    assert f3 <= f4\n    assert f4 > f3\n    assert f4 >= f3\n    assert not f3 > f4\n    assert not f3 >= f4\n    assert not f4 < f3\n    assert not f4 <= f3\n    assert f3 == f4\n    assert not f3 != f4\n\n    f5 = Float('nan', 10)\n    assert not f5 < f2\n    assert not f5 <= f2\n    assert not f5 > f2\n    assert not f5 >= f2\n    assert not f5 == f2\n    assert f5 != f2\n"], "sample_324": ["def test_token_rotation(self):\n    \"\"\"\n    The token is rotated when rotate_token() is called.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    old_token = get_token(req)\n    rotate_token(req)\n    new_token = get_token(req)\n    self.assertNotEqual(old_token, new_token)\n", "def test_cookie_reset_on_post_request(self):\n    \"\"\"\n    The csrf token used in posts is changed on every request (although\n    stays equivalent). The csrf cookie should be changed on accepted\n    POST requests.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, None)\n    self.assertIsNotNone(csrf_cookie, \"CSRF cookie was not set on accepted POST request\")\n    self.assertNotEqual(\n        csrf_cookie.value, self._csrf_id_cookie,\n        \"CSRF cookie was not changed on accepted POST request\"\n    )\n    self._check_token_present(resp, csrf_id=csrf_cookie.value)\n", "def test_https_good_referer_with_x_forwarded_port(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted when USE_X_FORWARDED_PORT=True.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = '10.0.0.2'\n    req.META['HTTP_REFERER'] = 'https://www.example.com/somepage'\n    req.META['SERVER_PORT'] = '8080'\n    req.META['HTTP_X_FORWARDED_HOST'] = 'www.example.com'\n    req.META['HTTP_X_FORWARDED_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n", "def test_get_token_no_session(self):\n    \"\"\"\n    get_token() works when no session is available.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    with self.assertRaises(ImproperlyConfigured) as cm:\n        mw.process_request(req)\n    self.assertEqual(\n        str(cm.exception),\n        'CSRF_USE_SESSIONS is enabled, but request.session is not set. '\n        'SessionMiddleware must appear before CsrfViewMiddleware in MIDDLEWARE.'\n    )\n", "def test_bad_origin_csrf_trusted_origin_bad_domain(self):\n    \"\"\"\n    A request with an origin with a bad domain compared to\n    CSRF_TRUSTED_ORIGINS is rejected.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://evil.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self._check_referer_rejects(mw, req)\n    self.assertIs(mw._origin_verified(req), False)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        response = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(response.status_code, 403)\n    msg = REASON_BAD_ORIGIN % req.META['HTTP_ORIGIN']\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % msg)\n"], "sample_179": ["def test_check_unique_together_required_db_features(self):\n    class Model(models.Model):\n        field1 = models.IntegerField()\n        field2 = models.IntegerField()\n\n        class Meta:\n            required_db_features = {'supports_unique_constraints_index_together'}\n            unique_together = [['field1', 'field2']]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_index_with_expression(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=50)\n\n            class Meta:\n                constraints = [\n                    models.Index(\n                        models.functions.Lower('name'),\n                        name='lower_name_idx',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_together_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n            group = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = [('age', 'group'), ('group',)]\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_gte_100',\n                        condition=models.Q(age__gte=100),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_indexes else [\n            Warning(\n                '%s does not support unique constraints with conditions.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W036',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "def test_deferrable_unique_constraint_required_db_features(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n\n        class Meta:\n            required_db_features = {'supports_deferrable_unique_constraints'}\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['age'],\n                    name='unique_age_deferrable',\n                    deferrable=models.Deferrable.DEFERRED,\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_multiple_unique_together(self):\n    class Model(models.Model):\n        one = models.IntegerField()\n        two = models.IntegerField()\n        three = models.IntegerField()\n\n        class Meta:\n            unique_together = (('one', 'two'), ('one', 'three'))\n\n    self.assertEqual(Model.check(), [])\n"], "sample_451": ["    def test_replace_named_groups(self):\n        from django.contrib.admindocs.utils import replace_named_groups\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), r'^<a>/b/<c>/$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), r'^<a>/b/<c>')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)'), r'^<a>/b/(\\w+)')\n", "def test_replace_named_groups(self):\n    self.assertEqual(replace_named_groups(r\"^(?P<a>\\w+)/b/(\\w+)$\"), r\"^<a>/b/(\\w+)$\")\n    self.assertEqual(replace_named_groups(r\"^(?P<a>\\w+)/b/(?P<c>\\w+)/$\"), r\"^<a>/b/<c>/$\")\n    self.assertEqual(replace_named_groups(r\"^(?P<a>\\w+)/b/(\\w+)\"), r\"^<a>/b/(\\w+)\")\n    self.assertEqual(replace_named_groups(r\"^(?P<a>\\w+)/b/(?P<c>\\w+)\"), r\"^<a>/b/<c>\")\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, r'^<a>/b/<c>/$')\n", "    def test_replace_named_groups(self):\n        from django.contrib.admindocs.utils import replace_named_groups\n        self.assertEqual(replace_named_groups(\"^(?P<a>\\w+)/b/(?P<c>\\w+)/$\"), \"^<a>/b/<c>/$\")\n        self.assertEqual(replace_named_groups(\"^(?P<a>\\w+)/b/(\\w+)$\"), \"^<a>/b/(\\w+)$\")\n        self.assertEqual(replace_named_groups(\"^(?P<a>\\w+)/b/(?P<c>\\w+)\"), \"^<a>/b/<c>\")\n        self.assertEqual(replace_named_groups(\"^(?P<a>\\w+)/b/(\\w+)\"), \"^<a>/b/(\\w+)\")\n", "    def test_replace_metacharacters(self):\n        self.assertEqual(replace_metacharacters(r'abc\\*def'), 'abc*def')\n        self.assertEqual(replace_metacharacters(r'abc\\?def'), 'abc?def')\n        self.assertEqual(replace_metacharacters(r'abc\\\\*def'), 'abc\\\\*def')\n        self.assertEqual(replace_metacharacters(r'abc\\\\?def'), 'abc\\\\?def')\n"], "sample_608": ["def test_diff_attrs_repr_with_list(self) -> None:\n    attrs_a = {\"attr\": [0, 1]}\n    attrs_b = {\"attr\": [0, 1, 2]}\n    expected = dedent(\n        \"\"\"\\\n        Differing attributes:\n        L   attr: [0, 1]\n        R   attr: [0, 1, 2]\n        \"\"\"\n    ).strip()\n    actual = formatting.diff_attrs_repr(attrs_a, attrs_b, \"equals\")\n    assert expected == actual\n", "def test_short_data_repr_large_data_array() -> None:\n    da = xr.DataArray(np.random.randn(1000000))\n    result = formatting.short_data_repr(da)\n    expected = \"[1000000 values with dtype=float64]\"\n    assert result == expected\n", "def test_format_array_flat_with_empty_array() -> None:\n    array = np.array([])\n    actual = formatting.format_array_flat(array, 2)\n    expected = \"\"\n    assert expected == actual\n", "def test_diff_attrs_repr_with_duck_array() -> None:\n    attrs_a = {\"attr\": np.array([0, 1])}\n\n    attrs_b = {\"attr\": np.array([0, 1, 2])}\n    expected = dedent(\n        \"\"\"\\\n        Differing attributes:\n        L   attr: [0 1]\n        R   attr: [0 1 2]\n        \"\"\"\n    ).strip()\n    actual = formatting.diff_attrs_repr(attrs_a, attrs_b, \"equals\")\n    assert expected == actual\n\n    attrs_c = {\"attr\": np.array([-3, 5])}\n    expected = dedent(\n        \"\"\"\\\n        Differing attributes:\n        L   attr: [0 1]\n        R   attr: [-3  5]\n        \"\"\"\n    ).strip()\n    actual = formatting.diff_attrs_repr(attrs_a, attrs_c, \"equals\")\n    assert expected == actual\n", "def test_dataset_repr_limit_rows(self) -> None:\n    ds = xr.Dataset(coords={\"foo\": np.arange(10), \"bar\": np.arange(10)})\n    for i in range(1, 11):\n        ds[f\"var{i}\"] = xr.DataArray([0], dims=\"test\")\n\n    with xr.set_options(display_max_rows=5):\n        actual = formatting.dataset_repr(ds)\n        expected = dedent(\n            \"\"\"\\\n        <xarray.Dataset>\n        Dimensions: (foo: 10, bar: 10, test: 1)\n        Coordinates:\n          * foo      (foo) int64 0 1 2 3 4 5 ... 6 7 8 9\n          * bar      (bar) int64 0 1 2 3 4 5 ... 6 7 8 9\n        Data variables: (10/11)\n          * var1     (test) int64 0\n          * var2     (test) int64 0\n          * var3     (test) int64 0\n          ...\n          * var9     (test) int64 0\n          * var10    (test) int64 0\n        \"\"\"\n        )\n\n        assert actual == expected\n"], "sample_1041": ["def test_MatrixElement_addition():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n\n    assert (A[0, 0] + B[0, 0]).doit() == A[0, 0] + B[0, 0]\n    assert (A[0, 0] + A[0, 0]).doit() == 2 * A[0, 0]\n    assert (A[0, 0] + A[0, 1]).doit() == A[0, 0] + A[0, 1]\n", "def test_as_explicit():\n    A = MatrixSymbol(\"A\", 2, 2)\n    explicit_A = A.as_explicit()\n    assert explicit_A.shape == A.shape\n    assert explicit_A[0, 0] == A[0, 0]\n    assert explicit_A[0, 1] == A[0, 1]\n    assert explicit_A[1, 0] == A[1, 0]\n    assert explicit_A[1, 1] == A[1, 1]\n", "def test_matrix_symbol_entry():\n    A = MatrixSymbol('A', n, m)\n    i, j = symbols('i j')\n    assert A._entry(i, j) == MatrixElement(A, i, j)\n", "def test_matrixelement_derivative():\n    A = MatrixSymbol('A', n, m)\n    X = MatrixSymbol('X', n, n)\n    i, j = symbols('i j')\n\n    assert A[i, j].diff(X[k, l]) == KroneckerDelta(i, k)*KroneckerDelta(j, l)\n    assert X[i, j].diff(A[k, l]) == 0\n    assert X[i, j].diff(X[k, l]) == KroneckerDelta(i, k)*KroneckerDelta(j, l)\n\n    # Test derivative with respect to the matrix itself\n    assert (A*A.T)[i, j].diff(A[k, l]) == 2*KroneckerDelta(j, l)*A[i, k]\n\n    # Test derivative with respect to a transposed matrix\n    assert (A.T*A)[i, j].diff(A[k, l]) == 2*KroneckerDelta(i, l)*A[j, k]\n\n    # Test derivative with respect to an inverse matrix\n    assert (X.I)[i, j].diff(X[k, l]) == -X.I[i, k]*X.I[l, j]\n", "def test_MatrixElement_symbolic_indices():\n    i, j, k, l = symbols('i j k l', integer=True)\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n\n    assert A[i, j].diff(A[k, l]) == KroneckerDelta(i, k)*KroneckerDelta(j, l)\n    assert A[i, j].diff(B[k, l]) == S.Zero\n\n    A_inv = Inverse(A)\n    assert A_inv[i, j].diff(A[k, l]) == -A_inv[i, k]*A[k, l]*A_inv[j, i]\n"], "sample_298": ["def test_token_with_different_password(self):\n    \"\"\"Changing the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_without_timeout(self):\n    \"\"\"The token is valid indefinitely when PASSWORD_RESET_TIMEOUT is None.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    now = datetime.now()\n    p0 = MockedPasswordResetTokenGenerator(now)\n    tk1 = p0.make_token(user)\n    p1 = MockedPasswordResetTokenGenerator(now + timedelta(days=365))\n    self.assertIs(p1.check_token(user, tk1), True)\n", "    def test_token_with_different_algorithm(self):\n        \"\"\"\n        A valid token can be created with a different hashing algorithm by\n        using the PasswordResetTokenGenerator.algorithm attribute.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        new_algorithm = 'sha1'\n        # Create and check a token with a different algorithm.\n        p0 = PasswordResetTokenGenerator()\n        p0.algorithm = new_algorithm\n        tk0 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk0), True)\n        # Create and check a token with the default algorithm.\n        p1 = PasswordResetTokenGenerator()\n        self.assertEqual(p1.algorithm, 'sha256')\n        self.assertNotEqual(p1.algorithm, new_algorithm)\n        tk1 = p1.make_token(user)\n        # Tokens created with a different algorithm don't validate.\n        self.assertIs(p0.check_token(user, tk1), False)\n        self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_same_user_and_password(self):\n    \"\"\"Changing the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('samepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_different_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_94": ["def test_fields_with_fk_missing_required_field(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n\n    msg = 'You must use --username with --noinput.'\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            email=email.email,\n            group=group.pk,\n            stdout=new_io,\n        )\n", "def test_validate_password_minimum_length(self):\n    new_io = StringIO()\n    short_password = 'short'\n    entered_passwords = [short_password, short_password, 'superduperunguessablepassword', 'superduperunguessablepassword']\n\n        return entered_passwords.pop(0)\n\n    @mock_inputs({\n        'password': bad_then_good_password,\n        'username': 'whatever',\n        'group': Group.objects.create(name='mygroup').pk,\n        'email': 'joey@example.com',\n        'bypass': 'n',\n    })\n        call_command(\n            'createsuperuser',\n            interactive=True,\n            stdin=MockTTY(),\n            stdout=new_io,\n            stderr=new_io,\n        )\n        self.assertEqual(\n            new_io.getvalue().strip(),\n            \"This password is too short. It must contain at least 8 characters.\\n\"\n            \"Superuser created successfully.\"\n        )\n\n    test(self)\n", "def test_createsuperuser_with_unique_username_constraint(self):\n    new_io = StringIO()\n    user = CustomUser.objects.create(username='joe')\n    with self.assertRaisesMessage(CommandError, 'Error: That username is already taken.'):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username='joe',\n            email='joe@somewhere.org',\n            stdout=new_io,\n        )\n", "def test_fields_with_fk_interactive_required_field_validation(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n\n    @mock_inputs({\n        'password': 'nopasswd',\n        'Username (Email.id): ': '',  # Empty value to trigger validation error\n        'Email (Email.email): ': email.email,\n        'Group (Group.id): ': group.pk,\n    })\n        with self.assertRaisesMessage(CommandError, 'This field cannot be blank.'):\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdout=new_io,\n                stdin=MockTTY(),\n            )\n\n    test(self)\n", "def test_minimum_length_password(self):\n    new_io = StringIO()\n    short_password = 'short'\n    entered_passwords = [short_password, short_password, 'longpassword', 'longpassword']\n\n        return entered_passwords.pop(0)\n\n    @mock_inputs({\n        'password': bad_then_good_password,\n        'username': 'whatever',\n        'first_name': 'john',\n        'date_of_birth': '1970-01-01',\n        'email': 'john@example.com',\n        'bypass': 'n',\n    })\n        call_command(\n            'createsuperuser',\n            interactive=True,\n            stdin=MockTTY(),\n            stdout=new_io,\n            stderr=new_io,\n        )\n        self.assertEqual(\n            new_io.getvalue().strip(),\n            \"This password is too short. It must contain at least 8 characters.\\n\"\n            \"Superuser created successfully.\"\n        )\n\n    test(self)\n"], "sample_1095": ["def test_permutation_apply_integer_assumption():\n    x = Symbol('x', integer=True)\n    p = Permutation(0, 1, 2)\n    assert p.apply(x) == AppliedPermutation(p, x)\n    assert AppliedPermutation(p, x).subs(x, 0) == 1\n\n    x = Symbol('x', integer=False)\n    raises(NotImplementedError, lambda: p.apply(x))\n    x = Symbol('x', negative=True)\n    raises(NotImplementedError, lambda: p.apply(x))\n", "def test_permutation_commutator():\n    p = Permutation(0, 1, 2)\n    q = Permutation(2, 0, 1)\n    assert p.commutator(q) == Permutation(0, 2, 1)\n    assert q.commutator(p) == Permutation(1, 0, 2)\n", "def test_permutation_size():\n    p = Permutation([0, 1, 2])\n    assert p.size == 3\n    p = Permutation([0, 1, 2, 3, 4, 5])\n    assert p.size == 6\n    p = Permutation([0, 2, 1, 3])\n    assert p.size == 4\n", "def test_permutation_identity():\n    p = Permutation(3)\n    assert p.is_identity\n    assert p.order() == 1\n    assert p.rank() == 0\n    assert p.array_form == list(range(3))\n    assert p.cyclic_form == []\n    assert p.full_cyclic_form == [[0], [1], [2]]\n    assert p.support() == []\n    assert p.ascents() == []\n    assert p.descents() == []\n    assert p.inversions() == 0\n    assert p.signature() == 1\n    assert p.length() == 0\n    assert p.runs() == [[0], [1], [2]]\n    assert p.index() == 0\n    assert p.get_precedence_distance(p) == 0\n    assert p.get_adjacency_distance(p) == 0\n    assert p.get_positional_distance(p) == 0\n    assert p.cycle_structure == {1: 3}\n    assert p.cycles == 3\n", "def test_permutation_apply_integer_argument():\n    p = Permutation(0, 1, 2)\n    assert p.apply(0) == 1\n    assert p.apply(1) == 2\n    assert p.apply(2) == 0\n    assert p.apply(3) is None  # Permutation size is 3, so 3 is not a valid argument\n"], "sample_638": ["def test_graphviz_unsupported_image_format(mock_writer, capsys):\n    \"\"\"Test that an error is raised if the image format is unsupported.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"xyz\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert (\n        \"Format: 'XYZ' not recognized. Use one of: bmp, canon, cgimage, cmap, cmapx, cmapx_np, dot, dot_json, eps, exr, fig, gd, gd2, gif, gv, icns, ico, imap, imap_np, ismap, jp2, jpe, jpeg, jpg, json, json0, mp, pct, pdf, pic, pict, plain, plain-ext, png, pov, ps, ps2, psd, sgi, svg, svgz, tga, tif, tiff, tk, vdx, vml, vmlz, vrml, wbmp, webp, xdot, xdot1.2, xdot1.4, xdot_json\"\n        in capsys.readouterr().err\n    )\n    # Check that pyreverse did not make the call to create the diagram and we exit with an error\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n", "def test_graphviz_unsupported_image_format(mock_writer, capsys):\n    \"\"\"Test that an error is raised when an unsupported image format is used.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"unsupported_format\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert \"Unsupported format: unsupported_format\" in capsys.readouterr().err\n    # Check that pyreverse did not make the call to create the diagram and we exit with error\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n", "def test_graphviz_unsupported_image_format(mock_writer, mock_subprocess, capsys):\n    \"\"\"Test that an error is raised if the image format is not supported.\"\"\"\n    mock_subprocess.run.return_value.stderr = \"Format: \\\"XYZ\\\" not recognized.\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"XYZ\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert \"Error: Unsupported output format 'XYZ'.\" in capsys.readouterr().err\n    # Check that pyreverse did not make the call to create the diagram and we exit with an error code\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n", "def test_graphviz_unsupported_image_format(mock_writer, mock_subprocess, capsys):\n    \"\"\"Test that an error is raised when an unsupported image format is provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"unsupported_format\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert \"unsupported_format is not a supported format.\" in capsys.readouterr().err\n    # Check that the diagram writer was not called\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n", "def test_graphviz_unsupported_image_format(mock_writer, capsys):\n    \"\"\"Test that an error is raised if the image format is not supported.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"unsupported_format\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert \"unsupported_format is not a supported output format.\" in capsys.readouterr().err\n    # Check that pyreverse didn't make the call to create the diagram and we exit with an error code\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n"], "sample_288": ["def test_key_transform_exact(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo__exact='bar'),\n        [self.objs[7]],\n    )\n", "def test_key_transform_raw_expression_with_params(self):\n    expr = RawSQL(self.raw_sql, ['{\"x\": \"bar\"}'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr, params=('bar',))),\n        [self.objs[7]],\n    )\n", "def test_key_exact_with_integer_string(self):\n    obj = NullableJSONModel.objects.create(value={'key': '123'})\n    self.assertEqual(NullableJSONModel.objects.filter(value__key__exact=123).get(), obj)\n", "def test_key_transform_integer_lookup(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__gt=4.1),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__lt=4.3),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__gte=4.2),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__lte=4.2),\n        [self.objs[4]],\n    )\n", "def test_key_transform_cast(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__0__isnull=False).annotate(\n            key=KeyTransform('d', 'value'),\n            casted=Cast('key', models.JSONField()),\n        ).filter(casted__0=F('value__d__0')),\n        [self.objs[4]],\n    )\n"], "sample_489": ["def test_update_conflicts_with_db_column(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(db_column1=1, db_column2=1, name=\"a\"),\n            FieldsWithDbColumns(db_column1=2, db_column2=2, name=\"b\"),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n\n    conflicting_objects = [\n        FieldsWithDbColumns(db_column1=1, db_column2=1, name=\"c\"),\n        FieldsWithDbColumns(db_column1=2, db_column2=2, name=\"d\"),\n    ]\n    unique_fields = [\"db_column1\"]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=unique_fields,\n        update_fields=[\"name\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"db_column1\", \"db_column2\", \"name\"),\n        [\n            {\"db_column1\": 1, \"db_column2\": 1, \"name\": \"c\"},\n            {\"db_column1\": 2, \"db_column2\": 2, \"name\": \"d\"},\n        ],\n    )\n", "def test_update_conflicts_pk_in_unique_fields(self):\n    msg = \"Primary key is allowed in unique_fields.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        BigAutoFieldModel.objects.bulk_create(\n            [BigAutoFieldModel()],\n            update_conflicts=True,\n            update_fields=[\"id\"],\n            unique_fields=[\"pk\"],\n        )\n", "def test_update_conflicts_ignores_auto_fields(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 1)\n\n    conflicting_object = UpsertConflict(number=1, rank=2, name=\"Steve\")\n    results = UpsertConflict.objects.bulk_create(\n        [conflicting_object],\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\"],\n    )\n    self.assertEqual(len(results), 1)\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(UpsertConflict.objects.count(), 1)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": 1, \"name\": \"Steve\"},\n        ],\n    )\n", "def test_update_conflicts_with_db_column(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(id=1, name=\"old\"),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 1)\n\n    conflicting_objects = [\n        FieldsWithDbColumns(id=1, name=\"new\"),\n    ]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"id\"],\n        update_fields=[\"name\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 1)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"id\", \"name\"),\n        [\n            {\"id\": 1, \"name\": \"new\"},\n        ],\n    )\n", "def test_update_conflicts_nullable_fields(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\", nullable_field=\"old_value\"),\n            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 2)\n\n    conflicting_objects = [\n        UpsertConflict(number=1, rank=4, name=\"Steve\", nullable_field=\"new_value\"),\n        UpsertConflict(number=2, rank=2, name=\"Olivia\", nullable_field=\"new_value\"),\n        UpsertConflict(number=3, rank=1, name=\"Hannah\", nullable_field=None),\n    ]\n    results = UpsertConflict.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\", \"nullable_field\"],\n        unique_fields=[\"number\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\", \"nullable_field\"),\n        [\n            {\"number\": 1, \"rank\": 4, \"name\": \"Steve\", \"nullable_field\": \"new_value\"},\n            {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\", \"nullable_field\": \"new_value\"},\n            {\"number\": 3, \"rank\": 1, \"name\": \"Hannah\", \"nullable_field\": None},\n        ],\n    )\n"], "sample_445": ["def test_years_edge(self):\n    t = datetime.datetime(2022, 1, 1)\n    tests = [\n        (datetime.datetime(2023, 1, 1), \"1\\xa0year\"),\n        (datetime.datetime(2023, 12, 31), \"1\\xa0year, 11\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2024, 1, 1), \"2\\xa0years\"),\n        (datetime.datetime(2024, 12, 31), \"2\\xa0years, 11\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2025, 1, 1), \"3\\xa0years\"),\n        (datetime.datetime(2025, 12, 31), \"3\\xa0years, 11\\xa0months, 4\\xa0weeks\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value), expected)\n", "def test_future_datetime(self):\n    \"\"\"Test with a future datetime.\"\"\"\n    future = self.t + 3 * self.oneweek + 2 * self.oneday + 1 * self.onehour\n    self.assertEqual(timesince(future, self.t), \"3 weeks, 2 days, 1 hour\")\n    self.assertEqual(timeuntil(self.t, future), \"3 weeks, 2 days, 1 hour\")\n", "def test_future_datetime(self):\n    \"\"\"Test when the given datetime is in the future.\"\"\"\n    future_datetime = self.t + self.onehour\n    self.assertEqual(timesince(future_datetime, self.t), \"0\\xa0minutes\")\n    self.assertEqual(timeuntil(future_datetime, self.t), \"1\\xa0hour\")\n", "def test_years_edge(self):\n    t = datetime.datetime(2000, 1, 1)\n    tests = [\n        (datetime.datetime(2000, 12, 31), \"11\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2001, 1, 1), \"1\\xa0year\"),\n        (datetime.datetime(2001, 12, 31), \"1\\xa0year, 11\\xa0months\"),\n        (datetime.datetime(2002, 1, 1), \"2\\xa0years\"),\n        (datetime.datetime(2002, 12, 31), \"2\\xa0years, 11\\xa0months\"),\n        (datetime.datetime(2003, 1, 1), \"3\\xa0years\"),\n        (datetime.datetime(2004, 1, 1), \"4\\xa0years\"),\n        (datetime.datetime(2005, 1, 1), \"5\\xa0years\"),\n        (datetime.datetime(2010, 1, 1), \"10\\xa0years\"),\n        (datetime.datetime(2020, 1, 1), \"20\\xa0years\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value), expected)\n", "def test_time_strings_override(self):\n    \"\"\"Test that time_strings can be overridden.\"\"\"\n    custom_time_strings = {\n        \"year\": npgettext_lazy(\"custom-year\", \"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n        \"month\": npgettext_lazy(\"custom-month\", \"%(num)d mth\", \"%(num)d mths\", \"num\"),\n        \"week\": npgettext_lazy(\"custom-week\", \"%(num)d wk\", \"%(num)d wks\", \"num\"),\n        \"day\": npgettext_lazy(\"custom-day\", \"%(num)d dy\", \"%(num)d dys\", \"num\"),\n        \"hour\": npgettext_lazy(\"custom-hour\", \"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n        \"minute\": npgettext_lazy(\"custom-minute\", \"%(num)d min\", \"%(num)d mins\", \"num\"),\n    }\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings),\n        \"1\\xa0min\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings),\n        \"1\\xa0hr\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings),\n        \"1\\xa0dy\",\n    )\n"], "sample_278": ["def test_expression_wrapper_in_filter(self):\n    wrapped_value = ExpressionWrapper(Value(10), output_field=IntegerField())\n    qs = Number.objects.filter(integer=wrapped_value)\n    self.assertEqual(qs.count(), 1)\n    self.assertEqual(qs.first().integer, 10)\n", "def test_expression_wrapper_output_field(self):\n    expr = ExpressionWrapper(F('integer') + 1, output_field=CharField())\n    self.assertEqual(expr.output_field, CharField())\n", "def test_expression_wrapper_with_f(self):\n    n = Number.objects.create(integer=10, float=123.45)\n    qs = Number.objects.annotate(\n        integer_float=ExpressionWrapper(F('integer'), output_field=FloatField()),\n        float_integer=ExpressionWrapper(F('float'), output_field=IntegerField()),\n    )\n    n = qs.get(pk=n.pk)\n    self.assertEqual(n.integer_float, 10.0)\n    self.assertEqual(n.float_integer, 123)\n", "def test_expression_wrapper_with_annotated_field(self):\n    Company.objects.create(name='Test Co.', num_employees=10, num_chairs=5)\n    qs = Company.objects.annotate(\n        num_employees_plus_10=ExpressionWrapper(F('num_employees') + 10, output_field=IntegerField())\n    ).filter(num_employees_plus_10__gt=20)\n    self.assertEqual(qs.count(), 0)\n", "def test_expression_wrapper_combined(self):\n    expr1 = ExpressionWrapper(Value(3), output_field=IntegerField())\n    expr2 = ExpressionWrapper(Value(5), output_field=IntegerField())\n    self.assertEqual(expr1 + expr2, ExpressionWrapper(Value(8), output_field=IntegerField()))\n    self.assertEqual(expr1 - expr2, ExpressionWrapper(Value(-2), output_field=IntegerField()))\n    self.assertEqual(expr1 * expr2, ExpressionWrapper(Value(15), output_field=IntegerField()))\n    self.assertEqual(expr1 / expr2, ExpressionWrapper(Value(0.6), output_field=FloatField()))\n"], "sample_807": ["def test_calibration_sparse_matrix():\n    \"\"\"Test calibration with sparse matrix input\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf = MultinomialNB()\n    for method in ['isotonic', 'sigmoid']:\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        pc_clf.fit(X_sparse, y)\n        prob_pos_pc_clf_sparse = pc_clf.predict_proba(X_sparse)[:, 1]\n\n        pc_clf.fit(X, y)\n        prob_pos_pc_clf_dense = pc_clf.predict_proba(X)[:, 1]\n\n        assert_array_almost_equal(prob_pos_pc_clf_sparse, prob_pos_pc_clf_dense)\n", "def test_calibration_accepts_ndarray_predict_proba():\n    \"\"\"Test that calibration's predict_proba accepts n-dimensional arrays as input\"\"\"\n    X = np.random.RandomState(42).randn(15, 5, 2)\n    y = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]\n\n    clf = MockTensorClassifier()\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())\n    cal_clf.fit(X, y)\n\n    # Test predict_proba\n    proba = cal_clf.predict_proba(X)\n    assert_equal(proba.shape, (15, 2))\n    assert_array_almost_equal(proba.sum(axis=1), np.ones(proba.shape[0]))\n\n    # Test predict\n    pred = cal_clf.predict(X)\n    assert_equal(pred.shape, (15,))\n    assert_array_equal(pred, np.argmax(proba, axis=1))\n", "def test_calibration_sample_weight_different_length():\n    # Test that calibration raises an error if sample_weight has different length than y\n    X, y = make_classification(n_samples=10, n_features=5)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size - 1)\n    clf = LinearSVC(C=1.0)\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())\n    assert_raises(ValueError, cal_clf.fit, X, y, sample_weight)\n", "def test_calibration_with_decision_function():\n    \"\"\"Test calibration for a classifier that implements decision_function\"\"\"\n    clf = LinearSVC()\n    X, y = make_classification(n_samples=100, n_features=2, random_state=42)\n\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf.fit(X_train, y_train)\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        cal_clf.fit(X_train, y_train)\n        probas = cal_clf.predict_proba(X_test)\n        assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test)))\n\n        # Check that log-loss of calibrated classifier is smaller than\n        # log-loss of naively turned OvR decision function to probabilities\n        # via softmax\n            e = np.exp(-y_pred)\n            return e / e.sum(axis=1).reshape(-1, 1)\n\n        uncalibrated_log_loss = log_loss(y_test, softmax(clf.decision_function(X_test)))\n        calibrated_log_loss = log_loss(y_test, probas)\n        assert_greater_equal(uncalibrated_log_loss, calibrated_log_loss)\n", "def test_calibration_predict_proba_zero():\n    # Test to check that predict_proba returns a probability matrix with all\n    # elements equal to 1/n_classes when all probabilities are 0\n    X = np.random.randn(10, 5)\n    y = np.zeros(10)\n    clf = LinearSVC(C=1.0)\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())\n    cal_clf.fit(X, y)\n\n    proba = cal_clf.predict_proba(X)\n    assert_array_equal(proba, np.full_like(proba, 0.5))\n"], "sample_32": ["def test_de_density_scale(self, cosmo, z):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    expected = (z + 1.0) ** (3.0 * (1.0 + cosmo.w0 - cosmo.wz)) * np.exp(3.0 * cosmo.wz * z)\n    assert u.allclose(cosmo.de_density_scale(z), expected)\n", "def test_de_density_scale(self, cosmo, z):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    expected_value = (z + 1.0) ** (3.0 * (1.0 + cosmo.w0 - cosmo.wz)) * np.exp(3.0 * cosmo.wz * z)\n    assert u.allclose(cosmo.de_density_scale(z), expected_value)\n", "def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    assert u.allclose(cosmo.de_density_scale(1.0), np.exp(3.0))\n    assert u.allclose(\n        cosmo.de_density_scale([0.0, 0.5, 1.0, 1.5, 2.3]),\n        [1.0, 1.875, np.exp(3.0), np.exp(4.5), np.exp(6.9)]\n    )\n", "def test_de_density_scale(self, cosmo, z, expected):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    assert u.allclose(cosmo.de_density_scale(z), expected)\n", "def test_comoving_distance_Ode0(self, cosmo_cls, args, kwargs, expected):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.comoving_distance` for non-zero Ode0.\"\"\"\n    cosmo = cosmo_cls(*args, **kwargs)\n    z = np.array([0.0, 0.5, 1.0, 2.0])\n    assert u.allclose(cosmo.comoving_distance(z), expected, atol=1e-5 * u.Mpc)\n"], "sample_771": ["def test_power_transformer_sparse_input(method, standardize):\n    # check that power transformer works correctly with sparse inputs\n    X = sparse.csr_matrix(X_2d)\n    if method == 'box-cox':\n        X.data = np.abs(X.data)\n\n    pt = PowerTransformer(method, standardize)\n    X_trans = pt.fit_transform(X)\n    assert sparse.issparse(X_trans)\n    assert_array_almost_equal(X_trans.toarray(), pt.fit_transform(X_2d))\n\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert sparse.issparse(X_inv_trans)\n    assert_array_almost_equal(X_inv_trans.toarray(), X_2d)\n", "def test_power_transformer_zero_variance():\n    # Check PowerTransformer on toy data with zero variance feature\n    X = [[0., 1., +0.5],\n         [0., 1., -0.1],\n         [0., 1., +1.1]]\n\n    pt = PowerTransformer(method='yeo-johnson')\n    X_trans = pt.fit_transform(X)\n\n    X_expected = [[0., 0., +0.0],\n                  [0., 0., -1.0],\n                  [0., 0., +1.0]]\n    assert_array_almost_equal(X_trans, X_expected)\n    X_trans_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_trans_inv)\n\n    # make sure new data gets transformed correctly\n    X_new = [[+0., 2., 0.5],\n             [-1., 1., 0.0],\n             [+0., 1., 1.5]]\n    X_trans_new = pt.transform(X_new)\n    X_expected_new = [[+0., 1., +0.],\n                      [-1., 0., -0.83333],\n                      [+0., 0., +1.66667]]\n    assert_array_almost_equal(X_trans_new, X_expected_new, decimal=3)\n", "def test_power_transformer_sparse_input():\n    # Check that sparse input is handled correctly\n    X = sparse.csr_matrix(np.abs(X_2d))\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(\n            X, method='box-cox',\n            standardize=standardize\n        )\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            assert sparse.issparse(X_trans)\n            X_trans = X_trans.toarray()\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.boxcox(X.toarray()[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(sparse.csr_matrix(X_trans))\n            assert_array_almost_equal(X_inv.toarray(), X.toarray())\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_single_feature(method):\n    # Check that the transformer works correctly when there is a single feature\n    X = np.random.RandomState(0).rand(100, 1)\n    if method == 'box-cox':\n        X = np.abs(X)\n    pt = PowerTransformer(method=method)\n    X_trans = pt.fit_transform(X)\n    assert X_trans.shape == (100, 1)\n", "def test_power_transformer_multiple_fits():\n    # Check that fitting multiple times returns the same values as fitting once\n    X = X_2d\n    pt1 = PowerTransformer(method='yeo-johnson', standardize=True)\n    pt2 = PowerTransformer(method='yeo-johnson', standardize=True)\n\n    pt1.fit(X)\n    X_trans1 = pt1.transform(X)\n\n    pt2.fit(X[:len(X)//2]).fit(X[len(X)//2:])\n    X_trans2 = pt2.transform(X)\n\n    assert_array_almost_equal(X_trans1, X_trans2)\n"], "sample_11": ["def test_dropped_dimensions_with_serialized_classes():\n    wcs = WCS_SPECTRAL_CUBE\n\n    sub = SlicedLowLevelWCS(wcs, np.s_[:, 0, 0])\n    sub.serialized_classes = True\n\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    validate_info_dict(dwd, {\n        \"value\": [0.5],\n        \"world_axis_physical_types\": [\"em.freq\"],\n        \"world_axis_names\": [\"Frequency\"],\n        \"world_axis_units\": [\"Hz\"],\n        \"serialized_classes\": True,\n        \"world_axis_object_components\": [('spectral', 0, 'value')],\n        })\n\n    assert wao_classes['spectral'][0:3] == (u.Quantity, (), {\"unit\": \"Hz\"})\n", "def test_world_to_pixel_values_different_int_types():\n    int_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n    np64_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, np.int64(0), :])\n    world_arrays = ([12.86995801, 20.49217541], [20.0, 25.0])\n    for int_coord, np64_coord in zip(int_sliced.world_to_pixel_values(*world_arrays),\n                                     np64_sliced.world_to_pixel_values(*world_arrays)):\n        assert all(int_coord == np64_coord)\n", "def test_pixel_to_world_values_float_types():\n    float_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0.0, :])\n    np64_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, np.float64(0.0), :])\n    pixel_arrays = ([0, 1], [0, 1])\n    for float_coord, np64_coord in zip(float_sliced.pixel_to_world_values(*pixel_arrays),\n                                       np64_sliced.pixel_to_world_values(*pixel_arrays)):\n        assert all(float_coord == np64_coord)\n", "def test_world_to_pixel_values_different_float_types():\n    float_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n    np64_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, np.float64(0), :])\n    world_arrays = ([10, 11], [20, 21], [25, 26])\n    for float_coord, np64_coord in zip(float_sliced.world_to_pixel_values(*world_arrays),\n                                       np64_sliced.world_to_pixel_values(*world_arrays)):\n        assert all(float_coord == np64_coord)\n", "def test_world_to_pixel_values_different_int_types():\n    int_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n    np64_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, np.int64(0), :])\n    world_arrays = ([12.86995801, 10.86995801], [20.49217541, 22.49217541])\n    for int_coord, np64_coord in zip(int_sliced.world_to_pixel_values(*world_arrays),\n                                     np64_sliced.world_to_pixel_values(*world_arrays)):\n        assert all(int_coord == np64_coord)\n"], "sample_1065": ["def test_binomial_expand_func():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    x = Symbol('x')\n\n    assert binomial(n, 0).expand_func() == 1\n    assert binomial(n, 1).expand_func() == n\n    assert binomial(n, 2).expand_func() == n*(n - 1)/2\n    assert binomial(n, k).expand_func() == n*(n - 1)*...*(n - k + 1)/factorial(k)\n    assert binomial(x, k).expand_func() == binomial(x, k)\n", "def test_subfactorial():\n    n = Symbol('n', integer=True, nonnegative=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(n).is_even.func == And\n    assert subfactorial(n).is_odd.func == And\n    assert subfactorial(n).is_integer\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / E\n    assert subfactorial(n).is_nonnegative\n", "def test_binomial_symbolic_evaluation():\n    n, k = symbols('n k', integer=True)\n    assert binomial(n, n) == 1\n    assert binomial(n, 0) == 1\n    assert binomial(n, 1) == n\n    assert binomial(n, n-1) == n\n    assert binomial(n, -k) == 0\n    assert binomial(n, k).subs({n: 5, k: 2}) == 10\n", "def test_rf_ff_evaluation():\n    x, y = symbols('x,y')\n    n, k = symbols('n k', integer=True)\n\n    assert rf(x, y) == x*(x + 1)*...*(x + y - 1)\n    assert ff(x, y) == x*(x - 1)*...*(x - y + 1)\n    assert rf(n, k) == n*(n + 1)*...*(n + k - 1)\n    assert ff(n, k) == n*(n - 1)*...*(n - k + 1)\n    assert rf(n, 0) == 1\n    assert ff(n, 0) == 1\n    assert rf(n, n) == factorial(n)\n    assert ff(n, n) == factorial(n)\n", "def test_subfactorial():\n    n = Symbol('n', integer=True, nonnegative=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(n + 1).func == subfactorial\n\n    assert subfactorial(n).is_even == fuzzy_bool(Not(n % 2))\n    assert subfactorial(n).is_odd == fuzzy_bool(n % 2)\n    assert subfactorial(n).is_integer is True\n    assert subfactorial(n).is_nonnegative is True\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n"], "sample_86": ["def test_lazy_hashing(self):\n    \"\"\"\n    hash() works correctly for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n    self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n", "def test_lazy_modulo(self):\n    \"\"\"\n    % works correctly for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 'Lazy %s', str)\n    lazy_b = lazy(lambda: 'text', str)\n    self.assertEqual(lazy_a() % lazy_b(), 'Lazy text')\n", "    def test_lazy_equality_with_non_promise(self):\n        \"\"\"\n        == and != work correctly for Promises and non-Promise objects.\n        \"\"\"\n        lazy_a = lazy(lambda: 4, int)\n        non_lazy_b = 4\n        non_lazy_c = 5\n\n        self.assertEqual(lazy_a(), non_lazy_b)\n        self.assertNotEqual(lazy_a(), non_lazy_c)\n", "def test_lazy_str(self):\n    \"\"\"\n    lazystr correctly handles str objects.\n    \"\"\"\n    original_string = \"Lazy string\"\n    lazy_string = lazystr(original_string)\n    self.assertEqual(str(lazy_string), original_string)\n    self.assertEqual(repr(lazy_string), repr(original_string))\n", "def test_lazy_hashing(self):\n    \"\"\"\n    Hash values are correct for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n    self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n"], "sample_1199": ["def test_tensor_product_trace():\n    assert Tr(TP(A, B)).doit() == Tr(A)*Tr(B)\n    assert Tr(TP(A, B), indices=[0]).doit() == Tr(A)*B\n    assert Tr(TP(A, B), indices=[1]).doit() == A*Tr(B)\n", "def test_tensor_product_trace():\n    assert Tr(TP(A, B)) == Tr(A)*Tr(B)\n    assert Tr(TP(A, B), indices=[0]) == Tr(A)*B\n    assert Tr(TP(A, B), indices=[1]) == A*Tr(B)\n", "def test_tensor_product_trace():\n    assert Tr(TP(A, B)) == Tr(A)*Tr(B)\n    assert Tr(TP(A, B), indices=[0]) == Tr(A)*B\n    assert Tr(TP(A, B), indices=[1]) == A*Tr(B)\n", "def test_tensor_product_trace():\n    assert Tr(TP(A, B)) == Tr(A) * Tr(B)\n    assert Tr(TP(A, B), indices=(0,)) == Tr(A) * B\n    assert Tr(TP(A, B), indices=(1,)) == A * Tr(B)\n", "def test_tensor_product_trace():\n    assert Tr(TP(A, B), indices=[0]) == Tr(A) * B\n    assert Tr(TP(A, B), indices=[1]) == A * Tr(B)\n    assert Tr(TP(A, B)) == Tr(A) * Tr(B)\n"], "sample_1080": ["def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.positive(x)) == 1\n    assert refine(sign(x), Q.negative(x)) == -1\n", "def test_sign_nonzero():\n    x = Symbol('x', real=True)\n    assert refine(sign(x), Q.nonzero(x)) != 0\n    assert refine(sign(x), Q.nonzero(x) & Q.positive(x)) == 1\n    assert refine(sign(x), Q.nonzero(x) & Q.negative(x)) == -1\n\n    y = Symbol('y', imaginary=True)\n    assert refine(sign(y), Q.nonzero(y)) != 0\n    assert refine(sign(y), Q.nonzero(y) & Q.positive(im(y))) == S.ImaginaryUnit\n    assert refine(sign(y), Q.nonzero(y) & Q.negative(im(y))) == -S.ImaginaryUnit\n\n    z = Symbol('z', complex=True)\n    assert refine(sign(z), Q.nonzero(z) & Q.positive(re(z))) == 1\n    assert refine(sign(z), Q.nonzero(z) & Q.negative(re(z))) == -1\n    assert refine(sign(z), Q.nonzero(z) & Q.positive(im(z))) == S.ImaginaryUnit\n    assert refine(sign(z), Q.nonzero(z) & Q.negative(im(z))) == -S.ImaginaryUnit\n", "def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.zero(x)) == 0\n    assert refine(sign(x), Q.nonzero(x)) == sign(x)\n", "def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.zero(x)) == 0\n    assert refine(sign(x), Q.real(x) & Q.positive(x)) == 1\n    assert refine(sign(x), Q.real(x) & Q.negative(x)) == -1\n    assert refine(sign(x), Q.imaginary(x) & Q.positive(im(x))) == S.ImaginaryUnit\n    assert refine(sign(x), Q.imaginary(x) & Q.negative(im(x))) == -S.ImaginaryUnit\n", "def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.zero(x)) == 0\n    assert refine(sign(x), Q.real(x) & Q.positive(x)) == 1\n    assert refine(sign(x), Q.real(x) & Q.negative(x)) == -1\n    assert refine(sign(x), Q.imaginary(x) & Q.positive(im(x))) == S.ImaginaryUnit\n    assert refine(sign(x), Q.imaginary(x) & Q.negative(im(x))) == -S.ImaginaryUnit\n    assert refine(sign(x), True) == sign(x)\n"], "sample_783": ["def test_simple_imputer_with_constant_strategy(X, missing_values, strategy, fill_value, X_trans_exp):\n    imputer = SimpleImputer(missing_values=missing_values, strategy=strategy, fill_value=fill_value)\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X_trans, X_trans_exp)\n", "def test_imputation_error_input_shape():\n    # Test error is raised when input and statistics_ shape mismatch\n    X = np.array([[np.nan, 1.0], [2.0, 3.0]])\n    imputer = SimpleImputer(strategy=\"mean\")\n    imputer.fit(X)\n    X_mismatch = np.array([[np.nan, 1.0, 2.0]])\n    with pytest.raises(ValueError, match=\"X has 3 features per sample\"):\n        imputer.transform(X_mismatch)\n", "def test_missing_indicator_shape(X, missing_values, n_features):\n    indicator = MissingIndicator(missing_values=missing_values)\n    X_trans = indicator.fit_transform(X)\n    assert X_trans.shape == (X.shape[0], n_features)\n", "def test_imputation_shape_after_deletion(strategy):\n    # Verify the shapes of the imputed matrix for different strategies\n    # when some columns are deleted due to missing values.\n    X = np.random.randn(10, 5)\n    X[::2, 0] = np.nan  # Introduce missing values in the first column\n    X[::2, 2] = np.nan  # Introduce missing values in the third column\n\n    imputer = SimpleImputer(strategy=strategy)\n    X_imputed = imputer.fit_transform(X)\n    assert X_imputed.shape == (10, 3)  # Only three columns should remain after imputation\n", "def test_missing_indicator_transform_new_features(X_fit, X_trans, n_features, features_indices):\n    indicator = MissingIndicator(missing_values=np.nan, features='missing-only', error_on_new=True)\n    indicator.fit(X_fit)\n    with pytest.raises(ValueError, match=\"have missing values in transform but have no missing values in fit\"):\n        indicator.transform(X_trans)\n"], "sample_563": ["def test_paddedbox_padding(pad):\n    ta = TextArea(\"test\")\n    pb = PaddedBox(ta, pad=pad)\n    fig, ax = plt.subplots()\n    ax.add_artist(pb)\n    fig.draw_without_rendering()\n    bbox = pb.get_bbox(fig.canvas.get_renderer())\n    assert bbox.width == ta.get_bbox(fig.canvas.get_renderer()).width + 2 * pad\n    assert bbox.height == ta.get_bbox(fig.canvas.get_renderer()).height + 2 * pad\n", "def test_offsetimage():\n    fig, ax = plt.subplots()\n    data = np.random.random((10, 10))\n    image = OffsetImage(data, zoom=2)\n    ab = AnnotationBbox(image, (0.5, 0.5))\n    ax.add_artist(ab)\n", "def test_anchoredoffsetbox_properties():\n    child = DrawingArea(20, 20, 0, 0, clip=True)\n    ao = AnchoredOffsetbox('upper left', child=child)\n    assert ao.get_child() == child\n\n    new_child = DrawingArea(30, 30, 0, 0, clip=True)\n    ao.set_child(new_child)\n    assert ao.get_child() == new_child\n\n    ao.set_bbox_to_anchor([0.2, 0.3, 0.4, 0.5])\n    assert ao.get_bbox_to_anchor().bounds == (0.2, 0.3, 0.4, 0.5)\n", "def test_anchoredoffsetbox_bbox_to_anchor():\n    fig, ax = plt.subplots()\n\n    # Test default bbox_to_anchor (None)\n    ab = AnchoredOffsetbox(\"upper left\", child=DrawingArea(20, 20))\n    ax.add_artist(ab)\n    assert ab.get_bbox_to_anchor() == ax.bbox\n\n    # Test with custom bbox_to_anchor (list of [left, bottom, width, height])\n    custom_bbox = [0.1, 0.1, 0.2, 0.2]\n    ab = AnchoredOffsetbox(\"upper left\", child=DrawingArea(20, 20), bbox_to_anchor=custom_bbox)\n    ax.add_artist(ab)\n    assert ab.get_bbox_to_anchor().bounds == tuple(custom_bbox)\n\n    # Test with custom bbox_to_anchor (list of [left, bottom])\n    custom_bbox = [0.3, 0.3]\n    ab = AnchoredOffsetbox(\"upper left\", child=DrawingArea(20, 20), bbox_to_anchor=custom_bbox)\n    ax.add_artist(ab)\n    assert ab.get_bbox_to_anchor().bounds == (custom_bbox[0], custom_bbox[1], 0, 0)\n\n    # Test with custom bbox_to_anchor (Bbox instance)\n    from matplotlib.transforms import Bbox\n    custom_bbox = Bbox.from_bounds(0.4, 0.4, 0.1, 0.1)\n    ab = AnchoredOffsetbox(\"upper left\", child=DrawingArea(20, 20), bbox_to_anchor=custom_bbox)\n    ax.add_artist(ab)\n    assert ab.get_bbox_to_anchor() == custom_bbox\n\n    # Test with custom bbox_to_anchor and bbox_transform\n    from matplotlib.transforms import Affine2D\n    custom_", "def test_offsetimage():\n    fig, ax = plt.subplots()\n\n    img = np.random.rand(10, 10)\n    oi = OffsetImage(img, zoom=2)\n    ab = AnchoredOffsetbox('upper left', child=oi)\n    ax.add_artist(ab)\n\n    img = np.random.rand(5, 5)\n    oi = OffsetImage(img, zoom=3, cmap='viridis')\n    ab = AnchoredOffsetbox('upper right', child=oi)\n    ax.add_artist(ab)\n\n    img = np.random.rand(15, 15)\n    oi = OffsetImage(img, zoom=1.5, filternorm=False)\n    ab = AnchoredOffsetbox('lower right', child=oi)\n    ax.add_artist(ab)\n"], "sample_211": ["def test_resolve_redirect_view(self):\n    match = resolve('/redirect/')\n    self.assertIs(match.func.view_class, RedirectView)\n    self.assertEqual(match.func.view_initkwargs['url'], '/')\n", "def test_get_redirect_url_with_query_string(self):\n    \"\"\"\n    Test the get_redirect_url method with query string.\n    \"\"\"\n    view = RedirectView.as_view(url='/bar/', query_string=True)\n    request = self.rf.get('/foo/?pork=spam')\n    request.META['QUERY_STRING'] = 'pork=spam'\n    url = view(request).url\n    self.assertEqual(url, '/bar/?pork=spam')\n", "def test_head_with_options_method(self):\n    \"\"\"\n    Test a view which allows both HEAD and OPTIONS methods.\n    \"\"\"\n    view = SimplePostView.as_view()\n    response = view(self.rf.head('/'))\n    self.assertEqual(response.status_code, 200)\n    response = view(self.rf.options('/'))\n    self.assertEqual(response.status_code, 200)\n    self._assert_allows(response, 'GET', 'HEAD', 'POST', 'OPTIONS')\n", "    def test_get_queryset(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_queryset = [{'name': 'Lennon'}, {'name': 'McCartney'}]\n        test_view.get_queryset = lambda: test_queryset\n        response = test_view.get(self.rf.get('/'))\n        context = response.context_data\n        self.assertEqual(context['object_list'], test_queryset)\n", "    def test_direct_instantiation_with_init_kwargs(self):\n        \"\"\"\n        It should be possible to use the view by directly instantiating it\n        with init kwargs without going through .as_view() (#21564).\n        \"\"\"\n        view = RedirectView(url='/bar/')\n        response = view.dispatch(self.rf.head('/foo/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/')\n"], "sample_740": ["def test_check_array_with_memmap():\n    # Test that check_array does not copy memmap arrays\n    with TempMemmap(shape=(10, 10), dtype=np.float32) as X:\n        X[:] = 0\n        X_checked = check_array(X, copy=False)\n        X_checked[:] = 1\n        assert_array_equal(X.ravel(), X_checked.ravel())\n        X[:] = 0\n", "def test_check_array_with_memmap_and_sparse():\n    # Confirm that input validation code doesn't copy memory mapped arrays\n    # when they are sparse\n\n    check_sparse = lambda x: check_array(x, accept_sparse=True, copy=False)\n\n    with NamedTemporaryFile(prefix='sklearn-test') as tmp:\n        M = np.memmap(tmp, shape=(10, 10), dtype=np.float32)\n        M[:] = 0\n\n        for f in (check_sparse,):\n            X = f(M)\n            X.data[:] = 1\n            assert_array_equal(X.data.ravel(), M.ravel())\n            X.data[:] = 0\n", "def test_check_array_large_indices_non_supported_scipy_version_csr(X_64bit):\n    # Large indices should not be allowed for scipy<0.14.0\n    if not LARGE_SPARSE_SUPPORTED:\n        msg = (\"Scipy version %s does not support large\"\n               \" indices, please upgrade your scipy\"\n               \" to 0.14.0 or above\" % scipy_version)\n        assert_raise_message(ValueError, msg, check_array,\n                             X_64bit, accept_sparse='csr')\n", "def test_check_array_complex_data_warning():\n    # Test that check_array does not raise a warning for complex data\n    X = np.array([[1 + 1j, 2 + 2j], [3 + 3j, 4 + 4j]])\n    assert_no_warnings(check_array, X)\n", "def test_check_array_dense_sparse_interaction():\n    X_dense = np.array([[1, 2], [3, 4]])\n    X_sparse = sp.csr_matrix(X_dense)\n\n    X_checked_dense = check_array(X_dense, accept_sparse='csr')\n    assert_allclose_dense_sparse(X_checked_dense, X_sparse.toarray())\n\n    X_checked_sparse = check_array(X_sparse, dtype='float64', accept_sparse=True)\n    assert_allclose_dense_sparse(X_checked_sparse, X_dense)\n    assert_equal(X_checked_sparse.format, 'csr')\n"], "sample_595": ["def test_encode_decode_errors():\n    data = xr.DataArray([\"a\", \"b\", \"a\\u0100\"])\n    with pytest.raises(UnicodeDecodeError):\n        decoded = data.str.decode(\"ascii\")\n    with pytest.raises(UnicodeEncodeError):\n        encoded = data.str.encode(\"ascii\")\n", "def test_encode_decode_errors():\n    data = xr.DataArray([\"a\", \"b\", \"a\\xe4\"])\n    encoded = data.str.encode(\"utf-8\")\n    # Test with a replacement error handler\n    decoded = encoded.str.decode(\"utf-8\", errors=\"replace\")\n    expected = xr.DataArray([\"a\", \"b\", \"a?\"])\n    assert_equal(decoded, expected)\n\n    # Test with an ignore error handler\n    decoded = encoded.str.decode(\"utf-8\", errors=\"ignore\")\n    expected = xr.DataArray([\"a\", \"b\", \"a\"])\n    assert_equal(decoded, expected)\n", "def test_decode_errors(dtype):\n    # Test handling of decode errors\n    values = xr.DataArray([b\"abc\", b\"\\xc3\", b\"def\"]).astype(dtype)\n\n    result = values.str.decode(\"utf-8\", errors=\"replace\")\n    expected = xr.DataArray([\"abc\", \"\\ufffd\", \"def\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.decode(\"utf-8\", errors=\"ignore\")\n    expected = xr.DataArray([\"abc\", \"\", \"def\"]).astype(dtype)\n    assert_equal(result, expected)\n", "def test_slice_replace_out_of_bounds():\n    da = lambda x: xr.DataArray(x)\n    values = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n\n    expected = da([\"shrt\", \"a it longer\", \"evenlongerthanthat\", \"\"])\n    result = values.str.slice_replace(10, 20)\n    assert_equal(result, expected)\n\n    expected = da([\"shzrt\", \"a zit longer\", \"evenlongerthanthat\", \"z\"])\n    result = values.str.slice_replace(10, 20, \"z\")\n    assert_equal(result, expected)\n\n    expected = da([\"shzrt\", \"a zbit longer\", \"evzenlongerthanthat\", \"z\"])\n    result = values.str.slice_replace(10, 10, \"z\")\n    assert_equal(result, expected)\n", "def test_decode_errors():\n    data = xr.DataArray([b\"a\", b\"b\", b\"a\\xe4\\xff\"])\n    decoded = data.str.decode(\"utf-8\", errors=\"replace\")\n    expected = xr.DataArray([\"a\", \"b\", \"a\ufffd\ufffd\"])\n    assert_equal(decoded, expected)\n"], "sample_123": ["    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n        self.assertEqual(escape_leading_slashes('/path'), '/path')\n        self.assertEqual(escape_leading_slashes('///path'), '/%2F/path')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n", "    def test_parsing_invalid(self):\n        invalid_dates = [\n            'Invalid date',\n            'Sun, 06 Nov 1994 25:49:37 GMT',\n            'Sun, 06 Nov 1994 08:61:37 GMT',\n            'Sun, 06 Nov 1994 08:49:61 GMT',\n            'Sun, 32 Nov 1994 08:49:37 GMT',\n            'Sun, 06 Feb 30 08:49:37 GMT',\n        ]\n        for date in invalid_dates:\n            with self.subTest(date=date):\n                with self.assertRaises(ValueError):\n                    parse_http_date(date)\n", "    def test_parsing_invalid_rfc850(self, mocked_datetime):\n        mocked_datetime.side_effect = datetime\n        mocked_datetime.utcnow = mock.Mock()\n        utcnow = datetime(2020, 11, 6, 8, 49, 37)\n        mocked_datetime.utcnow.return_value = utcnow\n        invalid_rfc850str = 'Invalid, 31-Dec-99 08:49:37 GMT'\n        with self.assertRaises(ValueError):\n            parse_http_date(invalid_rfc850str)\n", "    def test_escaping(self):\n        self.assertEqual(escape_leading_slashes('///example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n"], "sample_735": ["def test_gaussian_mixture_n_iter():\n    # Test that the number of iterations is correct\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 50, 3, 2\n    X = rng.randn(n_samples, n_features)\n    for cv_type in COVARIANCE_TYPE:\n        g = GaussianMixture(\n            n_components=n_components, covariance_type=cv_type,\n            random_state=rng, max_iter=200, tol=1e-6)\n        g.fit(X)\n        assert_greater_equal(g.n_iter_, 1)\n        assert_less_equal(g.n_iter_, 200)\n", "def test_initialize():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_samples, n_features, n_components = rand_data.n_samples, rand_data.n_features, rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        resp = rng.rand(n_samples, n_components)\n        resp /= resp.sum(axis=1)[:, np.newaxis]\n\n        gmm = GaussianMixture(n_components=n_components, covariance_type=covar_type, random_state=rng)\n        gmm._initialize(X, resp)\n\n        # Check if the initial parameters are of the correct shape\n        assert_equal(gmm.weights_.shape, (n_components,))\n        assert_equal(gmm.means_.shape, (n_components, n_features))\n        if covar_type == 'full':\n            assert_equal(gmm.covariances_.shape, (n_components, n_features, n_features))\n        elif covar_type == 'tied':\n            assert_equal(gmm.covariances_.shape, (n_features, n_features))\n        elif covar_type == 'diag':\n            assert_equal(gmm.covariances_.shape, (n_components, n_features))\n        elif covar_type == 'spherical':\n            assert_equal(gmm.covariances_.shape, (n_components,))\n\n        # Check if the initial parameters are not None\n        assert gmm.weights_ is not None\n        assert gmm.means_ is not None\n        assert gmm.covariances_ is not None\n        assert gmm.precisions_cholesky_ is not None\n", "def test_gaussian_mixture_with_custom_init_params():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        Y = rand_data.Y\n        g = GaussianMixture(n_components=rand_data.n_components,\n                            random_state=rng, weights_init=rand_data.weights,\n                            means_init=rand_data.means,\n                            precisions_init=rand_data.precisions[covar_type],\n                            covariance_type=covar_type,\n                            init_params='kmeans')\n\n        # Check a warning message arrives if we use custom init params with 'kmeans'\n        assert_warns_message(UserWarning,\n                             \"Custom initial parameters are ignored when \"\n                             \"using 'kmeans' to initialize the weights and means.\",\n                             g.fit, X)\n", "def test_gaussian_mixture_init_params_kmeans():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_components = rand_data.n_components\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        g = GaussianMixture(n_components=n_components, n_init=1, reg_covar=0,\n                            random_state=rng, covariance_type=covar_type,\n                            init_params='kmeans')\n        g.fit(X)\n        # Verify that the means initialized by kmeans are close to the true means\n        assert_allclose(np.sort(g.means_, axis=0), np.sort(rand_data.means, axis=0),\n                        atol=0.5)\n", "def test_gaussian_mixture_initialization():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        weights = rand_data.weights\n        means = rand_data.means\n        precisions = rand_data.precisions[covar_type]\n        g = GaussianMixture(n_components=n_components, random_state=rng,\n                            weights_init=weights, means_init=means,\n                            precisions_init=precisions,\n                            covariance_type=covar_type)\n        g.fit(X)\n        assert_array_equal(g.weights_init, weights)\n        assert_array_equal(g.means_init, means)\n        assert_array_equal(g.precisions_init, precisions)\n"], "sample_932": ["def test_xref_content_consistency(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag})>(?P<content>.*?)</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_xref_layout(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?>(?P<content>.*?)</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_xref_consistency_extended(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        role_classes = RoleClasses(role, root, contents)\n        for tag in contents:\n            content_classes = role_classes.content_classes[tag]\n            assert role_classes.classes == content_classes, \\\n                f\"Inconsistent classes for role `{role}` with tag `{tag}` in `{test}`\"\n\n    # Add more roles and tags here as needed\n    roles_and_tags = [\n        ('class', ['a', 'span']),\n        ('struct', ['a', 'span']),\n        ('union', ['a', 'span']),\n        ('func', ['a', 'span']),\n        ('member', ['a', 'span']),\n        ('var', ['a', 'span']),\n        ('type', ['a', 'span']),\n        ('concept', ['a', 'span']),\n        ('enum', ['a', 'span']),\n        ('enumerator', ['a', 'span']),\n    ]\n\n    for role, tags in roles_and_tags:\n        assert_class_consistency(role, 'strong', tags)\n", "def test_inline_expression_role():\n    app = restructuredtext.build_restructuredtext(\"Test :cpp:expr:`i++`\")\n    assert_node(app, [desc, desc_signature, inline, emphasis],\n                text=\"Test i++\")\n\n    app = restructuredtext.build_restructuredtext(\"Test :cpp:texpr:`i++`\")\n    assert_node(app, [desc, desc_signature, inline, literal],\n                text=\"Test i++\")\n", "def test_template_function_parsing():\n    check('function', 'template<typename T> void f(T t)', {2: 'I0E1f1T'})\n    check('function', 'template<typename T = int> void f(T t)', {2: 'I0E1f1T'})\n    check('function', 'template<typename... Ts> void f(Ts... ts)', {2: 'IDpE1fDp2Ts'})\n    check('function', 'template<typename T, typename... Ts> void f(T t, Ts... ts)', {2: 'I00DpE1f1T2Ts'})\n"], "sample_99": ["def test_trunc_func_with_none_value(self):\n    self.create_model(None, None)\n    self.assertIsNone(DTModel.objects.annotate(truncated=Trunc('start_datetime', 'year')).first().truncated)\n", "def test_trunc_func_with_tzinfo_and_timezone_override(self):\n    start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n    end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            day_melb=TruncDay('start_datetime'),\n            day_utc=TruncDay('start_datetime', tzinfo=timezone.utc),\n            hour_melb=TruncHour('start_datetime'),\n            hour_utc=TruncHour('start_datetime', tzinfo=timezone.utc),\n        ).order_by('start_datetime').get()\n\n    self.assertEqual(model.day_melb, truncate_to(start_datetime.astimezone(melb), 'day', melb))\n    self.assertEqual(model.day_utc, truncate_to(start_datetime, 'day', timezone.utc))\n    self.assertEqual(model.hour_melb, truncate_to(start_datetime.astimezone(melb), 'hour', melb))\n    self.assertEqual(model.hour_utc, truncate_to(start_datetime, 'hour', timezone.utc))\n", "def test_trunc_func_with_timezone_and_output_field(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=output_field, tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb).time() if output_field == TimeField() else truncate_to(start_datetime.astimezone(melb), kind, melb).date()),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb).time() if output_field == TimeField() else truncate_to(end_datetime.astimezone(melb), kind, melb).date())\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_output_field_kind('day', DateField())\n    test_output_field_kind('hour', TimeField())\n", "def test_extract_func_with_invalid_input(self):\n    msg = 'Extract input expression must be DateField, DateTimeField, TimeField, or DurationField.'\n    with self.assertRaisesMessage(ValueError, msg):\n        list(DTModel.objects.annotate(extracted=Extract('name', 'hour')))\n\n    with self.assertRaisesMessage(\n            ValueError, \"Cannot extract time component 'second' from DateField 'start_date'.\"):\n        list(DTModel.objects.annotate(extracted=Extract('start_date', 'second')))\n\n    msg = \"Cannot extract component 'month' from DurationField 'duration'.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        list(DTModel.objects.annotate(extracted=Extract('duration', 'month')))\n", "def test_extract_with_timezone_edge_cases(self):\n    \"\"\"\n    Test extraction functions with timezones at the edge of daylight saving\n    time changes.\n    \"\"\"\n    # Test timezone with daylight saving time change\n    tz = pytz.timezone('Europe/Paris')\n\n    # Edge of daylight saving time change\n    dst_start = datetime(2016, 3, 27, 2, 0, 0, tzinfo=tz)\n    dst_end = datetime(2016, 10, 30, 3, 0, 0, tzinfo=tz)\n\n    self.create_model(dst_start, dst_end)\n\n    # Test extraction functions at the edge of daylight saving time change\n    for lookup in ('year', 'iso_year', 'month', 'day', 'week', 'week_day', 'quarter', 'hour', 'minute', 'second'):\n        with self.subTest(lookup):\n            qs = DTModel.objects.annotate(extracted=Extract('start_datetime', lookup, tzinfo=tz)).get()\n            self.assertEqual(getattr(qs, 'extracted'), getattr(dst_start, lookup))\n\n    # Test truncation functions at the edge of daylight saving time change\n    for kind in ('year', 'quarter', 'month', 'week', 'day', 'hour', 'minute', 'second'):\n        with self.subTest(kind):\n            qs = DTModel.objects.annotate(truncated=Trunc('start_datetime', kind, tzinfo=tz)).get()\n            self.assertEqual(qs.truncated, truncate_to(dst_start, kind, tzinfo=tz))\n"], "sample_378": ["def test_nullable_fk_before_related_save(self):\n    parent = RelatedObject.objects.create()\n    child = SingleObject.objects.create()\n    parent.single = child\n    RelatedObject.objects.bulk_update([parent], fields=['single'])\n    self.assertEqual(parent.single_id, child.pk)\n    parent.refresh_from_db()\n    self.assertEqual(parent.single, child)\n", "def test_bulk_create_with_none_pk(self):\n    objs = [Note(note='test', misc='test') for _ in range(5)]\n    Note.objects.bulk_create(objs)\n    for obj in objs:\n        self.assertIsNotNone(obj.pk)\n", "def test_nullable_fk_with_null_value(self):\n    parent = RelatedObject.objects.create(single=SingleObject.objects.create())\n    parent.single = None\n    RelatedObject.objects.bulk_update([parent], fields=['single'])\n    parent.refresh_from_db()\n    self.assertIsNone(parent.single)\n", "def test_update_with_callable_default(self):\n    # Test updating a field with a callable default value\n        return 'default'\n\n    class TestModel(models.Model):\n        field = models.CharField(max_length=10, default=callable_default)\n\n    obj = TestModel.objects.create()\n    obj.field = 'updated'\n    TestModel.objects.bulk_update([obj], ['field'])\n    obj.refresh_from_db()\n    self.assertEqual(obj.field, 'updated')\n", "def test_related_object_update(self):\n    parent = RelatedObject.objects.create()\n    child = SingleObject.objects.create()\n    parent.single = child\n    parent.save()\n    new_child = SingleObject.objects.create()\n    parent.single = new_child\n    RelatedObject.objects.bulk_update([parent], fields=['single'])\n    parent.refresh_from_db()\n    self.assertEqual(parent.single, new_child)\n"], "sample_130": ["def test_multiple_lookups(self):\n    query = Query(Author)\n    where = query.build_where(Q(num__gt=2, num__lt=10))\n    self.assertEqual(len(where.children), 2)\n\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n\n    lookup = where.children[1]\n    self.assertIsInstance(lookup, LessThan)\n    self.assertEqual(lookup.rhs, 10)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_query_with_extra_filters(self):\n    query = Query(Author)\n    query.add_q(Q(num__gt=2) & Q(name='John'))\n    where = query.where\n    self.assertEqual(len(where.children), 2)\n    num_gt_lookup, name_exact_lookup = where.children\n    self.assertIsInstance(num_gt_lookup, GreaterThan)\n    self.assertEqual(num_gt_lookup.rhs, 2)\n    self.assertIsInstance(name_exact_lookup, Exact)\n    self.assertEqual(name_exact_lookup.rhs, 'John')\n", "def test_transform_with_none_value(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        with self.assertRaisesMessage(ValueError, \"Cannot use None as a query value\"):\n            query.build_where(Q(name__lower=None))\n", "def test_filtered_relation(self):\n    query = Query(Author)\n    query.add_filtered_relation(FilteredRelation('books', None, Q(books__title='Test')), 'books')\n    where = query.build_where(Q(books__publisher='Test Publisher'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.rhs, 'Test Publisher')\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('books').related_model._meta.get_field('publisher'))\n", "def test_complex_filter_with_transform(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(~Q(num__gt=2) & Q(name__lower__exact='foo'))\n    self.assertTrue(where.negated)\n    num_lookup, name_lookup = where.children\n    self.assertIsInstance(num_lookup, GreaterThan)\n    self.assertEqual(num_lookup.rhs, 2)\n    self.assertEqual(num_lookup.lhs.target, Author._meta.get_field('num'))\n    self.assertIsInstance(name_lookup, Exact)\n    self.assertIsInstance(name_lookup.lhs, Lower)\n    self.assertIsInstance(name_lookup.lhs.lhs, SimpleCol)\n    self.assertEqual(name_lookup.lhs.lhs.target, Author._meta.get_field('name'))\n"], "sample_23": ["def test_angle_with_unicode_characters():\n    a = Angle(\"180\u00b0\")\n    assert_allclose(a.degree, 180.0)\n\n    a = Angle(\"1\u00b02\u20323\u2033\")\n    assert_allclose(a.degree, 1.03416667)\n\n    a = Angle(\"1\u00b02\u20323\u2033N\")\n    assert_allclose(a.degree, 1.03416667)\n\n    a = Angle(\"1d2m3.4s\")\n    assert_allclose(a.degree, 1.03427778)\n\n    a = Angle(\"1d2m3.4sS\")\n    assert_allclose(a.degree, -1.03427778)\n\n    a = Angle(\"-1h2m3s\")\n    assert_allclose(a.hour, -1.03416667)\n\n    a = Angle(\"-1h2m3sE\")\n    assert_allclose(a.hour, -1.03416667)\n\n    a = Angle(\"-1h2.5m\")\n    assert_allclose(a.hour, -1.04166667)\n\n    a = Angle(\"-1h2.5mW\")\n    assert_allclose(a.hour, 1.04166667)\n", "def test_angle_quantity_operations():\n    \"\"\"\n    Test operations between Angle and Quantity objects\n    \"\"\"\n    a1 = Angle(54.12412, unit=u.degree)\n    q1 = 2.0 * u.deg\n\n    # Addition\n    result = a1 + q1\n    assert isinstance(result, Angle)\n    assert_allclose(result.degree, 56.12412)\n\n    # Subtraction\n    result = a1 - q1\n    assert isinstance(result, Angle)\n    assert_allclose(result.degree, 52.12412)\n\n    # Multiplication\n    result = a1 * q1\n    assert isinstance(result, u.Quantity)\n    assert_allclose(result.value, 108.24824)\n    assert result.unit == u.deg**2\n\n    # Division\n    result = a1 / q1\n    assert isinstance(result, u.Quantity)\n    assert_allclose(result.value, 27.06206)\n    assert result.unit == u.dimensionless_unscaled\n", "def test_angle_to_string_format():\n    \"\"\"\n    Test that Angle.to_string() works correctly with the format parameter.\n    \"\"\"\n    angle = Angle(45, u.deg)\n\n    assert angle.to_string(format=\"unicode\") == \"45\u00b0\"\n    assert angle.to_string(format=\"latex\") == \"$45^\\circ$\"\n    assert angle.to_string(format=\"latex_inline\") == r\"$45^\\circ$\"\n    assert angle.to_string(format=\"latex\", decimal=True) == r\"$45.0^\\circ$\"\n    assert angle.to_string(format=\"unicode\", decimal=True) == \"45.0\u00b0\"\n\n    with pytest.raises(ValueError):\n        angle.to_string(format=\"invalid_format\")\n", "def test_angle_hms_dms_roundtrip():\n    \"\"\"\n    Test that converting from hms to dms and back gives the same value\n    \"\"\"\n    a = Angle(\"1h2m3s\")\n    a_dms = a.dms\n    a_hms_from_dms = Angle(dms_to_hours(*a_dms), unit=u.hourangle)\n    assert_allclose(a.hourangle, a_hms_from_dms.hourangle)\n\n    a = Angle(\"1d2m3s\")\n    a_hms = a.hms\n    a_dms_from_hms = Angle(dms_to_degrees(*a_hms), unit=u.degree)\n    assert_allclose(a.degree, a_dms_from_hms.degree)\n", "def test_longitude_out_of_limits():\n    \"\"\"\n    Test that values slightly larger than 2*pi are rejected for different dtypes.\n    Test cases for issue #13708\n    \"\"\"\n    with pytest.raises(ValueError, match=r\"Longitude angle\\(s\\) must be within.*\"):\n        Longitude(2 * np.pi + 0.0001, u.rad, dtype=np.float32)\n"], "sample_1135": ["def test_divmod_with_zero_divisor():\n    with raises(ZeroDivisionError):\n        divmod(x, 0)\n\n    with raises(ZeroDivisionError):\n        divmod(0, x)\n", "def test_divmod_is_zero():\n    x, y = symbols('x y', zero=True)\n    assert divmod(x, y) == (S.Zero, S.Zero)\n\n    assert divmod(0, x) == (S.Zero, S.Zero)\n    assert divmod(0, y) == (S.Zero, S.Zero)\n", "def test_Pow_as_coeff_mul_doesnt_expand_issue_15493():\n    i, j = symbols('i j', integer=True, positive=True)\n    assert Pow(3*i, 2, evaluate=False).as_coeff_mul() == (9, (i, 2))\n    assert Pow(8*i/j, 4, evaluate=False).as_coeff_mul() == (16*64, (i/j, 4))\n    assert Pow(8*i, 4, evaluate=False).as_coeff_mul() == (65536, (i, 4))\n", "def test_divmod_with_integers():\n    assert divmod(10, 3) == (3, 1)\n    assert divmod(-10, 3) == (-4, 2)\n    assert divmod(10, -3) == (-4, -2)\n    assert divmod(-10, -3) == (3, -1)\n\n    # Testing with zero\n    assert divmod(10, 0) == (nan, 10)\n    assert divmod(0, 3) == (0, 0)\n    assert divmod(0, 0) == (nan, nan)\n\n    # Testing with symbols\n    x, y = symbols('x y', integer=True)\n    assert divmod(x, y) == (x//y, x % y)\n    assert divmod(x, 3) == (x//3, x % 3)\n    assert divmod(3, x) == (3//x, 3 % x)\n\n    # Testing with complex numbers\n    assert divmod(x + I*y, 2) == (x//2 + I*(y//2), x % 2 + I*(y % 2))\n", "def test_issue_15108():\n    assert Add(*[x**2], evaluate=False).is_Add is True\n    assert Add(*[x**2], evaluate=True).is_Pow is True\n    assert Add(*[x**2], evaluate=False).expand() == x**2\n"], "sample_556": ["def test_figure_add_axes_no_args():\n    fig = plt.figure()\n    with pytest.raises(TypeError, match=\"missing 1 required positional argument: 'rect'\"):\n        fig.add_axes()\n", "def test_savefig_metadata_unsupported():\n    with pytest.raises(ValueError):\n        Figure().savefig(io.BytesIO(), format='jpg', metadata={})\n    with pytest.raises(ValueError):\n        Figure().savefig(io.BytesIO(), format='tiff', metadata={})\n    with pytest.raises(ValueError):\n        Figure().savefig(io.BytesIO(), format='webp', metadata={})\n    with pytest.raises(ValueError):\n        Figure().savefig(io.BytesIO(), format='raw', metadata={})\n    with pytest.raises(ValueError):\n        Figure().savefig(io.BytesIO(), format='rgba', metadata={})\n", "def test_layout_change_padding(layout):\n    \"\"\"\n    Check that the padding can be changed after the layout engine is set.\n    \"\"\"\n    fig, ax = plt.subplots(layout=layout)\n    fig.set_layout_engine(layout, pad=1.0)\n    assert fig.get_layout_engine().get_info()['pad'] == 1.0\n    fig.set_layout_engine(layout, pad=2.0)\n    assert fig.get_layout_engine().get_info()['pad'] == 2.0\n", "def test_subfigure_size():\n    fig = plt.figure(constrained_layout=True)\n    gs = fig.add_gridspec(2, 2)\n    sub_fig = fig.add_subfigure(gs[0, :])\n    sub_fig.set_size_inches(5, 3)\n    np.testing.assert_allclose(sub_fig.get_size_inches(), [5, 3])\n", "def test_suptitle_transform():\n    fig = plt.figure()\n    fig.suptitle('title', x=0.2, y=0.8, transform=fig.transFigure)\n    fig.draw_without_rendering()\n    assert fig.texts[0].get_transform() == fig.transFigure\n    assert fig.texts[0].get_position() == (0.2, 0.8)\n"], "sample_371": ["    def test_unicode_error(self):\n        try:\n            # This will raise a UnicodeDecodeError\n            '\\x80'.encode('ascii')\n        except UnicodeError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        request = self.rf.get('/')\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('Unicode error hint', html)\n        self.assertIn('The string that could not be encoded/decoded was: \\'\\\\x80\\'', html)\n\n        text = reporter.get_traceback_text()\n        self.assertIn('Unicode error hint', text)\n        self.assertIn('The string that could not be encoded/decoded was: \\\\x80', text)\n", "def test_sensitive_variables_with_non_string_keys(self):\n    \"\"\"\n    The sensitive_variables decorator should handle non-string keys in the\n    frame's local variables dictionary without raising an exception.\n    \"\"\"\n    request = self.rf.post('/some_url/', self.breakfast_data)\n    with self.assertRaises(TypeError):\n        sensitive_variables()(lambda: {42: \"should not be displayed\"})()\n    response = sensitive_view(request)\n    self.assertNotContains(response, \"should not be displayed\", status_code=500)\n", "    def test_non_ascii_in_settings(self):\n        \"\"\"\n        The debug page should handle non-ASCII characters in settings.\n        \"\"\"\n        with self.settings(DEBUG=True, FOOBAR='S\u00fcper_S\u00ebcr\u00eat'):\n            response = self.client.get('/raises500/')\n            self.assertContains(response, 'S\u00fcper_S\u00ebcr\u00eat', status_code=500)\n", "    def test_non_ascii_unicode_error(self):\n        try:\n            'non-ASCII }^\\u2019'.encode('ascii')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        text = reporter.get_traceback_text()\n        self.assertIn(\"'non-ASCII }^\\\\u2019'\", text)\n", "    def test_sensitive_post_parameters_decorator(self):\n        @sensitive_post_parameters('sausage-key', 'bacon-key')\n            return sensitive_view(request)\n\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(test_view)\n            self.verify_unsafe_email(test_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(test_view)\n            self.verify_safe_email(test_view)\n"], "sample_384": ["def test_nonexistent_related_field(self):\n    with self.assertRaisesMessage(\n        FieldDoesNotExist, \"Note has no field named 'nonexistent_related'\"\n    ):\n        Note.objects.bulk_update([], [\"nonexistent_related\"])\n", "def test_update_functions(self):\n    notes = [Note.objects.create(note=str(i)) for i in range(10)]\n    for note in notes:\n        note.note = Lower(F(\"note\"))\n    Note.objects.bulk_update(notes, [\"note\"])\n    self.assertCountEqual(Note.objects.values_list(\"note\", flat=True), [str(i) for i in range(10)])\n", "def test_update_m2m_fields(self):\n    # Create notes and tags\n    notes = [Note.objects.create(note=str(i)) for i in range(10)]\n    tags = [Tag.objects.create(name=str(i)) for i in range(10)]\n\n    # Assign tags to notes\n    for note in notes:\n        note.tags.set(tags[:5])  # Assign the first 5 tags to each note\n\n    # Update tags for notes\n    for note in notes:\n        note.tags.set(tags[5:])  # Change tags to the last 5 tags\n\n    # Use bulk_update to update the many-to-many relationship\n    Note.objects.bulk_update(notes, [\"tags\"])\n\n    # Verify the changes\n    for note in notes:\n        note.refresh_from_db()\n        self.assertCountEqual(note.tags.all(), tags[5:])\n", "    def test_functions_with_null(self):\n        Note.objects.update(note=None)\n        for note in self.notes:\n            note.note = Lower(\"note\")\n        Note.objects.bulk_update(self.notes, [\"note\"])\n        self.assertEqual(set(Note.objects.values_list(\"note\", flat=True)), {None})\n", "def test_bulk_update_with_expressions(self):\n    notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(10)]\n    for note in notes:\n        note.note = F(\"note\") + \" updated\"\n        note.misc = F(\"misc\") + \" changed\"\n    Note.objects.bulk_update(notes, [\"note\", \"misc\"])\n    updated_notes = Note.objects.filter(note__contains=\"updated\", misc__contains=\"changed\")\n    self.assertCountEqual(updated_notes, notes)\n"], "sample_203": ["def test_file_extension_validator(self):\n    validator = validators.FileExtensionValidator(allowed_extensions=['txt', 'pdf'])\n    valid_file = SimpleUploadedFile(\"file.txt\", b\"file_content\")\n    invalid_file = SimpleUploadedFile(\"file.doc\", b\"file_content\")\n\n    self.assertIsNone(validator(valid_file))\n\n    with self.assertRaises(ValidationError):\n        validator(invalid_file)\n", "def test_file_extension_validator(self):\n    validator = validators.FileExtensionValidator(allowed_extensions=['txt', 'pdf'])\n    valid_file = SimpleUploadedFile(\"valid_file.txt\", b\"file_content\")\n    invalid_file = SimpleUploadedFile(\"invalid_file.doc\", b\"file_content\")\n\n    # Valid file should not raise an exception\n    try:\n        validator(valid_file)\n    except ValidationError:\n        self.fail(\"FileExtensionValidator raised ValidationError unexpectedly!\")\n\n    # Invalid file should raise an exception\n    with self.assertRaises(ValidationError):\n        validator(invalid_file)\n", "def test_validate_file_extension(self):\n    validator = validators.FileExtensionValidator(allowed_extensions=['jpg', 'png'])\n    valid_file = SimpleUploadedFile(\"valid_image.jpg\", b\"file_content\", content_type=\"image/jpeg\")\n    invalid_file = SimpleUploadedFile(\"invalid_image.txt\", b\"file_content\", content_type=\"text/plain\")\n\n    # Test valid file extension\n    self.assertIsNone(validator(valid_file))\n\n    # Test invalid file extension\n    with self.assertRaises(ValidationError):\n        validator(invalid_file)\n", "def test_file_extension_validator(self):\n    validator = validators.FileExtensionValidator(allowed_extensions=['txt', 'pdf'])\n    invalid_file = SimpleUploadedFile(\"invalid_file.doc\", b\"file_content\", content_type=\"application/msword\")\n    with self.assertRaises(ValidationError) as e:\n        validator(invalid_file)\n    self.assertEqual(e.exception.message, 'File extension \u201cdoc\u201d is not allowed. Allowed extensions are: txt, pdf.')\n\n    valid_file = SimpleUploadedFile(\"valid_file.txt\", b\"file_content\", content_type=\"text/plain\")\n    try:\n        validator(valid_file)\n    except ValidationError:\n        self.fail(\"FileExtensionValidator raised ValidationError unexpectedly!\")\n", "def test_file_extension_validator(self):\n    validator = validators.FileExtensionValidator(allowed_extensions=['txt', 'pdf'])\n\n    # Test with allowed extension\n    file = SimpleUploadedFile(\"example.txt\", b\"file_content\", content_type=\"text/plain\")\n    validator(file)  # This should not raise any error\n\n    # Test with disallowed extension\n    file = SimpleUploadedFile(\"example.docx\", b\"file_content\", content_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n    with self.assertRaises(ValidationError) as e:\n        validator(file)\n    self.assertEqual(e.exception.code, 'invalid_extension')\n    self.assertEqual(e.exception.params, {\n        'extension': 'docx',\n        'allowed_extensions': 'txt, pdf',\n        'value': file,\n    })\n"], "sample_918": ["def test_pyexception_signature_old(app):\n    text = (\".. py:exception:: IOError\\n\"\n            \"   :annotation: exception\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pyfunction_signature_with_module(app):\n    text = \".. py:function:: hello(name: str) -> str\\n   :module: example\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"example.\"],\n                                                    [desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"str\"])])\n", "def test_pyfunction_signature_no_args(app):\n    text = \".. py:function:: hello()\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist)],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1], desc_parameterlist, [])\n", "def test_pyfunction_signature_with_annotations(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [desc_sig_name, pending_xref, \"str\"])])\n    assert_node(doctree[1][0][2],\n                [desc_returns, pending_xref, \"str\"])\n", "def test_pyclass_signature_with_annotation(app):\n    text = \".. py:class:: ExampleClass(BaseClass)\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"ExampleClass\"],\n                                                    desc_parameterlist,\n                                                    [desc_addname, \"BaseClass\"])],\n                                  desc_content)]))\n\n    assert 'ExampleClass' in domain.objects\n    assert domain.objects['ExampleClass'] == ('index', 'ExampleClass', 'class')\n"], "sample_369": ["def test_add_model_with_field_removed_from_parent_model(self):\n    \"\"\"\n    Removing a field from a parent model takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'parent', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'parent', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'child', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.parent',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='parent')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='child')\n", "def test_add_model_with_base_model_added(self):\n    \"\"\"\n    Adding a new base model takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n        ModelState('app', 'newbase', [\n            ('title', models.CharField(max_length=250)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel', 'AddField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='newbase')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='title', model_name='book')\n", "def test_add_field_with_index(self):\n    \"\"\"#23562 - Adding a field with db_index=True should add an Index operation.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_indexed_name])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddIndex\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', index=models.Index(fields=['name'], name='testapp_author_name_idx'))\n", "def test_add_field_with_decimal_places(self):\n    \"\"\"#23405 - Adding a DecimalField with decimal_places should not prompt for a default.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_decimal_field])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"decimal_field\")\n", "def test_many_to_many_field_with_through(self):\n    class CustomThrough(models.Model):\n        author = models.ForeignKey('Author', on_delete=models.CASCADE)\n        book = models.ForeignKey('Book', on_delete=models.CASCADE)\n\n    author = ModelState('app', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n    ])\n    book = ModelState('app', 'Book', [\n        ('id', models.AutoField(primary_key=True)),\n    ])\n    author_with_m2m = ModelState('app', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('books', models.ManyToManyField(book, through='app.CustomThrough')),\n    ])\n\n    changes = self.get_changes([author, book], [author_with_m2m, book])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel', 'AddField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='CustomThrough')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='books', model_name='author')\n"], "sample_974": ["def test_ccode_AugmentedAssignment():\n    expr = aug_assign(x, '+=', y + z)\n    assert ccode(expr) == 'x += y + z;'\n", "def test_ccode_AugmentedAssignment():\n    expr = aug_assign(x, '+=', y * z)\n    assert ccode(expr) == 'x += y * z;'\n", "def test_ccode_for_loop():\n    expr = For(x, Range(0, 10), x + 1)\n    assert ccode(expr) == (\n        \"for (int x=0; x<10; x++){\\n\"\n        \"   x = x + 1;\\n\"\n        \"}\")\n", "def test_ccode_dereference():\n    expr = x + y\n    assert ccode(expr, dereference=[x]) == '(*x) + y'\n\n    expr = x * y\n    assert ccode(expr, dereference=[x, y]) == '(*x) * (*y)'\n\n    expr = x ** y\n    assert ccode(expr, dereference=[x, y]) == 'pow((*x), (*y))'\n", "def test_ccode_aug_assign():\n    assert ccode(aug_assign(x, '+=', y + z)) == 'x += y + z;'\n    assert ccode(aug_assign(x, '-=', y - z)) == 'x -= y - z;'\n    assert ccode(aug_assign(x, '*=', y * z)) == 'x *= y * z;'\n    assert ccode(aug_assign(x, '/=', y / z)) == 'x /= y / z;'\n"], "sample_388": ["    def test_configure_user_deprecation(self):\n        msg = (\n            \"`created=True` must be added to the signature of \"\n            \"CustomRemoteUserBackend.configure_user().\"\n        )\n        with self.assertWarnsMessage(RemovedInDjango50Warning, msg):\n            CustomRemoteUserBackend().configure_user(None, User())\n", "def test_user_email_update(self):\n    \"\"\"\n    The user's email address should be updated if it changes between requests.\n    \"\"\"\n    User.objects.create(username=\"knownuser\", email=\"old@example.com\")\n    response = self.client.get(\n        \"/remote_user/\",\n        **{\n            self.header: \"knownuser\",\n            self.email_header: \"new@example.com\",\n        },\n    )\n    self.assertEqual(response.context[\"user\"].email, \"new@example.com\")\n    knownuser = User.objects.get(username=\"knownuser\")\n    self.assertEqual(knownuser.email, \"new@example.com\")\n", "    def test_clean_username(self):\n        \"\"\"\n        The clean_username method should remove the email from the username.\n        \"\"\"\n        response = self.client.get(\n            \"/remote_user/\",\n            **{\n                self.header: \"newuser@example.com\",\n                self.email_header: \"user@example.com\",\n            },\n        )\n        self.assertEqual(response.context[\"user\"].username, \"newuser\")\n        self.assertEqual(response.context[\"user\"].email, \"user@example.com\")\n", "    def test_superuser_creation(self):\n        num_users = User.objects.count()\n        num_superusers = User.objects.filter(is_superuser=True).count()\n        response = self.client.get(\n            \"/remote_user/\",\n            **{\n                self.header: \"superuser\",\n                \"REMOTE_SUPERUSER\": \"1\",\n            },\n        )\n        self.assertEqual(response.context[\"user\"].username, \"superuser\")\n        self.assertTrue(response.context[\"user\"].is_superuser)\n        self.assertEqual(User.objects.count(), num_users + 1)\n        self.assertEqual(User.objects.filter(is_superuser=True).count(), num_superusers + 1)\n", "    def test_configure_user_created(self):\n        \"\"\"\n        Tests the configure_user method when a user is created.\n        \"\"\"\n        response = self.client.get(\"/remote_user/\", **{self.header: \"newuser\"})\n        self.assertEqual(response.context[\"user\"].username, \"newuser\")\n        self.assertEqual(response.context[\"user\"].email, \"\")\n        self.assertEqual(response.context[\"user\"].last_name, \"\")\n"], "sample_817": ["def test_non_default_threshold():\n    # Test VarianceThreshold with non-default threshold\n    threshold = 1.5\n    for X in [data, csr_matrix(data)]:\n        sel = VarianceThreshold(threshold=threshold).fit(X)\n        expected_support = [1, 3]\n        assert_array_equal(expected_support, sel.get_support(indices=True))\n\n    # Test that ValueError is raised when all variances are less than threshold\n    with pytest.raises(ValueError):\n        VarianceThreshold(threshold=10).fit(data)\n", "def test_sparse_zero_variance():\n    # Test VarianceThreshold with sparse matrix and zero variance.\n    X = csc_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    sel = VarianceThreshold().fit(X)\n    assert_equal(sel.get_support(indices=True), [])\n", "def test_variance_threshold_with_constant_values():\n    # Test VarianceThreshold with threshold=0 and constant values.\n    # The constant values should be removed.\n    X = [[0, 0, 0, 0],\n         [1, 1, 1, 1],\n         [2, 2, 2, 2]]\n    sel = VarianceThreshold(threshold=0).fit(X)\n    assert_array_equal([], sel.get_support(indices=True))\n", "def test_sparse_matrix_zero_variance():\n    # Test VarianceThreshold with sparse matrix and zero variance.\n    # This test case is to improve coverage of the existing unit test file for the sparse matrix input.\n    X = csr_matrix([[1, 2], [1, 2], [1, 2]])\n    sel = VarianceThreshold().fit(X)\n    assert_array_equal([], sel.get_support(indices=True))\n    assert_raises(ValueError, VarianceThreshold().fit, X)\n", "def test_sparse_zero_variance():\n    # Test VarianceThreshold with zero variance on sparse matrices with different densities.\n    X = bsr_matrix(data, blocksize=(2, 2))\n    sel = VarianceThreshold().fit(X)\n    assert_array_equal([0, 1, 3, 4], sel.get_support(indices=True))\n\n    X = bsr_matrix(data, blocksize=(1, 1))\n    sel = VarianceThreshold().fit(X)\n    assert_array_equal([0, 1, 3, 4], sel.get_support(indices=True))\n"], "sample_259": ["def test_foreignkey_forward_then_m2m(self):\n    \"\"\"A foreignkey can be followed through another m2m.\"\"\"\n    readers = list(Reader.objects.all())\n    with self.assertNumQueries(2):\n        prefetch_related_objects(readers, 'books_read__authors')\n\n    with self.assertNumQueries(0):\n        self.assertEqual(\n            [\n                [[str(a) for a in b.authors.all()] for b in r.books_read.all()]\n                for r in readers\n            ],\n            [\n                [['Charlotte', 'Anne', 'Emily'], ['Jane']],  # Amy - Poems, Sense and Sensibility\n                [['Charlotte'], ['Jane']],                    # Belinda - Jane Eyre, Sense and Sensibility\n            ]\n        )\n", "def test_m2m_through_forward(self):\n    reader1 = Reader.objects.get(id=self.reader1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects([reader1], 'books_read')\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(reader1.books_read.all(), [self.book1, self.book4])\n", "def test_m2m_then_foreignkey(self):\n    \"\"\"A m2m can be followed through a foreign key.\"\"\"\n    authors = list(Author.objects.all())\n    with self.assertNumQueries(2):\n        prefetch_related_objects(authors, 'books__first_time_author')\n\n    with self.assertNumQueries(0):\n        self.assertEqual(\n            [\n                [b.first_time_author for b in a.books.all()]\n                for a in authors\n            ],\n            [\n                [self.author1, self.author1],  # Charlotte - Poems, Jane Eyre\n                [self.author1],               # Anne - Poems\n                [self.author3, None],         # Emily - Poems, Wuthering Heights\n                [self.author4],               # Jane - Sense and Sense\n            ]\n        )\n", "def test_m2m_then_foreignkey(self):\n    \"\"\"A m2m can be followed through a foreignkey.\"\"\"\n    authors = list(Author.objects.all())\n    with self.assertNumQueries(2):\n        prefetch_related_objects(authors, 'books__first_time_authors')\n\n    with self.assertNumQueries(0):\n        self.assertEqual(\n            [\n                [[str(r) for r in b.first_time_authors.all()] for b in a.books.all()]\n                for a in authors\n            ],\n            [\n                [['Charlotte'], ['Charlotte']],  # Charlotte - Poems, Jane Eyre\n                [['Anne']],                     # Anne - Poems\n                [['Emily'], []],                 # Emily - Poems, Wuthering Heights\n                [['Jane']],                      # Jane - Sense and Sense\n            ]\n        )\n", "def test_prefetch_object_with_queryset(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    book4 = Book.objects.get(id=self.book4.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects([book1], Prefetch('authors', queryset=Author.objects.filter(name__startswith='C')))\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.authors.all(), [self.author1, self.author3])\n        with self.assertRaises(AttributeError):\n            book4.authors  # authors should not be cached\n"], "sample_169": ["def test_key_exact_with_transform(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a=KeyTransform('b', Value({'b': 'b'}))),\n        [self.objs[3], self.objs[4]],\n    )\n", "def test_deep_lookup_with_null_key(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__k__j__isnull=True),\n        self.objs[:4] + self.objs[5:],\n    )\n", "def test_key_transform_invalid_parameter(self):\n    with self.assertRaisesMessage(TypeError, \"key_names must be a list or tuple.\"):\n        NullableJSONModel.objects.filter(value__contains={}).annotate(\n            key=KeyTransform(123, 'value'),\n        )\n", "def test_key_sql_injection_escape_nested(self):\n    query = str(JSONModel.objects.filter(**{\n        \"\"\"value__test__nested\") = '\"a\"' OR 1 = 1 OR (\"d\"\"\": 'x',\n    }).query)\n    self.assertIn('\"test\"->\"nested\\\\\"', query)\n    self.assertIn('\\\\\"d', query)\n", "def test_deep_lookup_bool(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__h=True),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__i=False),\n        [self.objs[4]],\n    )\n"], "sample_561": ["def test_marker_style_bool():\n    marker = markers.MarkerStyle(\"o\")\n    assert bool(marker)\n    marker = markers.MarkerStyle(None)\n    assert not bool(marker)\n", "def test_marker_scaled_symmetry():\n    marker = markers.MarkerStyle(\"o\")\n    new_marker = marker.scaled(2)\n    symmetric_marker = new_marker.scaled(0.5)\n    assert symmetric_marker.get_user_transform() == marker.get_user_transform()\n", "def test_marker_scaled_with_transform():\n    marker = markers.MarkerStyle(\"2\", transform=Affine2D().translate(1, 1))\n    new_marker = marker.scaled(2)\n    assert new_marker is not marker\n    expected = Affine2D().translate(1, 1).scale(2)\n    assert new_marker.get_user_transform() == expected\n    assert marker._user_transform is not new_marker._user_transform\n", "def test_marker_scaled_with_transform():\n    marker = markers.MarkerStyle(\"1\", transform=Affine2D().translate(1, 1))\n    new_marker = marker.scaled(2, 3)\n    assert new_marker is not marker\n    expected = Affine2D().translate(1, 1).scale(2, 3)\n    assert new_marker.get_user_transform() == expected\n    assert marker._user_transform is not new_marker._user_transform\n", "def test_marker_fillstyle_transform(fillstyle, expected_transforms):\n    marker = markers.MarkerStyle('o', fillstyle=fillstyle)\n    assert marker.get_fillstyle() == fillstyle\n    assert marker.get_alt_transform() == expected_transforms[0]\n    assert marker.get_transform() == expected_transforms[1]\n"], "sample_374": ["def test_nested_prefetch_related_with_to_attr(self):\n    \"\"\"\n    Nested prefetches with to_attr are allowed.\n    \"\"\"\n    occupants = Person.objects.prefetch_related(\n        Prefetch('houses', to_attr='person_houses'),\n        Prefetch('person_houses__rooms', to_attr='person_rooms'),\n    )\n    houses = House.objects.prefetch_related(Prefetch('occupants', queryset=occupants))\n    with self.assertNumQueries(5):\n        self.traverse_qs(list(houses), [['occupants', 'person_houses', 'person_rooms']])\n", "def test_nested_prefetch_through_foreign_key(self):\n    with self.assertNumQueries(3):\n        rooms = Room.objects.prefetch_related(\n            Prefetch(\n                'house',\n                queryset=House.objects.prefetch_related('occupants'),\n            ),\n        )\n        for room in rooms:\n            self.assertIsInstance(room.house.occupants.all(), QuerySet)\n", "def test_nested_prefetch_related_with_multiple_prefetchers(self):\n    \"\"\"\n    Nested prefetches whose name clashes with descriptor names\n    (Person.houses here) are allowed.\n    \"\"\"\n    houses_qs = House.objects.prefetch_related(Prefetch('rooms'))\n    persons = Person.objects.prefetch_related(\n        Prefetch('houses', queryset=houses_qs, to_attr='some_attr_name'),\n        Prefetch('houses', queryset=House.objects.prefetch_related('main_room')),\n    )\n    with self.assertNumQueries(5):\n        self.traverse_qs(list(persons), [['houses', 'rooms'], ['houses', 'main_room']])\n", "def test_prefetch_related_objects_with_nested_prefetch(self):\n    # Test that prefetch_related_objects works correctly with nested prefetches\n    house = House.objects.prefetch_related(\n        Prefetch('rooms', queryset=Room.objects.all(), to_attr='all_rooms')\n    ).get(pk=self.house.pk)\n    prefetch_related_objects([house], 'all_rooms__cleaning_schedule')\n    self.assertIsInstance(house.all_rooms[0]._prefetched_objects_cache['cleaning_schedule'], QuerySet)\n", "    def test_prefetch_related_with_nested_queryset(self):\n        house = House.objects.create(name='Small house', address='45 Side St')\n        room = Room.objects.create(name='Living room', house=house)\n        room.occupants.add(self.person1)\n        person1_house = House.objects.filter(occupants=self.person1)\n        room_with_occupants = Room.objects.prefetch_related(\n            Prefetch('occupants', queryset=Person.objects.filter(name='Joe')),\n            Prefetch('house', queryset=person1_house),\n        )\n        with self.assertNumQueries(3):\n            rooms = list(room_with_occupants)\n            self.assertEqual(len(rooms), 2)\n            self.assertEqual(rooms[0].occupants.count(), 1)\n            self.assertEqual(rooms[1].occupants.count(), 0)\n            self.assertEqual(rooms[0].house, house)\n            self.assertEqual(rooms[1].house, self.house1)\n"], "sample_910": ["def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1')\n    with prefixed_warnings(\"PREFIX:\"):\n        logger.warning('message2')\n        logger.warning('message3')\n\n    assert 'WARNING: message1' in warning.getvalue()\n    assert 'PREFIX: WARNING: message2' in warning.getvalue()\n    assert 'PREFIX: WARNING: message3' in warning.getvalue()\n\n    # nested prefixed_warnings\n    with prefixed_warnings(\"PREFIX1:\"):\n        logger.warning('message4')\n        with prefixed_warnings(\"PREFIX2:\"):\n            logger.warning('message5')\n        logger.warning('message6')\n\n    assert 'PREFIX1: WARNING: message4' in warning.getvalue()\n    assert 'PREFIX1: PREFIX2: WARNING: message5' in warning.getvalue()\n    assert 'PREFIX1: WARNING: message6' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1')\n    with prefixed_warnings('PREFIX: '):\n        logger.warning('message2')\n\n    assert 'WARNING: message1\\nPREFIX: WARNING: message2' in strip_escseq(warning.getvalue())\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1')\n    with prefixed_warnings(\"test:\"):\n        logger.warning('message2')\n        assert 'WARNING: message1\\n' in warning.getvalue()\n        assert 'test: WARNING: message2\\n' in warning.getvalue()\n\n    assert 'WARNING: message1\\n' in warning.getvalue()\n    assert 'test: WARNING: message2\\n' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message')\n    assert 'WARNING: message' in warning.getvalue()\n\n    with prefixed_warnings('prefix: '):\n        warning.truncate(0)\n        warning.seek(0)\n        logger.warning('message')\n    assert 'WARNING: prefix: message' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        assert 'prefix: WARNING: message1' in warning.getvalue()\n\n    warning.truncate(0)\n    warning.seek(0)\n\n    with prefixed_warnings(\"another_prefix:\"):\n        logger.warning('message2')\n        assert 'another_prefix: WARNING: message2' in warning.getvalue()\n\n    warning.truncate(0)\n    warning.seek(0)\n\n    logger.warning('message3')\n    assert 'WARNING: message3' in warning.getvalue()\n"], "sample_720": ["def test_power_transformer_yeo_johnson():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_yeo_johnson():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_yeo_johnson():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_yeo_johnson():\n    X = np.abs(X_2d)\n    pt = PowerTransformer(method='yeo-johnson')\n\n    X_trans_class = pt.fit_transform(X)\n    X_trans_func = power_transform(X, method='yeo-johnson')\n\n    for X_trans in [X_trans_class, X_trans_func]:\n        for j in range(X_trans.shape[1]):\n            X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n            assert_almost_equal(X_trans[:, j], X_expected)\n            assert_almost_equal(lmbda, pt.lambdas_[j])\n\n        # Test inverse transformation\n        X_inv = pt.inverse_transform(X_trans)\n        assert_array_almost_equal(X_inv, X)\n\n    assert len(pt.lambdas_) == X.shape[1]\n    assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_sparse_input():\n    X_sparse = sparse.csr_matrix(np.abs(X_2d))\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n\n        X_trans = pt.fit_transform(X_sparse)\n        X_trans_func = power_transform(X_sparse, standardize=standardize)\n\n        for X_trans in [X_trans, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.boxcox(X_sparse[:, j].toarray().flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j].toarray(), X_expected.reshape(-1, 1))\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv.toarray(), np.abs(X_2d))\n\n        assert len(pt.lambdas_) == X_sparse.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n"], "sample_792": ["def test_gnb_small_alpha():\n    \"\"\"Test if a warning is raised when alpha is too small\"\"\"\n    clf = GaussianNB(var_smoothing=1e-20)\n    assert_warns(UserWarning, clf.fit, X, y)\n", "def test_gnb_sparse_input():\n    # Test whether GaussianNB handles sparse input correctly\n    X_sparse = scipy.sparse.csr_matrix(X)\n    clf = GaussianNB()\n    clf.fit(X_sparse, y)\n    y_pred_sparse = clf.predict(X_sparse)\n    assert_array_equal(y_pred_sparse, y)\n", "def test_complementnb_binary_classification():\n    # Test ComplementNB with a binary classification problem\n    X = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 0], [0, 1, 1], [1, 0, 1]])\n    y = np.array([0, 0, 0, 1, 1, 1])\n    clf = ComplementNB(alpha=1.0)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n", "def test_partial_fit_with_single_class():\n    \"\"\"Test partial fit when there is only a single class in the training set\"\"\"\n    X = np.array([[1, 0], [1, 1]])\n    y = np.array([0, 0])\n    nb = GaussianNB()\n    # Fit once\n    nb.fit(X, y)\n    # Partial fit twice\n    nb.partial_fit(X[:1], y[:1], classes=[0])\n    nb.partial_fit(X[1:], y[1:])\n    # Check that the results are the same\n    assert_array_almost_equal(nb.theta_, nb.theta_)\n    assert_array_almost_equal(nb.sigma_, nb.sigma_)\n    assert_array_almost_equal(nb.class_prior_, nb.class_prior_)\n", "def test_discretenb_predict_proba_partial_fit():\n    # Test discrete NB classes' probability scores with partial_fit\n\n    X_bernoulli = [[1, 100, 0], [0, 1, 0], [0, 100, 1]]\n    X_multinomial = [[0, 1], [1, 3], [4, 0]]\n\n    y = [0, 0, 2]\n    for cls, X in zip([BernoulliNB, MultinomialNB],\n                      [X_bernoulli, X_multinomial]):\n        clf = cls()\n        clf.partial_fit(X[:2], y[:2], classes=[0, 2])\n        clf.partial_fit(X[2:], y[2:])\n        assert_equal(clf.predict(X[-1:]), 2)\n        assert_equal(clf.predict_proba([X[0]]).shape, (1, 2))\n        assert_array_almost_equal(clf.predict_proba(X[:2]).sum(axis=1),\n                                  np.array([1., 1.]), 6)\n"], "sample_955": ["def test_unparse_arguments():\n    source = \"def func(a: int, b: str = 'default', *args, c: float, **kwargs): pass\"\n    expected = \"a: int, b: str = 'default', *args, c: float, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=1, /, c, *, d, e=2, **f): ...\"\n    module = ast.parse(source)\n    func = module.body[0]\n    assert ast.unparse(func.args, source) == \"a, b=1, /, c, *, d, e=2, **f\"\n", "def test_unparse_complex_expression():\n    source = \"1 + 2 * (3 - 4) / 5\"\n    expected = \"1 + 2 * (3 - 4) / 5\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_Subscript_tuple():\n    source = \"a[1, 2]\"\n    expected = \"a[1, 2]\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_Arg():\n    code = \"def f(a: int, b: str = 'default'): pass\"\n    module = ast.parse(code)\n    arg_str = ast.unparse(module.body[0].args.args[0])\n    assert arg_str == \"a: int\"\n"], "sample_869": ["def test_balanced_accuracy_score_binary():\n    y_true = [0, 0, 1, 1]\n    y_pred = [0, 1, 0, 1]\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == 0.5\n", "def test_balanced_accuracy_score_no_labels():\n    # Test balanced accuracy score with no labels in y_true\n    y_true = np.array([0, 0, 0])\n    y_pred = np.array([0, 0, 1])\n    error_message = \"y_true contains no labeled data.\"\n    with pytest.raises(ValueError, match=error_message):\n        balanced_accuracy_score(y_true, y_pred)\n", "def test_balanced_accuracy_score_values(y_true, y_pred, expected):\n    assert balanced_accuracy_score(y_true, y_pred) == pytest.approx(expected)\n", "def test_balanced_accuracy_score_with_params(y_true, y_pred, adjusted, expected):\n    balanced = balanced_accuracy_score(y_true, y_pred, adjusted=adjusted)\n    assert balanced == pytest.approx(expected)\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 1, 2]\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    expected_balanced = (recall_score(y_true, y_pred, average=None, labels=[0, 1, 2]).mean())\n    assert balanced == pytest.approx(expected_balanced)\n"], "sample_1177": ["def test_issue_14216_conjugate():\n    from sympy.functions.elementary.complexes import conjugate\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert conjugate(A[0, 0]) == conjugate(A)[0, 0]\n    assert conjugate(A[0, 0]*A[1, 0]) == conjugate(A[0, 0])*conjugate(A[1, 0])\n", "def test_issue_22191():\n    x = Symbol('x')\n    assert Abs(Abs(x)**2 - x**2) == 0\n", "def test_re_im_properties():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    z = Symbol('z', complex=True, zero=False)\n\n    assert re(x).is_real is True\n    assert re(x).is_imaginary is False\n    assert re(x).is_complex is False\n    assert re(x).is_finite is True\n\n    assert im(x).is_real is True\n    assert im(x).is_imaginary is False\n    assert im(x).is_complex is False\n    assert im(x).is_zero is True\n\n    assert re(y).is_real is True\n    assert re(y).is_imaginary is False\n    assert re(y).is_complex is False\n    assert re(y).is_zero is True\n\n    assert im(y).is_real is True\n    assert im(y).is_imaginary is False\n    assert im(y).is_complex is False\n    assert im(y).is_finite is True\n\n    assert re(z).is_real is None\n    assert re(z).is_imaginary is False\n    assert re(z).is_complex is None\n    assert re(z).is_finite is None\n\n    assert im(z).is_real is None\n    assert im(z).is_imaginary is False\n    assert im(z).is_complex is None\n    assert im(z).is_finite is None\n", "def test_abs_assumptions():\n    nn = Symbol('nonnegative', nonnegative=True)\n    np = Symbol('nonpositive', nonpositive=True)\n    nz = Symbol('nonzero', zero=False)\n    nznn = Symbol('nonzerononnegative', zero=False, nonnegative=True)\n    nznp = Symbol('nonzerononpositive', zero=False, nonpositive=True)\n\n    assert Abs(nn).is_nonnegative is True\n    assert Abs(np).is_nonnegative is True\n    assert Abs(nz).is_nonzero is True\n    assert Abs(nznn).is_nonzero is True\n    assert Abs(nznp).is_nonzero is True\n", "def test_sign_assumptions():\n    pos = Symbol('positive', positive=True)\n    neg = Symbol('negative', negative=True)\n    nneg = Symbol('nonnegative', nonnegative=True)\n    npos = Symbol('nonpositive', nonpositive=True)\n    z = Symbol('zero', zero=True)\n    nz = Symbol('nonzero', nonzero=True)\n\n    assert sign(pos).is_positive is True\n    assert sign(pos).is_nonnegative is True\n    assert sign(pos).is_nonpositive is False\n    assert sign(pos).is_zero is False\n\n    assert sign(neg).is_positive is False\n    assert sign(neg).is_nonnegative is False\n    assert sign(neg).is_nonpositive is True\n    assert sign(neg).is_zero is False\n\n    assert sign(nneg).is_positive is None\n    assert sign(nneg).is_nonnegative is True\n    assert sign(nneg).is_nonpositive is None\n    assert sign(nneg).is_zero is None\n\n    assert sign(npos).is_positive is None\n    assert sign(npos).is_nonnegative is None\n    assert sign(npos).is_nonpositive is True\n    assert sign(npos).is_zero is None\n\n    assert sign(z).is_positive is False\n    assert sign(z).is_nonnegative is True\n    assert sign(z).is_nonpositive is True\n    assert sign(z).is_zero is True\n\n    assert sign(nz).is_positive is None\n    assert sign(nz).is_nonnegative is None\n    assert sign(nz).is_nonpositive is None\n    assert sign(nz).is_zero is False\n"], "sample_965": ["def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_is_builtin_class_method_multiple_inheritance():\n    class Base1:\n            pass\n\n    class Base2:\n            pass\n\n    class MyClass(Base1, Base2):\n            pass\n\n    assert inspect.is_builtin_class_method(MyClass, 'my_method') is False\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.evaluate_signature(inspect.signature(func), globals(), locals())\n    assert sig.parameters['x'].annotation == list\n    assert sig.return_annotation == dict\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    sig = inspect.evaluate_signature(sig, globals())\n    assert sig.parameters['x'].annotation == list\n    assert sig.parameters['y'].annotation == Optional[str]\n    assert sig.return_annotation == tuple\n\n    # Test with local namespace\n            pass\n\n        sig = inspect.signature(func)\n        sig = inspect.evaluate_signature(sig, globals(), locals())\n        assert sig.parameters['x'].annotation == list\n        assert sig.parameters['y'].annotation == Optional[str]\n        assert sig.return_annotation == tuple\n\n    wrapper()\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is False\n"], "sample_775": ["def test_indent_at_name():\n    # Check the indent_at_name param\n    pp = _EstimatorPrettyPrinter(indent_at_name=False)\n    lr = LogisticRegression(C=99)\n    expected = \"\"\"\n    LogisticRegression(C=99, class_weight=None, dual=False,\n                       fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                       max_iter=100, multi_class='warn', n_jobs=None,\n                       penalty='l2', random_state=None, solver='warn',\n                       tol=0.0001, verbose=0, warm_start=False)\"\"\"\n    expected = expected[1:]  # remove first \\n\n    assert pp.pformat(lr) == expected\n", "def test_nested_params():\n    # Render a pipeline with an estimator that has an estimator as a param\n    estimator = SVC()\n    rfe = RFE(estimator=estimator)\n    pipeline = make_pipeline(StandardScaler(), rfe)\n    expected = \"\"\"", "def test_indent_at_name():\n    # Make sure the indent_at_name param is correctly used\n    pp = _EstimatorPrettyPrinter(compact=True, indent=2, indent_at_name=True)\n\n    lr = LogisticRegression(C=99)\n    expected = \"\"\"\n  LogisticRegression(C=99)\"\"\"\n    assert pp.pformat(lr) == expected.rstrip()\n\n    # Check with a repr that doesn't fit on a single line\n    lr = LogisticRegression(C=99, class_weight=.4, fit_intercept=False,\n                            tol=1234, verbose=True)\n    expected = \"\"\"\n  LogisticRegression(C=99,\n                     class_weight=0.4,\n                     fit_intercept=False,\n                     tol=1234,\n                     verbose=True)\"\"\"\n    assert pp.pformat(lr) == expected.rstrip()\n\n    # Check with a repr that fits on a single line\n    lr = LogisticRegression(C=99, class_weight=.4)\n    expected = \"\"\"LogisticRegression(C=99, class_weight=0.4)\"\"\"\n    assert pp.pformat(lr) == expected\n\n    # Check with indent_at_name=False\n    pp = _EstimatorPrettyPrinter(compact=True, indent=2, indent_at_name=False)\n\n    lr = LogisticRegression(C=99)\n    expected = \"\"\"\n  LogisticRegression(C=99, class_weight=None, dual=False,\n                     fit_intercept=True, intercept_scaling=1,\n                     l1_ratio=None, max_iter=100, multi_class='warn',\n                     n_jobs=None, penalty='l2', random_state=None,\n                     solver='warn', tol=0.0001, verbose=0,\n                     warm_start=False)\"\"\"\n    assert pp.pformat(lr) == expected.", "def test_indent_at_name():\n    # Test that the indent_at_name parameter is correctly used\n    pp = _EstimatorPrettyPrinter(compact=True, indent=2, indent_at_name=True)\n    lr = LogisticRegression(C=99, class_weight=.4, fit_intercept=False,\n                            tol=1234, verbose=True)\n    expected = \"\"\"", "def test_indent_at_name():\n    # Test the indent_at_name parameter\n    pp = _EstimatorPrettyPrinter(compact=False, indent=4, indent_at_name=True)\n    lr = LogisticRegression(C=99, class_weight=.4, fit_intercept=False, tol=1234, verbose=True)\n    expected = \"\"\""], "sample_325": ["def test_field_deep_copy_widget(self):\n    class CustomTextInput(TextInput):\n            kwargs['attrs'] = {'class': 'custom-text-input'}\n            super().__init__(**kwargs)\n\n    widget = CustomTextInput()\n    widget_copy = copy.deepcopy(widget)\n    self.assertIsInstance(widget_copy, CustomTextInput)\n    self.assertIsNot(widget_copy.attrs, widget.attrs)\n", "def test_field_deep_copy_widget_attrs(self):\n    class CustomTextInput(TextInput):\n            default_attrs = {'class': 'custom-class'}\n            if attrs:\n                default_attrs.update(attrs)\n            super().__init__(attrs=default_attrs, **kwargs)\n\n    field = CharField(widget=CustomTextInput)\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsNot(field_copy.widget, field.widget)\n    self.assertIsInstance(field_copy.widget, CustomTextInput)\n    self.assertIsNot(field_copy.widget.attrs, field.widget.attrs)\n    self.assertEqual(field_copy.widget.attrs, {'class': 'custom-class'})\n", "    def test_kwarg_takes_precedence(self):\n        custom1 = CustomRenderer()\n        custom2 = CustomRenderer()\n        form = Form(renderer=custom1, default_renderer=custom2)\n        self.assertEqual(form.renderer, custom1)\n", "def test_required_css_class_multi_value_field(self):\n    class NameField(MultiValueField):\n            fields = (CharField(label='First name', max_length=10),\n                      CharField(label='Last name', max_length=10))\n            super().__init__(fields=fields, *args, **kwargs)\n\n            return ' '.join(data_list)\n\n    class NameForm(Form):\n        name = NameField()\n        required_css_class = 'required'\n\n    form = NameForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p class=\"required\"><label class=\"required\" for=\"id_name_0\">First name:</label> '\n        '<input id=\"id_name_0\" name=\"name_0\" type=\"text\" maxlength=\"10\" required> '\n        '<label class=\"required\" for=\"id_name_1\">Last name:</label> '\n        '<input id=\"id_name_1\" name=\"name_1\" type=\"text\" maxlength=\"10\" required></p>'\n    )\n", "def test_renderer_attribute_subclass(self):\n    class SubCustomRenderer(CustomRenderer):\n        pass\n\n    class CustomForm(Form):\n        default_renderer = SubCustomRenderer\n\n    form = CustomForm()\n    self.assertIsInstance(form.renderer, CustomForm.default_renderer)\n"], "sample_205": ["    def test_message_dict_property(self):\n        message_dict = {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']}\n        exception = ValidationError(message_dict)\n        self.assertEqual(exception.message_dict, message_dict)\n\n        with self.assertRaises(AttributeError):\n            ValidationError('message').message_dict\n", "    def test_update_error_dict(self):\n        error1 = ValidationError({'field1': 'message', 'field2': 'other'})\n        error2 = ValidationError({'field2': 'another', 'field3': 'error'})\n        error_dict = error1.update_error_dict({})\n        self.assertEqual(error_dict, {'field1': ['message'], 'field2': ['other']})\n        error_dict = error2.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message'], 'field2': ['other', 'another'], 'field3': ['error']})\n\n        error3 = ValidationError('non_field_error')\n        error_dict = error3.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message'], 'field2': ['other', 'another'], 'field3': ['error'], '__all__': ['non_field_error']})\n\n        error4 = ValidationError([ValidationError('error1'), 'error2'])\n        error_dict = error4.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message'], 'field2': ['other', 'another'], 'field3': ['error'], '__all__': ['non_field_error', 'error1', 'error2']})\n", "def test_message_dict_returns_error_dict(self):\n    error_dict = {\n        'field1': ['E1', 'E2'],\n        'field2': ['E3', 'E4']\n    }\n    exception = ValidationError(error_dict)\n    self.assertEqual(exception.message_dict, error_dict)\n", "    def test_message_dict_property(self):\n        error_dict = {\n            'field1': ['error1'],\n            'field2': ['error2', 'error3'],\n        }\n        exception = ValidationError(error_dict)\n        self.assertEqual(exception.message_dict, error_dict)\n        with self.assertRaises(AttributeError):\n            ValidationError('message').message_dict\n", "    def test_update_error_dict(self):\n        error1 = ValidationError({'field1': ['message1'], 'field2': ['message2']})\n        error2 = ValidationError({'field2': ['message3'], 'field3': ['message4']})\n        error_dict = {}\n        error1.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1'], 'field2': ['message2']})\n        error2.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1'], 'field2': ['message2', 'message3'], 'field3': ['message4']})\n        error3 = ValidationError('non_field_error')\n        error3.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1'], 'field2': ['message2', 'message3'], 'field3': ['message4'], '__all__': ['non_field_error']})\n"], "sample_85": ["def test_fast_delete_parent_link(self):\n    p = Parent.objects.create()\n    c = Child.objects.create(parent_ptr=p)\n    # 1 for fast delete of child, 1 for delete of parent\n    self.assertNumQueries(2, c.delete)\n    self.assertFalse(Child.objects.exists())\n    self.assertFalse(Parent.objects.exists())\n", "def test_fast_delete_m2m_through(self):\n    m = M.objects.create()\n    r = R.objects.create()\n    MR.objects.create(m=m, r=r)\n    # 1 to delete r, 1 to fast-delete m's mr_set\n    self.assertNumQueries(2, r.delete)\n    self.assertFalse(MR.objects.exists())\n\n    r = R.objects.create()\n    MR.objects.create(m=m, r=r)\n    # 1 to delete m, 1 to fast-delete m's mr_set\n    self.assertNumQueries(2, m.delete)\n    self.assertFalse(MR.objects.exists())\n", "def test_fast_delete_signal(self):\n    \"\"\"\n    Fast deletion works if deletion signals are connected.\n    \"\"\"\n        pass\n\n    u = User.objects.create(avatar=Avatar.objects.create())\n    models.signals.pre_delete.connect(receiver, sender=User)\n    a = Avatar.objects.get(pk=u.avatar_id)\n    # 1 query to fast-delete the user\n    # 1 query to delete the avatar\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    models.signals.pre_delete.disconnect(receiver, sender=User)\n", "def test_fast_delete_bulk_create(self):\n    # Test fast delete with bulk_create\n    objs = [Avatar() for _ in range(2000)]\n    Avatar.objects.bulk_create(objs)\n    # Calculate the number of queries needed\n    batch_size = connection.ops.bulk_batch_size(['pk'], objs)\n    # The Avatar objects are going to be deleted in batches of GET_ITERATOR_CHUNK_SIZE\n    queries = ceil(len(objs) / GET_ITERATOR_CHUNK_SIZE)\n    self.assertNumQueries(queries, Avatar.objects.all().delete)\n    self.assertFalse(Avatar.objects.exists())\n", "    def test_fast_delete_with_signals(self):\n        \"\"\"\n        Test that fast_delete() is not used when pre_delete or post_delete signals are connected.\n        \"\"\"\n            pass\n\n        signals.pre_delete.connect(receiver, sender=User)\n        signals.post_delete.connect(receiver, sender=User)\n\n        u = User.objects.create(\n            avatar=Avatar.objects.create()\n        )\n        a = Avatar.objects.get(pk=u.avatar_id)\n\n        # If signals are connected, we should not fast-delete the user, so we expect 3 queries:\n        # 1 to delete the user, 1 to null out the avatar foreign key, and 1 to delete the avatar.\n        self.assertNumQueries(3, a.delete)\n\n        signals.pre_delete.disconnect(receiver, sender=User)\n        signals.post_delete.disconnect(receiver, sender=User)\n"], "sample_634": ["def test_expand_modules_with_ignore_paths(self, files_or_modules, expected):\n    \"\"\"Test expand_modules with a custom value of ignore-paths\"\"\"\n    ignore_list, ignore_list_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_non_existent_module(self, files_or_modules, expected):\n    \"\"\"Test expand_modules with a non-existent module\"\"\"\n    ignore_list, ignore_list_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    assert modules == expected\n    assert errors[0][\"key\"] == \"fatal\"\n    assert errors[0][\"mod\"] == \"non_existent_module\"\n", "def test_expand_modules_with_ignore_paths() -> None:\n    files_or_modules = [__file__, str(Path(__file__).parent)]\n    expected = [\n        init_of_package,\n        test_pylinter,\n        this_file_from_init,\n        unittest_lint,\n    ]\n    ignore_list, ignore_list_re = [], []\n    ignore_paths_re = [re.compile(r\".*test_utils\\.py\")]\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        ignore_paths_re,\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_ignore_paths(self, files_or_modules, expected):\n    \"\"\"Test expand_modules with a specific value of ignore-paths\"\"\"\n    ignore_list, ignore_list_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_ignore_list_re() -> None:\n    files_or_modules = [__file__]\n    ignore_list = []\n    ignore_list_re = [re.compile(\"unittest_.*\")]\n    ignore_list_paths_re = []\n    expected = []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        ignore_list_paths_re,\n    )\n    assert modules == expected\n    assert not errors\n"], "sample_909": ["def test_custom_generic_sections_in_numpy_docstring(self):\n\n    docstrings = ((\"\"\"\\", "def test_code_block_in_parameters_section(self):\n    docstring = \"\"\"", "def test_code_block_in_parameters_section(self):\n    docstring = \"\"\"", "    def test_custom_generic_sections_numpydoc(self):\n        docstrings = ((\"\"\"\\", "def test_list_in_parameter_description_with_code_block(self):\n    docstring = \"\"\"One line summary.\n"], "sample_612": ["def test_ds_resample_apply_func_kwargs(self):\n        return arg1.mean(\"time\") + arg2 + arg3\n\n    times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n    ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n    expected = xr.Dataset({\"foo\": (\"time\", [4.0, 4.0, 4.0]), \"time\": times})\n    actual = ds.resample(time=\"D\").map(func, arg2=2.0, arg3=1.0)\n    assert_identical(expected, actual)\n", "def test_resample_tolerance(self):\n    # Test tolerance keyword for upsample methods bfill, pad, nearest\n    times = pd.date_range(\"2000-01-01\", freq=\"1D\", periods=2)\n    times_upsampled = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=5)\n    ds = Dataset({\"foo\": (\"time\", np.arange(2)), \"time\": times})\n\n    # Forward fill\n    actual = ds.resample(time=\"6H\").ffill(tolerance=\"12H\")\n    expected = Dataset({\"foo\": (\"time\", [0.0, 0.0, 0.0, np.nan, 1.0]), \"time\": times_upsampled})\n    assert_identical(expected, actual)\n\n    # Backward fill\n    actual = ds.resample(time=\"6H\").bfill(tolerance=\"12H\")\n    expected = Dataset({\"foo\": (\"time\", [0.0, np.nan, 1.0, 1.0, 1.0]), \"time\": times_upsampled})\n    assert_identical(expected, actual)\n\n    # Nearest\n    actual = ds.resample(time=\"6H\").nearest(tolerance=\"6H\")\n    expected = Dataset({\"foo\": (\"time\", [0, 0, np.nan, 1, 1]), \"time\": times_upsampled})\n    assert_identical(expected, actual)\n", "def test_groupby_bins_errors():\n    array = DataArray(np.arange(4), dims=\"dim_0\")\n    bins = [0, 1.5, 5]\n\n    with pytest.raises(ValueError, match=r\"All bin edges are NaN.\"):\n        array.groupby_bins(\"dim_0\", bins=[np.nan, np.nan, np.nan])\n\n    with pytest.raises(ValueError, match=r\"None of the data falls within bins with edges\"):\n        array = array * 0  # make all data zero\n        array.groupby_bins(\"dim_0\", bins)\n", "def test_groupby_bins_multidim_non_monotonic():\n    array = DataArray(\n        [[[0, 1], [2, 3]], [[5, 10], [15, 20]]],\n        coords={\n            \"lon\": ([\"ny\", \"nx\"], [[30, 40], [40, 50]]),\n            \"lat\": ([\"ny\", \"nx\"], [[10, 20], [20, 10]]),\n        },\n        dims=[\"time\", \"ny\", \"nx\"],\n    )\n    bins = [0, 15, 20]\n    bin_coords = pd.cut(array[\"lat\"].values.flat, bins).categories\n    expected = DataArray([16, 28], dims=\"lat_bins\", coords={\"lat_bins\": bin_coords})\n    actual = array.groupby_bins(\"lat\", bins).map(lambda x: x.sum())\n    assert_identical(expected, actual)\n", "def test_ds_resample_apply_func_kwargs(self):\n        return arg1.mean(\"time\") + arg2 + arg3\n\n    times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n    ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n    expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n    actual = ds.resample(time=\"D\").map(func, kwargs={\"arg2\": 1.0, \"arg3\": 1.0})\n    assert_identical(expected, actual)\n"], "sample_399": ["def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum(\"price\", default=0.0),\n    )\n    self.assertEqual(result[\"value\"], 0.0)\n", "def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum(\"price\", default=0.0),\n    )\n    self.assertEqual(result[\"value\"], 0.0)\n", "def test_aggregation_filter_with_join(self):\n    publishers_having_more_than_one_book_qs = (\n        Book.objects.values(\"publisher\")\n        .annotate(cnt=Count(\"isbn\"))\n        .filter(cnt__gt=1, publisher__num_awards__gt=3)\n    )\n    query = publishers_having_more_than_one_book_qs.query.exists(\n        using=connection.alias\n    )\n    _, _, group_by = query.get_compiler(connection=connection).pre_sql_setup()\n    self.assertEqual(len(group_by), 1)\n", "def test_aggregation_default_with_filter_none_values(self):\n    result = Book.objects.aggregate(\n        value=Sum(\"price\", filter=Q(rating__lt=3.0), default=None),\n    )\n    self.assertIsNone(result[\"value\"])\n", "def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum(\"price\", default=0.0),\n    )\n    self.assertEqual(result[\"value\"], 0.0)\n"], "sample_91": ["def test_permission_denied(self):\n    \"The permission_denied view raises a 403 status\"\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "def test_permission_denied(self):\n    \"\"\"\n    403.html template is picked by the permission_denied handler.\n    \"\"\"\n    request = self.request_factory.get('/protected_url/')\n    response = permission_denied(request, Exception('Permission Denied'))\n    self.assertContains(response, \"exception: Permission Denied\", status_code=403)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception(\"Permission denied\"))\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n    self.assertContains(response, b'Permission denied')\n", "def test_permission_denied(self):\n    \"\"\"\n    The permission_denied view returns a 403 status with the correct template.\n    \"\"\"\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n    self.assertContains(response, str(Exception()))\n", "def test_permission_denied(self):\n    \"\"\"\n    403.html template is picked by the permission_denied handler.\n    \"\"\"\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception('Permission denied'))\n    self.assertContains(response, \"exception: Permission denied\", status_code=403)\n"], "sample_1104": ["def test_TrigonometricFunctions():\n    assert str(sin(x)) == \"sin(x)\"\n    assert str(cos(x)) == \"cos(x)\"\n    assert str(tan(x)) == \"sin(x)/cos(x)\"\n    assert str(asin(x)) == \"asin(x)\"\n    assert str(acos(x)) == \"acos(x)\"\n    assert str(atan(x)) == \"atan(x)\"\n    assert str(sinh(x)) == \"sinh(x)\"\n    assert str(cosh(x)) == \"cosh(x)\"\n    assert str(tanh(x)) == \"sinh(x)/cosh(x)\"\n    assert str(asinh(x)) == \"asinh(x)\"\n    assert str(acosh(x)) == \"acosh(x)\"\n    assert str(atanh(x)) == \"atanh(x)\"\n", "def test_MatPow_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n\n    assert str(A ** 2) == \"A**2\"\n    assert str(A ** -1) == \"A**(-1)\"\n    assert str(A ** B) == \"A**B\"\n", "def test_MatPow():\n    A = MatrixSymbol('A', 2, 2)\n    assert str(A**-1) == \"A**(-1)\"\n    assert str(A**2) == \"A**2\"\n    assert str((A + A)**2) == \"(A + A)**2\"\n    assert str((A + A)**-1) == \"(A + A)**(-1)\"\n", "def test_issue_18143():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    e = Tr(A*B)\n    assert str(e) == 'Tr(A*B)'\n", "def test_Ordinal():\n    from sympy.functions.combinatorial.numbers import ordinal\n    assert str(ordinal(1)) == \"1st\"\n    assert str(ordinal(2)) == \"2nd\"\n    assert str(ordinal(3)) == \"3rd\"\n    assert str(ordinal(4)) == \"4th\"\n    assert str(ordinal(10)) == \"10th\"\n    assert str(ordinal(11)) == \"11th\"\n    assert str(ordinal(12)) == \"12th\"\n    assert str(ordinal(13)) == \"13th\"\n"], "sample_293": ["    def test_include_with_prefix(self):\n        included_url_patterns = [\n            path('inner/', views.empty_view, name='included-view'),\n        ]\n        url_patterns = [\n            path('prefix/', include(included_url_patterns)),\n        ]\n        resolver = URLResolver(RegexPattern(r'^/'), url_patterns)\n        self.assertEqual(resolver.reverse('included-view'), '/prefix/inner/')\n", "    def test_view_name(self):\n        tests = [\n            ('normal-view', 'normal-view'),\n            ('view-class', 'view-class'),\n            ('included_namespace_urls:inc-normal-view', 'included_namespace_urls:inc-normal-view'),\n            ('included_namespace_urls:inc-view-class', 'included_namespace_urls:inc-view-class'),\n            ('test-ns1:urlobject-view', 'test-ns1:urlobject-view'),\n            ('included_namespace_urls:test-ns3:urlobject-view', 'included_namespace_urls:test-ns3:urlobject-view'),\n        ]\n        for url_name, expected_view_name in tests:\n            with self.subTest(url_name=url_name):\n                match = resolve(reverse(url_name))\n                self.assertEqual(match.view_name, expected_view_name)\n", "    def test_include_nested(self):\n        url_patterns = [\n            path('nested/', include(self.url_patterns), name='nested'),\n        ]\n        included = include(url_patterns)\n        self.assertEqual(\n            included[0][0].resolve('nested/inner/').kwargs,\n            {'arg1': '42', 'arg2': '37'}\n        )\n", "def test_repr(self):\n    self.assertEqual(\n        repr(resolve('/no_kwargs/42/37/')),\n        \"ResolverMatch(func=urlpatterns_reverse.views.empty_view, \"\n        \"args=('42', '37'), kwargs={}, url_name='no-kwargs', app_names=[], \"\n        \"namespaces=[], route='^no_kwargs/([0-9]+)/([0-9]+)/$')\",\n    )\n", "    def test_reverse_with_args(self):\n        test_urls = [\n            ('lookahead-positive', ['a-city'], '/lookahead+/a-city/'),\n            ('lookahead-negative', ['a-city'], '/lookahead-/a-city/'),\n            ('lookbehind-positive', ['a-city'], '/lookbehind+/a-city/'),\n            ('lookbehind-negative', ['a-city'], '/lookbehind-/a-city/'),\n        ]\n        for name, args, expected in test_urls:\n            with self.subTest(name=name, args=args):\n                self.assertEqual(reverse(name, args=args), expected)\n"], "sample_56": ["def test_check_autocomplete_fields_invalid_type(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = 'title'\n\n    self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'autocomplete_fields' must be a list or tuple.\",\n            obj=SongAdmin,\n            id='admin.E036',\n        )\n    ])\n", "def test_save_as_boolean_check(self):\n    class SongAdmin(admin.ModelAdmin):\n        save_as = 'test'\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'save_as' must be a boolean.\",\n            obj=SongAdmin,\n            id='admin.E101',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_fields_and_fieldsets(self):\n    \"\"\"\n    The fields and fieldsets checks are mutually exclusive.\n    \"\"\"\n    class FieldsAndFieldsetsAdmin(admin.ModelAdmin):\n        fields = ['title']\n        fieldsets = [\n            (None, {\n                \"fields\": [\"title\", \"original_release\"],\n            }),\n        ]\n\n    errors = FieldsAndFieldsetsAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"Both 'fieldsets' and 'fields' are specified.\",\n            obj=FieldsAndFieldsetsAdmin,\n            id='admin.E005',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_list_display_calls_get_list_display(self):\n    \"\"\"\n    The list_display_links checks are skipped when the ModelAdmin.get_list_display() method\n    is overridden.\n    \"\"\"\n    class OverriddenGetListDisplayAdmin(admin.ModelAdmin):\n            return ['title', 'original_release']\n        list_display_links = ['title']\n\n    errors = OverriddenGetListDisplayAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_autocomplete_fields(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = ('album',)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_260": ["def test_create_model_alter_unique_together(self):\n    \"\"\"\n    AlterUniqueTogether should optimize into CreateModel with options.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [\n                (\"a\", models.IntegerField()),\n                (\"b\", models.IntegerField()),\n            ]),\n            migrations.AlterUniqueTogether(\"Foo\", [[\"a\", \"b\"]]),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [\n                (\"a\", models.IntegerField()),\n                (\"b\", models.IntegerField()),\n            ], options={'unique_together': {('a', 'b')}}),\n        ],\n    )\n", "def test_create_model_with_default_manager(self):\n    \"\"\"\n    CreateModel with a custom default manager should preserve it.\n    \"\"\"\n    managers = [('custom_manager', models.Manager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                'MyModel',\n                fields=[],\n                managers=managers,\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                'MyModel',\n                fields=[],\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_create_model_add_index(self):\n    \"\"\"\n    AddIndex should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"], name=\"idx_foo_name\")),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [\n                (\"name\", models.CharField(max_length=255)),\n            ], options={\"indexes\": [models.Index(fields=[\"name\"], name=\"idx_foo_name\")]}),\n        ],\n    )\n", "def test_create_model_alter_managers(self):\n    \"\"\"\n    AlterModelManagers should optimize into CreateModel.\n    \"\"\"\n    managers = [('custom_manager', EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n            ),\n            migrations.AlterModelManagers(\"Foo\", managers),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_create_model_through_m2m_field(self):\n    \"\"\"\n    AddField with through m2m should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel('Employee', [('name', models.CharField(max_length=255))]),\n            migrations.CreateModel('Employer', [('name', models.CharField(max_length=255))]),\n            migrations.CreateModel('Employment', [\n                ('employee', models.ForeignKey('migrations.Employee', models.CASCADE)),\n                ('employer', models.ForeignKey('migrations.Employer', models.CASCADE)),\n            ]),\n            migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n            migrations.AddField('Foo', 'employers', models.ManyToManyField(\n                'migrations.Employer', through='migrations.Employment',\n            )),\n        ],\n        [\n            migrations.CreateModel('Employee', [('name', models.CharField(max_length=255))]),\n            migrations.CreateModel('Employer', [('name', models.CharField(max_length=255))]),\n            migrations.CreateModel('Employment', [\n                ('employee', models.ForeignKey('migrations.Employee', models.CASCADE)),\n                ('employer', models.ForeignKey('migrations.Employer', models.CASCADE)),\n            ]),\n            migrations.CreateModel('Foo', [\n                ('name', models.CharField(max_length=255)),\n                ('employers', models.ManyToManyField(\n                    'migrations.Employer', through='migrations.Employment',\n                )),\n            ]),\n        ],\n    )\n"], "sample_889": ["def test_calibration_with_non_sample_aligned_fit_param_in_cv(data):\n    \"\"\"Check that CalibratedClassifierCV does not enforce sample alignment\n    for fit parameters in cross-validation.\"\"\"\n\n    class TestClassifier(LogisticRegression):\n            assert fit_param is not None\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    CalibratedClassifierCV(estimator=TestClassifier()).fit(\n        *data, fit_param=[np.ones(len(data[1]) // 2), np.ones(len(data[1]) // 2)]\n    )\n", "def test_calibration_with_sample_weight_base_estimator_calibrator(data):\n    \"\"\"Check that sample_weight is passed to the calibrator.\"\"\"\n    X, y = data\n    sample_weight = np.ones_like(y)\n\n    clf = CheckingClassifier(expected_sample_weight=True)\n    calibrated_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n\n    calibrated_clf.fit(X, y, sample_weight=sample_weight)\n", "def test_calibrated_classifier_cv_sample_weights_vs_fit_params(method, ensemble):\n    \"\"\"Check that passing a sample_weight is equivalent to passing a\n    fit_param with the same values.\"\"\"\n    X, y = load_iris(return_X_y=True)\n    # Scale the data to avoid any convergence issue\n    X = StandardScaler().fit_transform(X)\n    # Only use 2 classes\n    X, y = X[:100], y[:100]\n    sample_weight = np.ones_like(y) * 2\n\n    estimator = LogisticRegression()\n    calibrated_clf_with_weights = CalibratedClassifierCV(\n        estimator,\n        method=method,\n        ensemble=ensemble,\n        cv=2,\n    )\n    calibrated_clf_with_fit_params = clone(calibrated_clf_with_weights)\n\n    calibrated_clf_with_weights.fit(X, y, sample_weight=sample_weight)\n    calibrated_clf_with_fit_params.fit(X, y, sample_weight=np.full_like(y, 1), sample_weight_fit_param=sample_weight)\n\n    # Check that the underlying fitted estimators have the same coefficients\n    for est_with_weights, est_with_fit_params in zip(\n        calibrated_clf_with_weights.calibrated_classifiers_,\n        calibrated_clf_with_fit_params.calibrated_classifiers_,\n    ):\n        assert_allclose(\n            est_with_weights.estimator.coef_,\n            est_with_fit_params.estimator.coef_,\n        )\n\n    # Check that the predictions are the same\n    y_pred_with_weights = calibrated_clf_with_weights.predict_proba(X)\n   ", "def test_calibration_with_non_sample_aligned_fit_param_validation(data):\n    \"\"\"Check that CalibratedClassifierCV validates fit parameters that are\n    not aligned with the sample size.\"\"\"\n\n    class TestClassifier(LogisticRegression):\n            assert fit_param is not None\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    calibrated_classifier = CalibratedClassifierCV(estimator=TestClassifier())\n    with pytest.raises(ValueError, match=\"Parameter fit_param is not aligned\"):\n        calibrated_classifier.fit(*data, fit_param=np.ones(len(data[1]) + 1))\n", "def test_calibration_with_different_classes_order(data):\n    \"\"\"Check that CalibratedClassifierCV handles correctly the case where\n    the classes are not in the same order in training and prediction.\"\"\"\n    X, y = data\n    # Shuffle the classes\n    y_shuffled = np.random.permutation(y)\n    calibrated_clf = CalibratedClassifierCV(cv=2)\n    calibrated_clf.fit(X, y_shuffled)\n\n    # Check that the predicted classes are the same as the original ones\n    assert np.array_equal(calibrated_clf.predict(X), y)\n"], "sample_1175": ["def test_issue_18273():\n    x = Symbol('x')\n    assert upretty(ConditionSet(x, Eq(-x + exp(x), 0), S.Complexes)) == \\\n    '\u23a7  \u2502         \u239b      x    \u239e\u23ab\\n'\\\n    '\u23a8x \u2502 x \u220a \u2102 \u2227 \u239d-x + \u212f  = 0\u23a0\u23ac\\n'\\\n    '\u23a9  \u2502                      \u23ad'\n", "def test_Str_printing():\n    # test cases for Str printing\n    s = Str('hello')\n    assert pretty(s) == \"'hello'\"\n    assert upretty(s) == \"'hello'\"\n\n    s = Str('world')\n    assert pretty(s) == \"'world'\"\n    assert upretty(s) == \"'world'\"\n\n    s = Str('hello world')\n    assert pretty(s) == \"'hello world'\"\n    assert upretty(s) == \"'hello world'\"\n", "def test_issue_18301():\n    assert pretty(Integral(x**2, (x, -oo, oo))) == \\\n    '  oo    \\n'\\\n    ' __     \\n'\\\n    ' \\\\ `    \\n'\\\n    '  )  2   \\n'\\\n    ' /_, x   \\n'\\\n    'x = -oo  '\n\n    assert upretty(Integral(x**2, (x, -oo, oo))) == \\\n    ' \u221e     \\n'\\\n    ' ___   \\n'\\\n    ' \u2572     \\n'\\\n    '  \u2572    \\n'\\\n    '  \u2571  2 \\n'\\\n    ' \u2571   x \\n'\\\n    ' \u203e\u203e\u203e   \\n'\\\n    'x = -\u221e '\n", "def test_issue_18363():\n    expr = DirichletGroup(2).character(1)\n    assert pretty(expr) == 'chi_{2}(1, .)'\n    assert upretty(expr) == '\u03c7\u2082(1, .)'\n", "def test_issue_18465():\n    assert upretty(IndexedBase('\u03b4')[1]) == '\u03b4\u2081'\n    assert upretty(IndexedBase('\u03b4')[1, 2, 3]) == '\u03b4\u2081\u2082\u2083'\n    assert upretty(IndexedBase('\u03b4')[1][2][3]) == '\u03b4\u2081\u2082\u2083'\n    assert upretty(IndexedBase('\u03b4')['a', 'b', 'c']) == '\u03b4abc'\n    assert upretty(IndexedBase('\u03b4')['a']['b']['c']) == '\u03b4abc'\n    assert upretty(IndexedBase('\u03b4')['a', 1, 'c']) == '\u03b4a\u2081c'\n    assert upretty(IndexedBase('\u03b4')['a', 1, 'c', 2]) == '\u03b4a\u2081c\u2082'\n    assert upretty(IndexedBase('\u03b4')[1, 'b', 'c']) == '\u03b4\u2081bc'\n"], "sample_389": ["def test_script_prefix_with_absolute_paths(self):\n    # Ensure absolute paths are not affected by SCRIPT_NAME.\n    absolute_paths = (\"/path/\", \"http://myhost.com/path/\", \"http://myhost/path/\", \"https://myhost/path/\")\n    for path in absolute_paths:\n        new_settings = {\"MEDIA_URL\": path, \"STATIC_URL\": path}\n        for script_name in [\"/somesubpath\", \"/somesubpath/\", \"/\", \"\", None]:\n            with self.subTest(script_name=script_name, **new_settings):\n                try:\n                    self.set_script_name(script_name)\n                    self.assertEqual(settings.MEDIA_URL, path)\n                    self.assertEqual(settings.STATIC_URL, path)\n                finally:\n                    clear_script_prefix()\n", "    def set_script_name(self, val):\n        clear_script_prefix()\n        if val is not None:\n            set_script_prefix(val)\n", "def test_media_url_with_script_name(self):\n    # Add SCRIPT_NAME prefix to MEDIA_URL when it's a relative path.\n    tests = (\"/path/\",)\n    for path in tests:\n        new_settings = {\"MEDIA_URL\": path}\n        for script_name in [\"/somesubpath\", \"/somesubpath/\", \"/\", \"\", None]:\n            with self.subTest(script_name=script_name, **new_settings):\n                try:\n                    self.set_script_name(script_name)\n                    if script_name:\n                        self.assertEqual(settings.MEDIA_URL, script_name + path)\n                    else:\n                        self.assertEqual(settings.MEDIA_URL, path)\n                finally:\n                    clear_script_prefix()\n", "    def test_prefixed(self):\n        # Add SCRIPT_NAME prefix to relative paths.\n        for setting in ('MEDIA_URL', 'STATIC_URL'):\n            new_settings = {setting: 'path/'}\n            for script_name in ['/somesubpath', '/somesubpath/', '/', '', None]:\n                expected_path = (script_name or '') + 'path/'\n                with self.settings(**new_settings):\n                    try:\n                        self.set_script_name(script_name)\n                        self.assertEqual(getattr(settings, setting), expected_path)\n                    finally:\n                        clear_script_prefix()\n", "    def test_get_full_path_with_scheme_and_host(self):\n        request = HttpRequest()\n        request.path = \"/path/\"\n        request.META = {\"QUERY_STRING\": \"key=value\"}\n        request.get_host = lambda: \"example.com\"\n        request._get_scheme = lambda: \"https\"\n        expected_url = \"https://example.com/path/?key=value\"\n        self.assertEqual(request.get_full_path(), expected_url)\n"], "sample_336": ["def test_include_pattern(self):\n    url_patterns = [\n        path('included/', include('urlpatterns_reverse.included_urls')),\n    ]\n    resolver = URLResolver(RegexPattern(r'^/'), (url_patterns, 'test'), 'test')\n    resolver._populate()\n    self.assertEqual(resolve('/included/outer-no-kwargs/42/inner-no-kwargs/1/').kwargs, {'outer': '42', 'inner': '1'})\n", "    def test_namespace_included_namespace(self):\n        \"\"\"Namespaces can be included from other namespaces.\"\"\"\n        test_urls = [\n            ('inc-ns1:test-ns2:inner-nothing', [], {}, '/ns-included1/test2/inner/'),\n            ('inc-ns1:test-ns2:inner-nothing', [37, 42], {}, '/ns-included1/test2/inner/37/42/'),\n            ('inc-ns1:test-ns2:inner-nothing', [], {'outer': 42}, '/ns-included1/42/inner/'),\n            ('inc-ns1:test-ns2:inner-nothing', [42], {}, '/ns-included1/42/inner/'),\n            ('inc-ns1:test-ns2:inner-nothing', [], {'arg1': 42, 'arg2': 37, 'outer': 42}, '/ns-included1/42/inner/42/37/'),\n            ('inc-ns1:test-ns2:inner-nothing', [42, 37, 4], {}, '/ns-included1/42/inner/37/4/'),\n        ]\n        for name, args, kwargs, expected in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs):\n                self.assertEqual(reverse(name, args=args, kwargs=kwargs), expected)\n", "    def test_pattern_startswith_slash(self):\n        test_urls = [\n            ('^pattern/', True, 1),\n            ('pattern/', False, 1),\n            ('pattern/$', False, 1),\n            ('^/pattern/', True, 1),\n            ('/pattern/', False, 1),\n            ('/pattern/$', False, 1),\n            ('pattern/', True, 0),\n        ]\n        with override_settings(APPEND_SLASH=True):\n            for pattern, expected, num_warnings in test_urls:\n                with self.subTest(pattern=pattern):\n                    regex_pattern = RegexPattern(pattern)\n                    warnings = regex_pattern._check_pattern_startswith_slash()\n                    self.assertEqual(len(warnings), num_warnings)\n                    if num_warnings:\n                        self.assertEqual(warnings[0].id, 'urls.W002')\n", "    def test_default_kwargs(self):\n        url_patterns = [\n            path('inner/<int:arg1>/', views.empty_view, name='urlobject-view', kwargs={'arg2': 37}),\n        ]\n        resolver = URLResolver(RegexPattern(r'^$'), url_patterns)\n        self.assertEqual(resolver.reverse('urlobject-view', arg1=42), '/inner/42/')\n        self.assertEqual(resolver.reverse('urlobject-view', arg1=42, arg2=43), '/inner/42/')\n", "def test_include_pattern(self):\n    included_url_patterns = [\n        path('included/', include('urlpatterns_reverse.included_urls')),\n    ]\n    urlconf_name = 'urlpatterns_reverse.included_urls'\n    resolver = URLResolver(RegexPattern(r'^/'), included_url_patterns)\n    included_resolver = URLResolver(RegexPattern(r'^included/'), urlconf_name)\n    self.assertEqual(resolver.resolve('/included/inner-nothing/42/'), included_resolver.resolve('/inner-nothing/42/'))\n"], "sample_276": ["def test_simplify_regex(self):\n    # Test with a simple regex pattern\n    self.assertEqual(simplify_regex(\"^test/$\"), \"/test/\")\n\n    # Test with a named group\n    self.assertEqual(simplify_regex(\"^(?P<name>[a-z]+)/$\"), \"/<name>/\")\n\n    # Test with an unnamed group\n    self.assertEqual(simplify_regex(\"^(?:[a-z]+)/$\"), \"/[a-z]+/\")\n\n    # Test with a combination of named and unnamed groups\n    self.assertEqual(simplify_regex(\"^(?P<name>[a-z]+)/(?:[0-9]+)/$\"), \"/<name>/[0-9]+/\")\n\n    # Test with a pattern that doesn't start with '^'\n    self.assertEqual(simplify_regex(\"test/$\"), \"/test/\")\n", "    def test_simplify_regex_without_groups(self):\n        pattern = \"/some/simple/url/\"\n        self.assertEqual(simplify_regex(pattern), pattern)\n", "    def test_simplify_regex(self):\n        test_cases = [\n            ('^(?P<sport_slug>\\\\w+)/athletes/(?P<athlete_slug>\\\\w+)/$', '/<sport_slug>/athletes/<athlete_slug>/'),\n            ('^users/(?P<id>\\\\d+)/$', '/users/<id>/'),\n            ('^$', '/'),\n            ('^about/', '/about/'),\n            ('^contact/?$', '/contact/'),\n        ]\n        for input_pattern, expected_output in test_cases:\n            with self.subTest(input_pattern=input_pattern):\n                self.assertEqual(simplify_regex(input_pattern), expected_output)\n", "    def test_get_return_data_type_for_get_list(self):\n        self.assertEqual(get_return_data_type('get_names_list'), 'List')\n", "    def test_simplify_regex(self):\n        # Test with a simple regex pattern\n        self.assertEqual(simplify_regex('^simple/$'), '/simple/')\n\n        # Test with a regex pattern containing named groups\n        self.assertEqual(simplify_regex('^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'), '/<sport_slug>/athletes/<athlete_slug>/')\n\n        # Test with a regex pattern containing unnamed groups\n        self.assertEqual(simplify_regex('^(\\w+)/athletes/(\\w+)/$'), '/<1>/athletes/<2>/')\n\n        # Test with a regex pattern containing optional elements\n        self.assertEqual(simplify_regex('^simple/?$'), '/simple/')\n\n        # Test with a regex pattern not starting with '/'\n        self.assertEqual(simplify_regex('simple/$'), '/simple/')\n"], "sample_757": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_handle_unknown_transform():\n    X = np.array([['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]])\n    X2 = np.array([['xyz', 2, 55]])\n\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(\n        enc.transform(X2_passed).toarray(),\n        np.array([[0., 0., 1., 0., 1., 0.]]))\n    # ensure transformed data was not modified in place\n    assert_allclose(X2, X2_passed)\n", "def test_encoder_dtypes_manual_categories():\n    # check dtype when manually specifying categories\n    enc = OneHotEncoder(categories=[[1, 2, 3], [4, 5, 6]])\n    exp = np.array([[1., 0., 0., 1., 0., 0.], [0., 0., 1., 0., 1., 0.]], dtype='float64')\n\n    X = np.array([[1, 4], [3, 6]])\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == 'int64' for i in range(2)])\n    assert_array_equal(enc.transform(X).toarray(), exp)\n\n    X = np.array([[1, 4], [3, 6]], dtype='object')\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])\n    assert_array_equal(enc.transform(X).toarray(), exp)\n", "def test_one_hot_encoder_categories_deprecation():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    # test deprecated n_values argument\n    with pytest.warns(DeprecationWarning):\n        enc = OneHotEncoder(n_values=3)\n    assert_array_equal(enc.fit_transform(X).toarray(), [[0, 1], [1, 0]])\n    assert_array_equal(enc.categories_, [['a', 'b']])\n\n    # test deprecated n_values argument with 'auto'\n    with pytest.warns(DeprecationWarning):\n        enc = OneHotEncoder(n_values='auto')\n    assert_array_equal(enc.fit_transform(X).toarray(), [[0, 1], [1, 0]])\n    assert_array_equal(enc.categories_, [['a', 'b']])\n\n    # test deprecated n_values argument with integer\n    with pytest.warns(DeprecationWarning):\n        enc = OneHotEncoder(n_values=2)\n    assert_array_equal(enc.fit_transform(X).toarray(), [[0, 1], [1, 0]])\n    assert_array_equal(enc.categories_, [['a', 'b']])\n\n    # test deprecated n_values argument with array of integers\n    with pytest.warns(DeprecationWarning):\n        enc = OneHotEncoder(n_values=[2, 2])\n    assert_array_equal(enc.fit_transform(X).toarray(), [[0, 1], [1, 0]])\n    assert_array_equal(enc.categories_, [['a', 'b']])\n\n    # test deprecated n_values argument with wrong type\n    with pytest.raises(TypeError):\n        enc = OneHotEncoder(n_values='3')\n", "def test_one_hot_encoder_ordered_categories(X, cat_exp, cat_dtype):\n    enc = OneHotEncoder(categories='auto')\n    enc.fit(X)\n    assert enc.categories_[0].tolist() == list(cat_exp[0])\n    assert np.issubdtype(enc.categories_[0].dtype, cat_dtype)\n    exp = np.array([[1., 0.], [0., 1.]])\n    assert_array_equal(enc.transform(X).toarray(), exp)\n"], "sample_1166": ["def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n"], "sample_128": ["    def test_covering_ops_class_partial_index(self):\n        index = Index(\n            name='covering_ops_class_partial_headline_idx',\n            fields=['headline'],\n            opclasses=['varchar_pattern_ops'],\n            include=['pub_date'],\n            condition=Q(pub_date__isnull=False),\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s varchar_pattern_ops) INCLUDE (%s) WHERE %s ' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('pub_date'),\n                ),\n                str(index.create_sql(IndexedArticle2, editor)),\n            )\n            editor.add_index(IndexedArticle2, index)\n            with connection.cursor() as cursor:\n                cursor.execute(SchemaIndexesPostgreSQLTests.get_opclass_query % index.name)\n                self.assertCountEqual(cursor.fetchall(), [('varchar_pattern_ops', index.name)])\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=IndexedArticle2._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date'],\n                )\n            editor.remove_index(IndexedArticle2, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=IndexedArticle2._meta.db_table,\n                ))", "    def test_covering_index_descending(self):\n        index = Index(\n            name='covering_headline_desc_idx',\n            fields=['-headline'],\n            include=['pub_date', 'published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s DESC) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "    def test_covering_index_with_tablespace(self):\n        index_name = 'covering_headline_tblspace_idx'\n        index = Index(\n            name=index_name,\n            fields=['headline'],\n            include=['pub_date', 'published'],\n            db_tablespace='pg_default',\n        )\n        with connection.schema_editor() as editor:\n            editor.add_index(Article, index)\n            self.assertIn(\n                'TABLESPACE \"pg_default\"',\n                str(index.create_sql(Article, editor)),\n            )\n        with editor.connection.cursor() as cursor:\n            constraints = connection.introspection.get_constraints(\n                cursor=cursor, table_name=Article._meta.db_table,\n            )\n            self.assertIn(index_name, constraints)\n            self.assertEqual(\n                constraints[index_name]['columns'],\n                ['headline', 'pub_date', 'published'],\n            )\n            self.assertEqual(\n                constraints[index_name]['tablespace'],\n                'pg_default',\n            )\n        editor.remove_index(Article, index)\n        with connection.cursor() as cursor:\n            self.assertNotIn(index_name, connection.introspection.get_constraints(\n                cursor=cursor, table_name=Article._meta.db_table,\n            ))\n", "    def test_index_tablespace(self):\n        index = Index(\n            name='test_index_tablespace',\n            fields=['headline'],\n            db_tablespace='test_tablespace',\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                'TABLESPACE \"test_tablespace\"',\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertEqual(constraints[index.name]['tablespace'], 'test_tablespace')\n            editor.remove_index(Article, index)\n", "    def test_covering_index_multiple_columns(self):\n        index = Index(\n            name='covering_headline_pub_date_idx',\n            fields=['headline', 'pub_date'],\n            include=['published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s, %s) INCLUDE (%s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n"], "sample_803": ["def test_ranking_loss_sample_weighting_zero_labels():\n    # Degenerate sample labeling (e.g., zero labels for a sample) is a valid\n    # special case for label ranking loss (the sample is considered to achieve\n    # perfect precision), but this case is not tested in test_common.\n    # For these test samples, the losses are 0.5, 1.5, and 0.0 (default for zero\n    # labels).\n    y_true = np.array([[1, 0, 0, 0], [1, 0, 0, 1], [0, 0, 0, 0]],\n                      dtype=np.bool)\n    y_score = np.array([[0.3, 0.4, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4],\n                        [0.4, 0.3, 0.2, 0.1]])\n    samplewise_losses = np.array([0.5, 1.5, 0.0])\n    sample_weight = np.array([1.0, 1.0, 0.0])\n\n    assert_almost_equal(\n        label_ranking_loss(y_true, y_score, sample_weight=sample_weight),\n        np.sum(sample_weight * samplewise_losses) / np.sum(sample_weight))\n", "def test_label_ranking_loss_errors():\n    # Raise an error for non-multilabel-indicator y_true\n    y_true = np.array([0, 1])\n    y_pred = np.array([0.25, 0.75])\n    error_message = \"binary format is not supported\"\n    assert_raise_message(ValueError, error_message, label_ranking_loss, y_true, y_pred)\n", "def test_roc_auc_score_max_fpr():\n    # Test Area Under Partial Curve (AUPC) computation with max_fpr\n    y_true, _, probas_pred = make_prediction(binary=True)\n    max_fpr = 0.6\n    expected_aupc = _partial_roc_auc_score(y_true, probas_pred, max_fpr)\n\n    au_partial_curve = roc_auc_score(y_true, probas_pred, max_fpr=max_fpr)\n    assert_array_almost_equal(au_partial_curve, expected_aupc, decimal=2)\n", "def test_roc_auc_score_max_fpr():\n    # Test roc_auc_score function with max_fpr parameter\n    y_true = [0, 0, 1, 1]\n    y_score = [0.1, 0.4, 0.35, 0.8]\n    assert_almost_equal(roc_auc_score(y_true, y_score, max_fpr=0.5), _partial_roc_auc_score(y_true, y_score, 0.5), decimal=5)\n\n    # Test invalid max_fpr values\n    assert_raises(ValueError, roc_auc_score, y_true, y_score, max_fpr=0)\n    assert_raises(ValueError, roc_auc_score, y_true, y_score, max_fpr=-0.1)\n    assert_raises(ValueError, roc_auc_score, y_true, y_score, max_fpr=1.1)\n", "def test_label_ranking_loss_without_tie_and_increasing_score():\n    # Check that Label ranking loss works for various\n    # Basic check with increasing label space size and decreasing score\n    for n_labels in range(2, 10):\n        y_score = n_labels - (np.arange(n_labels).reshape((1, n_labels)) + 1)\n\n        # First and last\n        y_true = np.zeros((1, n_labels))\n        y_true[0, 0] = 1\n        y_true[0, -1] = 1\n        assert_almost_equal(label_ranking_loss(y_true, y_score), 1)\n\n        # Check for growing number of consecutive relevant label\n        for n_relevant in range(1, n_labels):\n            # Check for a bunch of position\n            for pos in range(n_labels - n_relevant):\n                y_true = np.zeros((1, n_labels))\n                y_true[0, pos:pos + n_relevant] = 1\n                assert_almost_equal(label_ranking_loss(y_true, y_score),\n                                    sum((n_labels - pos - r) for r in range(n_relevant)))\n"], "sample_28": ["def test_append_with_comment(self):\n    \"\"\"Check that the append method correctly handles adding a comment when none is provided.\"\"\"\n    header = MyHeader()\n    header.append((\"a\", 1.0))\n    assert header.comments[\"a\"] == \"no comment\"\n", "def test_subclass_assignment(self):\n    class MyHeader(fits.Header):\n            if isinstance(value, tuple) and len(value) == 1:\n                value = (value[0], \"assigned comment\")\n            super().__setitem__(key, value)\n\n    my_header = MyHeader()\n    my_header['e'] = 5.0\n    assert my_header.comments['e'] == 'assigned comment'\n    my_header['f'] = (6.0, \"custom comment\")\n    assert my_header.comments['f'] == 'custom comment'\n", "def test_remove_commentary(self):\n    \"\"\"\n    Test that the remove() method works correctly with commentary cards.\n    \"\"\"\n    header = fits.Header([(\"A\", \"B\"), (\"COMMENT\", \"C\"), (\"D\", \"E\")])\n    header.remove(\"COMMENT\")\n    assert len(header) == 2\n    assert list(header) == [\"A\", \"D\"]\n    assert \"C\" not in header\n\n    header = fits.Header([(\"A\", \"B\"), (\"HISTORY\", \"C\"), (\"D\", \"E\")])\n    header.remove(\"HISTORY\")\n    assert len(header) == 2\n    assert list(header) == [\"A\", \"D\"]\n    assert \"C\" not in header\n", "def test_hdu_copy(self):\n    \"\"\"Check that copying HDUs preserves subclassing.\"\"\"\n\n    class MyHDU(fits.ImageHDU):\n            super().__init__(data, header)\n            self.extra_attribute = \"value\"\n\n    hdu = MyHDU(data=np.ones(5))\n    hdu_copy = hdu.copy()\n\n    assert isinstance(hdu_copy, MyHDU)\n    assert hdu_copy.extra_attribute == \"value\"\n    assert np.array_equal(hdu_copy.data, hdu.data)\n    assert hdu_copy.header == hdu.header\n", "def test_update_from_dict_with_comment(self):\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n    header.update({\"A\": (\"E\", \"F\"), \"G\": (\"H\", \"I\")})\n    assert header[\"A\"] == \"E\"\n    assert header[0] == \"E\"\n    assert header.comments[\"A\"] == \"F\"\n    assert \"G\" in header\n    assert header[\"G\"] == \"H\"\n    assert header[-1] == \"H\"\n    assert header.comments[\"G\"] == \"I\"\n"], "sample_883": ["def test_bayesian_ridge_max_iter_default(global_random_seed):\n    \"\"\"Check the default value of `max_iter`.\"\"\"\n    X, y = diabetes.data, diabetes.target\n    model = BayesianRidge()\n    model.fit(X, y)\n    assert model.max_iter == 300\n    assert model.n_iter_ == 300  # Actual number of iterations should be equal to max_iter\n", "def test_bayesian_ridge_prediction_std():\n    # Test BayesianRidge prediction with standard deviation\n    X = np.array([[1], [2], [3], [4], [5]])\n    Y = np.array([1, 2, 3, 4, 5])\n    clf = BayesianRidge()\n    clf.fit(X, Y)\n\n    # Predict with standard deviation\n    y_mean, y_std = clf.predict(X, return_std=True)\n\n    # Check that the mean prediction is close to the true values\n    assert_array_almost_equal(y_mean, Y, decimal=2)\n\n    # Check that the standard deviation is non-negative and not too large\n    assert_array_less(y_std, np.ones(len(Y)))\n    assert_array_less(np.zeros(len(Y)), y_std)\n", "def test_bayesian_ridge_no_iter():\n    \"\"\"Check that the model converges when max_iter is not set.\"\"\"\n    X, y = diabetes.data, diabetes.target\n    clf = BayesianRidge(tol=1e-6)\n    clf.fit(X, y)\n\n    # Check that the model converged after a reasonable number of iterations\n    assert clf.n_iter_ < clf.max_iter\n", "def test_bayesian_ridge_max_iter_default(global_random_seed):\n    # Test BayesianRidge with default max_iter value\n    X = np.random.RandomState(global_random_seed).normal(size=(250, 3))\n    y = X[:, 1]\n\n    regressor = BayesianRidge()\n    regressor.fit(X, y)\n\n    # Check that max_iter default value is used correctly\n    assert regressor.n_iter_ <= 300\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge intercept with and without fit_intercept\n    X = np.array([[1], [2], [3]])\n    Y = np.array([2, 4, 6])\n\n    clf_with_intercept = BayesianRidge(fit_intercept=True)\n    clf_with_intercept.fit(X, Y)\n    intercept_with_intercept = clf_with_intercept.intercept_\n\n    clf_without_intercept = BayesianRidge(fit_intercept=False)\n    clf_without_intercept.fit(X, Y)\n    intercept_without_intercept = clf_without_intercept.intercept_\n\n    assert intercept_with_intercept != 0.0\n    assert intercept_without_intercept == 0.0\n"], "sample_511": ["def test_pylab_integration_no_pylab():\n    IPython = pytest.importorskip(\"IPython\")\n    mpl.testing.subprocess_run_helper(\n        IPython.start_ipython,\n        \"-c\",\n        \";\".join((\n            \"import matplotlib.pyplot as plt\",\n            \"assert plt._REPL_DISPLAYHOOK != plt._ReplDisplayHook.IPYTHON\",\n        )),\n        timeout=60,\n    )\n", "def test_subplot_mosaic():\n    fig, ax_dict = plt.subplot_mosaic(\"AB;CD\")\n    assert isinstance(fig, plt.Figure)\n    assert isinstance(ax_dict, dict)\n    assert set(ax_dict.keys()) == {\"A\", \"B\", \"C\", \"D\"}\n    for ax in ax_dict.values():\n        assert isinstance(ax, plt.Axes)\n", "def test_pyplot_hist_smoke():\n    # Small smoke test for hist (see #12405)\n    plt.hist([1, 2, 3], bins=3)\n", "def test_legend_smoke():\n    # Small smoke test for legend (see #12345)\n    plt.plot([1, 2, 3], [1, 2, 3], label='Test')\n    plt.legend()\n", "def test_pyplot_gcf_and_gca():\n    fig1 = plt.figure()\n    ax1 = plt.gca()\n    fig2 = plt.figure()\n    ax2 = plt.gca()\n    assert fig1 != fig2\n    assert ax1 != ax2\n    plt.figure(fig1)\n    assert plt.gcf() == fig1\n    assert plt.gca() == ax1\n    plt.sca(ax2)\n    assert plt.gcf() == fig2\n    assert plt.gca() == ax2\n"], "sample_676": ["def test_line_with_reprcrash_message(monkeypatch):\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n\n    mocked_pos = \"some::nodeid\"\n\n        return mocked_pos\n\n    monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n    class config(object):\n        pass\n\n    class rep(object):\n            return mocked_verbose_word\n\n    class longrepr(object):\n        class reprcrash(object):\n            message = \"This is a test message\"\n\n    rep.longrepr = longrepr()\n\n    termwidth = 80\n    result = _get_line_with_reprcrash_message(config(), rep, termwidth)\n    expected = \"FAILED some::nodeid - This is a test message\"\n    assert result == expected\n    assert wcswidth(result) <= termwidth\n", "def test_get_line_with_reprcrash_message():\n    class config(object):\n        pass\n\n    class rep(object):\n            return \"FAILED\"\n\n        class longrepr(object):\n            class reprcrash(object):\n                message = \"Some error message\"\n\n    config = config()\n    rep = rep()\n    termwidth = 80\n    result = _get_line_with_reprcrash_message(config, rep, termwidth)\n    expected = \"FAILED some::nodeid - Some error message\"\n    assert result == expected\n", "def test_line_with_reprcrash_message_no_space_for_message():\n    termwidth = 20\n    line = _get_line_with_reprcrash_message(config(), rep(), termwidth)\n    assert line == \"FAILED some::nodeid...\"\n", "def test_line_with_reprcrash_crash_message(monkeypatch):\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n    mocked_pos = \"some::nodeid\"\n    mocked_crash_message = \"Custom crash message\"\n\n        return mocked_pos\n\n    class config(object):\n        pass\n\n    class rep(object):\n            return mocked_verbose_word\n\n        class longrepr(object):\n            class reprcrash(object):\n                @property\n                    return mocked_crash_message\n\n    monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n    termwidth = 80\n    result = _pytest.terminal._get_line_with_reprcrash_message(config, rep, termwidth)\n\n    expected_line = f\"{mocked_verbose_word} {mocked_pos} - {mocked_crash_message}...\"\n    assert result == expected_line\n", "def test_line_with_reprcrash_message(monkeypatch):\n    mocked_reprcrash_message = \"Mocked reprcrash message\"\n\n    class MockedRepCrash(object):\n        message = mocked_reprcrash_message\n\n    class MockedLongRepr(object):\n        reprcrash = MockedRepCrash()\n\n    class MockedRep(object):\n        longrepr = MockedLongRepr()\n\n    termwidth = 80\n    expected_line = \"{} {} - {}\".format(mocked_verbose_word, mocked_pos, mocked_reprcrash_message)\n\n    result_line = _get_line_with_reprcrash_message(config(), MockedRep(), termwidth)\n\n    assert result_line == expected_line\n"], "sample_152": ["def test_fast_delete_reverse_fk(self):\n    a = Avatar.objects.create()\n    User.objects.create(avatar=a)\n    User.objects.create(avatar=a)\n    # 1 to delete avatar, 1 to fast-delete related users\n    self.assertNumQueries(2, a.delete)\n    self.assertEqual(User.objects.count(), 0)\n", "def test_fast_delete_with_signal_listeners(self):\n    # Connect a signal listener to User model\n        pass\n\n    models.signals.pre_delete.connect(user_pre_delete_listener, sender=User)\n\n    # Create a User instance with an Avatar\n    u = User.objects.create(avatar=Avatar.objects.create())\n    a = Avatar.objects.get(pk=u.avatar_id)\n\n    # Since there is a signal listener, fast delete should not be possible\n    collector = Collector(using='default')\n    self.assertFalse(collector.can_fast_delete(u))\n\n    # Confirm that the delete operation is not fast\n    self.assertNumQueries(3, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n\n    # Disconnect the signal listener\n    models.signals.pre_delete.disconnect(user_pre_delete_listener, sender=User)\n", "def test_fast_delete_recursive_relationships(self):\n    a = Avatar.objects.create()\n    u = User.objects.create(avatar=a)\n    u2 = User.objects.create(avatar=a)\n    # 1 query to fast-delete the users\n    # 1 query to delete the avatar\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n", "def test_fast_delete_with_restricted_field(self):\n    a = create_a('restrict')\n    b = B.objects.create(restrict=a.restrict)\n    msg = (\n        \"Cannot delete some instances of model 'R' because they are \"\n        \"referenced through restricted foreign keys: 'A.restrict', 'B.restrict'.\"\n    )\n    with self.assertRaisesMessage(RestrictedError, msg):\n        a.restrict.delete()\n    self.assertTrue(A.objects.filter(name='restrict').exists())\n    self.assertTrue(R.objects.filter(pk=a.restrict_id).exists())\n    self.assertTrue(B.objects.filter(pk=b.pk).exists())\n\n    # Object referenced also with CASCADE relationship can be deleted.\n    a.cascade.delete()\n    self.assertFalse(A.objects.filter(name='restrict').exists())\n    self.assertFalse(R.objects.filter(pk=a.restrict_id).exists())\n    self.assertFalse(B.objects.filter(pk=b.pk).exists())\n", "def test_fast_delete_select_related(self):\n    u = User.objects.create(avatar=Avatar.objects.create(desc='a'))\n    # Test that fast delete handles select_related\n    self.assertNumQueries(1, Avatar.objects.select_related('user').filter(desc='a').delete)\n    self.assertEqual(User.objects.count(), 0)\n    self.assertEqual(Avatar.objects.count(), 0)\n"], "sample_247": ["def test_alias_filter_with_subquery(self):\n    long_books_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n    ).values('publisher').annotate(count=Count('pk')).values('count')\n    publisher_books_qs = Publisher.objects.alias(\n        total_books=Count('book'),\n    ).filter(\n        total_books=Subquery(long_books_qs, output_field=IntegerField()),\n    ).values('name')\n    self.assertCountEqual(publisher_books_qs, [{'name': 'Apress'}])\n", "def test_alias_filter_with_subquery(self):\n    long_books_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n    ).values('publisher').annotate(count=Count('pk')).values('count')\n    publisher_books_qs = Publisher.objects.alias(\n        total_books=Count('book'),\n    ).filter(\n        total_books=Subquery(long_books_qs, output_field=IntegerField()),\n    ).values('name')\n    self.assertCountEqual(publisher_books_qs, [{'name': 'Apress'}])\n", "def test_annotation_with_foreign_key(self):\n    qs = Book.objects.alias(\n        publisher_name_alias=F('publisher__name'),\n    ).annotate(publisher_name=F('publisher_name_alias'))\n    self.assertIs(hasattr(qs.first(), 'publisher_name_alias'), False)\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertEqual(book.publisher_name, book.publisher.name)\n", "def test_annotation_filter_with_m2m_reverse(self):\n    qs = Book.objects.filter(\n        name='Practical Django Projects',\n        store__name='Amazon.com',\n    ).annotate(\n        store_name=F('store__name'),\n    ).filter(\n        store__name='Books.com',\n    ).annotate(\n        another_store_name=F('store__name'),\n    ).values('store_name', 'another_store_name')\n    self.assertCountEqual(\n        qs,\n        [{'store_name': 'Books.com', 'another_store_name': 'Books.com'}],\n    )\n", "def test_alias_multiple_expressions(self):\n    qs = Book.objects.alias(\n        rating_plus_one=F('rating') + 1,\n        rating_times_two=F('rating') * 2,\n    ).annotate(\n        final_rating=F('rating_plus_one') + F('rating_times_two'),\n    )\n    book = qs.first()\n    self.assertIs(hasattr(book, 'rating_plus_one'), False)\n    self.assertIs(hasattr(book, 'rating_times_two'), False)\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertEqual(book.final_rating, book.rating * 3 + 1)\n"], "sample_715": ["def test_learning_curve_pandas():\n    # check learning_curve doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        X_df, y_ser = InputFeatureType(X), TargetType(y2)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        learning_curve(clf, X_df, y_ser, cv=3)\n", "def test_cross_val_predict_multilabel():\n    X = np.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1],\n                  [-2, 1], [0, 0], [-2, -1], [-1, -2], [1, -2]])\n    y = np.array([[1, 1], [0, 1], [0, 1], [0, 1], [1, 1],\n                  [0, 1], [1, 0], [1, 1], [1, 0], [0, 0]])\n    clf = KNeighborsClassifier(n_neighbors=1)\n    preds = cross_val_predict(clf, X, y, cv=5)\n    assert_array_equal(preds.shape, y.shape)\n", "def test_learning_curve_predict_method():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = LogisticRegression()\n    methods = ['decision_function', 'predict_proba', 'predict_log_proba']\n    for method in methods:\n        train_sizes, train_scores, test_scores = learning_curve(\n            estimator, X, y, cv=3, scoring=method,\n            train_sizes=np.linspace(0.1, 1.0, 10))\n        assert_equal(train_scores.shape, (10, 3))\n        assert_equal(test_scores.shape, (10, 3))\n", "def test_cross_val_predict_with_method_no_fit():\n    # Test that an estimator that does not implement fit raises an error\n    class NoFitEstimator:\n            pass\n\n    X, y = load_iris(return_X_y=True)\n    assert_raises(AttributeError, cross_val_predict, NoFitEstimator(), X, y, method='predict_proba')\n", "def test_cross_val_predict_input_dimensions():\n    # Test with X and y of different lengths\n    X = np.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1]])\n    y = np.array([1, 1, 0, 1])\n    clf = KNeighborsClassifier(n_neighbors=1)\n    assert_raises(ValueError, cross_val_predict, clf, X, y)\n\n    # Test with X and y of different number of features\n    X = np.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1]])\n    y = np.array([[1, 1], [0, 1], [0, 1], [1, 1], [1, 1]])\n    clf = KNeighborsClassifier(n_neighbors=1)\n    assert_raises(ValueError, cross_val_predict, clf, X, y)\n"], "sample_544": ["def test_norm_none(fig_test, fig_ref):\n    t = np.random.rand(10, 10)\n    axt = fig_test.subplots()\n    axt.imshow(t, norm=None)\n    axr = fig_ref.subplots()\n    axr.imshow(t, norm=colors.Normalize())\n", "def test_invalid_origin():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=\"origin must be 'upper' or 'lower'\"):\n        ax.imshow(np.zeros((2, 2)), origin='invalid')\n", "def test_pcolor_image(fig_test, fig_ref):\n    x = np.linspace(0, 1, 11)\n    y = np.linspace(0, 1, 11)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) * np.cos(Y)\n    ax_test = fig_test.subplots()\n    ax_ref = fig_ref.subplots()\n    im_test = PcolorImage(ax_test, x, y, Z)\n    im_ref = ax_ref.pcolor(X, Y, Z, shading='auto')\n    ax_test.add_image(im_test)\n", "def test_imshow_scalar(fig_test, fig_ref):\n    ax = fig_test.subplots()\n    ax.imshow(1, vmin=0, vmax=2, cmap='viridis')\n\n    ax = fig_ref.subplots()\n    ax.imshow(np.array([[1]]), vmin=0, vmax=2, cmap='viridis')\n", "def test_str_power_norms(fig_test, fig_ref):\n    t = np.random.rand(10, 10) * .8 + .1  # between 0 and 1\n    axts = fig_test.subplots(1, 4)\n    axts[0].imshow(t, norm=\"power\", gamma=0.5)\n    axts[1].imshow(t, norm=\"power\", gamma=1.5)\n    axts[2].imshow(t, norm=\"power\", gamma=0.5, vmin=0.3, vmax=0.7)\n    axts[3].imshow(t, norm=\"power\", gamma=1.5, vmin=0.3, vmax=0.7)\n    axrs = fig_ref.subplots(1, 4)\n    axrs[0].imshow(t, norm=colors.PowerNorm(gamma=0.5))\n    axrs[1].imshow(t, norm=colors.PowerNorm(gamma=1.5))\n    axrs[2].imshow(t, norm=colors.PowerNorm(gamma=0.5, vmin=0.3, vmax=0.7))\n    axrs[3].imshow(t, norm=colors.PowerNorm(gamma=1.5, vmin=0.3, vmax=0.7))\n"], "sample_545": ["def test_subplot_mosaic_gridspec_kw():\n    fig, axd = plt.subplots(2, 2, gridspec_kw={'width_ratios': [2, 1]})\n    assert axd[0, 0].get_gridspec().get_width_ratios() == [2, 1]\n    assert axd[1, 0].get_gridspec().get_width_ratios() == [2, 1]\n    assert axd[0, 1].get_gridspec().get_width_ratios() == [2, 1]\n    assert axd[1, 1].get_gridspec().get_width_ratios() == [2, 1]\n", "def test_constrained_layout_with_no_axes():\n    fig = plt.figure(layout='constrained')\n    fig.draw_without_rendering()\n    assert fig.get_layout_engine() is not None\n", "def test_pickle_subfigures():\n    fig = plt.figure(layout='constrained')\n    sub_fig1 = fig.subfigures(1, 2)\n\n    sub_fig2 = sub_fig1[0].subfigures(2, 1, height_ratios=[1, 1.4])\n\n    fig_pickled = pickle.loads(pickle.dumps(fig))\n\n    assert len(fig_pickled.subfigs) == 1\n    assert len(fig_pickled.subfigs[0].subfigs) == 2\n    assert len(fig_pickled.subfigs[0].subfigs[0].subfigs) == 0\n    assert len(fig_pickled.subfigs[0].subfigs[1].subfigs) == 0\n", "def test_layout_change_style(layout):\n    \"\"\"\n    Test that changing the layout style raises a UserWarning.\n    \"\"\"\n    with plt.style.context('seaborn-darkgrid'):\n        fig, ax = plt.subplots(layout=layout)\n        with pytest.warns(UserWarning, match='The figure layout has changed'):\n            plt.tight_layout()\n", "def test_savefig_multiple_figures():\n    fig1, ax1 = plt.subplots()\n    fig2, ax2 = plt.subplots()\n    ax1.plot([0, 1], [0, 1])\n    ax2.plot([0, 1], [1, 0])\n    buffer = io.BytesIO()\n    with pytest.raises(ValueError):\n        plt.savefig(buffer, format='png')\n"], "sample_640": ["def test_returns_bool():\n    node = astroid.extract_node(\"return True\")\n    assert utils.returns_bool(node)\n\n    node = astroid.extract_node(\"return False\")\n    assert utils.returns_bool(node)\n\n    node = astroid.extract_node(\"return 1\")\n    assert not utils.returns_bool(node)\n\n    node = astroid.extract_node(\"return\")\n    assert not utils.returns_bool(node)\n", "def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import overload\n\n    @overload\n\n        return str(arg)\n    \"\"\"\n    )\n    assert isinstance(code, nodes.FunctionDef)\n    assert utils.is_overload_stub(code) is True\n", "def test_get_node_first_ancestor_of_type() -> None:\n    node = astroid.extract_node(\n        \"\"\"\n        class OuterClass:\n                    pass  # @\n        \"\"\"\n    )\n    assert isinstance(node, nodes.FunctionDef)\n    assert utils.get_node_first_ancestor_of_type(node, nodes.ClassDef).name == \"OuterClass\"\n    assert utils.get_node_first_ancestor_of_type(node, nodes.Module) is None\n", "def test_get_import_name() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    import os\n    from collections import defaultdict\n    from .. import parent\n    from ..parent import module\n    \"\"\"\n    )\n    assert utils.get_import_name(code[0], \"os\") == \"os\"\n    assert utils.get_import_name(code[1], \"collections\") == \"collections\"\n    assert utils.get_import_name(code[2], \"parent\") == \"parent\"\n    assert utils.get_import_name(code[3], \"module\") == \"parent.module\"\n", "def test_is_deleted_after_current() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n            var = 1\n            del var  #@\n            var = 2\n            return var\n        \"\"\"\n    )\n    assert utils.is_deleted_after_current(code, \"var\")\n"], "sample_698": ["def test_get_auto_indent() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    assert PercentStyleMultiline._get_auto_indent(None) == 0\n    assert PercentStyleMultiline._get_auto_indent(True) == -1\n    assert PercentStyleMultiline._get_auto_indent(False) == 0\n    assert PercentStyleMultiline._get_auto_indent(5) == 5\n    assert PercentStyleMultiline._get_auto_indent(\"True\") == -1\n    assert PercentStyleMultiline._get_auto_indent(\"False\") == 0\n    assert PercentStyleMultiline._get_auto_indent(\"5\") == 5\n    assert PercentStyleMultiline._get_auto_indent(\"junk\") == 0\n    assert PercentStyleMultiline._get_auto_indent({}) == 0\n", "def test_coloredlogformatter_with_extra_args() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s %(extra_arg)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n        extra={\"extra_arg\": \"Extra Info\"},\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[32mINFO    \\x1b[0m Test Message Extra Info\"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 INFO     Test Message Extra Info\")\n", "def test_coloredlogformatter_with_custom_level():\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n    custom_level = 15\n    custom_level_name = \"CUSTOM\"\n\n    logging.addLevelName(custom_level, custom_level_name)\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=custom_level,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 CUSTOM   Test Message\"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 CUSTOM   Test Message\")\n", "def test_get_auto_indent():\n    from _pytest.logging import PercentStyleMultiline\n\n    assert PercentStyleMultiline._get_auto_indent(None) == 0\n    assert PercentStyleMultiline._get_auto_indent(True) == -1\n    assert PercentStyleMultiline._get_auto_indent(False) == 0\n    assert PercentStyleMultiline._get_auto_indent(5) == 5\n    assert PercentStyleMultiline._get_auto_indent(\"True\") == -1\n    assert PercentStyleMultiline._get_auto_indent(\"False\") == 0\n    assert PercentStyleMultiline._get_auto_indent(\"5\") == 5\n    assert PercentStyleMultiline._get_auto_indent(\"junk\") == 0\n    assert PercentStyleMultiline._get_auto_indent({}) == 0\n", "def test_coloredlogformatter_no_levelname_fmt() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == \"dummypath                   10 Test Message\"\n"], "sample_423": ["def test_alter_field_and_index_together_decrease(self):\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"index_together\": {(\"name\",), (\"age\",)},\n        },\n    )\n    author_new_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"index_together\": {(\"name\",)},\n        },\n    )\n    changes = self.get_changes([initial_author], [author_new_constraints])\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterIndexTogether\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        index_together={(\"name\",)},\n    )\n", "def test_swappable_first_inheritance_proxy(self):\n    \"\"\"Swappable models get their CreateModel first, even if they are proxies.\"\"\"\n    changes = self.get_changes([], [self.custom_user, self.author_proxy])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"thirdapp\", 1)\n    self.assertOperationTypes(\n        changes, \"thirdapp\", 0, [\"CreateModel\", \"CreateModel\"]\n    )\n    self.assertOperationAttributes(changes, \"thirdapp\", 0, 0, name=\"CustomUser\")\n    self.assertOperationAttributes(changes, \"thirdapp\", 0, 1, name=\"AuthorProxy\")\n", "def test_add_model_with_fk_to_other_app_model(self):\n    book_with_other_app_fk = ModelState(\n        \"testapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"other_app_model\", models.ForeignKey(\"otherapp.OtherModel\", models.CASCADE)),\n        ],\n    )\n    other_model = ModelState(\n        \"otherapp\",\n        \"OtherModel\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    )\n    changes = self.get_changes(\n        [],\n        [book_with_other_app_fk, other_model],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Book\")\n    self.assertMigrationDependencies(\n        changes, \"testapp\", 0, [(\"otherapp\", \"__first__\")]\n    )\n", "    def test_alter_model_options_managed(self):\n        changes = self.get_changes(\n            [self.author_empty], [self.author_with_managed_option]\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            options={\"managed\": False},\n        )\n", "def test_remove_model_order_with_respect_to_index_together(self):\n    after = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ],\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.book, AutodetectorTests.author_with_book_order_wrt],\n        [AutodetectorTests.book, after],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterOrderWithRespectTo\", \"AlterIndexTogether\"],\n    )\n"], "sample_911": ["def test_template_deduction_guides():\n    check('function', 'template<typename T> A(const T&)', {2: 'I0E1AIXT1EE'})\n    check('function', 'template<typename T> A(T&&)', {2: 'I0E1AO1T'})\n    check('function', 'template<typename T> A(const T&) -> T', {2: 'I0E1AIXT1EE'})\n    check('function', 'template<typename T> A(T&&) -> T&', {2: 'I0E1AO1T'})\n", "def test_unclosed_braces():\n    # test unclosed braces\n    with pytest.raises(DefinitionError):\n        parse('member', 'int v = {1, 2, 3')\n    with pytest.raises(DefinitionError):\n        parse('member', 'int v = {1, 2, 3,')\n    with pytest.raises(DefinitionError):\n        parse('member', 'int v = {1, 2, 3]')\n", "def test_user_defined_literal():\n    check('function', 'void operator\"\" _udl()', {2: 'li4_udlv'})\n    check('function', 'void operator\"\" _udl(char)', {2: 'li4_udlE'})\n    check('function', 'void operator\"\" _udl(char...)', {2: 'li4_udlEDp'})\n    check('function', 'void operator\"\" _udl(const char*)', {2: 'li4_udlEPKc'})\n    check('function', 'void operator\"\" _udl(const char*...)', {2: 'li4_udlEPKcDp'})\n    check('function', 'void operator\"\" _udl(char, std::size_t)', {2: 'li4_udlEmNSt6size_tE'})\n    check('function', 'void operator\"\" _udl(char..., std::size_t)', {2: 'li4_udlEDpNSt6size_tE'})\n    check('function', 'void operator\"\" _udl(const char*, std::size_t)', {2: 'li4_udlEPKcmNSt6size_tE'})\n    check('function', 'void operator\"\" _udl(const char*..., std::size_t)', {2: 'li4_udlEPKcDpNSt6size_tE'})\n", "def test_xref_resolution(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_resolution.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = r'<a.*?href=[\"\\']#(?P<anchor>.*?)[\"\\'].*?>{}</a>'.format(text)\n        result = re.search(pattern, output)\n        expect = '''", "def test_alias_role():\n        pattern = '<code class=\"[^\"]*\">%s</code>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    f = 'alias-role.html'\n    t = (app.outdir / f).read_text()\n    check('alias', 'typeAlias', t, f)\n    check('alias', 'typeAlias<T>', t, f)\n    check('alias', 'typeAlias<int>', t, f)\n    check('alias', 'typeAlias<T> alias<T>', t, f)\n    check('alias', 'typeAlias<int> alias<int>', t, f)\n    check('alias', 'functionAlias()', t, f)\n    check('alias', 'functionAlias(T)', t, f)\n    check('alias', 'functionAlias(int)', t, f)\n    check('alias', 'functionAlias(T) functionAlias<T>(T)', t, f)\n    check('alias', 'functionAlias(int) functionAlias<int>(int)', t, f)\n    check('alias', 'functionAlias<T>() functionAlias<T>()', t, f)\n    check('alias', 'functionAlias<int>() functionAlias<int>()', t, f)\n    check('alias', 'variableAlias', t, f)\n    check('alias', 'variableAlias<T>', t, f)\n    check('alias', 'variableAlias<int>', t, f)\n    check('alias', 'variableAlias<T> variableAlias', t, f)\n    check('alias', 'variableAlias<int> variableAlias', t, f)\n    check('alias', 'operatorAlias()', t, f)\n    check('alias', 'operatorAlias(T)', t, f)\n    check('alias', 'operatorAlias(int)', t, f)\n    check('alias', 'operatorAlias(T) operatorAlias<T>(T)', t, f)\n    check('alias', 'operatorAlias(int) operatorAlias<int>("], "sample_1169": ["def test_issue_19662():\n    a = Symbol('0')\n    assert latex(Commutator(Bd(a), B(a)**2)\n                 ) == '\\\\left[b^\\\\dagger_{0},b_{0}^{2}\\\\right]'\n", "def test_substitute_dummies_new_indices_with_pretty_indices():\n    i, j = symbols('i j', below_fermi=True, cls=Dummy)\n    a, b = symbols('a b', above_fermi=True, cls=Dummy)\n    p, q = symbols('p q', cls=Dummy)\n    f = Function('f')\n    pretty_indices = {'above': 'st', 'below': 'uv'}\n    assert substitute_dummies(f(i, a, p) - f(j, b, q), new_indices=True, pretty_indices=pretty_indices) == f(u, s, p_0) - f(v, t, q_0)\n", "def test_substitute_dummies_no_dummies():\n    a, b = symbols('a b')\n    assert substitute_dummies(att(a, b) + 2) == att(a, b) + 2\n    assert substitute_dummies(att(a, b) + 1) == att(a, b) + 1\n", "def test_substitute_dummies_with_new_indices():\n    i, j = symbols('i j', below_fermi=True, cls=Dummy)\n    a, b = symbols('a b', above_fermi=True, cls=Dummy)\n    p, q = symbols('p q', cls=Dummy)\n    f = Function('f')\n    assert substitute_dummies(f(i, a, p) - f(j, b, q), new_indices=True) == 0\n    # Test with pretty_indices specified\n    pretty_indices = {'above': 'cd', 'below': 'xy'}\n    assert substitute_dummies(f(i, a, p) - f(j, b, q), new_indices=True, pretty_indices=pretty_indices) == f(x, c, p) - f(y, d, q)\n", "def test_fock_state_boson_ket():\n    i, j, k, l = symbols('i j k l', below_fermi=True)\n    a, b, c, d = symbols('a b c d', above_fermi=True)\n    p, q, r, s = symbols('p q r s', cls=Dummy)\n\n    state = FockStateBosonKet((2, 1))\n    assert state.q_number() == 3\n    assert state.n() == 2\n    assert state.n(0) == 2\n    assert state.n(1) == 1\n    assert state.n(2) == 0\n\n    assert str(state) == \"|2,1>\"\n    assert repr(state) == \"FockStateBosonKet((2, 1))\"\n    assert srepr(state) == \"FockStateBosonKet((2, 1))\"\n    assert latex(state) == r\"|2,1>\"\n"], "sample_660": ["def test_record_property_different_names(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"foo\", \"bar\")\n            record_property(\"baz\", \"qux\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rw\")\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    psnode = tnode.find_first_by_tag(\"properties\")\n    pnodes = psnode.find_by_tag(\"property\")\n    pnodes[0].assert_attr(name=\"foo\", value=\"bar\")\n    pnodes[1].assert_attr(name=\"baz\", value=\"qux\")\n", "def test_legacy_family_alias(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_family = legacy\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            record_xml_attribute(\"foo\", \"bar\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    tnode.assert_attr(foo=\"bar\")\n", "def test_escaped_xfailreason_issue3533(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason='1 <> 2')\n            assert False\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    snode = node.find_first_by_tag(\"skipped\")\n    assert \"1 <> 2\" in snode.text\n    snode.assert_attr(message=\"1 <> 2\")\n", "def test_record_testsuite_property_junit_family(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"all good\")\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_family = legacy\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p_node = properties_node.find_first_by_tag(\"property\")\n    p_node.assert_attr(name=\"stats\", value=\"all good\")\n", "def test_record_testsuite_property_multiple_calls(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"all good\")\n            record_testsuite_property(\"info\", \"additional info\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"--junitxml=tests.xml\")\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p1_node.assert_attr(name=\"stats\", value=\"all good\")\n    p2_node.assert_attr(name=\"info\", value=\"additional info\")\n"], "sample_798": ["def test_ridge_classifier_cv_no_support_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n", "def test_ridge_classifier_cv_no_support_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n", "def test_ridge_classifier_cv_store_cv_values_with_sample_weights():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.choice([0, 1], size=(n_samples,))\n    sample_weight = rng.rand(n_samples)\n\n    ridge_cv = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True)\n    ridge_cv.fit(X, y, sample_weight=sample_weight)\n\n    assert ridge_cv.cv_values_.shape == (n_samples, 1, 3)\n", "def test_ridge_classifier_cv_sample_weights():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=0)\n    sample_weight = np.random.RandomState(0).rand(100)\n\n    ridge_classifier_cv = RidgeClassifierCV()\n    ridge_classifier_cv.fit(X, y, sample_weight=sample_weight)\n    assert_equal(len(ridge_classifier_cv.coef_.shape), 2)\n", "def test_ridge_classifier_cv_no_support_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n"], "sample_1188": ["def test_pretty_print_unicode_d():\n    assert upretty(d[0]) == '(0|0)'\n    assert upretty(d[1]) == '(i_N|k_N)'\n    assert upretty(d[4]) == '(a) (i_N|k_N)'\n    assert upretty(d[5]) == '(a) (i_N|k_N) + (-b) (j_N|k_N)'\n    assert upretty(d[7]) == upretty_d_7\n    assert upretty(d[10]) == '(cos(a)) (i_C|k_N) + (-sin(a)) (j_C|k_N)'\n", "def test_pretty_print_unicode_d():\n    assert upretty(d[0]) == '(0|0)'\n    assert upretty(d[1]) == '(i_N|k_N)'\n    assert upretty(d[4]) == '(a) (i_N|k_N)'\n    assert upretty(d[5]) == '(a) (i_N|k_N) + (-b) (j_N|k_N)'\n    assert upretty(d[8]) == '(j_N|k_N) + (C.x**2 - \u222bf(b) db) (k_N|k_N)'\n", "def test_pretty_print_unicode_d():\n    assert upretty(d[0]) == '(0|0)'\n    assert upretty(d[1]) == '(i_N|k_N)'\n    assert upretty(d[2]) == '(-1) (i_N|k_N)'\n    assert upretty(d[3]) == '(i_N|k_N) + (j_N|k_N)'\n    assert upretty(d[4]) == '(a) (i_N|k_N)'\n    assert upretty(d[5]) == '(a) (i_N|k_N) + (-b) (j_N|k_N)'\n    assert upretty(d[6]) == '(a**2 + x_N) (i_N|k_N) + (k_N|k_N)'\n    assert upretty(d[7]) == upretty_d_7\n    assert upretty(d[8]) == '(j_N|k_N) + (\u222b f(b) db) (k_N|k_N)'\n    assert upretty(d[9]) == '(k_C|k_N) + (i_N|k_N)'\n    assert upretty(d[10]) == '(cos(a)) (i_C|k_N) + (-sin(a)) (j_C|k_N)'\n", "def test_pretty_printing_del():\n    assert pretty(Del()) == '\u2207'\n    assert pretty(Del('C')) == '\u2207_C'\n", "def test_pretty_print_unicode_del():\n    delop = Del()\n    v = delop(N.x)\n    assert upretty(v) == '\u2207x_N'\n"], "sample_97": ["def test_tick_triggers_on_file_creation(self, mock_notify_file_changed):\n    new_file = self.ensure_file(self.tempdir / 'new_file.py')\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.tempdir]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        self.increment_mtime(new_file)\n        next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n        self.assertCountEqual(mock_notify_file_changed.call_args[0], [new_file])\n", "    def test_watch_dir_without_absolute(self):\n        reloader = autoreload.BaseReloader()\n        with self.assertRaisesMessage(ValueError, 'test.py must be absolute.'):\n            reloader.watch_dir('test.py', '*.py')\n", "def test_nonexistent_file_in_glob(self, mocked_modules, notify_mock):\n    nonexistent_py_file = self.tempdir / 'nonexistent.py'\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        self.increment_mtime(nonexistent_py_file)\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_tick_triggers_on_file_creation(self, mock_notify_file_changed):\n    new_file = self.tempdir / 'new_file.py'\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, new_file]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        new_file.touch()\n        next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n        self.assertCountEqual(mock_notify_file_changed.call_args[0], [new_file])\n", "def test_nonexistent_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.nonexistent_file)\n    with self.tick_twice():\n        # The nonexistent file should not trigger a change notification\n        pass\n    self.assertEqual(notify_mock.call_count, 0)\n"], "sample_851": ["def test_regression_multioutput_invalid_weights():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n\n    with pytest.raises(ValueError, match=\"There must be equally many custom weights\"):\n        mean_squared_error(y_true, y_pred, multioutput=[0.4, 0.6, 0.2])\n\n    with pytest.raises(ValueError, match=\"There must be equally many custom weights\"):\n        mean_absolute_error(y_true, y_pred, multioutput=[0.4, 0.6, 0.2])\n\n    with pytest.raises(ValueError, match=\"There must be equally many custom weights\"):\n        r2_score(y_true, y_pred, multioutput=[0.4, 0.6, 0.2])\n\n    with pytest.raises(ValueError, match=\"There must be equally many custom weights\"):\n        explained_variance_score(y_true, y_pred, multioutput=[0.4, 0.6, 0.2])\n\n    with pytest.raises(ValueError, match=\"There must be equally many custom weights\"):\n        mean_squared_log_error(y_true, y_pred, multioutput=[0.4, 0.6, 0.2])\n", "def test_regression_multioutput_single_output():\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n\n    # Test that metrics return the same value for single output and multioutput\n    mse_single = mean_squared_error(y_true, y_pred)\n    mse_multi = mean_squared_error([[val] for val in y_true], [[val] for val in y_pred])\n    assert_almost_equal(mse_single, mse_multi)\n\n    mae_single = mean_absolute_error(y_true, y_pred)\n    mae_multi = mean_absolute_error([[val] for val in y_true], [[val] for val in y_pred])\n    assert_almost_equal(mae_single, mae_multi)\n\n    r2_single = r2_score(y_true, y_pred)\n    r2_multi = r2_score([[val] for val in y_true], [[val] for val in y_pred])\n    assert_almost_equal(r2_single, r2_multi)\n\n    evs_single = explained_variance_score(y_true, y_pred)\n    evs_multi = explained_variance_score([[val] for val in y_true], [[val] for val in y_pred])\n    assert_almost_equal(evs_single, evs_multi)\n", "def test_max_error():\n    y_true = [3, 2, 7, 1]\n    y_pred = [4, 2, 7, 1]\n    assert_almost_equal(max_error(y_true, y_pred), 1)\n\n    # Test for multioutput\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n", "def test_regression_multioutput_variance_weighted():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n\n    r = r2_score(y_true, y_pred, multioutput='variance_weighted')\n    evs = explained_variance_score(y_true, y_pred, multioutput='variance_weighted')\n\n    assert_almost_equal(r, 0.9558, decimal=4)\n    assert_almost_equal(evs, 0.9558, decimal=4)\n", "def test_regression_metrics_with_sample_weight():\n    y_true = np.array([2, 3, 4, 5])\n    y_pred = np.array([1, 2, 3, 4])\n    sample_weight = np.array([0.2, 0.3, 0.1, 0.4])\n\n    # Test mean_squared_error with sample_weight\n    mse = mean_squared_error(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(mse, 0.6, decimal=2)\n\n    # Test mean_absolute_error with sample_weight\n    mae = mean_absolute_error(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(mae, 0.7, decimal=2)\n\n    # Test r2_score with sample_weight\n    r2 = r2_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(r2, 0.87, decimal=2)\n\n    # Test explained_variance_score with sample_weight\n    evs = explained_variance_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(evs, 0.92, decimal=2)\n\n    # Test mean_tweedie_deviance with sample_weight\n    mtd = mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight, power=0)\n    assert_almost_equal(mtd, mse, decimal=2)  # power=0 corresponds to MSE\n"], "sample_449": ["def test_persistent_connections_require_threading_server(self):\n    \"\"\"Persistent connections require threading server.\"\"\"\n\n        \"\"\"A WSGI app that returns a simple response.\"\"\"\n        start_response(\"200 OK\", [])\n        return [b\"Hello World\"]\n\n    rfile = BytesIO(b\"GET / HTTP/1.1\\r\\nConnection: keep-alive\\r\\n\\r\\n\")\n    rfile.seek(0)\n\n    wfile = UnclosableBytesIO()\n\n        if mode == \"rb\":\n            return rfile\n        elif mode == \"wb\":\n            return wfile\n\n    request = Stub(makefile=makefile)\n    server = Stub(base_environ={}, get_app=lambda: test_app)\n\n    # Prevent logging from appearing in test output.\n    with self.assertLogs(\"django.server\", \"INFO\"):\n        # Instantiating a handler runs the request as side effect.\n        handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n    wfile.seek(0)\n    lines = list(wfile.readlines())\n    # The connection should be closed if threading is not used.\n    self.assertIn(b\"Connection: close\\r\\n\", lines)\n\n    # Use a threading server.\n    server = Stub(base_environ={}, get_app=lambda: test_app)\n    server.__class__ = type(\"WSGIServer\", (socketserver.ThreadingMixIn, WSGIServer), {})\n\n    wfile = UnclosableBytesIO()\n    request = Stub(makefile=makefile)\n\n    with self.assertLogs(\"django.server\", \"INFO\"):\n        handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n    wfile.seek(0)\n    lines = list(wfile.readlines())\n    # The connection should not be closed if threading is used", "def test_connection_header_for_head_requests(self):\n        \"\"\"A WSGI app that returns a hello world.\"\"\"\n        start_response(\"200 OK\", [])\n        return [b\"\"]\n\n    rfile = BytesIO(b\"HEAD / HTTP/1.1\\r\\n\")\n    rfile.seek(0)\n\n    wfile = UnclosableBytesIO()\n\n        if mode == \"rb\":\n            return rfile\n        elif mode == \"wb\":\n            return wfile\n\n    request = Stub(makefile=makefile)\n    server = Stub(base_environ={}, get_app=lambda: test_app)\n\n    with self.assertLogs(\"django.server\", \"INFO\"):\n        WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n    wfile.seek(0)\n    lines = list(wfile.readlines())\n    self.assertIn(b\"Connection: close\\r\\n\", lines)\n", "def test_connection_close_header(self):\n    \"\"\"WSGIRequestHandler closes the connection when a 'Connection: close' header is sent.\"\"\"\n\n        \"\"\"A WSGI app that sends a 'Connection: close' header.\"\"\"\n        start_response(\"200 OK\", [(\"Connection\", \"close\")])\n        return [b\"Hello World\"]\n\n    rfile = BytesIO(b\"GET / HTTP/1.1\\r\\n\")\n    rfile.seek(0)\n\n    wfile = UnclosableBytesIO()\n\n        if mode == \"rb\":\n            return rfile\n        elif mode == \"wb\":\n            return wfile\n\n    request = Stub(makefile=makefile, close=lambda: None)\n    server = Stub(base_environ={}, get_app=lambda: test_app)\n\n    with self.assertLogs(\"django.server\", \"INFO\"):\n        handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n        handler.handle_one_request()\n\n    self.assertTrue(handler.close_connection)\n", "def test_broken_pipe_error(self):\n    \"\"\"WSGIServer handles broken pipe errors correctly.\"\"\"\n\n    class BrokenPipeRequest:\n            # Raise a BrokenPipeError when trying to write to the socket.\n            raise BrokenPipeError()\n\n    request = BrokenPipeRequest()\n    server = Stub(base_environ={})\n\n    with self.assertLogs(\"django.server\", \"INFO\") as cm:\n        WSGIServer.handle_error(server, request, (\"192.168.0.2\", 12345))\n\n    self.assertIn(\"Broken pipe from\", cm.output[0])\n", "def test_threaded_server(self):\n    \"\"\"\n    Test the ThreadedWSGIServer with a mock WSGI application.\n    \"\"\"\n    # Mock WSGI application\n        start_response(\"200 OK\", [(\"Content-Type\", \"text/plain\")])\n        return [b\"Hello, World!\"]\n\n    # Create a mock connection\n    mock_conn = Stub(makefile=lambda *args, **kwargs: BytesIO())\n\n    # Create a mock server\n    mock_server = Stub(\n        server_address=(\"127.0.0.1\", 8000),\n        RequestHandlerClass=WSGIRequestHandler,\n        base_environ={},\n        get_app=lambda: mock_application,\n    )\n\n    # Create a threaded WSGI server\n    server = ThreadedWSGIServer((\"127.0.0.1\", 8000), WSGIRequestHandler)\n    server.socket = mock_server\n    server.server_bind()\n\n    # Handle a mock request\n    server.process_request_thread(mock_conn, (\"127.0.0.1\", 12345))\n\n    # Check the response\n    mock_conn.makefile(\"wb\").seek(0)\n    response = mock_conn.makefile(\"wb\").read().decode()\n    self.assertIn(\"200 OK\", response)\n    self.assertIn(\"Hello, World!\", response)\n\n    # Close the server\n    server.server_close()\n"], "sample_756": ["def test_min_maxima_ratio():\n    # Test that the min_maxima_ratio parameter affects the clustering result\n    X1 = np.vstack((C1, C2, C3))\n    X2 = np.vstack((C4, C5, C6))\n    X = np.vstack((X1, X2))\n\n    clust1 = OPTICS(min_samples=9, min_maxima_ratio=0.001).fit(X)\n    clust2 = OPTICS(min_samples=9, min_maxima_ratio=0.01).fit(X)\n\n    assert not np.array_equal(clust1.labels_, clust2.labels_)\n", "def test_maxima_ratio_range():\n    # Test that maxima_ratio is within the valid range (0, 1)\n    clust = OPTICS(maxima_ratio=1.1)\n    with pytest.raises(ValueError, match=\"maxima_ratio must be in the range\"):\n        clust.fit(X)\n\n    clust = OPTICS(maxima_ratio=0)\n    with pytest.raises(ValueError, match=\"maxima_ratio must be in the range\"):\n        clust.fit(X)\n", "def test_core_distances_computation():\n    # Test the computation of core distances with a known input and output\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n    nbrs = NearestNeighbors(n_neighbors=3).fit(X)\n    clust = OPTICS(min_samples=3)\n    expected_core_distances = np.array([np.inf, np.inf, 2.82842712, np.inf, np.inf])\n    computed_core_distances = clust._compute_core_distances_(X, nbrs)\n    assert_allclose(computed_core_distances, expected_core_distances)\n", "def test_min_samples_check():\n    # Test that min_samples is correctly checked\n    msg = \"Number of training samples (n_samples=3) must be greater than min_samples (min_samples=10) used for clustering.\"\n\n    # Create data with less samples than min_samples\n    X = [[1, 1], [2, 2], [3, 3]]\n    clust = OPTICS(min_samples=10)\n\n    # Try fitting the data and check if the correct error is raised\n    assert_raise_message(ValueError, msg, clust.fit, X)\n", "def test_min_maxima_ratio():\n    # Test min_maxima_ratio parameter\n    redX = X[::2]  # reduce for speed\n    clust = OPTICS(min_samples=9, min_maxima_ratio=0.01).fit(redX)\n    local_maxima = _find_local_maxima(clust.reachability_[clust.ordering_].tolist(),\n                                      int(0.01 * len(redX)))\n    assert len(local_maxima) > 0  # Ensure that local maxima are found\n\n    # Check min_maxima_ratio parameter with large value\n    clust_large = OPTICS(min_samples=9, min_maxima_ratio=0.5).fit(redX)\n    local_maxima_large = _find_local_maxima(clust_large.reachability_[clust_large.ordering_].tolist(),\n                                            int(0.5 * len(redX)))\n    assert len(local_maxima_large) == 0  # Ensure no local maxima found with large value\n"], "sample_115": ["def test_sensitive_variables_in_kwargs(self):\n    \"\"\"\n    Sensitive variables don't leak in the sensitive_variables decorator's\n    frame, when those variables are passed as keyword arguments to the\n    decorated function.\n    \"\"\"\n    request = self.rf.post('/some_url/', self.breakfast_data)\n    response = sensitive_kwargs_function_caller(request)\n    self.assertNotContains(response, 'bacon-value', status_code=500)\n    self.assertNotContains(response, 'worcestershire', status_code=500)\n", "def test_template_encoding_in_production(self):\n    \"\"\"\n    The templates are loaded directly, not via a template loader, and\n    should be opened as utf-8 charset as is the default specified on\n    template engines, even in production mode.\n    \"\"\"\n    reporter = ExceptionReporter(None, None, None, None)\n    with mock.patch.object(DebugPath, 'open') as m:\n        reporter.get_traceback_html()\n        m.assert_called_once_with(encoding='utf-8')\n        m.reset_mock()\n        reporter.get_traceback_text()\n        m.assert_called_once_with(encoding='utf-8')\n", "    def test_callable_setting_returns_dict(self):\n            return {\"PASSWORD\": \"This should not be displayed\"}\n\n        with self.settings(DEBUG=True, FOOBAR=callable_setting):\n            response = self.client.get('/raises500/')\n            self.assertNotContains(response, \"This should not be displayed\", status_code=500)\n", "    def test_unicode_exception_message(self):\n        try:\n            raise ValueError(\"French caf\u00e9\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(self.rf.get('/test_view/'), exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('<pre class=\"exception_value\">French caf&#xe9;</pre>', html)\n        text = reporter.get_traceback_text()\n        self.assertIn(\"French caf\\xe9\", text)\n", "def test_exception_reporter_frames_source(self):\n    \"\"\"\n    Test that the ExceptionReporter._get_lines_from_file method correctly\n    returns the source code lines and line numbers for a given file,\n    function, and line number.\n    \"\"\"\n    # Define a simple function to get its source code lines\n        x = 1\n        y = 2\n        return x + y\n\n    # Get the source code lines for the test_func function\n    filename = inspect.getsourcefile(test_func)\n    lineno = inspect.getsourcelines(test_func)[1]\n    context_lines = 1\n    source = inspect.getsourcelines(test_func)[0]\n\n    # Create an instance of ExceptionReporter\n    reporter = ExceptionReporter(None, None, None, None)\n\n    # Call the _get_lines_from_file method\n    pre_context_lineno, pre_context, context_line, post_context = reporter._get_lines_from_file(\n        filename, lineno, context_lines\n    )\n\n    # Assert that the source code lines and line numbers are correct\n    self.assertEqual(pre_context, source[lineno - context_lines - 1:lineno])\n    self.assertEqual(context_line, source[lineno])\n    self.assertEqual(post_context, source[lineno + 1:lineno + context_lines + 1])\n    self.assertEqual(pre_context_lineno, lineno - context_lines)\n"], "sample_778": ["def test_nmf_with_zeros_in_input():\n    # Test that NMF can handle input matrices with zeros\n    X = np.array([[1, 0, 2], [0, 3, 4], [5, 6, 0]])\n    model = NMF(n_components=2, solver='cd', init='random', random_state=0)\n    W = model.fit_transform(X)\n    assert not np.any(np.isnan(W))\n", "def test_nmf_zero_input():\n    # Test that an error is raised when X contains zeros\n    X = np.array([[0, 1], [2, 1], [3, 1.2]])\n    msg = \"Negative values in data passed to NMF (input X)\"\n    assert_raise_message(ValueError, msg, NMF().fit, X)\n", "def test_nmf_fixed_H():\n    # Test that NMF works when fixing H\n    n_samples = 6\n    n_features = 5\n    n_components = 3\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(n_samples, n_features))\n    H = np.abs(rng.randn(n_components, n_features))\n\n    for solver in ('cd', 'mu'):\n        model = NMF(n_components=n_components, solver=solver, init='random',\n                    random_state=0, max_iter=1000)\n        W_fit = model.fit_transform(X)\n        W_transform = model.transform(X, H=H, update_H=False)\n\n        assert_array_almost_equal(W_fit, W_transform, decimal=2)\n", "def test_nmf_beta_loss_convergence():\n    # Test that the reconstruction error decreases with more iterations\n    # for all beta_loss values.\n    n_samples = 10\n    n_features = 5\n    n_components = 3\n\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(n_samples, n_features))\n\n    for beta_loss in [-1.2, 0, 0.2, 1., 2., 2.5]:\n        model = NMF(n_components=n_components, solver='mu',\n                    beta_loss=beta_loss, random_state=0, max_iter=10)\n        model.fit_transform(X)\n        err_10_iter = model.reconstruction_err_\n\n        model.max_iter = 20\n        model.fit_transform(X)\n        err_20_iter = model.reconstruction_err_\n\n        assert_less(err_20_iter, err_10_iter)\n", "def test_nmf_max_iter(solver):\n    # Test that NMF stops when max_iter is reached\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(6, 5))\n    m = NMF(solver=solver, n_components=3, init='random',\n            random_state=0, tol=1e-5, max_iter=1)\n    m.fit(A)\n    assert m.n_iter_ == 1\n"], "sample_72": ["def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(complex(1, 2), (\"complex((1+2j))\", set()))\n", "def test_serialize_complex(self):\n    complex_num = complex(1, 2)\n    self.assertSerializedEqual(complex_num)\n    self.assertSerializedResultEqual(\n        complex_num,\n        (\"complex((1+2j))\", set())\n    )\n", "def test_serialize_custom_serializer(self):\n    class CustomClass:\n            self.a = a\n            self.b = b\n\n    class CustomClassSerializer(BaseSerializer):\n            a_string, a_imports = serializer_factory(self.value.a).serialize()\n            b_string, b_imports = serializer_factory(self.value.b).serialize()\n            imports = {'import migrations.test_writer', *a_imports, *b_imports}\n            return \"migrations.test_writer.CustomClass(%s, %s)\" % (a_string, b_string), imports\n\n    Serializer.register(CustomClass, CustomClassSerializer)\n    try:\n        custom_obj = CustomClass('test', 123)\n        result = self.serialize_round_trip(custom_obj)\n        self.assertIsInstance(result, CustomClass)\n        self.assertEqual(result.a, 'test')\n        self.assertEqual(result.b, 123)\n    finally:\n        Serializer.unregister(CustomClass)\n", "def test_serialize_complex(self):\n    complex_num = complex(2, 3)\n    self.assertSerializedEqual(complex_num)\n    self.assertSerializedResultEqual(\n        complex_num,\n        (\"complex('2+3j')\", {})\n    )\n\n    # Register the custom serializer for complex numbers\n    Serializer.register(complex, ComplexSerializer)\n\n    # Test serialization after registering the custom serializer\n    string, imports = MigrationWriter.serialize(complex_num)\n    self.assertEqual(string, \"complex('2+3j')\")\n    self.assertEqual(imports, set())\n\n    # Test round-trip serialization after registering the custom serializer\n    new_complex_num = self.serialize_round_trip(complex_num)\n    self.assertEqual(new_complex_num, complex_num)\n\n    # Unregister the custom serializer for complex numbers\n    Serializer.unregister(complex)\n\n    # Test serialization after unregistering the custom serializer\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (2+3j)\\n\"):\n        MigrationWriter.serialize(complex_num)\n", "def test_serialize_complex(self):\n    complex_value = 1 + 2j\n    self.assertSerializedEqual(complex_value)\n    self.assertSerializedResultEqual(\n        complex_value,\n        (\"complex('(1+2j)')\", {})\n    )\n"], "sample_846": ["def test_column_transformer_callable_specifier_pandas_index():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, index=['first', 'second'], columns=['A', 'B'])\n    X_res_first = np.array([[0, 1, 2]]).T\n\n        assert_array_equal(X.index, X_df.index)\n        return ['first']\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == ['first']\n", "def test_column_transformer_transformer_with_no_transform_method():\n    class TransNoTransform(BaseEstimator):\n            return self\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans', TransNoTransform(), [0])])\n    assert_raise_message(AttributeError, \"transform\", ct.fit_transform, X_array)\n    assert_raise_message(AttributeError, \"transform\", ct.fit, X_array)\n", "def test_column_transformer_with_custom_transformer():\n    # Test the ColumnTransformer with a custom transformer\n\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            return self\n\n            return X * 2\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_both = X_array * 2\n\n    ct = ColumnTransformer([('trans', CustomTransformer(), [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n", "def test_column_transformer_sparse_input():\n    X_sparse = sparse.csr_matrix([[1, 0], [0, 1]])\n    X_res = np.array([[2], [2]])\n    ct = ColumnTransformer([('trans', FunctionTransformer(func=lambda x: 2*x))])\n    assert_array_equal(ct.fit_transform(X_sparse).toarray(), X_res)\n    assert_array_equal(ct.fit(X_sparse).transform(X_sparse).toarray(), X_res)\n", "def test_column_transformer_reordered_column_names_remainder_with_added_rows():\n    \"\"\"Regression test for issue #14223: 'Named col indexing fails with\n       ColumnTransformer remainder on changing DataFrame column ordering'\n\n       Should raise error on changed order combined with remainder.\n       Should allow for added rows in `transform` input DataFrame\n       as long as all columns match.\n    \"\"\"\n    pd = pytest.importorskip('pandas')\n\n    X_fit_array = np.array([[0, 1], [2, 4]])\n    X_fit_df = pd.DataFrame(X_fit_array, columns=['first', 'second'])\n\n    X_trans_array = np.array([[0, 1], [2, 4], [3, 6]])\n    X_trans_df = pd.DataFrame(X_trans_array, columns=['first', 'second'])\n\n    tf = ColumnTransformer([('bycol', Trans(), 'first')],\n                           remainder=Trans())\n\n    tf.fit(X_fit_df)\n    err_msg = 'Number of features'\n    warn_msg = (\"Given feature/column names or counts do not match the ones \"\n                \"for the data given during fit.\")\n    with pytest.raises(ValueError, match=err_msg):\n        tf.transform(X_trans_df)\n\n    # No error for added rows if columns are identical\n    X_extended_df = pd.DataFrame(np.vstack([X_fit_array, [3, 6]]),\n                                 columns=['first', 'second'])\n    with pytest.warns(DeprecationWarning, match=warn_msg):\n        tf.transform(X_extended_df)  # No error should be raised, for now\n\n    # No error when transform input is a numpy array with added rows\n    X_array = np.vstack([X_fit_array, [3, 6]])\n    with pytest.warns(DeprecationWarning, match=warn_msg):\n        tf.transform(X_array)\n"], "sample_538": ["def test_transformwrapper_dimension_check():\n    t = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    with pytest.raises(ValueError, match=(\n            r\"The input and output dims of the new child \\(2, 3\\) \"\n            r\"do not match those of current child \\(2, 2\\)\")):\n        t.set(mtransforms.Affine2D().scale(1, 2, 3))\n", "def test_transform_path_affine():\n    t = mtransforms.Affine2D()\n    path = Path([[0, 0], [1, 1], [2, 2]], [1, 2, 79])\n    tpath = t.transform_path_affine(path)\n    assert_array_equal(tpath.vertices, path.vertices)\n    assert_array_equal(tpath.codes, path.codes)\n\n    r2 = 1 / np.sqrt(2)\n    t.rotate(np.pi / 4)\n    tpath = t.transform_path_affine(path)\n    assert_allclose(tpath.vertices, [[0, 0], [r2, r2], [2 * r2, 2 * r2]], atol=1e-15)\n    assert_array_equal(tpath.codes, path.codes)\n", "def test_transformwrapper_equality():\n    t1 = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    t2 = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    t3 = mtransforms.TransformWrapper(mtransforms.Affine2D().scale(2))\n\n    # Two wrappers of equal transforms should be equal.\n    assert t1 == t2\n\n    # Two wrappers of different transforms should not be equal.\n    assert t1 != t3\n\n    # A wrapper should be equal to its child transform.\n    assert t1 == t1._child\n", "def test_transformwrapper_dimensionality():\n    t = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    # Check that setting a transform with different dimensions raises an error\n    with pytest.raises(ValueError, match=(\n            r\"The input and output dims of the new child \\(2, 3\\) \"\n            r\"do not match those of current child \\(2, 2\\)\")):\n        t.set(mtransforms.Affine2D().scale(1, 2, 3))\n", "def test_transformwrapper_dimensions():\n    t = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    t.set(mtransforms.Affine2D().scale(2))  # This should be valid\n\n    # Try setting a transform with different input dimensions\n    with pytest.raises(ValueError, match=(\n            r\"The input and output dims of the new child \\(3, 3\\) \"\n            r\"do not match those of current child \\(2, 2\\)\")):\n        t.set(mtransforms.Affine2D().scale(3, 3))\n\n    # Try setting a transform with different output dimensions\n    with pytest.raises(ValueError, match=(\n            r\"The input and output dims of the new child \\(2, 3\\) \"\n            r\"do not match those of current child \\(2, 2\\)\")):\n        t.set(mtransforms.Affine2D().scale(1, 2))\n"], "sample_850": ["def test_nystroem_kernel_params():\n    # Test Nystroem with kernel_params.\n    rnd = np.random.RandomState(42)\n    n_samples = 10\n    X = rnd.uniform(size=(n_samples, 4))\n\n        return np.exp(-gamma * np.sum((x - y) ** 2))\n\n    nystroem = Nystroem(kernel=custom_kernel, n_components=n_samples, gamma=0.5, kernel_params={'gamma': 0.5})\n    X_transformed = nystroem.fit_transform(X)\n    K = custom_kernel(X[:, np.newaxis, :], X[np.newaxis, :, :], gamma=0.5)\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n", "def test_nystroem_kernel_params():\n    # Test Nystroem with custom kernel parameters.\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    custom_params = {'custom_param': 0.5}\n\n        return np.dot(X * custom_param, Y.T)\n\n    nystroem = Nystroem(n_components=2, kernel=custom_kernel, kernel_params=custom_params, random_state=rnd)\n    X_transformed = nystroem.fit_transform(X)\n    assert X_transformed.shape == (X.shape[0], 2)\n\n    # Check that the custom kernel parameter is used.\n    assert nystroem._get_kernel_params()['custom_param'] == custom_params['custom_param']\n", "def test_nystroem_kernel_params():\n    # Test Nystroem with kernel_params.\n    rnd = np.random.RandomState(42)\n    n_samples = 10\n    X = rnd.uniform(size=(n_samples, 4))\n\n        \"\"\"Custom kernel that uses a parameter.\"\"\"\n        return np.exp(-alpha * np.sum((x - y) ** 2))\n\n    alpha = 0.5\n    K = custom_kernel(X[:, np.newaxis, :], X[np.newaxis, :, :], alpha)\n    nystroem = Nystroem(kernel=custom_kernel, n_components=n_samples, kernel_params={'alpha': alpha})\n    X_transformed = nystroem.fit_transform(X)\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n", "def test_nystroem_kernel_params():\n    # Test Nystroem with kernel_params\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n        return np.exp(-gamma * np.linalg.norm(X - Y))\n\n    nystroem = Nystroem(kernel=custom_kernel, n_components=5, gamma=0.5, random_state=rnd)\n    X_transformed = nystroem.fit_transform(X)\n    K = custom_kernel(X[:, np.newaxis], X[np.newaxis, :], gamma=0.5)\n    K2 = np.dot(X_transformed, X_transformed.T)\n    assert_array_almost_equal(K, K2)\n", "def test_nystroem_error_handling():\n    # Test that Nystroem raises errors when appropriate\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    # Test that using gamma, coef0, degree with a callable kernel raises an error\n        return np.dot(X, Y.T)\n    nystroem = Nystroem(kernel=custom_kernel, n_components=2, gamma=0.5)\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n\n    # Test that using an invalid kernel raises an error\n    nystroem = Nystroem(kernel='invalid_kernel', n_components=2)\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n\n    # Test that using a precomputed kernel raises an error\n    nystroem = Nystroem(kernel='precomputed', n_components=2)\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n"], "sample_174": ["def test_explain_query_prefix_raise_not_supported_error(self):\n    msg = 'This backend does not support explaining query execution.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        self.ops.explain_query_prefix()\n", "def test_explain_query_prefix(self):\n    msg = 'This backend does not support explaining query execution.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        self.ops.explain_query_prefix()\n", "def test_autoinc_sql(self):\n    self.assertIsNone(self.ops.autoinc_sql('table', 'column'))\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    max_digits = 5\n    decimal_places = 2\n    result = self.ops.adapt_decimalfield_value(value, max_digits, decimal_places)\n    self.assertEqual(result, '3.14')\n", "def test_savepoint_sql(self):\n    savepoint_id = 'test_savepoint'\n    self.assertEqual(self.ops.savepoint_create_sql(savepoint_id), 'SAVEPOINT \"{}\"'.format(savepoint_id))\n    self.assertEqual(self.ops.savepoint_commit_sql(savepoint_id), 'RELEASE SAVEPOINT \"{}\"'.format(savepoint_id))\n    self.assertEqual(self.ops.savepoint_rollback_sql(savepoint_id), 'ROLLBACK TO SAVEPOINT \"{}\"'.format(savepoint_id))\n"], "sample_88": ["    def test_send_messages_with_empty_string_recipients(self):\n        backend = smtp.EmailBackend()\n        backend.connection = True\n        email = EmailMessage('Subject', 'Content', 'from@example.com', to=['', 'to@example.com'])\n        sent = backend.send_messages([email])\n        self.assertEqual(sent, 1)\n", "def test_send_messages_after_close(self):\n    \"\"\"\n    send_messages() shouldn't try to send messages if close() was called.\n    \"\"\"\n    backend = smtp.EmailBackend()\n    # Simulate an already open connection.\n    backend.connection = True\n    backend.close()\n    email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    self.assertEqual(backend.send_messages([email]), 0)\n", "def test_message_from_bytes(self):\n    \"\"\"\n    Email message can be created from bytes.\n    \"\"\"\n    msg_str = (\n        \"From: from@example.com\\n\"\n        \"To: to@example.com\\n\"\n        \"Subject: Subject\\n\"\n        \"\\n\"\n        \"Content\\n\"\n    )\n    msg = mail.message_from_bytes(msg_str.encode())\n    self.assertEqual(msg[\"subject\"], \"Subject\")\n    self.assertEqual(msg.get_payload(), \"Content\")\n    self.assertEqual(msg[\"from\"], \"from@example.com\")\n    self.assertEqual(msg.get_all(\"to\"), [\"to@example.com\"])\n", "def test_send_messages_different_from_recipients(self):\n    \"\"\"A message is sent if it has recipients different from the sender.\"\"\"\n    backend = smtp.EmailBackend()\n    backend.connection = True\n    email = EmailMessage('Subject', 'Content', 'from@example.com', to=['to@example.com'])\n    sent = backend.send_messages([email])\n    self.assertEqual(sent, 1)\n", "def test_ssl_tls_connection(self):\n    \"\"\"\n    Test that the backend can connect to an SMTP server using SSL/TLS.\n    \"\"\"\n    # Start an SMTP server with SSL/TLS support.\n    self.server.starttls = True\n    self.server.start()\n\n    # Create an email backend with SSL/TLS enabled.\n    backend = smtp.EmailBackend(use_ssl=True)\n    backend.open()\n    self.assertTrue(backend.connection._is_ssl)\n\n    # Send a test email.\n    email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    backend.send_messages([email])\n\n    # Verify that the email was sent.\n    self.assertEqual(len(self.server.get_sink()), 1)\n    sent_email = self.server.get_sink()[0]\n    self.assertEqual(sent_email.get('subject'), 'Subject')\n    self.assertEqual(sent_email.get('from'), 'from@example.com')\n    self.assertEqual(sent_email.get('to'), 'to@example.com')\n\n    # Close the connection.\n    backend.close()\n\n    # Stop the SMTP server.\n    self.server.stop()\n"], "sample_552": ["def test_savefig_pil_kwargs():\n    fig = plt.figure()\n    with io.BytesIO() as buf:\n        fig.savefig(buf, format='png', pil_kwargs={'optimize': True})\n        img = Image.open(buf)\n        img.verify()\n", "def test_savefig_metadata_invalid_key():\n    with pytest.raises(ValueError, match=\"Unrecognized metadata key: invalid_key\"):\n        Figure().savefig(io.BytesIO(), format='png', metadata={'invalid_key': 'value'})\n", "def test_invalid_gridspec_kw():\n    with pytest.raises(ValueError, match=\"Unrecognized gridspec_kw: 'invalid_kw'\"):\n        plt.subplots(1, 2, gridspec_kw={'invalid_kw': True})\n", "def test_rcparams_different_values():\n    fig_test = Figure()\n    fig_ref = Figure()\n\n    fig_ref.supxlabel(\"xlabel\", weight='bold', size=15)\n    fig_ref.supylabel(\"ylabel\", weight='light', size=20)\n    fig_ref.suptitle(\"Title\", weight='bold', size=15)\n\n    with mpl.rc_context({'figure.labelweight': 'bold',\n                         'figure.labelsize': 15,\n                         'figure.titleweight': 'light',\n                         'figure.titlesize': 20}):\n        fig_test.supxlabel(\"xlabel\")\n        fig_test.supylabel(\"ylabel\", weight='light', size=20)\n        fig_test.suptitle(\"Title\")\n\n    assert fig_test.supxlabel.get_weight() == fig_ref.supxlabel.get_weight()\n    assert fig_test.supylabel.get_weight() != fig_ref.supylabel.get_weight()\n    assert fig_test.suptitle.get_weight() != fig_ref.suptitle.get_weight()\n    assert fig_test.supxlabel.get_size() == fig_ref.supxlabel.get_size()\n    assert fig_test.supylabel.get_size() != fig_ref.supylabel.get_size()\n    assert fig_test.suptitle.get_size() != fig_ref.suptitle.get_size()\n", "def test_savefig_metadata_eps(recwarn):\n    metadata = {'Creator': 'Test Creator'}\n    with io.BytesIO() as buf:\n        Figure().savefig(buf, format='eps', metadata=metadata)\n        buf.seek(0)\n        eps_content = buf.read()\n        assert b'%%Creator: Test Creator' in eps_content\n\n    metadata = {'InvalidKey': 'Invalid Value'}\n    with pytest.raises(ValueError, match='Metadata contains unsupported keys'):\n        Figure().savefig(io.BytesIO(), format='eps', metadata=metadata)\n\n    metadata = {'Creator': 'Test Creator', 'InvalidKey': 'Invalid Value'}\n    with pytest.raises(ValueError, match='Metadata contains unsupported keys'):\n        Figure().savefig(io.BytesIO(), format='eps', metadata=metadata)\n\n    metadata = {'Creator': 'Test Creator'}\n    with pytest.warns(UserWarning, match='Metadata for eps format supports'):\n        Figure().savefig(io.BytesIO(), format='eps', metadata=metadata, encapsulated='svg')\n"], "sample_706": ["def test_mixed_identifiers(expr: str, expected: bool) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_operator_precedence(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is True\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_identifiers(expr: str, ident: str, expected: bool) -> None:\n    matcher = {ident: True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is eval(expr, {\"True\": True, \"False\": False})\n"], "sample_315": ["    def test_en_redirect(self):\n        response = self.client.get('/en/account/register', HTTP_ACCEPT_LANGUAGE='en', follow=True)\n        # We only want one redirect, bypassing CommonMiddleware\n        self.assertEqual(response.redirect_chain, [('/en/account/register/', 302)])\n        self.assertRedirects(response, '/en/account/register/', 302)\n", "    def test_script_prefix_with_prefix(self):\n        response = self.client.get('/test/en/account/register/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers['content-language'], 'en')\n        self.assertEqual(response.context['LANGUAGE_CODE'], 'en')\n", "    def test_middleware_fallback(self):\n        response = self.client.get('/fallback/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers['content-language'], settings.LANGUAGE_CODE)\n        self.assertEqual(response.context['LANGUAGE_CODE'], settings.LANGUAGE_CODE)\n", "    def test_en_redirect(self):\n        response = self.client.get('/not-prefixed/', HTTP_ACCEPT_LANGUAGE='en')\n        self.assertRedirects(response, '/en/not-prefixed/')\n\n        response = self.client.get(response.headers['location'])\n        self.assertEqual(response.status_code, 200)\n", "    def test_no_language_prefix_redirect(self):\n        response = self.client.get('/account/register/')\n        self.assertRedirects(response, '/fr/account/register/')\n\n        response = self.client.get(response.headers['location'])\n        self.assertEqual(response.status_code, 200)\n"], "sample_601": ["def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"H\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 16)],\n            [cftime_date_type(1, 1, 1, 23), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"H\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 12)],\n            [cftime_date_type(1, 1, 1, 23), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n"], "sample_1092": ["def test_issue_18203_variation():\n    eq = CRootOf(x**5 + 12*x - 3, 0) + CRootOf(x**5 + 12*x - 3, 1)\n    assert cse(eq) == ([], [eq])\n", "def test_cse_with_roots():\n    eq = CRootOf(x**3 - 2, 0) + CRootOf(x**3 - 2, 1) + CRootOf(x**3 - 2, 2)\n    substs, reduced = cse(eq)\n    assert len(substs) > 0\n    assert reduced[0] == eq.subs(substs)\n", "def test_cse_order():\n    exprs = [x**5 + x**4 + x**3 + x**2, x + y + z, x + y + z + w]\n    substs_canonical, _ = cse(exprs, order='canonical')\n    substs_none, _ = cse(exprs, order='none')\n    assert substs_canonical == substs_none, \"CSE results should be independent of order\"\n", "def test_issue_18203_different_indices():\n    eq = CRootOf(x**5 + 11*x - 2, 0) + CRootOf(x**5 + 11*x - 2, 2)\n    assert cse(eq) == ([], [eq])\n", "def test_cse_matrices():\n    from sympy import Matrix\n    A = Matrix([[x, y], [z, w]])\n    B = Matrix([[x + z, y + w], [2*x, 2*y]])\n    exprs = [A, B, A + B]\n    subst, red = cse(exprs)\n    assert len(subst) == 1  # A should be common subexpression\n    assert red[0] == A\n    assert red[1] == B\n    assert red[2] == subst[0][0] + B  # A + B should use the common subexpression\n"], "sample_600": ["def test_CFMaskCoder_decode_with_multiple_fill_values():\n    original = xr.Variable((\"x\",), [0, -1, 1, -999], {\"_FillValue\": [-1, -999]})\n    expected = xr.Variable((\"x\",), [0, np.nan, 1, np.nan])\n    coder = variables.CFMaskCoder()\n    with pytest.warns(variables.SerializationWarning):\n        encoded = coder.decode(original)\n    assert_identical(expected, encoded)\n", "def test_CFScaleOffsetCoder_decode():\n    original = xr.Variable((\"x\",), [10.0, 11.0, 12.0], {\"scale_factor\": 2.0, \"add_offset\": 1.0})\n    expected = xr.Variable((\"x\",), [5.0, 6.0, 7.0])\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.decode(original)\n    assert_identical(expected, encoded)\n", "def test_CFMaskCoder_encode_decode_with_nan_fill_values():\n    original = xr.Variable((\"x\",), [0.0, np.nan, 1.0], {\"_FillValue\": np.nan})\n    expected = xr.Variable((\"x\",), [0.0, np.nan, 1.0])\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    decoded = coder.decode(encoded)\n    assert_identical(expected, decoded)\n", "def test_CFMaskCoder_conflicting_fill_values():\n    original = xr.Variable((\"x\",), [0.0, -1.0, 1.0], encoding={\"_FillValue\": -1.0, \"missing_value\": 1.0})\n    coder = variables.CFMaskCoder()\n    with pytest.raises(ValueError, match=\"has conflicting _FillValue and missing_value.\"):\n        coder.encode(original, name=\"x\")\n", "def test_CFMaskCoder_decode_multiple_fill_values():\n    original = xr.Variable((\"x\",), [0, -1, 1], {\"_FillValue\": [-1, 2]})\n    with pytest.warns(variables.SerializationWarning):\n        encoded = variables.CFMaskCoder().decode(original)\n    expected = xr.Variable((\"x\",), [0, np.nan, 1])\n    assert_identical(expected, encoded)\n"], "sample_1031": ["def test_quantity_dimension_consistency():\n    # Test that the dimension of a quantity is consistent with its scale factor\n    for _scale_factor, _dimension in zip(\n            Quantity.SI_quantity_scale_factors.values(),\n            Quantity.SI_quantity_dimension_map.values()):\n        dimex = Quantity.get_dimensional_expr(_scale_factor)\n        assert dimex == 1 or dimsys_default.equivalent_dims(_dimension, Dimension(dimex))\n", "def test_quantity_scaling():\n    # Test that quantities are correctly scaled based on their dimension\n    assert m.scale_factor == S.One\n    assert kg.scale_factor == S.One\n    assert s.scale_factor == S.One\n\n    # Test a derived quantity\n    assert c.scale_factor == 299792458 * m / s\n\n    # Test a custom quantity\n    Js = Quantity(\"Js\")\n    Js.set_dimension(action)\n    Js.set_scale_factor(S.One)\n    assert Js.scale_factor == S.One\n\n    # Test a quantity with a non-SI scale factor\n    inch = Quantity(\"inch\")\n    inch.set_dimension(length)\n    inch.set_scale_factor(Rational(3048, 10000) * m)\n    assert inch.scale_factor == Rational(3048, 10000) * m\n", "def test_quantity_conversion():\n    # Test conversion of quantity between different units\n    m_to_cm = m.convert_to(cm)\n    assert m_to_cm == 100 * cm\n    assert m.convert_to(km) == m / 1000\n\n    # Test conversion of quantity with scale factor\n    kg_to_g = kg.convert_to(g)\n    assert kg_to_g == 1000 * g\n    assert kg.convert_to(mg) == 1e6 * mg\n\n    # Test conversion of derived unit\n    J_to_eV = J.convert_to(eV)\n    assert J_to_eV == J / electronvolt\n\n    # Test conversion of complex unit expression\n    expr = (m / s).convert_to(km / hour)\n    assert expr == m / s * hour / km\n", "def test_quantity_scale_factor():\n    # Test that the scale factors have the correct dimensions\n    for scale_factor, dimension in zip(\n            Quantity.SI_quantity_scale_factors.values(),\n            Quantity.SI_quantity_dimension_map.values()):\n        dimex = Quantity.get_dimensional_expr(scale_factor)\n        assert dimex == 1 or dimension.system.equivalent_dims(dimension, Dimension(dimex))\n", "def test_unit_conversion():\n    # Test conversion between units of the same dimension\n    assert Quantity(2, 'm').to('cm') == Quantity(200, 'cm')\n    assert Quantity(2, 'kg').to('g') == Quantity(2000, 'g')\n    assert Quantity(2, 's').to('ms') == Quantity(2000, 'ms')\n\n    # Test conversion between units of different dimensions\n    assert Quantity(2, 'N').to('J') == Quantity(2, 'm')  # Force to Energy\n    assert Quantity(2, 'J').to('W') == Quantity(2, 'J/s')  # Energy to Power\n    assert Quantity(2, 'Pa').to('J/m**3') == Quantity(2, 'J/m**3')  # Pressure to Energy Density\n"], "sample_764": ["def test_column_transformer_callable_specifier_empty():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        assert_array_equal(X, X_array)\n        return []\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), np.empty((3, 0)))\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.empty((3, 0)))\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == []\n", "def test_column_transformer_empty_transformer_list():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([])\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'drop'\n    assert_array_equal(ct.transformers_[-1][2], [0, 1])\n", "def test_column_transformer_empty_input():\n    X_array = np.array([])\n    ct = ColumnTransformer([('trans1', 'drop', [])])\n    assert_array_equal(ct.fit_transform(X_array).shape, (0, 0))\n    assert_array_equal(ct.fit(X_array).transform(X_array).shape, (0, 0))\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][0] != 'remainder'\n\n    ct = ColumnTransformer([('trans1', 'passthrough', [])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array).shape, (0, 0))\n    assert_array_equal(ct.fit(X_array).transform(X_array).shape, (0, 0))\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][0] != 'remainder'\n", "def test_column_transformer_drop_columns_in_remainder_transformer():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    # Only the second column is doubled when remainder = DoubleTrans\n    X_res_both = X_array.copy()\n    X_res_both[:, 1] *= 2\n\n    ct = ColumnTransformer([('trans1', 'drop', [0])],\n                           remainder=DoubleTrans())\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both[:, 1:])\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both[:, 1:])\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1])\n", "def test_column_transformer_remainder_transformer_with_empty_columns():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [])],\n                           remainder=DoubleTrans())\n\n    assert_array_equal(ct.fit_transform(X_array), 2 * X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), 2 * X_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [0, 1, 2])\n"], "sample_836": ["def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n    confidences = np.array([[0.5, 0.3, 0.6], [0.4, 0.7, 0.2], [0.8, 0.9, 0.1]])\n    n_classes = 3\n\n    expected_output = np.array([[0.33333333, -0.03333333, 0.03333333],\n                               [0.03333333, 0.33333333, -0.03333333],\n                               [0.33333333, -0.03333333, 0.03333333]])\n\n    output = _ovr_decision_function(predictions, confidences, n_classes)\n    assert_allclose(output, expected_output, atol=1e-7)\n", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with a simple example\n    predictions = np.array([[0, 1, 1], [2, 2, 0]])\n    confidences = np.array([[0.2, 0.5, 0.7], [0.3, 0.6, 0.1]])\n    n_classes = 3\n    expected_result = np.array([[0.08571429, 0.31428571, 0.59047619],\n                                [0.40952381, 0.68571429, 0.23809524]])\n    result = _ovr_decision_function(predictions, confidences, n_classes)\n    assert_allclose(result, expected_result, rtol=1e-5)\n", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with simple binary case\n    predictions = np.array([[0, 1], [1, 0]])\n    confidences = np.array([[0.9, 0.1], [0.2, 0.8]])\n    n_classes = 2\n    expected_output = np.array([[0.9/3, 0.1/3], [0.1/3, 0.9/3]])\n    assert_allclose(_ovr_decision_function(predictions, confidences, n_classes), expected_output)\n\n    # Test _ovr_decision_function with multiclass case\n    predictions = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n    confidences = np.array([[0.9, 0.1, 0.2], [0.2, 0.8, 0.7], [0.3, 0.6, 0.4]])\n    n_classes = 3\n    expected_output = np.array([[1.0/3, 0.0/3, 0.0/3], [0.0/3, 1.0/3, 0.0/3], [0.0/3, 0.5/3, 0.5/3]])\n    assert_allclose(_ovr_decision_function(predictions, confidences, n_classes), expected_output)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 1], [0, 0, 1]])\n    confidences = np.array([[0.8, 0.9, 0.7], [0.6, 0.5, 0.9]])\n    n_classes = 3\n    expected_output = np.array([[1.0, -0.13333333, 1.13333333],\n                                [1.0, 1.0, -0.13333333]])\n    output = _ovr_decision_function(predictions, confidences, n_classes)\n    assert_allclose(output, expected_output, atol=1e-6)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 2], [1, 2, 0], [2, 0, 1]])\n    confidences = np.array([[0.2, 0.3, 0.5], [0.4, 0.6, 0.1], [0.7, 0.1, 0.2]])\n    n_classes = 3\n\n    expected_output = np.array([[1, 0, -1/3], [0, -1/3, 2/3], [2/3, 1/3, -1/3]])\n    output = _ovr_decision_function(predictions, confidences, n_classes)\n\n    assert_array_almost_equal(output, expected_output)\n"], "sample_560": ["def test_loc_validation_numeric_value_out_of_range():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=('loc must be string, coordinate tuple, or an integer 0-10, not 11')):\n        ax.legend(loc=11)\n\n    with pytest.raises(ValueError, match=('loc must be string, coordinate tuple, or an integer 0-10, not -1')):\n        ax.legend(loc=-1)\n", "def test_loc_validation_string_value():\n    fig, ax = plt.subplots()\n    ax.legend(loc=\"best\")\n    ax.legend(loc=\"upper right\")\n    ax.legend(loc=\"right\")\n    ax.legend(loc=\"center left\")\n    ax.legend(loc=\"lower center\")\n    with pytest.raises(ValueError, match='loc must be string, coordinate tuple, or an integer 0-10, not \"invalid\"'):\n        ax.legend(loc=\"invalid\")\n", "def test_legend_loc_outside_figure():\n    # Test that 'outside' option for loc works for figure legends\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = fig.legend(loc='outside right upper')\n    assert leg._get_loc() == mlegend.Legend.codes['upper right']\n    assert leg._outside_loc == 'right'\n", "def test_legend_title_fontprop_fontsize_none():\n    # test the title_fontsize kwarg when it's set to None\n    plt.plot(range(10))\n    leg = plt.legend(title='Aardvark', title_fontsize=None)\n    assert leg.get_title().get_fontsize() == mpl.rcParams['font.size']\n\n    fig, axes = plt.subplots(2, 3, figsize=(10, 6))\n    axes = axes.flat\n    mpl.rcParams['legend.title_fontsize'] = 20\n    axes[5].plot(range(10))\n    leg5 = axes[5].legend(title='Aardvark', title_fontsize=None)\n    assert leg5.get_title().get_fontsize() == mpl.rcParams['font.size']\n", "def test_legend_get_set_loc():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n    assert leg._get_loc() == leg._loc\n    leg.set_loc('upper right')\n    assert leg._get_loc() == mlegend.Legend.codes['upper right']\n"], "sample_685": ["def test_get_records(caplog, logging_during_setup_and_teardown):\n    assert not caplog.get_records(\"call\")\n    logger.info(\"a_call_log\")\n    assert [x.message for x in caplog.get_records(\"call\")] == [\"a_call_log\"]\n    assert [x.message for x in caplog.get_records(\"setup\")] == [\"a_setup_log\"]\n    assert [x.message for x in caplog.get_records(\"teardown\")] == [\"a_teardown_log\"]\n", "def test_caplog_records_correct_order(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"first log\")\n    logger.info(\"second log\")\n    logger.info(\"third log\")\n    assert [record.msg for record in caplog.records] == [\"first log\", \"second log\", \"third log\"]\n", "def test_caplog_messages_with_multiline(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"multiline\\nmessage\\nwith\\nnewlines\")\n    assert caplog.messages[0] == \"multiline\\nmessage\\nwith\\nnewlines\"\n    assert caplog.text.count(\"\\n\") == 4\n", "def test_caplog_captures_with_format_params(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\", \"arg\")\n    logger.info(\"bar %s\\nbaz %s\", \"arg1\", \"arg2\")\n\n    assert \"boo arg\" in caplog.text\n    assert \"bar arg1\\nbaz arg2\" in caplog.text\n    assert len(caplog.records) == 2\n    assert caplog.records[0].args == (\"arg\",)\n    assert caplog.records[1].args == (\"arg1\", \"arg2\")\n", "def test_logging_formatter_with_colors(testdir, caplog):\n    # Given\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli = True\n        log_cli_level = INFO\n        log_cli_format = %(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\n        log_cli_date_format = %H:%M:%S\n        color = yes\n        \"\"\"\n    )\n\n    # When\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger('test_logger')\n            logger.info(\"Test INFO message\")\n            logger.warning(\"Test WARNING message\")\n            logger.error(\"Test ERROR message\")\n            logger.debug(\"Test DEBUG message\")\n            logger.critical(\"Test CRITICAL message\")\n        \"\"\"\n    )\n    result = testdir.runpytest()\n\n    # Then\n    result.stdout.fnmatch_lines([\"*Test INFO message*\", \"*Test WARNING message*\", \"*Test ERROR message*\", \"*Test DEBUG message*\", \"*Test CRITICAL message*\"])\n    assert \"INFO\" in result.stdout.str()\n    assert \"WARNING\" in result.stdout.str()\n    assert \"ERROR\" in result.stdout.str()\n    assert \"DEBUG\" in result.stdout.str()\n    assert \"CRITICAL\" in result.stdout.str()\n"], "sample_843": ["def test_kernel_operator_associativity():\n    # Adding and multiplying kernels should be associative.\n    k1, k2, k3 = RBF(2.0), RBF(3.0), RBF(4.0)\n\n    # Check addition\n    assert_almost_equal((k1 + k2) + k3(X),\n                        k1 + (k2 + k3)(X))\n\n    # Check multiplication\n    assert_almost_equal((k1 * k2) * k3(X),\n                        k1 * (k2 * k3)(X))\n", "def test_kernel_bounds(kernel):\n    # Check that the bounds of the kernel are consistent with the bounds of its hyperparameters\n    bounds = kernel.bounds\n    assert bounds.shape[0] == len(kernel.hyperparameters)\n    assert bounds.shape[1] == 2\n    for i, hyperparameter in enumerate(kernel.hyperparameters):\n        assert_array_equal(bounds[i], hyperparameter.bounds)\n", "def test_kernel_bounds(kernel):\n    # Check that kernel.bounds returns the correct bounds\n    bounds = kernel.bounds\n    theta = kernel.theta\n    assert bounds.shape == (len(theta), 2)\n    for i, hyperparameter in enumerate(kernel.hyperparameters):\n        if not hyperparameter.fixed:\n            assert np.all(bounds[i] == np.log(hyperparameter.bounds))\n", "def test_kernel_hyperparameter_bounds():\n    # Test that the bounds on hyperparameters are correctly enforced.\n    with pytest.raises(ValueError):\n        RBF(length_scale=-1.0)\n\n    with pytest.raises(ValueError):\n        RBF(length_scale=[1.0, -1.0])\n\n    with pytest.raises(ValueError):\n        RBF(length_scale_bounds=[-1.0, 1.0])\n\n    with pytest.raises(ValueError):\n        RBF(length_scale_bounds=[[1.0, 2.0], [-1.0, 1.0]])\n", "def test_kernel_equality(kernel):\n    # Check that kernels are equal to themselves and clones\n    assert kernel == kernel\n    assert kernel == clone(kernel)\n"], "sample_1158": ["def test_sympify_numpy_complex():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    c = numpy.complex(1 + 2j)\n    s = sympify(c)\n    assert isinstance(s, Complex)\n    assert s == 1 + 2*I\n\n    c_array = numpy.array([1 + 2j, 3 + 4j])\n    s_array = sympify(c_array)\n    assert isinstance(s_array, ImmutableDenseNDimArray)\n    assert numpy.all(s_array == c_array)\n", "def test_sympify_numpy_complex():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    z = numpy.complex(1 + 2j)\n    assert sympify(z) == S(1.0 + 2.0*I)\n    assert _sympify(z) == S(1.0 + 2.0*I)\n    assert sympify(z, strict=True) == S(1.0 + 2.0*I)\n\n    z_array = numpy.array([1 + 2j, 3 + 4j])\n    assert sympify(z_array) == ImmutableDenseNDimArray([1.0 + 2.0*I, 3.0 + 4.0*I], 1)\n    assert _sympify(z_array) == ImmutableDenseNDimArray([1.0 + 2.0*I, 3.0 + 4.0*I], 1)\n    assert sympify(z_array, strict=True) == ImmutableDenseNDimArray([1.0 + 2.0*I, 3.0 + 4.0*I], 1)\n", "def test_issue_sympify_complex():\n    # Test sympify for complex numbers\n    assert sympify('1+2j') == 1 + 2*I\n    assert sympify('(1+2j)') == 1 + 2*I\n    assert sympify('(1+2j)+(3-4j)') == 4 - 2*I\n    assert sympify('(1+2j)*(3-4j)') == 11 - 2*I\n    assert sympify('(1+2j)/(3-4j)') == -0.4 + 0.6*I\n", "def test_issue_18066_no_fallback():\n    class NoStr:\n        pass\n\n    with raises(SympifyError):\n        sympify(NoStr())\n", "def test_issue_18066():\n    # test that _sympy_ is used instead of the fallback to str\n    class CustomSymbol(Symbol):\n            return Symbol(self.name.upper())\n\n    a = sympify(CustomSymbol('a'))\n    assert a == Symbol('A')\n"], "sample_587": ["def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n    ds2 = xr.Dataset({\"x\": 2, \"z\": 3})\n    expected = xr.Dataset({\"x\": 2, \"y\": 1, \"z\": 3})\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=\"x\"))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"x\"]))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=set([\"x\"])))\n", "def test_merge_override(self):\n    ds1 = xr.Dataset({\"x\": (\"y\", [1, 2]), \"y\": [0, 1]})\n    ds2 = xr.Dataset({\"x\": (\"y\", [2, 3]), \"y\": [1, 2]})\n    expected = xr.Dataset({\"x\": (\"y\", [2, 3]), \"y\": [1, 2]})\n    assert expected.identical(xr.merge([ds1, ds2], compat=\"override\"))\n", "def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n    ds2 = xr.Dataset({\"x\": 2, \"z\": 3})\n    expected = xr.Dataset({\"x\": 2, \"y\": 1, \"z\": 3})\n    actual = ds1.merge(ds2, overwrite_vars=[\"x\", \"z\"])\n    assert expected.identical(actual)\n\n    expected = xr.Dataset({\"x\": 0, \"y\": 1, \"z\": 3})\n    actual = ds1.merge(ds2, overwrite_vars=[\"z\"])\n    assert expected.identical(actual)\n", "def test_merge_with_overwrite_vars(self):\n    ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n    ds2 = xr.Dataset({\"x\": 2, \"z\": 3})\n\n    expected = xr.Dataset({\"x\": 2, \"y\": 1, \"z\": 3})\n    actual = ds1.merge(ds2, overwrite_vars=[\"x\", \"z\"])\n    assert expected.identical(actual)\n\n    expected = xr.Dataset({\"x\": 0, \"y\": 1, \"z\": 3})\n    actual = ds1.merge(ds2, overwrite_vars=\"z\")\n    assert expected.identical(actual)\n", "def test_merge_with_pandas_objects(self):\n    import pandas as pd\n    ds1 = xr.Dataset({\"x\": pd.Series([1, 2], index=[\"a\", \"b\"])})\n    ds2 = xr.Dataset({\"y\": pd.Series([3, 4], index=[\"a\", \"b\"])})\n    expected = xr.Dataset(\n        {\"x\": (\"index\", [1, 2], {\"index\": [\"a\", \"b\"]}), \"y\": (\"index\", [3, 4], {\"index\": [\"a\", \"b\"]})}\n    )\n    assert expected.identical(ds1.merge(ds2))\n"], "sample_970": ["def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_getdoc_partial_function():\n        \"\"\"\n        This is a docstring for the function.\n        \"\"\"\n        pass\n\n    p = functools.partial(func, 10)\n\n    assert inspect.getdoc(p) == func.__doc__\n", "def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is False\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_isabstractmethod(app):\n    from target.methods import Base\n    from abc import abstractmethod\n\n    assert inspect.isabstractmethod(Base.abstractmeth) is True\n    assert inspect.isabstractmethod(Base.meth) is False\n    assert inspect.isabstractmethod(abstractmethod(Base.meth)) is True\n", "def test_getdoc_partial():\n        \"\"\"\n        docstring\n            indented text\n        \"\"\"\n\n    p = functools.partial(func, 1)\n\n    assert inspect.getdoc(p) == func.__doc__\n"], "sample_150": ["def test_check_method_called(self, mocked_check):\n    command = BaseCommand()\n    command.requires_system_checks = True\n    options = {'skip_checks': False}\n    command.execute(**options)\n    self.assertTrue(mocked_check.called)\n", "    def test_command_error_message(self):\n        with self.assertRaises(CommandError) as context:\n            raise CommandError(\"Test error message\")\n        self.assertEqual(str(context.exception), \"Test error message\")\n", "def test_check_system_checks_called(self, mocked_check):\n    command = BaseCommand()\n    command.execute()\n    self.assertTrue(mocked_check.called)\n", "def test_check_migrations_called(self, mocked_check):\n    command = BaseCommand()\n    command.requires_migrations_checks = True\n    command.execute()\n    self.assertTrue(mocked_check.called)\n", "    def test_command_error_message(self):\n        from django.core.management import CommandError\n        message = \"Test error message\"\n        with self.assertRaises(CommandError) as context:\n            raise CommandError(message)\n        self.assertEqual(str(context.exception), message)\n"], "sample_972": ["def test_restify_type_hints_custom_generic():\n    class CustomGeneric(Generic[T]):\n        pass\n\n    assert restify(CustomGeneric[int]) == \":py:class:`tests.test_util_typing.CustomGeneric`\\\\ [:py:class:`int`]\"\n", "def test_stringify_type_hints_TypeVar_bound():\n    T = TypeVar('T', bound=int)\n    assert stringify(T) == \"tests.test_util_typing.T\"\n    assert stringify(T, \"smart\") == \"~tests.test_util_typing.T\"\n\n    assert stringify(List[T]) == \"List[tests.test_util_typing.T]\"\n    assert stringify(List[T], \"smart\") == \"~typing.List[~tests.test_util_typing.T]\"\n", "def test_restify_type_ForwardRef_with_annotations():\n    from typing import ForwardRef\n\n    class MyClass(metaclass=ForwardRef):\n        __annotations__ = {'x': 'int'}\n\n    assert restify(MyClass) == \":py:class:`tests.test_util_typing.MyClass`\"\n", "def test_restify_type_hints_pep_585():\n    assert restify(list[str]) == \":py:class:`list`\\\\ [:py:class:`str`]\"  # type: ignore\n    assert restify(dict[str, str]) == (\":py:class:`dict`\\\\ \"  # type: ignore\n                                       \"[:py:class:`str`, :py:class:`str`]\")\n    assert restify(dict[str, tuple[int, ...]]) == (\":py:class:`dict`\\\\ \"  # type: ignore\n                                                   \"[:py:class:`str`, :py:class:`tuple`\\\\ \"\n                                                   \"[:py:class:`int`, ...]]\")\n", "def test_restify_type_hints_type_with_module():\n    assert restify(datetime.datetime) == \":py:class:`datetime.datetime`\"\n    assert restify(datetime.date) == \":py:class:`datetime.date`\"\n\n    class MyCustomClass:\n        __module__ = 'custom_module'\n\n    assert restify(MyCustomClass) == \":py:class:`custom_module.MyCustomClass`\"\n"], "sample_1105": ["def test_matmul_identity():\n    assert MatMul(A, GenericIdentity()) == A\n    assert MatMul(GenericIdentity(), A) == A\n    assert MatMul(A, GenericIdentity(), B) == MatMul(A, B)\n", "def test_matmul_noncommutative_scalars():\n    a, b = symbols('a b', commutative=False)\n    assert MatMul(a, b, A, A.T) == MatMul(a*b, A, A.T)\n    assert MatMul(A, A.T, a, b) == MatMul(A, A.T, a*b)\n", "def test_matmul_transpose_inverse():\n    assert transpose(Inverse(A*B)) == Inverse(B.T*A.T)\n", "def test_matmul_with_scalars():\n    assert MatMul(2, 3) == 6\n    assert MatMul(2, A) == 2*A\n    assert MatMul(A, 3) == 3*A\n", "def test_matmul_inverse():\n    assert Inverse(MatMul(A, B)) == MatMul(Inverse(B), Inverse(A))\n    assert Inverse(2*A*B) == (1/2)*MatMul(Inverse(B), Inverse(A))\n    assert Inverse(2*I*C) == (-1/(2*I))*Inverse(C)\n\n    M = Matrix(2, 2, [1, 2 + I, 3, 4])\n    assert Inverse(M)*M == Identity(2)\n    assert M*Inverse(M) == Identity(2)\n"], "sample_916": ["def test_build_domain_cpp_duplicate_declaration(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"duplicate-declaration\")\n    assert len(ws) == 1\n    assert \"WARNING: Duplicate C object description of Sphinx::version, other instance in index\" in ws[0]\n", "def test_xref_consistency_for_expression_role():\n    role = 'expr'\n    tag = 'span'\n    expected_classes = {'xref', 'cpp', 'c-expr'}\n    assert classes(role, tag) == expected_classes\n", "def test_constexpr_functions():\n    check('function', 'constexpr int get_value()', {1: \"get_valueCE\", 2: \"9get_valuev\"})\n    check('function', 'constexpr int get_value() noexcept', {1: \"get_valueCE\", 2: \"9get_valuev\"})\n    check('function', 'constexpr int get_value() noexcept(true)', {1: \"get_valueCE\", 2: \"9get_valuev\"})\n    check('function', 'constexpr int get_value() noexcept(false)', {1: \"get_valueCE\", 2: \"9get_valuev\"})\n    check('function', 'constexpr int get_value() noexcept(1 + 2)', {1: \"get_valueCE\", 2: \"9get_valuev\"})\n    check('function', 'constexpr int get_value() noexcept(get_value() > 0)', {1: \"get_valueCE\", 2: \"9get_valuev\"})\n", "def test_variable_templates():\n    check('variable', 'constexpr int a = 42', {1: 'a__iC', 2: '3a'})\n    check('variable', 'inline int b = 42', {1: 'b__i', 2: '3b'})\n    check('variable', 'const int c = 42', {1: 'c__iC', 2: '3c'})\n    check('variable', 'int d = 42', {1: 'd__i', 2: '1d'})\n", "def test_concept_constraints():\n    check('concept', 'template<typename T> requires A<T> && B<T> C',\n          {2: 'I0ERA1AIT1EEERA1BIT1EEE1C'})\n    check('concept', 'template<typename T> requires A<T> || B<T> C',\n          {2: 'I0EROA1AIT1EEEOA1BIT1EEE1C'})\n    check('concept', 'template<typename T> requires A<T> && (B<T> || C<T>) D',\n          {2: 'I0ERA1AIT1EEEROA1BIT1EEEEA1CIT1EEE1D'})\n    check('concept', 'template<typename T> requires (!A<T> && B<T>) || C<T> D',\n          {2: 'I0EROntA1AIT1EEEA1BIT1EEEERA1CIT1EEE1D'})\n    check('concept', 'template<typename T> requires A<T> && requires B<T> && C<T> D',\n          {2: 'I0ERA1AIT1EEERA1BIT1EEERA1CIT1EEE1D'})\n    check('concept', 'template<typename T> requires A<T> && requires B<T> || C<T> D',\n          {2: 'I0ERA1AIT1EEERA1BIT1EEEOA1CIT1EEE1D'})\n    check('concept', 'template<typename T> requires A<T> && (requires B<T> || C<T>) D',\n          {2: 'I0ERA1AIT1EEERA1BIT1EEEOA1CIT1EEE1D'})\n    check('concept', 'template<typename T> requires (!A<T> && requires B<T>) || C<T> D',\n          {2: 'I0EROntA1AIT1EEERA1BIT1EEEERA1CIT1EEE1D'})\n"], "sample_320": ["def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\",\n            models.CASCADE,\n            limit_choices_to={\"field\": \"value\"},\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"field\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_db_constraint(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, db_constraint=False),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n", "def test_references_field_by_through_fields(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ManyToManyField(\"Other\", through_fields=[\"from\", \"to\"])\n    )\n    self.assertIs(operation.references_field(\"Model\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"from\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"to\", \"migrations\"), False)\n", "def test_references_field_by_through_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\"Other\", through_fields=[\"from\", \"to\"]),\n    )\n    self.assertIs(operation.references_field(\"Model\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"from\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), True)\n", "def test_alter_field_with_default_change(self):\n    \"\"\"\n    Tests the AlterField operation with changing default value.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_alflwd\")\n    # Test the state alteration\n    operation = migrations.AlterField(\n        \"Pony\", \"pink\", models.IntegerField(default=2)\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_alflwd\", new_state)\n    self.assertEqual(\n        project_state.models[\"test_alflwd\", \"pony\"].fields[\"pink\"].default,\n        1,\n    )\n    self.assertEqual(\n        new_state.models[\"test_alflwd\", \"pony\"].fields[\"pink\"].default,\n        2,\n    )\n    # Test the database alteration\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_alflwd\", editor, project_state, new_state)\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_alflwd\", editor, new_state, project_state)\n"], "sample_1157": ["def test_split_symbols_greek():\n    transformations = standard_transformations + \\\n                      (split_symbols, implicit_multiplication,)\n    theta = Symbol('theta')\n\n    assert parse_expr(\"theta\") == theta\n    assert parse_expr(\"theta\", transformations=transformations) == theta\n", "def test_split_symbols_underscore():\n    transformations = standard_transformations + \\\n                      (split_symbols, implicit_multiplication,)\n    x_y = Symbol('x_y')\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr(\"x_y\") == x_y\n    assert parse_expr(\"x_y\", transformations=transformations) == x*y\n", "def test_implicit_multiplication_with_functions():\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    f = Function('f')\n    g = Function('g')\n    assert parse_expr(\"f(x)g(x)\", transformations=transformations) == f(x)*g(x)\n", "def test_convert_xor_to_pow():\n    transformations = standard_transformations + (convert_xor,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"x^y\", transformations=transformations) == x**y\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    f = Lambda(x, x + 2)\n    assert parse_expr(\"lambda x: x + 2\", transformations=transformations) == f\n"], "sample_947": ["def test_cstruct(app):\n    text = \".. c:struct:: PyTypeObject\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"struct\",\n                domain=\"c\", objtype=\"struct\", noindex=False)\n\n    entry = _get_obj(app, 'PyTypeObject')\n    assert entry == ('index', 'c.PyTypeObject', 'struct')\n", "def test_build_duplicate_declaration(app, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"duplicate_declaration\")\n    assert len(ws) == 1\n    assert \"WARNING: Duplicate C declaration, also defined at index.rst:7.\" in ws[0]\n", "def test_noindexentry_multiple(app):\n    text = (\".. c:function:: void f()\\n\"\n            \".. c:function:: void g()\\n\"\n            \".. c:function:: void h()\\n\"\n            \"   :noindexentry:\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C function)', 'c.f', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C function)', 'c.g', '', None)])\n    assert_node(doctree[4], addnodes.index, entries=[])\n", "def test_build_duplicate_decl(app, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"duplicate-decl\")\n    assert len(ws) == 1\n    assert \"WARNING: Duplicate C declaration, also defined at duplicate-decl.rst:17.\" in ws[0]\n\n    # check that the last declaration was used\n    entries = extract_role_links(app, \"duplicate-decl.html\")\n    assert entries == [\n        ('c.duplicate_decl.g', 'g', 'g'),\n        ('c.duplicate_decl.f', 'f', 'f'),\n    ]\n", "def test_build_allow_pre_v3(app, warning):\n    text = \"\"\""], "sample_874": ["def test_get_support_mask():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(support, sel._get_support_mask())\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y, feature_names=feature_names)\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y, feature_names=feature_names)\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with input_features\n    feature_names_out_actual_with_input = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual_with_input)\n\n    # Check with mismatched input_features\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=feature_names[:-1])\n\n    # Check with unseen feature_names_in_\n    sel = StepSelector()\n    sel.fit(X, y)\n    with pytest.warns(UserWarning):\n        sel.get_feature_names_out(input_features=feature_names)\n", "def test_fit_transform():\n    sel = StepSelector()\n    Xt_actual = sel.fit_transform(X)\n    assert_array_equal(Xt, Xt_actual)\n    assert_array_equal(support, sel.get_support())\n\n    # Check dtype matches\n    assert np.int32 == sel.fit_transform(X.astype(np.int32)).dtype\n    assert np.float32 == sel.fit_transform(X.astype(np.float32)).dtype\n", "def test_fit_transform_with_different_steps():\n    steps = [1, 2, 3, 5, 10]\n    for step in steps:\n        sel = StepSelector(step=step)\n        Xt_actual = sel.fit_transform(X)\n        Xt_expected = X[:, ::step]\n        assert_array_equal(Xt_expected, Xt_actual)\n"], "sample_1005": ["def test_Quaternion_multiplication_printing():\n    q1 = Quaternion(x, y, z, t)\n    q2 = Quaternion(a, b, c, d)\n    assert latex(q1 * q2) == \"x a - y b - z c + t d + x b i + y a j + z d i - t c j + y c k + z b k - x d k\"\n", "def test_issue_14428():\n    # test case for issue #14428\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert latex(A & B) == r\"A \\wedge B\"\n    assert latex(A | B) == r\"A \\vee B\"\n", "def test_Quaternion_properties():\n    from sympy import I, J, K\n    q = Quaternion(x, y, z, t)\n    assert q.components == (x, y, z, t)\n    assert q.scalar == x\n    assert q.vector == (y, z, t)\n    assert q.conjugate() == Quaternion(x, -y, -z, -t)\n    assert q.norm() == x**2 + y**2 + z**2 + t**2\n    assert q.normalized() == Quaternion(x/sqrt(q.norm()), y/sqrt(q.norm()), z/sqrt(q.norm()), t/sqrt(q.norm()))\n    assert q.inverse() == Quaternion(x/q.norm(), -y/q.norm(), -z/q.norm(), -t/q.norm())\n    assert q * I == Quaternion(-y, x, t, -z)\n    assert q * J == Quaternion(-z, -t, x, y)\n    assert q * K == Quaternion(-t, z, -y, x)\n", "def test_Quaternion_operations():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert latex(q1 + q2) == \"6 + 8 i + 10 j + 12 k\"\n    assert latex(q1 - q2) == \"-4 - 4 i - 4 j - 4 k\"\n    assert latex(q1 * q2) == \"-60 + 12 i + 30 j + 24 k\"\n", "def test_issue_16057():\n    from sympy import Symbol, Eq, solve, latex\n    from sympy.abc import x\n\n    eq = Eq(x**2, 1)\n    sols = solve(eq)\n    assert latex(sols) == r'\\left [ -1, \\quad 1\\right ]'\n\n    eq = Eq(x**3, 1)\n    sols = solve(eq)\n    assert latex(sols) == r'\\left [ 1\\right ]'\n"], "sample_1153": ["def test_issue_14238_complex():\n    # doesn't cause recursion error\n    z = Symbol('z', complex=True)\n    assert Abs(z + Piecewise((0, re(z) > 0), (1 - z, True)))\n", "def test_complex_assumptions():\n    c = Symbol('c', complex=True)\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n\n    assert re(c).is_complex is False\n    assert im(c).is_complex is False\n\n    assert re(p).is_positive is True\n    assert im(p).is_zero is True\n\n    assert re(n).is_negative is True\n    assert im(n).is_zero is True\n", "def test_polar_lift():\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n    z = Symbol('z', polar=True)\n\n    assert polar_lift(1) == 1*exp_polar(0)\n    assert polar_lift(-1) == 1*exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(0) == 0\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(z) == z\n    assert polar_lift(p) == p\n    assert polar_lift(1 + I) == polar_lift(1 + I)\n    assert polar_lift(x + I) == x + polar_lift(I)\n    assert polar_lift(x*I) == x*polar_lift(I)\n    assert polar_lift(x*z) == x*z\n    assert polar_lift(x*p) == x*p\n", "def test_issue_15020():\n    from sympy import polar_lift, exp, I, pi\n    x = Symbol('x', real=True)\n    assert polar_lift(exp(I*x)) == exp(I*x)\n    assert polar_lift(exp(I*x)) != exp(polar_lift(I*x))\n    assert polar_lift(exp(I*pi)) == -1\n    assert polar_lift(exp(I*pi)) != exp(polar_lift(I*pi))\n", "def test_complex_functions():\n    x = Symbol('x', complex=True)\n    assert im(re(x)) == 0\n    assert re(im(x)) == 0\n    assert Abs(x) == sqrt(re(x)**2 + im(x)**2)\n    assert arg(x) == atan2(im(x), re(x))\n    assert conjugate(x) == re(x) - I*im(x)\n    assert exp(I*arg(x)) == x / Abs(x)\n    assert sqrt(x) == sqrt(Abs(x)) * exp(I*arg(x)/2)\n    assert abs(x) == Abs(x)\n    assert sign(x) == x / Abs(x)\n"], "sample_924": ["def test_build_domain_cpp_warn_unresolved_references(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"unresolved-references\")\n    assert len(ws) == 0\n", "def test_build_domain_cpp_user_defined_literals(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"user-defined-literals\")\n    assert len(ws) == 0\n", "def test_template_constraints():\n    check('class', 'template<typename T> requires A {key}B', {2: 'I_1A1B'})\n    check('class', 'template<typename T> requires A && B || C and D {key}E', {2: 'I_1EE'})\n    check('class', 'template<typename T> requires requires A {key}F', {2: 'I_1A1F'})\n    check('class', 'template<typename T> requires requires A && B || C and D {key}G', {2: 'I_1G'})\n", "def test_xref_consistency_for_any_role(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_build_domain_cpp_template_aliases(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"template-aliases\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:identifier reference target not found: typeAliasTemplate\" in ws[0]\n"], "sample_308": ["def test_format_function(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    # Test the convenience function\n    self.assertEqual(dateformat.format(my_birthday, 'Y-m-d'), '1979-07-08')\n    self.assertEqual(dateformat.format(my_birthday, 'H:i'), '22:00')\n    self.assertEqual(dateformat.time_format(my_birthday.time(), 'h:i A'), '10:00 PM')\n", "    def test_time_formats_with_zero_minutes(self):\n        my_birthday = datetime(1979, 7, 8, 22, 0)\n\n        self.assertEqual(dateformat.format(my_birthday, 'f'), '10')\n        self.assertEqual(dateformat.format(my_birthday, 'P'), '10 p.m.')\n", "def test_I_format_naive_datetime(self):\n    dt = datetime(2009, 5, 16, 5, 30, 30)\n    self.assertEqual(dateformat.format(dt, 'I'), '')\n", "def test_time_formats_with_non_en_locale(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    with translation.override('fr'):\n        self.assertEqual(dateformat.format(my_birthday, 'P'), '10 p.m.')  # French translation should not affect this format\n        self.assertEqual(dateformat.format(my_birthday, 's'), '00')\n        self.assertEqual(dateformat.format(my_birthday, 't'), '31')  # French translation should not affect this format\n        self.assertEqual(dateformat.format(my_birthday, 'w'), '0')  # French translation should not affect this format\n        self.assertEqual(dateformat.format(my_birthday, 'W'), '27')  # French translation should not affect this format\n        self.assertEqual(dateformat.format(my_birthday, 'y'), '79')\n        self.assertEqual(dateformat.format(my_birthday, 'Y'), '1979')\n        self.assertEqual(dateformat.format(my_birthday, 'z'), '189')  # French translation should not affect this format\n", "def test_ambiguous_timezone(self):\n    # Test the behavior of dateformat with a datetime object that is ambiguous in a timezone that has DST.\n    # This will ensure that an exception is not raised when trying to format the datetime object.\n    dt = datetime(2015, 11, 1, 1, 30, 0)\n    tz = get_fixed_timezone(180)  # Timezone is 3 hours ahead of UTC.\n    aware_dt = dt.replace(tzinfo=tz)\n\n    # The datetime object is ambiguous in the timezone because it occurs during the DST-to-standard time transition.\n    # The timezone information of this datetime object is correct, so it should not raise an exception.\n    self.assertEqual(dateformat.format(aware_dt, 'Z'), '10800')\n"], "sample_232": ["def test_key_transform_exact(self):\n    self.assertIs(NullableJSONModel.objects.filter(value__foo=KeyTransform('foo', 'value')).exists(), True)\n    self.assertIs(NullableJSONModel.objects.filter(value__foo=KeyTransform('bar', 'value')).exists(), False)\n", "def test_key_transform_exact(self):\n    self.assertIs(NullableJSONModel.objects.filter(value__foo=KeyTransform('foo', 'value')).exists(), True)\n", "def test_key_transform_lookup(self):\n    obj = NullableJSONModel.objects.create(value={'a': {'b': 1}})\n    self.assertEqual(NullableJSONModel.objects.filter(value__a__b=1).get(), obj)\n", "def test_key_transform_exact(self):\n    tests = [\n        (KeyTransform('foo', 'value'), 'bar'),\n        ('value__foo', KeyTransform('foo', KeyTransform('bax', 'value'))),\n        ('value__foo', KeyTransform('foo', KeyTransform('baz', KeyTransform('bax', 'value')))),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(**{lookup: value}),\n                [self.objs[7]],\n            )\n", "def test_key_transform_case_insensitive(self):\n    obj = NullableJSONModel.objects.create(value={'Foo': 'bAr'})\n    self.assertIs(NullableJSONModel.objects.filter(value__Foo__iexact='bar').exists(), True)\n    self.assertSequenceEqual(NullableJSONModel.objects.filter(value__Foo__iexact='BaR'), [obj])\n    self.assertIs(NullableJSONModel.objects.filter(value__Foo__icontains='ar').exists(), True)\n    self.assertIs(NullableJSONModel.objects.filter(value__Foo__istartswith='B').exists(), True)\n    self.assertIs(NullableJSONModel.objects.filter(value__Foo__iendswith='r').exists(), True)\n    self.assertIs(NullableJSONModel.objects.filter(value__Foo__iregex=r'^bAr$').exists(), True)\n"], "sample_610": ["def test_to_datetimeindex_invalid_calendar(calendar):\n    index = xr.cftime_range(\"2000\", periods=5, calendar=calendar)\n    with pytest.raises(ValueError, match=\"Invalid calendar\"):\n        index.to_datetimeindex()\n", "def test_cftimeindex_add_timedelta_array(index):\n    date_type = index.date_type\n    expected_dates = [\n        date_type(1, 1, 2),\n        date_type(1, 2, 3),\n        date_type(2, 1, 3),\n        date_type(2, 2, 4),\n    ]\n    expected = CFTimeIndex(expected_dates)\n    other = np.array([timedelta(days=1), timedelta(days=2), timedelta(days=2), timedelta(days=3)])\n    result = index + other\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n", "def test_infer_freq_no_freq():\n    # CFTimeIndex with no explicit frequency\n    indx = xr.CFTimeIndex([pd.Timestamp(\"2000-01-01\"), pd.Timestamp(\"2000-01-03\"), pd.Timestamp(\"2000-01-05\")])\n    assert xr.infer_freq(indx) == \"2D\"\n\n    # DataArray with no explicit frequency\n    da = xr.DataArray([1, 2, 3], coords=[indx], dims=[\"time\"])\n    assert xr.infer_freq(da) == \"2D\"\n", "def test_infer_freq_invalid_frequency():\n    indx = xr.cftime_range(\"1990-02-03\", periods=4, freq=\"M\")\n    with pytest.raises(ValueError, match=\"Only valid for monthly or daily frequencies\"):\n        xr.infer_freq(indx)\n", "def test_cftimeindex_repr_name_attribute():\n    \"\"\"Test that cftimeindex.__repr__ includes name attribute.\"\"\"\n    name = \"my_time\"\n    index = xr.cftime_range(start=\"2000\", periods=3, name=name)\n    repr_str = index.__repr__()\n    assert f\"'{name}'\" in repr_str\n"], "sample_455": ["def test_validate_expression_with_custom_error(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\", violation_error_message=\"Custom error message\", violation_error_code=\"custom_error_code\")\n    msg = \"Custom error message\"\n    with self.assertRaisesMessage(ValidationError, msg) as cm:\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    self.assertEqual(cm.exception.code, \"custom_error_code\")\n", "def test_validate_custom_error_expression(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_uniq\",\n        violation_error_message=\"Duplicate name\",\n        violation_error_code=\"duplicate_name\",\n    )\n    msg = \"Duplicate name\"\n    with self.assertRaisesMessage(ValidationError, msg) as cm:\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    self.assertEqual(cm.exception.code, \"duplicate_name\")\n", "def test_expression_with_opclasses(self):\n    msg = \"UniqueConstraint.opclasses cannot be used with expressions. Use django.contrib.postgres.indexes.OpClass() instead.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            Lower(\"field\"),\n            name=\"test_func_opclass\",\n            opclasses=[\"jsonb_path_ops\"],\n        )\n", "def test_validate_expression_exclude(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_uniq\",\n        condition=models.Q(color__isnull=False),\n    )\n    non_unique_product = UniqueConstraintProduct(name=self.p2.name.upper(), color=\"red\")\n    msg = \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    # Field from a condition is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"color\"},\n    )\n", "def test_validate_multiple_conditions(self):\n    UniqueConstraintConditionProduct.objects.create(name=\"p1\", color=\"red\", size=\"small\")\n    constraint = UniqueConstraintConditionProduct._meta.constraints[1]\n    msg = \"Constraint \u201cname_size_without_color_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintConditionProduct,\n            UniqueConstraintConditionProduct(name=\"p1\", color=None, size=\"small\"),\n        )\n    # Values not matching both conditions are ignored.\n    constraint.validate(\n        UniqueConstraintConditionProduct,\n        UniqueConstraintConditionProduct(name=\"p1\", color=\"blue\", size=\"large\"),\n    )\n    # Unique fields are excluded.\n    constraint.validate(\n        UniqueConstraintConditionProduct,\n        UniqueConstraintConditionProduct(name=\"p1\", color=None, size=\"small\"),\n        exclude={\"name\"},\n    )\n    constraint.validate(\n        UniqueConstraintConditionProduct,\n        UniqueConstraintConditionProduct(name=\"p1\", color=None, size=\"small\"),\n        exclude={\"color\"},\n    )\n    constraint.validate(\n        UniqueConstraintConditionProduct,\n        UniqueConstraintConditionProduct(name=\"p1\", color=None, size=\"small\"),\n        exclude={\"size\"},\n    )\n"], "sample_576": ["def test_single_layer_multi_variable_with_none(self, xy):\n\n    s1 = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s1\")\n    s2 = pd.Series([\"m\", None, \"p\", \"m\"], name=\"s2\")\n    sem = dict(color=s1, marker=s2)\n    p = Plot(**xy).add(MockMark(), **sem).plot()\n    e1, e2 = p._legend_contents\n\n    variables = {v.name: k for k, v in sem.items()}\n\n    for e, s in zip([e1, e2], [s1, s2]):\n        assert e[0] == (s.name, s.name)\n\n        labels = categorical_order(s.dropna())\n        assert e[-1] == labels\n\n        artists = e[1]\n        assert len(artists) == len(labels)\n        for a, label in zip(artists, labels):\n            assert isinstance(a, mpl.artist.Artist)\n            assert a.value == label\n            assert a.variables == [variables[s.name]]\n", "def test_legend_with_custom_labels(self, xy):\n\n    s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n    labels = {\"a\": \"Apple\", \"b\": \"Banana\", \"c\": \"Cherry\"}\n    p = Plot(**xy).add(MockMark(), color=s).label(color=labels).plot()\n    e, = p._legend_contents\n\n    assert e[0] == (s.name, s.name)\n    assert e[-1] == [\"Apple\", \"Banana\", \"Cherry\"]\n\n    artists = e[1]\n    assert len(artists) == len(labels)\n    for a, label in zip(artists, labels.values()):\n        assert isinstance(a, mpl.artist.Artist)\n        assert a.value in labels\n        assert a.variables == [\"color\"]\n", "def test_computed_coordinate_legend(self, long_df):\n\n    class MockComputeStat(Stat):\n            other = {\"x\": \"y\", \"y\": \"x\"}[orient]\n            return df.assign(**{other: df[orient] * 2})\n\n    m = MockMark()\n    p = Plot(long_df, y=\"y\").add(m, MockComputeStat()).plot()\n    legend = p._figure.legends[0]\n    expected_values = list(long_df[\"y\"].unique() * 2)\n    legend_values = [float(t.get_text()) for t in legend.get_texts()]\n    assert legend_values == expected_values\n", "def test_legend_with_datetime(self, long_df):\n\n    s = pd.Series(pd.date_range(start='2022-01-01', periods=len(long_df)))\n    p = Plot(long_df, x=s, y=\"z\").add(MockMark(), color=\"a\").plot()\n    e, = p._legend_contents\n\n    labels = categorical_order(long_df[\"a\"])\n\n    assert e[0] == (\"a\", \"a\")\n    assert e[-1] == labels\n\n    artists = e[1]\n    assert len(artists) == len(labels)\n    for a, label in zip(artists, labels):\n        assert isinstance(a, mpl.artist.Artist)\n        assert a.value == label\n        assert a.variables == [\"color\"]\n", "def test_multiple_identical_variables_in_layers(self, long_df):\n    s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n    p = (\n        Plot(**long_df, color=s)\n        .add(MockMark(), color=s)\n        .add(MockMark(), color=s)\n        .plot()\n    )\n    assert len(p._legend_contents) == 1\n    assert len(p._figure.legends[0].get_texts()) == len(s.unique())\n"], "sample_724": ["def test_imputation_with_different_missing_values():\n    # Test imputation with different missing values.\n    X = np.array([\n        [1, np.nan, 3],\n        [4, 5, np.inf],\n        [7, -np.inf, 9],\n        [10, 11, np.NINF],\n    ])\n\n    X_true_nan = np.array([\n        [1, 5, 3],\n        [4, 5, 6],\n        [7, 5, 9],\n        [10, 11, 5],\n    ])\n\n    X_true_inf = np.array([\n        [1, np.nan, 3],\n        [4, 5, 6],\n        [7, 5, 9],\n        [10, 11, 5],\n    ])\n\n    _check_statistics(X, X_true_nan, \"mean\", [1, 5, 6], np.nan)\n    _check_statistics(X, X_true_inf, \"mean\", [1, 5, 6], np.inf)\n", "def test_imputation_dense_with_nans():\n    # Test imputation with dense arrays containing NaNs.\n    X = np.array([\n        [np.nan, 1, np.nan],\n        [2, np.nan, 3],\n        [np.nan, 5, 6],\n    ])\n\n    X_imputed_mean = np.array([\n        [3, 1, 4.5],\n        [2, 3, 3],\n        [3, 5, 6],\n    ])\n    statistics_mean = [3, 3, 4.5]\n\n    X_imputed_median = np.array([\n        [2, 1, 6],\n        [2, 3, 3],\n        [2, 5, 6],\n    ])\n    statistics_median = [2, 3, 6]\n\n    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, 'NaN')\n    _check_statistics(X, X_imputed_median, \"median\", statistics_median, 'NaN')\n", "def test_imputation_invalid_input():\n    # Test imputation with invalid input.\n    X = np.array([\n        [1, 2, 3],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n\n    # Test with invalid strategy\n    imputer = Imputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test with invalid axis\n    imputer = Imputer(axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test with mismatched shape in transform\n    imputer = Imputer()\n    imputer.fit(X)\n    X_mismatched_shape = np.array([[1, 2]])\n    assert_raises(ValueError, imputer.transform, X_mismatched_shape)\n\n    # Test with all missing values in a row when axis=1\n    X_all_missing = np.array([\n        [np.nan, np.nan, np.nan],\n        [4, 5, 6]\n    ])\n    imputer = Imputer(axis=1)\n    imputer.fit(X_all_missing)\n    assert_raises(ValueError, imputer.transform, X_all_missing)\n", "def test_imputation_copy_parameter():\n    # Test the copy parameter in the transform method\n    X = np.array([\n        [1, np.nan, 3],\n        [4, np.nan, 6],\n        [7, 8, 9],\n    ])\n\n    X_true = np.array([\n        [1, 8, 3],\n        [4, 8, 6],\n        [7, 8, 9],\n    ])\n\n    # Copy is True, by default\n    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n    imputer.fit(X)\n    X_trans = imputer.transform(X.copy())\n    assert_array_equal(X_trans, X_true)\n    assert_false(np.may_share_memory(X, X_trans))\n\n    # Copy is False\n    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0, copy=False)\n    imputer.fit(X)\n    X_trans = imputer.transform(X.copy())\n    assert_array_equal(X_trans, X_true)\n    assert np.may_share_memory(X, X_trans)\n", "def test_imputation_with_different_dtypes():\n    # Test imputation with different data types.\n    X_int = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]], dtype=int)\n    X_float = np.array([[1.0, 2.0, np.nan], [4.0, np.nan, 6.0], [7.0, 8.0, 9.0]], dtype=float)\n\n    X_int_true = np.array([[1, 2, 6], [4, 6, 6], [7, 8, 9]], dtype=int)\n    X_float_true = np.array([[1.0, 2.0, 6.0], [4.0, 6.0, 6.0], [7.0, 8.0, 9.0]], dtype=float)\n\n    statistics_int = [np.nan, 2, 6]\n    statistics_float = [np.nan, 2.0, 6.0]\n\n    _check_statistics(X_int, X_int_true, \"mean\", statistics_int, np.nan)\n    _check_statistics(X_float, X_float_true, \"mean\", statistics_float, np.nan)\n"], "sample_242": ["def test_year_lookup_bounds(self):\n    lookup = YearLookup(Value(datetime(2022, 1, 1)), Value(2022))\n    connection = mock.MagicMock()\n    connection.ops.year_lookup_bounds_for_datetime_field.return_value = (datetime(2022, 1, 1), datetime(2022, 12, 31, 23, 59, 59, 999999))\n    connection.ops.year_lookup_bounds_for_date_field.return_value = (datetime(2022, 1, 1), datetime(2022, 12, 31))\n\n    # Test with DateTimeField\n    lookup.lhs.lhs.output_field = DateTimeField()\n    self.assertEqual(\n        lookup.year_lookup_bounds(connection, 2022),\n        (datetime(2022, 1, 1), datetime(2022, 12, 31, 23, 59, 59, 999999))\n    )\n\n    # Test with non-DateTimeField\n    lookup.lhs.lhs.output_field = mock.MagicMock()\n    self.assertEqual(\n        lookup.year_lookup_bounds(connection, 2022),\n        (datetime(2022, 1, 1), datetime(2022, 12, 31))\n    )\n", "    def test_year_lookup_bounds(self):\n        lookup = YearLookup(DateTimeField(), 2022)\n        with mock.patch('django.db.backends.utils.ConnectionWrapper.ops') as mock_ops:\n            mock_ops.year_lookup_bounds_for_datetime_field.return_value = (datetime(2022, 1, 1), datetime(2022, 12, 31))\n            bounds = lookup.year_lookup_bounds(mock_ops, 2022)\n            mock_ops.year_lookup_bounds_for_datetime_field.assert_called_once_with(2022)\n            self.assertEqual(bounds, (datetime(2022, 1, 1), datetime(2022, 12, 31)))\n", "    def test_year_lookup(self):\n        lookup = YearLookup(Value(2000), Value(2001))\n        self.assertEqual(lookup.year_lookup_bounds(mock.MagicMock(), 2000),\n                         mock.ANY)\n        self.assertEqual(lookup.get_bound_params(datetime(2000, 1, 1), datetime(2000, 12, 31)),\n                         mock.ANY)\n", "def test_year_lookup_bounds(self):\n    year_lookup = YearLookup()\n\n    # Test with DateTimeField\n    connection = mock.Mock()\n    connection.ops.year_lookup_bounds_for_datetime_field.return_value = ('start_datetime', 'finish_datetime')\n    year_lookup.lhs = mock.Mock()\n    year_lookup.lhs.lhs = mock.Mock()\n    year_lookup.lhs.lhs.output_field = DateTimeField()\n\n    bounds = year_lookup.year_lookup_bounds(connection, 2022)\n    self.assertEqual(bounds, ('start_datetime', 'finish_datetime'))\n    connection.ops.year_lookup_bounds_for_datetime_field.assert_called_once_with(2022)\n\n    # Test with non-DateTimeField\n    connection.ops.year_lookup_bounds_for_date_field.return_value = ('start_date', 'finish_date')\n    year_lookup.lhs.lhs.output_field = mock.Mock()\n\n    bounds = year_lookup.year_lookup_bounds(connection, 2022)\n    self.assertEqual(bounds, ('start_date', 'finish_date'))\n    connection.ops.year_lookup_bounds_for_date_field.assert_called_once_with(2022)\n", "def test_year_lookup_bounds(self):\n    lookup = YearLookup(Value(datetime(2020, 1, 1)), Value(2020))\n    with self.settings(USE_TZ=False):\n        start, end = lookup.year_lookup_bounds(None, 2020)\n        self.assertEqual(start, datetime(2020, 1, 1))\n        self.assertEqual(end, datetime(2020, 12, 31, 23, 59, 59, 999999))\n\n    with self.settings(USE_TZ=True):\n        start, end = lookup.year_lookup_bounds(None, 2020)\n        self.assertEqual(start, datetime(2020, 1, 1, tzinfo=DateTimeField().tzinfo))\n        self.assertEqual(end, datetime(2020, 12, 31, 23, 59, 59, 999999, tzinfo=DateTimeField().tzinfo))\n"], "sample_842": ["def test_kernel_diagonal_values():\n    # Test that the diagonal values of the kernel are always 1.0\n    for kernel in kernels:\n        K = kernel(X)\n        assert_array_almost_equal(np.diag(K), np.ones(X.shape[0]))\n", "def test_kernel_operator_associative():\n    # Adding kernels and multiplying kernels should be associative.\n    # Check addition\n    assert_almost_equal((RBF(2.0) + RBF(3.0) + RBF(4.0))(X),\n                        (RBF(2.0) + (RBF(3.0) + RBF(4.0)))(X))\n\n    # Check multiplication\n    assert_almost_equal((RBF(2.0) * RBF(3.0) * RBF(4.0))(X),\n                        (RBF(2.0) * (RBF(3.0) * RBF(4.0)))(X))\n", "def test_kernel_diagonal_values(kernel):\n    # Test that the diagonal values of the kernel matrix are positive.\n    K = kernel(X)\n    assert np.all(np.diag(K) >= 0)\n", "def test_kernel_hyperparameters(kernel):\n    # Test if kernel hyperparameters are correctly set and retrieved\n    params = kernel.get_params()\n    for hyperparameter in kernel.hyperparameters:\n        param_value = params[hyperparameter.name]\n        assert np.isscalar(param_value) or isinstance(param_value, list)\n        if isinstance(param_value, list):\n            assert len(param_value) == hyperparameter.n_elements\n", "def test_kernel_hyperparameters_bounds():\n    # Test that setting hyperparameters with bounds works correctly.\n    kernel = RBF(length_scale=2.0, length_scale_bounds=(0.5, 3.0))\n    kernel.set_params(length_scale=1.0)\n    assert kernel.length_scale == 1.0\n    kernel.set_params(length_scale=4.0)\n    assert kernel.length_scale == 3.0  # Test that it clips at the upper bound\n    kernel.set_params(length_scale=0.1)\n    assert kernel.length_scale == 0.5  # Test that it clips at the lower bound\n"], "sample_1026": ["def test_lambdify_with_scipy_special():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    # Testing the use of scipy.special functions in lambdify\n    f = lambdify(x, scipy.special.jv(0, x), modules=\"scipy\")\n    assert abs(f(3.0) - scipy.special.jv(0, 3.0)) < 1e-15\n", "def test_numpy_complex_args():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    expr = Abs(x)\n    func = lambdify(x, expr, modules=\"numpy\")\n    a = numpy.array([3+4j, 1-2j])\n    numpy.testing.assert_array_equal(func(a), numpy.array([5, numpy.sqrt(5)]))\n", "def test_lambdify_with_sympy_function():\n    f = Function('f')\n    assert lambdify(x, f(x))(2) == f(2)\n    assert lambdify(x, f(x))(2.5) == f(2.5)\n", "def test_numexpr_unpacking():\n    if not numexpr:\n        skip(\"numexpr not installed.\")\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    r = sqrt(x**2 + y**2)\n    expr = diff(1/r, x)\n\n    xn = yn = numpy.linspace(1, 10, 16)\n    # expr(xn, xn) = -xn/(sqrt(2)*xn)^3\n    fv_exact = -numpy.sqrt(2.)**-3 * xn**-2\n\n    fv_numexpr = lambdify((xn, yn), expr, modules='numexpr')\n    numpy.testing.assert_allclose(fv_numexpr, fv_exact, rtol=1e-10)\n", "def test_lambdify_with_non_python_identifiers():\n    # Test for issue 15331\n    from sympy.abc import x\n    from sympy import symbols\n    from sympy.utilities.lambdify import lambdastr\n\n    alpha = symbols('\\u03b1')\n    beta = symbols('beta\\u0301')\n    expr = alpha + beta\n\n    # Test with dummify\n    result = lambdastr([alpha, beta], expr, dummify=True)\n    assert result == 'lambda _0,_1: (_0 + _1)'\n\n    # Test without dummify\n    result = lambdastr([alpha, beta], expr, dummify=False)\n    assert result == 'lambda alpha,beta\\u0301: (alpha + beta\\u0301)'\n"], "sample_153": ["def test_model_prepared_on_save(self, mocked_prepare):\n    model_instance = Model()\n    model_instance.save()\n    mocked_prepare.assert_called_once()\n", "    def test_clean_fields(self):\n        # Assuming there is a model 'TestModel' with a field 'test_field'\n        # that has a clean method that raises a ValidationError\n        class TestModel(models.Model):\n            test_field = models.CharField(max_length=10)\n\n                super().clean_fields(exclude=exclude)\n                if self.test_field == \"raise_error\":\n                    raise ValidationError(\"Test error\")\n\n        test_model = TestModel(test_field=\"raise_error\")\n        with self.assertRaises(ValidationError):\n            test_model.clean_fields()\n", "def test_model_check_called(self, mocked_check):\n    from django.db import models\n\n    class TestModel(models.Model):\n        pass\n\n    models.Model.check(databases=self.databases)\n    self.assertTrue(mocked_check.called)\n", "def test_database_checks_errors(self, mocked_check):\n    mocked_check.return_value = [(1, 'Test error 1'), (2, 'Test error 2')]\n    errors = check_database_backends(databases=self.databases)\n    self.assertEqual(len(errors), 2)\n    self.assertEqual(errors[0], (1, 'Test error 1', []))\n    self.assertEqual(errors[1], (2, 'Test error 2', []))\n", "def test_model_check_with_databases(self):\n    from django.db import models\n    from django.core import checks\n    from django.db.models.fields import Field\n\n    class TestModel(models.Model):\n        test_field = models.CharField(max_length=10)\n\n    databases = {'default', 'other'}\n\n    with mock.patch.object(Field, 'check', return_value=[checks.Error(\"Field error\")]):\n        errors = TestModel.check(databases=databases)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].msg, \"Field error\")\n"], "sample_1056": ["def test_custom_object():\n    obj = CustomPrintedObject()\n    assert lambdarepr(obj) == 'lambda'\n    assert TensorflowPrinter().doprint(obj) == 'tensorflow'\n    assert NumExprPrinter().doprint(obj) == 'numexpr'\n", "def test_custom_printed_object():\n    obj = CustomPrintedObject()\n    assert LambdaPrinter().doprint(obj) == 'lambda'\n    assert TensorflowPrinter().doprint(obj) == 'tensorflow'\n    assert NumExprPrinter().doprint(obj) == 'numexpr'\n    assert lambdify(x, obj)(1) == 'lambda'\n", "def test_numexpr_printer():\n    # Test the NumExprPrinter with a simple expression\n    expr = sin(x) + cos(y) + log(z)\n    printer = NumExprPrinter()\n    result = printer.doprint(expr)\n    assert result == \"evaluate('sin(x) + cos(y) + log(z)', truediv=True)\"\n\n    # Test the NumExprPrinter with a complex expression\n    expr = sqrt(x) + abs(y) + conjugate(z)\n    printer = NumExprPrinter()\n    result = printer.doprint(expr)\n    assert result == \"evaluate('sqrt(x) + abs(y) + conj(z)', truediv=True)\"\n\n    # Test the NumExprPrinter with a blacklisted function\n    expr = Matrix([[x, y], [y*x, z**2]])\n    printer = NumExprPrinter()\n    with raises(TypeError):\n        printer.doprint(expr)\n\n    # Test the NumExprPrinter with a custom printed object\n    obj = CustomPrintedObject()\n    printer = NumExprPrinter()\n    result = printer.doprint(obj)\n    assert result == \"evaluate('numexpr', truediv=True)\"\n", "def test_numexpr_printer():\n    # Test printing a function that is supported by NumExprPrinter\n    expr = sin(x) + cos(y) + sqrt(z)\n    l = lambdarepr(expr, method=\"numexpr\")\n    assert l == \"evaluate('sin(x) + cos(y) + sqrt(z)', truediv=True)\"\n\n    # Test printing a function that is not supported by NumExprPrinter\n    expr = log(x)\n    with raises(TypeError):\n        l = lambdarepr(expr, method=\"numexpr\")\n", "def test_custom_printed_object():\n    obj = CustomPrintedObject()\n    assert lambdarepr(obj) == \"lambda\"\n    assert TensorflowPrinter().doprint(obj) == \"tensorflow\"\n    assert NumExprPrinter().doprint(obj) == \"numexpr\"\n    assert lambdarepr(obj, method=\"numpy\") == \"numpy\"\n    assert lambdarepr(obj, method=\"mpmath\") == \"mpmath\"\n"], "sample_1076": ["def test_PythonCodePrinter_Rational():\n    prntr = PythonCodePrinter({'standard': 'python2'})\n    assert prntr.doprint(Rational(1, 2)) == '1./2.'\n\n    prntr = PythonCodePrinter({'standard': 'python3'})\n    assert prntr.doprint(Rational(1, 2)) == '1/2'\n", "def test_log1p_log2():\n    from sympy import log1p, log2\n\n    expr1 = log1p(x)\n    expr2 = log2(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.log1p(x)'\n    assert prntr.doprint(expr2) == 'numpy.log(x)/numpy.log(2)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log1p(x)'\n    assert prntr.doprint(expr2) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log1p(x)'\n    assert prntr.doprint(expr2) == 'math.log(x)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log1p(x)'\n    assert prntr.doprint(expr2) == 'mpmath.log(x)/mpmath.log(2)'\n", "def test_SymPyPrinter_print_Function():\n    from sympy import Function\n    f = Function('custom_func')\n    expr = f(x, y, z)\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'sympy.custom_func(x, y, z)'\n", "def test_printmethod_for_unknown_function():\n    prntr = PythonCodePrinter()\n    expr = CustomPrintedObject()\n    assert prntr.doprint(expr) == 'CustomPrintedObject()'\n", "def test_PythonCodePrinter_standard_python2():\n    prntr = PythonCodePrinter({'standard':'python2'})\n\n    assert prntr.doprint(print(\"Hello, World!\")) == 'print \"Hello, World!\"'\n"], "sample_1057": ["def test_render_as_module():\n    from sympy import symbols\n\n    x, y = symbols('x y')\n    expr = x + y\n\n    expected_output = \"from sympy import symbols, Add\\n\\n\"\n    expected_output += \"x, y = symbols('x y')\\n\"\n    expected_output += \"expr = Add(x, y)\\n\"\n\n    assert render_as_module(expr) == expected_output\n", "def test_render_as_module():\n    from sympy import symbols, sin\n    x = symbols('x')\n    content = Print(sin(x))\n    result = render_as_module(content)\n    expected_result = \"from sympy import sin\\n\\nprint(sin(x))\"\n    assert result == expected_result\n", "def test_render_as_module():\n    x, y = symbols('x y')\n    expr = x + y\n    rendered = render_as_module(expr)\n    assert \"from sympy import symbols\" in rendered\n    assert \"x + y\" in rendered\n", "def test_render_as_module():\n    from sympy import symbols\n\n    x, y = symbols('x y')\n    content = Print(x + y)\n    output = render_as_module(content)\n\n    assert 'from sympy import symbols' in output\n    assert 'print(x + y)' in output\n", "def test_render_as_module():\n    from sympy import symbols, sin\n    x = symbols('x')\n    content = Print(sin(x))\n    expected_output = \"from sympy import sin\\nx = symbols('x')\\nprint(sin(x))\\n\"\n    assert render_as_module(content) == expected_output\n"], "sample_196": ["def test_explain_query_prefix_raise_not_supported_error(self):\n    msg = 'This backend does not support explaining query execution.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        self.ops.explain_query_prefix()\n", "def test_explain_query_prefix_raise_not_supported_error(self):\n    msg = 'This backend does not support explaining query execution.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        self.ops.explain_query_prefix()\n", "    def test_combine_expression(self):\n        expressions = ['a=1', 'b=2', 'c=3']\n        expected_result = 'a=1 AND b=2 AND c=3'\n        self.assertEqual(self.ops.combine_expression('AND', expressions), expected_result)\n", "def test_explain_query_prefix_raise_not_supported_error(self):\n    msg = 'This backend does not support explaining query execution.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        self.ops.explain_query_prefix()\n", "def test_explain_query_prefix_not_supported(self):\n    msg = 'This backend does not support explaining query execution.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        self.ops.explain_query_prefix()\n"], "sample_1106": ["def test_matadd_doit():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[2, 3], [4, 5]])\n    assert MatAdd(A, B).doit() == ImmutableMatrix([[3, 5], [7, 9]])\n", "def test_matadd_identity():\n    assert MatAdd() == GenericZeroMatrix()\n    assert MatAdd(A) == A\n    assert MatAdd(A, GenericZeroMatrix()) == A\n    assert MatAdd(A, ZeroMatrix(n, m)) == A\n    assert MatAdd(ZeroMatrix(n, m), A) == A\n", "def test_matadd_construction():\n    assert MatAdd(A) == A\n    assert MatAdd(A, B) == A + B\n    assert MatAdd(A, B, C) == A + B + C\n    assert MatAdd(A, ZeroMatrix(n, m)) == A\n    assert MatAdd(A, ZeroMatrix(n, m), B) == A + B\n", "def test_MatAdd_construction():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n\n    # Test construction of MatAdd with different number of arguments\n    assert MatAdd() == GenericZeroMatrix()\n    assert MatAdd(A) == A\n    assert MatAdd(A, B) == A + B\n    assert MatAdd(A, B, C) == A + B + C\n\n    # Test construction of MatAdd with zero matrices\n    assert MatAdd(A, ZeroMatrix(2, 2)) == A\n    assert MatAdd(A, ZeroMatrix(2, 2), B) == A + B\n\n    # Test construction of MatAdd with evaluate=True\n    assert MatAdd(A, B, evaluate=True) == A + B\n    assert MatAdd(A, ZeroMatrix(2, 2), B, evaluate=True) == A + B\n", "def test_matadd_args():\n    assert MatAdd(A, B).args == (A, B)\n    assert MatAdd(A, B, C).args == (A, B, C)\n    assert MatAdd(A, B, C, evaluate=True).args != (A, B, C)\n    assert isinstance(MatAdd(A, B, C, evaluate=True), Matrix)\n"], "sample_1088": ["def test_viete():\n    f = a*x**2 + b*x + c\n    roots = [r1, r2]\n    assert viete(f, roots, x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n\n    f = a*x**3 + b*x**2 + c*x + d\n    roots = [r1, r2, r3]\n    assert viete(f, roots, x) == [(r1 + r2 + r3, -b/a), (r1*r2 + r1*r3 + r2*r3, c/a), (r1*r2*r3, -d/a)]\n\n    with raises(ValueError):\n        viete(1, roots, x)\n\n    with raises(MultivariatePolynomialError):\n        viete(x**2 + y**2, roots, x, y)\n", "def test_viete():\n    f = a*x**2 + b*x + c\n    roots = [r1, r2]\n    result = viete(f, roots, x)\n    assert result == [(r1 + r2, -b/a), (r1*r2, c/a)]\n\n    f = x**3 - 6*x**2 + 11*x - 6\n    roots = [r1, r2, r3]\n    result = viete(f, roots, x)\n    assert result == [(r1 + r2 + r3, -6), (r1*r2 + r1*r3 + r2*r3, 11), (r1*r2*r3, -6)]\n\n    with raises(ValueError):\n        viete(1, roots, x)\n\n    with raises(MultivariatePolynomialError):\n        viete(x*y + 1, roots, x, y)\n", "def test_viete():\n    assert viete(x**2 + x + 1, [x - 1, x + 1]) == [(x - 1 + x + 1, -1), (x - 1 * (x + 1), 1)]\n    assert viete(x**3 - x**2 - x + 1, [x - 1, 1, 1 - x]) == [(x - 1 + 1 + 1 - x, 1), (x - 1 * 1 + x - 1 * (1 - x), -1), (x - 1 * 1 * (1 - x), -1)]\n\n    raises(MultivariatePolynomialError, lambda: viete(x**2 + y**2, [x - 1, x + 1]))\n    raises(ValueError, lambda: viete(1, [x - 1, x + 1]))\n    raises(ValueError, lambda: viete(x**2 + x + 1, [x - 1]))\n", "def test_viete():\n    assert viete(x**2 - 2, [a, b]) == [(a + b, 2), (a*b, -2)]\n    assert viete(x**3 - 6*x**2 + 11*x - 6, [a, b, c]) == [(a + b + c, -6), (a*b + b*c + a*c, 11), (a*b*c, -6)]\n\n    raises(ValueError, lambda: viete(1, [a]))\n    raises(ValueError, lambda: viete(x, [a, b]))\n    raises(MultivariatePolynomialError, lambda: viete(x*y + x))\n    raises(ValueError, lambda: viete(x**2, [a, b, c]))\n", "def test_viete():\n    f = a*x**2 + b*x + c\n    roots = [r1, r2]\n    result = viete(f, roots, x)\n    assert result == [(r1 + r2, -b/a), (r1*r2, c/a)]\n\n    f = a*x**3 + b*x**2 + c*x + d\n    roots = [r1, r2, r3]\n    result = viete(f, roots, x)\n    assert result == [(r1 + r2 + r3, -b/a), (r1*r2 + r1*r3 + r2*r3, c/a), (r1*r2*r3, -d/a)]\n\n    f = x**4 - 2*x**3 + 3*x**2 - 4*x + 5\n    roots = symbols('r1:5')\n    result = viete(f, roots, x)\n    assert result == [(r1 + r2 + r3 + r4, 2), (r1*r2 + r1*r3 + r1*r4 + r2*r3 + r2*r4 + r3*r4, -3),\n                     (r1*r2*r3 + r1*r2*r4 + r1*r3*r4 + r2*r3*r4, 4), (r1*r2*r3*r4, -5)]\n\n    f = a*x**2 + b*x + c\n    with raises(ValueError):\n        viete(f, x)\n\n    f = a*x*y + b*x + c*y + d\n    with raises(MultivariatePolynomialError):\n        viete(f, [r1, r2], x, y)\n\n    f = 1\n    with raises(ValueError):\n        viete(f, [r1], x)\n"], "sample_1068": ["def test_DiracDelta_printing():\n    assert octave_code(DiracDelta(x)) == 'dirac(x)'\n    assert octave_code(DiracDelta(x, y)) == 'dirac(y, x)'\n", "def test_DiracDelta_printing():\n    assert octave_code(DiracDelta(x)) == 'dirac(x)'\n    assert octave_code(DiracDelta(x, y)) == 'dirac(y, x)'\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 5, 5)\n\n    assert mcode(A[:2, :2]) == \"A(1:2, 1:2)\"\n    assert mcode(A[2:, 2:]) == \"A(3:end, 3:end)\"\n    assert mcode(A[1:3:2, 1:3:2]) == \"A(2:2:3, 2:2:3)\"\n    assert mcode(A[::2, ::2]) == \"A(1:2:end, 1:2:end)\"\n", "def test_LambertW_printing():\n    assert octave_code(LambertW(x)) == 'lambertw(x)'\n    assert octave_code(LambertW(x, n)) == 'lambertw(n, x)'\n", "def test_user_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n      \"f\": \"existing_octave_fcn\",\n      \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n            (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        'existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n"], "sample_973": ["def test_is_builtin_class_method_custom_int():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is False\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'args'\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (1, 2)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwonlydefaults == None\n    assert argspec.annotations == {}\n", "def test_is_builtin_class_method_custom_class():\n    class MyCustomClass:\n            pass\n\n    assert inspect.is_builtin_class_method(MyCustomClass, 'my_method') is False\n", "def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n    assert inspect.is_builtin_class_method(int, '__add__') is True\n    assert inspect.is_builtin_class_method(int, 'my_method') is False\n", "def test_getdoc_partial_function():\n        \"\"\"\n        docstring\n            indented text\n        \"\"\"\n        pass\n\n    pfunc = functools.partial(func, 1)\n\n    assert inspect.getdoc(pfunc) == func.__doc__\n"], "sample_1154": ["def test__linsolve_complex():\n    eqs = [\n        x + I*y - 1,\n        x - I*y - 1\n    ]\n    sol = {x: 0.5, y: 0.5*I}\n    assert _linsolve(eqs, (x, y)) == sol\n", "def test__linsolve_complex():\n    eqs = [\n        Eq(x + I*y, 1 + I),\n        Eq(x - I*y, 1 - I)\n    ]\n    sol = {x: 1, y: 1}\n    assert _linsolve(eqs, (x, y)) == sol\n", "def test__linsolve_underdetermined():\n    eqs = [x + y]\n    sol = {x: -y, y: y}\n    assert _linsolve(eqs, (x, y)) == sol\n", "def test__linsolve_large():\n    # Test with a large system of equations\n    # This will test the efficiency and accuracy of the linsolve function\n    eqs = [Eq(x + y + z, 1), Eq(2*x + y - z, 2), Eq(x - y + 2*z, 3)]\n    sol = _linsolve(eqs, (x, y, z))\n    assert sol == {x: S(2)/3, y: S(2)/3, z: S(1)/3}\n", "def test__linsolve_complex():\n\n    # Test system with complex coefficients\n    eqs = [\n        x + I*y - 1,\n        I*x + y - 2*I\n    ]\n    sol = {x: S(1)/2, y: S(1)/2*I}\n    assert _linsolve(eqs, (x, y)) == sol\n"], "sample_1119": ["def test_matpow():\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, 2) == C*C\n    assert MatPow(C, n) == C**n\n    assert MatPow(C, -n) == Inverse(C**n)\n    assert MatPow(C, n*m) == (C**n)**m\n    assert MatPow(C, n/m) == C**(n/m) if m.is_integer else None\n", "def test_matrix_pow():\n    assert MatPow(C, 2) == C*C\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, 0) == Identity(C.rows)\n\n    assert MatPow(C, n).doit(deep=False) == MatPow(C, n)\n    assert MatPow(C, n).doit(inv_expand=False) == MatPow(C, n)\n\n    assert MatPow(C, n).doit(deep=True, inv_expand=True) == C**n\n", "def test_matpow():\n    assert MatPow(C, 2) == C*C\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, 0) == Identity(n)\n\n    assert MatPow(C, n).doit() == C**n\n    assert MatPow(C, n).doit(deep=False) == C**n\n\n    assert MatPow(C, -n).doit() == Inverse(C**n)\n    assert MatPow(C, -n).doit(deep=False) == Inverse(C**n)\n\n    assert MatPow(C, 1/n).doit() == C**(1/n)\n    assert MatPow(C, 1/n).doit(deep=False) == C**(1/n)\n", "def test_matpow():\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, 2) == C*C\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, -2) == Inverse(C*C)\n\n    assert MatPow(C, n).doit(inv_expand=False) == MatPow(C, n)\n    assert MatPow(C, n).doit(inv_expand=True) == C**n\n\n    assert MatPow(C, n).inverse() == MatPow(C, -n)\n    assert MatPow(C, n).inverse().doit() == C**(-n)\n\n    assert MatPow(C, n).inverse().inverse() == MatPow(C, n)\n", "def test_inverse_properties():\n    assert (Inverse(C)*C).simplify() == Identity(n)\n    assert (C*Inverse(C)).simplify() == Identity(n)\n    assert (Inverse(C)*C*C).simplify() == Inverse(C)\n    assert (C*C*Inverse(C)).simplify() == C\n\n    assert (Inverse(C)*D).is_Matrix\n    assert (C*Inverse(D)).is_Matrix\n    assert not (Inverse(C)*D).is_Identity\n    assert not (C*Inverse(D)).is_Identity\n\n    assert Inverse(C**2).doit() == (C**-2)\n"], "sample_1036": ["def test_matmul_numerical_evaluation():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    assert MatMul(A, B).doit().evalf() == ImmutableMatrix([[19, 22], [43, 50]])\n", "def test_matmul_with_symbol():\n    x = Symbol('x')\n    M = Matrix([[x]])\n    A = MatrixSymbol('A', 1, 1)\n    assert MatMul(M, A).doit() == M*A\n    assert MatMul(A, M).doit() == A*M\n", "def test_matmul_zero_matrix():\n    assert MatMul(C, ZeroMatrix(n, n)).doit() == ZeroMatrix(n, n)\n    assert MatMul(ZeroMatrix(n, n), C).doit() == ZeroMatrix(n, n)\n    assert MatMul(ZeroMatrix(n, m), ZeroMatrix(m, n)).doit() == ZeroMatrix(n, n)\n", "def test_matmul_commutativity():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    assert MatMul(A, B, evaluate=False).is_commutative == False\n    assert MatMul(A, B).is_commutative == False\n    assert MatMul(A.T, B, evaluate=False).is_commutative == False\n    assert MatMul(A.T, B).is_commutative == False\n    assert MatMul(B, A, evaluate=False).is_commutative == False\n    assert MatMul(B, A).is_commutative == False\n", "def test_matmul_distributive():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = Matrix([[9, 10], [11, 12]])\n    assert MatMul(A, B + C).doit() == MatMul(A, B) + MatMul(A, C)\n"], "sample_927": ["def test_xref_consistency_contents(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n    # check contents of each role\n    roles = ['any', 'class', 'struct', 'union', 'func', 'member', 'var', 'type', 'concept', 'enum', 'enumerator']\n    for role in roles:\n        classes = RoleClasses(role, 'a', 'span')\n        assert 'xref' in classes.classes\n        assert 'cpp' in classes.classes\n        assert 'cpp-' + role in classes.classes\n        assert 'cpp-' + role in classes.content_classes['span']\n", "def test_xref_consistency_next_test(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n    # Add a new test case for a different role or tag to improve coverage\n    new_role = 'type'\n    new_tag = 'a'\n\n    new_classes = classes(new_role, new_tag)\n    expect = '''\\", "def test_xref_consistency_function_parameter():\n    role = 'member'\n    root = 'span'\n    contents = ['em']\n    function_parameter_classes = RoleClasses(role, root, contents)\n\n    expected_classes = {\n        'xref', 'cpp', 'cpp-member', 'cpp-parameter'\n    }\n    assert function_parameter_classes.classes == expected_classes, f\"Expected classes for role `{role}`: {expected_classes}, but got: {function_parameter_classes.classes}\"\n\n    expected_content_classes = {\n        'em': {'cpp-parameter-name'}\n    }\n    assert function_parameter_classes.content_classes == expected_content_classes, f\"Expected content classes for role `{role}`: {expected_content_classes}, but got: {function_parameter_classes.content_classes}\"\n", "def test_ref_consistency(app, status, warning):\n    app.builder.build_all()\n\n    test = 'ref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'<a .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'{target}'\n                   r'</a>').format(target=re.escape(target))\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_xref_consistency_with_duplicates(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency_with_duplicates.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        matches = re.findall(pattern, output)\n        assert len(matches) > 1, f\"Expected multiple instances of the '{role}' role in '{test}'\"\n        for match in matches:\n            assert set(match.split()) == expected_classes, \\\n                f\"Classes for '{role}' role in '{test}' do not match expected classes\"\n\n    check_duplicate_roles('func', 'code', {'xref', 'cpp', 'func'})\n"], "sample_588": ["def test_combine_nested_with_no_datasets(self):\n    actual = combine_nested([], concat_dim=\"x\")\n    expected = Dataset()\n    assert_identical(expected, actual)\n", "def test_auto_combine_concat_dim_explicitly_provided(self):\n    objs = [Dataset({\"x\": [0], \"y\": 1})]\n    dim = DataArray([100], name=\"baz\", dims=\"baz\")\n    with pytest.warns(FutureWarning, match=\"`concat_dim`\"):\n        auto_combine(objs, concat_dim=dim)\n", "def test_auto_combine_with_concat_dim_and_coords(self):\n    objs = [\n        Dataset({\"foo\": (\"x\", [0])}, coords={\"x\": (\"x\", [1])}),\n        Dataset({\"foo\": (\"x\", [1])}, coords={\"x\": (\"x\", [0])}),\n    ]\n    with pytest.warns(FutureWarning, match=\"supplied have global\"):\n        auto_combine(objs, concat_dim=\"x\")\n", "def test_auto_combine_with_existing_coords(self):\n    objs = [\n        Dataset({\"foo\": (\"x\", [0])}, coords={\"x\": (\"x\", [1])}),\n        Dataset({\"foo\": (\"x\", [1])}, coords={\"x\": (\"x\", [0])}),\n    ]\n    with pytest.warns(FutureWarning, match=\"supplied have global\"):\n        auto_combine(objs)\n", "def test_auto_combine_compat(compat):\n    objs = [Dataset({\"x\": [0], \"y\": [0]}), Dataset({\"x\": [1], \"y\": [1]})]\n    actual = auto_combine(objs, concat_dim=\"x\", compat=compat)\n    expected = Dataset({\"x\": [0, 1], \"y\": [0, 1]})\n    assert_identical(expected, actual)\n\n    # Test when data variables are not compatible\n    objs = [Dataset({\"x\": [0], \"y\": [0]}), Dataset({\"x\": [1], \"y\": [2]})]\n    if compat == \"equals\":\n        with pytest.raises(ValueError):\n            auto_combine(objs, concat_dim=\"x\", compat=compat)\n    else:\n        actual = auto_combine(objs, concat_dim=\"x\", compat=compat)\n        if compat == \"override\":\n            expected = Dataset({\"x\": [0, 1], \"y\": [0, 2]})\n        elif compat == \"no_conflicts\":\n            expected = Dataset({\"x\": [0, 1], \"y\": [0, np.nan]})\n        elif compat == \"broadcast_equals\":\n            expected = Dataset({\"x\": [0, 1], \"y\": [0, 2]})\n        assert_identical(expected, actual)\n"], "sample_430": ["def test_alter_index_together_fk_to_m2m(self):\n    changes = self.get_changes(\n        [self.author_empty, self.book_index_together],\n        [\n            self.author_empty,\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ManyToManyField(\"testapp.Author\")),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"AlterIndexTogether\", \"RemoveField\", \"AddField\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\", index_together=set()\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"author\"\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 2, model_name=\"book\", name=\"author\"\n    )\n", "    def test_rename_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameModel(\"OldModelName\", \"NewModelName\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"rename_oldmodelname_to_newmodelname\")\n", "compilation error", "def test_index_together_and_indexes(self):\n    # Indexes with same fields as index_together don't generate a migration.\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, self.book_index_together],\n        [AutodetectorTests.author_empty, AutodetectorTests.book_indexes],\n    )\n    self.assertEqual(len(changes), 0)\n", "def test_rename_index_together_to_index_extra_options(self):\n    # Indexes with extra options don't match indexes in index_together.\n    book_partial_index = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(\n                    fields=[\"author\", \"title\"],\n                    condition=models.Q(title__startswith=\"The\"),\n                    name=\"book_title_author_idx\",\n                )\n            ],\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, self.book_index_together],\n        [AutodetectorTests.author_empty, book_partial_index],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"AlterIndexTogether\", \"AddIndex\"],\n    )\n"], "sample_959": ["def test_domain_cpp_parse_noindexentry_with_argument(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \"   :noindexentry: Custom Entry\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'Custom Entry', '_CPPv41gv', '', None)])\n", "def test_domain_cpp_parse_noindexentry_for_class(app):\n    text = (\".. cpp:class:: MyClass\\n\"\n            \"   :noindexentry:\\n\"\n            \"   .. cpp:function:: void f()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index))\n    assert_node(doctree[0], addnodes.index, entries=[])\n    assert_node(doctree[1], desc, noindex=True)\n    assert_node(doctree[1][1], addnodes.index, entries=[])\n", "def test_domain_cpp_build_template_param_in_function_param(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    rolePatterns = [\n        ('', 'f<TParam>'),\n        ('', 'f<TParam>(param)'),\n        ('', 'f<TParam>(param = 42)'),\n        ('', 'f<TParam>(param = std::declval<U>())'),\n        ('', 'f<TParam>(param = TParam())'),\n        ('', 'f<TParam>(TParam param)'),\n        ('', 'f<TParam>(TParam param = 42)'),\n        ('', 'f<TParam>(TParam param = std::declval<U>())'),\n        ('', 'f<TParam>(TParam param = TParam())'),\n    ]\n\n    f = 'template-param-in-function-param.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n", "def test_domain_cpp_ast_function_definitions_multiple_params_with_default():\n    check('function', 'void f(int a = 1, float b = 2.0, std::string c = \"hello\")',\n          {1: \"f__i.f.ss\", 2: \"3fifiNSt6stringE\"})\n", "def test_domain_cpp_ast_trailing_return_type():\n    check('function', 'auto f(int a) -> int', {1: 'f__i', 2: '1fi'})\n"], "sample_1118": ["def test_matpow():\n    assert MatPow(C, 2) == C*C\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, -1) == Inverse(C)\n\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n", "def test_matpow():\n    assert MatPow(C, S.Zero) == Identity(n)\n    assert MatPow(C, S.One) == C\n    assert MatPow(C, S.NegativeOne) == Inverse(C)\n    assert MatPow(C, n).shape == (n, n)\n    assert MatPow(C, -n) == Inverse(MatPow(C, n))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n    raises(NonSquareMatrixError, lambda: MatPow(A, S.NegativeOne))\n", "def test_MatPow():\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, 2) == C*C\n    assert MatPow(C, -1) == Inverse(C)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n", "def test_matrix_power():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, -1) == Inverse(C)\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n", "def test_matpow():\n    assert MatPow(C, 2) == C*C\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, 0) == Identity(n)\n\n    raises(TypeError, lambda: MatPow(n, 2))\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n\n    assert MatPow(C, 1) == C\n    assert MatPow(C, -2) == Inverse(C*C)\n"], "sample_969": ["def test_restify_type_union_operator():\n    assert restify(int | str) == \":py:class:`int` | :py:class:`str`\"  # type: ignore\n    assert restify(int | None) == \":py:class:`int` | :py:obj:`None`\"  # type: ignore\n    assert restify(int | str | None) == (\":py:class:`int` | :py:class:`str` | \"  # type: ignore\n                                         \":py:obj:`None`\")\n", "def test_restify_type_ForwardRef_in_globalns():\n    import typing\n    from sphinx.util.typing import _ForwardRef\n    globalns = {'MyInt': typing.NewType('MyInt', int)}\n    localns = {}\n    assert restify(_ForwardRef('MyInt', is_argument=False)) == \":py:class:`tests.test_util_typing.MyInt`\"\n", "def test_restify_type_hints_generic_alias():\n    from typing import Generic, TypeVar\n\n    T = TypeVar('T')\n\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert restify(MyGeneric) == \":py:class:`tests.test_util_typing.MyGeneric`\"\n    assert restify(MyGeneric[int]) == \":py:class:`tests.test_util_typing.MyGeneric`\\\\ [:py:class:`int`]\"\n", "def test_restify_type_union_operator():\n    assert restify(int | None) == \":py:class:`int` | :py:obj:`None`\"  # type: ignore\n    assert restify(int | str) == \":py:class:`int` | :py:class:`str`\"  # type: ignore\n    assert restify(int | str | None) == (\":py:class:`int` | :py:class:`str` | \"  # type: ignore\n                                         \":py:obj:`None`\")\n", "def test_restify_type_union_operator():\n    assert restify(int | None) == \":py:class:`int` | :py:obj:`None`\"  # type: ignore\n    assert restify(int | str) == \":py:class:`int` | :py:class:`str`\"  # type: ignore\n    assert restify(int | str | None) == (\":py:class:`int` | :py:class:`str` | \"  # type: ignore\n                                         \":py:obj:`None`\")\n"], "sample_1141": ["def test_MatrixSymbol_transpose():\n    A = MatrixSymbol('A', n, m)\n    assert A.T.shape == (m, n)\n    assert A.T.T == A\n", "def test_matrix_subtraction():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n\n    assert isinstance(A - B, MatAdd)\n    assert (A - B).shape == A.shape\n    assert isinstance(A - A + 2*B, MatMul)\n\n    raises(ShapeError, lambda: A - B.T)\n    raises(TypeError, lambda: A - 1)\n    raises(TypeError, lambda: 5 - A)\n\n    assert A - ZeroMatrix(n, m) - A == ZeroMatrix(n, m)\n", "def test_MatrixElement_symbolic_diff():\n    A = MatrixSymbol('A', n, m)\n    x, y = symbols('x y')\n    B = Matrix([[x, y], [z, w]])\n    assert (A[0, 0].diff(x) == 0).simplify()\n    assert (A[0, 0].diff(A[0, 0]) == 1).simplify()\n    assert (B[0, 0].diff(x) == 1).simplify()\n    assert (B[0, 1].diff(x) == 0).simplify()\n", "def test_MatrixSet_complex_elements():\n    M = MatrixSet(2, 2, set=S.Complexes)\n    X = Matrix([[1, S.ImaginaryUnit*2], [S.ImaginaryUnit*3, 4]])\n    assert X in M\n    X = Matrix([[1, S.ImaginaryUnit*2], [3, 4]])\n    assert X in M\n    X = Matrix([[1, 2], [S.ImaginaryUnit*3, 4]])\n    assert X in M\n    X = Matrix([[1, 2], [3, 4]])\n    assert X in M\n    X = Matrix([[1, 2], [3, S.Infinity]])\n    assert X not in M\n    X = Matrix([[1, 2], [3, S.NaN]])\n    assert X not in M\n", "def test_matrixsymbol_as_explicit():\n    # Test that as_explicit() method works correctly for MatrixSymbol\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    A_explicit = A.as_explicit()\n    B_explicit = B.as_explicit()\n    assert isinstance(A_explicit, ImmutableMatrix)\n    assert isinstance(B_explicit, ImmutableMatrix)\n    assert A_explicit.shape == A.shape\n    assert B_explicit.shape == B.shape\n"], "sample_1174": ["def test_polar_lift():\n    from sympy import polar_lift, exp_polar, pi, I\n\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n\n    p = Symbol('p', polar=True)\n    x = Symbol('x')\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n", "def test_conjugate_properties():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    z = Symbol('z', complex=True, zero=False)\n\n    assert Abs(conjugate(x)) == Abs(x)\n    assert Abs(conjugate(y)) == Abs(y)\n    assert Abs(conjugate(z)) == Abs(z)\n\n    assert sign(conjugate(x)) == sign(x)\n    assert sign(conjugate(y)) == -sign(y)\n    assert sign(conjugate(z)) == conjugate(sign(z))\n\n    assert arg(conjugate(x)) == arg(x)\n    assert arg(conjugate(y)) == -arg(y)\n    assert arg(conjugate(z)) == -arg(z)\n", "def test_polar_lift():\n    from sympy import polar_lift, exp_polar, I, pi, S\n    assert polar_lift(4) == exp_polar(0) * 4\n    assert polar_lift(-4) == exp_polar(I*pi) * 4\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*S.Half) == exp_polar(0) * 2\n    p = Symbol('p', polar=True)\n    x = Symbol('x')\n    assert polar_lift(4*x) == exp_polar(0) * 4 * x\n    assert polar_lift(4*p) == 4*p\n", "def test_conjugate_transpose_matrix():\n    A = MatrixSymbol('A', 2, 3)\n    assert conjugate(transpose(A)) == adjoint(A)\n    assert transpose(conjugate(A)) == adjoint(A)\n    assert adjoint(transpose(A)) == conjugate(A)\n    assert transpose(adjoint(A)) == conjugate(A)\n    assert adjoint(conjugate(A)) == transpose(A)\n    assert conjugate(adjoint(A)) == transpose(A)\n\n    A_values = Matrix([[1, 2 + 3*I], [-2, 4*I]])\n    assert conjugate(transpose(A_values)) == Matrix([[1, -2], [2 - 3*I, -4*I]])\n    assert transpose(conjugate(A_values)) == Matrix([[1, -2], [2 - 3*I, -4*I]])\n    assert adjoint(transpose(A_values)) == Matrix([[1, -2 - 3*I], [2, -4*I]])\n    assert transpose(adjoint(A_values)) == Matrix([[1, -2 - 3*I], [2, -4*I]])\n    assert adjoint(conjugate(A_values)) == Matrix([[1, -2], [2 - 3*I, -4*I]])\n    assert conjugate(adjoint(A_values)) == Matrix([[1, -2], [2 - 3*I, -4*I]])\n", "def test_issue_14692():\n    from sympy import principal_branch, polar_lift, exp_polar, I, pi\n    x = Symbol('x', positive=True)\n    assert principal_branch(exp_polar(I*pi)*x, 2*pi) == principal_branch(-x, 2*pi)\n"], "sample_133": ["def test_i18n_language_english_default_with_invalid_package(self):\n    \"\"\"\n    Check if the JavaScript i18n view raises a ValueError if an invalid package is provided.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        with self.assertRaisesMessage(ValueError, 'Invalid package(s) provided to JavaScriptCatalog: invalid_package'):\n            self.client.get('/jsi18n/invalid_package/')\n", "def test_jsi18n_multiple_domains(self):\n    \"\"\"\n    Test the javascript_catalog view with multiple domains.\n    \"\"\"\n    # Test with multiple domains\n    with override('fr'):\n        response = self.client.get('/jsi18n/domain1/domain2/')\n        self.assertContains(response, 'Cette cha\u00eene doit \u00eatre traduite (domaine 1)')\n        self.assertContains(response, 'Cette cha\u00eene doit \u00eatre traduite (domaine 2)')\n", "    def test_jsi18n_plural_function(self):\n        with override('pl'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, 'django.pluralidx = function(n) { return (n==1 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2); };')\n", "def test_i18n_different_plurals(self):\n    \"\"\"\n    Test the JavaScript i18n view with a language that has a different number of plurals.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('ar'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, '{count} plural6')\n        self.assertContains(response, ['{count} plural6', '{count} plural6s', '{count} plural6p', '{count} plural6d'])\n", "    def test_setlang_changes_language(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_template/')\n        elem = self.selenium.find_element_by_id(\"gettext\")\n        self.assertEqual(elem.text, \"Remove\")\n        self.selenium.get(self.live_server_url + '/i18n/setlang/?language=es')\n        self.selenium.get(self.live_server_url + '/jsi18n_template/')\n        elem = self.selenium.find_element_by_id(\"gettext\")\n        self.assertEqual(elem.text, \"Eliminar\")\n"], "sample_1058": ["def test_issue_14284():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(oo) == \"float('inf')\"\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n", "def test_PythonCodePrinter_settings():\n    settings = {'fully_qualified_modules': False}\n    prntr = PythonCodePrinter(settings)\n\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.module_imports == {}\n", "def test_issue_14283_additional_coverage():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(oo) == \"float('inf')\"\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = sqrt(x)\n    assert 'sympy' in p.module_imports\n    assert p.doprint(expr) == 'sympy.sqrt(x)'\n    assert 'sympy' in p.module_imports\n"], "sample_828": ["def test_check_paired_arrays():\n    # Ensures that paired array check works for dense matrices.\n    # Check that if XB is None, an error is raised.\n    XA = np.resize(np.arange(40), (5, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, None)\n\n    # Ensure that if XA and XB are given correctly, they return as equal.\n    XB = np.resize(np.arange(40), (5, 8))\n    XA_checked, XB_checked = check_paired_arrays(XA, XB)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n", "def test_check_mismatched_dtypes():\n    # Ensures that checks return valid arrays with same dtype.\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked.dtype, XB_checked.dtype)\n", "def test_check_bool_conversion():\n    # Ensures that type bool is preserved or converted to bool correctly.\n    XA = np.array([[True, False], [True, True]])\n    XB = np.array([[False, True], [True, False]])\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked.dtype, bool)\n    assert_equal(XB_checked.dtype, bool)\n\n    XA = np.array([[1, 0], [1, 1]], dtype=np.int32)\n    XB = np.array([[0, 1], [1, 0]], dtype=np.int32)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype=bool)\n    assert_equal(XA_checked.dtype, bool)\n    assert_equal(XB_checked.dtype, bool)\n", "def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    X = [[0], [0]]\n    Y = [[1], [2]]\n    D = paired_cosine_distances(X, Y)\n    assert_array_almost_equal(D, [0.5, 1.0])\n", "def test_check_precomputed_metric():\n    # Ensure that precomputed metric is handled correctly.\n    XA = np.array([[0, 1], [2, 3]])\n    XB = None\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, precomputed=True)\n    assert_array_equal(XA, XA_checked)\n    assert XB_checked is None\n\n    XA = np.array([[0, 1], [2, 3]])\n    XB = np.array([[4, 5], [6, 7]])\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, precomputed=True)\n"], "sample_827": ["def test_inplace_csr_row_normalize_l1():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n    X_l1_norm = X / np.sum(np.abs(X), axis=1, keepdims=True)\n    inplace_csr_row_normalize_l1(X_csr)\n    assert_array_almost_equal(X_csr.toarray(), X_l1_norm)\n\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float32)\n    X_csr = sp.csr_matrix(X)\n    X_l1_norm = X / np.sum(np.abs(X), axis=1, keepdims=True)\n    inplace_csr_row_normalize_l1(X_csr)\n    assert_array_almost_equal(X_csr.toarray(), X_l1_norm)\n", "def test_inplace_csr_row_normalize_l1():\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 200)\n    X_csr = sp.csr_matrix(X)\n    X_norm_csr = X_csr.copy()\n    norm = np.sum(np.abs(X), axis=1)\n    X_norm = X / norm[:, np.newaxis]\n    inplace_csr_row_normalize_l1(X_norm_csr)\n    assert_array_almost_equal(X_norm.toarray(), X_norm_csr.toarray())\n\n    X = X.astype(np.float32)\n    X_csr = sp.csr_matrix(X)\n    X_norm_csr = X_csr.copy()\n    norm = np.sum(np.abs(X), axis=1)\n    X_norm = X / norm[:, np.newaxis]\n    inplace_csr_row_normalize_l1(X_norm_csr)\n    assert_array_almost_equal(X_norm.toarray(), X_norm_csr.toarray())\n", "def test_inplace_csr_row_normalize_l1():\n    X = np.array([[0, 3, 0],\n                  [2, 0, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n\n    X_normalized = X / np.sum(np.abs(X), axis=1, keepdims=True)\n    inplace_csr_row_normalize_l1(X_csr)\n    assert_array_almost_equal(X_normalized, X_csr.toarray())\n\n    X = np.array([[0, 3, 0],\n                  [2, 0, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float32)\n    X_csr = sp.csr_matrix(X)\n\n    X_normalized = X / np.sum(np.abs(X), axis=1, keepdims=True)\n    inplace_csr_row_normalize_l1(X_csr)\n    assert_array_almost_equal(X_normalized, X_csr.toarray())\n\n    X = np.array([[0, 3, 0],\n                  [2, 0, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.int64)\n    X_csr = sp.csr_matrix(X)\n\n    X_normalized = X / np.sum(np.abs(X), axis=1, keepdims=True)\n    inplace_csr_row_normalize_l1(X_csr)\n    assert_array_almost_equal(X_normalized, X_csr.toarray())\n", "def test_inplace_csr_row_normalize_l1():\n    rng = np.random.RandomState(0)\n    X = sp.rand(100, 200, 0.05)\n    X_csr = X.tocsr()\n    X_csr_copy = X_csr.copy()\n    inplace_csr_row_normalize_l1(X_csr)\n    X_csr_copy = X_csr_copy.multiply(1. / X_csr_copy.sum(axis=1))\n    assert_array_almost_equal(X_csr.toarray(), X_csr_copy.toarray())\n", "def test_inplace_csr_row_normalize_l1(dtype, sparse_format):\n    X = np.array([[1, 2, 3],\n                  [4, 0, 6],\n                  [0, 7, 8],\n                  [9, 10, 0]], dtype=dtype)\n    X_sparse = sparse_format(X)\n    expected_result = X / np.sum(X, axis=1, keepdims=True)\n    inplace_csr_row_normalize_l1(X_sparse)\n    assert_array_almost_equal(X_sparse.toarray(), expected_result)\n"], "sample_154": ["    def test_database_checks_return_issues(self, mocked_check):\n        mocked_check.return_value = ['Issue 1', 'Issue 2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['Issue 1', 'Issue 2'])\n", "    def test_database_checks_return_issues(self, mocked_check):\n        mocked_check.return_value = ['Issue1', 'Issue2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['Issue1', 'Issue2'])\n", "    def test_database_checks_result(self, mocked_check):\n        issues = ['Some issue']\n        mocked_check.return_value = issues\n        result = check_database_backends(databases=self.databases)\n        self.assertEqual(result, issues)\n", "def test_database_checks_issues(self, mocked_check):\n    mocked_check.return_value = [('error_code', 'Error message')]\n    issues = check_database_backends(databases=self.databases)\n    self.assertEqual(len(issues), 2)\n    self.assertEqual(issues[0], ('error_code', 'Error message'))\n", "    def test_database_checks_issues(self, mocked_validation):\n        issues = [(mock.Mock(id='db.E001'), {'hint': 'Sample hint'})]\n        mocked_validation.check.return_value = issues\n        results = check_database_backends(databases=self.databases)\n        self.assertEqual(results, issues * len(self.databases))\n"], "sample_319": ["def test_add_constraint_suggest_name(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.AddConstraint(\n                \"Person\",\n                models.UniqueConstraint(\n                    fields=[\"name\"], name=\"person_name_unique\"\n                ),\n            ),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    self.assertEqual(migration.suggest_name(), \"person_person_name_unique\")\n", "def test_add_model_with_multiple_fields_removed_from_base_model(self):\n    \"\"\"\n    Removing multiple base fields takes place before adding a new inherited\n    model that has fields with the same names.\n    \"\"\"\n    before = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n                (\"content\", models.TextField()),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [\n                (\"title\", models.CharField(max_length=200)),\n                (\"content\", models.TextField()),\n            ],\n            bases=(\"app.readable\",),\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(\n        changes, \"app\", 0, [\"RemoveField\", \"RemoveField\", \"CreateModel\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, name=\"title\", model_name=\"readable\"\n    )\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 1, name=\"content\", model_name=\"readable\"\n    )\n    self.assertOperationAttributes(changes, \"app\", 0, 2, name=\"book\")\n", "def test_create_model_with_multiple_indexes(self):\n    author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n                models.Index(fields=[\"age\"], name=\"author_age_idx\"),\n            ]\n        },\n    )\n    changes = self.get_changes([], [author])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"Author\",\n        options={\"indexes\": [\n            models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n            models.Index(fields=[\"age\"], name=\"author_age_idx\"),\n        ]},\n    )\n", "    def test_multiple_operations_with_same_suggested_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddIndex(\"Person\", models.Index(fields=[\"first_name\"])),\n                migrations.AddIndex(\"Person\", models.Index(fields=[\"last_name\"])),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_add_index_and_more\")\n", "    def test_alter_index_together_to_index(self):\n        changes = self.get_changes(\n            [AutodetectorTests.author_empty, self.book_index_together],\n            [AutodetectorTests.author_empty, AutodetectorTests.book_indexes],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"AlterIndexTogether\", \"AddIndex\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            name=\"book\",\n            index_together=set(),\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            1,\n            model_name=\"book\",\n            name=\"book_title_author_idx\",\n        )\n"], "sample_415": ["def test_contains_expressions(self):\n    constraint = models.UniqueConstraint(Lower(\"title\"), F(\"author\"), name=\"book_func_uq\")\n    self.assertTrue(constraint.contains_expressions)\n    constraint = models.UniqueConstraint(fields=[\"title\", \"author\"], name=\"book_field_uq\")\n    self.assertFalse(constraint.contains_expressions)\n", "    def test_contains_expressions(self):\n        constraint_with_expressions = models.UniqueConstraint(Lower(\"title\"), F(\"author\"), name=\"book_func_uq\")\n        self.assertTrue(constraint_with_expressions.contains_expressions)\n        constraint_without_expressions = models.UniqueConstraint(fields=[\"title\", \"author\"], name=\"book_fields_uq\")\n        self.assertFalse(constraint_without_expressions.contains_expressions)\n", "def test_opclasses_database_constraint(self):\n    UniqueConstraintProduct.objects.create(name=\"p1\", color=\"red\")\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name=\"P1\", color=\"blue\")\n", "def test_validate_opclasses(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"],\n        name=\"opclasses_unique\",\n        opclasses=[\"varchar_pattern_ops\"],\n    )\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper())\n    msg = \"Constraint \u201copclasses_unique\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n", "    def test_validate_expression_f_object(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            F(\"author\"),\n            name=\"book_func_author_uq\",\n        )\n        book1 = UniqueConstraintProduct.objects.create(name=\"Book\", author=\"Author1\")\n        book2 = UniqueConstraintProduct(name=\"book\", author=\"Author2\")\n        msg = \"Constraint \u201cbook_func_author_uq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, book2)\n        # Existing instances have their existing row excluded.\n        constraint.validate(UniqueConstraintProduct, book1)\n        # Unique fields are excluded.\n        constraint.validate(\n            UniqueConstraintProduct,\n            book2,\n            exclude={\"name\"},\n        )\n        constraint.validate(\n            UniqueConstraintProduct,\n            book2,\n            exclude={\"author\"},\n        )\n"], "sample_826": ["def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['ghi', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [1, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 2, 12, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_n_values(n_values):\n    enc = OneHotEncoder(n_values=n_values)\n    X = np.array([[1, 0, 1], [0, 1, 1]])\n    X_trans = enc.fit_transform(X)\n    assert_equal(X_trans.shape, (2, 3 + 2 + 2))\n    assert_array_equal(enc.n_values_, [3, 2, 2])\n    # check that testing with larger feature works:\n    X = np.array([[2, 0, 1], [0, 1, 1]])\n    enc.transform(X)\n    # test that an error is raised when out of bounds:\n    X_too_large = np.array([[0, 2, 1], [0, 1, 1]])\n    assert_raises(ValueError, enc.transform, X_too_large)\n", "def test_invalid_drop_values(drop):\n    enc = OneHotEncoder(drop=drop)\n    assert_raises_regex(\n        ValueError,\n        \"The following categories were supposed\",\n        enc.fit, [['abc', 2, 55], ['def', 1, 55], ['def', 3, 56]])\n", "def test_one_hot_encoder_drop_first(X):\n    enc = OneHotEncoder(drop='first')\n    X_tr = enc.fit_transform(X)\n    exp = np.array([[1, 0, 0], [0, 1, 1]]) if isinstance(X[0][0], str) else np.array([[0, 1], [1, 0]])\n    assert_array_equal(X_tr.toarray(), exp)\n    assert_array_equal(np.array(X[1:], dtype=object), enc.inverse_transform(X_tr[1:]))\n", "def test_encoder_dtypes_numpy(X):\n    # check dtype (similar to test_encoder_dtypes for numpy arrays)\n    enc = OneHotEncoder(categories='auto')\n    enc.fit(X)\n    assert all([np.issubdtype(enc.categories_[i].dtype, X.dtype.kind) for i in range(2)])\n"], "sample_781": ["def test_forest_feature_importances_nonnegative():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                 n_estimators=200).fit(X, y)\n    assert np.all(clf.feature_importances_ >= 0)\n", "def test_multi_output_regression(name):\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(bootstrap=True, oob_score=True)\n\n    X = iris.data\n\n    # Make multi column regression target.\n    y = np.vstack([\n        iris.data[:, 0],\n        iris.data[:, 1],\n        iris.data[:, 2],\n    ]).T\n\n    # Try to fit and predict.\n    clf.fit(X, y)\n    clf.predict(X)\n", "def test_class_weight_dict_format(name):\n    # Test if class_weight accepts a dict of dicts for multi-output\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n    _y = np.vstack((y, np.array(y) * 2)).T\n    clf = ForestClassifier(class_weight={0: {-1: 0.5, 1: 1.}, 1: {-2: 1., 2: 1.}}, random_state=0)\n    clf.fit(X, _y)\n", "def test_multi_output_multi_class():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3, n_outputs=2)\n    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                 n_estimators=200).fit(X, y)\n    predictions = clf.predict(X)\n    assert_equal(predictions.shape, (15, 2))\n    assert set(np.unique(predictions[:, 0])).issubset(set(np.unique(y[:, 0])))\n    assert set(np.unique(predictions[:, 1])).issubset(set(np.unique(y[:, 1])))\n", "def test_multi_output_string_target(name):\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(bootstrap=True)\n\n    X = iris.data\n\n    # Make multi column string target.\n    y = np.vstack([\n        iris.target.astype(str),\n        iris.target.astype(str),\n    ]).T\n\n    # Try to fit and predict.\n    clf.fit(X, y)\n    clf.predict(X)\n"], "sample_195": ["def test_adapt_datetimefield_value_with_settings_use_tz_false(self):\n    now = timezone.now()\n    msg = 'SQLite backend does not support timezone-aware datetimes when USE_TZ is False.'\n    with self.settings(USE_TZ=False):\n        with self.assertRaisesMessage(ValueError, msg):\n            self.ops.adapt_datetimefield_value(timezone.make_aware(now))\n", "def test_sql_flush_with_tables(self):\n    with self.assertNumQueries(3):\n        Author.objects.create(name='Author 1')\n        Book.objects.create(title='Book 1', author_id=1)\n        sql = connection.ops.sql_flush(no_style(), ['backends_author', 'backends_book'])\n        with connection.cursor() as cursor:\n            for query in sql:\n                cursor.execute(query)\n        self.assertEqual(Author.objects.count(), 0)\n        self.assertEqual(Book.objects.count(), 0)\n", "def test_adapt_datetimefield_value_unaware(self):\n    now = timezone.now()\n    self.assertEqual(self.ops.adapt_datetimefield_value(now), str(now))\n", "def test_sql_flush_with_cascade(self):\n    Author.objects.create(name='Author1')\n    Book.objects.create(title='Book1', author=Author.objects.get(name='Author1'))\n\n    with connection.cursor() as cursor:\n        cursor.execute(\"SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='backends_author'\")\n        author_table_exists = cursor.fetchone()[0] > 0\n        cursor.execute(\"SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='backends_book'\")\n        book_table_exists = cursor.fetchone()[0] > 0\n\n    self.assertTrue(author_table_exists)\n    self.assertTrue(book_table_exists)\n\n    connection.ops.sql_flush(no_style(), ['backends_author'], allow_cascade=True)\n\n    with connection.cursor() as cursor:\n        cursor.execute(\"SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='backends_author'\")\n        author_table_exists = cursor.fetchone()[0] > 0\n        cursor.execute(\"SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='backends_book'\")\n        book_table_exists = cursor.fetchone()[0] > 0\n\n    self.assertFalse(author_table_exists)\n    self.assertFalse(book_table_exists)\n", "    def test_datetime_extract_sql_with_tz(self):\n        result = self.ops.datetime_extract_sql('HOUR', 'test_field', 'UTC')\n        self.assertEqual(result, \"django_datetime_extract('hour', test_field, 'UTC', 'UTC')\")\n"], "sample_1152": ["def test_issue_17635():\n    x, y, z = symbols('x y z', real=True)\n    e = (x + y*sqrt(z))**2\n    assert powsimp(e) == x**2 + 2*x*y*sqrt(z) + y**2*z\n", "def test_powsimp_real_base():\n    a = symbols(\"a\", real=True)\n    b = symbols(\"b\", real=True, positive=True)\n    assert powsimp(a**b) == a**b\n    assert powsimp(b**a) == b**a\n    assert powsimp(a**(-b)) == 1 / a**b\n    assert powsimp(b**(-a)) == 1 / b**a\n", "def test_issue_xxxxx():\n    # Add a test case for the scenario where the base and exponent are both negative,\n    # and the exponent is a fraction. This case was not explicitly tested in the existing unit tests.\n    a = symbols('a', negative=True)\n    f = Rational(-1, 2)\n    assert powsimp(a**f) == (a**f).expand(func=True)\n", "def test_powsimp_rational_base():\n    x, y = symbols('x y', real=True)\n    base = (x**2 + y**2)/(x - y)\n    exponent = Rational(1, 2)\n    assert powsimp(base**exponent) == ((x + y)**2/(x - y))**(Rational(1, 2))\n", "def test_issue_XXXXX():\n    x, y = symbols('x y', real=True)\n    assert powsimp((x/y)**(-1)) == y/x\n    assert powsimp((x/y)**(-2)) == y**2/x**2\n"], "sample_934": ["def test_xref_consistency_extended(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n    # Add more roles and tags to check for class consistency\n    additional_roles_tags = [\n        ('expr', 'span'),\n        ('texpr', 'span'),\n        # Add more roles and tags as needed\n    ]\n\n    for role, tag in additional_roles_tags:\n        role_classes = RoleClasses(role, 'code', [tag])\n        assert role_classes.name == role\n        assert 'xref' in role_classes.classes\n        assert 'cpp' in role_classes.classes\n        assert role in role_classes.classes\n        assert tag in role_classes.content_classes\n        assert role in role_classes.content_classes[tag]\n        assert role_classes.classes == role_classes.content_classes[tag]\n", "def test_cross_reference_consistency(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<a .*?href=\"(?P<href>.*?)\".*?>.*?</a>').format(role=role)\n        results = re.findall(pattern, output)\n        expect = '''\\", "def test_xref_consistency_with_current_page(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency_current_page.html'\n    output = (app.outdir / test).read_text()\n\n    current_page_ref = re.search(r'<a class=\"reference internal\" href=\"#\">\\s*Current page\\s*</a>', output)\n    assert current_page_ref, 'Current page reference not found in the output'\n\n    current_page_link = re.search(r'<a class=\"current reference internal\" href=\"#\">\\s*Current page\\s*</a>', output)\n    assert current_page_link, 'Current page link not found in the output'\n", "def test_cpp_class_definition():\n    check('class', '{key}MyClass', {1: 'MyClass', 2: '1MyClass'})\n    check('class', '{key}MyClass final', {1: 'MyClass', 2: '1MyClass'})\n    check('class', '{key}MyClass final : public BaseClass', {1: 'MyClass', 2: '1MyClass'})\n    check('class', '{key}MyClass : public BaseClass1, private BaseClass2',\n          {1: 'MyClass', 2: '1MyClass'})\n    check('class', '{key}MyClass<T>', {2: 'IE1MyClassI1TE'})\n    check('class', '{key}MyClass<T1, T2>', {2: 'I00E1MyClassI1T1I1T2EE'})\n", "def test_user_defined_literals():\n    check('function', 'void f(int a = 42_km)', {1: 'f__int', 2: '1f1i'})\n    check('function', 'void f(double b = 3.14_deg)', {1: 'f__double', 2: '1f1d'})\n    check('function', 'void f(char c = 'a'_ascii)', {1: 'f__char', 2: '1f1c'})\n    check('function', 'void f(wchar_t d = L'b'_utf16)', {1: 'f__wchar_t', 2: '1f1t'})\n    check('function', 'void f(char16_t e = u'c'_utf16)', {1: 'f__char16_t', 2: '1f1t'})\n    check('function', 'void f(char32_t f = U'd'_utf32)', {1: 'f__char32_t', 2: '1f1t'})\n"], "sample_132": ["    def test_template_not_found_error(self):\n        # Raises a TemplateDoesNotExist exception and shows the debug view.\n        url = reverse('raises_template_does_not_exist', kwargs={\"path\": \"notfound.html\"})\n        with self.assertLogs('django.request', 'ERROR'):\n            response = self.client.get(url)\n        self.assertContains(response, '<div class=\"context\" id=\"', status_code=500)\n", "    def test_request_without_meta(self):\n        \"\"\"\n        The debug view can be rendered if the request doesn't have a META dictionary.\n        \"\"\"\n        request = self.rf.get('/test_view/')\n        request.META = None\n\n        try:\n            raise ValueError('Oops')\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertInHTML('<h1>ValueError at /test_view/</h1>', html)\n        self.assertIn('<pre class=\"exception_value\">Oops</pre>', html)\n\n        text = reporter.get_traceback_text()\n        self.assertIn('ValueError at /test_view/', text)\n        self.assertIn('Oops', text)\n", "def test_template_source_encoding(self):\n    \"\"\"\n    The templates are loaded directly, not via a template loader, and\n    should be opened with the encoding specified in the template's first line.\n    \"\"\"\n    with mock.patch.object(DebugPath, 'open') as m:\n        m.return_value.__enter__.return_value = mock.mock_open(read_data='# -*- coding: latin-1 -*-\\n')\n        technical_500_response(None, None, None, None)\n        m.assert_called_once_with(encoding='latin-1')\n        m.reset_mock()\n        technical_404_response(mock.MagicMock(), mock.Mock())\n        m.assert_called_once_with(encoding='latin-1')\n", "    def test_template_not_exist_does_not_have_template_source(self):\n        \"\"\"\n        ExceptionReporter should not have template_source when template does not exist.\n        \"\"\"\n        try:\n            request = self.rf.get('/template_does_not_exist/')\n            raise TemplateDoesNotExist('template_does_not_exist.html')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        self.assertIsNone(reporter.template_info)\n", "def test_unicode_error_location(self):\n    try:\n        \"\u20ac\".encode('ascii')\n    except Exception:\n        exc_type, exc_value, tb = sys.exc_info()\n    request = self.rf.get('/test_view/')\n    reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n    text = reporter.get_traceback_text()\n    self.assertIn('File \"/.../test_views.py\", line 30, in test_unicode_error_location', text)\n"], "sample_731": ["def test_fetch_california_housing_return_X_y():\n    # Test that fetch_california_housing returns data and target\n    # when return_X_y is True.\n    data, target = fetch_california_housing(return_X_y=True)\n    assert data.shape == (20640, 8)\n    assert target.shape == (20640,)\n", "def test_california_housing_return_X_y():\n    check_return_X_y(partial(fetch, return_X_y=True))\n", "def test_fetch_california_housing_return_X_y():\n    \"\"\"Test that fetch_california_housing returns (data, target) when return_X_y is True\"\"\"\n    data, target = fetch(return_X_y=True)\n    assert isinstance(data, np.ndarray)\n    assert isinstance(target, np.ndarray)\n    assert data.shape[0] == target.shape[0]\n", "def test_fetch_california_housing_return_X_y():\n    \"\"\"Test that fetch_california_housing returns X and y as expected.\"\"\"\n    X, y = fetch_california_housing(return_X_y=True)\n    assert X.shape[0] == y.shape[0]\n    assert X.shape[1] == 8\n", "def test_fetch_california_housing_return_X_y():\n    X, y = fetch(return_X_y=True)\n    assert X.shape == (20640, 8)\n    assert y.shape == (20640,)\n"], "sample_603": ["def test_summarize_variable_with_unsafe_name_and_dtype():\n    var = xr.DataArray(np.random.rand(3, 4), dims=('<dim1>', '<dim2>'), name='<var>')\n    formatted = fh.summarize_variable('<name>', var, dtype='<dtype>')\n    assert \"&lt;name&gt;\" in formatted\n    assert \"&lt;dim1&gt;, &lt;dim2&gt;\" in formatted\n    assert \"&lt;dtype&gt;\" in formatted\n", "def test_summarize_variable_with_unsafe_name_and_value():\n    var = xr.Variable((\"x\",), np.random.rand(3), attrs={\"<key>\": \"<value>\"})\n    formatted = fh.summarize_variable(\"<name>\", var)\n    assert \"&lt;name&gt;\" in formatted\n    assert \"&lt;key&gt; :\" in formatted\n    assert \"&lt;value&gt;\" in formatted\n", "def test_summarize_coord_with_index(dataset):\n    coord = dataset.coords[\"time\"]\n    formatted = fh.summarize_coord(\"time\", coord)\n    assert \"time\" in formatted\n    assert \"class='xr-has-index'\" in formatted\n    assert \"datetime64[ns]\" in formatted\n", "def test_summary_of_variable_with_safe_name_and_dtype(dataarray):\n    formatted = fh.summarize_variable(\"safe_name\", dataarray, dtype=\"float64\")\n    assert \"safe_name\" in formatted\n    assert \"float64\" in formatted\n", "def test_summarize_variable_with_unsafe_name_and_dtype():\n    var = xr.DataArray(np.random.randn(4, 6), name=\"<name>\", dtype=\"<dtype>\")\n    formatted = fh.summarize_variable(\"foo\", var)\n    assert \"&lt;name&gt;\" in formatted\n    assert \"&lt;dtype&gt;\" in formatted\n"], "sample_935": ["def test_xref_consistency_class(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n    class_classes = RoleClasses('class', 'code', ['span'])\n    assert 'xref' in class_classes.classes\n    assert 'cpp' in class_classes.classes\n    assert 'class' in class_classes.classes\n    assert 'identifier' in class_classes.content_classes['span']\n", "def test_build_domain_cpp_xref_consistency(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        root_classes = classes(role, root)\n        for tag in contents:\n            content_classes = classes(role, tag)\n            assert root_classes == content_classes, \\\n                f'Inconsistent classes for role `{role}`: root `{root_classes}` != content `{content_classes}`'\n\n    roles = {\n        'class': {'root': 'a', 'contents': ['em']},\n        'struct': {'root': 'a', 'contents': ['em']},\n        'union': {'root': 'a', 'contents': ['em']},\n        'func': {'root': 'a', 'contents': ['em']},\n        'member': {'root': 'a', 'contents': ['em']},\n        'var': {'root': 'a', 'contents': ['em']},\n        'type': {'root': 'a', 'contents': ['em']},\n        'concept': {'root': 'a', 'contents': ['em']},\n        'enum': {'root': 'a', 'contents': ['em']},\n        'enumerator': {'root': 'a', 'contents': ['em']},\n    }\n\n    for role, tags in roles.items():\n        check_consistency(role, tags['root'], tags['contents'])\n", "def test_build_domain_cpp_roles_targets_function(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"roles-targets-function\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:var reference target not found: func_with_same_name\" in ws[0]\n    assert \"WARNING: cpp:var reference target not found: func_with_same_name_param\" in ws[1]\n", "def test_namespace_definitions():\n    check('function', 'void f()', {1: 'f', 2: '1fv'})\n    check('function', 'namespace::void f()', {1: 'namespace::f', 2: 'N10namespace1fEv'})\n    check('function', 'namespace::class_name::void f()', {1: 'namespace::class_name::f', 2: 'N10namespace10class_name1fEv'})\n    check('function', 'namespace::class_name::template<typename T> void f()', {2: 'N10namespace10class_nameI0E1fEv'})\n    check('function', 'namespace::class_name::template<typename T> void f(T a)', {2: 'N10namespace10class_nameI0E1f1TE'})\n", "def test_type_parsing():\n        parser = DefinitionParser(type_string, location=None, config=None)\n        ast = parser.parse_type()\n        parser.assert_end()\n        assert str(ast) == expected_output\n\n    check(\"const int\", \"intC\")\n    check(\"volatile int\", \"intV\")\n    check(\"const volatile int\", \"intCV\")\n    check(\"int *\", \"intP\")\n    check(\"const int *\", \"intCP\")\n    check(\"int &\", \"intR\")\n    check(\"const int &\", \"intCR\")\n    check(\"int *const\", \"intPC\")\n    check(\"int *volatile\", \"intPV\")\n    check(\"int *const volatile\", \"intPVC\")\n    check(\"int (*)(int)\", \"FipiE\")\n    check(\"int (A::*)(int)\", \"FMAipiE\")\n    check(\"int (A::*const)(int)\", \"FKMAipiE\")\n    check(\"int (A::*volatile)(int)\", \"FVMAipiE\")\n    check(\"int (A::*const volatile)(int)\", \"FVKMAipiE\")\n    check(\"int (A::*const&)(int)\", \"FRKMAipiE\")\n    check(\"int (A::*const&&)(int)\", \"FROMAipiE\")\n"], "sample_923": ["def test_build_domain_cpp_with_add_function_parentheses_with_backslash(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n    rolePatterns = [\n        ('', 'Sphinx\\\\\\\\'),\n        ('', 'Sphinx::version'),\n        ('', 'version'),\n        ('', 'List'),\n        ('', 'MyEnum')\n    ]\n    parenPatterns = [\n        ('ref function without parens ', r'paren_1\\\\\\(\\\\)'),\n        ('ref function with parens ', r'paren_2\\\\\\(\\\\)'),\n        ('ref function without parens, explicit title ', 'paren_3_title'),\n        ('ref function with parens, explicit title ', 'paren_4_title'),\n        ('ref op call without parens ', r'paren_5::operator\\\\\\(\\\\)'),\n        ('ref op call with parens ', r'paren_6::operator\\\\\\(\\\\)'),\n        ('ref op call without parens, explicit title ', 'paren_7_title'),\n        ('ref op call with parens, explicit title ', 'paren_8_title')\n    ]\n\n    f = 'roles.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n    for s in parenPatterns:\n        check(s, t, f)\n\n    f = 'any-role.html'\n    t = (app.outdir / f).read_text()\n    for s in parenPatterns:\n        check(s, t, f)\n", "def test_concept_requires_clauses():\n    check('concept', 'template<typename T> requires requires(T t) { { t.size() } -> std::size_t; } {key}C', {2: 'I0EIQR1CE'})\n", "def test_build_domain_cpp_no_old_id(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"no-old-id\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:no old id available for signature void f()\" in ws[0]\n", "def test_build_domain_cpp_roles_consistency(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n    rolePatterns = [\n        ('ref class ', 'MyClass'),\n        ('ref struct ', 'MyStruct'),\n        ('ref union ', 'MyUnion'),\n        ('ref func ', 'myFunction'),\n        ('ref member ', 'myMember'),\n        ('ref var ', 'myVar'),\n        ('ref type ', 'MyType'),\n        ('ref concept ', 'MyConcept'),\n        ('ref enum ', 'MyEnum'),\n        ('ref enumerator ', 'MyEnumerator'),\n        ('ref tParam ', 'T'),\n        ('ref functionParam ', 'param'),\n    ]\n\n    f = 'roles.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n\n    f = 'any-role.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n", "def test_concept_constraint_expressions():\n    check('concept', 'template<typename T> {key}Concept', {2: 'I0E7Concept'})\n    check('concept', 'template<typename T> {key}Concept requires Expression', {2: 'I0E7ConceptIQ10ExpressionEE'})\n    check('concept', 'template<typename T> {key}Concept requires Expression<T>', {2: 'I0E7ConceptIQ10ExpressionI1TEEE'})\n    check('concept', 'template<typename T> {key}Concept requires Expression<T> && AnotherExpression<T>', {2: 'I0E7ConceptIQoo10ExpressionI1TEEE16AnotherExpressionI1TEEE'})\n    check('concept', 'template<typename T> {key}Concept requires requires Expression', {2: 'I0E7ConceptIQIQ10ExpressionEE'})\n"], "sample_302": ["    def test_runshell(self):\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runshell') as mock_runshell:\n            self.client.runshell(['-c', 'SELECT 1;'])\n            self.assertEqual(signal.getsignal(signal.SIGINT), signal.SIG_IGN)\n        self.assertEqual(signal.getsignal(signal.SIGINT), sigint_handler)\n        mock_runshell.assert_called_once_with(['-c', 'SELECT 1;'])\n", "    def test_runshell(self):\n        with mock.patch('django.db.backends.base.client.signal') as mock_signal:\n            with mock.patch.object(BaseDatabaseClient, 'runshell') as mock_runshell:\n                self.client.runshell(parameters=[])\n                mock_signal.signal.assert_called_with(signal.SIGINT, signal.SIG_IGN)\n                mock_runshell.assert_called_once_with([])\n", "    def test_runshell(self):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runshell') as mock_runshell, \\\n             mock.patch('signal.signal') as mock_signal:\n            self.client.runshell([])\n            mock_signal.assert_called_with(signal.SIGINT, signal.SIG_IGN)\n            mock_runshell.assert_called_once_with([])\n", "    def test_runshell(self, mock_popen):\n        parameters = ['-c', 'SELECT 1;']\n        self.client.settings_to_cmd_args_env = mock.MagicMock(return_value=(['psql'], None))\n        self.client.runshell(parameters)\n        mock_popen.assert_called_once_with(['psql', '-c', 'SELECT 1;'], env=None)\n", "    def test_runshell(self):\n        # Mock the super method to prevent actual execution of the command\n        with mock.patch.object(super(BaseDatabaseClient, self.client), 'runshell') as mock_runshell:\n            # Call the runshell method\n            self.client.runshell(parameters=[])\n\n            # Assert that the signal handler was ignored and restored\n            self.assertEqual(mock.call.signal(signal.SIGINT, signal.SIG_IGN), mock_runshell.mock_calls[0])\n            self.assertEqual(mock.call.signal(signal.SIGINT, signal.signal(signal.SIGINT)), mock_runshell.mock_calls[2])\n"], "sample_732": ["def test_invalid_subset():\n    try:\n        fetch_kddcup99(subset='invalid_subset')\n    except ValueError:\n        pass\n    else:\n        raise AssertionError(\"Expected ValueError for invalid subset\")\n", "def test_invalid_subset():\n    with assert_raises(ValueError):\n        fetch_kddcup99(subset='invalid_subset')\n", "def test_subset_with_shuffle():\n    try:\n        data = fetch_kddcup99('SF', download_if_missing=False)\n        data_shuffled = fetch_kddcup99('SF', shuffle=True, random_state=0)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n", "def test_subset_shuffle():\n    try:\n        data = fetch_kddcup99(subset='SF', shuffle=True, random_state=0, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    data_shuffled = fetch_kddcup99(subset='SF', shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n    # Check that shuffling actually changed the order of the data\n    assert not (data.data == data_shuffled.data).all()\n    assert not (data.target == data_shuffled.target).all()\n", "def test_percent10_subset_shuffle():\n    try:\n        data = fetch_kddcup99(subset='SF', shuffle=True, random_state=0, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    data_shuffled = fetch_kddcup99(subset='SF', shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n    assert not (data.data == data_shuffled.data).all()  # Check that shuffling has occurred\n"], "sample_575": ["def test_label_concise(self, t):\n\n    s = Temporal().label(concise=True)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(10, 1000)\n    label, = a.major.formatter.format_ticks([100])\n    assert label != \"\"\n", "def test_color_tuple_values_temporal(self, t, x):\n\n    cmap = color_palette(\"blend:b,g\", as_cmap=True)\n    s = Temporal((\"b\", \"g\"))._setup(t, Color())\n    normed = (x - x.min()) / (x.max() - x.min())\n    assert_array_equal(s(t), cmap(normed)[:, :3])  # FIXME RGBA\n", "def test_label_concise(self, t):\n\n    s = Temporal().label(concise=True)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(0, 365)\n    labels = a.major.formatter.format_ticks(a.major.locator())\n    for text in labels:\n        assert len(text) > 0\n", "def test_label_concise(self, t):\n\n    s = Temporal().label(concise=True)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(0, 365)\n    label, = a.major.formatter.format_ticks([180])\n    assert label == \"6 months\"\n", "def test_label_concise(self, t):\n\n    s = Temporal().label(concise=True)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(10, 1000)\n    label, = a.major.formatter.format_ticks([100])\n    assert label == \"Jan 1, 1970\"\n"], "sample_926": ["def test_xref_class_consistency(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n    # Check classes for 'class' role\n    class_role = RoleClasses('class', 'strong', ['span', 'em'])\n    assert 'xref' in class_role.classes\n    assert 'xref' in class_role.content_classes['span']\n    assert 'cpp' in class_role.content_classes['span']\n    assert 'class' in class_role.content_classes['span']\n    assert 'cpp-class' in class_role.content_classes['em']\n", "def test_xref_consistency_cpp(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n    cpp_role = RoleClasses('cpp', 'a', {'span'})\n    assert 'xref' in cpp_role.classes\n    assert 'cpp' in cpp_role.classes\n    assert 'xref' in cpp_role.content_classes['span']\n    assert 'cpp' in cpp_role.content_classes['span']\n\n    member_role = RoleClasses('member', 'a', {'span'})\n    assert 'xref' in member_role.classes\n    assert 'cpp' in member_role.classes\n    assert 'cpp-member' in member_role.classes\n    assert 'xref' in member_role.content_classes['span']\n    assert 'cpp' in member_role.content_classes['span']\n    assert 'cpp-member' in member_role.content_classes['span']\n\n    # Add similar assertions for other roles as needed\n", "def test_xref_consistency_non_callable_roles(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n    non_callable_roles = ['class', 'struct', 'union', 'type', 'concept', 'enum', 'enumerator']\n    for role in non_callable_roles:\n        role_classes = RoleClasses(role, 'em', {'code': ['descname', 'descclassname']})\n        assert 'descname' in role_classes.content_classes['code']\n        assert 'descclassname' in role_classes.content_classes['code']\n        assert role_classes.content_classes['code'] == role_classes.classes\n", "def test_concept_requirements():\n    check('concept', 'template<typename Param> {key}A::B::Concept requires A',\n          {2: 'I0EN1A1B7ConceptI1AE'})\n    check('concept', 'template<typename A, typename B, typename ...C> {key}Foo requires A && B',\n          {2: 'I00DpE3FooI00EE'})\n    check('concept', 'template<typename T> {key}C requires A<T> && B<T>',\n          {2: 'I0E1CI0E1TI1AEI1BE'})\n    with pytest.raises(DefinitionError):\n        parse('concept', 'template<typename T> {key}C requires A<T> B<T>')\n    with pytest.raises(DefinitionError):\n        parse('concept', 'template<typename T> {key}C requires A<T> && B<T> &&')\n", "def test_xref_consistency_cpp_expr_role():\n    role = 'cpp:expr'\n    root = 'span'\n    contents = ['span']\n\n    rc = RoleClasses(role, root, contents)\n    assert rc.classes == {'xref', 'cpp', role.replace(':', '-'), 'cpp-expr'}\n    assert rc.content_classes['span'] == {'xref', 'cpp', role.replace(':', '-'), 'cpp-expr'}\n"], "sample_279": ["def test_empty_fields(self):\n    msg = 'At least one field is required to define a unique constraint.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(name='uniq', fields=[])\n", "def test_opclasses_database_constraint(self):\n    UniqueConstraintProduct.objects.create(name='p1', color='red')\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name='p1', color='red')\n", "def test_opclasses_name(self):\n    constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\n    expected_name = 'opclasses_field_uniq'\n    self.assertIn(expected_name, constraints)\n", "    def test_multiple_fields(self):\n        UniqueConstraintProduct.objects.create(name='p1', color='red')\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', color='red')\n\n        UniqueConstraintProduct.objects.create(name='p2', color='blue')\n        UniqueConstraintProduct.objects.create(name='p1', color='blue')\n        UniqueConstraintProduct.objects.create(name='p2', color='red')\n", "def test_opclasses(self):\n    constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\n    expected_opclasses = ['varchar_pattern_ops']\n    for constraint in constraints.values():\n        if constraint['columns'] == ['name']:\n            self.assertEqual(constraint['opclasses'], expected_opclasses)\n"], "sample_611": ["def test_date_range_like_noleap_to_gregorian(start, freq, cal_src, cal_tgt, use_cftime, exp0, exp_pd):\n    source = date_range(start, periods=12, freq=freq, calendar=cal_src)\n\n    out = date_range_like(source, cal_tgt, use_cftime=use_cftime)\n\n    assert len(out) == 12\n    assert infer_freq(out) == freq\n\n    assert out[0].isoformat().startswith(exp0)\n\n    if exp_pd:\n        assert isinstance(out, pd.DatetimeIndex)\n    else:\n        assert isinstance(out, CFTimeIndex)\n        assert out.calendar == cal_tgt\n", "def test_date_range_like_different_frequency(start, freq, cal_src, cal_tgt, use_cftime, exp0, exp_pd):\n    source = date_range(start, periods=12, freq=freq, calendar=cal_src)\n\n    out = date_range_like(source, cal_tgt, use_cftime=use_cftime)\n\n    assert len(out) == 12\n    assert infer_freq(out) == freq\n\n    assert out[0].isoformat().startswith(exp0)\n\n    if exp_pd:\n        assert isinstance(out, pd.DatetimeIndex)\n    else:\n        assert isinstance(out, CFTimeIndex)\n        assert out.calendar == cal_tgt\n", "def test_add_offset_with_specific_month(calendar, offset, expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(1, 1, 1)\n    result = initial + offset\n    expected = date_type(*expected_date_args)\n    assert result == expected\n", "def test_date_range_like_year_start(start, end, freq, cal_tgt, use_cftime, exp0, exp_end, exp_pd):\n    source = date_range(start, end=end, freq=freq)\n\n    out = date_range_like(source, cal_tgt, use_cftime=use_cftime)\n\n    assert len(out) == len(source)\n    assert infer_freq(out) == freq\n\n    assert out[0].isoformat().startswith(exp0)\n    assert out[-1].isoformat().startswith(exp_end)\n\n    if exp_pd:\n        assert isinstance(out, pd.DatetimeIndex)\n    else:\n        assert isinstance(out, CFTimeIndex)\n        assert out.calendar == cal_tgt\n", "def test_date_range_like_julian_calendar(start, freq, cal_src, cal_tgt, use_cftime, exp0, exp_pd):\n    source = date_range(start, periods=12, freq=freq, calendar=cal_src)\n\n    out = date_range_like(source, cal_tgt, use_cftime=use_cftime)\n\n    assert len(out) == 12\n    assert infer_freq(out) == freq\n\n    assert out[0].isoformat().startswith(exp0)\n\n    if exp_pd:\n        assert isinstance(out, pd.DatetimeIndex)\n    else:\n        assert isinstance(out, CFTimeIndex)\n        assert out.calendar == cal_tgt\n"], "sample_1064": ["def test_MatrixElement_assignment():\n    A = MatrixSymbol(\"A\", 1, 3)\n    B = MatrixSymbol(\"B\", 1, 3)\n\n    A[0, 0] = B[0, 0]\n    assert tensorflow_code(A) == \"A = tensorflow.tensor_scatter_nd_update(A, tf.constant([[0, 0]]), tf.gather_nd(B, tf.constant([[0, 0]])))\"\n", "def test_tensorflow_functions():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    f = Function('f')\n    expr = f(x)\n    assert tensorflow_code(expr) == \"f(x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = Derivative(f(x), x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(f(x), x)[0]\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n", "def test_derivative_printing():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    f = Function('f')(x, y)\n    expr = Derivative(f, x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(f(x, y), x)[0]\"\n\n    expr = Derivative(f, x, y)\n    with raises(NotImplementedError):\n        tensorflow_code(expr)\n\n    expr = Derivative(f, (x, 2))\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.gradients(f(x, y), x)[0], x)[0]\"\n", "def test_Matrix_functions():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Max(M, N)\n    assert tensorflow_code(expr) == \"tensorflow.math.maximum(M, N)\"\n    _compare_tensorflow_matrix((M, N), expr)\n\n    expr = Min(M, N)\n    assert tensorflow_code(expr) == \"tensorflow.math.minimum(M, N)\"\n    _compare_tensorflow_matrix((M, N), expr)\n\n    expr = Derivative(M[0, 0], x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(M[0, 0], x)[0]\"\n\n    expr = Derivative(M[0, 0], (x, 2))\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.gradients(M[0, 0], x)[0], x)[0]\"\n", "def test_TensorflowPrinter_derivative():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    f = Function('f')\n    x = Symbol('x')\n    expr = Derivative(f(x), x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(f(x), x)[0]\"\n\n    # Test derivative with multiple variables\n    y = Symbol('y')\n    expr = Derivative(f(x, y), x, y)\n    try:\n        tensorflow_code(expr)\n    except NotImplementedError:\n        pass\n    else:\n        assert False, \"Expected NotImplementedError for derivative with multiple variables\"\n"], "sample_948": ["def test_unresolved_reference(app, warning):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   Calls :cpp:func:`g`.\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 2\n    assert \"index.rst:2: WARNING: undefined label: g (if the link is right, please contact the author)\" in ws[0]\n    assert ws[1] == \"\"\n", "def test_mix_decl_duplicate_2(app, warning):\n    # Test for mixing function and class with the same name\n    text = (\".. cpp:function:: void A()\\n\"\n            \".. cpp:class:: A\\n\"\n            \".. cpp:function:: void A()\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 7\n    assert \"index.rst:2: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[0]\n    assert \"Declaration is '.. cpp:class:: A'.\" in ws[1]\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[2]\n    assert \"Declaration is '.. cpp:function:: void A()'.\" in ws[3]\n    assert ws[4] == \"\"\n", "def test_build_domain_cpp_default_arguments(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    default_arg_patterns = [\n        ('ref function with default argument ', r'default_1\\(int a = 0\\)'),\n        ('ref function with default argument and space ', r'default_2\\(int a = 0\\)'),\n        ('ref function with default argument and newline ', r'default_3\\(int a = 0\\)'),\n        ('ref function with default argument and comment ', r'default_4\\(int a = 0\\)'),\n        ('ref function with default argument and nested default argument ', r'default_5\\(int a = default_1\\(\\)\\)'),\n    ]\n\n    f = 'default_arguments.html'\n    t = (app.outdir / f).read_text()\n    for s in default_arg_patterns:\n        check(s, t, f)\n", "def test_backticks_in_expressions():\n    check('function', 'void f(int `a`)', {1: 'f__i', 2: '1fia'})\n    check('function', 'void f(int *`a`)', {1: 'f__iP', 2: '1fia'})\n    check('function', 'void f(int (&`a`)[10])', {1: 'f__iA', 2: '1fRAL10Eia'})\n    check('function', 'void f(int (`A`::*`p`)(float, double))', {2: '1fM1CFifdEp'})\n", "def test_multi_decl_roles():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n        roles = parser.get_roles_for_declaration()\n        assert sorted(roles) == sorted(expected_roles)\n    check('f', ['any', 'func'])\n    check('f()', ['any', 'func'])\n    check('void f()', ['any', 'func'])\n    check('T f()', ['any', 'func', 'type'])\n    check('T::U f()', ['any', 'func', 'type', 'member'])\n    check('T::U::V f()', ['any', 'func', 'type', 'member'])\n    check('T::operator U()', ['any', 'func', 'type', 'member'])\n    check('T::operator U::V()', ['any', 'func', 'type', 'member'])\n"], "sample_1069": ["def test_gamma_printing():\n    assert octave_code(gamma(x)) == 'gamma(x)'\n    assert octave_code(gamma(x, y)) == 'gamma(x, y)'\n", "def test_symbol_printing():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert mcode(x + y) == \"x + y\"\n    assert mcode(x - y) == \"x - y\"\n    assert mcode(x * y) == \"x.*y\"\n    assert mcode(x / y) == \"x./y\"\n    assert mcode(x ** y) == \"x.^y\"\n", "def test_MatrixElement_printing_with_subs():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n\n    F = A[0, 0].subs(A, B)\n    assert mcode(F) == \"B(1, 1)\"\n\n    G = A[0, 0].subs(A, B - A)\n    assert mcode(G) == \"(B - A)(1, 1)\"\n", "def test_octave_MatrixElement_complex():\n    A = MatrixSymbol(\"A\", 2, 2, complex=True)\n    assert mcode(A[0, 1]) == \"A(1, 2)\"\n    assert mcode(A[1, 0]) == \"A(2, 1)\"\n", "def test_octave_matrix_vector():\n    A = Matrix([[1, 2, 3]])\n    B = MatrixSymbol('B', 3, 1)\n    C = MatrixSymbol('C', 1, 3)\n    assert mcode(A*B) == \"A*B\"\n    assert mcode(C*A) == \"C*A\"\n    assert mcode(2*A*B) == \"2*A*B\"\n    assert mcode(A*2*B) == \"A*(2*B)\"\n    assert mcode(A*(B + 3*Identity(3))) == \"A*(3*eye(3) + B)\"\n    assert mcode(A**(x**2)) == \"A^(x.^2)\"\n    assert mcode(A**3) == \"A^3\"\n    assert mcode(A**(S.Half)) == \"A^(1/2)\"\n"], "sample_1125": ["def test_operator_mul():\n    O = Operator('O')\n    I = IdentityOperator()\n    assert O*I == O\n", "def test_operator_multiplication():\n    A = Operator('A')\n    B = Operator('B')\n    I = IdentityOperator()\n    assert A*I == A\n    assert I*A == A\n    assert A*B != B*A\n    assert (A*B)*C == A*(B*C)\n", "def test_operator_multiplication():\n    A = Operator('A')\n    B = Operator('B')\n    assert A * B != B * A\n    assert (A * B).is_commutative is False\n    assert (A * B * B).expand() == A * B**2\n    assert (A * B).inv() == B.inv() * A.inv()\n", "def test_operator_initialization():\n    O = Operator('O')\n    assert isinstance(O, Operator)\n    assert O.label == ('O',)\n    assert O.hilbert_space == 'H'\n    assert not O.is_hermitian\n    assert not O.is_commutative\n", "def test_operator_multiplication():\n    A = Operator('A')\n    B = Operator('B')\n    assert A*B != B*A\n    assert (A*B).is_commutative is False\n    assert (B*A).is_commutative is False\n\n    # Test multiplication with identity operator\n    assert A*IdentityOperator() == A\n    assert IdentityOperator()*A == A\n"], "sample_723": ["def test_imputation_error_cases():\n    # Test error cases for SimpleImputer\n    X = np.array([[np.nan, 2], [6, np.nan], [7, 8]])\n\n    # Test invalid strategy\n    imputer = SimpleImputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test invalid axis\n    imputer = SimpleImputer(axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test mismatch between number of features in fit and transform\n    imputer = SimpleImputer()\n    imputer.fit(X)\n    X_transform = np.array([[np.nan, 2, 3]])\n    assert_raises(ValueError, imputer.transform, X_transform)\n", "def test_imputation_with_non_finite_values():\n    # Test imputation when the input contains non-finite values.\n    X = np.array([\n        [np.inf, 2, 3],\n        [4, 5, np.nan],\n        [6, np.NINF, 8],\n    ])\n\n    X_imputed_mean = np.array([\n        [5, 2, 3],\n        [4, 5, 4.5],\n        [6, 4.5, 8],\n    ])\n    statistics_mean = [5, 5, 4.5]\n\n    X_imputed_median = np.array([\n        [4, 2, 3],\n        [4, 5, 4.5],\n        [6, 4.5, 8],\n    ])\n    statistics_median = [4, 5, 4.5]\n\n    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, np.nan)\n    _check_statistics(X, X_imputed_median, \"median\", statistics_median, np.nan)\n", "def test_imputation_with_copy():\n    # Test imputation with copy parameter.\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 5],\n        [6, 7, np.nan],\n    ])\n\n    X_true = np.array([\n        [4.5, 1, 2],\n        [3, 4.5, 5],\n        [6, 7, 4.5],\n    ])\n\n    # With copy=True, the original data should not be modified\n    imputer = SimpleImputer(missing_values='NaN', strategy='mean', copy=True)\n    X_copy = X.copy()\n    X_trans = imputer.fit_transform(X_copy)\n    assert_array_equal(X_trans, X_true)\n    assert_false(np.array_equal(X, X_copy))\n\n    # With copy=False, the original data should be modified in-place\n    imputer = SimpleImputer(missing_values='NaN', strategy='mean', copy=False)\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X_trans, X_true)\n    assert_array_equal(X, X_true)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy.\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n    imputer = SimpleImputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_with_float_missing_values():\n    # Test imputation when missing values are floats.\n    X = np.array([\n        [1.0, np.nan, 3.0],\n        [4.0, 5.0, np.nan],\n        [7.0, 8.0, 9.0],\n    ])\n\n    X_true = np.array([\n        [1.0, 5.0, 3.0],\n        [4.0, 5.0, 6.0],\n        [7.0, 8.0, 9.0],\n    ])\n\n    _check_statistics(X, X_true, \"mean\", [1.0, 5.0, 6.0], np.nan)\n"], "sample_1142": ["def test_MatrixElement_doit_deep():\n    u = MatrixSymbol('u', 2, 1)\n    v = ImmutableMatrix([3, 5])\n    assert u[0, 0].subs(u, v).doit(deep=False) == u[0, 0]\n    assert u[0, 0].subs(u, v).doit(deep=True) == v[0, 0]\n", "def test_MatrixElement_diff_with_scalar():\n    A = MatrixSymbol('A', n, m)\n    s = Symbol('s')\n    assert (s*A[0, 0]).diff(s) == A[0, 0]\n", "def test_MatrixSymbol_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert A.subs(n, l) == MatrixSymbol('A', l, m)\n    assert A.subs(m, n) == MatrixSymbol('A', n, n)\n    assert A.subs({n: l, m: n}) == MatrixSymbol('A', l, n)\n    assert (A*B).subs(A, C) == C*B\n    assert (A*B).subs(B, C) == A*C\n    assert (A*B).subs({A: C, B: D}) == C*D\n", "def test_matrix_symbol_diff():\n    A = MatrixSymbol('A', n, m)\n    x = Symbol('x')\n    assert A.diff(x) == ZeroMatrix(n, m)\n", "def test_matrix_symbol_sympify():\n    n = Symbol('n')\n    A = MatrixSymbol('A', n, n)\n    assert MatrixSymbol(A, n, n) == A\n    raises(TypeError, lambda: MatrixSymbol(1, n, n))\n    raises(TypeError, lambda: MatrixSymbol(n**2, n, n))\n"], "sample_309": ["    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('http://example.com/path'), 'http://example.com/path')\n        self.assertEqual(escape_leading_slashes('//example.com/path'), '/%2Fexample.com/path')\n        self.assertEqual(escape_leading_slashes('///example.com/path'), '/%2F/example.com/path')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('/path'), '/path')\n        self.assertEqual(escape_leading_slashes('//path'), '/%2Fpath')\n        self.assertEqual(escape_leading_slashes('///path'), '/%2F%2Fpath')\n", "    def test_parsing_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date')\n        with self.assertRaises(ValueError):\n            parse_http_date('Sun Nov  6 08:49:37')  # Missing year\n        with self.assertRaises(ValueError):\n            parse_http_date('Sun Nov  6 08:49')  # Missing seconds\n        with self.assertRaises(ValueError):\n            parse_http_date('Sun Nov  6 08:49:61 1994')  # Invalid seconds\n        with self.assertRaises(ValueError):\n            parse_http_date('Sun Nov 31 08:49:37 1994')  # Invalid day\n", "    def test_escaping(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n        self.assertEqual(escape_leading_slashes('/path'), '/path')\n        self.assertEqual(escape_leading_slashes('path'), 'path')\n        self.assertEqual(escape_leading_slashes('///path'), '/%2F%2Fpath')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com/path'), '/%2Fexample.com/path')\n        self.assertEqual(escape_leading_slashes('/example.com/path'), '/example.com/path')\n        self.assertEqual(escape_leading_slashes('http://example.com/path'), 'http://example.com/path')\n        self.assertEqual(escape_leading_slashes('https://example.com/path'), 'https://example.com/path')\n"], "sample_1038": ["def test_MatrixElement_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    assert A[0, 0].subs(A, B) == B[0, 0]\n", "def test_MatrixElement_diff_non_MatrixElement():\n    A = MatrixSymbol('A', n, m)\n    x = symbols('x')\n    assert A[0, 0].diff(x) == 0\n", "def test_MatMul_doit():\n    A = MatrixSymbol(\"A\", n, m)\n    B = MatrixSymbol(\"B\", m, l)\n    AB = MatMul(A, B)\n    assert AB.doit() == A*B\n    assert AB.doit(deep=False) == AB\n", "def test_MatrixElement_diff_with_inverse():\n    i, j = symbols(\"i j\")\n    A = MatrixSymbol(\"A\", n, n)\n    expr = diff((A.I)[i, j], A[k, l])\n    # Insert the expected result based on the mathematical calculation\n    # This could be a complicated expression involving KroneckerDelta and Inverse(A)\n    expected_result = ...  # Insert the expected result here\n    assert expr == expected_result\n", "def test_MatrixElement_diff_with_Index():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    X = MatrixSymbol('X', n, n)\n    i, j, k = symbols('i j k', integer=True)\n\n    # Testing differentiation of MatrixElement with indices\n    assert (A[i, j]*B[j, k]).diff(A[i, j]) == B[j, k]\n    assert (A[i, j]*B[j, k]).diff(B[j, k]) == A[i, j]\n    assert (A[i, j]*A[j, i]).diff(A[i, j]) == A[j, i]\n    assert (A[i, j]*A[j, i]).diff(A[j, i]) == A[i, j]\n    assert (X[i, i]).diff(X[i, i]) == 1\n"], "sample_431": ["def test_refresh_defer(self):\n    a = Article.objects.create(pub_date=datetime.now(), headline=\"Original headline\")\n    deferred_a = Article.objects.defer(\"headline\").get(pk=a.pk)\n    a.headline = \"Updated headline\"\n    a.save()\n    with self.assertNumQueries(1):\n        deferred_a.refresh_from_db()\n    self.assertEqual(deferred_a.headline, \"Original headline\")\n", "def test_refresh_fk_with_custom_manager(self):\n    a1 = Article.objects.create(pub_date=datetime.now())\n    a2 = Article.objects.create(pub_date=datetime.now())\n    s1 = SelfRef.objects.create(article=a1)\n    s1_copy = SelfRef.objects.get(pk=s1.pk)\n    s1_copy.article.headline = \"new headline\"\n    s1.article = a2\n    s1.save()\n    with self.assertNumQueries(1):\n        s1_copy.refresh_from_db()\n    with self.assertNumQueries(1):\n        # The old related instance was thrown away (the article_id has\n        # changed). It needs to be reloaded on access, so one query\n        # executed.\n        self.assertNotEqual(s1_copy.article.headline, \"new headline\")\n        self.assertEqual(s1_copy.article, a2)\n", "def test_refresh_with_default_value(self):\n    a = Article.objects.create(pub_date=datetime.now())\n    a_copy = Article.objects.get(pk=a.pk)\n    a.headline = \"New Headline\"\n    a.save()\n    with self.assertNumQueries(1):\n        a_copy.refresh_from_db(fields=[\"headline\"])\n        self.assertEqual(a_copy.headline, \"New Headline\")\n        self.assertEqual(a_copy.pub_date, None)\n", "def test_refresh_related_after_create(self):\n    article = Article.objects.create(\n        headline=\"Parrot programs in Python\",\n        pub_date=datetime(2005, 7, 28),\n    )\n    featured = FeaturedArticle.objects.create(article_id=article.pk)\n    article.featured = featured\n    article.save()\n    article.refresh_from_db()\n    self.assertEqual(article.featured, featured)\n", "def test_refresh_deferred_fields(self):\n    a = Article.objects.create(pub_date=datetime.now())\n    a.refresh_from_db(fields=[\"pub_date\"])\n    with self.assertNumQueries(0):\n        # Accessing a deferred field triggers a query\n        self.assertIsNotNone(a.headline)\n    with self.assertNumQueries(1):\n        a.refresh_from_db(fields=[\"headline\"])\n    with self.assertNumQueries(0):\n        self.assertIsNotNone(a.pub_date)\n"], "sample_604": ["def test_short_data_repr_large_duck_array():\n    class LargeDuckArray:\n            self.size = size\n            self.dtype = np.float64\n\n            return f\"LargeDuckArray with {self.size} values\"\n\n    large_duck_array = LargeDuckArray(int(1e6))\n    variable = xr.Variable(\"x\", large_duck_array)\n\n    actual = formatting.short_data_repr(variable)\n    expected = \"[1000000 values with dtype=float64]\"\n\n    assert actual == expected\n", "def test_inline_variable_array_repr_custom_repr_with_large_array():\n    class CustomArray:\n            self.value = value\n            self.attr = attr\n\n            return \"CustomArray with attribute: \" + self.attr\n\n            return NotImplemented\n\n        @property\n            return self.value.shape\n\n        @property\n            return self.value.dtype\n\n        @property\n            return self.value.ndim\n\n    value = CustomArray(np.random.randn(100, 5, 1), \"m\")\n    variable = xr.Variable(\"x\", value)\n\n    max_width = 50\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    assert len(actual) <= max_width\n", "def test_coords_repr_with_large_col_width():\n    ds = xr.Dataset(\n        coords={\n            \"long_coord_name\": (\"x\", np.array([\"a\", \"b\"])),\n            \"another_long_coord_name\": (\"x\", np.array([1, 2])),\n        }\n    )\n    expected = dedent(\n        \"\"\"\\\n    Coordinates:\n      * long_coord_name          (x) <U1 'a' 'b'\n      * another_long_coord_name  (x) int64 1 2\"\"\"\n    )\n    actual = formatting.coords_repr(ds.coords, col_width=30)\n    assert actual == expected\n", "def test_coords_repr_with_multilevel_index():\n    index = pd.MultiIndex.from_tuples(\n        [(\"a\", \"x\"), (\"a\", \"y\"), (\"b\", \"x\"), (\"b\", \"y\")], names=[\"first\", \"second\"]\n    )\n    ds = xr.Dataset(coords={\"coord\": (\"index\", np.arange(4), index)})\n\n    actual = formatting.coords_repr(ds.coords)\n    expected = dedent(\n        \"\"\"\\\n        Coordinates:\n          * coord    (index) MultiIndex\n          - first    (index) <U1 'a' 'a' 'b' 'b'\n          - second   (index) <U1 'x' 'y' 'x' 'y'\"\"\"\n    )\n\n    assert actual == expected\n", "def test_format_timedelta_out_of_bounds():\n    from datetime import timedelta\n\n    duration = timedelta(days=1000000000000)\n    expected = \"11574 days\"\n    result = formatting.format_timedelta(duration)\n    assert result == expected\n\n    duration = timedelta(days=-1000000000000)\n    expected = \"-11574 days\"\n    result = formatting.format_timedelta(duration)\n    assert result == expected\n"], "sample_917": ["def test_build_domain_cpp_unknown_targets(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"unknown-targets\")\n    assert len(ws) == 3\n    assert \"WARNING: cpp:any reference target not found: Unknown\" in ws[0]\n    assert \"WARNING: cpp:any reference target not found: Unknown::Unknown\" in ws[1]\n    assert \"WARNING: cpp:any reference target not found: Unknown::Unknown::Unknown\" in ws[2]\n", "def test_template_typedefs():\n    check('type', 'template<typename T> using Vec = std::vector<T>', {2: 'I0E3Vec'})\n    check('type', 'template<typename T> using Vec<T> = std::vector<T>', {2: 'I0E3VecI1TE'})\n    check('type', 'template<typename T, std::size_t N> using Arr = std::array<T, N>', {2: 'I00E3Arr'})\n", "def test_multiple_declarations():\n    check('function', 'void f(int); void f(float)', {1: \"f__i\", 2: \"1fLi\", 3: \"f__f\", 4: \"1fLf\"})\n    check('function', 'template<typename T> void f(T); template<typename T> void f(T, T)',\n          {2: \"I0E1f1T\", 3: \"I0E1f2T\"})\n    check('member', 'int a; int a', {1: \"a__i\", 2: \"1a\"})\n    check('member', 'int a; static int a', {1: \"a__i\", 2: \"1a\"})\n    check('member', 'void f(); void f()', {1: \"f\", 2: \"1fv\", 3: \"f\", 4: \"1fvv\"})\n    check('member', 'void f(int); void f(float)', {1: \"f__i\", 2: \"1fi\", 3: \"f__f\", 4: \"1ff\"})\n    check('member', 'template<typename T> void f(T); template<typename T> void f(T, T)',\n          {2: \"I0E1f1T\", 3: \"I0E1f2T\"})\n", "def test_xref_consistency_cpp_expr_role():\n        pattern = r'<span class=\"cpp-expr\">%s</span>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    f = 'cpp-expr-role.html'\n    t = (app.outdir / f).read_text()\n\n    # Check that the correct classes are assigned to the generated nodes\n    assert classes('cpp-expr', 'span') == {'cpp-expr'}\n\n    # Check that the correct content is rendered\n    check(r'\\(int\\)2', t, f)\n    check(r'5 + 42', t, f)\n", "def test_xref_consistency_member_function():\n    output = (app.outdir / \"xref_consistency.html\").read_text()\n\n    # Check that the class name and member function name are correctly linked\n    pattern = r'<a class=\"cpp cpp-class\" href=\"#c-class-cpp\">C::Class</a>::<a class=\"cpp cpp-function\" href=\"#c-class-f-member-cpp\">f_member</a>'\n    assert re.search(pattern, output), f\"Pattern {pattern} not found in output\"\n\n    # Check that the class name is correctly linked when the member function is in an unnamed namespace\n    pattern = r'<a class=\"cpp cpp-class\" href=\"#c-class-cpp\">C::Class</a>::<a class=\"cpp cpp-function\" href=\"#anonymous-namespace-0-g-cpp\">g</a>'\n    assert re.search(pattern, output), f\"Pattern {pattern} not found in output\"\n"], "sample_1159": ["def test_issue_16978():\n    c = Symbol('c', complex=True)\n    assert c.is_finite is True\n    assert c.is_real is None\n    assert c.is_imaginary is None\n", "def test_issue_17807():\n    n = Symbol('n', integer=True)\n    p = Symbol('p', prime=True)\n    assert (n*p).is_composite is None  # n could be 1\n", "def test_issue_17906():\n    x = Symbol('x', integer=True, positive=True)\n    assert (x**2).is_prime is False\n    assert (x**2).is_composite is True\n    x = Symbol('x', integer=True)\n    assert (x**2).is_prime is None\n    assert (x**2).is_composite is None\n", "def test_issue_17579():\n    # Test for issue 17579: Add test for complex property in assumptions\n    z = 2 + 3j\n    assert z.is_complex is True\n    assert z.is_real is False\n    assert z.is_imaginary is False\n\n    z = 3j\n    assert z.is_complex is True\n    assert z.is_real is False\n    assert z.is_imaginary is True\n\n    z = 0\n    assert z.is_complex is True\n    assert z.is_real is True\n    assert z.is_imaginary is False\n", "def test_issue_17803():\n    # Test for issue #17803:\n    # Symbol.is_imaginary should return False for complex numbers with real part\n    x = Symbol('x', real=True)\n    z = x + I\n    assert z.is_imaginary is True\n\n    # For complex numbers with non-zero real part, is_imaginary should be None\n    y = Symbol('y')\n    w = x + I*y\n    assert w.is_imaginary is None\n\n    # For real numbers, is_imaginary should be False\n    assert x.is_imaginary is False\n"], "sample_1173": ["def test_implicit_multiplication_application():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    theta = Symbol('theta')\n    assert parse_expr(\"10sin**2 x**2 + 3xyz + tan theta\", transformations=transformations) == 3*x*y*z + 10*sin(x**2)**2 + tan(theta)\n", "def test_implicit_multiplication_application():\n    transformations = (standard_transformations + (implicit_multiplication_application,))\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    f = Function('f')\n    assert parse_expr(\"xyz\", transformations=transformations) == x*y*z\n    assert parse_expr(\"f(x)y\", transformations=transformations, local_dict={'f': f}) == f(x)*y\n", "def test_split_symbols_custom_predicate():\n        if symbol not in ('list', 'of', 'unsplittable', 'names'):\n                return _token_splittable(symbol)\n        return False\n\n    transformations = standard_transformations + (split_symbols_custom(can_split), implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    list_ = Symbol('list')\n    of = Symbol('of')\n    unsplittable = Symbol('unsplittable')\n    names = Symbol('names')\n\n    assert parse_expr(\"xyz\") == x*y*z\n    assert parse_expr(\"xyz\", transformations=transformations) == x*y*z\n    assert parse_expr(\"listof\") == list_*of\n    assert parse_expr(\"listof\", transformations=transformations) == list_*of\n    assert parse_expr(\"unsplittable\") == unsplittable\n    assert parse_expr(\"unsplittable\", transformations=transformations) == unsplittable\n    assert parse_expr(\"names\") == names\n    assert parse_expr(\"names\", transformations=transformations) == names\n", "def test_issue_11382():\n    inputs = {\n        'sin(x)**2': 'sin(x)**2',\n        'sin(x)**-2': 'sin(x)**(-2)',\n        'sin(x)**-1*2': 'sin(x)**(-1)*2',\n        'sin(x)**-1*cos(x)**2': 'sin(x)**(-1)*cos(x)**2',\n        'sin(x)**2*cos(x)': 'sin(x)**2*cos(x)',\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, evaluate=False) == parse_expr(result, evaluate=False)\n", "def test_split_symbols_function_exponent():\n    transformations = standard_transformations + \\\n                      (split_symbols, implicit_multiplication, function_exponentiation)\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Symbol('a')\n    f = Function('f')\n    g = Function('g')\n\n    assert parse_expr(\"af^y(x+1)\", transformations=transformations,\n                      local_dict={'f':f}) == a*f(x+1)**y\n    assert parse_expr(\"ag^y(x+1)\", transformations=transformations,\n                      local_dict={'g':g}) == a*g(x+1)**y\n"], "sample_1034": ["def test_apply_grover():\n    numqubits = 2\n    oracle = return_one_on_two\n    expected = IntQubit(2, nqubits=numqubits)\n    assert qapply(apply_grover(oracle, numqubits)) == expected\n", "def test_apply_grover():\n    numqubits = 2\n    oracle = return_one_on_two\n    expected = IntQubit(2, nqubits=numqubits)\n    assert qapply(apply_grover(oracle, numqubits)) == expected\n", "def test_apply_grover():\n    numqubits = 2\n    expected = IntQubit(2, numqubits)\n    assert qapply(apply_grover(return_one_on_two, numqubits)) == expected\n", "def test_apply_grover():\n    nqubits = 2\n    oracle = return_one_on_two\n    result = apply_grover(oracle, nqubits)\n    expected = IntQubit(2, nqubits=nqubits)\n    assert qapply(result) == expected\n", "def test_apply_grover():\n    nqubits = 2\n    oracle = return_one_on_two\n    expected = IntQubit(2, nqubits=nqubits)\n    assert qapply(apply_grover(oracle, nqubits)) == expected\n"], "sample_437": ["def test_no_logs_without_debug_or_force_debug_cursor(self):\n    with self.assertNoLogs(\"django.db.backends\", \"DEBUG\"):\n        with self.assertRaises(Exception), transaction.atomic():\n            Person.objects.create(first_name=\"first\", last_name=\"last\")\n            raise Exception(\"Force rollback\")\n\n        conn = connections[DEFAULT_DB_ALIAS]\n        self.assertEqual(len(conn.queries_log), 0)\n", "def test_health_checks_enabled_after_rollback(self):\n    self.patch_settings_dict(conn_health_checks=True)\n    self.assertIsNone(connection.connection)\n    # Newly created connections are considered healthy without performing\n    # the health check.\n    with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n        self.run_query()\n\n    old_connection = connection.connection\n    # Simulate rollback in a transaction.\n    with transaction.atomic():\n        self.run_query()\n        connection.rollback()\n\n    # Simulate request_finished.\n    connection.close_if_unusable_or_obsolete()\n    # The underlying connection is being reused further with health checks\n    # succeeding.\n    self.run_query()\n    self.assertIs(old_connection, connection.connection)\n", "    def test_savepoint_commit(self):\n        conn = connections[DEFAULT_DB_ALIAS]\n        sid = conn.savepoint()\n        self.assertIsNotNone(sid)\n        self.assertEqual(len(conn.savepoint_ids), 0)\n        conn.savepoint_commit(sid)\n        self.assertEqual(len(conn.savepoint_ids), 0)\n", "    def test_validate_thread_sharing_allowed(self):\n        connection.inc_thread_sharing()\n        connection.validate_thread_sharing()\n        connection.dec_thread_sharing()\n", "    def test_validate_thread_sharing(self):\n        # Test that validation error is raised when accessing connection from another thread\n        with self.assertRaises(DatabaseError):\n            with threading.Thread(target=connection.cursor):\n                connection.cursor()\n\n        # Test that validation doesn't raise error when accessing connection from same thread\n        connection.validate_thread_sharing()\n\n        # Test that validation error is raised when accessing connection from another thread after allow_thread_sharing is set\n        connection.inc_thread_sharing()\n        connection.validate_thread_sharing()\n        connection.dec_thread_sharing()\n\n        # Test that validation error is raised when allow_thread_sharing is set but not accessed from the same thread\n        with self.assertRaises(DatabaseError):\n            with threading.Thread(target=connection.cursor):\n                connection.cursor()\n"], "sample_1155": ["def test_construct_domain_with_composite():\n    dom = ZZ[x][y]\n    assert construct_domain([2*x*y, 3*x]) == \\\n        (dom, [dom.convert(2*x*y), dom.convert(3*x)])\n\n    dom = QQ[x][y]\n    assert construct_domain([x/2*y, 3*x]) == \\\n        (dom, [dom.convert(x/2*y), dom.convert(3*x)])\n", "def test_construct_domain_with_symbols_and_extensions():\n    dom = QQ[x].algebraic_field(sqrt(2))\n    assert construct_domain([sqrt(2)*x, x/2, sqrt(2)], extension=True) == \\\n        (dom, [dom.convert(sqrt(2)*x), dom.convert(x/2), dom.convert(sqrt(2))])\n", "def test_rational_functions_with_symbols_and_fractions():\n    dom = ZZ.frac_field(x, y)\n    assert construct_domain([2/(x+y), 3*y/(x-1)]) == \\\n        (dom, [dom.convert(2/(x+y)), dom.convert(3*y/(x-1))])\n", "def test_rational_functions():\n    dom = ZZ.frac_field(x)\n\n    assert construct_domain([2/(x+1), 3]) == \\\n        (dom, [dom.convert(2/(x+1)), dom.convert(3)])\n\n    dom = QQ.frac_field(x, y)\n\n    assert construct_domain([2/(x+1), 3*y]) == \\\n        (dom, [dom.convert(2/(x+1)), dom.convert(3*y)])\n", "def test_algebraic_numbers():\n    g = GoldenRatio\n    c = Catalan\n    alg = QQ.algebraic_field(g)\n    assert construct_domain([g, 1/g, c], extension=True) == (\n        alg,\n        [alg.convert(g),\n         alg.convert(1/g),\n         alg.convert(c)]\n    )\n"], "sample_1037": ["def test_MatrixElement_simplify():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    assert simplify(A[0, 0]*B[0, 0]) == A[0, 0]*B[0, 0]\n    assert simplify(A[0, 0]*A[0, 0]) == (A[0, 0])**2\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    matmul_expr = MatMul(A, B, C)\n    # Testing doit() with deep=True\n    result_deep = matmul_expr.doit(deep=True)\n    assert isinstance(result_deep, MatMul)\n    # Testing doit() with deep=False\n    result_shallow = matmul_expr.doit(deep=False)\n    assert result_shallow == matmul_expr\n", "def test_MatrixElement_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = Matrix([[1, 2], [3, 4]])\n\n    assert A[0, 0].subs(A, C) == C[0, 0]\n    assert (A*B)[1, 0].subs({A: C, B: C.inv()}) == C[1, 0] * C.inv()[0, 0]\n", "def test_MatMul_simplification():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n    assert MatMul(A, B, C).doit() == MatMul(MatMul(A, B), C)\n    assert MatMul(A, Identity(2), B).doit() == MatMul(A, B)\n    assert MatMul(A, ZeroMatrix(2, 2), B).doit() == ZeroMatrix(2, 2)\n    assert MatMul(A, Inverse(A)).doit() == Identity(2)\n    assert MatMul(A, Transpose(A)).doit() == MatMul(A, A.T)\n    assert MatMul(A, Adjoint(A)).doit() == MatMul(A, A.H)\n", "def test_MatMul_simplify():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, p)\n    D = MatrixSymbol('D', p, n)\n    E = MatrixSymbol('E', m, m)\n\n    assert simplify(MatMul(A, E.inv(), B)) == MatMul(A, B)\n    assert simplify(MatMul(C, D, A)) == MatMul(A)\n    assert simplify(MatMul(A, Identity(m), B)) == MatMul(A, B)\n"], "sample_1063": ["def test_abs_numpy():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    f = lambdify(x, Abs(x), \"numpy\")\n    assert f(-1) == 1\n    assert f(1) == 1\n    assert f(3+4j) == 5\n", "def test_issue_17000():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    x = symbols(\"x\")\n    f = lambdify(x, sqrt(x), modules='scipy')\n    assert abs(f(4) - sqrt(4)) <= 1e-10\n", "def test_issue_17227():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    x = symbols(\"x\")\n    f = lambdify(x, besselj(0, x), modules='scipy')\n    assert abs(f(0) - besselj(0, 0).evalf()) <= 1e-10\n", "def test_issue_17047():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    x = symbols(\"x\")\n    f = lambda x: sqrt(sin(x))\n    f_ = lambdify(x, f(x), modules='scipy')\n    assert abs(sqrt(sin(0.5)) - f_(0.5)) <= 1e-10\n", "def test_issue_17648():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    from sympy.physics.quantum import TensorProduct\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = TensorProduct(A, B)\n    f = lambdify(A, C)\n    A_val = numpy.array([[1, 2], [3, 4]])\n    B_val = numpy.array([[5, 6], [7, 8]])\n    expected_result = numpy.kron(A_val, B_val)\n    assert numpy.array_equal(f(A_val), expected_result)\n"], "sample_586": ["def test_concat_different_coords(self):\n    ds1 = Dataset({\"a\": 1}, coords={\"x\": 1, \"y\": 2})\n    ds2 = Dataset({\"a\": 2}, coords={\"x\": 1, \"y\": 3})\n    expected = Dataset({\"a\": (\"z\", [1, 2]), \"x\": 1, \"y\": (\"z\", [2, 3])}, coords={\"z\": [0, 1]})\n    actual = concat([ds1, ds2], dim=\"z\", coords=\"different\")\n    assert_identical(actual, expected)\n", "def test_concat_different_data_vars(self):\n    ds1 = Dataset({\"a\": 1, \"b\": 2})\n    ds2 = Dataset({\"a\": 3, \"c\": 4})\n    expected = Dataset({\"a\": (\"dim\", [1, 3]), \"b\": 2, \"c\": 4}, coords={\"dim\": [0, 1]})\n    actual = concat([ds1, ds2], dim=\"dim\", data_vars=\"different\")\n    assert_identical(expected, actual)\n", "def test_concat_data_vars_all(self):\n    data = Dataset({\"foo\": (\"x\", np.random.randn(10)), \"bar\": (\"x\", np.random.randn(10))})\n    objs = [data.isel(x=slice(5)), data.isel(x=slice(5, None))]\n    for data_vars in [\"all\", [\"foo\", \"bar\"]]:\n        actual = concat(objs, dim=\"x\", data_vars=data_vars)\n        assert_identical(data, actual)\n", "def test_concat_compat_override(self):\n    ds1 = Dataset(\n        {\n            \"has_x_y\": ((\"y\", \"x\"), [[1, 2]]),\n            \"has_x\": (\"x\", [1, 2]),\n            \"no_x_y\": (\"z\", [1, 2]),\n        },\n        coords={\"x\": [0, 1], \"y\": [0], \"z\": [-1, -2]},\n    )\n    ds2 = Dataset(\n        {\n            \"has_x_y\": ((\"y\", \"x\"), [[3, 4]]),\n            \"has_x\": (\"x\", [1, 3]),\n            \"no_x_y\": ((\"q\", \"z\"), [[1, 2]]),\n        },\n        coords={\"x\": [0, 1], \"y\": [1], \"z\": [-1, -2], \"q\": [0]},\n    )\n\n    result = concat([ds1, ds2], dim=\"y\", data_vars=\"minimal\", compat=\"override\")\n    assert_equal(ds1[\"has_x\"], result[\"has_x\"])\n    assert_equal(ds2[\"no_x_y\"], result[\"no_x_y\"].transpose())\n", "def test_concat_with_positions(self):\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2, 3])}, {\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"foo\": (\"x\", [4, 5, 6])}, {\"x\": [3, 4, 5]})\n\n    expected = Dataset({\"foo\": (\"x\", [1, 2, 4, 5, 3, 6])}, {\"x\": [0, 1, 3, 4, 2, 5]})\n\n    actual = concat([ds1, ds2], dim=\"x\", positions=[list(range(3)), list(range(3, 6))])\n\n    assert_identical(expected, actual)\n"], "sample_780": ["def test_lda_transform_before_fit():\n    # test `transform` before `fit`\n    rng = np.random.RandomState(0)\n    X = rng.randint(4, size=(20, 10))\n    lda = LatentDirichletAllocation()\n    regex = (\"This LatentDirichletAllocation instance is not fitted yet. \"\n             \"Call 'fit' with appropriate arguments before using this method.\")\n    assert_raises_regexp(NotFittedError, regex, lda.transform, X)\n", "def test_lda_transform_before_fit():\n    # test `transform` before `fit`\n    rng = np.random.RandomState(0)\n    X = rng.randint(4, size=(20, 10))\n    lda = LatentDirichletAllocation()\n    regex = (\"This LatentDirichletAllocation instance is not fitted yet. \"\n             \"Call 'fit' with appropriate arguments before using this method.\")\n    assert_raises_regexp(NotFittedError, regex, lda.transform, X)\n", "def test_lda_sparse_input():\n    # Test LDA with sparse input.\n    # Transform result should be the same as the dense input\n    rng = np.random.RandomState(0)\n    n_components, X_dense = _build_sparse_mtx()\n    X_sparse = csr_matrix(X_dense)\n    lda_dense = LatentDirichletAllocation(n_components=n_components,\n                                          learning_method='batch', random_state=rng)\n    lda_sparse = LatentDirichletAllocation(n_components=n_components,\n                                           learning_method='batch', random_state=rng)\n    lda_dense.fit(X_dense)\n    lda_sparse.fit(X_sparse)\n    X_trans_dense = lda_dense.transform(X_dense)\n    X_trans_sparse = lda_sparse.transform(X_sparse)\n    assert_array_almost_equal(X_trans_dense, X_trans_sparse)\n", "def test_lda_negative_prior():\n    # test pass negative value for prior parameters\n    X = np.ones((5, 10))\n    lda = LatentDirichletAllocation(doc_topic_prior=-0.5, topic_word_prior=-0.5)\n    regex = r\"^Prior parameters should be non-negative\"\n    assert_raises_regexp(ValueError, regex, lda.fit, X)\n", "def test_lda_transform_before_fit(method):\n    # Test LDA transform before fit\n    # Should raise NotFittedError\n    rng = np.random.RandomState(0)\n    X = rng.randint(10, size=(50, 20))\n    lda = LatentDirichletAllocation(n_components=5, learning_method=method,\n                                    random_state=rng)\n    regex = (\"This LatentDirichletAllocation instance is not fitted yet. \"\n             \"Call 'fit' with appropriate arguments before using this method.\")\n    assert_raises_regexp(NotFittedError, regex, lda.transform, X)\n"], "sample_1075": ["def test_beta_function_differentiation():\n    x, y = Symbol('x'), Symbol('y')\n    assert diff(beta(x, y), x) == (digamma(x) - digamma(x + y)) * beta(x, y)\n    assert diff(beta(x, y), y) == (digamma(y) - digamma(x + y)) * beta(x, y)\n    assert raises(ArgumentIndexError, lambda: diff(beta(x, y), 3))\n", "def test_beta_function_symmetry():\n    x, y = Symbol('x'), Symbol('y')\n    assert beta(x, y) == beta(y, x)\n", "def test_beta_function_properties():\n    x, y = Symbol('x'), Symbol('y')\n\n    # Testing property B(a, 1) = 1/a\n    assert beta(x, 1) == 1/x\n\n    # Testing property B(a, b) = B(b, a)\n    assert beta(x, y) == beta(y, x)\n\n    # Testing derivative of beta function with respect to x\n    assert diff(beta(x, y), x) == beta(x, y)*(digamma(x) - digamma(x + y))\n\n    # Testing derivative of beta function with respect to y\n    assert diff(beta(x, y), y) == beta(x, y)*(digamma(y) - digamma(x + y))\n\n    # Testing conjugate of beta function\n    assert conjugate(beta(x, y)) == beta(conjugate(x), conjugate(y))\n\n    # Testing expansion of beta function in terms of gamma function\n    assert expand_func(beta(x, y)) == gamma(x)*gamma(y) / gamma(x + y)\n\n    # Testing argument index error for beta function\n    with raises(ArgumentIndexError):\n        beta(x, y).fdiff(3)\n", "def test_beta_function():\n    x, y = Symbol('x'), Symbol('y')\n\n    # Test the fdiff method\n    assert diff(beta(x, y), x) == beta(x, y)*(digamma(x) - digamma(x + y))\n    assert diff(beta(x, y), y) == beta(x, y)*(digamma(y) - digamma(x + y))\n\n    # Test the _eval_expand_func method\n    assert expand_func(beta(x, y)) == gamma(x)*gamma(y) / gamma(x + y)\n\n    # Test the _eval_is_real method\n    assert beta(x, y)._eval_is_real() is None\n    assert beta(1, 1)._eval_is_real() is True\n    assert beta(1 + I, 1)._eval_is_real() is False\n\n    # Test the _eval_conjugate method\n    assert conjugate(beta(x, y)) == beta(conjugate(x), conjugate(y))\n\n    # Test the _eval_rewrite_as_gamma method\n    assert beta(x, y)._eval_rewrite_as_gamma() == gamma(x)*gamma(y) / gamma(x + y)\n\n    # Test ArgumentIndexError exception\n    with raises(ArgumentIndexError):\n        beta(x, y).fdiff(3)\n", "def test_beta_differentiation():\n    x, y = Symbol('x'), Symbol('y')\n    assert diff(beta(x, y), x, 2) == (-digamma(1, x + y) + digamma(2, x + y))*beta(x, y)\n    assert diff(beta(x, y), y, 2) == (-digamma(1, x + y) + digamma(2, x + y))*beta(x, y)\n    assert diff(beta(x, y), x, y) == -beta(x, y)*polygamma(1, x + y)\n"], "sample_906": ["def test_domain_cpp_parse_noindex(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \"   :noindex:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, desc))\n    assert_node(doctree[0], desc, noindex=False)\n    assert_node(doctree[1], desc, noindex=True)\n", "def test_domain_cpp_ast_concept_definitions():\n    check('concept', 'template<typename Param> {key}A::B::Concept',\n          {2: 'I0EN1A1B7ConceptE'})\n    check('concept', 'template<typename A, typename B, typename ...C> {key}Foo',\n          {2: 'I00DpE3Foo'})\n    # Test with no template\n    with pytest.raises(DefinitionError):\n        parse('concept', '{key}Bar')\n    # Test with multiple templates\n    with pytest.raises(DefinitionError):\n        parse('concept', 'template<typename T> template<typename U> {key}Baz')\n", "def test_domain_cpp_build_anon_dup_decl_with_enum_struct(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"anon-dup-decl\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:identifier reference target not found: @a\" in ws[0]\n    assert \"WARNING: cpp:identifier reference target not found: @b\" in ws[1]\n", "def test_domain_cpp_ast_explicit_instantiation():\n    check('function', 'explicit instantiation void f<int>(int)', {1: 'f__i', 2: '1fIiE', 4: '1fIiEv'},\n          output='template void f<int>(int); /*explicit instantiation*/')\n    check('function', 'explicit instantiation void f<int>(int) = delete', {1: 'f__i', 2: '1fIiE', 4: '1fIiEv'},\n          output='template void f<int>(int) = delete; /*explicit instantiation*/')\n    check('member', 'explicit instantiation A a', {1: 'a__A', 2: '1a'},\n          output='template A a; /*explicit instantiation*/')\n", "def test_domain_cpp_ast_function_parameters():\n    check('function', 'void f(int a, double b)', {1: 'f__i.d', 2: '1fid'})\n    check('function', 'void f(int a, double b, ...)', {1: 'f__i.d.z', 2: '1fidz'})\n    check('function', 'void f(int a, double b = 3.14)', {1: 'f__i.d', 2: '1fid'})\n    check('function', 'void f(int a = 42, double b = 3.14)', {1: 'f__i.d', 2: '1fid'})\n    check('function', 'void f(const int a, volatile double b)', {1: 'f__iC.dV', 2: '1fKiVd'})\n    check('function', 'void f(int a, double b[3])', {1: 'f__i.dA', 2: '1fidA3d'})\n    check('function', 'void f(int a, double b[3][4])', {1: 'f__i.dA', 2: '1fidA3A4d'})\n    check('function', 'void f(int a, double b[][4])', {1: 'f__i.dA', 2: '1fidA4d'})\n    check('function', 'void f(int a, double (*b)(int))', {1: 'f__i.PFidE', 2: '1fidPFidE'})\n    check('function', 'void f(int a, double b(int))', {1: 'f__i.FidE', 2: '1fidFidE'})\n    check('function', 'void f(int a, double b(void))', {1: 'f__i.FidE', 2: '1fidFidEv'})\n    check('function', 'void f(int a, double b(int) = nullptr)', {1: 'f__i.FidE"], "sample_825": ["def test_pls_algorithm_errors():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    # Test invalid algorithm parameter\n    clf = pls_.PLSCanonical(algorithm=\"invalid_algorithm\")\n    assert_raise_message(ValueError, \"Got algorithm invalid_algorithm when only 'svd' and 'nipals' are known\",\n                         clf.fit, X, Y)\n", "def test_pls_nipals_svd_consistency():\n    # Test that the NIPALS and SVD algorithms produce consistent results\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls_bynipals = pls_.PLSCanonical(n_components=X.shape[1], algorithm=\"nipals\")\n    pls_bysvd = pls_.PLSCanonical(algorithm=\"svd\", n_components=X.shape[1])\n    pls_bynipals.fit(X, Y)\n    pls_bysvd.fit(X, Y)\n    assert_array_almost_equal(pls_bynipals.x_scores_, pls_bysvd.x_scores_, decimal=4,\n                              err_msg=\"NIPALS and SVD implementations lead to different x scores\")\n    assert_array_almost_equal(pls_bynipals.y_scores_, pls_bysvd.y_scores_, decimal=4,\n                              err_msg=\"NIPALS and SVD implementations lead to different y scores\")\n", "def test_pls_cca_comparison():\n    # Compare PLSCanonical with CCA for the same parameters\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls_cca = pls_.PLSCanonical(n_components=2)\n    cca = CCA(n_components=2)\n\n    X_pls, Y_pls = pls_cca.fit_transform(X, Y)\n    X_cca, Y_cca = cca.fit_transform(X, Y)\n\n    assert_array_almost_equal(X_pls, X_cca, decimal=4)\n    assert_array_almost_equal(Y_pls, Y_cca, decimal=4)\n", "def test_pls_fit_transform():\n    # Test if fit_transform method returns the same result as fit and transform separately\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        X_score_fit, Y_score_fit = clf.transform(X, Y)\n        X_score_fit_transform, Y_score_fit_transform = clf.fit_transform(X, Y)\n        assert_array_almost_equal(X_score_fit, X_score_fit_transform, decimal=4)\n        assert_array_almost_equal(Y_score_fit, Y_score_fit_transform, decimal=4)\n", "def test_pls_modes():\n    # Check that the 'A' and 'B' modes of PLS give different results\n    X = np.random.rand(50, 3)\n    Y = np.random.rand(50, 2)\n\n    pls_A = pls_.PLSCanonical(n_components=2, mode='A')\n    pls_B = pls_.PLSCanonical(n_components=2, mode='B')\n\n    pls_A.fit(X, Y)\n    pls_B.fit(X, Y)\n\n    # Check that the scores are different\n    assert not np.allclose(pls_A.x_scores_, pls_B.x_scores_)\n    assert not np.allclose(pls_A.y_scores_, pls_B.y_scores_)\n\n    # Check that the weights are different\n    assert not np.allclose(pls_A.x_weights_, pls_B.x_weights_)\n    assert not np.allclose(pls_A.y_weights_, pls_B.y_weights_)\n\n    # Check that the loadings are different\n    assert not np.allclose(pls_A.x_loadings_, pls_B.x_loadings_)\n    assert not np.allclose(pls_A.y_loadings_, pls_B.y_loadings_)\n"], "sample_1004": ["def test_contains_CondSet():\n    c = ConditionSet(x, x > 0, S.Reals)\n    assert c.contains(1)\n    assert not c.contains(-1)\n    assert not c.contains(0)\n", "def test_CondSet_union():\n    input_conditionset1 = ConditionSet(x, x < 2, Interval(1, 4, False, False))\n    input_conditionset2 = ConditionSet(x, x > 3, Interval(2, 5, False, False))\n    output_conditionset = ConditionSet(x, x != 2, Interval(1, 5, False, False))\n    assert Union(input_conditionset1, input_conditionset2) == output_conditionset\n", "def test_CondSet_union():\n    input_conditionset1 = ConditionSet(x, x**2 > 4, Interval(1, 3, False, False))\n    input_conditionset2 = ConditionSet(x, x > 2, Interval(0, 5, False, False))\n    output_conditionset = ConditionSet(x, x**2 > 4, Interval(0, 5, False, False))\n    assert Union(input_conditionset1, input_conditionset2) == output_conditionset\n", "def test_CondSet_union():\n    input_conditionset1 = ConditionSet(x, x**2 > 4, Interval(1, 4, False, False))\n    input_conditionset2 = ConditionSet(x, x < 0, Interval(-3, 3, False, False))\n    output_conditionset = ConditionSet(x, (x**2 > 4) | (x < 0), Interval(-3, 4, False, False))\n    assert Union(input_conditionset1, input_conditionset2) == output_conditionset\n", "def test_CondSet_union():\n    set1 = ConditionSet(x, x > 0, Interval(0, 5))\n    set2 = ConditionSet(x, x < 3, Interval(1, 6))\n    expected_set = ConditionSet(x, Or(x > 0, x < 3), Interval(0, 6))\n    assert Union(set1, set2) == expected_set\n"], "sample_958": ["def test_domain_cpp_build_warn_on_allowed_pre_v3_false(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"warn-on-allowed-pre-v3\")\n    assert len(ws) == 0\n", "def test_domain_cpp_ast_static_assert():\n    check('function', 'static_assert(sizeof(int) == 4)', {1: None, 2: None})\n    check('function', 'static_assert(sizeof(int) == 4, \"error message\")', {1: None, 2: None})\n", "def test_domain_cpp_parse_index():\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :noindex:\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \"   :noindex:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[])\n    assert_node(doctree[2], addnodes.index, entries=[])\n", "def test_domain_cpp_ast_varargs():\n    check('function', 'void f(int a, ...)', {1: 'f__i.z', 2: '6fiz'})\n    check('function', 'void f(int a, int b, ...)', {1: 'f__i.i.z', 2: '6fiiz'})\n    check('function', 'void f(int a, int b, ..., int c)', {1: 'f__i.i.z.i', 2: '6fiiiz'})\n", "def test_domain_cpp_build_noindexentry(app, status, warning):\n    app.builder.build_all()\n    output = (app.outdir / 'noindexentry.html').read_text()\n\n        pattern = r'<li><a href=\"{}\">{}</a></li>'.format(expected[0], expected[1])\n        assert pattern in text, f\"Pattern {pattern} not found in {text}\"\n\n    check_index(output, ('_CPPv41fv', 'f (C++ function)'))\n    assert '_CPPv41gv' not in output\n"], "sample_303": ["def test_runshell(self, mock_subprocess_run):\n    # Mock the settings_to_cmd_args_env method\n    self.client.settings_to_cmd_args_env = mock.MagicMock(return_value=(['mock_command'], {'MOCK_ENV': 'value'}))\n\n    self.client.runshell(parameters={'mock': 'parameters'})\n\n    # Check if subprocess.run was called with the correct arguments\n    mock_subprocess_run.assert_called_once_with(['mock_command'], env={'MOCK_ENV': 'value', **os.environ}, check=True)\n", "def test_runshell(self):\n    with mock.patch('subprocess.run') as mock_run:\n        with mock.patch.object(self.client, 'settings_to_cmd_args_env', return_value=(['cmd'], {'ENV': 'value'})):\n            self.client.runshell('params')\n            mock_run.assert_called_once_with(['cmd'], env={'ENV': 'value', **os.environ})\n", "    def test_runshell(self, mock_subprocess_run):\n        mock_args = ['mock_command']\n        mock_env = {'mock_env': 'value'}\n        self.client.settings_to_cmd_args_env = mock.MagicMock(return_value=(mock_args, mock_env))\n\n        self.client.runshell({})\n\n        self.client.settings_to_cmd_args_env.assert_called_once_with(self.client.connection.settings_dict, {})\n        mock_subprocess_run.assert_called_once_with(mock_args, env={**os.environ, **mock_env}, check=True)\n", "def test_runshell(self):\n    mock_settings_dict = {'key': 'value'}\n    mock_parameters = {}\n    mock_args = ['mock_command']\n    mock_env = {'key': 'value'}\n    self.client.connection.settings_dict = mock_settings_dict\n    self.client.settings_to_cmd_args_env = mock.MagicMock(return_value=(mock_args, mock_env))\n\n    with mock.patch('subprocess.run') as mock_subprocess_run:\n        self.client.runshell(mock_parameters)\n        self.client.settings_to_cmd_args_env.assert_called_once_with(mock_settings_dict, mock_parameters)\n        mock_subprocess_run.assert_called_once_with(mock_args, env={**os.environ, **mock_env}, check=True)\n", "    def test_runshell(self, mock_subprocess_run):\n        self.client.settings_to_cmd_args_env = mock.MagicMock(return_value=(['mock_arg'], {'mock_env': 'value'}))\n        self.client.runshell(parameters={'mock_param': 'value'})\n        mock_subprocess_run.assert_called_once_with(['mock_arg'], env={'**os.environ, **env'}, check=True)\n"], "sample_1126": ["def test_operator_dagger():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_operator_dagger():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_operators():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_operator_commutativity():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n", "def test_dagger_operator():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n"], "sample_1117": ["def test_matrix_element_sets_transpose():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n", "def test_matrix_element_sets_slices_blocks_complex():\n    from sympy.matrices.expressions import BlockMatrix\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4, complex=True)\n    assert ask(Q.complex_elements(X[:, 3]), Q.complex_elements(X))\n    assert ask(Q.complex_elements(BlockMatrix([[Y], [Y]])),\n                        Q.complex_elements(Y))\n", "def test_integer_elements():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.integer_elements(X + Y), Q.integer_elements(X) & Q.integer_elements(Y))\n    assert ask(Q.integer_elements(X - Y), Q.integer_elements(X) & Q.integer_elements(Y))\n    assert ask(Q.integer_elements(X * Y), Q.integer_elements(X) & Q.integer_elements(Y))\n    assert ask(Q.integer_elements(X**2), Q.integer_elements(X))\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n    assert ask(Q.integer_elements(Trace(X)), Q.integer_elements(X))\n    assert ask(Q.integer_elements(Determinant(X)), Q.integer_elements(X))\n    alpha = Symbol('alpha', integer=True)\n    assert ask(Q.integer_elements(alpha*X), Q.integer_elements(X))\n    assert ask(Q.integer_elements(LofLU(X)), Q.integer_elements(X))\n    e = Symbol('e', integer=True, positive=True)\n    assert ask(Q.integer_elements(X**e), Q.integer_elements(X))\n", "def test_factorization():\n    X = MatrixSymbol('X', 3, 3)\n    Y = MatrixSymbol('Y', 3, 3)\n    LU = LofLU(X)\n    assert ask(Q.orthogonal(LU), Q.orthogonal(X) & Q.upper_triangular(LU))\n    assert ask(Q.unitary(LU), Q.unitary(X) & Q.upper_triangular(LU))\n    assert ask(Q.diagonal(LU), Q.diagonal(X) & Q.upper_triangular(LU))\n    assert ask(Q.upper_triangular(LU), Q.upper_triangular(X) & Q.upper_triangular(LU))\n    assert ask(Q.lower_triangular(LU), Q.lower_triangular(X) & Q.lower_triangular(LU))\n    assert ask(Q.orthogonal(LU*Y), Q.orthogonal(LU) & Q.orthogonal(Y))\n    assert ask(Q.unitary(LU*Y), Q.unitary(LU) & Q.unitary(Y))\n    assert ask(Q.diagonal(LU*Y), Q.diagonal(LU) & Q.diagonal(Y))\n    assert ask(Q.upper_triangular(LU*Y), Q.upper_triangular(LU) & Q.upper_triangular(Y))\n    assert ask(Q.lower_triangular(LU*Y), Q.lower_triangular(LU) & Q.lower_triangular(Y))\n", "def test_non_square_matrices():\n    X = MatrixSymbol('X', 3, 2)\n    assert not ask(Q.square(X))\n    assert ask(Q.invertible(X)) is False\n    assert ask(Q.symmetric(X)) is False\n    assert ask(Q.orthogonal(X)) is False\n    assert ask(Q.unitary(X)) is False\n    assert ask(Q.fullrank(X)) is None\n    assert ask(Q.positive_definite(X)) is False\n    assert ask(Q.upper_triangular(X)) is None\n    assert ask(Q.lower_triangular(X)) is None\n    assert ask(Q.diagonal(X)) is False\n"], "sample_1035": ["def test_grover_iteration_3():\n    numqubits = 3\n    basis_states = superposition_basis(numqubits)\n    v = OracleGate(numqubits, return_one_on_two)\n    # After (pi/4)sqrt(pow(2, n)), IntQubit(2) should have highest prob\n    # In this case, after around pi times (4 or 5)\n    iterated = grover_iteration(basis_states, v)\n    for _ in range(4):\n        iterated = qapply(iterated)\n        iterated = grover_iteration(iterated, v)\n    # In this case, probability was highest after 4 iterations\n    # Probability of Qubit('010') was 63/64 (4) vs 25/32 (5)\n    expected = (-7*basis_states)/16 + 120*IntQubit(2, numqubits)/64\n    assert qapply(expected) == iterated\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import Qubit, measure_all\n    from sympy.physics.quantum.gate import H\n    from sympy.physics.quantum.qapply import qapply\n\n    c = H(0)*H(1)*Qubit('00')\n    q = qapply(c)\n    result = measure_all(q)\n    expected = [(Qubit('00'), 1/4), (Qubit('01'), 1/4), (Qubit('10'), 1/4), (Qubit('11'), 1/4)]\n    assert result == expected\n", "def test_measure_all_oneshot():\n    from sympy.physics.quantum.qubit import measure_all_oneshot, Qubit\n    from sympy.physics.quantum.state import outerproduct\n    from sympy.physics.quantum.gate import H\n\n    # Create a superposition state\n    qubit = (H(0) * Qubit('0')) / sqrt(2) + (H(0) * Qubit('1')) / sqrt(2)\n\n    # Perform a oneshot measurement\n    result = measure_all_oneshot(qubit)\n\n    # Check that the result is either |0> or |1>\n    assert result in (Qubit('0'), Qubit('1'))\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import measure_all, Qubit\n    from sympy.physics.quantum.gate import H\n    from sympy.physics.quantum.qapply import qapply\n\n    # Create a superposition state\n    q = H(0)*H(1)*Qubit('00')\n    q = qapply(q)\n\n    # Measure all qubits\n    result = measure_all(q)\n\n    # The result should be a list of tuples, each containing a Qubit object and its probability\n    assert isinstance(result, list)\n    assert all(isinstance(item, tuple) for item in result)\n    assert all(isinstance(item[0], Qubit) for item in result)\n    assert all(isinstance(item[1], float) for item in result)\n\n    # The probabilities should sum up to 1\n    assert sum(item[1] for item in result) == 1.0\n\n    # Each possible outcome should be in the result\n    expected_outcomes = [Qubit('00'), Qubit('01'), Qubit('10'), Qubit('11')]\n    for outcome in expected_outcomes:\n        assert any(item[0] == outcome for item in result)\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import Qubit, measure_all\n    q = Qubit('00')\n    result = measure_all(q)\n    assert result == [(Qubit('00'), 1)]\n\n    q = Qubit('00') + Qubit('01')\n    result = measure_all(q)\n    assert result == [(Qubit('00'), 1/2), (Qubit('01'), 1/2)]\n\n    q = (Qubit('00') + Qubit('01')) / sqrt(2)\n    result = measure_all(q)\n    assert result == [(Qubit('00'), 1/2), (Qubit('01'), 1/2)]\n"], "sample_1116": ["def test_inverse_properties():\n    # Test that the inverse of a matrix product is the product of the inverses in reverse order\n    assert Inverse(C*D) == D.I*C.I\n\n    # Test that the inverse of a matrix power is the matrix power with the negative exponent\n    assert Inverse(C**2) == C**(-2)\n\n    # Test that the inverse of the inverse of a matrix is the matrix itself\n    assert Inverse(C.I) == C\n", "def test_inverse_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2, real=True)\n    B = Matrix([[x, 1], [2, x]])\n\n    assert (A.I).diff(x) == -A.I * A.diff(x) * A.I\n    assert (B.I).diff(x) == -B.I * B.diff(x) * B.I\n", "def test_inverse_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', n, n, real=True)\n    assert Inverse(A).diff(x) == -A.I.diff(x)*A.I\n", "def test_inverse_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 3, 3, real=True)\n    B = MatrixSymbol('B', 3, 3, real=True)\n    f = Inverse(A + x*B)\n    df = f._eval_derivative(x)\n    assert df == -A.I * B * A.I\n", "def test_matrix_pow_with_inverse():\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, -2) == Inverse(C*C)\n    assert MatPow(C, -3) == Inverse(C*C*C)\n"], "sample_779": ["def test_check_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n    from sklearn.datasets import load_boston\n    from sklearn.linear_model import LogisticRegression\n\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    e = LogisticRegression()\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # test check_class_weight_balanced_linear_classifier function\n    from sklearn.linear_model import LogisticRegression\n    check_class_weight_balanced_linear_classifier(\"LogisticRegression\", LogisticRegression)\n", "def test_check_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n    from sklearn.datasets import load_boston\n    from sklearn.base import clone\n\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    e = clone(BaseBadClassifier())\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n", "def test_check_estimator_class_weight_balanced_linear_classifier():\n    # check that check_class_weight_balanced_linear_classifier works\n    # on actual estimator\n    check_class_weight_balanced_linear_classifier(\"LinearRegression\", LinearRegression)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # check that check_class_weight_balanced_linear_classifier works on all classifiers\n    for Classifier in [LinearRegression, SGDClassifier]:\n        check_class_weight_balanced_linear_classifier(Classifier.__name__, Classifier)\n"], "sample_454": ["def test_opclasses_database_constraint(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"],\n        name=\"name_opclasses_uniq\",\n        opclasses=[\"text_pattern_ops\"],\n    )\n    msg = \"Constraint \u201cname_opclasses_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper()),\n        exclude={\"name\"},\n    )\n", "def test_validate_multiple_expressions(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        Lower(\"color\"),\n        name=\"name_color_lower_uniq\",\n    )\n    non_unique_product = UniqueConstraintProduct(\n        name=self.p1.name.upper(), color=self.p1.color.upper()\n    )\n    msg = \"Constraint \u201cname_color_lower_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\", color=\"another-color\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique fields are excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"color\"},\n    )\n", "def test_opclasses_database_constraint(self):\n    UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n", "def test_validate_custom_error_message_constraint(self):\n    constraint = models.CheckConstraint(\n        check=models.Q(price__gt=1),\n        name=\"price_gt_one\",\n        violation_error_message=\"Price must be greater than 1\",\n        violation_error_code=\"price_too_low\",\n    )\n    # Invalid product.\n    invalid_product = Product(price=0, discounted_price=5)\n    msg = \"Price must be greater than 1\"\n    with self.assertRaisesMessage(ValidationError, msg) as cm:\n        constraint.validate(Product, invalid_product)\n    self.assertEqual(cm.exception.code, \"price_too_low\")\n", "def test_tablespace_database_constraint(self):\n    table_name = UniqueConstraintProduct._meta.db_table\n    with connection.cursor() as cursor:\n        cursor.execute(\"CREATE TABLESPACE test_tablespace\")\n        try:\n            constraint = UniqueConstraint(fields=[\"name\"], name=\"name_tablespace_uniq\", tablespace=\"test_tablespace\")\n            constraint.create_sql(UniqueConstraintProduct, connection.schema_editor())\n            cursor.execute(constraint.create_sql(UniqueConstraintProduct, connection.schema_editor()))\n            UniqueConstraintProduct.objects.create(name=\"p1\")\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(name=\"p1\")\n        finally:\n            cursor.execute(\"DROP TABLESPACE test_tablespace\")\n"], "sample_1087": ["def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n"], "sample_243": ["def test_negated_transform(self):\n    query = Query(Author, alias_cols=False)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(~Q(name__lower__exact='foo'))\n    self.assertTrue(where.negated)\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "def test_filter_non_conditional_join(self):\n    query = Query(Item)\n    msg = 'Joined field references are not permitted in this query'\n    with self.assertRaisesMessage(FieldError, msg):\n        query.build_where(Q(creator__num__gt=F('note__id')))\n", "def test_filter_with_lookup(self):\n    query = Query(Item)\n    where = query.build_where(Q(name__startswith='foo'))\n    startswith = where.children[0]\n    self.assertIsInstance(startswith, StartsWith)\n    self.assertEqual(startswith.rhs, 'foo')\n    self.assertIsInstance(startswith.lhs, Col)\n    self.assertEqual(startswith.lhs.target, Item._meta.get_field('name'))\n", "def test_complex_query_with_transform(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where((Q(num__gt=2) | Q(num__lt=0)) & Q(name__lower__icontains='foo'))\n    self.assertEqual(where.connector, AND)\n\n    or_condition = where.children[0]\n    self.assertEqual(or_condition.connector, OR)\n\n    lookup = or_condition.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n\n    lookup = or_condition.children[1]\n    self.assertIsInstance(lookup, LessThan)\n    self.assertEqual(lookup.rhs, 0)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n\n    lookup = where.children[1]\n    self.assertIsInstance(lookup, Contains)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n    self.assertEqual(lookup.rhs, 'foo')\n", "def test_filter_with_custom_lookup(self):\n    class MyLookup(Lookup):\n        lookup_name = 'my_lookup'\n\n            lhs, lhs_params = self.process_lhs(compiler, connection)\n            rhs, rhs_params = self.process_rhs(compiler, connection)\n            params = lhs_params + rhs_params\n            return 'CUSTOM_LOOKUP(%s, %s)' % (lhs, rhs), params\n\n    register_lookup(CharField, MyLookup)\n    query = Query(Author)\n    where = query.build_where(Q(name__my_lookup='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, MyLookup)\n    self.assertEqual(lookup.rhs, 'foo')\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n"], "sample_1025": ["def test_PythonCodePrinter_with_fully_qualified_modules_False():\n    prntr = PythonCodePrinter(settings={'fully_qualified_modules': False})\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.doprint(acos(x)) == 'acos(x)'\n", "def test_PythonCodePrinter_complex_numbers():\n    p = PythonCodePrinter()\n    assert p.doprint(1j) == '1j'\n    assert p.doprint(complex(1, 2)) == '(1+2j)'\n", "def test_NumPyPrinter_relational():\n    p = NumPyPrinter()\n    assert p.doprint(x < y) == 'numpy.less(x, y)'\n    assert p.doprint(x <= y) == 'numpy.less_equal(x, y)'\n    assert p.doprint(x > y) == 'numpy.greater(x, y)'\n    assert p.doprint(x >= y) == 'numpy.greater_equal(x, y)'\n    assert p.doprint(x != y) == 'numpy.not_equal(x, y)'\n", "def test_CodeGenArrayTensorProduct():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    expr = CodegenArrayTensorProduct(A, B)\n    assert NumPyPrinter().doprint(expr) == 'numpy.einsum(A, [0, 2], B, [1, 3])'\n", "def test_print_complex_expression():\n    expr = acos(x) + sign(y) + z**2\n    np_output = \"numpy.arccos(x) + numpy.sign(y) + y**2\"\n    scipy_output = \"numpy.arccos(x) + numpy.sign(y) + y**2\"\n    assert NumPyPrinter().doprint(expr) == np_output\n    assert SciPyPrinter().doprint(expr) == scipy_output\n"], "sample_976": ["def test_symbol_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    assert x.is_real is True\n    assert x.is_positive is True\n    assert x.is_commutative is True\n\n    y = Symbol('y', commutative=False)\n    assert y.is_commutative is False\n\n    z = Symbol('z', commutative=None)\n    raises(ValueError, lambda: z.is_commutative)\n", "def test_Symbol_assumptions():\n    a = Symbol(\"a\", commutative=False)\n    b = Symbol(\"b\", real=True)\n\n    assert a.is_commutative is False\n    assert b.is_real is True\n    assert b.is_commutative is True\n\n    c = Symbol(\"c\", commutative=None)\n    raises(ValueError, lambda: c)\n", "def test_symbol_assumptions():\n    a = Symbol('a', real=True, positive=True)\n    assert a.is_real\n    assert a.is_positive\n\n    b = Symbol('b', real=False, positive=False)\n    assert not b.is_real\n    assert not b.is_positive\n\n    c = Symbol('c', integer=True, odd=True)\n    assert c.is_integer\n    assert c.is_odd\n\n    d = Symbol('d', integer=False, odd=False)\n    assert not d.is_integer\n    assert not d.is_odd\n\n    e = Symbol('e', commutative=False)\n    assert not e.is_commutative\n\n    f = Symbol('f', commutative=True)\n    assert f.is_commutative\n", "def test_free_symbols():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert x.free_symbols == {x}\n    assert (x + y).free_symbols == {x, y}\n    assert (x*y*z).free_symbols == {x, y, z}\n    assert (x**2).free_symbols == {x}\n    assert (x*y + z).free_symbols == {x, y, z}\n    assert (x + y).subs(x, z).free_symbols == {y, z}\n    assert (x + y).subs({x: z}).free_symbols == {y, z}\n", "def test_symbol_comparison():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert x != y\n    assert x != z\n    assert y != z\n\n    x_eq = Symbol('x')\n    assert x == x_eq\n    assert x != x_eq.as_dummy()\n\n    x_comm = Symbol('x', commutative=False)\n    assert x_comm != x\n    assert x_comm.is_commutative is False\n    assert x_comm == x_comm.as_dummy()\n"], "sample_907": ["def test_domain_cpp_build_noindex(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"noindex\")\n    assert len(ws) == 0\n", "def test_domain_cpp_parse_exception_specification(app):\n    text = (\".. cpp:function:: void f() noexcept\\n\"\n            \".. cpp:function:: void g() throw()\\n\"\n            \".. cpp:function:: void h() throw(int)\\n\"\n            \".. cpp:function:: void i() throw(int, double)\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc, addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C++ function)', '_CPPv41gv', '', None)])\n    assert_node(doctree[4], addnodes.index, entries=[('single', 'h (C++ function)', '_CPPv41hv', '', None)])\n    assert_node(doctree[6], addnodes.index, entries=[('single', 'i (C++ function)', '_CPPv41iv', '', None)])\n", "def test_domain_cpp_parse_qualified_noindexentry(app):\n    text = (\".. cpp:class:: A\\n\"\n            \".. cpp:function:: void A::f()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. cpp:function:: void A::g()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'A (C++ class)', '_CPPv41A', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'A::g (C++ function)', '_CPPv4N1A1gEv', '', None)])\n", "def test_domain_cpp_parse_multiple_function_decls():\n    text = (\".. cpp:function:: void f()\\n\"\n            \".. cpp:function:: void g()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, desc))\n    assert_node(doctree[0], desc, signatures=[('void f()', 'f')])\n    assert_node(doctree[1], desc, signatures=[('void g()', 'g')])\n", "def test_domain_cpp_build_operator_overloading(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    operatorPatterns = [\n        ('ref operator without args ', r'Operator::operator\\+\\+\\(\\)'),\n        ('ref operator with args ', r'Operator::operator\\+\\+\\(int\\)'),\n        ('ref operator without args, explicit title ', 'operator_plus_plus_title'),\n        ('ref operator with args, explicit title ', 'operator_plus_plus_int_title')\n    ]\n\n    f = 'operator-overloading.html'\n    t = (app.outdir / f).read_text()\n    for s in operatorPatterns:\n        check(s, t, f)\n"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with default feature names\n    sel.fit(X)\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check with input features not matching\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(feature_names[1:])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check that it works when input_features is None\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check that it raises error when feature names are not available\n    sel = StepSelector()\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out()\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n"]}, "raw_scored_generations": {"sample_0": [false, false, false, false, false], "sample_3": [false, false, false, true, false], "sample_1": [false, false, false, false, false], "sample_5": [true, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_2": [true, true, true, true, true], "sample_6": [false, false, false, false, true], "sample_7": [true, true, true, false, true], "sample_8": [false, false, false, false, false], "sample_9": [true, true, false, false, false], "sample_10": [false, false, true, true, true], "sample_11": [false, true, false, false, true], "sample_12": [false, false, true, false, false], "sample_13": [true, false, false, false, false], "sample_14": [true, true, true, false, true], "sample_15": [false, false, false, false, false], "sample_16": [false, false, true, false, false], "sample_17": [false, false, false, false, false], "sample_18": [false, false, false, false, false], "sample_19": [true, false, true, false, false], "sample_20": [false, false, false, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [true, true, false, false, false], "sample_24": [false, false, false, false, false], "sample_25": [false, false, false, false, false], "sample_26": [true, false, false, false, false], "sample_27": [false, false, false, false, false], "sample_28": [false, false, false, false, false], "sample_29": [false, false, false, false, false], "sample_30": [true, true, true, false, false], "sample_31": [false, false, false, false, false], "sample_32": [false, false, false, false, false], "sample_33": [false, false, false, false, false], "sample_34": [false, true, false, false, false], "sample_35": [false, false, false, true, false], "sample_36": [false, true, false, false, false], "sample_37": [false, false, false, false, false], "sample_38": [false, false, true, true, false], "sample_39": [true, true, false, true, false], "sample_40": [false, false, false, false, false], "sample_41": [true, false, false, true, true], "sample_42": [false, false, false, false, false], "sample_43": [false, false, true, true, false], "sample_44": [false, false, false, false, false], "sample_45": [false, true, false, true, true], "sample_46": [true, false, true, true, true], "sample_47": [true, true, false, true, false], "sample_48": [true, true, true, true, true], "sample_49": [false, true, false, true, false], "sample_50": [true, true, true, true, true], "sample_51": [false, false, false, false, false], "sample_52": [true, true, true, true, true], "sample_54": [true, true, true, true, true], "sample_53": [true, false, true, true, false], "sample_55": [false, true, true, false, true], "sample_58": [true, false, false, false, false], "sample_56": [true, true, true, false, true], "sample_57": [false, false, true, true, true], "sample_59": [true, true, true, true, true], "sample_60": [false, false, false, false, false], "sample_61": [true, false, false, true, true], "sample_62": [true, true, true, true, true], "sample_63": [true, false, true, true, false], "sample_64": [false, true, false, true, true], "sample_65": [true, true, true, true, true], "sample_67": [true, false, true, true, true], "sample_66": [true, false, true, true, false], "sample_68": [false, true, true, false, false], "sample_69": [true, true, true, true, true], "sample_70": [true, false, false, true, false], "sample_71": [false, false, false, false, false], "sample_72": [true, true, false, true, true], "sample_73": [true, true, true, true, false], "sample_75": [true, false, true, true, true], "sample_74": [true, true, true, true, true], "sample_76": [false, true, true, true, true], "sample_77": [true, true, false, true, true], "sample_78": [false, false, false, true, true], "sample_79": [false, false, false, false, false], "sample_80": [true, true, true, true, true], "sample_82": [true, true, true, true, true], "sample_81": [true, false, false, false, false], "sample_83": [true, false, false, false, false], "sample_85": [true, true, false, true, false], "sample_84": [true, true, true, false, true], "sample_86": [true, true, true, true, true], "sample_88": [false, true, true, true, true], "sample_87": [true, true, true, true, true], "sample_89": [false, true, true, true, true], "sample_90": [false, true, false, false, false], "sample_91": [true, true, true, true, true], "sample_92": [false, false, false, false, false], "sample_93": [true, true, true, true, true], "sample_94": [true, false, true, false, false], "sample_95": [false, false, false, true, true], "sample_98": [true, false, false, false, false], "sample_96": [false, false, true, false, false], "sample_99": [true, true, false, true, true], "sample_97": [true, true, true, true, true], "sample_100": [false, true, true, false, true], "sample_102": [true, true, true, true, true], "sample_101": [true, false, true, false, false], "sample_103": [true, true, true, true, true], "sample_104": [false, true, true, true, true], "sample_107": [true, true, false, false, true], "sample_106": [true, true, true, true, true], "sample_105": [false, true, false, true, true], "sample_108": [false, true, true, true, false], "sample_109": [false, false, true, false, true], "sample_111": [true, true, true, true, true], "sample_110": [true, true, true, true, true], "sample_112": [true, true, true, true, false], "sample_113": [true, true, true, true, false], "sample_114": [true, true, true, true, true], "sample_115": [true, true, false, false, false], "sample_116": [true, true, false, false, true], "sample_117": [false, true, true, false, true], "sample_118": [true, true, true, true, true], "sample_119": [true, true, true, true, true], "sample_120": [false, true, true, true, false], "sample_121": [true, true, true, true, true], "sample_122": [true, true, true, true, true], "sample_123": [true, true, true, false, true], "sample_124": [true, true, true, true, true], "sample_125": [true, true, true, true, true], "sample_126": [true, true, true, true, true], "sample_127": [true, true, true, true, true], "sample_128": [true, true, true, true, true], "sample_129": [true, true, true, false, false], "sample_130": [true, true, true, true, true], "sample_131": [false, false, true, true, true], "sample_132": [false, false, true, false, true], "sample_133": [true, true, true, true, true], "sample_135": [true, true, false, true, true], "sample_134": [true, true, false, true, false], "sample_136": [true, true, true, true, true], "sample_139": [true, true, false, true, true], "sample_137": [true, true, true, true, true], "sample_138": [true, true, true, true, true], "sample_140": [false, false, false, false, false], "sample_141": [true, true, false, true, true], "sample_142": [true, true, true, true, false], "sample_143": [true, true, true, true, true], "sample_144": [true, true, true, true, true], "sample_145": [false, false, false, false, false], "sample_146": [true, true, true, true, false], "sample_147": [true, true, true, true, true], "sample_148": [true, true, true, true, true], "sample_151": [true, true, true, true, true], "sample_149": [false, false, false, false, false], "sample_152": [true, false, true, true, true], "sample_150": [true, false, true, true, true], "sample_153": [true, false, true, true, true], "sample_154": [false, false, false, true, false], "sample_155": [true, true, true, true, true], "sample_156": [true, true, true, true, true], "sample_157": [false, false, false, false, false], "sample_158": [true, true, true, true, true], "sample_159": [true, true, true, false, false], "sample_160": [true, true, false, false, false], "sample_161": [true, true, true, true, false], "sample_162": [true, true, true, true, true], "sample_163": [true, true, false, false, true], "sample_164": [false, false, false, false, false], "sample_165": [true, true, true, true, false], "sample_166": [true, true, true, true, true], "sample_167": [false, false, false, false, false], "sample_168": [false, true, true, true, true], "sample_169": [true, true, true, true, true], "sample_171": [true, true, true, true, true], "sample_170": [false, false, false, true, false], "sample_172": [false, true, true, true, true], "sample_173": [true, true, true, false, true], "sample_174": [true, true, true, true, true], "sample_175": [true, true, false, true, true], "sample_176": [true, true, false, true, true], "sample_177": [false, true, true, true, true], "sample_178": [false, true, true, true, true], "sample_180": [true, true, true, false, true], "sample_179": [true, false, true, true, true], "sample_182": [true, true, true, true, true], "sample_181": [true, true, true, true, true], "sample_183": [true, true, true, true, true], "sample_184": [true, true, true, true, true], "sample_185": [true, true, true, true, true], "sample_186": [true, true, true, true, true], "sample_187": [true, false, false, true, true], "sample_188": [true, true, true, true, true], "sample_189": [true, true, true, true, true], "sample_190": [true, true, true, true, true], "sample_191": [true, true, true, true, true], "sample_192": [true, true, false, false, true], "sample_193": [true, true, true, true, true], "sample_194": [true, true, true, true, true], "sample_195": [true, true, true, true, false], "sample_196": [true, true, false, true, true], "sample_198": [true, true, true, true, true], "sample_197": [true, true, false, true, true], "sample_199": [true, true, true, true, true], "sample_200": [true, true, true, true, true], "sample_201": [true, true, true, true, true], "sample_202": [true, true, true, true, true], "sample_203": [true, true, true, true, true], "sample_204": [true, true, true, true, true], "sample_205": [true, false, true, true, false], "sample_206": [false, true, false, true, true], "sample_207": [true, true, true, true, true], "sample_208": [true, true, true, true, false], "sample_209": [false, true, true, true, true], "sample_210": [true, true, false, true, false], "sample_211": [true, true, true, false, false], "sample_213": [true, false, false, true, false], "sample_212": [false, false, false, false, false], "sample_214": [true, true, true, true, true], "sample_215": [false, false, false, false, true], "sample_216": [true, true, true, true, true], "sample_217": [true, false, false, false, false], "sample_218": [true, false, false, true, true], "sample_219": [true, true, true, true, true], "sample_220": [true, true, true, true, true], "sample_221": [true, true, true, true, true], "sample_222": [true, false, true, false, false], "sample_223": [false, false, true, false, true], "sample_224": [true, true, true, true, true], "sample_225": [false, false, false, false, false], "sample_226": [true, false, false, false, false], "sample_227": [true, true, true, true, true], "sample_228": [true, true, true, true, true], "sample_229": [true, true, true, true, true], "sample_230": [true, true, true, true, false], "sample_231": [false, true, false, true, false], "sample_232": [true, true, true, true, true], "sample_233": [true, true, true, true, true], "sample_234": [true, true, true, true, true], "sample_235": [true, true, false, true, true], "sample_236": [true, true, true, true, false], "sample_237": [true, false, true, false, false], "sample_238": [true, true, true, true, true], "sample_239": [true, true, true, true, true], "sample_240": [true, true, true, true, true], "sample_241": [true, true, true, true, true], "sample_242": [true, false, false, true, true], "sample_243": [true, true, true, true, false], "sample_244": [true, true, true, false, true], "sample_245": [true, true, true, true, true], "sample_246": [true, true, true, true, true], "sample_247": [true, true, true, true, true], "sample_248": [true, false, true, true, true], "sample_249": [false, false, false, false, false], "sample_250": [true, true, false, true, false], "sample_251": [true, true, true, true, true], "sample_252": [true, true, true, true, true], "sample_253": [true, false, true, true, false], "sample_254": [true, true, true, true, false], "sample_256": [false, false, false, false, true], "sample_255": [true, true, true, true, true], "sample_257": [true, true, false, true, true], "sample_258": [false, false, false, false, false], "sample_259": [true, true, true, true, true], "sample_260": [true, true, true, true, true], "sample_261": [false, false, false, false, true], "sample_262": [true, false, false, false, true], "sample_263": [true, true, true, true, true], "sample_264": [false, true, true, true, true], "sample_265": [true, true, true, true, true], "sample_266": [true, true, true, true, true], "sample_267": [true, true, false, false, false], "sample_268": [true, false, false, true, true], "sample_269": [true, true, true, true, true], "sample_270": [true, true, true, true, true], "sample_271": [false, true, true, true, true], "sample_272": [false, false, false, true, true], "sample_273": [true, true, false, false, false], "sample_274": [true, true, true, true, false], "sample_275": [false, true, false, true, true], "sample_276": [true, true, true, true, false], "sample_277": [true, true, true, true, true], "sample_278": [true, true, true, true, true], "sample_279": [true, true, true, false, true], "sample_280": [true, true, true, true, true], "sample_281": [true, true, true, true, true], "sample_282": [true, true, true, true, true], "sample_283": [true, true, true, true, true], "sample_284": [true, false, true, true, true], "sample_285": [true, true, true, true, true], "sample_286": [true, true, true, true, true], "sample_287": [true, true, true, true, false], "sample_288": [true, true, true, true, true], "sample_289": [true, false, true, true, true], "sample_290": [true, true, false, true, true], "sample_291": [true, true, true, true, true], "sample_292": [true, true, true, true, true], "sample_293": [false, false, false, true, true], "sample_294": [true, true, true, true, false], "sample_295": [true, false, true, true, true], "sample_296": [true, true, true, true, true], "sample_297": [true, true, true, true, true], "sample_298": [true, true, true, true, true], "sample_299": [true, true, true, true, true], "sample_300": [true, true, true, true, true], "sample_301": [true, false, true, true, true], "sample_302": [false, false, false, false, false], "sample_303": [true, true, false, true, false], "sample_304": [true, false, false, false, false], "sample_305": [true, true, true, true, true], "sample_306": [false, false, false, false, false], "sample_307": [true, true, true, true, true], "sample_308": [true, true, true, true, true], "sample_309": [true, false, true, false, true], "sample_310": [true, true, true, true, true], "sample_312": [true, true, true, true, true], "sample_311": [true, true, true, false, true], "sample_313": [true, true, true, false, false], "sample_314": [false, false, false, true, true], "sample_315": [false, false, false, false, false], "sample_316": [false, false, false, true, true], "sample_317": [true, true, true, true, true], "sample_318": [true, false, false, true, false], "sample_319": [true, true, true, false, false], "sample_320": [true, true, true, true, true], "sample_321": [true, true, true, true, true], "sample_322": [false, true, true, true, true], "sample_323": [true, true, false, true, false], "sample_324": [true, true, true, true, true], "sample_325": [true, true, false, false, true], "sample_326": [true, true, true, true, true], "sample_327": [true, true, true, false, true], "sample_328": [true, true, true, true, false], "sample_329": [true, false, false, true, true], "sample_330": [true, true, false, false, false], "sample_331": [false, false, false, true, false], "sample_332": [false, true, true, false, false], "sample_333": [true, true, true, true, true], "sample_334": [true, true, false, false, true], "sample_335": [true, true, true, true, true], "sample_336": [true, false, false, false, true], "sample_337": [true, true, true, true, true], "sample_338": [true, false, true, true, false], "sample_339": [true, true, true, true, true], "sample_340": [true, true, true, true, true], "sample_341": [true, false, true, true, false], "sample_342": [false, false, true, true, true], "sample_343": [true, true, false, true, true], "sample_344": [true, true, true, true, true], "sample_345": [true, false, true, false, true], "sample_346": [false, false, false, false, false], "sample_347": [true, true, true, true, true], "sample_348": [false, false, false, false, false], "sample_349": [true, true, true, true, true], "sample_350": [true, true, true, true, true], "sample_351": [true, true, true, false, true], "sample_352": [true, false, false, true, false], "sample_353": [true, true, false, true, true], "sample_354": [true, true, true, true, true], "sample_355": [false, false, false, false, false], "sample_356": [false, true, true, true, false], "sample_357": [true, false, true, true, false], "sample_358": [false, false, false, false, false], "sample_359": [true, true, false, true, true], "sample_360": [true, false, true, true, false], "sample_361": [true, true, false, true, true], "sample_362": [true, true, false, true, false], "sample_363": [true, true, true, false, false], "sample_364": [false, false, false, false, false], "sample_365": [true, true, true, false, false], "sample_366": [true, false, true, false, true], "sample_367": [false, false, false, false, false], "sample_368": [true, true, true, true, true], "sample_369": [true, true, true, true, true], "sample_370": [true, true, true, true, false], "sample_371": [false, true, false, true, false], "sample_372": [false, false, false, false, true], "sample_373": [true, true, true, true, true], "sample_374": [true, true, true, true, false], "sample_375": [true, true, false, true, true], "sample_376": [true, true, true, true, true], "sample_377": [false, false, false, false, true], "sample_378": [true, true, true, false, true], "sample_379": [true, true, true, false, true], "sample_380": [true, true, true, true, true], "sample_381": [false, true, true, true, true], "sample_382": [true, false, false, false, true], "sample_383": [false, false, false, true, false], "sample_384": [true, true, true, false, true], "sample_385": [true, true, true, true, false], "sample_386": [true, false, false, false, false], "sample_387": [false, true, true, true, true], "sample_388": [false, true, false, false, false], "sample_389": [true, false, true, false, false], "sample_390": [false, true, true, true, false], "sample_391": [true, true, true, true, true], "sample_392": [true, true, true, true, true], "sample_393": [true, true, true, true, true], "sample_394": [true, true, true, true, false], "sample_395": [true, true, true, true, false], "sample_396": [false, true, true, true, false], "sample_397": [true, true, true, true, true], "sample_398": [true, true, true, false, true], "sample_399": [true, true, true, true, true], "sample_400": [true, true, true, true, false], "sample_401": [true, false, true, true, true], "sample_402": [true, true, true, true, true], "sample_403": [true, false, true, true, true], "sample_404": [true, true, true, true, true], "sample_405": [true, true, true, false, false], "sample_406": [true, false, true, false, true], "sample_407": [true, true, true, true, true], "sample_408": [true, false, true, true, true], "sample_409": [true, true, true, true, true], "sample_410": [false, false, false, false, false], "sample_411": [true, true, true, true, true], "sample_412": [true, true, true, true, true], "sample_413": [true, true, true, true, true], "sample_414": [true, true, true, false, true], "sample_415": [true, true, true, true, false], "sample_416": [true, true, true, true, true], "sample_417": [true, true, true, true, true], "sample_418": [true, true, true, true, true], "sample_419": [true, false, true, true, true], "sample_420": [false, true, true, false, true], "sample_421": [true, true, true, true, true], "sample_422": [true, true, false, true, true], "sample_423": [true, true, true, false, true], "sample_424": [false, true, true, true, true], "sample_425": [true, true, true, true, false], "sample_426": [true, true, true, false, true], "sample_427": [true, true, true, false, false], "sample_428": [false, false, false, false, false], "sample_429": [true, true, true, false, true], "sample_430": [true, false, false, true, true], "sample_431": [true, true, true, true, true], "sample_432": [true, true, true, true, true], "sample_433": [true, true, false, false, false], "sample_434": [true, true, true, false, true], "sample_435": [false, true, false, false, false], "sample_436": [true, true, true, true, false], "sample_437": [true, true, false, false, false], "sample_438": [true, true, true, true, true], "sample_439": [true, true, true, true, true], "sample_440": [false, false, true, true, false], "sample_441": [true, false, false, true, true], "sample_442": [true, true, true, true, true], "sample_443": [true, true, true, true, false], "sample_444": [true, true, false, false, true], "sample_445": [true, true, true, true, true], "sample_446": [true, true, false, true, true], "sample_447": [true, true, true, true, true], "sample_448": [true, true, true, false, true], "sample_449": [false, false, false, true, false], "sample_450": [true, true, true, true, true], "sample_451": [true, true, true, true, false], "sample_453": [true, true, true, true, false], "sample_452": [true, true, true, false, false], "sample_454": [true, true, true, true, true], "sample_455": [true, true, true, true, true], "sample_456": [false, true, true, true, true], "sample_457": [true, false, true, true, true], "sample_458": [false, true, false, true, true], "sample_459": [true, true, false, true, false], "sample_460": [true, true, false, false, true], "sample_461": [true, true, true, true, true], "sample_462": [true, true, true, true, false], "sample_463": [true, true, true, true, true], "sample_464": [false, false, true, false, true], "sample_465": [false, true, true, false, false], "sample_466": [true, true, true, true, true], "sample_467": [true, true, true, true, true], "sample_469": [true, true, true, true, true], "sample_468": [false, true, true, true, false], "sample_470": [false, false, false, false, true], "sample_471": [true, true, true, true, true], "sample_472": [true, true, true, true, true], "sample_473": [false, false, false, false, false], "sample_474": [true, true, true, false, true], "sample_475": [false, false, false, false, false], "sample_476": [false, true, false, false, false], "sample_477": [true, true, true, true, true], "sample_478": [false, false, false, false, false], "sample_479": [true, true, true, true, true], "sample_480": [true, true, true, true, true], "sample_481": [true, true, true, true, true], "sample_482": [true, true, true, true, true], "sample_483": [true, true, true, true, true], "sample_484": [true, true, true, true, true], "sample_485": [true, true, true, true, true], "sample_486": [true, true, true, true, true], "sample_487": [false, false, false, false, false], "sample_488": [false, true, false, true, false], "sample_489": [true, true, true, true, true], "sample_490": [true, true, true, true, true], "sample_491": [true, true, true, false, false], "sample_492": [true, true, false, true, false], "sample_493": [true, true, true, true, true], "sample_494": [true, true, true, true, false], "sample_495": [true, true, false, false, true], "sample_496": [false, false, true, true, true], "sample_497": [true, true, false, false, false], "sample_498": [true, true, true, true, false], "sample_499": [false, false, true, false, false], "sample_500": [false, true, false, true, false], "sample_501": [false, false, false, false, false], "sample_502": [false, false, false, true, false], "sample_503": [false, false, false, false, false], "sample_504": [true, false, true, false, false], "sample_505": [false, false, false, false, true], "sample_506": [false, false, false, false, false], "sample_507": [false, true, true, true, false], "sample_508": [false, false, false, false, false], "sample_509": [false, true, false, false, false], "sample_510": [false, false, false, false, false], "sample_511": [true, true, false, false, true], "sample_512": [false, false, false, true, false], "sample_513": [true, false, false, true, false], "sample_514": [true, true, false, false, false], "sample_515": [true, false, true, false, false], "sample_516": [false, true, false, false, false], "sample_517": [false, false, false, false, false], "sample_518": [true, true, true, true, false], "sample_519": [false, true, true, true, false], "sample_520": [false, false, false, true, true], "sample_521": [false, false, false, false, false], "sample_522": [true, true, true, true, false], "sample_523": [false, true, true, true, false], "sample_524": [false, false, true, true, false], "sample_525": [false, false, false, false, false], "sample_526": [false, true, false, false, false], "sample_527": [false, false, false, false, false], "sample_528": [false, false, true, true, true], "sample_529": [false, false, false, false, false], "sample_530": [false, false, false, false, false], "sample_531": [true, false, false, false, true], "sample_532": [true, true, false, true, true], "sample_533": [true, false, false, true, false], "sample_534": [true, true, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, true, false, false], "sample_537": [false, false, true, false, false], "sample_538": [false, false, true, false, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [true, false, false, false, false], "sample_542": [false, false, false, false, false], "sample_543": [false, false, true, false, false], "sample_544": [false, false, false, false, false], "sample_545": [true, true, false, false, false], "sample_546": [true, false, false, false, false], "sample_547": [true, false, false, false, false], "sample_548": [true, false, false, false, true], "sample_549": [true, true, true, false, false], "sample_550": [false, false, true, false, false], "sample_551": [false, false, false, false, false], "sample_552": [false, false, false, false, false], "sample_553": [false, false, false, false, false], "sample_554": [true, true, true, true, true], "sample_555": [false, true, false, false, false], "sample_556": [true, true, false, false, true], "sample_557": [true, false, false, false, false], "sample_558": [false, false, false, true, false], "sample_559": [false, false, false, false, false], "sample_560": [true, false, true, false, true], "sample_561": [false, false, true, true, false], "sample_562": [true, true, false, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, true, false, false, true], "sample_566": [false, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, true, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, false], "sample_571": [false, false, false, false, false], "sample_572": [false, false, false, false, false], "sample_573": [false, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, false, false, false], "sample_577": [false, false, false, false, false], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [true, false, true, true, false], "sample_581": [false, false, false, false, false], "sample_582": [true, false, true, false, false], "sample_583": [false, true, false, false, false], "sample_584": [false, false, false, false, false], "sample_585": [false, false, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, true], "sample_590": [false, false, false, false, false], "sample_591": [false, false, false, false, false], "sample_592": [false, false, false, false, false], "sample_593": [false, true, false, false, false], "sample_594": [true, false, false, true, false], "sample_595": [false, false, false, false, true], "sample_596": [false, true, false, false, false], "sample_597": [false, false, false, false, false], "sample_598": [false, false, false, false, false], "sample_599": [true, false, false, false, false], "sample_600": [true, false, true, false, true], "sample_601": [false, false, false, false, false], "sample_602": [true, false, false, false, false], "sample_603": [false, true, false, true, false], "sample_604": [false, false, false, false, false], "sample_605": [false, false, false, true, false], "sample_606": [false, false, false, false, false], "sample_607": [false, false, false, true, false], "sample_608": [false, false, true, true, false], "sample_609": [false, false, false, false, true], "sample_610": [false, true, false, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, false, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [true, true, true, false, false], "sample_615": [false, false, false, false, false], "sample_616": [true, false, false, false, false], "sample_617": [false, true, false, false, false], "sample_618": [true, true, false, false, true], "sample_619": [true, false, false, false, false], "sample_620": [false, false, true, true, true], "sample_621": [false, false, false, false, false], "sample_622": [false, false, false, false, false], "sample_623": [false, false, false, false, false], "sample_624": [true, false, false, false, false], "sample_625": [false, false, false, false, false], "sample_626": [true, false, false, false, true], "sample_627": [false, false, false, false, false], "sample_628": [false, false, false, false, false], "sample_629": [false, false, false, false, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, true, false, false], "sample_634": [false, false, false, false, true], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, false, false, false], "sample_639": [false, false, false, true, false], "sample_640": [false, false, false, false, false], "sample_641": [true, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [true, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [true, true, false, true, false], "sample_646": [false, false, false, false, true], "sample_647": [false, false, false, false, false], "sample_648": [false, false, false, false, false], "sample_649": [true, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, false, false, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, false, false, false, false], "sample_656": [false, false, false, false, false], "sample_657": [false, false, false, false, false], "sample_658": [false, false, false, false, false], "sample_659": [false, false, false, false, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, true, false, false], "sample_666": [true, true, true, false, true], "sample_667": [false, false, false, true, false], "sample_668": [false, false, false, false, false], "sample_669": [false, false, true, false, false], "sample_670": [false, false, false, false, false], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, false, false, false], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [false, false, false, false, false], "sample_678": [false, false, false, true, false], "sample_679": [false, false, false, true, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, true, false], "sample_684": [false, false, false, false, true], "sample_685": [false, true, true, true, false], "sample_686": [false, false, false, false, false], "sample_687": [true, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [false, false, false, false, false], "sample_693": [false, false, false, false, true], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [false, false, false, false, false], "sample_698": [true, false, true, true, true], "sample_699": [false, false, false, false, false], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, false, false], "sample_703": [true, false, false, true, false], "sample_704": [false, true, false, false, false], "sample_705": [true, false, false, true, true], "sample_706": [false, false, false, false, false], "sample_707": [false, false, false, false, false], "sample_708": [true, false, false, false, false], "sample_709": [false, false, true, true, false], "sample_710": [false, false, false, false, false], "sample_711": [false, false, false, false, false], "sample_712": [false, false, false, false, false], "sample_713": [false, false, false, false, false], "sample_714": [false, true, false, false, true], "sample_715": [true, true, false, false, false], "sample_716": [false, false, true, false, false], "sample_717": [true, false, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [true, false, false, false, false], "sample_720": [false, false, false, false, false], "sample_721": [false, false, false, true, false], "sample_722": [false, true, false, true, true], "sample_723": [true, false, false, true, false], "sample_724": [false, false, true, false, false], "sample_725": [false, false, false, false, false], "sample_726": [true, false, false, true, false], "sample_727": [true, false, true, true, true], "sample_728": [true, false, true, true, false], "sample_729": [true, true, false, true, true], "sample_730": [true, false, true, false, false], "sample_731": [true, false, false, true, true], "sample_732": [false, false, true, false, false], "sample_733": [false, false, true, true, false], "sample_734": [true, true, false, true, true], "sample_735": [false, true, false, true, true], "sample_736": [false, false, false, false, true], "sample_737": [false, true, false, false, false], "sample_738": [true, true, false, true, false], "sample_739": [true, false, false, false, true], "sample_740": [false, false, true, false, false], "sample_741": [false, false, false, false, true], "sample_742": [false, true, false, false, false], "sample_743": [false, false, false, false, false], "sample_744": [false, false, false, false, false], "sample_745": [false, false, false, true, false], "sample_746": [false, false, false, false, false], "sample_747": [false, false, false, false, false], "sample_748": [true, false, true, false, false], "sample_749": [false, false, true, false, true], "sample_750": [false, false, true, false, false], "sample_751": [false, false, false, false, false], "sample_752": [true, true, false, false, false], "sample_753": [true, true, false, true, false], "sample_754": [false, false, false, false, false], "sample_755": [false, true, true, true, true], "sample_756": [false, false, false, true, false], "sample_757": [true, false, false, false, false], "sample_758": [false, false, false, false, false], "sample_759": [true, true, false, false, true], "sample_760": [false, false, false, false, false], "sample_761": [true, false, false, false, false], "sample_762": [true, true, false, true, true], "sample_763": [false, true, false, false, false], "sample_764": [false, false, false, false, true], "sample_765": [true, false, false, true, false], "sample_766": [true, true, false, true, false], "sample_767": [false, false, false, false, false], "sample_768": [false, false, true, false, true], "sample_769": [false, true, false, false, false], "sample_770": [false, true, false, true, false], "sample_771": [false, false, false, false, false], "sample_772": [false, false, true, true, false], "sample_773": [false, false, true, false, false], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [true, false, false, false, false], "sample_777": [false, false, false, false, false], "sample_778": [true, false, false, true, false], "sample_779": [true, false, false, false, false], "sample_780": [true, true, false, false, false], "sample_781": [true, false, false, false, false], "sample_782": [false, false, false, false, false], "sample_783": [false, true, false, false, false], "sample_784": [false, false, false, false, true], "sample_785": [false, false, false, false, false], "sample_786": [true, false, false, false, false], "sample_787": [false, false, false, false, false], "sample_788": [false, true, false, false, true], "sample_789": [false, false, false, false, false], "sample_790": [false, false, false, false, true], "sample_791": [false, false, false, false, false], "sample_792": [false, false, false, true, true], "sample_793": [false, false, false, false, false], "sample_794": [false, false, true, false, false], "sample_795": [false, false, false, false, false], "sample_796": [false, true, true, true, true], "sample_797": [false, false, false, false, false], "sample_798": [false, false, true, false, false], "sample_799": [false, false, false, false, false], "sample_800": [true, false, false, false, false], "sample_801": [false, false, false, false, false], "sample_802": [true, true, true, true, true], "sample_803": [false, true, true, true, false], "sample_804": [false, false, false, false, false], "sample_805": [false, true, false, false, false], "sample_806": [false, false, false, false, false], "sample_807": [false, false, true, false, false], "sample_808": [false, true, true, true, false], "sample_809": [true, true, true, true, false], "sample_810": [false, false, false, false, false], "sample_811": [true, true, true, true, false], "sample_812": [true, false, true, false, false], "sample_813": [false, true, false, true, true], "sample_814": [false, true, false, false, false], "sample_815": [false, true, true, false, false], "sample_816": [true, false, false, false, false], "sample_817": [false, false, false, false, false], "sample_818": [false, false, true, false, false], "sample_819": [false, false, true, true, true], "sample_820": [false, false, false, false, true], "sample_821": [true, true, true, false, false], "sample_822": [true, true, true, true, false], "sample_823": [true, true, true, false, true], "sample_824": [true, false, true, false, true], "sample_825": [true, true, false, true, false], "sample_826": [false, false, false, false, false], "sample_827": [false, false, false, true, false], "sample_828": [false, true, false, false, false], "sample_829": [true, true, true, true, false], "sample_830": [true, true, true, true, false], "sample_831": [false, true, true, true, true], "sample_832": [true, false, true, true, false], "sample_833": [false, false, false, false, false], "sample_834": [true, false, true, false, false], "sample_835": [false, true, true, false, true], "sample_836": [false, false, false, false, false], "sample_837": [true, false, true, true, true], "sample_838": [false, false, false, true, false], "sample_839": [false, true, false, false, false], "sample_840": [false, false, false, true, false], "sample_841": [false, false, false, false, false], "sample_842": [false, true, false, false, false], "sample_843": [false, false, false, false, false], "sample_844": [false, false, false, false, true], "sample_845": [false, false, false, false, false], "sample_846": [false, false, false, false, false], "sample_847": [true, false, true, true, false], "sample_848": [false, true, true, true, false], "sample_849": [false, true, false, false, false], "sample_850": [false, false, false, false, false], "sample_851": [true, true, true, false, false], "sample_852": [true, true, true, true, false], "sample_853": [true, true, true, true, true], "sample_854": [true, true, false, false, false], "sample_855": [false, true, false, false, false], "sample_856": [false, false, true, false, false], "sample_857": [false, false, false, false, false], "sample_858": [false, false, true, true, false], "sample_859": [true, false, false, false, false], "sample_860": [true, false, true, false, false], "sample_861": [false, false, false, false, false], "sample_862": [false, false, false, false, false], "sample_863": [false, true, false, false, false], "sample_864": [false, true, false, true, false], "sample_865": [false, false, false, false, true], "sample_866": [false, true, true, true, false], "sample_867": [false, false, false, true, false], "sample_868": [false, false, false, false, false], "sample_869": [true, false, false, false, true], "sample_870": [false, false, false, false, false], "sample_871": [false, true, true, false, false], "sample_872": [false, false, false, false, true], "sample_873": [true, true, false, false, true], "sample_874": [true, false, false, true, true], "sample_875": [false, false, false, false, false], "sample_876": [false, true, false, false, true], "sample_877": [true, false, true, false, true], "sample_878": [false, false, false, true, false], "sample_879": [false, false, true, false, false], "sample_880": [false, false, false, false, false], "sample_881": [false, false, false, true, false], "sample_882": [false, false, false, false, false], "sample_883": [false, true, false, true, true], "sample_884": [false, true, true, true, false], "sample_885": [true, true, true, true, true], "sample_886": [true, true, true, true, true], "sample_887": [false, false, false, false, true], "sample_888": [true, true, false, true, false], "sample_889": [false, false, false, false, false], "sample_890": [false, false, false, false, false], "sample_891": [false, false, false, false, false], "sample_892": [true, false, false, false, false], "sample_893": [false, true, false, false, false], "sample_894": [false, false, false, false, true], "sample_895": [false, true, false, true, false], "sample_896": [false, false, true, false, false], "sample_897": [false, false, false, false, false], "sample_898": [true, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [false, false, false, false, false], "sample_901": [true, false, true, false, true], "sample_902": [true, true, true, true, true], "sample_903": [true, true, false, false, false], "sample_904": [false, true, false, false, false], "sample_905": [true, true, false, false, false], "sample_906": [false, true, false, false, false], "sample_907": [true, false, false, false, false], "sample_908": [true, false, true, false, false], "sample_909": [false, false, false, false, false], "sample_910": [false, false, false, false, false], "sample_911": [false, true, false, false, false], "sample_912": [false, false, false, false, true], "sample_913": [false, false, true, false, true], "sample_914": [false, false, false, true, false], "sample_915": [false, false, true, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, true, true, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [true, true, false, false, false], "sample_922": [true, false, false, true, true], "sample_923": [false, false, false, false, false], "sample_924": [true, true, false, false, false], "sample_925": [false, false, true, false, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [true, false, false, true, false], "sample_930": [false, false, false, false, false], "sample_931": [true, false, true, false, true], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [true, true, true, false, false], "sample_937": [false, false, false, true, false], "sample_938": [false, false, false, false, false], "sample_939": [false, false, true, false, true], "sample_940": [false, false, false, false, false], "sample_941": [true, false, false, true, false], "sample_942": [false, true, true, true, false], "sample_943": [false, false, false, false, false], "sample_944": [true, true, true, false, true], "sample_945": [true, false, false, false, true], "sample_946": [false, true, false, false, true], "sample_947": [true, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, true, false, false], "sample_950": [false, false, false, false, false], "sample_951": [true, true, false, false, false], "sample_952": [false, false, false, false, false], "sample_953": [false, true, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [true, false, false, true, true], "sample_956": [false, false, true, false, false], "sample_957": [true, false, true, false, false], "sample_958": [true, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, true, false, false], "sample_961": [true, true, false, false, false], "sample_962": [false, false, false, false, false], "sample_963": [true, false, false, false, true], "sample_964": [false, true, false, false, false], "sample_965": [true, true, false, false, false], "sample_966": [false, false, false, false, false], "sample_967": [false, false, false, false, false], "sample_968": [false, true, false, false, true], "sample_969": [false, false, false, false, false], "sample_970": [true, false, false, false, false], "sample_971": [true, false, false, false, false], "sample_972": [false, true, false, true, false], "sample_973": [false, false, true, true, false], "sample_974": [false, false, false, false, false], "sample_975": [true, true, false, false, false], "sample_976": [false, false, false, false, false], "sample_977": [false, false, false, false, false], "sample_978": [false, false, false, true, false], "sample_979": [true, true, false, false, true], "sample_980": [true, true, true, false, true], "sample_981": [true, false, false, false, true], "sample_982": [false, false, false, false, false], "sample_983": [true, true, true, true, true], "sample_984": [false, true, true, true, false], "sample_985": [true, true, false, true, false], "sample_986": [false, false, false, false, false], "sample_987": [false, false, false, true, false], "sample_988": [false, false, false, false, false], "sample_989": [true, false, true, false, true], "sample_990": [false, false, false, false, false], "sample_991": [false, false, false, true, false], "sample_992": [true, false, false, true, false], "sample_993": [false, false, false, true, false], "sample_994": [true, true, false, true, true], "sample_995": [false, false, false, false, true], "sample_996": [false, true, false, false, false], "sample_997": [false, false, true, false, false], "sample_998": [false, false, false, true, false], "sample_999": [false, true, false, false, false], "sample_1000": [true, true, true, true, true], "sample_1001": [false, false, false, false, true], "sample_1002": [true, true, false, false, false], "sample_1003": [true, true, false, false, true], "sample_1004": [true, false, false, false, false], "sample_1005": [false, false, false, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, true, false, false], "sample_1008": [false, true, false, false, false], "sample_1009": [true, false, false, false, false], "sample_1010": [false, false, false, false, false], "sample_1011": [true, false, false, true, false], "sample_1012": [false, true, false, false, false], "sample_1013": [true, true, false, true, true], "sample_1014": [false, false, false, false, true], "sample_1015": [true, false, false, false, false], "sample_1016": [true, false, false, true, true], "sample_1017": [false, false, false, false, false], "sample_1018": [true, false, true, false, false], "sample_1019": [false, false, false, false, false], "sample_1020": [false, false, false, false, false], "sample_1021": [true, true, false, true, false], "sample_1022": [false, false, false, true, true], "sample_1023": [true, false, false, false, true], "sample_1024": [true, false, false, false, false], "sample_1025": [true, false, false, false, false], "sample_1026": [true, true, false, true, false], "sample_1027": [false, true, true, false, false], "sample_1028": [true, false, true, false, true], "sample_1029": [false, false, false, true, false], "sample_1030": [false, true, true, true, false], "sample_1031": [true, false, false, true, false], "sample_1032": [false, false, false, false, false], "sample_1033": [true, false, false, true, false], "sample_1034": [true, true, true, true, true], "sample_1035": [false, true, false, false, true], "sample_1036": [true, true, true, false, true], "sample_1037": [true, true, false, false, false], "sample_1038": [true, true, true, false, false], "sample_1039": [false, false, false, false, true], "sample_1040": [false, false, false, true, true], "sample_1041": [true, true, true, false, false], "sample_1042": [true, false, false, false, true], "sample_1043": [true, false, false, true, false], "sample_1044": [true, false, true, true, true], "sample_1045": [true, true, true, true, false], "sample_1046": [true, false, false, false, false], "sample_1047": [true, false, false, false, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, false, false, false], "sample_1050": [false, true, false, true, true], "sample_1051": [false, false, false, false, false], "sample_1052": [false, false, false, false, false], "sample_1053": [true, true, false, false, false], "sample_1054": [true, true, false, true, false], "sample_1055": [true, false, false, false, false], "sample_1056": [false, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [true, true, false, true, false], "sample_1059": [false, false, false, false, false], "sample_1060": [true, false, false, false, false], "sample_1061": [true, false, true, false, false], "sample_1062": [true, false, false, false, false], "sample_1063": [true, true, true, true, true], "sample_1064": [false, true, true, true, true], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, true, false, true, false], "sample_1068": [false, false, true, false, true], "sample_1069": [false, true, false, false, false], "sample_1070": [false, false, false, false, false], "sample_1071": [false, false, true, true, true], "sample_1072": [false, false, false, true, false], "sample_1073": [true, false, false, true, true], "sample_1074": [false, false, false, false, false], "sample_1075": [false, false, false, false, false], "sample_1076": [true, false, false, false, false], "sample_1077": [false, false, false, false, false], "sample_1078": [false, false, false, false, true], "sample_1079": [true, false, false, true, false], "sample_1080": [false, false, true, false, false], "sample_1081": [true, true, true, true, true], "sample_1082": [true, false, false, false, false], "sample_1083": [true, false, false, false, false], "sample_1084": [true, false, true, false, true], "sample_1085": [true, true, false, false, false], "sample_1086": [false, false, false, false, false], "sample_1087": [true, true, true, true, true], "sample_1088": [false, false, false, false, false], "sample_1089": [true, false, false, false, true], "sample_1090": [false, false, true, false, false], "sample_1091": [true, true, true, true, false], "sample_1092": [true, false, true, true, false], "sample_1093": [false, false, true, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [true, false, true, false, false], "sample_1096": [false, false, false, false, false], "sample_1097": [false, false, false, false, false], "sample_1098": [false, true, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, false, false, false, true], "sample_1101": [false, false, false, false, false], "sample_1102": [false, true, true, true, false], "sample_1103": [false, false, true, false, false], "sample_1104": [false, true, false, true, false], "sample_1105": [false, false, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [false, false, true, true, false], "sample_1108": [true, false, true, false, false], "sample_1109": [true, false, true, false, true], "sample_1110": [true, true, false, true, true], "sample_1111": [false, false, false, false, false], "sample_1112": [true, false, false, true, true], "sample_1113": [false, true, false, false, true], "sample_1114": [false, false, false, false, true], "sample_1115": [false, false, false, false, false], "sample_1116": [false, false, false, false, false], "sample_1117": [true, false, false, false, false], "sample_1118": [false, false, false, false, false], "sample_1119": [false, false, false, false, false], "sample_1120": [true, false, false, true, false], "sample_1121": [true, false, false, false, false], "sample_1122": [true, true, false, false, false], "sample_1123": [false, false, false, false, false], "sample_1124": [false, true, false, false, false], "sample_1125": [true, false, false, false, true], "sample_1126": [true, true, true, true, true], "sample_1127": [false, true, false, false, false], "sample_1128": [false, true, true, false, false], "sample_1129": [false, false, false, false, true], "sample_1130": [false, true, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, false, false, false], "sample_1133": [false, false, true, false, false], "sample_1134": [true, false, false, false, false], "sample_1135": [false, false, false, false, false], "sample_1136": [false, false, true, false, false], "sample_1137": [false, false, false, false, false], "sample_1138": [true, true, true, false, false], "sample_1139": [true, false, false, true, false], "sample_1140": [false, false, false, false, false], "sample_1141": [true, true, false, true, false], "sample_1142": [false, true, false, true, false], "sample_1143": [true, true, true, true, true], "sample_1144": [true, true, true, true, false], "sample_1145": [true, false, false, false, true], "sample_1146": [false, true, false, false, false], "sample_1147": [true, false, false, false, false], "sample_1148": [true, false, false, false, false], "sample_1149": [false, false, false, false, false], "sample_1150": [false, false, false, false, false], "sample_1151": [true, false, false, false, false], "sample_1152": [false, true, true, false, true], "sample_1153": [true, false, false, false, false], "sample_1154": [false, true, true, false, false], "sample_1155": [false, false, true, false, false], "sample_1156": [false, false, false, true, false], "sample_1157": [true, false, false, true, false], "sample_1158": [true, true, false, false, false], "sample_1159": [true, true, false, false, false], "sample_1160": [false, false, true, false, false], "sample_1161": [true, false, true, false, false], "sample_1162": [true, true, true, true, true], "sample_1163": [true, true, true, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, false, false, false], "sample_1166": [false, false, false, false, false], "sample_1167": [true, true, false, false, false], "sample_1168": [true, true, true, false, false], "sample_1169": [false, false, true, false, false], "sample_1170": [false, false, false, false, false], "sample_1171": [true, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, true, false, true, false], "sample_1174": [true, false, false, false, true], "sample_1175": [true, false, false, false, false], "sample_1176": [false, false, true, true, false], "sample_1177": [true, false, false, false, false], "sample_1178": [true, false, false, false, false], "sample_1179": [true, false, false, false, false], "sample_1180": [true, true, false, true, true], "sample_1181": [true, false, true, true, true], "sample_1182": [false, false, false, false, false], "sample_1183": [true, false, false, true, true], "sample_1184": [true, true, false, false, false], "sample_1185": [true, true, true, true, false], "sample_1186": [true, true, false, true, true], "sample_1187": [true, false, false, false, false], "sample_1188": [true, false, false, false, false], "sample_1189": [false, true, true, false, false], "sample_1190": [true, false, false, false, true], "sample_1191": [false, false, false, false, false], "sample_1192": [true, true, false, false, true], "sample_1193": [false, false, false, false, false], "sample_1194": [true, true, false, true, false], "sample_1195": [true, false, false, true, true], "sample_1196": [false, false, true, true, false], "sample_1197": [true, false, true, true, false], "sample_1198": [false, true, false, false, true], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [true, true, false, false, false], "sample_1203": [true, true, false, true, false], "sample_1204": [false, false, false, false, false], "sample_1205": [true, false, false, false, false], "sample_1206": [false, true, false, false, false], "sample_1207": [true, false, false, false, false], "sample_1208": [false, true, true, false, false], "sample_1209": [true, false, false, false, false]}}